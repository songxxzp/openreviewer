[
    {
        "title": "Predicting the Performance of Foundation Models via Agreement-on-the-line"
    },
    {
        "review": {
            "id": "Z9DUPDjgTl",
            "forum": "4Qz9BT4mpM",
            "replyto": "4Qz9BT4mpM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8448/Reviewer_sUjj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8448/Reviewer_sUjj"
            ],
            "content": {
                "summary": {
                    "value": "This work explores how to accurately predict the out-of-distribution performance of foundation models in the case where one does not have access to labels for the OOD data. The authors rely on an intriguing observation made in earlier works: In order to predict OOD performance, it often suffices to determine to what degree different models agree on the in-distribution data, compared to the OOD data. While this phenomenon has been established for models trained from scratch, the strategy has not been extended to the case of pre-trained models. Such an extension comes with challenges: how can one ensure model diversity if the same foundation model is fine-tuned multiple times? The authors explore multiple methods to ensure diversity for linear probing for CLIP and demonstrate that accurate OOD prediction can be achieved if the linear heads are randomly initialised. The authors go one step further and demonstrate that reliable OOD prediction can even be achieved when employing several foundation models, that all are pre-trained on different data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Practitioners these days largely rely on fine-tuning foundation models instead of training models from scratch. Thus extending the techniques of Baek et al. to cover the case of fine-tuning is very valuable and might prove very useful to practice. I thus think the results in this work are a timely contribution.\n2. I think understanding how to induce diversity when dealing with a single foundation model is an interesting question on its own, especially when thinking of building ensembles based on multiple such training runs. The findings presented in this work are very surprising with that regard, e.g. using different initialisations for linear heads seems to play a very important role, which goes against my personal intuition. \n3. The method seems to work pretty well empirically across different modalities, if things are tuned correctly, which is very encouraging and makes the contributions more relevant also to practically-oriented people."
                },
                "weaknesses": {
                    "value": "1. I think the authors use the word **diversity** without explaining too much what exactly they mean and what is needed in order to make the agreement-on-the-line method work. Clearly there needs to be some diversity (otherwise all models would agree everywhere as correctly pointed out by the authors) but can there be too much diversity? This is especially confusing because the authors find that introducing stochasticity solely by using different initialisations is actually the most performant choice. But clearly, this cannot be more diverse than further using different batch orderings or even subsampling the data? Linear probing should even be a convex problem, so all runs should actually converge to the same minimum even from different initialisations, given that optimisation is performed for long enough. Could you elaborate on the role of diversity, e.g. can there be too much diversity? I think explaining this better would really help the subsequent empirical exploration.\n2. In general, many empirical phenomena are observed but the authors do not really make an attempt at explaining them. Why does only using differently initialised random heads give the \u201cright\u201d amount of diversity? Why do you observe a way higher rate of agreement OOD compared to ID when using other techniques such as data shuffling and subletting etc? If anything, I would have expected more agreement in-distribution as all the models at least were optimised for this. What do you mean by strictly lying on the diagonal line y=x? Wouldn\u2019t that suggest that in-distribution and out-of-distribution agreement are of the same magnitude? \u2028What happens if instead of linear probing, you perform full fine-tuning in case of CLIP? Does the additional diversity also hurt the predictive performance?\n3. It remains a bit unclear to me to what degree this method needs to be first validated before the results can be trusted. At least for linear probing with CLIP, getting the amount of diversity right seems to be very tricky as the method is highly unstable to small deviations. Moreover, in almost all cases, the experiments still show agreement-on-the-line, but don\u2019t necessarily correlate with test accuracy (i.e. sharing the same slope and intercept). Thus observing agreement-on-the-line does not suffice to conclude that extrapolated accuracy values will actually be accurate. I would appreciate if the authors could discuss more how their observations regarding diversity relate to the prior work Baek et al. Is the method significantly more stable to changes in the training protocol when training from scratch?"
                },
                "questions": {
                    "value": "1. What are the units of the x-axis and y-axis for Figure 1 and Figure 2? How is agreement measured? Is this a log-log scale? How are the linear fits obtained? \n2. What are the absolute test values in Table 1 and Table 2? Does the method tend to over-estimate or under-estimate the true test accuracy? It\u2019s also not clear how significant a deviation of 5% is if the absolute values are not known. E.g. if test performance is 20%, then a deviation of 5% is clearly more significant than if test performance is 95%."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750290028,
            "cdate": 1698750290028,
            "tmdate": 1699637053853,
            "mdate": 1699637053853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JKtNFTYuE8",
                "forum": "4Qz9BT4mpM",
                "replyto": "Z9DUPDjgTl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your time and constructive feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you very much for your time and thorough feedback! We have now made the following changes in response to your feedback:\n1. To illustrate the significance of the random head initialization for obtaining ensembles exhibiting Agreement-on-the-line (AGL)\n    - We study CLIP full fine-tuning with CIFAR10 on CIFAR10-C Shifts (see [Table 7](https://i.imgur.com/RmhQc3O.png))\n    - We extend the results of CLIP linear probe fine-tuning to the Office-Home dataset (see [Table 6](https://i.imgur.com/eYkJbL5.png)) \n    - We train OPT and BERT models with linear probe fine-tuning for Question Answering (see [Table 8 and 9](https://i.imgur.com/qMePdYh.png))\n    - We add a new Text Classification study (with both linear probe, and full fine-tuning) using GPT, OPT, and LLaMa (see [Table 10](https://i.imgur.com/0bKxH0V.png))\n2. We have changed the metric from MAE (Mean Absolute Error) to MAPE (Mean Absolute Percentage Error) to show the significance of the OOD accuracy estimates, as the MAPE captures the relative not just absolute deviation from the true value. We have also added a table with the MAE for different OOD accuracy ranges ([Appendix Section 9.12](https://i.imgur.com/5bzYHUb.png)) to show the consistent performance of ALine across high- and low-accuracy models.\n3. We have provided some preliminary intuition as to the specific advantage of random linear head initialization for obtaining ensembles which exhibit AGL in the response below. We can add this in our paper if it would be helpful. \nWe address your concerns below in more detail.\n\n### **[Use of the word diversity] I think the authors use the word diversity without explaining too much what exactly they mean and what is needed in order to make the agreement-on-the-line method work. Clearly there needs to be some diversity (otherwise all models would agree everywhere as correctly pointed out by the authors) but can there be too much diversity?**\nThank you for the feedback! We apologize for the confusion caused by our overloaded use of the term diversity. \n1. At a high level, like you rightly said, we need some diversity in model predictions, else the models will agree too much. However, not just any kind of diversity is sufficient\u2014for example, just adding noise to the models\u2019 predictions would make them diverse, but not in a way that would help us use agreement to predict OOD accuracy. It is not just an overall measure of diversity that matters, but the \u201ctype\u201d or \u201cform\u201d of diversity. \n2. This paper shows that random head initialization leads to the right kind of diversity in the predictions of lightly fine-tuned models; thus causing the agreement and accuracy lines to have the same slope and intercept i.e. Agreement-on-the-line (AGL). Other sources of diversity lead to just ACL (i.e. strong linear correlation between the ID and OOD accuracies). \n3. We will make this clear in our revised draft, that the big challenge in using foundation models is introducing some diversity, as just having any diverse ensemble on its own is not sufficient. \n4. If we use models that are trained on drastically different training sets, they might be \u201ctoo diverse\u201d, in that the accuracies of these models might follow completely different linear trends between ID and OOD accuracy (i.e. ACL itself might not hold). We clarify again that we do not claim to have a precise measure of diversity, and even more so, AGL requires the right type of diversity. \n\n### **[Conceptual understanding] Why does only using differently initialized random heads give the \u201cright\u201d amount of diversity?**\nThis is indeed a surprising observation, and we think it would be interesting future work to understand this precisely. In particular, we find this observation to be true even for linear probing on CLIP features, which is potentially amenable to theoretical analysis. Our (admittedly imprecise) intuition is that since we are doing only a few steps of fine-tuning, we never move too far from the random head initialization. Hence with different initializations, we end up with models that can be quite \u201cfar\u201d from each other, despite starting from the same foundation model. This paper [3] shows that random initialization dictates how the pretrained features end up changing, and provides some insight into why the head initialization can be so important. On the other hand, if we fix the random initialization, we might not end up moving too far, and as a result even with different data subsetting or data ordering, we might end up with very similar models that agree a lot ID and OOD, i.e. causing the agreement line to lie much higher than the accuracy line."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204840568,
                "cdate": 1700204840568,
                "tmdate": 1700204840568,
                "mdate": 1700204840568,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VE2UCVkSXb",
            "forum": "4Qz9BT4mpM",
            "replyto": "4Qz9BT4mpM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8448/Reviewer_47Ds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8448/Reviewer_47Ds"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies \"agreement on the line\" (AGL) phenomenon for the foundation model setting which can be used to predict out-of-distribution (OOD) accuracy without OOD labels. Roughly, AGL implies that for an appropriately chosen family of models, the in-distribution (ID) and OOD accuracies lie on a straight line (accuracy on line - ACL) and so does the ID and OOD agreements between pairs of models, and that these lines are the same. Since agreement for a family of models can be measured using ID and OOD unlabeled data, and ID accuracy can be measured using ID labeled data, one can predict OOD accuracy by estimating the AGL line. \n\nThis phenomenon has been previously studied and reported for the supervised learning setting. However the paper argues that AGL is challenging for foundation model setting:\n- For light fine-tuning from a single foundation model, it is hard to get a diverse set of models to observe AGL\n- For multiple foundation models, the set of models might be too diverse and AGL might fail\n\nThe paper puts AGL to test for (1) linear probe with CLIP on CIFAR & (2) fine-tuning with language model(s). It considers 4 different types of model families by varying (a) random head initialization, (b) hyperparameters, (c) data subsets, (d) data ordering. The main finding is that **\"random head initialization\" is the only setting that demonstrates AGL, for linear probe and finetuning.** Thus for this setting, careful selection of model family is necessary to observe AGL.\n\nFor the multiple foundation model setting, the paper finds that **AGL holds across a family of 41 language models** for QA finetuning, despite the LLMs being pretrained on different data sources. This contrasts earlier findings for pretrained models in vision setting. The presence of AGL allows better OOD accuracy prediction that other methods based on *confidence* based predictions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Originality of findings: It is an interesting finding that AGL also holds in fine-tuning setting, although with carefully selected model family. Also interesting to see that random head initialization is the best and only setting for which AGL holds. AGL across different pretrained LLMs is also interesting, given that the same doesn't hold for vision setting.\n\n- Clarity: The high level message and results are clearly presented. Some details could be presented better; see comments/questions below."
                },
                "weaknesses": {
                    "value": "- Novelty: The paper evaluates an existing idea of AGL using existing metrics, but in a different (but relevant) setting of foundation model fine-tuning and linear probing. It does not propose a new technique or a new evaluation method or a drastically new perspective, to the best of my knowledge. The findings are new mostly because of the new setting that is being considered.\n\n- Clarity: Some details are either deferred to the appendix (definition of ALINE) or the reader is directed to prior work (methods that utilize model confidence in Section 4.2), which made it harder to follow some details. Figures captions could also use more details and be more self-contained (e.g.  Figure 3, Figure 4 -- which column is what method?). More comments/questions below\n\n- While the paper presents findings on when AGL holds in the fine-tuning setting, there is not much insight into why these findings might hold. E.g. is there any intuition for why random head initialization leads to AGL and others do not?\n\nOverall it seems like the results are \"good to know\", but do not necessarily provide a lot of new insights or food for thought. A deeper analysis of some of the findings could make the paper significantly stronger in my opinion. Given that I did not find any major flaws, I would assign a score of weak accept."
                },
                "questions": {
                    "value": "- In Figure 4, which column corresponds to which method? Visually it seems like the second column is the best, and for no column does it seem like AGL always holds.\n\n- In Section 4.1 the phrase \u201censemble of foundation models\u201d is a bit confusing. Does it mean an ensemble in the machine learning sense or just a \"group\" of models?\n\n- In Section 4.2, how is temperature scaling relevant in this setting?\n\n- What is $\\Phi$ in Section 8.3?\n\n- Is there a specific reason to only do linear probing for the CLIP setting, and just 1 fine-tuning task of SQuAD for the LM setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811048641,
            "cdate": 1698811048641,
            "tmdate": 1699637053737,
            "mdate": 1699637053737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fM2pRNWk0a",
                "forum": "4Qz9BT4mpM",
                "replyto": "VE2UCVkSXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your time and constructive feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your time and feedback! We have now made the following changes in response to your feedback:\n1. We have added a paragraph in the introduction summarizing our main contributions and what insights we add compared to prior works\n2. We have created a new Section 5 discussing the comparison between the ALine method and the other baseline methods. \n3. We have made the table and image captions more descriptive.\n4. We have expanded our study of CLIP for Image Classification to full fine-tuning, and shown that the random linear head initialization is still the only method consistently yielding ensembles exhibiting Agreement-on-the-line (see [Table 7](https://i.imgur.com/RmhQc3O.png)).\n5. We have provided some preliminary intuition as to the specific advantage of random linear head initialization for obtaining ensembles which exhibit AGL in the response below. We could add it to the paper if it\u2019s helpful.  \n\nWe address your concerns below in more detail:\n\n&nbsp;\n\n### **Novelty and new perspectives**\nThank you for your feedback! We agree that the main \u201cnovelty\u201d in our study is that we study a new setting of fine-tuning foundation models. Methodologically, we do borrow heavily from prior work of Agreement-on-the-line [1], but we make some important changes to make it work in this setting. Prior work uses an ensemble of independently trained models, but this is intractable in the new setting. Therefore, we study three different ensembles that are tractable, and find that random head initialization is the only ensemble that works, but it works surprisingly well! We think our observations add the following new insights into the broader phenomenon of \u201cagreement-on-the-line\u201d, some of which surprisingly contradict hypotheses or suggestions made in prior works.\n1. No prior work shows any distinction between the agreements afforded by different ways of generating ensembles\u2014for example data subsetting, or different random initializations etc. We find that random head initialization is therefore a special method of introducing diversity. We hope our work inspires future theoretical studies on the effect of random head initialization. \n2. [1] found that linear models when trained from scratch do not show AGL. However, we see that linear models on top of CLIP features show AGL, suggesting that it\u2019s less a property of the model family and more a property of the data distribution. This observation also makes it more tractable to study AGL theoretically since linear models are easier to analyze than neural nets\n3. Prior work in ACL [2, 5] suggested that different pre-training data for vision models would lead to different effective robustness i.e. the trend between ID and OOD performance. We find the contrary in Section 4, that different pretrained language models can observe the same robustness. \n\n&nbsp;\n\n### **Intuition for why random head initialization is different**\nOur (admittedly imprecise) intuition is that since we are doing only a few steps of fine-tuning, we never move too far from the random head initialization. Hence with different initializations, we end up with models that can be quite \u201cfar\u201d from each other, despite starting from the same foundation model. This paper [4] shows that random initialization dictates how the pretrained features end up changing, and provides some insight into why the head initialization can be so important. On the other hand, if we fix the random initialization, we might not end up moving too far, and as a result even with different data subsetting or data ordering, we might end up with very similar models that agree a lot (agreement line is much higher than accuracy line). If this explanation is helpful, we\u2019d be happy to add it to our paper or discuss more. \n\n&nbsp;\n\n### **Some details are either deferred to the appendix (definition of ALINE) or the reader is directed to prior work (methods that utilize model confidence in Section 4.2), which makes it harder to follow some details.**\nThank you for the feedback! We have now added a sentence to summarize what ALine is doing in our baseline comparison section (Section 5)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203356941,
                "cdate": 1700203356941,
                "tmdate": 1700203356941,
                "mdate": 1700203356941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "epZVIMjrLQ",
                "forum": "4Qz9BT4mpM",
                "replyto": "VE2UCVkSXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Continued (2)"
                    },
                    "comment": {
                        "value": "### **Figures captions could also use more details and be more self-contained (e.g. Figure 3, Figure 4 -- which column is what method?).**\nThanks for pointing this out, and apologies for the unwanted confusion. We have rewritten all of our table and image captions to briefly recap the experiment, the dataset and methodology, and the drawn conclusion. Specifically, \n1. In Figure 3, we have updated our caption to -  _ACL and AGL observed on extractive Question Answering when computed over a set of models fine-tuned from different base foundation models. ACL and AGL are seen to hold for all four shifts of SQuAD-Shifts under this setup. The base models used here are OPT, GPT, and LLama_ . \n2. Figure 4 has been moved to Figure 6 in the updated paper with the caption - _ID vs OOD trends of accuracy and agreement of LLMs fine-tuned for Question Answering from a single pretrained base model (GPT2-Medium). Each column presents trends for different sources of stochasticity employed to obtain a diverse ensemble of fine-tuned models. We see that random linear head initialization is the best method to obtain a set of models exhibiting AGL behavior._ \n\n&nbsp;\n\n## **Questions:**\n\n### **In Figure 4, which column corresponds to which method? Visually it seems like the second column is the best, and for no column does it seem like AGL always holds.**\nWe apologize for the confusion. We have added labels for Figure 4 (the new Figure 6), it is random head initialization, data shuffling, and data subsetting from left to right. AGL holds the best with random head initialization, and the respective ALine (a method which leverages this proximity of the lines for estimating accuracy) MAPE errors can be found in table 2, which reaffirms this.  \n\n### **In Section 4.1 the phrase \u201censemble of foundation models\u201d is a bit confusing. Does it mean an ensemble in the machine learning sense or just a \"group\" of models?**\nWe apologize again for the confusion, we use ensemble here simply to refer to a collection of models. This usage is consistent with [1] and [3]. \n\n### **In Section 4.2, how is temperature scaling relevant in this setting?**\nWe temperature scale all our baselines which are confidence based, and report the numbers with the best performance between with and without temperature scaling.\n\n\n### **What is $\\Phi$ in Section 8.3?**\nWe apologize for the misclarification. $\\Phi^{-1}$ refers to probit scaling which is applied to induce a better linear fit as in [1] and [2]. We have added the explanation in the Appendix (Section 9.2).\n\n### **Is there a specific reason to only do linear probing for the CLIP setting, and just 1 fine-tuning task of SQuAD for the LM setting?**\n Thanks for your question. We initially picked linear probing for CLIP since we wanted to consider the \u201clightest\u201d form of fine-tuning. However, we have now added full fine-tuning as well.\n1. See our full fine-tuning experiments for CLIP based Image classification on the CIFAR10 dataset in Appendix Section 9.3. \n2. For SQuAD in the LM setting, linear probing resulted in poorly performing models and was hence omitted. \n3. We have included Text classification in Appendix Section 9.6/9.7 where we perform both full-finetuning and linear probing. Overall, our finding of the need for random head initialization for AGL, holds across all modalities, datasets, and fine-tuning methods.\n\nWe present a summary of our experiments below:\n\n**Image Classification with CLIP**\n\n* **Full Fine-tuning:**\n    1. (**_Newly added_**)  CIFAR10 Image classification: ID dataset: CIFAR10; OOD datasets: CIFAR10-C\n* **Linear Probing:**\n    1. CIFAR10 Image classification: ID dataset: CIFAR10; OOD datasets: CIFAR10-C, CIFAR10.1\n    2. (**_Newly added_**) OfficeHome: We use the standard domain adaptation benchmark with 4 domains: \u201cArt\u201d, \u201cClipArt\u201d, \u201cProduct\u201d, \u201cReal\u201d. We test all 3x3 settings where each domain is considered ID one at a time, with the remaining 3 domains being OOD\n    3. ImageNet: ID dataset: ImageNet, OOD datasets: ImageNetV2, ImageNetC\n    4. Satellite imagery: fMoW-WILDS\n    5. Species classification from camera traps: iWildCam-WILDS\n    6. Pathology: ID dataset: Camelyon17-WILDS\n\n### **NLP tasks**\n\n* **Full Fine-tuning:**\n    1. Question answering: ID dataset: SQuAD; OOD datasets: (i) SQuAD-Shifts - Reddit (ii) SQuAD-Shifts - Amazon (iii) SQuAD-Shifts - NYT (iv) SQuAD-Shifts - New Wiki\n        - Single Base Model: GPT2-Medium, OPT-125M (**_Newly added_**), BERT (**_Newly added_**)\n        - Multiple Base Models: GPT2 variant, GPT-Neo, OPT variants and Llama2\n    2. (**_Newly added_**) Text classification: ID dataset: MNLI-matched, OOD datasets: (i) MNLI-mismatched (ii) SNLI (iii) WNLI (iv) HANS\n* **Linear Probing:**\n    1. (**_Newly added_**) Text classification: ID dataset: MNLI-matched, OOD datasets: SNLI"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203823817,
                "cdate": 1700203823817,
                "tmdate": 1700204271362,
                "mdate": 1700204271362,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wQz7DRijbw",
                "forum": "4Qz9BT4mpM",
                "replyto": "VE2UCVkSXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Continued (3)"
                    },
                    "comment": {
                        "value": "### **Additional changes**\n1. (Minor) We have removed hyperparameters from the four sources of diversity as the category is too generic for a controlled study. This is reflected in the paper as well. Thus, we have three sources of diversity: random head initialization, data shuffling, and data subsetting. We\u2019d also like to emphasize that this does not change our results \u2013 random head initialization induces AGL most effectively. \n2. We have changed the error metric of ALine in the main body from mean absolute error (MAE) to mean absolute percentage error (MAPE) as the MAPE captures the relative not just absolute deviation from the true value. However, we have also included the MAE by accuracy range in the Appendix (Section 9.12).\n\n### **[References]**\n[1] Christina Baek, Yiding Jiang, Aditi Raghunathan, and J Zico Kolter. \u201cAgreement-on-the-line: Predicting the performance of neural networks under distribution shift.\u201d 2022.\\\n[2] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. \u201cAccuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization.\u201d 2021.\\\n[3] Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. \u201cAssessing generalization of sgd via disagreement. International Conference on Learning Representations\u201d 2022.\\\n[4] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. \"Fine-tuning can distort pretrained features and underperform out-of-distribution.\" 2022.\\\n[5] John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. \u201cThe effect of natural distribution shift on question answering models.\u201d 2020."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204407505,
                "cdate": 1700204407505,
                "tmdate": 1700204420740,
                "mdate": 1700204420740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oHQLxGCSdu",
            "forum": "4Qz9BT4mpM",
            "replyto": "4Qz9BT4mpM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8448/Reviewer_BrfQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8448/Reviewer_BrfQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of predicting the performance of fine-tuned foundation models under distribution shifts. To do so, they extend ALine (Baek et al. 2022), a method that leverages accuracy-on-the-line and agreement-on-the-line phenomena. The main idea of this extension is to inject diversity in model performance of fine-tuned foundation models via random linear head initialization, or use multiple foundation models trained with different hyperparameters and on different datasets. The empirical findings demonstrate that this approach outperforms existing methods in terms of MAE (mean absolute error).\n\nUpdate: the rebuttal addresses my main concerns, so I am increasing my score."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Shows that multiple foundation models in vision and LLM-based tasks exhibit accuracy-in-the-line and agreement-on-the-line phenomena.\n- Analysis on sources of diversity and its effect on ACL and AGL is thorough. This analysis results in a simple fix (random linear heads) to estimate OOD performance of a single foundation model fine-tuned on a downstream task. \n- Experiments show that ALine-D and Aline-D (Baek et al., 2022) outperform OOD prediction methods based on model confidence in predicting OOD performance of fine-tuned foundation models."
                },
                "weaknesses": {
                    "value": "- Organization of the paper is quite confusing. It starts off with a single model regime, where ALine needs to be extended due to diversity issues. Then, it states (in text) that multiple foundation models may have too much diversity, but empirically this is not a problem and that ALine works directly. It may be better to start with S4 (show that multiple foundation models exhibit AGL) and then move to the single foundation model setup that requires modifications to AGL-based OOD error estimation.\n- The novelty of this work is limited. S5 applies ALine (Baek et al. 2022) to larger pre-trained (foundation) models. S4 extends ALine by training a single foundation models with multiple random linear head initializations.\n- A major limitation of {accuracy, agreement}-on-the-line phenomena is that it is primarily a dataset-level property. First, there exist multiple datasets wherein the  ID-OOD ACL trend is not well explained by a linear function (see https://arxiv.org/abs/2209.00613 and https://arxiv.org/abs/2305.02995). Second, it is unclear when the ACL and AGL trend (i.e., same slope and intercept) hold. As a result, one cannot reliably use this method to predict OOD error in practice.\n- Comparison to the ProjNorm method (https://arxiv.org/abs/2202.05834), which performs better than the baselines considered in this paper, is missing."
                },
                "questions": {
                    "value": "It is unclear to me why injecting diversity via random linear heads works but not via other methods, e.g., data subsetting. Any intuition for what separates random linear heads from other interventions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8448/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8448/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8448/Reviewer_BrfQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698962737333,
            "cdate": 1698962737333,
            "tmdate": 1700854554998,
            "mdate": 1700854554998,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9dLNQzTPCd",
                "forum": "4Qz9BT4mpM",
                "replyto": "oHQLxGCSdu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your time and constructive feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer\n\nThank you very much for your time and helpful feedback! We have now made the following changes:\n1. We have clarified the role of different sections in the response below.\n2. Added a new baseline method, i.e. ProjNorm [5] (Appendix, Section 9.10), for OOD performance estimation for the Question-Answering model ensembles. \n3. We have provided some preliminary intuition as to the specific advantage of random linear head initialization for obtaining ensembles which exhibit AGL in the response below. We could add it to the paper if it\u2019s helpful. . \n\n### **Paper Organization (Putting Section 3 with more contributions after Section 4)**\n\nThank you for your feedback! We apologize for any confusion about the ordering of our contributions. \n1. We\u2019d like to clarify our reasoning for placing Section 3 before Section 4. Our primary focus was studying Agreement-on-the-line (AGL) in the setting wherein we fine-tune models from a single base foundation model. There may be many situations where we do not have access to multiple foundation models. \n2. Furthermore, since several foundation models are \u201cblack-box\u201d in different ways (we don\u2019t always know what they are trained on, for how long, what architecture etc), there\u2019s always the risk of models being too different to have their fine-tuned counterparts lie on the same ID-OOD accuracy line. Thus Section 4 was just to show an interesting independent and complementary observation that in some scenarios, where we do have access to multiple foundation models, AGL seems to hold when using this ensemble of existing models. \n3. It is true that Section 3 requires us to be smart when creating the ensemble for evaluating AGL, while Section 4 allows us to use existing models directly. But we think Section 3 is the more general setting that\u2019s of primary interest. We can make this more clear in our discussion of different sections and layout of the paper. Does this address your concern or do you still think we should reorder our sections?\n\n\n### **Limited novelty**\n1. We agree that in terms of **methodological advancements** our novelty might seem limited. However, we strongly believe that our paper makes **evaluative/conceptual** advancements to understand the phenomenon of agreement-on-the-line (AGL) in the setting of fine-tuning foundation models, where we do not have access to independently trained models from scratch.\n2. As the reviewer notes, AGL is not a universal phenomenon, and we need to identify the right ensemble of models to use. Prior work takes a set of independently trained models from scratch, trained on the same dataset. This is not feasible in the setting of foundation models\u2014we simply cannot train multiple foundation models on the same dataset. In fact, apriori we did not expect any form of AGL to hold with fine-tuned models from the same foundation model, because we expected their error to be highly correlated. \n3. We perform a systematic evaluation of different ways to create ensembles of fine-tuned models, and find that only one method worked reliably across the board: having different initializations of the random head. We believe this is a finding of great practical relevance because it allows us to perform unsupervised accuracy estimation when using foundation models. Our method achieves state-of-the-art. \n4. We also believe this work advances our understanding of the broader phenomenon of AGL. For example, [4] found that linear models when trained from scratch do not show AGL. However, we see that linear models on top of CLIP features show AGL, suggesting that it\u2019s less a property of the model family and more a property of the data distribution.  Prior work in ACL [3] suggested that different pre-training data for vision models would lead to different effective robustness i.e. the trend between ID and OOD performance. We find the contrary in Section 4, that different pretrained language models can observe the same robustness. \n\nWe hope that this discussion also clarifies the novelty and scientific value of our work. To summarize, we provide a rigorous study of limitations of applying AGL for foundation models. Methodologically, this also lends us SoTA estimation of the performance of foundation models. We have added a contributions section in the introduction to clarify the novelty and can further improve upon our writing for the camera-ready version. Thank you again for your time, please let us know if there is anything else you would suggest to help clarify our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202675292,
                "cdate": 1700202675292,
                "tmdate": 1700202675292,
                "mdate": 1700202675292,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OsHv87Kb5T",
                "forum": "4Qz9BT4mpM",
                "replyto": "oHQLxGCSdu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any further questions?"
                    },
                    "comment": {
                        "value": "Thank you again for the comprehensive and useful review! As you've suggested, we had incorporated more baselines (i.e. ProjNorm) and clarified our contributions (_our work, in addition to providing state-of-the-art performance estimation of large foundation models, debunks several hypotheses about agreement-on-the-line and accuracy-on-the-line from previous studies_). We've also provided some intuition for why random linear heads may be important to observe agreement-on-the-line, although we leave a more rigorous theoretical analysis for future work. We hope this addresses your concerns. \n\nDo you have any further questions? If you have any more suggestions or feedback, please let us know."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593926071,
                "cdate": 1700593926071,
                "tmdate": 1700594497563,
                "mdate": 1700594497563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ePvzpLQzhm",
            "forum": "4Qz9BT4mpM",
            "replyto": "4Qz9BT4mpM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8448/Reviewer_vvH6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8448/Reviewer_vvH6"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores various methodologies such as commonsense reasoning, medical image classification adaptation, and robust fine-tuning of zero-shot models. The paper contributes to the field by presenting a real-world dataset for medical image classification, discussing the challenges of model adaptation to out-of-distribution data, and evaluating the performance of fine-tuned models on new, unseen datasets. It also examines the correlation between in-distribution and out-of-distribution generalization, offering insights into the predictability of model performance across different domains. This work stands to impact the understanding of model robustness and the practical application of transfer learning in diverse machine learning tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper's approach to applying 'agreement-on-the-line' to foundation models is an original contribution that extends the utility of these models in novel ways. This method, as the paper suggests, could offer a fresh perspective on assessing and improving the performance of foundation models on out-of-distribution data. The authors' choice to explore this within the context of medical image classification and other domains also demonstrates an innovative application of machine learning techniques to real-world problems.\n\nThe authors' development and use of a real-world dataset for medical image classification suggest a commitment to grounding their findings in practical, applicable scenarios. The paper's methodological rigor is evident in the detailed descriptions of the experiments and the analytical techniques employed to assess model performance.\n\n\nDespite the complex nature of the subject matter, the paper maintains a level of clarity that is commendable. The authors have managed to explain their methodologies and findings in a way that is understandable, which is particularly important when addressing such advanced topics in AI. The clarity with which the paper discusses the implications of its findings for the field of machine learning is a notable strength.\n\nThe significance of the paper's contributions cannot be overstated. By addressing the challenge of model generalization and robustness, the paper tackles one of the most pressing issues in machine learning today. The potential impact of this research is broad, as it could influence a wide range of applications, from healthcare to autonomous systems, where the ability to perform well on out-of-distribution data is crucial.\n\nIn summary, the paper stands out for its original approach to a key problem in machine learning, its rigorous and quality research methodology, the clarity of its exposition, and the potential significance of its contributions to the field."
                },
                "weaknesses": {
                    "value": "The paper's exploration of foundation models and their application to various domains is commendable; however, there are areas where the work could be strengthened:\n\nSpecificity of Contributions:\nThe paper's contributions could be articulated more clearly. While the authors propose the application of the 'agreement-on-the-line' method to foundation models, they do not sufficiently differentiate this approach from existing methods. For instance, the paper states, \"We consider a variety of foundation models: GPT2, GPT-Neo, OPT, Llama2, and CLIP,\" but does not elaborate on how 'agreement-on-the-line' enhances or differs from the current state-of-the-art. To improve, the authors should explicitly state the unique advantages and contributions of their method over existing approaches, possibly by providing a direct comparison to highlight the novelty.\n\nComparative Analysis:\nThe experimental section lacks a comprehensive comparative analysis. The authors present experiments validating their method, yet there is no benchmarking against existing Out-Of-Distribution (OOD) performance estimation methods. The paper could be significantly improved by including comparisons with established baselines, as this would demonstrate the efficacy of the 'agreement-on-the-line' method over others. For example, when discussing the fine-tuning procedures, the authors could compare the OOD performance estimation with other known approaches to establish the superiority of their method.\n\nData Diversity and Volume:\nThe volume and variety of datasets used for validation appear limited. The paper mentions, \"Fine-tuning... we have access to labeled data from some distribution DID,\" but does not provide extensive validation across a broad range of datasets. Expanding experiments to include a wider array of datasets, especially those with larger scales and varying types, would lend more credibility and generalizability to the findings.\n\nWriting Quality:\nThe clarity and organization of the paper could be improved. The logical flow and language precision are areas where the paper seems to fall short. For instance, the use of terms like \"foundation models\" and \"agreement-on-the-line\" could be more clearly defined to avoid ambiguity. The authors are encouraged to refine the language and structure of the paper to enhance readability and ensure that the arguments are presented coherently.\n\nReference Breadth:\nThe paper seems to have a narrow scope of references, primarily citing a few articles by the authors themselves. To establish the research within the broader context of the field, it would be beneficial to cite a wider range of high-quality, related studies. This would not only position the paper within the existing body of knowledge but also provide a more robust background for readers.\n\nIn summary, while the paper presents interesting ideas, it would benefit from clearer articulation of its unique contributions, more extensive comparative analysis, broader and more diverse data validation, improved writing quality, and a more comprehensive set of references."
                },
                "questions": {
                    "value": "Methodological Clarification:  \nCould you elaborate on the theoretical underpinnings of the 'agreement-on-the-line' method? How does it theoretically and practically differ from existing methods for assessing model performance on out-of-distribution data?\n\nExperimental Comparisons:  \nThe paper would benefit from a direct comparison of your method with existing benchmarks. Could you include such a comparison to highlight the advantages of your approach?\n\nDataset Diversity and Volume:  \nYour experiments seem to be limited to a few datasets. Could you provide insights into how your method performs across a more diverse range of datasets, including those with larger scales?\n\nRobustness of Findings:  \nHow robust are your findings to changes in the model architecture or dataset characteristics? Are there any limitations to the applicability of the 'agreement-on-the-line' method?\n\nImpact of Fine-Tuning Procedures:  \nCan you discuss the impact of different fine-tuning procedures on the performance of foundation models using your method? How does the 'agreement-on-the-line' adapt to these variations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "How were the datasets, especially those involving medical images, obtained? Were appropriate consents from the subjects obtained, and were the datasets de-identified to protect privacy?"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8448/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8448/Reviewer_vvH6",
                        "ICLR.cc/2024/Conference/Submission8448/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699462328598,
            "cdate": 1699462328598,
            "tmdate": 1700633070678,
            "mdate": 1700633070678,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RYhDkBw0xG",
                "forum": "4Qz9BT4mpM",
                "replyto": "ePvzpLQzhm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your time and constructive feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you very much for your time and thorough feedback! We have now made the following changes which we believe address all your concerns and strengthen the paper\n- Added a paragraph in the introduction clearly describing our contributions and relation to prior work and state of the art\n- Clearly mention where we compare to baselines ([Table 3](https://i.imgur.com/rlCRcjC.png)), and added one more baseline method of ProjNorm [4]\n- Added results on OfficeHome dataset (with 4ID and 3 OOD settings for each) for image classification with CLIP, and MNLI to MNLI-Mismatched, SNLI, HANS, WNLI for text classification (See Appendix Sections [9.6](https://i.imgur.com/7RaLfqO.png), [9.7](https://i.imgur.com/HcF41ED.png)) . This now makes a total of 34 different distribution shifts for image classification and 8 different shifts in NLP tasks. \n- Considered two new architectures (OPT, BERT) in addition to GPT-2  and Llama for NLP tasks  (see [Table 8 and 9](https://i.imgur.com/qMePdYh.png))\n- Studied different kinds of fine-tuning: newly added full fine-tuning of CLIP ([Figure 5](https://i.imgur.com/TSy1Gdn.png)) in addition to linear probing\n\nWe elaborate on each of your concerns below:\n\n### **Specificity of Contributions: \u2026'agreement-on-the-line' enhances or differs from the current state-of-the-art. To improve, the authors should explicitly state the unique advantages and contributions of their method over existing approaches, possibly by providing a direct comparison to highlight the novelty.**\n\nThank you, to clarify our contributions, we modified the Introduction and added a \u201ccontribution\u201d paragraph at the end. To summarize: \n- We propose a new state-of-the-art method for unsupervised accuracy estimation under distribution shifts when using large pre-trained models (a.k.a. foundation models) that are lightly fine-tuned for specific tasks. Prior works have primarily dealt with the models trained from scratch, where there is no transfer learning or extensive pre-training to account for. Hence, we believe the setting studied is new, and extremely relevant in today\u2019s context. Prior work does not directly apply in this setting. \n- Furthermore, our work leveraging Agreement-on-the-line (AGL) for OOD estimation, builds on top of prior work [11]; but extends it in important ways to make it work in this new and important setting. Prior works have primarily dealt with the models trained from scratch, where there is no transfer learning or extensive pre-training to account for. Hence, we believe the setting studied is new, and extremely relevant in today\u2019s context. Admittedly, it is similar in overall methodology\u2014we compute the agreement between pairs in an ensemble of models. However, the key to making AGL work is obtaining the _right ensemble_. In [11], this ensemble was just a collection of models trained _independently from scratch_. A likewise (but infeasible) extension to pre-trained models would require numerous such models, all trained from scratch. Our work shows how to side-step this, by systematically identifying a computationally tractable method of obtaining the right ensemble. Specifically, we show that creating an ensemble with randomly initialized final linear heads and then fine-tuning can allow for AGL behavior, and apply downstream methods for unsupervised accuracy estimation, while other similar forms of ensembling (such as data ordering or data subsetting) do not.\n- We believe this is a significant finding of practical relevance, but also points to several interesting phenomena underlying AGL that go beyond previous knowledge. Prior work (section 3.4 in [11]) claimed that AGL does not hold for linear models. However, we find the contrary when using pre-trained features (such as CLIP). Furthermore, other prior work [14] suggests that the effective robustness (i.e. the linear fit b/w ID and OOD accuracy) would change depending on the pretraining data. We find that this is not the case for question answering with different pretrained LLMs. Thus we hope our findings can also advance our understanding of the robustness of ML models, particularly those that leverage foundation models."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201395871,
                "cdate": 1700201395871,
                "tmdate": 1700201395871,
                "mdate": 1700201395871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RSxN4ELBLM",
                "forum": "4Qz9BT4mpM",
                "replyto": "ePvzpLQzhm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Continued (2)"
                    },
                    "comment": {
                        "value": "### **Comparative Analysis: \u2026no benchmarking against existing Out-Of-Distribution (OOD) performance estimation methods\u2026**\n\nWe apologize for any confusion about the lack of baselines. Our submission does compare to other baselines (Naive Agreement, ATC, AC, DF) in Table 3. We apologize for the table being in Section 4.2, which would have been confusing. The table reports numbers we have calculated over both models with randomly initialized heads (Section 3) and models from different bases (Section 4). \n\nIn summary, ATC [1], AC [2], and DF [3] utilize model confidence to estimate OOD accuracy while Naive Agreement [9, 10]  directly utilizes agreement to predict accuracy. We have also added a new baseline of ProjNorm [4] in Section 9.10 which yields a score correlated with the OOD performance of the model. We see that our proposed method significantly outperforms baselines. The table below is a snapshot of some of our baseline comparisons; a full picture can be found in [Table 3](https://i.imgur.com/rlCRcjC.png). Based on your feedback, we have moved our baseline comparison section to its own standalone Section 5, and provided more details about the evaluated datasets, models, and baselines. Please let us know if there are any other baselines we should add.\n\n|    Shift     |  ALine   | Naive Agreement |  ATC   |   AC   |   DF   |\n|--------------|----------|-----------------|--------|--------|--------|\n| SQuAD-Shifts | **1.68%**    | 19.48%          | 9.16%  | 45.04% | 4.54%  |\n| CIFAR10-C    | **6.92%**    | 44.33%          | 18.69% | 48.66% | 32.79% |\n| ImageNet-C   |  **10.91%** | 56.76%          | 27.24% | 79.00% | 37.86% |\n&nbsp;\n\n### **Data Diversity \u2026 The volume and variety of datasets used for validation appear limited**\n\nWe list below the different datasets we evaluate on, including some that we added based upon your feedback (let us know if you feel this helps to address your concerns, and if there are particular new datasets that would be valuable to add in your opinion).\n\n**Image Classification with CLIP**\n1. CIFAR10 Image Classification: ID dataset: CIFAR10; OOD datasets: CIFAR10-C (which includes 19 different shifts; we test on each individually), CIFAR10.1\n2. (**_Newly added_**) OfficeHome: We use the standard domain adaptation benchmark with 4 domains: \u201cArt\u201d, \u201cClipArt\u201d, \u201cProduct\u201d, \u201cReal\u201d. We test all 4x3 settings where each domain is considered ID one at a time, with the remaining 3 domains being OOD\n3. ImageNet: ID dataset: ImageNet, OOD datasets: ImageNetV2, ImageNetC\n4. Satellite imagery: ID/OOD dataset: subsets of fMoW (WILDS) \n5. Species classification from camera traps: ID/OOD dataset: iWildCam (WILDS)\n6. Pathology: ID/OOD dataset: Camelyon-17 (WILDS) \n\n**NLP tasks with GPT, Bert, OPT, Llama**\n1. Question answering: ID dataset is SQuAD; OOD datasets are (i) SQuAD-Shifts - Reddit (ii) SQuAD-Shifts - Amazon (iii) SQuAD-Shifts - NYT (iv) SQuAD-Shifts - New Wiki\n2. (**_Newly added_**) Text classification: ID dataset is MNLI-matched, OOD datasets are (i) MNLI-mismatched (ii) SNLI (iii) WNLI (iv) HANS\n\nOur findings on when AGL occurs holds quite robustly across all these settings; i.e. random head initialization while fine-tuning is the only method consistently yielding ensembles where AGL holds, thus allowing for accurate OOD estimation with ALine methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201863728,
                "cdate": 1700201863728,
                "tmdate": 1700342293192,
                "mdate": 1700342293192,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1JJ2A1xJ31",
                "forum": "4Qz9BT4mpM",
                "replyto": "ePvzpLQzhm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any further questions?"
                    },
                    "comment": {
                        "value": "Thank you again for the comprehensive and useful review! As you've suggested, we had provided experiments on additional benchmarks (e.g. OfficeHome, MNLI), architectures (e.g. BERT, OPT, Llama), and reworded our contributions. We hope this addresses your concerns. \n\nDo you have any further questions? If you have any more suggestions or feedback, please let us know."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593487242,
                "cdate": 1700593487242,
                "tmdate": 1700593580898,
                "mdate": 1700593580898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]