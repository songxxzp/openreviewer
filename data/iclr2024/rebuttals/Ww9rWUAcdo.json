[
    {
        "title": "Theoretical Understanding of Learning from Adversarial Perturbations"
    },
    {
        "review": {
            "id": "5rxlJAoS7H",
            "forum": "Ww9rWUAcdo",
            "replyto": "Ww9rWUAcdo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission621/Reviewer_75Lo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission621/Reviewer_75Lo"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors focus on the embedded information in adversarial perturbations. Motivated by the prior work (Ilyas et al. 2019), they theoretically demonstrate that the adversarial perturbation can be represented as a weighted combination of the natural datapoints. And the model trained on these adversarial perturbations (with incorrect labels) can efficiently learn the data features and generate well on the natural data. Specifically, they provide some theorems showing the boundary of the model learning from the purturbations is partially determined by the model learning from the natural data. Some experiments are conducted to validate their theory."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The writing is generally clear.\n* Although there are some assumptions about the dataset and they only focus on one-hidden-layer neural network, the authors provide some insights into this research field.\n* The problem of why network learning from perturbation can generate well is worth exploring. This can shed some light on other interesting phenomena, such as the transferability of adversarial examples."
                },
                "weaknesses": {
                    "value": "My main concerns about this paper are:\n* In Section 4.1, the authors generate the adversarial examples according to the linear model. I understand the decision boundaries of the linear model $f^{bdy}(z)$ and the one-hidden-layer neural network are the same (Theorem 3.3). However, I don't think we can directly replace the neural work with the linear model to generate adversarial examples since adversarial examples are generated by optimizing the loss, which is different. Can the authors provide more theoretical explanations for this?\n* Although the authors claim that they focus on one-hidden-layer neural networks, they make some approximations and actually focus on linear models. So the contribution of this paper is reduced.\n\nSome minor concerns:\n* In the third paragraph of the introduction, the authors say, \"Nevertheless, neural networks can learn to accurately classify benign samples from such adversarially perturbed samples with seemingly incorrect labels\". Can you provide more explanation for this? Does it mean we can train a model that can accurately distinguish the adversarial examples from the clean sample?\n* In this work, the authors show that the model trained on adversarial perturbations with incorrect labels (i.e., $y^{adv}$) can generate well on the clean data. But as we know, traditional adversarial training is based on adversarial examples with correct labels (i.e., $y^{natural}$). So, why do these two methods both generate well on the clean data? \n* I would like to know the adversarial accuracy of the method in your experiments.\n* In Eq. (2), does $x_n$ refer to the training data $x_n$? It seems that $x_n$ here is new test data that is different from the training data $x_n$. Otherwise, the gradient calculation in Eq. (2) should be reconsidered. Please check this out.\n* The assumption in Theorem 4.2 is somewhat strong: $|\\sum\\lambda_n y_n <x_n,z>|=\\Theta(g(N,d))$ if $\\sum\\lambda_n  |<x_n,z>|=\\Theta(g(N,d))$. It is intuitive that the first term is much smaller than the second term. What would your theorem be without this assumption?\n* In Section 5.1, I have some confusion about some statements. For example, in Fig 1, the authors mentioned standard data and noisy data. But in their setting, there are two kinds of noisy data: the dataset is generated randomly and can be viewed as some noises; the adversarial perturbations are superposed on random noises. I am confused as to what \"standard data\" and \"noise\" refer to. I strongly recommend the authors make more clear clarifications in the paper.\n* As the authors claim in the experiments, there are some counterexamples. I think more discussions about these counterexamples are needed. Is it because the effect of learning from mislabeled natural data dominates the effect of learning from perturbation?\n* Since there are negative effects of learning from natural data or noises, why doesn't the learner solely learn from the perturbations?"
                },
                "questions": {
                    "value": "Please refer to the Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698222922152,
            "cdate": 1698222922152,
            "tmdate": 1699635989897,
            "mdate": 1699635989897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q606V2VKnj",
                "forum": "Ww9rWUAcdo",
                "replyto": "5rxlJAoS7H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 75Lo [1/1]"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s thorough review of our work.\n\n**First, we would like to clarify that our research is NOT related to adversarial training. Please kindly refer to the general comment.**\n\n### Main concerns\n> the authors generate the adversarial examples according to the linear model. ... I don't think we can directly replace the neural work with the linear model ... since adversarial examples are generated by optimizing the loss\n> \n> Although the authors claim that they focus on one-hidden-layer neural networks, they make some approximations and actually focus on linear models. So the contribution is reduced.\n\nIn Appendix D.1, we discuss the perturbations generated by optimizing the exponential loss on the one-hidden-layer neural network $f$ itself rather than the linear model $f^\\mathrm{bdy}$. The results are consistent with those in the main text, i.e., those for the perturbations targeting the linear model. Please refer to it.\n\n### Minor concerns\n> the authors say, \"Nevertheless, neural networks can learn to accurately classify benign samples from such adversarially perturbed samples with seemingly incorrect labels\". Can you provide more explanation for this? Does it mean we can train a model that can accurately distinguish the adversarial examples from the clean sample?\n> \n> the authors show that the model trained on adversarial perturbations with incorrect labels can generate well on the clean data. But as we know, traditional adversarial training is based on adversarial examples with correct labels. So, why do these two methods both generate well on the clean data?\n> \n> I would like to know the adversarial accuracy\n\n**Our research focus is NOT adversarial training. Please kindly refer to the general comment.**\n\n> In Eq. (2), does $x_n$ refer to the training data $x_n$? It seems that $x_n$ here is new test data that is different from the training data.\n\nYes, in Eq. (2), $x_n$ denotes the training data. We would appreciate more details on your interpretation of $x_n$ as new test data.\n\n> The assumption in Theorem 4.2 is somewhat strong. It is intuitive that the first term is much smaller than the second term. What would your theorem be without this assumption?\n\nWe acknowledge that this assumption introduced in the natural data scenario (cf. Definition 3.2) is not always reasonable. However, to determine the growth rate of $T_2$, i.e., the second term in Eq. (4), which includes the indeterminable $y_n$, certain assumptions are necessary. While this assumption can be relaxed, for example, to $|\\sum\\lambda_ny_n\\langle x_n,z\\rangle|=\\Theta(1)$, it inevitably weakens the conclusion of Theorem 4.2.\n\nTo circumvent these issues, we employed the noise data scenario. This approach allows us to validate the success of learning from perturbations without relying on such unreasonable assumptions (cf. Theorem 4.4). This feasibility stems from the fact that in the noise data scenario the base data (i.e., uniform noises) have well-defined probabilistic characteristics. This differs from the natural data scenario, where the probabilistic nature of the base data (i.e., natural data) and the corresponding labels cannot be defined.\n\n> In Section 5.1, I have some confusion about some statements. For example, in Fig 1, the authors mentioned standard data and noisy data. But in their setting, there are two kinds of noisy data: the dataset is generated randomly and can be viewed as some noises; the adversarial perturbations are superposed on random noises. I am confused as to what \"standard data\" and \"noise\" refer to. I strongly recommend the authors make more clear clarifications in the paper.\n\nThe reviwer is correct; \"standard data\" refers to images that might appear as noise to the human eye in this section. We will enhance the clarity of our experimental setup and the nature of the images in the revised version.\n\n> As the authors claim in the experiments, there are some counterexamples. I think more discussions about these counterexamples are needed. Is it because the effect of learning from mislabeled natural data dominates the effect of learning from perturbation?\n\nYes, following Theorems 4.1 and 4.3, the dominance of learning effects from mislabeled natural data over learning effects from perturbations is a plausible explanation. However, the theoretical evaluation of this effect is challenging in datasets such as MNIST, Fashion-MNIST, and CIFAR-10, where the data distribution cannot be quantitatively assessed.\n\n> Since there are negative effects of learning from natural data or noises, why doesn't the learner solely learn from the perturbations?\n\nThis is because classifiers cannot discern what is \"negative\" within a given dataset and are trained solely to minimize the loss on the dataset. Learning from mislabeled natural data or noises is counterproductive from our perspective to achieve high accuracy on a test dataset, but it serves as a useful feature to strongly fit a mislabeled training dataset."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893925635,
                "cdate": 1699893925635,
                "tmdate": 1699893925635,
                "mdate": 1699893925635,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wwxAQVVXcq",
            "forum": "Ww9rWUAcdo",
            "replyto": "Ww9rWUAcdo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission621/Reviewer_doyy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission621/Reviewer_doyy"
            ],
            "content": {
                "summary": {
                    "value": "This paper consider orthogonal training data, and prove that one hidden layer neural network trained with mislabeled data should have the sae dicision boundary with training on the natural data. Moreover, the adversarial perturbation can be treated as the weighted sum of benign training data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper provides a theoretical justification regarding an empirical phenomenon that first observed in Ilyas et a. 2019. To the best of my knowledge there\u2019s no theoretical work has been focused on this direction before."
                },
                "weaknesses": {
                    "value": "I\u2019m not exactly following the motivation of this work. Normally when people consider adversarial training to gain robustness, at each iteration the adversarial training examples is generated based on the current model weight, yet in this paper, from the definition 3.2, it seems that the adversarial examples of training samples is generated beforehand and independent of the current model weight, and it\u2019s fixed during the latter training procedure. Therefore it\u2019s unclear to me whether the perturbation considered in this paper is actually adversarial or not. The author claims the motivation comes from Ilyas et a. 2019, whereas in that paper there\u2019s also a math formulation of generating the designed adversarial perturbation, which seems different from the procedure described in definition 3.2. \n\nIt seems this paper\u2019s theoretical results largely build on the result from Frei et al 2023. Yet the discussion of the existing result and the corollary is limited. It\u2019s unclear to me why corollary 3.4 can be directly derived from Theorem 3.3. Theorem 3.3 requires clean training data to be nearly orthogonal, then for corollary 3.4 you should also prove that the adversarial example is almost orthogonal. Moreover, the adversarial perturbation considered depends on the decision boundary, but it\u2019s unclear to me whether the coefficient $\\lambda$ in the decision boundary is computable. The network only considers Leaky ReLU, yet it\u2019s unclear why cannot generalize to other activation functions and what\u2019s the technical difficulty.\n\nThe statement of each theorem is not self-contented. If $R_{min}, R_{max}, p_{max}$ is used all over the place, then they should be introduced in the notation part. The explanation of each theorem and corollary is not clear. Why we require equation (3)? Any intuitive explanation or simply one requirement for the proof to go through? When N goes to infinity, equation (3) is saying $p_{max}$ is negative, which is kind of counterintuitive. Similarly, what does equation (6) hold mean intuitively?"
                },
                "questions": {
                    "value": "I believe the writing of the paper can be largely improved. Most of the questions I have here and in the previous section are mainly because of the unclear writing or cannot find any pointers.\n\n1. Can the author provide any intuition as to why mislabeled data provide the same classifier direction as clean data? Is it mainly because of the orthogonality of training data so that even if the label is noisy the data is still orthogonal?\n\n2. For the discussion of theorem 4.1 ``Consistent Growth of Two Effects\u201d, it\u2019s unclear to me whether estimating the growth rate of T1, T2 by switching the absolute operator and the summation is reasonable. The magnitude could have a huge difference with proper label $y_n$, so why assume the rate of switching the operator is reasonable?\n\n3. It\u2019s unclear from a theoretical perspective in Appendix G why the standard decision boundary only depends on non-robust features as shown in equation A157.\n\n4. For Figure 2, and other figures, does the accuracy clean accuracy or adversarial robust accuracy? What\u2019s the procedure to generate adversarial perturbation? Does it relate to the decision boundary? What\u2019s the point of considering random selection vs. deterministic selection? Does the perturbation size actually align with what indicates by the theorem such as $O(d/N)$ or $O(\\sqrt{d/N})$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607163117,
            "cdate": 1698607163117,
            "tmdate": 1699635989825,
            "mdate": 1699635989825,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M0OVvYGwmT",
                "forum": "Ww9rWUAcdo",
                "replyto": "wwxAQVVXcq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer doyy [1/2]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer\u2019s insightful comments.\n\n**First, we would like to clarify that our research is NOT related to adversarial training. Please kindly refer to the general comment.**\n\n### Misinterpretation\n> when people consider adversarial training ...\n\n**Our research focus is NOT adversarial training.**\n\n> the motivation comes from Ilyas et a. 2019, ..., which seems different from definition 3.2\n\nThe reviewer may refer to the \"Robust training\" in section 2 of (Ilyas et al.), which is not the focus of our study. Learning from perturbations is described in Section 3.2 of (Ilyas et al.).\n\n> why mislabeled data provide the same classifier direction as clean data? because the orthogonality of training data so that even if the label is noisy the data is still orthogonal?\n\nIt seems this question arises from a misunderstanding of our experimental procedure as adversarial training. For the explanation of the success of learning from mislabeled data, please refer to the general comment.\n\nThis is because classifiers learn from mislabeled adversarial examples, not just mislabeled data. Following the hypothesis of Ilyas et al., adversarial perturbations contain invisible data features to humans. While learning on mislabeled adversarial examples seems nonsensical to us, classifiers rely on these invisible but generalizable features and can construct the same decision boundaries as those of standard classifiers.\n\n### Theoretical framework (Theorem 3.3 and Corollary 3.4)\n> the discussion of the existing result and the corollary is limited\n\nThe discussion has been summarized in Appendix A.\n\n> why corollary 3.4 can be directly derived from Theorem 3.3\n\nCorollary 3.4 is the special case of Theorem 3.3 for $\\\\{(x^\\mathrm{adv}\\_n,y^\\mathrm{adv}_n)\\\\}^{N^\\mathrm{adv}}\\_{n=1}$. In other words, we replace $\\\\{(x\\_n,y\\_n)\\\\}^N\\_{n=1}$ with $\\\\{(x^\\mathrm{adv}\\_n,y^\\mathrm{adv}\\_n)\\\\}^{N^\\mathrm{adv}}\\_{n=1}$. This change naturally leads to a modification in definitions, such as $R\\_\\mathrm{max}:=\\max\\_n\\\\|x\\_n\\\\|$ becoming $R^\\mathrm{adv}\\_\\mathrm{max}:=\\max\\_n\\\\|x^\\mathrm{adv}\\_n\\\\|$. Essentially, Corollary 3.4 is Theorem 3.3 with \"adv\" added to the variables.\n\n> for corollary 3.4 you should prove that the adversarial example is almost orthogonal.\n> \n> what do Eq. 3 and Eq. 6 mean?\n\nThe orthogonality condition of adversarial examples is $\\gamma^3{R^\\mathrm{adv}\\_\\mathrm{min}}^4/(3N^\\mathrm{adv}{R^\\mathrm{adv}\\_\\mathrm{max}}^2)\\geq p^\\mathrm{adv}\\_\\mathrm{max}$ in corollary 3.4. This condition depends on the definition of perturbations and the base images (natural images or uniform noises). For geometry-inspired perturbations (cf. Eq. (2)) on natural images, this condition is transformed into Eq. (3), and for those on uniform noises, Eq. (6).\n\n> it\u2019s unclear whether the coefficient $\\lambda$ in the decision boundary is computable\n\n(Theoretical perspective) $\\lambda$ is uniquely determined but is not computable (cf. Appendix A). However, the lower and upper bounds are computable, which is enough to derive our theorems.\n\n(Practical perspective) The perturbation Eq. (2) is not computable in practice, as $\\lambda$ and the boundary are not computable. Note that this perturbation form was chosen only because it represents the simple results. Moreover, in Appendix D.1, we derived consistent results with those in the main text for the perturbations using the network $f$ directly, which are based on the computable network output and thus practical to implement.\n\n> why cannot generalize to other activations\n\nThis is because Theorem 3.3 is applicable only to Leaky ReLU (cf. the proof of Theorem 3.2 in (Frei et al., 2023))."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893691033,
                "cdate": 1699893691033,
                "tmdate": 1699893691033,
                "mdate": 1699893691033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3vQLoipEMt",
                "forum": "Ww9rWUAcdo",
                "replyto": "wwxAQVVXcq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer doyy [2/2]"
                    },
                    "comment": {
                        "value": "### Main results\n> When $N$ goes to infinity, Eq. (3) is saying $p_\\mathrm{max}$ is negative?\n\nThis is true if $N$ increases infinitely while keeping other variables constant. However, Eq. (3) also indicates $O(d/N)\\geq p_\\mathrm{max}$. If the input dimension $d$ is sufficiently larger than $N$, $p_\\mathrm{max}$ will not be negative.\n \n> estimating the growth rate of T1, T2 by switching the absolute operator and the summation is reasonable?\n\nWe acknowledge that this assumption introduced in the natural data scenario (cf. Definition 3.2(a)) is not always reasonable. However, to determine the growth rate of $T_2$, i.e., the second term in Eq. (4), which includes the indeterminable $y_n$, certain assumptions are necessary. While this assumption can be relaxed, for example, to $|\\sum\\lambda_ny_n\\langle x_n,z\\rangle|=\\Theta(1)$, it inevitably weakens the conclusion of Theorem 4.2.\n\nTo circumvent these issues, we employed the noise data scenario (cf. Definition 3.2(b)). This approach allows us to validate the success of learning from perturbations without relying on such unreasonable assumptions (cf. Theorem 4.4). This feasibility stems from the fact that in the noise data scenario the base data (i.e., uniform noises) have well-defined probabilistic characteristics. This differs from the natural data scenario, where the probabilistic nature of the base data (i.e., natural data) and the corresponding labels cannot be defined.\n\n### Others\n> why the standard boundary only depends on non-robust features in Eq. A157\n\nUnder the assumption that standard classifiers focus exclusively on non-robust features, we consider that training classifiers on $\\\\{(x\\_n,y\\_n)\\\\}\\_n$ is equivalent to training on $\\\\{(x^\\mathrm{non}\\_n,y\\_n)\\\\}\\_n$. Thus, by applying Theorem 3.3 to $\\\\{(x^\\mathrm{non}\\_n,y\\_n)\\\\}\\_n$, we derived Eq. (A157).\n\n> For Figure 2, and other figures,\n> 1. clean accuracy or adversarial robust accuracy?\n> 2. What\u2019s the procedure to generate perturbation? \n> 3. Does it relate to the decision boundary? \n> 4. What\u2019s the point of random vs. deterministic selection? \n> 5. Does the perturbation size align with what indicates by the theorem?\n\n1. As we do not focus on adversarial training, robust accuracy is not considered in this study.\n2. Since we cannot practically obtain the decision boundary (cf. the comment above), we employed the PGD attack (Madry et al., 2018). The detailed experimental setup can be found in Appendix H.\n3. Yes. The accuracy in Figure 2 (left) is the same as that below the boundary in Figure 1 (middle).\n4. This followed the settings in (Ilyas et al., 2019). Random selection eliminates the correlation between visible features and labels, allowing for a purer observation of the effects of learning from perturbations (cf. the general comment). Experiments with deterministic selection, where visible features are anti-correlated with labels, strongly suggest that invisible perturbations contain data information. Please also refer to Section 3.2 in (Ilyas et al., 2019).\n5. Yes. In Figure 2 (right), we changed the number of noise samples with a fixed perturbation size to focus on the effect of the sample size variation. This fixed perturbation size was determined by the case with $N^\\mathrm{adv}=10,000$. Since the perturbation was controlled by $\\epsilon=O(\\sqrt{1/N^\\mathrm{adv}})$, this fixed size was suitable even for smaller $N^\\mathrm{adv}$. The left panel keeps $N^\\mathrm{adv}$ constant and varies input dimensions and perturbation sizes according to $O(\\sqrt{d})$ to avoid excessively strong L2 perturbations in lower dimensions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893844185,
                "cdate": 1699893844185,
                "tmdate": 1699894967120,
                "mdate": 1699894967120,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e1z4T9Noiz",
                "forum": "Ww9rWUAcdo",
                "replyto": "M0OVvYGwmT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission621/Reviewer_doyy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission621/Reviewer_doyy"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Author,\n\nThank you for your response. I still have some remaining concerns.\n\n\nLet me rephrase my question regarding the relationship between theorem 3.3 and corollary 3.4. After Theorem 3.3, the author writes \"This theorem only requires training data to be nearly-orthogona\u201d. My understanding this refers to $\\gamma^3 R_{min}^4/(3N R_{max}^2) \u2265 pmax$ in the statement of theorem 3.3, which is an **assumption**. In Corollary 3.4, the author directly replaces it with its adversarial version, meaning the author assumes, the training data, after adding the perturbation, is still nearly orthogonal. I regard this as a very strong assumption. I thought this was proven given the same assumption from the statement of theorem 3.3, but the author responded by saying they just replaced everything with the adversarial counterpart. ``For geometry-inspired perturbations (cf. Eq. (2)) on natural images, this condition is transformed into Eq. (3), and for those on uniform noises, Eq. (6).\u201d Eq(3) and Eq(6) are assumption, can they be proved given the same assumption as in the statement of theorem 3.3? Directly assuming it leads to the point that $p_{max}$ can be negative, which is the next part.\n\n\nIn terms of $p_{max}$ being negative, the author responds by ``If the input dimension\u00a0$d$\u00a0is sufficiently larger than\u00a0$N$,\u00a0$p_{max}$\u00a0will not be negative.\" This means the statement holds conditioned on high-dimensionality requirements such as d >> n. It covers a rather strict area as not many applications are satisfied. I\u2019m even fine with such an assumption (or called condition), but it has to be crystal clear and properly discussed, instead of buried in some equations."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538458884,
                "cdate": 1700538458884,
                "tmdate": 1700538458884,
                "mdate": 1700538458884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "obblAqY4jT",
                "forum": "Ww9rWUAcdo",
                "replyto": "wwxAQVVXcq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the reviewer's valuable feedback.\n\n> My understanding this refers to $\\gamma^3 R^4_\\mathrm{min} / (3 N R^2_\\mathrm{max}) \\geq p_\\mathrm{max}$ in the statement of theorem 3.3, which is an assumption. In Corollary 3.4, the author directly replaces it with its adversarial version, meaning the author assumes, the training data, after adding the perturbation, is still nearly orthogonal. I regard this as a very strong assumption.\n\nThe reviewer might be misunderstanding that Theorem 4.1 requires not only the orthogonality of the adversarial samples (i.e., Eq. (3)) but also that of the natural samples (i.e., $\\gamma^3 R^4_\\mathrm{min} / (3 N R^2_\\mathrm{max}) \\geq p_\\mathrm{max}$). We should note that it only assumes the former (i.e., the orthogonality of the adversarial samples). If the reviewer still considers the assumption to be strong, we would like the reviewer to futher clearify it.\n\n> Directly assuming it leads to the point that $p_\\mathrm{max}$ can be negative, which is the next part.  \n\nYes, if $N$ goes to infinity (with other variables being constant), $p_\\mathrm{max}$ becomes negative, and thus, we cannot use Eq. (4). The core implication of Eq. (3) (and Theorem 4.1) is that if $\\epsilon=O(\\sqrt{d/N})$ and $p_\\mathrm{max}=O(d/N)$ hold, learning from perturbations can be interpreted as Eq. (4). Namely, the decision boundary Eq. (4) consists of two terms, the effects of learning from mislabeled natural data and learning from perturbations, demonstrating the consistency between perturbation learning and standard learning (cf. Theorem 4.2). This is described in Sections 4.2 and 4.3.\n\nLastly, we have a few minor remarks.\n- The above conditions, $\\epsilon=O(\\sqrt{d/N})$ and $p_\\mathrm{max}=O(d/N)$, contain the case of $N\\to\\infty$.\n- Eq. (3) does not necessarily require $d \\gg n$ (this case was provided as an example).\n- \"Assume sufficiently large $N$\" in Theorem 4.1 might lead misunderstanding. The more accurate representation is described as \"If $N>C^2/R^2_\\mathrm{min}$ and Eq. (3) with C=..., ...\".\n- Please refer to Theorem B.2 for $N\\leq C^2/R^2_\\mathrm{min}$."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583729777,
                "cdate": 1700583729777,
                "tmdate": 1700583976483,
                "mdate": 1700583976483,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cjtvmqxD0Y",
            "forum": "Ww9rWUAcdo",
            "replyto": "Ww9rWUAcdo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission621/Reviewer_qaZD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission621/Reviewer_qaZD"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates learning from adversarial perturbations, which is motivated by the observation that neural networks trained on adversarial examples can generalize to natural test data. To that end, recent results on the implicit bias of the gradient flow of training single-hidden-layer networks are employed to show that, under certain assumptions, the decision boundaries of natural examples and adversarial examples align with each other."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea to apply the implicit bias results to learning from adversarial perturbations is novel.\n- The theoretical results admit interesting and relevant interpretations (eg. effect of learning misslabled data vs perturbation data)."
                },
                "weaknesses": {
                    "value": "- It is unclear what the motivation for studying the uniform perturbation model is.\n- Overall, the investigated model seems to be quite simple and theoretical assumptions restrictive (see also questions below).\n- The text is rather densely written (e.g. subsection 4.2 and 4.3), which makes understanding main statements and insights rather difficult (e.g. Theorem 4.2)."
                },
                "questions": {
                    "value": "- In the limitations paragraph it is mentioned that model is simple (one-hidden layer network and orthogonal training data). Could the authors extend their discussion on the limitations (e.g. attack model), and if possible, mention ideas on how to overcome the limitations in the future (e.g. $\\epsilon \\in O(d/N)$)?\n- The paper expands on established settings on 'learning from adversarial perturbations' (Definition 1.1. and Definition 3.2) by also discussing perturbations on uniform noises. In this 'perturbation on uniform noise' setting, i.e. $x_n^{adv} = X_n + \\eta_n$, with $X_n$ being uniform noise instead of a natural data point. What is the motivation for studying this setting? Why not considering $x^{adv}_n = x_n + X_n + \\eta_n$ (with $x_n$ the actual data points), which would be a more practical setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission621/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission621/Reviewer_qaZD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797294412,
            "cdate": 1698797294412,
            "tmdate": 1699635989712,
            "mdate": 1699635989712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pe39bLi3Eq",
                "forum": "Ww9rWUAcdo",
                "replyto": "cjtvmqxD0Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qaZD [1/1]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer\u2019s careful reading.\n\nTo address the concerns of some reviewers who might misunderstand our work as related to adversarial training, we have provided a detailed clarification in the general comment. Please also refer to it.\n\n### Noise data scenario\n> what the motivation for studying the uniform perturbation model\n\nWe consider the noise data scenario (cf. Definition 3.2(b)) with two motivations.\n\nFirst, this is to prevent unintentional leakage of useful features. In the natural data scenario (cf. Definition 3.2(a)), we labeled adversarial examples with incorrect classes. However, there remains the possibility that these images contain features related to the incorrectly assigned labels. For example, a frog image labeled as a horse may contain horses in the background. As long as we consider natural images, it is inevitable that unintentional alignment between images and labels will occur, causing classifiers to generalize from leaked features rather than perturbations. In contrast, uniform noises lack natural object features, allowing us to explore the purer dynamics of learning from perturbations.\n\nSecond, this is to validate the success of learning from perturbations under weaker conditions than in the natural data scenario. In the natural data scenario, the probabilistic nature of the base data (i.e., natural data) cannot be defined. For example, the distribution of frog images cannot be precisely determined. This uncertainty requires us to impose the assumption $|\\sum\\lambda_ny_n\\langle x_n,z\\rangle|=\\Theta(g(N,d))$ if $\\sum\\lambda_n|\\langle x_n,z\\rangle|=\\Theta(g(N,d))$. In the noise data scenario, thanks to the well-defined probabilistic properties of the base data (i.e., uniform noises), we can justify the success of perturbation learning under more reasonable assumptions (cf. Theorem 4.4).\n\n> Why not considering $x^\\mathrm{adv}_n=x_n+X_n+\\eta_n$, which would be a more practical?\n\nFirst, it is difficult to define what is \"practical\" in the context of learning from perturbations. This experiment aims to understand the origins of adversarial examples and the information contained in adversarial perturbations. From this perspective, both the natural and noise data scenarios align with the objectives, and we cannot determine the superiority. Similarly, we cannot determine which is better, $x^\\mathrm{adv}_n=X_n+\\eta_n$ or $x^\\mathrm{adv}_n=x_n+X_n+\\eta_n$. Please also refer to the general comment.\n\nFrom the perspective of a tractable theoretical analysis, the scenario where perturbations are on solely uniform noises can provide a more theoretically manageable formulation of learning from perturbations, as described in the comment above. Thus, we choosed $x^\\mathrm{adv}_n=X_n+\\eta_n$ rather than $x^\\mathrm{adv}_n=x_n+X_n+\\eta_n$.\n\n### Assumptions on networks and training data\n> the investigated model seems to be quite simple and theoretical assumptions restrictive\n> \n> it is mentioned that model is simple (one-hidden layer network and orthogonal training data). Could the authors extend their discussion on the limitations (e.g. attack model), and if possible, mention ideas on how to overcome the limitations in the future (e.g. $\\epsilon\\in O(d/N)$)?\n\nIf the reviewer regard \"attack model\" as the form of perturbation, we have explored three additional forms of perturbation in Appendix D that were not discussed in the main text in detail. Please refer to them.\n\nThe constraints of using a one-hidden-layer neural network, orthogonal training, and the resulting $\\epsilon\\in O(d/N)$ are based on the theoretical framework, Theorem 3.3 (Frei et al., 2023). To remove these constraints, we would need to extend Theorem 3.3 in the context of the implicit bias of gradient descent rather than learning from adversarial perturbations. Previous research [1,2] has demonstrated the implicit bias of unbiased deep ReLU networks under more relaxed dataset assumptions. Thus, we believe that these results could serve as a starting point to justify learning from perturbations under more relaxed assumptions.\n\n[1] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In ICLR. 2020.\n\n[2] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In NeurIPS. 2020.\n\n### Others\n> The text is rather densely written, which makes understanding main statements and insights rather difficult\n\nWe acknowledge that due to space constraints, some sections of our paper, particularly the explanation following Theorem 4.1, have become overly dense. We plan to address this by eliminating some redundant expressions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893477087,
                "cdate": 1699893477087,
                "tmdate": 1699893477087,
                "mdate": 1699893477087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "67XU9KP2Vj",
                "forum": "Ww9rWUAcdo",
                "replyto": "pe39bLi3Eq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission621/Reviewer_qaZD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission621/Reviewer_qaZD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for addressing my questions, particularly for elaborating on the motivation for the uniform noise model. I'll keep the positive score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684386131,
                "cdate": 1700684386131,
                "tmdate": 1700684386131,
                "mdate": 1700684386131,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iCEdt1yYIC",
            "forum": "Ww9rWUAcdo",
            "replyto": "Ww9rWUAcdo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission621/Reviewer_K8gK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission621/Reviewer_K8gK"
            ],
            "content": {
                "summary": {
                    "value": "This paper theoretically studies the learning of adversarial perturbations generated by geometric-inspired attacks with respect to a one-hidden-layer neural network and orthogonal training data. Specifically, two settings are considered for learning from adversarial perturbation: 1) perturbation on natural data and 2) perturbation on uniform noise. Theoretically, they show that the decision boundaries of models learned from adversarial perturbation are consistent with clean decision boundaries, except for examples strongly correlated with many natural samples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Theoretical understanding of how neural networks learn in the presence of adversarial perturbations is a challenging but important task. The paper is technically solid, with clearly presented assumptions and theoretical results. Theoretical results are accompanied by adequate discussions and high-level explanations of the proof idea. The considered setting where adversarial perturbations are added to uniform/Gaussian noise is new, which nicely supports the paper's key argument that generated adversarial perturbations contain abundant information about the underlying data structure."
                },
                "weaknesses": {
                    "value": "While I appreciate the theoretical nature of this paper, the motivation and some considered settings for studying adversarial perturbations in the context of learning need to be more convincing from my perspective. In the abstract, the paper claims that the phenomenon \u201cneural networks trained on mislabeled samples with adversarial perturbations can generalize to natural test data\u201d is counter-intuitive.  I do not understand why this is a counter-intuitive phenomenon and how this phenomenon motivates the problem of learning from adversarial perturbations. If I understand correctly, the adversarial perturbations are generated with respect to some pre-trained neural networks and are not necessarily \u201cmislabeled\u201d (unless the perturbation size is large). In addition, the definition of mislabeled data needs to be clarified because, typically, human is considered the underlying ground truth in judging whether an input image is mislabeled. However, you use a pre-trained neural network as the reference model for defining whether an input is mislabeled. I am also confused by the statement in the introduction, \u201cA classifier, trained on natural samples with adversarial perturbations labeled by such incorrect classes, can generalize to unperturbed data,\u201d which I believe is not the key message that Ilyas et al. (2019) wants to convey. From my perspective, the learnability of adversarially perturbated natural data is because the added perturbation is not large enough to destroy the useful generalizable features of the natural data. I hope the authors can clarify my concerns in the rebuttal on why the learnability from the added adversarial perturbations is worth studying, which will largely decide my judgement on whether this paper reaches the ICLR acceptance bar or not.\n\nMoreover, the imposed assumptions of orthogonal training data and the use of geometric-inspired attacks need to be better discussed. What is the mathematical definition of orthogonal training data? I do not understand the claim that the orthogonality of training data is a common property of high-dimensional data. Does this claim hold for any high-dimensional data distribution or specific distributions? In addition, I believe geometric-inspired attacks are less commonly studied in the adversarial ML community. How will the conclusion of your theoretical findings change if the perturbations are generated by PGD attacks (Madry et al., 2018)?"
                },
                "questions": {
                    "value": "Apart from the above questions, I would like to know how the degree of orthogonality of training data affects your theoretical results. If the underlying data distribution only partially satisfies this assumption, can you use similar proof techniques to characterize the learning from adversarial perturbations?\n\n\n============ Post Rebuttal Comments ===========\n\nI appreciate the authors' feedback. In summary, I will keep my score unchanged. I think the authors misunderstood my major concern, which has nothing to do with adversarial training or adversarial robustness. What is unconvinced from my perspective is the motivation for studying the setting of learning from adversarial perturbations. I do not find a convincing answer in the rebuttal, and I can hardly understand the benefits gained from studying this problem. The final conclusion drawn from the paper is that standard learning from adversarially perturbed inputs (with possibly incorrect labels) yields consistent predictions. Because of the assumptions on how the adversarial perturbations are generated, I am not sure if the paper's theoretical evidence is sufficient to support such general conclusion. In my opinion, the way adversarial perturbations are generated and the perturbation budget will highly affect the final conclusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission621/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission621/Reviewer_K8gK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699115013321,
            "cdate": 1699115013321,
            "tmdate": 1700808712644,
            "mdate": 1700808712644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vrWzrjqCGh",
                "forum": "Ww9rWUAcdo",
                "replyto": "iCEdt1yYIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission621/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K8gK [1/1]"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's fruitful suggestions and questions.\n\n**First, we would like to clarify that our research is NOT related to adversarial training. Please kindly refer to the general comment.**\n\n### Counterintuitiveness of learning from adversarial perturbations\n> adversarial perturbations are not necessarily mislabeled\n\nThis is true if it implies that adversarial examples may not always deceive a network.\n\n> the definition of mislabeled data needs to be clarified ... you use a pre-trained network as the reference model for defining whether an input is mislabeled.\n\nWe do **NOT** use a pre-trained network as the reference model. We consistently regard an image as mislabeled when it has a label different from the human prediction, i.e., an originally annotated label. For example, a natural image of a frog labeled as a horse is mislabeled. Similarly, an adversarial image of a frog (an image of a frog that is manipulated to lead classifier predictions to a horse but still seems to be a frog to humans) labeled as a horse is considered mislabeled.\n\n> the learnability of adversarially perturbated natural data is because the added perturbation is not large enough to destroy the useful generalizable features of the natural data.\n> \n> why this is a counter-intuitive\n\nThe reviewer might have misunderstood our focus. Please refer to the general comment.\n\nIf the perturbation size is not excessively large, it does not destroy the useful generalizable features of natural data. **Thus, learning from perturbations is counterintuitive.** Recall that our experiments, and Ilyas's, labeled images contrary to these generalizable features. **Since perturbations do not destroy generalizable features, these labels appear entirely nonsensical.** For example, frog images were labeled as horses. In fact, classifiers trained on this dataset classify frog images in this set as horses. However, they classify frog images in *test sets* as frogs, i.e. they achieve a counterintuitively high accuracy on test datasets.\n\n> how this phenomenon motivates the problem\n\nAs previously described, classifiers can achieve high test accuracy on standard datasets, even when trained on mislabeled adversarial images. This counterintuitive phenomenon led Ilyas et al. to propose a hypothesis: adversarial perturbations may contain features of the mislabeled class that are imperceptible to humans, and classifiers can learn data structures from them.\n\nThis intriguing hypothesis can explain not only why learning from perturbations succeeds but also why adversarial images can deceive classifiers: classifiers might respond to invisible horse features in adversarial frog images and thus misclassify frogs as horses. Moreover, this hypothesis accounts for other puzzling phenomena of adversarial images. Please refer to Section 2.\n\nAlthough this hypothesis seems reasonable, there has been no theoretical justification for its basis, i.e., the success of learning from adversarial perturbations. In this study, we are the first to provide the theoretical understanding and justification for it.\n\n### Orthogonality of training data\n> 1. mathematical definition of orthogonal training data\n> 2. how the degree of orthogonality of training data affects your theoretical results\n> 3. orthogonality of training data is a common property of high-dimensional data? Does this claim hold for any high-dimensional data or specific distributions? \n\n1. The orthogonality condition of training data is defined in Theorem 3.3 as $\\gamma^3R^4_\\mathrm{min}/(3NR^2_\\mathrm{max})\\geq p_\\mathrm{max}$ and extended to adversarial datasets as $\\gamma^3{R^\\mathrm{adv}\\_\\mathrm{min}}^4/(3N^\\mathrm{adv}{R^\\mathrm{adv}\\_\\mathrm{max}}^2)\\geq p^\\mathrm{adv}\\_\\mathrm{max}$. Due to $R^\\mathrm{adv}\\_\\mathrm{min}$, $R^\\mathrm{adv}\\_\\mathrm{max}$, and $p^\\mathrm{adv}\\_\\mathrm{max}$, the latter condition depends on the definition of perturbations and the base images (natural images or uniform noises). For geometry-inspired perturbations on natural images, this condition is transformed into Eq. (3), and for those on uniform noises, into Eq. (6).\n2. Our theoretical results hold even if the training data are not perfectly orthogonal, i.e., $p^\\mathrm{max}>0$. \n3. Statistically, two high-dimensional random vectors (e.g., with i.i.d. Gaussian entries) are nearly orthogonal. Although natural images are not random tensors, it has been empirically observed that they tend to be nearly orthogonal to each other. Please also refer to [1].\n\n[1] Sven-Ake Wegner. Lecture Notes on High-Dimensional Data.\n\n### Perturbation form\n> How will the conclusion of your theoretical findings change if the perturbations are generated by PGD attacks (Madry et al., 2018)?\n\nIn Appendix D, for different forms of perturbations, we derived the consistent results with those in the main text. The perturbations generated by PGD attacks are discussed in Appendix D.1. Please kindly refer to it."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893350262,
                "cdate": 1699893350262,
                "tmdate": 1699893350262,
                "mdate": 1699893350262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]