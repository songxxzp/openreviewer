[
    {
        "title": "When Semantic Segmentation Meets Frequency Aliasing"
    },
    {
        "review": {
            "id": "VXAaDGF4hp",
            "forum": "SYBdkHcXXK",
            "replyto": "SYBdkHcXXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission661/Reviewer_uz5r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission661/Reviewer_uz5r"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a challenging yet critical topic for semantic segmentation, i.e., pixel-wise aliasing. They categorize the hard pixel error into three types: false response, merging mistakes and displacements. Creatively, the de-aliasing filter and frequency mixing modules are introduced to alleviate the aliasing degradation. Experiments demonstrate that these findings can consistent improve the semantic segmentation performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "#1 The topic is attractive where the unexplored aliasing phenomenon for semantic segmentation are carefully investigated. This funding can motivate the researcher in the related community. \n#2 The authors utilize the DAF and FreqMix to remove the aliasing in Fourier domain and balance high-frequency components in the encoder block. Converting the features to frequency domain by DFT is straightforward, but finding the aliased false prediction for semantic segmentation is interesting."
                },
                "weaknesses": {
                    "value": "#1 Lacking visualization for the three type of errors in the experiment. Though the authors provide the results on ADE20K, it does not clearly present the visualization to the correction of three type of errors for semantic segmentation. In addition, how the DAF and FreqMix help locate these errors are not visualized."
                },
                "questions": {
                    "value": "#1. The authors must provide more visualizations on the three types of errors and the corresponding feature maps in frequence domain. \n#2. As to the DAF and FreqMix, how the filtered feature maps can help find the location of error map should be invastigated. \nOverall, the topic for finding the aliasing is interesting, the reviewer encourage the author further explore this missing parts in the semantic segmentation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission661/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764351677,
            "cdate": 1698764351677,
            "tmdate": 1699635993414,
            "mdate": 1699635993414,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h88rTyvQLT",
                "forum": "SYBdkHcXXK",
                "replyto": "VXAaDGF4hp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission661/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uz5r"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback and encouragement.\n\n## W1, Q1, Q2: More visualizations\n\n[Authors\u2019 response]\n\nWe acknowledge the importance of visualization in elucidating how the proposed methods operate and rectify errors. In response to your valuable advice, we have included comprehensive visualizations and analyses in the revised appendix. As the figure cannot be added in the comment box, we recommend referring to Section J of the revised appendix for a detailed visual representation. Section J.1 discusses how the FAD improves the feature during downsampling (Figure 5), Section J.2 discusses how the FreqMix suppresses unnecessary high frequency in the background and object centers (Figure 6), and Section J.3 provides a more visualized analysis of how the FAD and FreqMix improve the feature and prediction while relieving three types of errors (Figure 7). Section K provides additional frequency analysis of features (Figures 8 and 9). For your convenience, we have also attached our description of the visualizations below.\n\nIn Figure 7 of the revised appendix, we randomly selected image patches with a high aliasing score from the Cityscapes dataset validation set. We observed that aliasing leads to a jagged phenomenon[1,2], disrupting object shapes and boundaries, resulting in false responses (see Figure 7(b) and (l) in the revised appendix) and displacement errors (see Figures 7(c), (d), and (e) in the revised appendix). DAF and FreqMix relieve this phenomenon and improve the feature representation thus resulting in lower false responses and displacement errors.\n\nBesides, when high-frequency information is aliased, it transforms into false low-frequency information. For example, when two objects are close to each other, their high-frequency boundaries can be aliased to lower frequency during downsampling, causing the two objects to appear connected and their boundaries to be merged. This leads to merging errors (see Figures 7(a), (g), and (j) in the revised appendix). DAF/FreqMix solve this by removing/suppressing these high frequencies during downsampling/encoder block, leading to lower merging errors.\n\nMoreover, high-frequency components in the object center or background can result in false responses (see Figures 7(i) and (k) in the revised appendix). FreqMix addresses this issue by suppressing the high frequency in the object center or background while preserving the high frequency at the boundaries (see Figures 6(a) and (b) in the revised appendix).\n\nThanks for your valuable comments that help us improve our manuscript and clarify the proposed method more clearly!\n\n[1] Qian S, Shao H, Zhu Y, et al. Blending anti-aliasing into vision transformer. Advances in Neural Information Processing Systems, 2021, 34: 5416-5429.\n\n[2] Zou, X., Xiao, F., Yu, Z., Li, Y., & Lee, Y. J. (2023). Delving deeper into anti-aliasing in convnets. *International Journal of Computer Vision*, *131*(1), 67-81."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670395146,
                "cdate": 1700670395146,
                "tmdate": 1700710992546,
                "mdate": 1700710992546,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TC5pfRQVWO",
            "forum": "SYBdkHcXXK",
            "replyto": "SYBdkHcXXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission661/Reviewer_LE8t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission661/Reviewer_LE8t"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes a new anti-aliasing scheme by redefining the cutoff frequency. This is done by considering the expansion in both the channel dimension and the spatial dimension. The proposed aliasing score showed a strong correlation with various segmentation errors. Additionally, the authors introduced a new anti-aliasing and spectral filter that enhances the segmentation performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Investigates the critical yet under-explored question regarding the effect of aliasing in computer vision models.\n2. The proposed fixes enhance performance.\n3. The paper is well-written and clearly explained."
                },
                "weaknesses": {
                    "value": "1. The proposed module, FreqMix, requires additional forward and inverse Fourier transforms at each layer. Compared to other methods, this increase in time complexity (inference and training) needs to be discussed.\n2. Experiments are limited. It is necessary to conduct evaluations on other benchmark datasets, such as MS COCO and Pascal VOC.\n3. Improvements are marginal."
                },
                "questions": {
                    "value": "1. I need more clarification about the robustness of ESR (equivalent sampling rate). Clearly, the filters are orthogonal for the shown example (Fig 3). So there is no loss of information.\nBut what if the filters are not orthogonal to each other? Assume a worst-case scenario where all filters are equal. In that case, the proposed ESR will give us a wrong sampling rate. So, is the proposed ESR intended to replace the regular \u201csampling rate,\u201d or is it intended as a heuristic to select cutoff frequency? If it is the second, then the paper should clarify it.\n2. Section 3.2, \u201cMetrics for three errors.\u201d \u2014 I think the middle one should be the definition of MErr.\n3. \u201caligning with the observations in Figure 1 that false responses and merging mistakes predominantly exist in areas with relatively low aliasing scores.\u201d \u2014 could you clarify how we are associating aliasing scores with a region of an image? \n4. \u201cHowever, for deeper features at the second and third stages, the aliasing ratio decreases.\nThis decrease does not signify a reduction in aliasing; rather, it occurs because the earlier high level of aliasing results in severe degradation. Consequently, the features become \u2018hard pixels,\u2019 meaning that the following stage struggles to extract useful information and loses response to objects.\u201d \u2014 How can we confirm that it is caused by the side effect of aliasing? Applying significant noise can generate an out-of-distribution (OOD) sample of the internal layers, which can also lead to poor feature extraction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission661/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission661/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission661/Reviewer_LE8t"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission661/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699480239463,
            "cdate": 1699480239463,
            "tmdate": 1699635993320,
            "mdate": 1699635993320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZSkpQs5xmr",
                "forum": "SYBdkHcXXK",
                "replyto": "TC5pfRQVWO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission661/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LE8t weakness"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback!\n\n### W1: Time complexity analysis\n\n[Authors\u2019 response]\n\nHere, we conduct a comprehensive evaluation of the time complexity during both the training and testing phases. The results are presented below. We use a batch size of 16, a resolution of 512$\\times$512, and train for 40k iterations on a single RTX 3090 for evaluating training time. The test time is evaluated on 1024$\\times$2048 image patch with a batch size of 1.\n\n| Model               | Training time | Test time (1024x2048)        |\n| :------------------ | ------------- | ---------------------------- |\n| UPerNet             | 4.2 hours     | 45ms                         |\n| Ours (+DAF+FreqMix) | 5.9 hours     | 82ms (49ms without FFT&iFFT) |\n\nThe inclusion of DAF and FreqMix increases the training time from approximately 4.2 hours to around 5.9 hours. Similarly, it leads to an increase in inference time from approximately 45ms to around 82ms. Notably, the majority of this increase is attributed to the FFT/iFFT operations. When these operations are excluded, the inference time is reduced to 49ms. This gap is primarily due to the limited speed optimization in the engineering implementations of frequency transformations, such as FFT/iFFT. This phenomenon has also been observed in previous works [1]. This issue can be resolved by better engineering implementations of frequency transformations.\n\n\n\n[1] Huang, Z., Zhang, Z., Lan, C., Zha, Z. J., Lu, Y., & Guo, B. (2023). Adaptive Frequency Filters As Efficient Global Token Mixers. In *Proceedings of the IEEE/CVF International Conference on Computer Vision* (pp. 6049-6059).\n\n\n\n### W2: Experiments on COCO and Pascal VOC\n\n[Authors\u2019 response]\n\nFollowing your advice, we present additional quantitative results on the PASCAL VOC and COCO datasets, encompassing semantic segmentation, object detection, and instance segmentation tasks. On the PASCAL VOC, the proposed method demonstrates improvements in semantic segmentation results, with an increase of +1.8 mIoU, +2.5 BIoU, +2.1 BAcc, and a notable reduction in displacement error (DErr) by 2.5.\n\n| Method                     | mIoU$\\uparrow$  | BIoU$\\uparrow$ | BAcc$\\uparrow$ | FErr$\\downarrow$ | MErr$\\downarrow$ | DErr$\\downarrow$ | #FLOPs | #Params |\n| -------------------------- | --------------- | -------------- | -------------- | ---------------- | ---------------- | ---------------- | ------ | ------- |\n| UPerNet                    | 74.3            | 66.6           | 74.5           | 31.0             | 58.6             | 28.2             | 298.0G | 31.16M  |\n| **Ours** (+ DAF + FreqMix) | **76.1 (+1.8)** | **69.1**       | **76.6**       | **30.5**         | **58.5**         | **25.7**         | 298.6G | 32.47M  |\n\nOn the COCO, our proposed method enhances the average precision (AP) of object detection boxes by +1.6. Regarding instance segmentation, it results in a corresponding boost of +1.4 in box AP and +1.3 in mask AP.\nThese results demonstrate that the proposed method consistently leads to improvements by addressing the aliasing degradation, thereby verifying the generalization of our approach. They also highlight that the aliasing degradation widely exists in many fundamental computer vision tasks and needs to be urgently resolved.\n\n| Method                      | AP$^{box}$      | AP$^{mask}$     |\n| --------------------------- | --------------- | --------------- |\n| Faster R-CNN                | 36.4            | -               |\n| **Ours**  (+ DAF + FreqMix) | **38.0 (+1.6)** | -               |\n| Mask R-CNN                  | 37.2            | 34.1            |\n| **Ours** (+ DAF + FreqMix)  | **38.6 (+1.4)** | **35.4 (+1.3)** |\n\nWe thank the reviewer for encouraging us to fully verify the effectiveness on various datasets, making our work more solid."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670092855,
                "cdate": 1700670092855,
                "tmdate": 1700670277173,
                "mdate": 1700670277173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IWQmYGjs8T",
                "forum": "SYBdkHcXXK",
                "replyto": "TC5pfRQVWO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission661/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LE8t question 3-4"
                    },
                    "comment": {
                        "value": "### Q3: Incorrect figure reference.\n\n> \u201cAligning with the observations in Figure 1 that false responses and merging mistakes predominantly exist in areas with relatively low aliasing scores.\u201d \u2014 Could you clarify how we are associating aliasing scores with a region of an image?\n\n[Authors\u2019 response]\n\nThank you for your careful comments. The reference to the figure was indeed incorrect. We have rectified this to *\"Align with the observations in Figure **2**, where false responses and merging mistakes predominantly exist in areas with relatively low aliasing scores.\"* \n\nIn Table 1 of the main paper, we provide evidence indicating that increasing the blur kernel leads to a reduction in the overall aliasing score. Consequently, there is a decrease in displacement error, while the occurrences of false responses and merging mistakes exhibit an upward trend. This aligns with the statistical findings presented in Figure 2 of the main paper, illustrating the distribution of three errors. In this figure, we observed that the aliasing score of false responses and merging mistakes is lower than that of displacement error, providing consistent support for our observations.\n\nWe also have included comprehensive visualizations and analyses in the revised appendix (Figure 7 in the appendix). As the figure cannot be added in the comment box, we recommend referring to Section J of the revised appendix for a detailed visual representation.\n\n\n\n### Q4: How does aliasing lead to a loss of object response\n\n[Authors\u2019 response]\n\nThank you for your inspiring comments. We provide visualizations in Figure 5 of the revised appendix. As the figure cannot be added in the comment box, for your convenience, we have also attached our description of the visualization below. Please refer to Section J and Figure 5 of the revised appendix for a detailed visual representation.\n\nIn Figure 5 of the revised appendix, we have marked areas with high aliasing scores in the features with red boxes. Owing to the aliasing, the downsampled features exhibit a severe jagged phenomenon[2, 3], resulting in the degraded representation of object shapes and boundaries (see Figures 5(c) and (e) in the revised appendix). When downsampling increases to 4\u00d7, the response of some objects fades or is lost (see Figure 5(e) in the revised appendix). It is noteworthy that widely used state-of-the-art models, such as Swin Transformer and ConvNeXt, adopt a total downsampling stride of 32$\\times$, potentially leading to an even more severe loss of response. After applying the proposed DAF to address the aliasing artifact, the jagged phenomenon is relieved (see Figures 5(d) and (f) in the revised appendix) and the response of objects still remains strong in the 4\u00d7 downsampled features (see Figure 5(e) in the revised appendix). This indicates a strong correlation between the aliasing effect and a loss of object response.\n\nRecent work [2] also observes a similar phenomenon, where an increase in image noise raises the high frequency in features (leading to aliasing), resulting in deeper features losing the response of interested objects. Applying a low-pass filter (which can be regarded as an anti-aliasing filter) can alleviate this problem and recover the response of interested objects.\n\n[2] Qian S, Shao H, Zhu Y, et al. Blending anti-aliasing into vision transformer. Advances in Neural Information Processing Systems, 2021, 34: 5416-5429.\n\n[3] Zou, X., Xiao, F., Yu, Z., Li, Y., & Lee, Y. J. (2023). Delving deeper into anti-aliasing in convnets. *International Journal of Computer Vision*, *131*(1), 67-81.\n\n[4] Chen, L., Fu, Y., Wei, K., Zheng, D., & Heide, F. (2023). Instance Segmentation in the Dark. *International Journal of Computer Vision*, 1-21."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670339192,
                "cdate": 1700670339192,
                "tmdate": 1700670339192,
                "mdate": 1700670339192,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hifo7NSJtL",
            "forum": "SYBdkHcXXK",
            "replyto": "SYBdkHcXXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission661/Reviewer_qrZx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission661/Reviewer_qrZx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes two novel de-aliasing filter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing degradation by accurately removing or adjusting frequencies higher than the Nyquist frequency. The paper observes three different wrong segmentation types potentially caused by aliasing (a) False response, (b) Merging mistake (c) Displacement. In addition, the paper designed a simple de-aliasing filter to precisely remove aliasing as measured by their aliasing score. Additionally, we propose a novel frequency-mixing module to dynamically select and utilize both low and high-frequency information. The work has really comprehensive ablation studies, and the induction logic is comprehensive. They have proved their proposed method surpasses strong baselines Mask2Former, PointRend."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The experiment of the paper is very comprehensive and solid. The authors provide analysis of (1) the relationship between blur kernel size with Boundary, Three type errors, and Aliasing score.  (2) Noise level in effect to aliasing. (3) their cut-off frequency in relationship with accuracy and aliasing. (4) In comparison with other anti-aliasing modules. (5) Show effectiveness with model scaling up. (6) In Comparison with SoTA segmentation model.\n\n2. The paper introduces the concept of equivalent sampling rate for the Nyquist frequency calculation and proposes an aliasing score for quantitative measurement of aliasing levels."
                },
                "weaknesses": {
                    "value": "Actually, I think from a research perspective, I believe the paper is valid and sound so I recommend accepting this paper. However, from a higher level of view, many traditional problems including anti-aliasing have been eroded by the current trend of Large Models. The effectiveness of more training data or advances in pre-trained model weights will shrink the marginal gain of those methods. Especially for the transformer-based architecture, the downsample operation is no longer max-pooling even adaptive-pooling. this potentially alleviates the aliasing problem itself."
                },
                "questions": {
                    "value": "It would be really appreciated if the authors shared their thoughts on whether aliasing is still a problem if the backbone downsampling is fully adaptive in a transformer-based architecture."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission661/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission661/Reviewer_qrZx",
                        "ICLR.cc/2024/Conference/Submission661/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission661/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699570499750,
            "cdate": 1699570499750,
            "tmdate": 1701024747036,
            "mdate": 1701024747036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kMAgtvBhJf",
                "forum": "SYBdkHcXXK",
                "replyto": "Hifo7NSJtL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission661/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qrZx"
                    },
                    "comment": {
                        "value": "### Q1: Discussion about aliasing in a transformer-based architecture.\n\n[Authors\u2019 response]\n\nI sincerely appreciate your encouragement and valuable feedback. I would like to further discuss the question of whether aliasing remains a concern in a transformer-based architecture.\n\nWe think aliasing is still a problem in existing transformer-based architecture. Taking the renowned Vision Transformer (ViT) as an example, ViT [1] tokenizes images by splitting them into non-overlapping patches, which are then fed into transformer blocks. The tokenization and self-attention operations performed on these discontinuous patch embeddings can be viewed as downsampling operations, introducing a potential side effect of aliasing. It is essential to note that this downsampling operation is virtually unavoidable due to the spatial redundancy nature of the image [2] and huge computational costs without downsampling (increasing by 256$\\times$ without downsampling in ViT).\n\nSeveral existing studies have acknowledged this concern. A straightforward solution to alleviate aliasing is to increase the sampling rate. Similar trends are observed in vision transformers, where the use of overlapped tokens [3] and smaller patch sizes [4] contributes to improved performance. However, escalating sampling rates incur quadratic computational costs. Consequently, I hypothesize that integrating appropriate anti-aliasing filters into the 'attending' process could offer a viable solution. In fact, existing work has empirically explored blending anti-aliasing filters into the vision transformer, reporting observed improvements[5].\n\nIn conclusion, aliasing persists as a potential concern in transformer-based architectures, and prior studies have endeavored to address it empirically by enhancing the sampling rate or integrating anti-aliasing filters into the attention mechanism. Our work represents a step forward in quantitatively assessing and addressing aliasing in contemporary models, supported by theoretical foundations. This issue still presents ample opportunities for further investigation and exploration.\n\nWe appreciate your insightful questions that delve into the depth of our work and highlight its broad prospects.\n\n \n\n[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In *ICLR*, 2021. \n\n[2]He, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., & Girshick, R. Masked autoencoders are scalable vision learners. In CVPR, 2022\n\n[3] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. *in ICCV*, 2021. \n\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve\u0301 Je\u0301gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers.*in ICCV*, 2021. \n\n[5] Qian S, Shao H, Zhu Y, et al. Blending anti-aliasing into vision transformer. Advances in Neural Information Processing Systems, 2021, 34: 5416-5429."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669988092,
                "cdate": 1700669988092,
                "tmdate": 1700669988092,
                "mdate": 1700669988092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]