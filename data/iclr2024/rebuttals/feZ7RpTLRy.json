[
    {
        "title": "Bridging ML and algorithms: comparison of hyperbolic embeddings"
    },
    {
        "review": {
            "id": "0fgG9gBnf1",
            "forum": "feZ7RpTLRy",
            "replyto": "feZ7RpTLRy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission50/Reviewer_dipB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission50/Reviewer_dipB"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to bridge the gap between two communities that have recently focused on hyperbolic embeddings: the algorithmic and the machine learning communities. It emphasizes that, surprisingly, there are few cross-references between them, even though the machine learning community could benefit from these algorithmic developments. The paper compares the timing of the contributions, the specificities (such as speed) of the algorithmic solutions, and the datasets on which the methods are benchmarked. It provides a comprehensive analysis in terms of formulations and presents an extensive experimental comparison, demonstrating that solutions provided by the algorithmic community are often faster while delivering similar performances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of the paper are as follows:\n- A focus on certain works that are mostly ignored by the machine learning community while relying on the same tools.\n- A comprehensive analysis and comparison of the solutions, covering everything from formulation to experimental behavior.\n\nIn my opinion, the paper sheds light on somehow unknown works that exhibit interesting behavior in some cases. Consequently, the paper provides valuable insights into a topic that is of growing interest within the ML community."
                },
                "weaknesses": {
                    "value": "The primary weakness of the paper is that the solutions are primarily benchmarked against 2D Lorentz and Poincar\u00e9 embeddings. Although the content of the paper is intriguing, it is excessively specific, and I am not convinced of its overall significance. The paper fails to demonstrate how these results could enable the development of more efficient and faster solutions in general, especially in cases where higher-dimensional embeddings can be built. If there are some cases on which this should be done only on 2D, it should be emphasized. \n\nThe paper is sometimes difficult to follow, especially for readers like myself who come from the ML community. The explanation of the modeling of scale-free methods lacks clarity, and the experimental section is dense, making it difficult to grasp the big picture of the setup and the main conclusions."
                },
                "questions": {
                    "value": "- What prevents benchmarking the solutions against higher dimensional Poincar\u00e9 and Lorentz embeddings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission50/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698412477985,
            "cdate": 1698412477985,
            "tmdate": 1699635929087,
            "mdate": 1699635929087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "91q95F3iyB",
            "forum": "feZ7RpTLRy",
            "replyto": "feZ7RpTLRy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission50/Reviewer_4BpB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission50/Reviewer_4BpB"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about the lack of interaction between the ML community and TCS community. Specifically, that the ML community seems to be missing comparison to prior work on Hyperbolic Embedding and that these methods have some advantages over methods in the ML community."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper does a good job of thoroughly comparing Nickel and Kiela 2017 and 2018, with BFKL with and without the post processing step of DHRG. Specifically it shows the strengths of the TCS work. \n\nThe paper also does a very good job of listing related works and putting their work into context."
                },
                "weaknesses": {
                    "value": "My main concern is that the paper does not present any new algorithmic methods, nor does it prove any theory results, nor does it obtain significant new insights into hyperbolic representation learning. It primarily serves as a benchmark paper that compares two classes of methods that have not been compared before. \n\nI would suggest either submitting this to benchmarks track at NeurIPS or the new JMLR journal DMLR (Data-centric Machine Learning Research). I think this paper would be received very well in both of those venues."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission50/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698441741276,
            "cdate": 1698441741276,
            "tmdate": 1699635928945,
            "mdate": 1699635928945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "saFBK3kphJ",
            "forum": "feZ7RpTLRy",
            "replyto": "feZ7RpTLRy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission50/Reviewer_91h5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission50/Reviewer_91h5"
            ],
            "content": {
                "summary": {
                    "value": "This paper reviews some classic hyperbolic embedding methods in the ML and algorithm community. The paper then compares these methods and claim that a classic (and earlier) method from the algorithm community achieves better performance than other methods, with a much faster computation speed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper includes a very good review of the literature and investigated methods. It also makes effort to connect two different communities."
                },
                "weaknesses": {
                    "value": "While this paper has some nice review and interesting comparison, it does not propose any novel idea and therefore the contribution is limited. \n\nI cannot agree with the authors for their conclusion. Different algorithms are suitable for different tasks and data. While BFKL is faster, it is suitable for scale-free networks. The Poincar\u00e9 and Lorentz embeddings, in contrast, are suitable for data with specific data structures, and their outputs can be further used for clustering or classification. They are also able to capture complex relationships in the data that go beyond the geometric structure of a scale-free network. In addition, using an embedding dimention $=2$ is unfair because Poincar\u00e9 and Lorentz embeddings may have better performance at higher dimensions."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission50/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731625538,
            "cdate": 1698731625538,
            "tmdate": 1699635928824,
            "mdate": 1699635928824,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "VF34DBAEMr",
            "forum": "feZ7RpTLRy",
            "replyto": "feZ7RpTLRy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission50/Reviewer_wq3r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission50/Reviewer_wq3r"
            ],
            "content": {
                "summary": {
                    "value": "The research work made the following contributions:\n\nIt compares hyperbolic embedding methods from the machine learning community (Poincar\u00e9 and Lorentz embeddings) with methods from the algorithms community (BFKL embedding and DHRG improvement).\n\nIt finds that the BFKL embedding method is significantly faster (around 100x) than Poincar\u00e9 and Lorentz embeddings while achieving competitive results on common evaluation metrics like Mean Rank and Mean Average Precision.\n\nIt shows that discretizing the embeddings using DHRG can further improve results for the BFKL method in some cases. DHRG also helps avoid numerical precision issues.\n\n\nOverall, it demonstrates the potential for faster hyperbolic embedding methods from the algorithms literature to be competitive with popular machine learning approaches. More cross-pollination between communities could be beneficial."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work highlights the potential for faster algorithmic hyperbolic embedding techniques to be competitive with popular ML methods."
                },
                "weaknesses": {
                    "value": "The set of real-world networks tested is relatively small. Evaluating on a larger and more diverse set of networks could strengthen the empirical results. \n\nThe statistical analysis relates network factors like size and temperature to embedding quality, but the impact of other structural properties like clustering coefficient, degree distribution, etc., could also be enlightening.\n\nThe visualization analysis is limited. A more thorough qualitative analysis of the different embedding layouts and their strengths/weaknesses could provide additional insights."
                },
                "questions": {
                    "value": "The discrepancy between your WordNet noun results and those reported in Nickel & Kiela 2017 is quite large. Could there be differences in the dataset/preprocessing used? Please investigate potential reasons for this discrepancy.\n\nFor the ACM and MeSH hierarchies, you note inconsistencies in the number of nodes/edges compared to Nickel & Kiela 2018. Can you clarify the source of the data used in your experiments? Is it possible you are using different versions of these hierarchies?\n\nWhat is the largest network size you have managed to embed using the different methods? At what scale do you expect issues with the slower methods like Lorentz embeddings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission50/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782135298,
            "cdate": 1698782135298,
            "tmdate": 1699635928734,
            "mdate": 1699635928734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]