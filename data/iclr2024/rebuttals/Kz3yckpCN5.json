[
    {
        "title": "The False Promise of Imitating Proprietary Language Models"
    },
    {
        "review": {
            "id": "iq0ZufECrO",
            "forum": "Kz3yckpCN5",
            "replyto": "Kz3yckpCN5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4752/Reviewer_WxVs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4752/Reviewer_WxVs"
            ],
            "content": {
                "summary": {
                    "value": "The authors critically investigate the promise of finetuning language models using (imitation) data obtained from more capable language models(LMs). The paper challenges the assumption that this process improves an LM overall \u2013 but suggests that this is rather `mimicking their style`. They propose several interesting findings around what aspects of the performance improve or deteriorate upon tuning LMs on imitation data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I overall support the messages and findings in the paper.\n\n1. I appreciate this timely and critical investigation. Finetuning existing LMs on instruction-tuning data from more capable LMs has swiftly become common practice, yet we do not fully understand the change in behavior. The paper raises several critical questions that I would personally appreciate having in the literature.\n\n2. One novel and important finding is that the authors find that even though training on imitation data improves the results in crowd worker evaluations, they observe even a degradation in factuality. This is a significant consideration that should be kept in mind when finetuning models. This further raises a question about what other aspects of capabilities may be fluctuating during imitation data training.\n\n3. On the other hand, they inherit some of the useful properties, such as reduced toxicity / being more safe. Again, this further informs us about what really happens when models are trained on imitation data. It would be compelling to further explore a more fine-grained decomposition of performance gains and losses upon training on imitation data.\n\n4. The experiments are large-scale and informative yet not cheap to perform, thus findings enable valuable conclusions that are otherwise not easy to draw."
                },
                "weaknesses": {
                    "value": "There are 2 main points that I am concerned about.\n\n1. (IRB Approval / Exemption) The human study in the paper does not seem to have the relevant IRB approval or an exemption. The code of ethics states `Where human subjects are involved in the research process (e.g., in direct experiments, or as annotators), the need for ethical approvals from an appropriate ethical review board should be assessed and reported.` I am deferring the judgment about this to Ethics Reviewers / ACs. This should have most probably been done before the human subject study, but I encourage the authors to swiftly go through the IRB process for clarity.\n\n2. (Definition of Capability) The coverage of the capability evaluations is somewhat limited. Currently, authors evaluate mostly on factuality tests like MMLU/NQ/HumanEval and also show results around toxicity but how about other capabilities? Could it be that the imitation data leads LMs to reason better? Or, could it be that imitation data drives better calibration? While I do understand that there is a finite compute budget and there should be a limit in evaluation, the capability definition here is rather to limit the conclusions to draw. \n\n3. (Minor, Title) Given my concern in 2, I\u2019m personally slightly skeptical to call this a `false promise`. It is unclear if it is broadly a false promise, or if there are capabilities that are improved \u2013 it\u2019s rather there exists capabilities that this process even hurts, and we should be mindful about trusting crowdsourcing or other automated evaluations to understand the impact of a process."
                },
                "questions": {
                    "value": "1. The discussion seems to rely heavily on the concept of `tuning of imitation data`. However, I think one distinction is the kind of imitation data used to finetune most of the models, which are usually based on roughly arbitrary conversations between users and models. Would the authors agree that if the imitation data is constructed in a different way, then it may be possible to improve e.g. factuality of the finetuned model? For instance, I can imagine the way to construct imitation data to be around extracting rare knowledge, and then possibly the finetuned model could be improved a lot.\n\n2. Have the authors explored other capability definitions than factuality and toxicity? For instance, do we know anything about reasoning, calibration, creativity, truthfulness.. ? \n\n3. Did the authors get the necessary approvals from the IRB of their institution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The study involves human subjects (through MTurk), and it is unclear to me whether the authors got the necessary IRB approvals."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Reviewer_WxVs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698015898386,
            "cdate": 1698015898386,
            "tmdate": 1699636457611,
            "mdate": 1699636457611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cSGmMFYToy",
                "forum": "Kz3yckpCN5",
                "replyto": "iq0ZufECrO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments and suggestions and are glad that they believe our findings to be novel, important, and timely!\n\nWe respond to the main concerns below:\n\n```\n(IRB Approval / Exemption) The human study in the paper does not seem to have the relevant IRB approval or an exemption. The code of ethics states Where human subjects are involved in the research process (e.g., in direct experiments, or as annotators), the need for ethical approvals from an appropriate ethical review board should be assessed and reported. I am deferring the judgment about this to Ethics Reviewers / ACs. This should have most probably been done before the human subject study, but I encourage the authors to swiftly go through the IRB process for clarity.\n```\n\nWe have an irb for \u201cChatbots for Task-Oriented Dialogue\u201d Protocol # 2021-04-14272. We will include this in our acknowledgements section of the camera ready paper.\n\n```\n(Definition of Capability) The coverage of the capability evaluations is somewhat limited. Currently, authors evaluate mostly on factuality tests like MMLU/NQ/HumanEval and also show results around toxicity but how about other capabilities?\n```\n```\nHave the authors explored other capability definitions than factuality and toxicity? For instance, do we know anything about reasoning, calibration, creativity, truthfulness.. ?\n```\n\nWe agree that there are many additional questions that one could potentially ask about model imitation, and many of these could be a great subject for future work. In our paper we focused on a representative set of LM evaluations targeting factuality, coding ability, toxicity, and human preference. However, we agree that one notable exception from this list is reasoning. For that reason, we will add evaluations on gsm8k to our camera ready paper.\n\n```\n(Minor, Title) Given my concern in 2, I\u2019m personally slightly skeptical to call this a false promise. It is unclear if it is broadly a false promise, or if there are capabilities that are improved\n```\n\nWe agree with the sentiment of this comment and plan to adjust the framing of the title and abstract for the camera ready version of the paper.\n\n```\nWould the authors agree that if the imitation data is constructed in a different way, then it may be possible to improve e.g. factuality of the finetuned model?\n```\n\nYes, we would agree with this statement. In fact, there are some more recent works [1,2] which use novel methods for generating imitation data, and they show some signs that improving factuality may be possible.\n\n[1]: Orca: Progressive Learning from Complex Explanation Traces of GPT-4\n\n[2]: WizardLM: Empowering Large Language Models to Follow Complex Instructions"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511444929,
                "cdate": 1700511444929,
                "tmdate": 1700511444929,
                "mdate": 1700511444929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YoqolODvL0",
                "forum": "Kz3yckpCN5",
                "replyto": "cSGmMFYToy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_WxVs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_WxVs"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal! \n\n> IRB\n\nDoes the protocol number indicate it was approved in 2021, or is that just an arbitrary number? I'm not sure what is the situation in your institution, however, there are many institutions where IRB approvals are valid for only 1 year. Do you suggest that this is not the case with your institution and that the details of your human study protocol were included in 2021? \n\nI acknowledge the author's responses, and I do not have other questions. I do support this paper and its messages, however, we should be clear about what the suggested IRB entails and the authors practice a responsible study."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588032460,
                "cdate": 1700588032460,
                "tmdate": 1700588032460,
                "mdate": 1700588032460,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0h5ylCkMSZ",
            "forum": "Kz3yckpCN5",
            "replyto": "Kz3yckpCN5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4752/Reviewer_A4hH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4752/Reviewer_A4hH"
            ],
            "content": {
                "summary": {
                    "value": "The paper critically analyzes the approach of imitating proprietary systems (e.g., ChatGPT) by finetuning LMs using various model sizes, data sources, and imitation dataset sizes. Then the authors do human evaluation as well as evaluation on NLP benchmarks. Imitation models do not do well on tasks not heavily supported by the imitation data.\n\nOne interesting finding is that training on broad-coverage imitation data may decrease Natural Questions factuality, but training on NQ-like-data only will increase the accuracy. \n\nThe authors also conclude that the best action forward is to improve base LMs, instead of doing imitation on proprietary systems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I vaguely heard of this paper when it came out but it\u2019s my first time reading it. The motivation is excellent for sure (the public will care about this paper), given that many groups and startups are imitating proprietary language models potentially as a shortcut. \n\nThe findings are useful to many practitioners -- they'll likely carefully think whether knowledge distillation is useful or how it'll be useful. \n\nSome findings are quite interesting (see summary above for example)."
                },
                "weaknesses": {
                    "value": "I have three concerns related to crowdsourcing (see the next three paragraphs). \n\nDo human raters have low quality? The incentive design and crowdworker filtering seem lacking.  \n- What\u2019s the human agreement (e.g., Fleiss' kappa)?\n- What\u2019s the average time humans spend on each comparison?\n- Is there an option for humans to decline the comparison (because they may not be knowledgeable enough)? \n- How do you make sure that humans are rewarded based on correct choices, and potentially punished if they do extremely poorly?\n\nIt\u2019d be useful to have human evaluators write out rationales on why they chose one over another (or rate on multiple scales using multiple metrics). Otherwise concluding \u201chuman evaluators rate imitation models\u2019 outputs higher because of their style\u201d seems only a conjecture to me. \n\nIf crowdworkers have low quality (thus their annotations unreliable), then it doesn't seem prudent to use Figure 1(c) (crowdworker preference vs. number of model parameters) to reach the conclusion that we should improve base LLMs. \n\n\nI also have some other concerns: \n\nThere are two settings for imitation in the paper. The second setting is broad-coverage imitation. The imitation dataset size could be much larger. Currently the authors are using around (90+27+10)K examples, but this is quite a small number of examples \u2013 the dataset size is even smaller than most of the machine translation training sets from ten years ago.  \n\n\nThe results in this paper are only specific to supervised fine-tuning, not RLHF for example. This should be qualified in the intro paragraph. \n\nThe authors claim that matching ChatGPT using imitation would require an \u201cenormous\u201d amount of imitation examples. Is this supported anywhere in the paper?\n\n\n\nUpdate: I read the authors' response. I'm still a bit concerned about human annotation quality, but I raised the score."
                },
                "questions": {
                    "value": "Important: What are the decoding parameters for each model? (This is not addressed post-rebuttal.)\n\n\nBelow are minor (or very minor issues) in evaluating this paper:\n\nI wonder if practitioners may interleave ChatGPT imitation data with actual pretraining data and fine-tuning data. It\u2019s unclear if this setting would lead to the same problems. \n\nThe base models discussed in this paper are quite weak. For example, llama-1-13b is used instead of the SFT- and RLHF-tuned llama-2-13b-chat. The models are quite small too. Unclear if the results generalize to larger models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Reviewer_A4hH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698342775474,
            "cdate": 1698342775474,
            "tmdate": 1700801596326,
            "mdate": 1700801596326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gjZ7mfFQMF",
                "forum": "Kz3yckpCN5",
                "replyto": "0h5ylCkMSZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments and suggestions and are glad that they believe that our motivation is excellent and that our findings are useful and interesting!\n\nWe respond to the main concerns below:\n\n```\nDo human raters have low quality? The incentive design and crowdworker filtering seem lacking\n```\n\n\nWe will include further details on our human evaluation setup in the camera ready version of the paper. While we agree that many more sophisticated and costly human rater setups are possible, we and many others in the community use this setup or setups similar to ours to obtain human ratings for model outputs. Our point is not that humans in general can't distinguish between outputs from models of different capability levels but that many existing crowd worker evaluations are unable to distinguish these. This underscores our point that LM evaluation is a challenging open problem for future work to address.\n\n```\nIf crowdworkers have low quality (thus their annotations unreliable), then it doesn't seem prudent to use Figure 1(c) (crowdworker preference vs. number of model parameters) to reach the conclusion that we should improve base LLMs.\n```\n\nWe are not relying on Figure 1(c) to reach this conclusion, we are relying on an ensemble of evidence including the quantitative results on several benchmarks across several model sizes.\n\n```\nThere are two settings for imitation in the paper. The second setting is broad-coverage imitation. The imitation dataset size could be much larger. Currently the authors are using around (90+27+10)K examples\n```\n\nWe agree that training on larger datasets is indeed possible, but since we find that even increasing data by roughly an order of magnitude yields very few returns, we believe our conclusions hold.\n\n\n```\nThe authors claim that matching ChatGPT using imitation would require an \u201cenormous\u201d amount of imitation examples. Is this supported anywhere in the paper?\n```\n\nWe believe that this is supported by our scaling curves, but we will make this statement more precise in the camera ready version of our paper.\n\n```\nI wonder if practitioners may interleave ChatGPT imitation data with actual pretraining data and fine-tuning data. It\u2019s unclear if this setting would lead to the same problems. \n```\n\nContinuing to train on many more tokens of pre-training data is indeed a way to improve the base model, which would be inline with our conclusions about improvising base pretrained models. Therefore, we believe that this is out of scope for our paper, but could be interesting research for subsequent work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511380288,
                "cdate": 1700511380288,
                "tmdate": 1700511380288,
                "mdate": 1700511380288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dQKEbFs3fl",
            "forum": "Kz3yckpCN5",
            "replyto": "Kz3yckpCN5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4752/Reviewer_TcvA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4752/Reviewer_TcvA"
            ],
            "content": {
                "summary": {
                    "value": "The paper critically analyzes the method of using the output of a stronger LM to fine-tune and improve a weaker LM, pointing out that model imitation is not a free lunch.\nThe authors concluded that broadly matching ChatGPT using purely imitation would require (1) a concerted effort to collect enormous imitation datasets and (2) far more diverse and higher quality imitation data than is currently available."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Using the output of GPT-4 to cheaply improve a weaker language model by fine-tuning is widely adopted. This paper analyzes the drawbacks of doing so, which is helpful to guide the direction of developing more powerful LLMs.\n\n2. A large number of experiments and analyses prove the author's point of view."
                },
                "weaknesses": {
                    "value": "1. The author claimed that it is far more feasible to distill a specific behavior from ChatGPT as opposed to broadly matching its capabilities. However, the paper only conducted experiments on NQ-synthetic data.\n\n2. The paper claimed that imitation models are adept at mimicking ChatGPT's style but not its factuality and become far better at following instructions. But the other important ability of the model, that is, the ability to reason, has not been well studied."
                },
                "questions": {
                    "value": "Is model imitation still a good solution if the model's factual performance is decoupled to the retrieval model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Reviewer_TcvA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825097796,
            "cdate": 1698825097796,
            "tmdate": 1699636457450,
            "mdate": 1699636457450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kQ1I23eWEx",
                "forum": "Kz3yckpCN5",
                "replyto": "dQKEbFs3fl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments and suggestions and are glad that they think our work is \u201chelpful to guide the direction of developing more powerful LLMs\u201d.\n\nWe respond to the main concerns below:\n\n```\nThe author claimed that it is far more feasible to distill a specific behavior from ChatGPT as opposed to broadly matching its capabilities. However, the paper only conducted experiments on NQ-synthetic data.\n```\n\nWe will add an additional experiment on task-specific distillation for abstractive text summarization to the camera ready version of our paper.\n\n```\nThe paper claimed that imitation models are adept at mimicking ChatGPT's style but not its factuality and become far better at following instructions. But the other important ability of the model, that is, the ability to reason, has not been well studied.\n```\n\nWe agree that the effectiveness of model imitation for learning reasoning is indeed an interesting question! Some recent work has studied this [1]. However, we will also add evaluations on gsm8k to test this question ourselves.\n\n[1]: Orca: Progressive Learning from Complex Explanation Traces of GPT-4"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511253710,
                "cdate": 1700511253710,
                "tmdate": 1700511253710,
                "mdate": 1700511253710,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "usGBeLiDL1",
                "forum": "Kz3yckpCN5",
                "replyto": "kQ1I23eWEx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_TcvA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_TcvA"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. Could you provide some insights into the Questions \uff1f"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642686972,
                "cdate": 1700642686972,
                "tmdate": 1700642686972,
                "mdate": 1700642686972,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5cyQpNsPvN",
            "forum": "Kz3yckpCN5",
            "replyto": "Kz3yckpCN5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the question of acheiving performance parity with high-quality proprietary systems by training (smaller, generally lower quality models) on the outputs of the proprietary systems. The investigation is carried out over a range of data sizes collected from proprietary systems, or imitation data, and a range of model sizes. The authors' conclude that while training on some imitation data can improve the style of weaker models, there (i) is still a large performance gap to the proprietary models especially in evaluations of general capabilities, (ii) diminishing returns from increasing imitation data, (iii) greater gains (than collecting imitation data) by simply increasing the model size."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strengths of this work:\n- The question studied is important as most open-source models make use of imitation data for supervised finetuning.\n- The investigation along the data and model size axes is well thought out."
                },
                "weaknesses": {
                    "value": "The main weaknesses of this work:\n- The implicit assumption of this work (revealed in the title) is that there exists a claim or understanding that imitating proprietary language models by sampling their outputs for training is all that is needed to achieve performance parity - however, I contend that this isn't the prevalent understanding. It is understood that proprietary model output is a good source of finetuning data but not necessarily the only source. See for example the use of FLAN alongside imitation datasets like Alpaca for SFT.\n- The authors present style imitation of propreitary models as a negative aspect of training on imitation data (at least in the abstract), however the right amount of style imitation can be a definite source of improvement for open-source models - as the authors point out in Section 4.4. My suggestion would be to revise the abstract to reflect this more accurately."
                },
                "questions": {
                    "value": "In Figure 4. the 5-shot MMLU performance of the 13B imitiation model is quite low for a model of that size - can the authors describe the setup in more detail?\n\nSome discussion of the points raised in \"weaknesses\" would also be welcome."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU",
                        "ICLR.cc/2024/Conference/Submission4752/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843460902,
            "cdate": 1698843460902,
            "tmdate": 1700730153538,
            "mdate": 1700730153538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1OsE116hTh",
                "forum": "Kz3yckpCN5",
                "replyto": "5cyQpNsPvN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments and suggestions and we are glad that they believe that our work is important and that our experimental setup is well thought out!\n\nWe respond to the main concerns below:\n\n```\nThe implicit assumption of this work (revealed in the title) is that there exists a claim or understanding that imitating proprietary language models by sampling their outputs for \u2026\n```\n\nWe agree that there exist many alternative sources of high quality finetuning data which do not happen to be generated by proprietary language models, and that these sources may imbue different properties in the source model than the datasets that we consider in our paper.\n\nAdditionally, we agree that the prevailing understanding of model imitation and more broadly finetuning has quickly evolved in recent months and that this may not be the prevalent understanding today. We will therefore reframe the abstract and title to better reflect this for the camera ready.\n\n```\nThe authors present style imitation of propreitary models as a negative aspect of training on imitation data (at least in the abstract), however the right amount of style imitation can be a definite source of improvement for open-source models - as the authors point out in Section 4.4.\n```\n\nWe agree that there are many benefits to model imitation and will update our abstract and title to more clearly reflect this in the camera ready version.\n\n```\nIn Figure 4. the 5-shot MMLU performance of the 13B imitation model is quite low for a model of that size - can the authors describe the setup in more detail?\n```\n\nWe will add more details about our evaluation procedures to the appendix, but to answer your question, we use a standard MMLU evaluation procedure based on the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) repo with one exception. For our imitation models we abalated evaluations using both the log-probs of answer letters to select the answer choice and using the log-probs of the answer text. We found that using answer-letters consistently yielded higher MMLU performance on our imitation models, so we report the results using answer-letters in the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511126397,
                "cdate": 1700511126397,
                "tmdate": 1700511202744,
                "mdate": 1700511202744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "snImoh8PXj",
                "forum": "Kz3yckpCN5",
                "replyto": "1OsE116hTh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply - great to hear you acknowledge my concerns.\n\nCan you elaborate on how you are thinking of reframing this work?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644857082,
                "cdate": 1700644857082,
                "tmdate": 1700644857082,
                "mdate": 1700644857082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UBdWYszXoq",
                "forum": "Kz3yckpCN5",
                "replyto": "6L62LxvvOQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4752/Reviewer_msgU"
                ],
                "content": {
                    "comment": {
                        "value": "With the revised abstract and title more accurately reflecting the takeaways from this work, I am willing to increase my score to 6. \n\nHowever, I still believe we (as a field) are moving towards unlocking ways to improve model training on the outputs of larger proprietary models and it is too early to conclude the negative result. If accepted, this paper would serve as a historical note and an example of where model imitation breaks down."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730138807,
                "cdate": 1700730138807,
                "tmdate": 1700730138807,
                "mdate": 1700730138807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]