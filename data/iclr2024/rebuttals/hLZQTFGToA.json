[
    {
        "title": "Contrastive Learning is Spectral Clustering on Similarity Graph"
    },
    {
        "review": {
            "id": "CMGMWnYfdn",
            "forum": "hLZQTFGToA",
            "replyto": "hLZQTFGToA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1212/Reviewer_5UyX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1212/Reviewer_5UyX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proves that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, the authors extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Based on the maximum entropy principle, the authors demonstrate that the exponential kernels are the natural choices for capturing the local similarity structure for contrastive learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The  originality, quality, and significance are well supported by the proof of the equivalence of SimCLR and spectral clustering on the similarity graph and the extension to the multi-modal setting.\n\n2. The clarity is satisfied based on the clear illustration of the analysis in Figure 1 and the clear motivations and contributions in the Introduction part of this paper."
                },
                "weaknesses": {
                    "value": "1. The rationality of treating K_Z and \\pi as MRFs and comparing the induced probability distributions on subgraphs should be better explaned, which is not well persuasive as shown in this version of the submitted paper.\n\n2. For the definition of W, the authors are expected to further explain the mentioned unitary out-degree filter, which may be confused for the readers in understanding this definition.\n\n3. The reason that cross-entropy loss can be converted to the combination of repulsion and attraction terms is expected to be further given after Lemma 2.4. Is it closely related to Lemma 2.5 and what is the specific relation?\n\n4. In the experiment, the improvements of the proposed method is not obvious compared with SimCLR on the given datasets. The authors should further analyze the reason for 200 epochs and 400 epochs, respectively.\n\n5. The authors should repeat each experiment for many times and list the mean and deviation to avoid the possible randomness, i.e.,  Table 1 and Table 4."
                },
                "questions": {
                    "value": "1. Why choose Laplacian kernel and Simple Sum kernel for the MoCo experiment results should be further stressed, i.e., why the Gaussian kernel is not selected here.\n\n2. Why the authors choose p=1,q=0 and p=0.75 and q=0.2 in the syntetic experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1212/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1212/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1212/Reviewer_5UyX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697791708299,
            "cdate": 1697791708299,
            "tmdate": 1699636047781,
            "mdate": 1699636047781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sRmecoENeI",
                "forum": "hLZQTFGToA",
                "replyto": "CMGMWnYfdn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 5UyX"
                    },
                    "comment": {
                        "value": ">Q1: The rationality of treating K_Z and \\pi as MRFs and comparing the induced probability distributions on subgraphs should be better explaned, which is not well persuasive as shown in this version of the submitted paper.\n\nA1: Thank you for your suggestion. The reason why we compare $K_Z$ and $\\pi$ by MRF are mainly two-folds. **These matrices are too large and, as they are constructed by the augmentation process, also make it hard to explicitly form them.** Using MRF and comparing the induced probability can serve as a good necessary condition to compare $K_Z$ and $\\pi$, where the computation is not very costly by using the decomposition lemma 2.4.  We have added more explanations for this motivation in the revised manuscript.\n\n>Q2: For the definition of W, the authors are expected to further explain the mentioned unitary out-degree filter, which may be confused for the readers in understanding this definition.\n\nA2: Thank you for your suggestion. **We initially discussed the unitary out-degree filter right after the definition of $W$.** We have made changes in the revised manuscript to give clearer explanations to this defined term.\n\n>Q3: The reason that cross-entropy loss can be converted to the combination of repulsion and attraction terms is expected to be further given after Lemma 2.4. Is it closely related to Lemma 2.5 and what is the specific relation?\n\nA3: **The statement \"cross-entropy loss can be converted to the combination of repulsion and attraction terms\" is actually an intuitive summary of lemma 2.5.** We have rephrased the statement to make it directly point to lemma 2.5 in the revised manuscript. This statement is just naming the two terms in equation (4), the first term is an attraction and the second term a repulsion. These names are taken from the paper [1] as lemma 2.5 is just a slightly generalized result from this prior work.\n\n[1]: A Probabilistic Graph Coupling View of Dimension Reduction, Hugues Van Assel, Thibault Espinasse, Julien Chiquet, Franck Picard, NeurIPS 2022\n\n>Q4: In the experiment, the improvements of the proposed method is not obvious compared with SimCLR on the given datasets. The authors should further analyze the reason for 200 epochs and 400 epochs, respectively.\n\nA4: Thank you for your suggestion. **The experiments show that changing kernels may accelerate the convergence speed and improve classification accuracy.** Thus, we can see that changing kernel will usually result in an accuracy at 200 epochs that matches the accuracy of SimCLR at 400 epochs. When training proceeds, all methods exhibit an increase in accuracy. The reason why the proposed method is better than SimCLR may be intuitively understood by seeing our method as a sort of ensembles on kernel functions. The reason why the result of SimCLR is fairly good may be attributed to the good expressiveness of the Gaussian kernel. \n\n>Q5: The authors should repeat each experiment for many times and list the mean and deviation to avoid the possible randomness, i.e., Table 1 and Table 4.\n\nA5: Thank you for your suggestion. **We have repeated the experiments 3 times and updated the Tables in the revised manuscript.**\n\n>Q6: Why choose Laplacian kernel and Simple Sum kernel for the MoCo experiment results should be further stressed, i.e., why the Gaussian kernel is not selected here.\n\nA6: Thank you for your feedback. The reason why we choose Laplacian kernel and Simple Sum kernel is that it is consistent with what we did on the SimCLR experiments. **When the kernel is a Gaussian kernel, the loss is the initial MoCo loss, thus it is implicitly selected there.**\n\n>Q7: Why the authors choose p=1,q=0 and p=0.75 and q=0.2 in the syntetic experiment?\n\nA7: The goal of synthetic experiments is to give pictorial evidence of showing the equivalence of InfoNCE loss and spectral clustering. **The two sets of parameters  p=1,q=0 and p=0.75 and q=0.2 are chosen because they are typical.** p=1, q=0 depicts a scenario where points are fully connected and points among clusters are fully dis-connected. Thus a spectral clustering on this graph will have embeddings in the same clusters collapsed into a single point and different clusters have different collapse points. This phenomenon is clearly shown in the upper half of Figure 3. p=0.75 and q=0.2 shows another scenario where points have a much higher probability of another point in its cluster than connecting a point in another cluster. Thus, a spectral clustering on this graph will have embeddings that: the point from a same cluster will be closer and further from points in different clusters. This phenomenon is clearly shown in the lower half of Figure 3."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310192832,
                "cdate": 1700310192832,
                "tmdate": 1700409301975,
                "mdate": 1700409301975,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZKmgGvshdd",
            "forum": "hLZQTFGToA",
            "replyto": "hLZQTFGToA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1212/Reviewer_Hqsc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1212/Reviewer_Hqsc"
            ],
            "content": {
                "summary": {
                    "value": "- The paper proves that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph, which is defined by the data augmentation process. \n- The paper extends this result to the multi-modal setting and shows that CLIP is equivalent to spectral clustering on the pair graph."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It provides a novel theoretical analysis of contrastive learning and its connection to spectral clustering, which can help understand the underlying mechanisms and principles of this popular self-supervised learning method.\n- It proposes a new Kernel-InfoNCE loss with mixture of kernel functions that is inspired by theory and achieves better performance than the standard Gaussian kernel on several benchmark vision datasets"
                },
                "weaknesses": {
                    "value": "- I think the motivation is not good enough, such a conclusion is easy to obtain, i.e.,  InfoNCE loss is equivalent to spectral clustering. Since graph is pariwise relationship and constrastive is also pairwise relationship, both have the similar objective.\n- as the first point, I think the kernel infoNCE is also not well motivated. \n\n#### It's important to address these concerns regarding motivation in your paper. To improve the motivation for both the InfoNCE loss and the kernel InfoNCE, you might consider the following:\n\n> InfoNCE Loss Motivation:\n\n- Emphasize the practical significance and real-world applications of the InfoNCE loss. How does it relate to real-world problems or datasets in a way that goes beyond spectral clustering?\n- Highlight specific challenges or limitations in existing methods that the InfoNCE loss aims to address.\n\n> Kernel InfoNCE Motivation:\n\n- Explain how the kernel InfoNCE extends the motivation from the InfoNCE loss. What specific problems or scenarios does the kernel-InfoNCE address that are not covered by the standard InfoNCE?\n- Provide examples or use cases where kernel InfoNCE can be especially valuable.\n> By offering a more compelling rationale and demonstrating the practical relevance of these concepts, you can strengthen the motivation for these components in your paper."
                },
                "questions": {
                    "value": "> see the Weaknesses\n- Could you please share your reasons behind this?  it to be innovative?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854641083,
            "cdate": 1698854641083,
            "tmdate": 1699636047694,
            "mdate": 1699636047694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BYPiQkFRq7",
                "forum": "hLZQTFGToA",
                "replyto": "ZKmgGvshdd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Hqsc"
                    },
                    "comment": {
                        "value": ">Q1: I think the motivation is not good enough, such a conclusion is easy to obtain, i.e., InfoNCE loss is equivalent to spectral clustering. Since graph is pariwise relationship and constrastive is also pairwise relationship, both have the similar objective.\n\nA1: We are glad that you agree that our conclusion is evidently correct, but providing rigorous proof for this matter is not a trivial task. In fact, there hasn't been any proof showing the exact equivalence of SimCLR (InfoNCE loss) and spectral clustering. There have been analyses on variants of InfoNCE loss (spectral contrastive loss [1], euclidean MSE variant of SimCLR[2]), notably [1] showing a variant of InfoNCE loss is spectral clustering and receives oral in NIPS 2021. **However, as InfoNCE loss is the most popular contrastive loss used in literature, we fill the theoretical gap that InfoNCE is also conducting spectral clustering. The motivation for our work is to mainly clarify the equivalence of the initial InfoNCE loss (not its variant) and spectral clustering, which is meaningful and the techniques we used are also not trivial.**\nTherefore, it is important to emphasize our main contribution, which is the rigorous proof of the exact equivalence between contrastive learning (SimCLR) and spectral clustering. While this may be easily observed in the synthetic experiments we conducted, experimental observations cannot fully replace rigorous mathematical proofs. For instance, the simplex method is a widely used algorithm that was proven to be polynomial in the probabilistic sense later than the invention of the algorithm.\n\n[1] Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss, Jeff Z. HaoChen, Colin Wei, Adrien Gaidon, Tengyu Ma, NeurIPS 2021 (Oral)\n\n[2] Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods, Randall Balestriero, Yann LeCun, NeurIPS 2022\n\n>Q2: as the first point, I think the kernel infoNCE is also not well motivated.\n\nA2: The motivations for introducing kernel infoNCE are two-fold. **One is that this loss can be easily derived from our MRF framework by changing kernels.** The other is that the choices of exponential kernels (which are used in the loss) can be understood by a maximal entropy principle.\n\n>Q3: It's important to address these concerns regarding motivation in your paper. To improve the motivation for both the InfoNCE loss and the kernel InfoNCE, you might consider the following......, InfoNCE Loss Motivation:......, Kernel InfoNCE Motivation:......\n\nA3: Thank you for your suggestion. We did not propose InfoNCE loss ourselves, InfoNCE loss is itself one of the most popular losses used in contrastive learning. **The motivation of our paper is to rigorously prove its equivalence to spectral clustering, which fills the gap in the area of theoretical understanding of self-supervised learning algorithms.** The motivation for our kernel-InfoNCE loss is that we show that it can be seen as a natural generalization of InfoNCE loss from our MRF and maximal entropy theory in sections 3 and 5. Moreover, we test the effectiveness of kernel-InfoNCE loss empirically."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310838901,
                "cdate": 1700310838901,
                "tmdate": 1700409188447,
                "mdate": 1700409188447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ENx5AvTHkU",
            "forum": "hLZQTFGToA",
            "replyto": "hLZQTFGToA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1212/Reviewer_TB2Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1212/Reviewer_TB2Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper applies a probabilistic graph coupling perspective to view two typical constrastive learning methods, including SimCLR and CLIP, and interpretes them as spectral clustering or generalized spectral clustering. Moreover, it also attempts to propose to use exponential kernels to replace the Gaussian kernel. Preliminary experiments show that using a mixtures of exponential kernels to replace the Gaussian kernel in the SimCLR loss yields improved classification accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ It is interesting to interprete the InfoNCE loss in SimCLR and CLIP into the perspective of probablistic graph coupling and thus find the connection to spectral clustering or generalized spectral clustering."
                },
                "weaknesses": {
                    "value": "- The reviewer was confused by the discussion before introducing problem (P1). Since that it is required that the $\\mathbf \\alpha$ has fewer nonzero entries, some objective of sparsity-promoting property is necessary. However, in (P1) an entropy regularization term is imposed. It is well known that the optimal solution for the maximal entropy problem in the discrete random variable is a uniform distribution. Here, the optimal solution for $\\alpha_i$ should be $1/n$. It is weired to have a problem in (P2) and the solution in Theorem 5.1. Note that $\\tau$ is the Lagrangian multiplier, i.e., dual variable, it is incomplete to have the dual variable inside. \n\n- Moreover, there are mistakes in the formulation of (P2). It is neither the Lagrangian nor the Lagrangian dual problem. It is misleading to claim minimizing (P2) producing an upper bound of (P1). \n\n- In Section 5.2, it is stated that Theorem 5.1 suggests that the loss function of that form is a natural choice for characterizing the neighborhood similarity structure. The reviewer cannot see this point. Such a form is nothing but a choice on purpose to use the maximal entropy (or due ot mistakes?). \n\n- In Eq. (6), it is a RBF kernel, cannot be directly yielded from the form in Theorem 5.1. Because having an exponential form does not imply to have the property of a RBF kernel. In this way, the so-called kernel-InforNCE is nothing but a heuristic form to define the similarity in the InfoNCE loss function. \n\n- The related work is not good. Some remarks on the previous work are either improper or even misleading. \n\n- The experimenal evaluation is limited."
                },
                "questions": {
                    "value": "- The reviewer was confused by the discussion before introducing problem (P1). It is weired to have a problem in (P2) and the solution in Theorem 5.1. \n\n- Moreover, there are mistakes in the formulation of (P2). It is neither the Lagrangian nor the Lagrangian dual problem. It is misleading or something is missig to claim minimizing (P2) producing an upper bound of (P1). \n\n- The reviewer cannot see that ``Theorem 5.1 suggests that the loss function of that form is a natural choice for characterizing the neighborhood similarity structure\".  \n\n- The reviewer is not clear how to have a RBF kernel from the form in Theorem 5.1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1212/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1212/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1212/Reviewer_TB2Y"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859880098,
            "cdate": 1698859880098,
            "tmdate": 1700661076841,
            "mdate": 1700661076841,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z4CYgMjy7n",
                "forum": "hLZQTFGToA",
                "replyto": "ENx5AvTHkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Total response to reviewer TB2Y"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed review of our paper. We have noticed that you mainly have some confusion regarding our analysis in Section 5, and we would be able to clarify these confusions effectively.\n\nFirst of all, let's discuss the idea behind our optimization problem (P1). In fact, $\\psi_1$ represents the similarity between the query sample q and its positive sample. **In contrastive learning, we certainly want this similarity to be maximal among contrastive samples**. Therefore, we have the following 3 constraints: $\\alpha^T 1_n = 1$ and $\\psi_1 - \\sum \\alpha_i \\psi_i \\leq 0$ and $\\alpha \\geq 0$. It is important to note that the second constraint alone does not directly imply that $\\psi_1$ is maximized. Let's consider a few examples, where there will be 3 contrastive samples and the first one is a positive sample. We abbreviate $\\psi = (\\psi_1, \\psi_2, \\psi_3)$:\n\nExample 1:\n$\\psi = (0.8, 0.5, 0.5)$\n\nIn this case, there is a unique solution for $\\alpha$, which is (1, 0, 0). In fact, if $\\psi_1$ is maximal among contrastive samples $\\alpha$ must have a unique solution and should not be a uniform distribution. In this scenario, H($\\alpha$)=0.\n\nExample 2:\n$\\psi = (0.8, 0.8, 0.5)$\n\nIn this case, there are multiple solutions for $\\alpha$, such as (1, 0, 0), (0, 1, 0), (0.3, 0.7, 0), (0.5, 0.5, 0). The optimal solution for (P1) is (0.5, 0.5, 0), and in this case, H($\\alpha$)= $\\log 2$. Note that $\\alpha$ is still not a uniform distribution. Instead, it is a uniform distribution among the first two elements.\n\nExample 3:\n$\\psi = (0.8, 1, 0)$\n\nIn this case, there are multiple solutions for $\\alpha$ as well, such as (0, 0.8, 0.2), (0.2, 0.64, 0.16), and so on. The optimal solution for (P1) is not straightforward to compute.\n\n**Therefore, in general, the optimal solution to (P1) is not obtained when $\\alpha$ is a uniform distribution**. Moreover, as observed from the examples above, as long as $\\psi_1$ is not the maximal among contrastive samples, H($\\alpha$) will always be greater than 0. Hence, we leverage this characteristic and attempt to optimize $\\psi$ in a way that H($\\alpha$) tends to approach 0 (because H((1, 0, 0, 0, ...))=0). \n\nTherefore, if we can solve the program $min_\\psi \\max_\\alpha H(\\alpha)$  (where the max should be in fact optimization problem (P1), but we abbreviate the constraints  $\\psi_1 - \\sum \\alpha_i \\psi_i \\leq 0$ and $\\alpha^T 1_n = 1$ and $\\alpha \\geq 0$ for notation simplicity here), we can achieve our objective in contrastive learning. **However, contrastive learning does not directly use this program because $\\max_\\alpha H(\\alpha)$, which is (P1), is not easy to compute**.\n\nSo, we consider transforming (P1) as follows:\nWe introduce a function $L(\\alpha, \\tau) = H(\\alpha) - 1/\\tau (\\psi_1 - \\sum \\alpha_i \\psi_i)$ and impose the constraints $\\alpha^T 1_n = 1$ and $\\alpha \\geq 0$.\nWe can observe that for any $\\tau > 0$, $L(\\alpha, \\tau) \\geq$ the objective function of (P1). This is because for any $\\alpha$ that satisfies (P1), we have $\\psi_1 - \\sum \\alpha_i \\psi_i \\leq 0$. Therefore, $H(\\alpha) - 1/\\tau (\\psi_1 - \\sum \\alpha_i \\psi_i) \\geq H(\\alpha)$.\n\nHence, we can consider the following program: $max_\\alpha L(\\alpha, \\tau)$ subject to $\\alpha^T 1_n = 1$ and $\\alpha \\geq 0$. We know that the solution to this problem is an upper bound of the original problem.\n\n**The objective of (P2) is $-E(\\alpha) = \\tau L(\\alpha, \\tau)$.** Thus, from $\\tau (H(\\alpha) - (\\psi_1 - \\sum \\alpha_i \\psi_i)) \\geq \\tau (H(\\alpha))$. Thus, (P2) is an $\\tau$ times upper bound of (P1). The way we derive the above bounding property is known as weak duality in Lagrange duality theory (https://en.wikipedia.org/wiki/Duality_(optimization)).\n\nNotably, our Theorem 5.1 proves that (P2) has a closed-form solution, which is $-\\tau \\log \\frac{\\exp (\\frac{1}{\\tau} \\psi_{1})}{\\sum_{i=1}^n \\exp (\\frac{1}{\\tau} \\psi_{i})}$. Therefore, we know that $-\\log \\frac{\\exp (\\frac{1}{\\tau} \\psi_{1})}{\\sum_{i=1}^n \\exp (\\frac{1}{\\tau} \\psi_{i})}$ is an upper bound of (P1). **From this perspective, a loss form like InfoNCE is natural for solving contrastive learning problems.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311817822,
                "cdate": 1700311817822,
                "tmdate": 1700408551432,
                "mdate": 1700408551432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DGqqsMXPgo",
                "forum": "hLZQTFGToA",
                "replyto": "ENx5AvTHkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1 to reviewer TB2Y"
                    },
                    "comment": {
                        "value": ">Q1: The reviewer was confused by the discussion before introducing the problem (P1). Since that it is required that the \\alpha has fewer nonzero entries, some objective of sparsity-promoting property is necessary. However, in (P1) an entropy regularization term is imposed. It is well known that the optimal solution for the maximal entropy problem in the discrete random variable is a uniform distribution. Here, the optimal solution for \\alpha_i should be 1/n. It is weird to have a problem in (P2) and the solution in Theorem 5.1. Note that \\tau is the Lagrangian multiplier, i.e., dual variable, it is incomplete to have the dual variable inside.\n\nA1: **As we have explained above through examples, the optimal solution to the maximal entropy problem is 1/n when the constraint of $\\psi_1 - \\sum \\alpha_i \\psi_i \\leq 0$ does not exist.** However, this may not be the case where this constraint is added. The Lagrangian function $E(\\alpha, \\tau)$ is generated by introducing dual variable $\\tau$. In the initial manuscript, we abbreviated the dependency of E on $\\tau$ for notation simplicity. The problem (P2) is $max_{\\alpha} E(\\alpha, \\tau)$, which calculates the objective function used in a Lagrangian dual problem of (P1) where $\\tau$ is the dual variable.\n\n>Q2: Moreover, there are mistakes in the formulation of (P2). It is neither the Lagrangian nor the Lagrangian dual problem. It is misleading to claim minimizing (P2) producing an upper bound of (P1).\n\nA2: The formulation of (P2) is calculating the **objective function** in the Lagrangian dual problem of (P1). **As we discussed in the above total response, (P2) provides a $\\tau$ times upper bound for the initial problem (P1)**. Then minimizing (P2) will make (P1) smaller.\n\n>Q3: In Section 5.2, it is stated that Theorem 5.1 suggests that the loss function of that form is a natural choice for characterizing the neighborhood similarity structure. The reviewer cannot see this point. Such a form is nothing but a choice on purpose to use the maximal entropy (or due ot mistakes?).\n\nA3: **As we have discussed in the above total response, the value of H($\\alpha$) reflects how many contrastive samples have similar similarity to the query sample compared to the similarity of the positive sample and query sample**. Thus the optimal value of the optimization problem (P1) can characterize the neighborhood similarity structure. Then we show that the loss function of the form in Theorem 5.1 can bound the problem (P1), in this way we explain why the loss function of the form of InfoNCE is natural. \n\n>Q4: In Eq. (6), it is an RBF kernel, that cannot be directly yielded from the form in Theorem 5.1. Because having an exponential form does not imply having the property of an RBF kernel. In this way, the so-called kernel-InforNCE is nothing but a heuristic form to define the similarity in the InfoNCE loss function.\n\nA4: That is a great question! **There are various different ways to define the similarities between two points, and in (6), we pick the similarity function $\\psi_i = C - \\| f(q) - f(p_i) \\|^{\\gamma}$, where C is a large positive constant.** The form in theorem 5.1 is exactly the exponential kernels given by equation (6). Thus our derivation is not a heuristic.\n\n>Q5: The related work is not good. Some remarks on the previous work are either improper or even misleading.\n\nA5: Thank you for your suggestions. **We have added more summaries of existing works and made the contributions of previous works more accurate.**\n\n>Q6: The experimenal evaluation is limited.\n\nA6: We have conducted experiments by changing kernels based on SimCLR and MoCo. **The experiments show improvements upon baselines, which support our theory that the kernel function may be changed.**  As we construct kernels using the exponential family kernel, further validate our derivation that the exponential kernel is natural. We further conduct synthetic experiments to give a pictorial understanding of our main theorem: contrastive learning is spectral clustering."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700313120148,
                "cdate": 1700313120148,
                "tmdate": 1700408830951,
                "mdate": 1700408830951,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zGUhsNeUa9",
                "forum": "hLZQTFGToA",
                "replyto": "ENx5AvTHkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2 to reviewer TB2Y"
                    },
                    "comment": {
                        "value": ">Q7: The reviewer was confused by the discussion before introducing problem (P1). It is weired to have a problem in (P2) and the solution in Theorem 5.1.\n\nA7: **As we pointed out earlier in the total response, direct solving (P1) is hard. Therefore, we propose a transformation of (P1) as follows**: We introduce the function $L(\\alpha, \\tau) = H(\\alpha) - \\frac{1}{\\tau} (\\psi_1 - \\sum \\alpha_i \\psi_i)$ and impose the constraints $\\alpha^T 1_n = 1$ and $\\alpha \\geq 0$.\nWe observe that for any $\\tau > 0$, $L(\\alpha, \\tau) \\geq$ the objective function of (P1). This is because any \\alpha that satisfies (P1) also satisfies $\\psi_1 - \\sum \\alpha_i \\psi_i \\leq 0$. Therefore, $H(\\alpha) - \\frac{1}{\\tau} (\\psi_1 - \\sum \\alpha_i \\psi_i) \\geq H(\\alpha)$.\nHence, we can consider the following program: $\\max_\\alpha L(\\alpha, \\tau)$ subject to $\\alpha^T 1_n = 1$  and $\\alpha \\geq 0$. The solution to this problem provides an upper bound for the original problem.\nConsequently, we conclude that the objective of (P2), and (P2) can be analytically solved and give the solution in Theorem 5.1.\n\n>Q8: Moreover, there are mistakes in the formulation of (P2). It is neither the Lagrangian nor the Lagrangian dual problem. It is misleading or something is missig to claim minimizing (P2) producing an upper bound of (P1).\n\nA8: The introduction of the dual variable $\\tau$ leads to the generation of the Lagrangian function $E(\\alpha, \\tau)$. **In the original manuscript, we simplified the notation by omitting the explicit dependency of E on $\\tau$.** The problem (P2) corresponds to maximizing $E(\\alpha, \\tau)$, which represents the objective function in the Lagrangian dual problem of (P1), where \\tau serves as the dual variable.\n\n>Q9: The reviewer cannot see that \"Theorem 5.1 suggests that the loss function of that form is a natural choice for characterizing the neighborhood similarity structure\".\n\nA9: **As mentioned earlier in the total response, the value of H($\\alpha$) provides insight into the similarity between the query sample and contrastive samples, relative to the similarity between the query sample and the positive sample.** Consequently, the optimal solution of the optimization problem (P1) captures the neighborhood similarity structure. By demonstrating that the loss function in Theorem 5.1 can provide an upper bound for the problem (P1), we can explain the natural suitability of the InfoNCE loss function.\n\n>Q10: The reviewer is not clear how to have a RBF kernel from the form in Theorem 5.1.\n\nA10: Multiple approaches exist for defining the similarities between two points, and in equation (6), we specifically choose the similarity function $\\psi_i = C - \\| f(q) - f(p_i) \\|^{\\gamma}$, where C represents a large positive constant. The form presented in theorem 5.1 corresponds precisely to the exponential kernels described by equation (6). **When $\\gamma=2$, this recovers the RBF kernel, which is a special case of exponential kernel.**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314095202,
                "cdate": 1700314095202,
                "tmdate": 1700409068626,
                "mdate": 1700409068626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X6qH4HfdAw",
                "forum": "hLZQTFGToA",
                "replyto": "z4CYgMjy7n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1212/Reviewer_TB2Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1212/Reviewer_TB2Y"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer appreciates the great efforts in providing the clarification. After reading the rebuttal, and the revised manuscript, the raised major concerns or technical issues have been resolved.  Anyway, for the revised manuscript, the reviewer would like to also give a revised rating, though the related work and experiments are still not good enough."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660975349,
                "cdate": 1700660975349,
                "tmdate": 1700660975349,
                "mdate": 1700660975349,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FBwTx0WK1B",
            "forum": "hLZQTFGToA",
            "replyto": "hLZQTFGToA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1212/Reviewer_PnKu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1212/Reviewer_PnKu"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a theoretical result concerning contrastive learning. Contrastive learning is a semi-supervised task that aims to map objects into an embedding space such that similar objects are close and dissimilar objects are far apart. Their results concern the widely-used SimCLR loss, an example of an InfoNCE loss. The authors show that optimizing InfoNCE is equivalent to solving a spectral clustering problem. Based on this theoretical insight, they give an argument that exponential kernels are natural and propose a variant of InfoNCE, Kernel-InfoNCE, where they use an alternative exponential kernel in place of the usual Gaussian kernel. Doing so led them to using a Simple Sum kernel, which achieves slightly improved empirical performance on CIFAR image-text data sets. \n\nThis paper is closely related to HaoChen et al 2021; that paper proposed a type of contrastive loss that constitutes performing spectral clustering. The authors of this paper extend this by proving that SimCLR itself constitutes performing spectral clustering. This paper is also related to Van Assel et al., 2022, which analyzes dimensionality reduction methods such as t-SNE using a Markov random field framework adopted by the authors of this paper."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is an important theoretical result concerning a widely-used method. This work helps bridge the gap between theory and practice in contrastive learning. I have several comments, none of which are major."
                },
                "weaknesses": {
                    "value": "Minor comments: \n\nI found parts of the text difficult to follow because it lacks guideposts explaining the purpose of each section at a high level. \n\nIt would help to have a definition of spectral clustering for the purposes of the paper. \n\nEq1: I think this is meant to be a sum over different q's; the text says as much, but that's not how it's defined in Eq1. \n\n\"we will flip $X_i$ to get $X_j$ with probability 1/9; it's not clear what \"flip\" means or why the probability is 1/9."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1212/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698867764607,
            "cdate": 1698867764607,
            "tmdate": 1699636047556,
            "mdate": 1699636047556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GXVlbDgbIf",
                "forum": "hLZQTFGToA",
                "replyto": "FBwTx0WK1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1212/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer PnKu"
                    },
                    "comment": {
                        "value": "> Q1: I found parts of the text difficult to follow because it lacks guideposts explaining the purpose of each section at a high level.\n\nA1: Thank you for your suggestion. We have added a brief summary of the purposes of each section at the beginning of each section in the revised manuscript. \n\n>Q2: It would help to have a definition of spectral clustering for the purposes of the paper.\n\nA2: Thank you for your comment. We have added the definition of spectral clustering as definition 2.7 in the revised version of our manuscript.\n\n>Q3: Eq1: I think this is meant to be a sum over different q's; the text says as much, but that's not how it's defined in Eq1.\n\nA3: We are sorry for any confusion here, what we mean is that the total loss is a summation of query samples (i.e. over q's).\n\n>Q4: \"we will flip $X_i$ to get $X_j$ with probability 1/9; it's not clear what \"flip\" means or why the probability is 1/9.\n\nA4: This sentence is just an example to intuitively understand what $\\pi$ consists of. It does not mean the actual number is 1/9. The actual meaning of this sentence is that \"suppose $X_j$ has a probability, say 1/9, to be an augmentation of an object $X_i$\"."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1212/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305953427,
                "cdate": 1700305953427,
                "tmdate": 1700306002380,
                "mdate": 1700306002380,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]