[
    {
        "title": "Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification"
    },
    {
        "review": {
            "id": "sqWRQUhUZ7",
            "forum": "g6rZtxaXRm",
            "replyto": "g6rZtxaXRm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3107/Reviewer_LDou"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3107/Reviewer_LDou"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to extend the class descriptions to improve the performance of CLIP-style vision-language models (VLMs) in image classification. It points out that the class descriptions generated by existing methods may not provide useful information that differentiates the class from other classes. \n\nTo address this problem, this paper proposes the Follow-up Differential Descriptions (FuDD). FuDD first identifies a set of ambiguous classes using the basic class descriptions. Then, for each ambiguous class, a LLM is prompted to generate descriptions, which involve information that can distinguish it from other ambiguous classes. The generated class descriptions are then used to obtain the final classification results.\n\nThe empirical studies show that FuDD outperforms the basic class descriptions and na\u00efve LLM generated descriptions that do not consider differential attributes between classes. The experiments include two kinds of LLMs, i.e., the GPT-3.5 (accessed via API) and the open-sourced LLAMa2. It is shown that LLAMa2-based FuDD can achieve effective results after being fine-tuned on the descriptions generated by GPT-3.5."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The idea of generating differential class descriptions is well-motivated and reasonable.\n* The experiments are well-designed and the results are convincing, which demonstrates the effectiveness of using differential class descriptions.\n* The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "* The experiments only include one type of VLM, i.e., CLIP. The reviewer would like to know whether FuDD is compatible with more advanced VLMs.\n* There is no analysis on the trade-off between performance gain by FuDD and additional (computational or financial) cost. Since generating pair-wise class description is more expensive than generating independent description for each class, such analysis is important."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3107/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3107/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3107/Reviewer_LDou"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3107/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698028899050,
            "cdate": 1698028899050,
            "tmdate": 1699636257054,
            "mdate": 1699636257054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qzZP3If12c",
                "forum": "g6rZtxaXRm",
                "replyto": "sqWRQUhUZ7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3107/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer LDou"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments on our manuscript. Below we address your comments and questions about our work.\n\n> The experiments only include one type of VLM, i.e., CLIP. The reviewer would like to know whether FuDD is compatible with more advanced VLMs.\n\nThank you for your suggestion. To further evaluate FuDD, we run additional experiments with OpenCLIP, which is much stronger than CLIP, and BLIP2, which uses an entirely different training strategy. We observe similar performance improvement in this new set of experiments, which further supports the argument that FuDD benefits a wide range of VLMs.  \nPlease see our general comment to all reviewers for the results and further discussion.\n\n\n> There is no analysis on the trade-off between performance gain by FuDD and additional (computational or financial) cost. Since generating pair-wise class description is more expensive than generating independent description for each class, such analysis is important.\n\nThe number of LLM queries directly depends on the number of ambiguous classes, which is a hyperparameter set by the user. In Figure 3, we report the performance gains with different numbers of ambiguous classes (i.e., different values for k). As discussed in Section 4.1, the five most ambiguous classes account for most of FuDD\u2019s performance gains. As a result, to further decrease the costs, one can use smaller values for k, while preserving most of FuDD\u2019s benefits.  \nPlease also see our general comment to all reviewers for exact API costs and further discussion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512608393,
                "cdate": 1700512608393,
                "tmdate": 1700512608393,
                "mdate": 1700512608393,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x9BimYiLvz",
                "forum": "g6rZtxaXRm",
                "replyto": "qzZP3If12c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3107/Reviewer_LDou"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3107/Reviewer_LDou"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for taking the time to respond to my comments. It is nice to see that (1) FuDD can also boost the performance of OpenCLIP and BLIP2 and (2) the information on API costs, which enhances the soundness of the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667002247,
                "cdate": 1700667002247,
                "tmdate": 1700667002247,
                "mdate": 1700667002247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7xhABard4y",
            "forum": "g6rZtxaXRm",
            "replyto": "g6rZtxaXRm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3107/Reviewer_Y5Zb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3107/Reviewer_Y5Zb"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the prompt engineering. Instead of directly asking LLM or designing a set of prompts, the author proposed to let the model decides which sets of categories are ambiguous and then ask the LLM to clarify those sets of categories. This is actually a quite interesting and smart idea. Comparing with the other prompt engineering approach, the proposed one achieves higher performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed idea makes a lot of sense and achieves quite impressive performance.\n\n2. The paper showed that by finetuning the open-sourced model, the open-sourced model (LLAMA2) could also generate the descriptions with discriminative capability across ambiguous categories.\n\n3. The proposed approach achieves much better performance than simple prompt engineering.\n\n4. If this is the first paper presented in this field, I would rate a strong acceptance. This is a little bit out of my domain, I would rate acceptance."
                },
                "weaknesses": {
                    "value": "I think this paper is interesting and the proposed approaches make sense. I would not be able to state the weaknesses."
                },
                "questions": {
                    "value": "1. The section 5: I wonder why would the Llama2 can generate informative sentences for satellite images? Especially given the model only been finetuned on the ImageNet dataset sentences.\n\n2. What if the caption is so long and so complex that the image-text model could not understand? I wonder if the author encountered this scenario, if so, how to address it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3107/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803449387,
            "cdate": 1698803449387,
            "tmdate": 1699636256972,
            "mdate": 1699636256972,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tH30r5wqsL",
                "forum": "g6rZtxaXRm",
                "replyto": "7xhABard4y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3107/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Y5Zb"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments on our manuscript. Below we address your comments and questions about our work.\n\n> If this is the first paper presented in this field, I would rate a strong acceptance.\n\nOur work is the first to propose a **zero-shot**, **training-free** approach for generating class descriptions that better **differentiate** between the target classes, as well as providing analysis of what the characteristics of such descriptions are. Previous zero-shot approaches dismiss the commonalities between classes, and the closest work that aims for differentiating descriptions is LaBo[1] which uses training data and further optimization to select a subset of naive class descriptions that are more differentiating.\n\n> The section 5: I wonder why would the Llama2 can generate informative sentences for satellite images? Especially given the model only been finetuned on the ImageNet dataset sentences.\n\nWe believe that during fine-tuning, the LLM learns what types of features are useful for differentiating the target classes, while the world knowledge comes from the pre-training stage. As shown in Figure 4 and Figure 5, after fine-tuning, the LLM is describing more low-level visual features instead of higher-level semantic concepts. As another piece of evidence in support of this argument, in Table 3, we observe that the correctness of the descriptions increases moderately (i.e., fine-tuning has limited impact on LLM\u2019s world knowledge), while their helpfulness (i.e., being differentiating) increases significantly, which we attribute the increased performance to.\n\n> What if the caption is so long and so complex that the image-text model could not understand? I wonder if the author encountered this scenario, if so, how to address it?\n\nThat was exactly the case in our early experiments, where we got long, complex descriptions, sometimes expanding an entire paragraph. To alleviate this problem, we take advantage of the in-context learning capabilities of LLMs, and use two examples as part of our LLM prompt to guide the model to focus on concise visual features and follow the description style that is known to work well for VLMs, i.e., short photo captions that start with prefixes like \"a photo of a.\"\n\nInspecting the generated descriptions across all datasets, we find that, on average, each description contains 14 words and 99 percent of the descriptions start with simple prefixes like:  \n\"a photo of\"  \n\"a photograph of\"  \n\"a video of\"  \n\"a close-up of\"  \n\"a screenshot of\"  \n\"a close-up photo of\"  \n\"a microscopic image of\"  \n\nExcluding the prefix and the class name, there are approximately 10 remaining words in each description, which does not allow for much complexity in descriptions.\n\nMoreover, to keep the descriptions simple but at the same time benefit from a diverse set of described attributes, we guide the model in both the task instruction and the in-context examples to generate multiple descriptions, and then we average over the generated descriptions to compute the class representations. On average, over all datasets, we use approximately 4 descriptions per class. \n\n[1] Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-Burch, C., & Yatskar, M. (2023). Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. In CVPR 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512477256,
                "cdate": 1700512477256,
                "tmdate": 1700512477256,
                "mdate": 1700512477256,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kf2gPgr0Ey",
            "forum": "g6rZtxaXRm",
            "replyto": "g6rZtxaXRm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3107/Reviewer_sQgp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3107/Reviewer_sQgp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a simple data augmentation technique for zero-shot image classification tasks. Specifically, the idea is to utilize large language models (LLM) to generate more descriptive and informative class labels for ambiguous classes, with the hope that they can help the base image-text retrieval/classification model to classify the image. Experimental results suggest promising gains over baseline, in this case a CLIP model without this augmentation technique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is intuitive. It is expected that using more descriptive class labels can improve zero-shot tasks.\n2. Experimental results show consistent gains across different datasets."
                },
                "weaknesses": {
                    "value": "1. The method is incremental, as it is a small trick to boost zero-shot image classification results.\n2. The baseline is not strong enough. It is therefore not clear if the method can still improve state-of-the-art models on these tasks.\n3. More diverse and challenging tasks are missing from the experiment, e.g. image-text retrieval. It will be nice to show wider applicability of the method."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3107/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814676938,
            "cdate": 1698814676938,
            "tmdate": 1699636256883,
            "mdate": 1699636256883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Htk0gbOWR",
                "forum": "g6rZtxaXRm",
                "replyto": "Kf2gPgr0Ey",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3107/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer sQgp"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments on our manuscript. Below we address your comments and questions about our work.\n\n> The method is incremental, as it is a small trick to boost zero-shot image classification results.\n\nWe believe that our work contributes significantly to the field, especially as the need for efficient approaches to adapting VLMs to downstream tasks increases. We propose technical contributions that outperform previous methods, provide analysis of characteristics of effective natural language descriptions, study the utility of open-source LLMs and their fine-tuning,  and contribute to answering open questions in the field on the potential of natural language class descriptions for VLM adaptation compared to other sources of information (e.g., synthetic images) or alternative approaches like soft prompt tuning.\n\nPlease see our general comment to all reviewers for an extended discussion on our contributions.\n\n> The baseline is not strong enough. It is therefore not clear if the method can still improve state-of-the-art models on these tasks.\n\nTo address the reviewer\u2019s concern, we run additional experiments on two OpenCLIP model variants, which are stronger than their CLIP counterpart, as well as the BLIP2 model, which uses a different style of pre-training than CLIP. In both cases, we observe similar improvements, where the additional information provided by FuDD improves the performance over the naive LLM-generated descriptions. \n\nPlease see our general comment to all reviewers for additional experiments and further discussion.\n\n> More diverse and challenging tasks are missing from the experiment, e.g. image-text retrieval. It will be nice to show wider applicability of the method.\n\nThere are many interesting vision and language tasks that could benefit from additional semantic information. However, in keeping with most of the closely related work, here, we also focus on image classification, which remains a challenging and important problem (please refer to the manuscript for an extended discussion of previous work). Focusing on image classification allows more diverse comparisons and insights in this line of work, without confounding factors like task variation. For example, the comparison of image classification results in Section 6 uncovers the potential of natural language descriptions for VLM adaptation compared to other modalities or few-shot approaches proposed by previous work.\n\nAlthough outside of the focus of our work, we would like to discuss FuDD in the context of information retrieval (IR). We draw an analogy between FuDD and reranking methods for IR, where for each query, a low-cost approach selects a subset of potentially related documents (similar to Section 3.2 in FuDD), and a more complex method reranks this smaller subset of documents (similar to Section 3.3 and 3.4 in FuDD). In the context of IR, because of the large number of documents, it is financially and computationally impossible to generate alternative descriptions for all documents regardless of choosing naive or differential descriptions. In such a setting, focusing on ambiguous documents is the only feasible option (we have extended discussion and experiments on ambiguous classes in Sections 3.2 and 4.1).\nNow the question is, \"How effective are naive descriptions for reranking compared to FuDD?.\" Constrained by the rebuttal time window, we design new experiments to simulate text retrieval with our current descriptions and datasets. In these new experiments, we use test images as equivalent for queries and classes as the documents to be retrieved in a text-retrieval setup.\nWe first retrieve the ambiguous classes as described in Section 3.2 and then use differential descriptions and naive descriptions to rerank these ambiguous classes and compare the results. We report the accuracy and nDCG@10, which represents the improvement in the ranking of the documents (i.e., class names). Compared to classification, it shows how much closer the true class label is to the top of the list, even if it is not the top prediction.\n\n\n|-|Flowers102|Flowers102|Places365|Places365|\n|---|---|---|---|---|\n|-|Acc|nDCG@10|ACC|nDCG@10|\n|Naive|75.98|85.21|41.58|59.37|\n|FuDD|78.76|86.35|43.95|60.89|\n\n\nAs expected, we observe that better representations for each class coming from differential descriptions actually improve the performance in this case, where both naive and differential descriptions are used for reranking."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513395424,
                "cdate": 1700513395424,
                "tmdate": 1700513395424,
                "mdate": 1700513395424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "94kc6k8DdA",
            "forum": "g6rZtxaXRm",
            "replyto": "g6rZtxaXRm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3107/Reviewer_RXgc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3107/Reviewer_RXgc"
            ],
            "content": {
                "summary": {
                    "value": "Recent work shows that evaluating vision-language models with descriptions on object attributes improves image classification performance. This work proposes to prompt LLMs to specifically give \u201cdifferential\u201d descriptions (i.e., distinctive attributes between ambiguous classes) and using such more targeted descriptions outperforms generic descriptions. The method shows performance improvement over the baselines on a range of image classification benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea is simple and natural. Directly prompting LLM to give attributes for a certain class is an underspecified task; there are so many attributes of different granularities to enumerate. Generating differential descriptions given two classes is a smart way to generate targeted attributes."
                },
                "weaknesses": {
                    "value": "**1. No significant method improvement**\n\nThe idea of prompting LLM to give more differentiating descriptions is effective but not necessarily a significant innovation. \n\n\n**2. Reliance on pre-trained vision-language models**\n\nThe paper focuses only on getting differential descriptions; however, it is not guaranteed that CLIP can accurately interpret them. The effectiveness of such models hinges critically on and is bounded by the vision-language models. I suspect that this could be why the performance improvement is not very significant and why different CLIP variants could exhibit huge performance variations.\n\nI would appreciate an error analysis: how many of the mistakes are due to insufficient / erroneous differential descriptions and how many of the mistakes are due to CLIP\u2019s insensitivity to descriptions? This could be informative for future work.\n\n\n\n**3. API cost for a single inference on an image**\n\nIt is not immediately clear how many times we need to query LLM for a single inference on one image. For k=10, does it mean we need to create 55 queries? For experiments where k = |C|, will the cost become prohibitive (even with caching)?\n\nIt would be better if such details are discussed in the main text."
                },
                "questions": {
                    "value": "- For the equation in 3.2, what does the summation mean? I guess the equation simply means getting the top-k classes with the highest similarity with the image features? Then in this case, do we need to sum over c_i?\n\n- Would it be possible to give examples of differential descriptions for each of the evaluation datasets? It is especially interesting to see the huge performance on EuroSAT.\n\n- Would it be possible to prompt the LLM to come up with differential attributes given >2 classes in a single prompt?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3107/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829346117,
            "cdate": 1698829346117,
            "tmdate": 1699636256810,
            "mdate": 1699636256810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B3yCe9akN0",
                "forum": "g6rZtxaXRm",
                "replyto": "94kc6k8DdA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3107/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3107/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer RXgc"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments on our manuscript. Below we address your comments and questions about our work.\n\n> No significant method improvement\n\nWe respectfully disagree with the reviewer as our work makes technical contributions, achieves superior results, provides analysis on characteristics of effective natural language descriptions, studies the use of off-the-shelf and fine-tuned LLMs in the process, and positions natural language descriptions as a promising approach for adapting VLMs. This is especially important for future work as it advances the debate over promising approaches for VLM adaptation in favor of natural language descriptions compared to other sources of information like synthetic images or alternative approaches like soft prompt tuning.  \nPlease find our extended response in the general comment to all reviewers for further discussion.\n\n> The paper focuses only on getting differential descriptions; however, it is not guaranteed that CLIP can accurately interpret them. The effectiveness of such models hinges critically on and is bounded by the vision-language models.\n\nHere, we focus on generating class descriptions that provide the most useful information for the target set of classes and how we can use LLMs to do so effectively and efficiently. Of course, different VLMs use this information differently based on their capabilities and pre-training datasets. However, as shown in our experiments, VLMs are capable of taking advantage of this information to improve performance.\nPlease see our general comment to all reviewers for experiments with additional VLMs, where we observe similar improvements.\nClearly, VLMs are able to take advantage of this information. However, we believe an additional attempt to improve VLMs\u2019 understanding of natural language is a different question and is out of the scope of this work.\n\n> [...] I suspect that this could be why the performance improvement is not very significant.\n\nRespectfully, considering the challenging nature of the task, we believe our method makes significant improvements in most cases (e.g., 13.95 and 5.58 percentage points for EuroSAT and Oxford Pets). Especially, considering that without using any labeled data, FuDD\u2019s performance as a zero-shot method is comparable to other few-shot learning methods that use labeled images for adapting VLMs.\n\n> [...] how many of the mistakes are due to insufficient / erroneous differential descriptions and how many of the mistakes are due to CLIP\u2019s insensitivity to descriptions?\n\nAs an attempt to understand how the correctness and helpfulness (i.e., being differentiating) of descriptions impact the performance, we perform a manual evaluation as reported in Table 3, and observe that improving the helpfulness has a significant impact on performance even if the level of correctness of descriptions stays roughly the same. Another related experiment is described in Section 4.2, where we show that differentiating attributes are superior to generic attributes and that VLMs are sensitive to the described differences and can use them to their advantage.\n\nAs explained in Section 3.3, we use an ensemble of descriptions for each class, and often the ensemble is a mixture of helpful and correct and unhelpful or incorrect descriptions. As a result, it is challenging, if at all possible, to decide if the wrong prediction is caused by incorrect or unhelpful information or by the model\u2019s inability to understand the correct and helpful descriptions. Therefore, this hinders such a precise analysis of the cause of the error for each image as suggested by the reviewer.\n\n> API cost for a single inference on an image\n\nPlease, find our response in the general comment to all reviewers.\n\n> For the equation in 3.2, what does the summation mean? [...]\n\nAs mentioned by the reviewer, this equation selects the k-most similar classes to each image. In other words, the argmax is over sets of classes, and to score a set, we have to sum the similarities of all classes in each set.\n\n> Would it be possible to give examples of differential descriptions for each of the evaluation datasets?\n\nThank you for your suggestion. We include examples of differential descriptions for all datasets in the updated manuscript. It is interesting to see that differential descriptions are more effective in cases that describe low-level visual characteristics than high-level semantic features, which is consistent with our previous observations described in Section 5.\n\n> Would it be possible to prompt the LLM to come up with differential attributes given >2 classes in a single prompt?\n\nWe think it is really interesting to investigate these different approaches, and we do not see an immediate reason that it does not work. Since this is the first work on generating differentiating, dataset-specific class descriptions, we focused on two classes per query. But, this is an interesting direction for future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3107/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512186829,
                "cdate": 1700512186829,
                "tmdate": 1700514195680,
                "mdate": 1700514195680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]