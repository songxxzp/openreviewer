[
    {
        "title": "ODEFormer: Symbolic Regression of Dynamical Systems with Transformers"
    },
    {
        "review": {
            "id": "ulp7r9iBT3",
            "forum": "TzoHLiGVMo",
            "replyto": "TzoHLiGVMo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2511/Reviewer_hEs1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2511/Reviewer_hEs1"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a model for discovering the governing law of dynamical systems out of observed trajectory. The model treats trajectories as sequences of tokenized numbers and utilizes a transformer network to learn the symbolic form of governing laws from them. The authors conduct the experiment on 1D to 4D systems, and the proposed method outperforms the baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The paper is well motivated, as discovering the symbolic form of governing laws from observed data has always been the focus of scientific research.\n\n(2) The paper contributes to neural network models for symbolic regression of ordinary differential equations."
                },
                "weaknesses": {
                    "value": "(1)\tThe idea in this paper is not novel enough. The idea of transformer-based symbolic regression on tokenized sequences was previously reported[1]. The proposed model shares the same network structure but is applied to ordinary differential equation data. However, no analysis could be found in the paper on how the network is adapted to this new type of data.\n\n(2)\tFigures and explanations of the proposed model are way too rough and lack the necessary details.\n\n(3)\tThis paper lacks crucial detail on dataset separation and the definition of tasks in the experiment section.\n\nReference:\n\n[1] d'Ascoli S, Kamienny P A, Lample G, et al. Deep symbolic regression for recurrent sequences[J]. arXiv preprint arXiv:2201.04600, 2022."
                },
                "questions": {
                    "value": "(1)\tCould the authors give a detailed description and specific form of the loss function used during training? Is it symbolic or numerical? If the loss function is symbolic, does it measure the similarity between the symbolic tree structure or simply between the sequences? And as the ground-truth equations can be organized in different orders under both forms, which one of them is chosen in practice, and what are the reasons for refusing other possible sequences/trees?\n\n(2)\tAs the authors claimed in section 2, most existing approaches of symbolic regression require a separate optimization for each new observed system. However, it is unclear in the paper whether or not the proposed model needs separate optimization on unseen systems as previous models do. Could you add some analysis on this problem and provide reasons and results to support your arguments?\n\n(3)\tThe authors conduct subsampling in the experiment part and compare model performances under different subsampling rates. Could you explain further what condition in practice the subsampling process corresponds to? And how does the proposed model deal with the resulting irregular time-interval as most sequence transformers are built to encode trajectories with regular time intervals?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2511/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2511/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2511/Reviewer_hEs1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2511/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630308996,
            "cdate": 1698630308996,
            "tmdate": 1700725339345,
            "mdate": 1700725339345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VxiR2rfeXU",
                "forum": "TzoHLiGVMo",
                "replyto": "ulp7r9iBT3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2511/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2511/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review of our submission! We will respond to the points raised by you one by one below:\n\n### Weaknesses\n\n1) **Novelty**\n\nThank you for bringing this up. There are two key differences between our proposed model and the model by D\u2019Ascoli et al..\nThe model by D'Ascoli et al.'s model is designed for recurrent sequence prediction, focusing entirely on inherently discrete sequences. An example, taken from Table 1 of D\u2019Ascoli et al., is to find the underlying principle of the following (Josephus) sequence \u201c0,1,1,3,1,3,5,7,1,3\u201c, which is given by \u201cu_n = (u_n\u22121 +n)%(n\u22121)\u22121\u201d. In contrast, ODEFormer is developed to predict ordinary differential equations (ODEs), which represents a continuous function. Although input sequences for both models are discrete, ODEFormer assumes this discreteness to arise from sampling a continuous process at discrete measurement times.\n\nFurthermore, the model by D\u2019Ascoli only considers univariate input sequences whereas our model allows for multivariate observations, i.e., it predicts the dynamic interactions between different system components. This important difference makes our model applicable to many more modeling scenarios and also manifests itself in the model architecture, in particular in the embedding layer which in our case needs to combine multiple trajectories. We have improved the description of this aspect of our model in Section 4 in order to clarify this aspect.\n\nIn the revised manuscript we have added a new section in Appendix H which emphasizes the differences between our approach and the model by D\u2019Ascoli et al..\n\n2) **Figure and description need more details**\n\nThank you for this feedback! In the revised manuscript we have \n- **Modified the overview figure** (Figure 2) to include more details of the proposed approach,\n- **improved the description** of the model in Section 4, and extended the section on architecture details and training hyper-parameters in Appendix C.\nWe believe these adjustments indeed make the paper easier to follow and we are thus grateful for this comment. Should there still be sections that are too rough or lacking details, we would kindly ask the reviewer to point us more concretely towards them so that we can improve these.\n\n\n3) **Experimental task and dataset separation**\n\nThank you for this feedback! To clarify: the datasets used for the evaluation were not used for training. Indeed, this was not explicitly stated in the original submission - **we have hence updated the experimental section (Section 5)** to make this more clear. The revised manuscript also includes a concise statement of the experimental tasks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507968230,
                "cdate": 1700507968230,
                "tmdate": 1700507968230,
                "mdate": 1700507968230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zZt6u0zOXM",
                "forum": "TzoHLiGVMo",
                "replyto": "VxiR2rfeXU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2511/Reviewer_hEs1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2511/Reviewer_hEs1"
                ],
                "content": {
                    "title": {
                        "value": "reply to response"
                    },
                    "comment": {
                        "value": "-**On the novelty of the proposed model.** I can now understand the difference of the task between your work and the previous paper. However, it would seem to me that despite different tasks, the   previously reported model needs no adjustment to apply on the new task, particularly with the very similar method proposed by the authors. Could you explain further the difference between the previous model and yours from the methodology perspective and how exactly does the difference between sequences and ODEs affect the design of your model?\n\n-**The adjustments in figures, descriptions and experimental details.** Thank you for the update and for taking my suggestions into account.\n\n-**Computation of the loss function.** Thank you for the detailed explanation. I find your argument about the ambiguity of symbolic sequences quite convincing.\n\n-**The optimization.** Thank you for the adjustment. It would make the paper friendlier to the readers.\n\n-**The subsampling issue.** It is quite clear in the explanation the corresponding real-world scenarios. As you have pointed out, the irregular sampling grids would definitely harm those methods using finite difference estimates. However, it is unclear about the reason of the proposed model outperforming those methods without finite difference. Could you mark in the baselines those with finite difference estimates and those without, and add some analysis on the advantages of your model against other methods free of finite difference? Meanwhile, I have just noticed that the baseline models lack some crucial methods proposed recently such as [2].\n\n**Overall** The paper still needs more analysis on the method itself about its distinction over the previous model and the reason of its performance. Some crucial baseline models proposed recently should be added as well. Nevertheless, the paper is now clear in representation and contains detailed analysis on some vital problems. I would increase my score if proper analysis from the methodology perspective can be added as well as more recent baselines.\n\nReference:\n[2] Qian, Zhaozhi, Krzysztof Kacprzyk, and Mihaela van der Schaar. \"D-code: Discovering closed-form odes from observed trajectories.\" International Conference on Learning Representations. 2022."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616110465,
                "cdate": 1700616110465,
                "tmdate": 1700616110465,
                "mdate": 1700616110465,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tTizKQW6p8",
                "forum": "TzoHLiGVMo",
                "replyto": "BHR6f5LJag",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2511/Reviewer_hEs1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2511/Reviewer_hEs1"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed reply and for taking my suggestions into account. \n\nI now understand that the novelty of your proposed model mainly lies in discovering the ODE invariant of the time grid rather than a single sequence. As for the baselines, since the authors have contained some recent methods while the remaining ones lack the necessary documents for quick repetition, I agree that the current baseline models are sufficient for comparison.\n\nBased on the changes have made and your replies, I will update my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725276088,
                "cdate": 1700725276088,
                "tmdate": 1700725276088,
                "mdate": 1700725276088,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MQJJiHbUKc",
            "forum": "TzoHLiGVMo",
            "replyto": "TzoHLiGVMo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2511/Reviewer_znUh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2511/Reviewer_znUh"
            ],
            "content": {
                "summary": {
                    "value": "ODEFormer, a transformer model designed for dynamical symbolic regression. This model is capable of inferring multidimensional ordinary differential equation (ODE) systems from noisy and irregularly sampled data. It utilizes a pre-trained sequence-to-sequence transformer on synthetic data to generate symbolic expressions directly from observations. The paper also introduces ODEBench, a benchmark dataset for dynamical symbolic regression, comprising 63 ODEs sourced from the literature, modeling real-world phenomena across dimensions one to four, including chaotic systems.\n\nThe reviewer further highlights the evaluation and comparison of ODEFormer with existing methods. It assesses ODEFormer's performance on both the established Strogatz dataset and the new ODEBench dataset. The comparison encompasses various techniques based on genetic programming, regression, and Monte Carlo methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The introduction of ODEFormer represents a novel framework that serves the purpose of generating ordinary differential equations (ODEs) specifically designed for testing dynamical systems. This innovative approach allows researchers and practitioners to create ODE models that can accurately capture and represent the dynamics of real-world systems, providing a valuable tool for testing and understanding the behavior of complex dynamical systems.\n\nODEFormer introduces a pioneering use case for transformers in the realm of ODEs. Traditionally, transformers are employed in natural language processing and sequential data tasks. However, in this context, they are harnessed to directly infer ODE systems from noisy and irregularly sampled data. This expansion of transformer applications into the domain of ODEs signifies a breakthrough, offering a versatile and data-driven approach for modeling and analyzing complex dynamic systems, and opening up new possibilities for the fusion of machine learning techniques with physics-based modeling."
                },
                "weaknesses": {
                    "value": "One notable limitation in the presented work is the absence of comparisons with benchmarks employed in previous studies, such as the widely recognized benchmark datasets used in Neural ODE (Chen et al). This absence makes it challenging to gauge how the proposed ODEFormer framework performs in comparison to existing approaches on well-established and widely accepted testing scenarios.\n\nThe demonstrated applicability of ODEFormer on toy datasets represents another potential limitation. Toy datasets are typically simplistic and may not fully capture the complexity and variability encountered in real-world applications. Therefore, the extent to which ODEFormer can effectively handle and model more complex, real-world data remains an open question and warrants further investigation and validation on diverse and challenging datasets."
                },
                "questions": {
                    "value": "One key aspect that warrants further exploration is the practical application of the technique on real-world datasets. While the framework shows promise in artificially constructed datasets, its effectiveness in solving real-world problems, where data can be noisy, irregularly sampled, and complex, remains to be demonstrated. Evaluating its performance on non-artificial, real-world datasets across various domains, such as finance, healthcare, or environmental monitoring, would provide valuable insights into its applicability and limitations in practical scenarios.\n\nTime series data indeed represents a compelling use case for the framework. Time series forecasting is a critical application in various fields, including finance, energy, and climate modeling. Therefore, it is crucial to investigate the applicability of this method in a time series forecasting modeling scenario. Demonstrating its effectiveness in accurately modeling and predicting time-dependent data can significantly enhance its practical utility and establish its relevance in solving real-world, dynamic data challenges."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2511/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801665861,
            "cdate": 1698801665861,
            "tmdate": 1699636187336,
            "mdate": 1699636187336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kMrTH58u6Q",
                "forum": "TzoHLiGVMo",
                "replyto": "MQJJiHbUKc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2511/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2511/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time. \n\nThere are several aspects of the review on which we respectfully disagree with:\n\n**1) The claim that we did not compare our method with existing approaches on established datasets:** \n\nIn fact, the bulk of the paper consists of such comparisons (to relevant baselines such as Sindy, etc., as well as on established datasets such as the \u2018Strogatz\u2019 dataset which has been used in numerous previous works, e.g.  [1, 2, 3, 4]). Oddly, this aspect of our submission is also emphasized in your summary:\n\u201cThe reviewer further highlights the evaluation and comparison of ODEFormer with existing methods. It assesses ODEFormer's performance on both the established Strogatz dataset [...]\u201d\n\nReferences:\n\n- [1]: Nina Omejc et al. 2023, \u201cProbabilistic grammars for modeling dynamical systems from coarse, noisy, and partial data\u201d\n- [2]: Kronenberger et al. 2021, \u201cIdentification of Dynamical Systems using Symbolic Regression\u201d\n- [3]: La Cava et al. 2021, \u201cContemporary Symbolic Regression Methods and their Relative Performance\u201d\n- [4]: La Cava et al. 2016, \u201cInference of Compact Nonlinear Dynamic Models by Epigenetic Local Search\u201d\n\n\n**2) The claim that we only consider toy datasets without noise and irregular sampling:**\n\nA major advantage of our approach, emphasized throughout the paper, is that our method is much more robust to noise than existing methods, as demonstrated empirically. Our experiments in particular already consider noisy and irregularly sampled data.\n\n\n**3) The suggestion to include the established dataset from the Neural ODE paper:**\n\nHaving inspected the referenced Neural ODE paper again, we are unsure which benchmark dataset the reviewer is referring to as this reference simply does not offer any benchmark datasets. Moreover, as discussed in the related works section, we would like to emphasize that Neural ODEs operate in a completely different setting to ours: Neural ODEs are black box models which do not output a symbolic expression. Hence, it has higher expressivity, but is void of (symbolic) interpretability.\n\n**4)** We are also surprised that although the general tone sounds very positive as exemplified by your summary sentences below, the score is very negative. \n- \"This **innovative** approach [\u2026] providing a **valuable tool** for testing and understanding the behavior of complex dynamical systems\u201d\n- \"ODEFormer introduces a **pioneering use case** [...].\u201c\n- \"This expansion of transformer applications into the domain of ODEs **signifies a breakthrough**, offering a versatile and data-driven approach for modeling and analyzing complex dynamic systems, and **opening up new possibilities** for the fusion of machine learning techniques with physics-based modeling.\u201d\n\n\nIn particular, we would like to ask the reviewer how this summary of our submission leads to a contribution score of 1 (poor).\n\n\n**In light of these contradictions, it is hard for us to offer a sound rebuttal and a constructive revision that improves our submission.**"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558530654,
                "cdate": 1700558530654,
                "tmdate": 1700558530654,
                "mdate": 1700558530654,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HSWv9egCvL",
            "forum": "TzoHLiGVMo",
            "replyto": "TzoHLiGVMo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2511/Reviewer_hbTD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2511/Reviewer_hbTD"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of symbolic regression for dynamical systems, specifically ODE, with the use of transformers. Apart from the adjustments need to use a transformer based model for this task, authors also propose a new benchmark. This is claimed to be more diverse and larger than existing ones. Empirically, the proposed method is shown to outperform existing baseline methods in terms of reconstruction as well as generalization."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is extremely well-written (including notations, clearly stating contributions, etc.), very well motivated and the proposed method is shown to achieve state of the art performance.\n\nThe placement within existing literature is very well articulated.\n\nSince, authors propose a benchmark, it is appreciated that the data generation procedure is outlined precisely.\n\nThe section on filtering of the data to avoid rapidly converging and divergent systems is worth mentioning, details like these make a benchmark standout.\n\nTokenization, embedding process and encoding of the symbolic functions is very well justified.\n\nBaseline methods have been chosen appropriately and the empirical evidence is very convincing."
                },
                "weaknesses": {
                    "value": "The authors have mentioned a few of their limitations, which is great. \n\nWhile authors mention the presence of a very related work \"Becker, S\u00f6ren, et al. \"Predicting Ordinary Differential Equations with Transformers.\" (2023).\", I am not sure why they do not elucidate the difference between the paper mentioned and their proposed method, why is this method not used as a baseline? There is no attempt to illustrate the difference at all.\n\nI have another point which I would like to bring up with the authors regarding the generation of the data, specifically the way it is being integrated. As mentioned (and I think this is reasonable as a first step) the authors using fixed homogeneous grid to integrate, and it is claimed that number of points don't matter during inference (Figure 3), is this indeed an artifact of the way the data is generated. How do the results change when the integration procedure is altered, it would be great to have the authors comment on this issue.\n\nPlease see questions section for further."
                },
                "questions": {
                    "value": "1) Please compare and contrast the proposed method with the work \"Becker, S\u00f6ren, et al. \"Predicting Ordinary Differential Equations with Transformers.\" (2023).\". As acknowledged that this is closely related, it should be clearly articulated what are the differences. The mentioning of difference between univariate and multivariate is okay, but this needs more explanation. I strongly believe this should be a baseline for comparison in some setting.\n\n2) How should one think about inferring PDE with a variant of this framework, can we have a discussion around this as part of limitation if that is the case? \n\n3) The learning from multiple trajectories, doesn't seem to give promising performance, do authors have comments on explaining this behavior?\n\n4) For the noise corruption of the data, the authors have tried out adding Gaussian noise and dropping samples uniformly at random. Can authors comment on missing chunks of data instead. This might be of practical implication in certain cases, where the data collection mechanism (sensor) went faulty for some time before resuming activity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Authors have very aptly mentioned the limits that could arise in the very last paragraph of the paper, which is very much appreciated. I don't believe further review is needed."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2511/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2511/Reviewer_hbTD",
                        "ICLR.cc/2024/Conference/Submission2511/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2511/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808492566,
            "cdate": 1698808492566,
            "tmdate": 1700588426616,
            "mdate": 1700588426616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ziB5Gloetg",
                "forum": "TzoHLiGVMo",
                "replyto": "HSWv9egCvL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2511/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2511/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review of our submission! We will respond to the points raised by you one by one below:\n### Weaknesses\n1) **Comparison to Becker et al. 2023**\n\nThank you for raising this issue. We noted the major difference in the related works (Section 2): the model proposed by Becker et al. (2023) can only be applied to univariate ODEs, while our model can handle multivariate ODEs. However, we agree that this comparison deserves more details and **added a full section (\u201cDETAILED COMPARISON WITH NSODE (Becker et al. (2023)\u201d) in Appendix H** in the revised manuscript, describing the similarities and differences w.r.t. Becker et al. (2023).\n\nSince the model by Becker et al. (2023) can only be applied to univariate ODEs, it can not be benchmarked on the multivariate datasets used in our paper (Strogatz, ODEBench). Instead **we added an evaluation and performance comparison between ODEFormer and Becker et al. (2023)** using the original 1D datasets suggested by Becker et al. (2023). Our results show that ODEFormer shows competitive performances on those datasets as well, for details please refer to Appendix H.2 in the revised manuscript.\n\n2) **Question on robustness wrt ODE integration**\n\nThank you for bringing up this point, which reveals that our formulation was misleading: to clarify, we do not integrate on a homogeneous grid. Instead, we use the Runge-Kutta method of order 5(4) (often called \u201crk45\u201d) which is a classic adaptive step size solver. At every integration step, adaptive step size solvers choose the maximally allowed step size, which still keeps the integration error below a user-defined tolerance threshold. \nIn practice, this usually results in small steps within areas of high variation and larger steps in more \u201cflat\u201d areas, yielding an irregular sampling grid. While adapting the step size greatly increases integration speed,  the irregular sampling is often undesirable for modeling real-world data. \n\n In our particular case, the irregularities would be informative of the signal variation - an unreasonable general assumption for naturally observed data. Fortunately, most numerical solvers (in particular the rk45 implementation underlying scipy.integrate.solve_ivp, which we use) accept an additional parameter \u2018time\u2019 which allows the user to define a time grid along which one would like to obtain the numerical solution. In this case, the final solution returned by the solver corresponds to an interpolation of the adaptive step size solution, evaluated on the user-defined time grid. \n\nIn our data generation procedure (Section 3), we supply a fixed homogeneous grid of N points, where N is sampled independently for each generated sample from a discrete uniform distribution, i.e. N ~ U{50, 200}. Since a homogenous grid would yet again correspond to an unreasonable assumption for naturally observed data, we simulate irregular sampling by dropping a fraction of p time points at random (as described in Section 5 \u201cCorruptions\u201d). \nSo while the internal adaptive solver uses irregular steps, the numerical solution is an interpolation on a  homogeneous grid from which we randomly remove points to simulate imperfect real-world sampling. We believe this sampling procedure to be sound.\n\nWe have clarified this sampling procedure in the revised manuscript to avoid potential confusion.\n\nNevertheless, you bring up an interesting question regarding the robustness of ODEFormer w.r.t. the number of time points in an observation. The datasets in the additional comparison with Becker et al. (2023) (Appendix H in the revised manuscript, see our previous comment) include trajectories with 256 timepoints, which exceeds what ODEFormer has seen during training (trajectories during training include between 25 to 200 timepoints). Across all three datasets, results on trajectories with 256 time points are similar to results (frequently even exceeding those) with 128 time points, giving evidence that the robustness with respect to the number of time points observed in Figure 3 is not strictly limited to the training distribution.\n\nIn our original manuscript we write \u201c[...] ODEFormer is surprisingly insensitive to the number of points in the trajectory\u201d (section 5, \u201cResults on synthetic data\u201d). We did not intend to \u201cclaim that the number of points don't matter during inference\u201d but instead that one intuitively might expect better results when more data is available (hence the surprise). However, **we can see how our statement can lead to your interpretation and have hence rephrased this result in the revised manuscript to avoid confusion.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507169804,
                "cdate": 1700507169804,
                "tmdate": 1700507169804,
                "mdate": 1700507169804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2j6UaFrk0r",
                "forum": "TzoHLiGVMo",
                "replyto": "fyzOIkiCRf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2511/Reviewer_hbTD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2511/Reviewer_hbTD"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the detailed rebuttal. I am glad that the comments were helpful, hopefully these will get reflected in the final version. Since, my questions have been answered, I am updating my score accordingly.\n\nThank you"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588449308,
                "cdate": 1700588449308,
                "tmdate": 1700588449308,
                "mdate": 1700588449308,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]