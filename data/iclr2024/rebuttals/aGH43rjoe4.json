[
    {
        "title": "Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data"
    },
    {
        "review": {
            "id": "aLAXaPr5m8",
            "forum": "aGH43rjoe4",
            "replyto": "aGH43rjoe4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_9t6f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_9t6f"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model for multi-modal data based on a latent space using both shared and mode-specific latent subspaces and Gaussian Process priors. These GP priors are intended to capture smooth variations in the latent space, akin to Gaussian Process Factor Analysis (GPFA) for neural time series data. Another key ingredient is the use of a Fourier basis in the latent space, which allows for a natural low-frequency regularization on latent dynamics. Experiments include both simple synthetic data setups and applications to two real data sets in which neurophysiological recordings are combined with behavior."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Use of GP priors to smooth latent trajectories is a well-matched strategy for smoothing highly noisy neural data.\n- Moving to the Fourier basis is a clever idea that allows for a very natural regularization. \n- The idea of shared and specific latent subspaces has been done before but adds to the interpretability of the model.\n- Several tricks are combined here that render the notoriously inefficient GP inference to be performed efficiently.\n- Good set of experiments on real neural data."
                },
                "weaknesses": {
                    "value": "- The synthetic experiments tend to focus on smooth, continuous variation in \"incidental\" properties (e.g., size and angle of a digit) that may not be a good approximation in many real data sets.\n- The GP model may not work well in cases where data do not have a natural trial structure or produce very long behavioral bouts."
                },
                "questions": {
                    "value": "- As noted above, the GP method would seem to need data to be broken into snippets of reasonable size. For distinct trials, this is pretty obvious, but what if the time series are very long (e.g., natural behavior)? Does it work to simply snip the behavior at random? Does the model handle this? Do all snippets need to be the same size?\n- Is it possible to put some shrinkage priors on the $W$ matrices in (4) so that unneeded dimensions are pruned away? Can the model distinguish between \"true\" and \"false\" shared latents? Is there some notion of parsimony here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Reviewer_9t6f"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688032622,
            "cdate": 1698688032622,
            "tmdate": 1699636413786,
            "mdate": 1699636413786,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B0TtqOZoa9",
                "forum": "aGH43rjoe4",
                "replyto": "aLAXaPr5m8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive appraisal of our work. Below we address each of the questions and points raised by the reviewer.\n\n### Weaknesses:\n> The synthetic experiments tend to focus on smooth, continuous variation [...]\n\nOne of the main assumptions that we make with our model is that we assume these multimodal measurements have an underlying low-d smooth structure. Therefore, we agree with the reviewer that MM-GPVAE might not be suitable for multi-modal data that does not smoothly evolve in time (indeed, this is a limitation of GPVAE and GPFA models as well). We talk about limitations of our prior assumptions in the conclusion section of our manuscript to compare other approaches and limitations of other prior approaches. However, comparing multi-modal neuroscience models with different prior assumptions remains an important avenue for exploration.\n\n### Questions:\n> As noted above, the GP method would seem to need data to be broken into snippets of reasonable size. [...]\n\nThat is correct. For our evaluations on real datasets, we need to artificially split our data into trials of equal length, which we describe in more detail in the appendix, page 8 and 9.\n\nWe find that using the Fourier parameterization aids with this process, and the model is able to handle longer-time series trials if processed in the fourier domain (likely due to the lack of the need to invert the prior GP covariance matrix, and the overall reduction in the number of variational parameters)\n> Is it possible to put some shrinkage priors on the  matrices in (4) [...]\n\nThe introduction of shrinkage priors is a good idea, and an avenue we are pursuing for follow-up work. For now, we avoid unneeded dimensions through our process of cross-validation. As we indicated in our other responses, we have added an additional section in the appendix devoted to explaining this process (see \"Latent dimensionality selection\" on page 9 in appendix). Briefly, we assure we have no more dimensions than needed by first assessing the dimensionality per modality, and then increasing the number of shared dimensions one by one while evaluating cross-validated performance. In all tested datasets, using at least 1 shared dimension performed better than 0 shared dimensions, suggesting some cross-modality structure. We increased the number of shared dimensions sequentially until we no longer saw an increase in cross-validated performance, and used that to identify the latent dimensionality. This process, however, is somewhat intensive and a shrinkage prior would be very helpful in speeding up this procedure. \n\nWe again thank the reviewer for their thoughtful comments and suggestions, and for their time reviewing the paper. We hope that the revisions and responses in our updated paper have sufficiently addressed all of the concerns and questions raised by the reviewer. If there are still any questions or concerns, we would be happy to clarify them and engage in further discussion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365972028,
                "cdate": 1700365972028,
                "tmdate": 1700365972028,
                "mdate": 1700365972028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i07q0QkQta",
                "forum": "aGH43rjoe4",
                "replyto": "B0TtqOZoa9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Reviewer_9t6f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Reviewer_9t6f"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' thorough responses to all reviewers. I am convinced this is a valuable contribution to both the neuroscientific data analysis and multimodal VAE literatures and will be maintaining my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514750246,
                "cdate": 1700514750246,
                "tmdate": 1700514750246,
                "mdate": 1700514750246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kz2CHhKk0f",
            "forum": "aGH43rjoe4",
            "replyto": "aGH43rjoe4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_y53H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_y53H"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a variational auto encoder framework to jointly model neural activities and behavior data. Their contribution is to add Gaussian Process Prior and parameterize the latent in Fourier domain.  Their goal is to distinguish the separate or joint latents from multiple modalities, and assume a linear combination of those latents. The work is evaluated on one simulated dataset (MNIST + spike train) and two real-world datasets from different animal species (drosophila, Manduca)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The work is well-motivated,  jointly train latent variable models from multi-modal neural data is an important question.\n2. The contribution to use Gaussian Process prior to capture correlations in latent space, and parameterize the latent in Fourier space are novel contributions.\n3. The work is demonstrated on multiple datasets to improve its soundness, including one synthetic dataset and two real-world datasets from different animal species ."
                },
                "weaknesses": {
                    "value": "Methods:\n1. The assumption of linearity between latents in eqn (4) seems over-simplified, other nonlinearity variants could be further investigated. \n2. The construction of simulated dataset is confusing, it is not clear to see how the rotation angle of MNIST image is inherently related to spiking distribution. And it is puzzling that how it is tightly linked with real-world datasets. \n3. The paper proposed parameterize the latents in Fourier space to constrain them more distinguishable, and attempt to prune the high-frequency components. While it didn't provide solid ablation studies and compared to the original temporal space on real datasets, and it is problematic to claim no information preserved in high-frequency domains. \n4. The authors didn't provide clear explanation or guidance about important hyperparameters of latent dimensions. \n\nEvaluations:\n1. No quantitative measurements on real datasets only with a few qualitative samples, and not compared with other baselines.\n2. In Fig 4(d), different behavior states are not clearly separable in these subspaces.\n3. Blurriness and mismatch of reconstruction outputs and latents in the results.\n\nWriting:\n1. The paper is not well organized, missing quantitative evaluations and comparisons with baselines, a few important contents are pointed to supplementary, while it is not clearly described there either."
                },
                "questions": {
                    "value": "1. Is nonlinearity needed in eqn (4)?\n2. How to decide dimensions for separate and joint latent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Reviewer_y53H"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729047057,
            "cdate": 1698729047057,
            "tmdate": 1699636413704,
            "mdate": 1699636413704,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jD2Yn6VgwG",
                "forum": "aGH43rjoe4",
                "replyto": "kz2CHhKk0f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Methods:\n\n1. Equation 4 simply functions to combine the shared and modality specific latents in the first step of the decoder. This is akin to the latent space in probabilistic CCA models (see for example equation 2 in Gunderson et al. 2019) and so it acts to partition the variability between the shared and independent latent subspaces. This linear operation is therefore an essential feature of our multi-modal model. \n\n- Gundersen, Gregory, et al. \"End-to-end training of deep probabilistic CCA on paired biomedical observations.\"\n2. \n\n> The construction of simulated dataset is confusing,\n\nWe apologize for this confusion. Please see our response to reviewer YQRh, where we clarify this in detail. If you have any additional questions on this, please let us know.\n\n> [...]  linked with real-world datasets. \n\nWe draw the reviewers attention to the final figure, where it is precisely the case that the latent variables that describe the neural activity are a linear combination of latents that are shared with a high dimensional moving stimulus (closely related in structure to the MNIST image) and independent wing-flapping (like our other 1d GP neural-only latent in our simulated data). By validating our approach in this synthetic setting first, we can be more confident in understanding these uncovered latents in figure 5. \nTo re-emphasize this real-world finding, the neural activity is tuned from a two dimensional latent space, one dimension of which tracks the movement of the 2d visual stimulus, and the other dimension corresponds to wing-flapping. The overall Poisson rate for each neuron is thus a linear combination of these two experimental features, in a similar form to how we construct our synthetic data. \n\n3. Thank you for this suggestion. We have included the Fourier approach without pruning the frequencies in the appendix (see figure 6). As you can see, not pruning the frequencies prevents us from identifying the correct smoothly evolving latent structure, and so we did not continue with an unpruned approach in this work.\n\nImportantly, we do not claim that there is no high-frequency information in the latent space (there very likely is) but our pruning regularizes our model in such a way to be able to uncover meaningful latents with clear temporal structure. As the reviewer suggests, our model is thus limited in its ability to identify fast moving temporal structure, and we will be sure to emphasize this limitation more clearly in the final manuscript. \n\n4. We apologize for this not being more clear. We mentioned the latent dimension selection process in the appendix but agree our treatment was cursory. We clarify our approach to reviewer Kdv4, so please read this response for more information. We have additionally clearly described our dimensionality-selection method in the appendix, now it can be found under \u2018latent dimensionality selection\u2019 section.\n\nRegarding other model hyperparameters, we have a section devoted to the observation variance parameter in the supplement, as well as a thorough description of our selection of length scales for the GP latents and neural network architectures. In general, model performance is not sensitive to specific network architectures and length scale initializations can encourage smoother or less smooth after inference is complete to some extent (also see page 4, 6, 7, 9, and 10 of the appendix for more information).\nPlease let us know if you need any further clarification on any specific model hyperparameters."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367024625,
                "cdate": 1700367024625,
                "tmdate": 1700367024625,
                "mdate": 1700367024625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3hzHyIcas4",
                "forum": "aGH43rjoe4",
                "replyto": "kz2CHhKk0f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Cont. for Evaluations, Writing, and Questions"
                    },
                    "comment": {
                        "value": "### Evaluations:\n\n1. \n\n> No quantitative measurements on real datasets \n\nPlease see our response to reviewer bxWt, as we emphasize our quantitative measurements about the contributions of the shared and independent subspaces on the hawkmoth dataset, and propose the addition of additional reconstruction examples of data from each modality as we ablate the shared and independent latents in the appendix. \n\n> Not compared with other baselines\n\nAs we mention to other reviewers, we do compare to two time-series multi-modal models used in neuroscience today in the appendix, both on a synthetic dataset as well as a real neural dataset (fig 1 and 2). However, it is clear we did not highlight this enough in the main body of the manuscript, and so we will be sure to update this accordingly.\n\n2. That is correct, the MMGPVAE does not separate behavioral conditions particularly cleanly across fly behaviors. The separation of behavioral categories of the latents is an avenue for further scientific exploration, and may require tuning additional model features or adjusting the geometry of the latent prior. However, figure 4 is meant to demonstrate that the MMGPVAE is able to generate good predictive performance on this multi-modal dataset in held-out trials, and that each of the latent subspaces are able to be visualized and analyzed in this setting. (as far as we know, we are the first multi-modal model flexible enough to describe simultaneous 16-dimensional limb-tracking and calcium recordings.)\n\n3. The blurriness of the 3 in our model reconstructions is expected given our model. To be clear, our goal is not to best reconstruct the three, but rather to identify the smoothly evolving angle and scale of the three in the latent space. The reconstructed threes are thus provided only angle and scale information from the latents, and so they otherwise reflect an average of all handwritten threes in the training set (hence the blurry, canonical 3 looking the same at all angles and scales). This is an essential point in our work, and we are happy to clarify further if the reviewer has additional questions or concerns. (note also that the blurry canonical 3 is seen in other GPVAEs of Casale et al, Fortuin et al. and Ramchandran et al). \n### Writing:\n\nWe apologize for any confusion in our work. If the reviewer can be more specific about which important contents they feel are insufficiently addressed beyond what is discussed above, please let us know. \n### Questions:\n1. A nonlinear operation is used as the next step in each of our decoders (an exponential nonlinearity to generate Poisson rates for the GPFA description of neural data and a deep neural network to describe the behavior/stimulus), so the ultimate decoding mapping to data is a nonlinear operation. If this first step were nonlinear, it would be highly unusual, a more difficult to train and we would lose the ability to interpret our shared/independent latent partitioning. (see also Gunderson et al 2019 for additional context).\n\n2. We thank the reviewer (and other reviewers) for pointing this out and have more clearly highlighted this procedure in the Appendix, page 9. Please let us know if further clarification is needed.\n\n\nWe hope that the revisions and responses in our updated paper have sufficiently addressed all of the concerns and questions raised by the reviewer. If this is the case, we hope the reviewer could consider updating their score accordingly. If there are still any questions or concerns, we would be happy to clarify them and engage in further discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367459796,
                "cdate": 1700367459796,
                "tmdate": 1700484820320,
                "mdate": 1700484820320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PsiMT2k6rw",
                "forum": "aGH43rjoe4",
                "replyto": "jD2Yn6VgwG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Reviewer_y53H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Reviewer_y53H"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for adding experiment results and clarifications. I think part of my concerns are addressed. While I think this synthetic dataset is still oversimplified compared to the real-world datasets, it fails to demonstrate the real capabilities of the proposed methods. The added ablation studies and baselines are still performed in simulated data, not real data. Comprehensive evaluations require more datasets and baselines (i.e. existing benchmarks). As mentioned by other reviewers, the novelty of this work is also limited given it is a combination of existing methods (i.e. Fourier representation)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702532196,
                "cdate": 1700702532196,
                "tmdate": 1700702532196,
                "mdate": 1700702532196,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VnRm5wl0Hl",
            "forum": "aGH43rjoe4",
            "replyto": "aGH43rjoe4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_bxWt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_bxWt"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method, MM-GP-VAE, for modeling multimodal data, presented in the context of analyzing neural and behavioral data.  The balance that the proposed method strikes is retaining the interpretability and disentanglement enjoyed by simpler models, but retaining the expressivity and ease-of-use of deep methods.  Low-dimensional latent states describe each channel individually, and a shared low-dimensional state describes the shared covariance.  The recurrent model is implemented as a Fourier-domain GP-VAE, to encourage smoothness in the latent state and increase expressivity.  The model is benchmarked on a synthetic MNIST-like experiment, and is then applied to neural data.  The model appears to perform well against some sensible baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is incredibly well presented.  Figures are prepared exceptionally well.  The prose is clear, and presents a self-contained introduction to all of the necessary techniques and considerations. \n\nThe method itself is gracefully simple.  Although there is not a huge methodological contribution, the correct components parts are assembled and deployed in a way that is very experimentally useful.  I could see this methodology having real uptake within the community, and engendering multiple follow-up works."
                },
                "weaknesses": {
                    "value": "I do have several queries/concerns however:\n\n- **a. Fixed time horizon**:  The use of an MLP to convert the per-timestep embeddings into per-sequence Fourier coefficients means that you can only consider fixed-length sequences.  This seems to me to be a real limitation, since often neural/behavioral data \u2013 especially naturalistic behavior \u2013 is not of a fixed length.  This could be remedied by using an RNN or neural process in place of the MLP, so this is not catastrophic as far as I can tell.  However, I at least expect to see this noted as a limitation of the method, and, preferably, substitute in an RNN or neural process for the MLP in one of the examples, just to concretely demonstrate that this is not a fundamental limitation. \n\n- **b. Hidden hyperparameters and scaling issues**:  Is there a problem if the losses/likelihoods from the channels are \u201cunbalanced\u201d?  E.g. if the behavioral data is 1080p video footage, and you have say 5 EEG channels, then a model with limited capacity may just ignore the EEG data.  This is not mentioned anywhere.  I think this can be hacked by including a $\\lambda$ multiplier on the first term of (6) or raising one of the loss terms to some power (under some sensible regularization), trading off the losses incurred by each channel and making sure the model pays attention to all the data.  I am not 100% sure about this though.  Please can the authors comment. \n\n- **c.  Missing experiments**:  There are a couple of experiments/baselines that I think should be added. \n  - Firstly, in Figure 3, I'd like to see a model that uses the data independently to estimate the latent states and reconstruction.  It seems unfair to compare multimodal methods to methods that use just one channel.  I\u2019m not 100% sure what this would look like, but an acceptable baseline would be averaging the predictions of image-only and neuron-only models (co-trained with this loss).  At least then all models have access to the same data, and it is your novel structure that is increasing the performance.\n  - Secondly, I would like to see an experiment sweeping over the number of observed neurons in the MNIST experiment.  If you have just one neuron, then performance of MM-GP-VAE should be basically equivalent to GP-VAE.  If you have 1,000,000 neurons, then you should have near-perfect latent imputations (for a sufficiently large model), which can be attributed solely to the neural module.  This should be a relatively easy experiment to add and is a good sanity check.\n  - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out).  This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task.  \n\n  Again, these should be reasonably easy experiments to run.  I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\n\n- **d.  Slightly lacking analysis**:  This is not a deal-breaker for me, but the analysis of the inferred latents is somewhat lacking.  I\u2019d like to see some more incisive analysis of what the individual and shared features pull out of the data \u2013 are there shared latent states that indicate \u201cspeed\u201d, or is this confined to the individual behavioral latent?  Could we decode a stimulus type from the continuous latent states?  How does decoding accuracy from each of the three different $z$ terms differ? etc.  I think this sort of analysis is the point of training and deploying models like this, and so I was disappointed to not see any attempt at such an analysis.  This would just help drive home the benefits of the method.  \n\n\n### Minor weaknesses / typographical errors:\n\n1.  Page 3:  why are $\\mu_{\\psi}$ and $\\sigma_{\\psi}^2$ indexed by $\\psi$?  These are variational posteriors and are a function of the data; whereas $\\psi$ are static model parameters. \n\n2.  Use \\citet{} for textual citations (e.g. \u201cGP-VAE, see (Casale et al., 2018).\u201d ->   \u201cGP-VAE, see Casale et al. (2018).\u201d)\n\n3.  The discussion of existing work is incredibly limited (basically two citations).  There is a plethora of work out there tackling computational ethology/neural data analysis/interpretable methods.  This notable weakens the paper in my opinion, because it paints a bit of an incomplete picture of the field, and actually obfuscates why this method is so appealing!  I expect to see a much more thorough literature review in any final version.\n\n4.  Text in Figure 5 is illegible.  \n\n5.  Only proper nouns should be capitalized (c.f. Pg 2 \u201cGaussian Process\u201d -> \u201cGaussian process\u201d), and all proper nouns should be capitalized (c.f. Pg 7 \u201cfigure 4(c)\u201d).\n\n6.  Figure 1(a):  Is there are sampling step to obtain $\\tilde{\\mu}$ and $\\tilde{\\sigma}^2$?  This sample step should be added, because right now it looks like a deterministic map.  \n\n7.  I think \u201ctruncate\u201d is more standard than \u201cprune\u201d for omitting higher-frequency Fourier terms.\n\n8.  I find the use of \u201cA\u201d and \u201cB\u201d very confusing \u2013 the fact that A is Behaviour, and B is Neural?  I\u2019m not sure what better terms are.  I would suggest B for Behavioural \u2013 and then maybe A for neural?  Or A for (what is currently referred to as) behavioral, but be consistent (sometimes you call it \u201cother\u201d) and refer to it as Auxiliary or Alternative data, and then B is \u201cBrain\u201d data or something.  \n\n9.  The weakest section in terms of writing is Section 3.  The prose in there could do with some tightening.  (It\u2019s not terrible, but it\u2019s not as polished as the rest of the text).\n\n10.  Use backticks for quotes (e.g. \u2018behavioral modality\u2019 -> ``behavioral modality\u2019\u2019)."
                },
                "questions": {
                    "value": "I don\u2019t have any further questions from those outlined in Weaknesses.\n\n**Note:** If the authors can allay my concerns, then I fully intend to increase my review score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4398/Reviewer_bxWt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699142647707,
            "cdate": 1699142647707,
            "tmdate": 1700369623630,
            "mdate": 1700369623630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lu5f2eskOi",
                "forum": "aGH43rjoe4",
                "replyto": "VnRm5wl0Hl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for highlighting the relevance of this work to the neuroscience community and praising the approach and experiments. Below we address each of the questions and points raised by the reviewer.\n### Weaknesses\n> a.\n\nWe agree with the reviewer that this is a limitation of the model, and we will be sure to highlight this in the main body of the final manuscript. We do, in fact, need to split our datasets into \u2018trials\u2019 of fixed length to fit the model (as is briefly mentioned in the appendix sections on pages 8 and 9). Fortunately, this did not practically prevent our model from providing insight in the datasets in 4 and 5. \n> b.\n\nThis is a good point, and is discussed in some detail in the appendix (see page 10 in appendix). To highlight the important point \u2013 our learned $\\sigma^2$ parameter did not vary much from its initialized value, and so our choice of the initialization of $\\sigma^2$ reflects exactly the kind of trade-off the reviewer describes. We therefore initialized the $\\sigma^2$ parameter to a value proportional dimensionality of the observed behavioral modality, and this assured a balancing across modalities for any of the real-world datasets we tested.\n> c.\n\n> Firstly [...]\n\nWe apologize but we are a little unclear as to this point. Can the reviewer clarify further what they mean here? Our ablations in figure 3 do use the datasets independently, and the latent dimensionality is kept the same per-modality (two dimensions for each unimodal evaluation). In other words, each unimodal piece of our model has identical flexibility whether or not the model is trained jointly or separately. Therefore, it is the cross-modality structure that increases the performance (the neural latents are able to use the structure of the image data to better reconstruct rates, and vice-versa). \n> Secondly [...]\n\nThank you for this suggestion. We have added a figure, fig 7,  to the appendix showing exactly this. We see that increasing the number of neurons only improves the identification of shared and neural latents, and, expectedly, does not affect the ability of the model to identify the image-only latent structure.  We also see improvement in reconstructions across both modalities as the number of neurons increases \n> Finally [...]\n\nAn excellent suggestion. We have added it to the appendix. Our smooth gp-based approach allows for better identification of an uncorrupted image than a standard VAE while retaining the ability to reconstruct neural rates, rendering the MMGPVAE robust to measurement errors.\n> d.\n\nIncidentally, since submitting the manuscript, we have follow-up work that begins to answer this question in the context of the hawkmoth dataset. As we mention in the manuscript in our evaluation of the hawkmoth dataset, the shared latent accounts for about 30% of the variability of image reconstructions, and about 5% of the variability of neural data. As we point out, this is expected as the neural data is dominated by wing-flapping. We have since looked at what features of the stimulus and neural activity are encoded in the shared subspace by removing this shared component and plotting the data reconstructions. We find that removing the shared component removes the slowly varying features of neural reconstructions and blurs and distorts the overall image reconstruction, particularly at the times when the stimulus changes direction. If the reviewer feels strongly, we could add these figures to an appendix, but currently we feel it is more appropriate for follow-up work.\n\n### Minor weaknesses\nThank you for the improvements to our text. All the changes will be implemented in the final manuscript. We address some of the more important minor points below:\n> 3.\n\nWe will change this section title in the discussion, to \u201ccompeting multi-modal approaches in neuroscience\u201d, and thank the reviewer for pointing out this confusion. While we survey the field significantly more broadly in the introduction, in this discussion section, we wanted to simply point out two existing multi-modal time-series LVMs that have been used in similar settings in neuroscience. We find it important to point out these models as we compare our approach to them in two figures in the appendix. We will be sure to clarify the writing here and make sure we conclude with a more accurate picture of the field. \n> 8.\n\nWe apologize for this confusion. We propose flipping the notational indices, where A reflects \u201cactivity\u201d and B reflects \u201cbehavior\u201d \n\nWe again thank the reviewer for their thoughtful comments and suggestions, and for their time reviewing the paper. We hope that the revisions and responses in our updated paper have sufficiently addressed all of the concerns and questions raised by the reviewer. If this is the case, we hope the reviewer could consider updating their score accordingly. If there are still any questions or concerns, we would be happy to clarify them and engage in further discussion."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339108706,
                "cdate": 1700339108706,
                "tmdate": 1700339108706,
                "mdate": 1700339108706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MOJRwDClra",
                "forum": "aGH43rjoe4",
                "replyto": "VnRm5wl0Hl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Reviewer_bxWt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Reviewer_bxWt"
                ],
                "content": {
                    "title": {
                        "value": "Mostly there"
                    },
                    "comment": {
                        "value": "To the authors,\n\nThank you for your response and clarifications, and for swiftly updating the paper.  \n\nIn response to \"C. Firstly\":  As far as I understand, in Figure 3c, the baselines are only provided with one channel, whereas your proposed method is provided with two channels.  Is the performance increase coming from simply getting to see both channels, whereas the baselines only see one channel?  Are the authors saying that \"image-only GP-VAE\" is in some way accessing the neural data?  \n\nE.g., You're making a salad.  It is unfair to compare a salad made by someone who just had access to tomatoes, to someone who just had lettuce, to someone who gets to use both tomatoes and lettuce.  I want you to compare four salads (models): just tomato (neuron only GPFA), just lettuce (image only GP-VAE), lettuce and tomato side by side but no mixing (the baseline I'm requesting, e.g. averaging predictions/reconstructions of neuron only GPFA and image only GP-VAE), and a salad with lettuce and tomato all mixed together (MM-GPVAE).*  \n\nPlease correct me if my understanding of the experiment presented is incorrect.  \n\nIn response to \"D.\":  I do feel reasonably strongly on this, actually.  If you have such results, then I think at least a taster of them should be included here.  If you are angling _this_ paper as an interpretable neuroscience method, then I think some interpretations/neuroscience should be included.  I don't think you should include so much analysis that you cannot publish another paper somewhere else, but I think this paper might've been \"sliced a bit thin\" in that context.  I would very much like to see some simple analysis in this vein included in the final version.  \n\nOverall, I think this paper is of publication quality.  Subject to the inclusion of a more thorough discussion of related techniques, and the comparisons suggested by Kdv4, i vote for the inclusion of this paper.  \n\nThank you very much, and good work,\n\nbxWt\n\n_*This is the most ridiculous thing I've written in a review.  I hope you enjoyed it and/or found it useful...!_"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369452149,
                "cdate": 1700369452149,
                "tmdate": 1700592545682,
                "mdate": 1700592545682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fQpBHs5b2y",
            "forum": "aGH43rjoe4",
            "replyto": "aGH43rjoe4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_YQRh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_YQRh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multi-modal gaussian process VAE model for neuroscientific data. Due to the time-varying nature of neuronal data, the authors use gaussian priors for Fourier frequencies of the data. Thus the network features FFT and IFFT layers. Results are shown on synthetic data as well as on calcium signaling data from Drosophila and Moth EMG data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The use of Fourier frequencies as latent variables can have some advantages in learning neural features that correlate with motor or other behaviors. The authors also separate shared from independent variability in the neural data which can be useful in understanding what part of behavior could actually be predicted from specific neuronal measurements."
                },
                "weaknesses": {
                    "value": "-The synthetic data seems weak and unnecessary. Why not use a simulator like NEST or augment one and create \"behavioral\" data with known connections to this. MNIST images have a different structure because they are a two dimensional image. \n-The authors lack comparison to anything but an ablation of their own model, and that too on the above artificial dataset. \n-At least the authors should compare to RNN/transformers/ODE or other sequential models for this data\n-The authors seem to criticize DNNs for transforming the data too much such that the latent variables are \"not obviously related\" to external variables, and yet they too use DNNs. \n-There is no theory or study of identifiability of the dynamics or conditions/noise under which dynamics are recovered."
                },
                "questions": {
                    "value": "The synthetic data does not make sense to me, what exactly is the connection between the digits and the \"spikes\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699151347673,
            "cdate": 1699151347673,
            "tmdate": 1699636413476,
            "mdate": 1699636413476,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1FPe7u2VHi",
                "forum": "aGH43rjoe4",
                "replyto": "fQpBHs5b2y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for finding MM-GPVAE useful for understanding the components of behavioral and neural data. Below we address each of the questions and points raised by the reviewer.\n\n### Weaknesses:\n> [...] simulator like NEST [...]\n\nAssessing performance for GPFA models in neuroscience to synthetic poisson or gaussian spiking data generated from a low-dimensional subspace is nearly ubiquitous for this class of models (e.g. see Yu et al 2009, Lakshmanan et al 2015, Zhao and Park 2017, Dunker and Sahani 2018, Keeley et al 2020, Jensen et al 2021, Gokcen et al 2022) A simulator like NEST is not designed to generate spikes with true low-dimensional generative latent structure so unless we\u2019ve misunderstood the reviewer we would have no way of validating our LVM approach on such synthetic data. Similarly, GPVAE models each use rotating digits as benchmarks for their model performance (see Casale et al 2018, Fortuin et al 2020, Ramchandran et al 2021) generated from a low-dimensional latent space. Because, in each of these cases in these works, true generative low-dimensional GP latents give rise to very different kinds of data (MNIST and Poisson spikes), we consider this precise type of synthetic data vital in validating our inference approach, and necessary to relate our model pieces to the GPFA and GPVAE literature upon which we draw.  By using rotating digits and GPFA-generated spike trains, we are able to demonstrate that we maintain state-of-the-art performance on the relevant datasets for each unimodal model (GPFA and GPVAE). This would be more difficult to do if we generated a completely new type of synthetic dataset. \n> MNIST images have a different structure[...]\n\nMNIST images have a different structure than our experimental validation in figure 4 but quite similar structure to our experimental validation in figure 5, where we assess the model on spiking motor neurons and a smoothly moving 2d visual stimulus. \n> The authors lack comparison [...]  authors should compare to RNN/transformers/ODE [...]\n\nWe apologize if we did not highlight these clearly enough in the main manuscript, but we do, in fact, compare MM-GPVAE to an RNN based model (Hurwitz et al.) as well as an ODE based model (Sani et al.). Both of these existing models are multi-modal models used in neuroscience . We evaluate these existing multi-modal models on both synthetic and real-world datasets in the figures 1 and 2 of the appendix. \n\n- Sani et al. (2021) \"Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification\"\n\n- Hurwitz et al. (2021) \u201cTargeted Neural Dynamical Modeling\u201d \n\n> The authors seem to criticize DNNs [...]\n\nWe are of course not critical of DNNs in general. As the reviewer points out, there are many DNNs used in our work, including in the encoding networks in all models, and in the decoding networks for behavioral data modalities. These networks are crucial for model inference and greatly enhance the flexibility of our approach. However, DNNs are limited in their abilities to provide scientific insight, and this is precisely what motivates the use of a linear decoder for the neural data. By having a linear decoder, our map from latents to rates is akin to GPFA, allowing the MMGPVAE to capitalize on its wide successes (see the GPFA citations below). In addition, it  allows us to perform the analyses in figure 5 of the manuscript, where we can evaluate how individual neurons are independently tuned to shared (stimulus-tracking) and independent (wing-flapping) subspaces. We are pursuing follow-up work which uses these linearly-identified tuning features to better understand the hawkmoth motor system.\n > There is no theory [...]\n\nThis is correct. We will be sure to highlight this limitation in the final version of the manuscript, and we thank the reviewer for pointing this out."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365347167,
                "cdate": 1700365347167,
                "tmdate": 1700365347167,
                "mdate": 1700365347167,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W1fYLDWrJz",
            "forum": "aGH43rjoe4",
            "replyto": "aGH43rjoe4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_Kdv4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4398/Reviewer_Kdv4"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a multi-modal Fourier-domain Gaussian Process variational autoencoder (MM-GPVAE), aiming at jointly modeling observed neural and behavioral data in neuroscience applications with both shared and independent latent subspaces. MM-GPVAE combines the ideas in previous GP-VAE, GP Factor Analysis (GPFA), and spectral representation. The authors have also presented simulations and two neuroscience case studies showing the effectiveness of MM-GPVAE modeling neural measurement / spiking data with either visual stimulus or position data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This submission exploits the multi-modal latent variable models based on GP-VAE, using Poisson and normal likelihoods for spiking and image data respectively, which can help modeling both shared and independent factors across modalities in neuroscience. \n\n2. Experiments on simulated and real-world datasets with visualized results showing the potential in jointly analyzing neuroscience data. \n\n3. The presentation is clear with articulated motivations of the presented mm-GPVAE. Source code is also provided."
                },
                "weaknesses": {
                    "value": "1. Methodological contributions may be limited. Many papers have done similar things, such as extending the latent space into frequency domain [1,3] and using the Poisson model for spiking data [1,2]. The method proposed in this paper is combination of existing methods, not fundamentally different. \n\n2. The experimental results can be more comprehensive: (1) More baselines, especially similar methods such as [1,4] need to be compared; (2) The authors stated one main advantage of the Fourier domain is that high frequencies can be pruned and thus we can sparsify the variational parameters. However, no results are provided about this point; (3) An ablation study between the models with Frequency prior and with other priors (e.g., a temporal form) should be added; (4) More discussions on the claimed interpretability should be provided. \n\n[1] Keeley S, Aoi M, Yu Y, et al. Identifying signal and noise structure in neural population activity with Gaussian process factor models. Advances in neural information processing systems, 2020, 33: 13795-13805.\n\n[2] Duncker L, Sahani M. Temporal alignment and latent Gaussian process factor inference in population spike trains. Advances in neural information processing systems, 2018, 31.\n\n[3] Hensman J, Durrande N, Solin A. Variational Fourier Features for Gaussian Processes. J. Mach. Learn. Res., 2017, 18(1): 5537-5588.\n\n[4] Pearce M. The Gaussian Process prior VAE for interpretable latent dynamics from pixels. Symposium on advances in approximate Bayesian inference. PMLR, 2020: 1-12."
                },
                "questions": {
                    "value": "1. How sensitive is the performance with respect to the setting of several hyperparameters, including $F$ and the dimension of shared and independent latent subspace, etc.? \n\n2. How optimization was done with FFT/iFFT involved in MM-GPVAE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699412808329,
            "cdate": 1699412808329,
            "tmdate": 1699636413409,
            "mdate": 1699636413409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fBktl8rmpE",
                "forum": "aGH43rjoe4",
                "replyto": "W1fYLDWrJz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive appraisal of the manuscript. We address some of the concerns and misunderstandings below.\n> The method proposed in this paper is combination of existing methods, not fundamentally different.\n\nWe agree with the reviewer that the core contribution of this work represents a combination of GPFA, and GP-VAEs along linear partitioning of a latent space for datasets into shared and independent components of two modalities, and so is in a sense a combination of existing methods. However, we believe that we are the first time-series latent variable model developed that uses this partitioning to separate both shared and independent latent trajectories in multi-modal settings in neuroscience, and we do so in a way that is more flexible and scalable compared to competing multi-modal methods (see appendix). We feel that this is therefore a novel approach with wide applicability across a range of multi-modal neuroscientific experimental set-ups that we demonstrate in the manuscript. We also would like to highlight our novel amortization method in the Fourier domain for a deep GP model, as well as our derivation of a model specific ELBO in the appendix that precludes the need to approximate two of the terms in the objective. \n> More baselines, similar methods such as [1,4]\n\n[1] and [4] are not multi-modal models, and so we do not believe that they are fundamentally similar to our approach. We do, however, recognize that [1] and [4] each relate to one piece of our model, and we agree with the reviewer that comparisons of our multi-modal model to these unimodal variants will help us understand model performance. However, our comparisons of GPFA and GP-VAE in figures 1 and 3 are identical to the relevant variant of the method outlined in [1] and closely related to that of [4]. ELBO from [4] takes a similar general form to the GP-VAE ELBO we adapted which was presented in Casale et al 2018, barring a different observation likelihood and differently factorized variational distribution. Therefore, we consider our comparison to Casale et al 2018 to be a very close approximation to the comparison in [4]. We thank the reviewer for pointing us to this work and will add the citation of [4] and discussion as to this point in our revised version.\n> More baselines\n\nWe benchmark against two prominent multi-modal models used for neuroscience in the appendix - one of these uses ODEs to characterize the latents, and the other uses an RNN. If the reviewer finds these benchmarks insufficient, please let us know. \n> Ablation study with fourier frequencies \n\nThank you for this suggestion, we now address this in the global response.\n\n > Interpretability\n\nWe are very specific about interpretability in this work. In the synthetic examples, the \"interpretability\u201d of our model refers to the fact that we can recover latents that identify two features of the data that are meaningful to us, in this case, the digit scaling factor and angle. In the case of the real data examples, interpretability means the identified latent in the data corresponds to something scientifically meaningful - for example, the position of a visual stimulus, the beating pattern of a hawkmoth wing (Fig 5) , the behavioral state of a fly (Fig 4) , or the reaching position of a monkey (Fig 2 in appendix). We will edit our wording to be more specific about our use of the term \u2018interpretability'\n\n### Questions\n> 1\n\nFor the real neural dataset examples in the main paper, the dimensionality was chosen for the neural dataset using PCA (selected for capturing greater than 97% of the variance) and the dimensionality was chosen for the behavioral data by increasing the number of latent dimensions until cross-validated performance did not show significant improvement. To determine the shared dimensionality, we similarly systematically increased the number of dimensions shared between datasets until predictive performance no longer improved. However, the results are not particularly sensitive to latent dimensionality within a few dimensions of either modality. \n\n> 2\n\nThe encoder networks first identified latent variables per time-point, and was then passed through a FFT and reduced in dimensionality across time before being used to sample for the ELBO. Thus, inference was done in the Fourier domain. When decoded, the latents were mapped back out of the Fourier domain using an inverse transform. Because each of these transformations are linear operations, they are simply parts of the encoder and decoder networks. Network schematics for the exact mappings for each of the analyses can be found in figures 9-12 of the appendix. Please let us know if the reviewer needs any further clarification.\n\nWe again thank the reviewer for their thoughtful comments and suggestions, and for their time reviewing the paper. If there are still any questions or concerns, we would be happy to clarify them and engage in further discussion."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337386289,
                "cdate": 1700337386289,
                "tmdate": 1700337386289,
                "mdate": 1700337386289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gfqASnkM6c",
                "forum": "aGH43rjoe4",
                "replyto": "W1fYLDWrJz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Cont. for [1] and [4]"
                    },
                    "comment": {
                        "value": "> More baselines, especially similar methods such as [1,4] need to be compared\n\nTo elaborate further, the model in [1]  is a unimodal model developed for neural data that explores trial-by-trial deviations in trail-based neural data with exact repeated stimulus conditions, so alone the model introduced in [1]  is not directly applicable to our setting. However, we recognize that a \u201csignal only\u201d version of [1]  is an equivalent model to the poisson GPFA model first introduced in Zhao and Park 2017 learned with a Fourier representation, similar to our approach in an exclusively linear setting without deep amortization. Therefore, this version of [1] is an identical model to the \u201cGPFA only\u201d column in figure 3 in our paper. So, while we did not make this connection explicit in our paper, we do have a direct comparison here to the relevant modified version of [1]. (the only difference is our model uses amortized inference, instead of a black-box VI approach used in [1], which improves out-of-sample performance and allows us to adapt this representation to the deep setting). We will make sure this comparison to this version of a unimodal poisson-GPFA is clearer upon revision.\n\nSimilarly, [4] is a unimodal GP-VAE model, and it uses Bernoulli likelihood and different factorized variational distribution but otherwise is closely related to the ELBO of Casale et al. Our primary contribution is comparing this same ELBO with and without the Fourier domain representation of the prior and latents (The prior and latents of [4] are identical to that of Casale et al.)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368175065,
                "cdate": 1700368175065,
                "tmdate": 1700440174237,
                "mdate": 1700440174237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DpLmoGg0t4",
                "forum": "aGH43rjoe4",
                "replyto": "W1fYLDWrJz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4398/Reviewer_Kdv4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4398/Reviewer_Kdv4"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the effort to address some of the questions. I am wondering for a given new dataset, how much tuning time would be needed if latent dimension, frequency pruning, and other hyperparameters were involved with cross validation, etc. \n\nRegarding the baseline comparison, I still felt that the model itself has been proposed or very similar to the existing ones. If the authors would like to emphasize their latent space partitioning idea, it may be important to compare with multi-modal feature fusion and other similar multi-modal embedding methods. \n\nI am little bit confused with the authors' statement: \"highlight our novel amortization method in the Fourier domain for a deep GP model, as well as our derivation of a model specific ELBO in the appendix that precludes the need to approximate two of the terms in the objective.\" If the model has to be trained in the Fourier domain, is this what have to be done? It would be great if the authors can provide more details why the amortization methods in the Fourier domain is 'novel'."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661765500,
                "cdate": 1700661765500,
                "tmdate": 1700661765500,
                "mdate": 1700661765500,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]