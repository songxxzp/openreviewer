[
    {
        "title": "Invariance-based Learning of Latent Dynamics"
    },
    {
        "review": {
            "id": "vo8TKSjyX0",
            "forum": "EWTFMkTdkT",
            "replyto": "EWTFMkTdkT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4895/Reviewer_Fm8i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4895/Reviewer_Fm8i"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel framework called \"Latent Dynamics via Invariant Decomposition\" (LaDID) for predicting dynamical trajectories from high-dimensional empirical data. LaDID merges variational autoencoders with spatio-temporal attention, emphasizing scientifically-motivated invariances, allowing the model to predict system behavior at any continuous time and generalize beyond seen training data. Using a transformer-based architecture, it distinguishes between system-specific and universal dynamics, showing efficiency and scalability in experiments. The method is validated on various spatio-temporal systems, LaDID outperforms current neural-dynamical models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper uniquely combines variational autoencoders with spatio-temporal attention\n\n2. empirical tests on various spatio-temporal systems demonstrate that LaDID not only has practical utility but also massively outperforms existing neural-dynamical models"
                },
                "weaknesses": {
                    "value": "1.The definition of a system with realization-specific and realization-invariant properties is not clear. In chapter 3, I understand the authors try to explain its difference to the ode like systems and want to extend the ODE-like system setting to a more general settings with RS and RI properties, but I cannot find a specific example which is included in the RS-RI setting but excluded in the ODE setting. Can the author provide a useful example?\n\nIt seems like all the experiments in the paper can be treated as ODE-like dynamical system, so I was wondering what is the major difference here?\n\n2.There is a missing literature review regarding invariant learning and its application in data-driven dynamical system learning, there are a few branches of related work worth mentioning and they are very close to the topic in this paper:\nPhysics/Conservation invariant:\n\n\"Hamiltonian neural networks.\" Advances in neural information processing systems 32 (2019).\n\nLagrangian Neural Networks. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations.\n\nConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. International Conference on Machine Learning 2023.\n\nSymmetry invariant: Incorporating Symmetry into Deep Dynamics Models for Improved Generalization, ICLR 2021\n\nActually the Hamiltonian Neural network paper provides an image-based pendulum example which is similar to the experiment in this paper. \n\n3.Regarding the novelty of the model structure, I feel like it still belongs to the encoder-latent dynamics-decoder model class like many other prior works. Despite the author claimed the model decomposes the realization-specific (RS) and realization-invariant (RI) information, there is no evidence proving the claim both experimentally and theoretically.\n\n4.Honestly I\u2019m a bit skeptical regarding the massive improvement of the proposed method over the existing work, especially in figure 4 and 5 (see my questions)"
                },
                "questions": {
                    "value": "1. In the performance comparison figure 4, is the X axis the time axis? The results surprised me because the prediction from the first 3 models is bad starting from the beginning of the prediction, which means the error mostly comes from decoder reconstruction rather than dynamics prediction. Especially for ODE2VAE, VAE should be able to reconstruct the pendulum image fairly well.\n\n2. In Figure 5, the MSE from the proposed method is 10-100 times smaller than the prior works. This is another surprise as such a difference usually happens between white box model and black box model. Could there be bugs in your code, or is there any unfair comparison? I\u2019m not trying to be mean but it does not look reasonable for me unless formal claims/evidence is provided.\n\n3. In the latent dynamics model (part v in figure 1), how does the query time t work? Does this model need to solve it iteratively like ODE-like solver to time t or the neural network directly takes t as input and outputs the latent state at time t."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4895/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4895/Reviewer_Fm8i"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698353645885,
            "cdate": 1698353645885,
            "tmdate": 1700413827651,
            "mdate": 1700413827651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DlsnG1WgjY",
                "forum": "EWTFMkTdkT",
                "replyto": "vo8TKSjyX0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Definition of RI and RS properties / Real-world motivation\\\nThank you for this question. Indeed, as you point out, our approach shares similarities with (neural) ODE approaches, but seeks to provide much more flexibility to bypass limits on the complexity of the trajectories that ODEs can model. Please see the overall response for a detailed description of the novelty of our contribution.\n\nOur long-term real-world motivation comes from modelling of trajectories in the biomedical domain: \nHere, underlying processes are too complex to permit explicit description, but we think the invariances we identify can still hold.  Based on our experience (neural) ODE approaches are typically not well suited to capture these complex, high-dimensional systems including strong system delays, discontinuities and distribution shifts (e.g. under  treatment), and therefore require the development of more general and flexible dynamics models.\n\nQ2: Extension of related work\\\nWe added a new subsection to Section A - Extended Related Work in the Appendix according to your suggestion and included the mentioned work, amongst others. Thank you for this suggestion. \n\nQ3: Contribution of work / Guarantee of decomposition\\\nThank you for this feedback, but we politely disagree with the reviewers opinion. Please see the overall response for a precise description of the fundamental differences of our method to other approaches and discussion re: decomposition in RI dynamical model and RS trajectory representation.\n\nOf course, we agree that specific model components like convolutions, attention, and VAEs are not new, but combining these ideas under our theoretical view and the presented graphical model results in a novel and demonstrably powerful inference approach.\n\nQ4: Code release to prove empirical performance\\\nWe are sorry that you are not convinced.  To provide more  confidence in the empirical work, we have released an anonymous code repository as outlined in the global answer. \n\nQ5: Poor baseline performance at the start\\\nThank you for mentioning this detail. You are right, the x-axis depicts the temporal evolution of the system. Regarding your concern, there is a simple explanation: In Fig. 4, the visualization of the predicted trajectory starts after $25\\\\%$ of the entire length meaning that only future time predictions are visualized without the initial reconstruction of the input. Unfortunately, we missed to provide this detail in the figure caption, but amended it now for clarification. Apologies for the inconvenience.\n\nQ6: Analogy to black/white-box model / masssive emperical performance boost\\\nThis is a great observation and your analogy of black and white-box model describes perfectly the difference between our approach and neural ODE based systems. Based on the conditional independence between latent states as outlined in the global answer we can directly predict the latent state $z_{t_{t_q}}$ without needing any information from previous states.  As you correctly point out,  this is equivalent to a black-box model for state transitions. In contrast, other approaches rely on the Markov assumption, meaning that they are modeling explicitly the transition between consecutive states. This is effectively a white-box description of state transition which we mitigate with our novel approach.\n    \nFor all baseline methods we used the code provided in the respective official repositories. Therefore, we do not think it is likely that bugs underlie the results.  Moreover, the test results in Appendix J including Figs. J.1 - J.6 nicely illustrate that the baselines are indeed able to reconstruct and predict future trajectory samples, but simply not at the same level of accuracy compared to LaDID. In addition, previous studies reported in literature outline the same weaknesses of the baseline methods and show almost identical results of future prediction of similar test cases, see for example Figs. 13, 22, 23 \\& 24 in \"Latent Neural ODEs with sparse Bayesian Multiple Shooting\", Iakovlev et al., ICLR 2023.\\\nThese arguments support our view that the empirical results reflect real behaviour. Rather, we think the performance gap is due to the effectiveness of the decomposition we propose and study.\n\nQ7: Encoding of query time t\\\nThank you for this question. We represent each queried time step $t_q$ as a relative time embedding using a sin/cos encoding. Therefore, our dynamics model is continuous in time and, more importantly, can handle irregular time grid by default. Additionally, we are not forced to solve for intermediate time steps."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263837001,
                "cdate": 1700263837001,
                "tmdate": 1700263837001,
                "mdate": 1700263837001,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UwZk9mp8kZ",
                "forum": "EWTFMkTdkT",
                "replyto": "DlsnG1WgjY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4895/Reviewer_Fm8i"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4895/Reviewer_Fm8i"
                ],
                "content": {
                    "title": {
                        "value": "reviewer feedback"
                    },
                    "comment": {
                        "value": "I thank the authors for the rebuttal and clarification.\n\nq1: Upon checking the rebuttal response and other reviews, honestly I'm still confused about the definition of RS/RI. I don't agree with your reply that ODEs are not suited for complex or delayed systems because these delays can be treated as additional system states, or multiple steps can be concatenated as a single step to capture the delay. Taking that aside, since all the experiments in the paper are ODE/PDE-like dynamical systems, can you specify the physics meaning of RS/RI in one example and the difference between RS/RI to initial condition/system coefficients? I asked this question in the original review but not answered.\n\nq3: I checked the appendix this is not a convergence guarantee of what the method proposed to do (8oY3 also pointed out). There is neither intermediate experiment step to show how RS/RI decomposition works (e.g. how the embedding looks like wrt initial condition/system coefficients). It's difficult to justify the improvement of final results without intermediate steps.\n\nq5: why do you start drawing at 25% of the prediction steps? why don't start at the beginning to show how different methods diverge? given the information density of the current 13 drawing steps, there is plenty of room to accommodate the first 25%.\n\nq6: Regarding the method's effectiveness / massive improvement over prior methods, I'd like to hear the opinions from other reviewers.\n\nGiven the rebuttal effort, I will increase my score to 5."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413809210,
                "cdate": 1700413809210,
                "tmdate": 1700413809210,
                "mdate": 1700413809210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gn23mRTC4C",
            "forum": "EWTFMkTdkT",
            "replyto": "EWTFMkTdkT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4895/Reviewer_8oY3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4895/Reviewer_8oY3"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider an encoder decoder model for predicting the trajectory of time series from high dimensional observations. A key difference of the model is that the authors do not impose a particular type of known structure on the dynamics such that the dynamics must follow this structure. The authors then introduce a probabilistic multiple shooting method such that the method can be trained.\u00a0The authors illustrate the proposed method on a number of different datasets to validate its performance. The main question in the performance evaluation is how well the method works on unseen conditions, i.e. changing the parameters of the true underlying dynamical system."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The premise of the paper is very well founded. In general, most time series approaches with latent dynamics consider some model from the applied mathematics literature (e.g. SDEs, ODEs) and then parameterize it according to a neural network. However, the authors consider a more general framework where they split the dynamics according to realization-specific components and invariant components. This induces a new algebra that should somehow be given by the parameterization of the model. The empirical results are also very good and suggest that the model outperforms the baseline methods."
                },
                "weaknesses": {
                    "value": "While the motivation of the paper is nice, there exist a number of limitations in the implementation and in the model itself. For one, there is no guarantee that the model will converge to something that has these separable parameter sets within the motivation of the work. The loss function seems rather standard in the sense that it is the standard loss when one parameterizes a latent variable time series model. It would have been nice to show under what assumptions does the model decompose into the disjoint parameter sets -- the invariant ones and the instance-specific ones.\u00a0\n\nWhile this is a small weakness, the paper seems to claim a bit too much (though I may have also misunderstood). For example, the authors mention they make no assumptions on the structure of the model in the sense of an ODE type of factorization, but still the fact that the first $K$ steps of the time series are used for conditioning does induce a particular structure. In some sense, the ODE interpretation is amenable since one can use the known properties of ODEs to probe the model.\n\nA major goal is to generalize the output on a new domains where some invariant properties are maintained, but the experiments only demonstrated this in the case of changing parameters. However, maybe it would be helpful to see the behavior under shifts in the observation distribution.\u00a0For example, could the distribution of the observation space change in a trivial way (e.g. different color map for the wave equation example) and could the distribution still be predicted?\n\nThere are also some properties that the authors impose in their loss (e.g. smoothness of the latent space) but that assumption seems to violate some of the motivating ideas behind the paper. Smoothness almost implies a differential equation in the latent space."
                },
                "questions": {
                    "value": "Is there anything in the loss that guarantees that the factorization in terms of the RI and RS components? If not how can one impose this on the latent representation?\n\nAssumptions on the latent space being Gaussian seem a bit restrictive, is there a reason for making this assumption? Specifically am wondering how this affects the motivation that very little structure should be imposed on the latent distribution. Will the smoothness and Gaussian terms will force the learned dynamics to resemble something that satisfies an ODE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4895/Reviewer_8oY3",
                        "ICLR.cc/2024/Conference/Submission4895/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698497900187,
            "cdate": 1698497900187,
            "tmdate": 1700498521419,
            "mdate": 1700498521419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6rUfKPiSqj",
                "forum": "EWTFMkTdkT",
                "replyto": "gn23mRTC4C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Guarantee of decomposition through loss and graphical model\\\nThank you very much for your feedback. We have addressed your concerns in the overall response and hope you find this new insight satisfactory. \n\nQ2: Assumption on underlying system\\\nThank you for your feedback. By stating that we do not make assumptions on the underlying system, we wanted to stress that our approach is likewise suited for ODE and PDE-based system and in principle should also be applicable to longitudinal observations for which we do not know the governing equations since our framework is inspired by an amortized VAE (of course with various modifications). In this context, we also wanted to stress that LaDID does not require access to the underlying system variables (in contrast to neural operators) and can also deal with pure observations such as a video of the swinging single/double pendulum (instead of the angle/angular velocity) or the vorticity distribution in fluid mechanical test cases (instead of the velocity fields).\nAs a result, LaDID requires access to a little number of consecutive time steps since a single observation might not provide sufficient information on the dynamics, for instance, based on a single video frame, we cannot tell in which direction the pendulum eventually is swinging. \n\nQ3: Generalizaton to color shifts/scaling\\\nThis is an interesting aspect which we originally wanted to outline by means of the second experiment in Section 6.2. Here, we shifted the cylinder location within the simulation which essentially represents a shift in the observational distribution. \n    \nHowever, the idea of a color shift in the observations is certainly a more intuitive example. Therefore, we trained our approach on a swinging pendulum rendered in the RGB image space and modified the color space according to your suggestion. During training we applied standard image augmentations such as brightness, contrast, saturation and hue transformations. Results are shown in Sec. M2 of the Appendix and highlight that our approach can deal well with different color shifts. \n\nQ4: Properties of smoothness prior\\\nIn our work, the smoothness prior is used exclusively to ensure a smooth transition between subtrajectories when training under a multiple shooting augmentation. The smoothness prior does not impose any regularization on the form or shape of latent trajectories, but simply stitches together the last and first state of consecutive subtrajectories with no effect on the latent roll-out at inference. Therefore, we observed in our tests that the smoothness prior mainly stabilizes the training and improves the final evaluation performance slightly. That is, training without a smoothness eventually may also converge and yields valuable system predictions as shown in Tab. N.3 of the Appendix.\n\nQ5: Restrictions of Gaussian latent state prior\\\nThank you for your insightful question. The assumption of the latent space being Gaussian in our model is indeed a modeling choice that provides certain advantages, namely tractable inference and known effectiveness in various settings. The adoption of a Gaussian prior serves the purpose of establishing a gradient field within the latent space. Specifically, this prior acts to consolidate the estimated latent states $z_{t_q}$ and, in doing so, mitigates the emergence of empty regions devoid of meaningful semantics. This ensures that the latent trajectory roll-outs are well-defined under distribution shifts, noise, and other perturbations. While the Gaussian assumption introduces a level of smoothness, it does not necessarily force the learned dynamics to resemble a specific form, such as an ODE. This flexibility allows the model to capture complex and non-linear dynamics, making it a reasonable choice for many scenarios."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263267095,
                "cdate": 1700263267095,
                "tmdate": 1700263267095,
                "mdate": 1700263267095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xeaAF5suz0",
                "forum": "EWTFMkTdkT",
                "replyto": "6rUfKPiSqj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4895/Reviewer_8oY3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4895/Reviewer_8oY3"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks to the authors for their response and modifications in the manuscript. I appreciate all the experiments and the additional context. I have increased my score as a result."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498508231,
                "cdate": 1700498508231,
                "tmdate": 1700498508231,
                "mdate": 1700498508231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fiYbIbBICg",
            "forum": "EWTFMkTdkT",
            "replyto": "EWTFMkTdkT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4895/Reviewer_1fGG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4895/Reviewer_1fGG"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a general framework for predicting dynamical trajectories based from data. The model architecture specificially aims at extracting realization-invariant and realization-specific information from different observations of underlying dynamical processes, iterated e.g. from different initial states or with different model parameters. To this end they employ a sophisticated encoder architecture, based on CNNs, temporal self-attention and a multi-shooting augmentation to obtain a condensed representation that can be used as a conditioning for a dynamics forecast. The learned latent dynamics model can be queried at irregular times, and the predicted dynamics decoded via a CNN-based decoder model and compared with the data. The model shows SOTA performance on 6 complicated and high-dimensional observations of dynamical processes, such as fluid flows. The author also investigate transfer learning between different parameter settings of the same underlying systems via few-shot learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The presentation, including the figures and writing, is high quality. I also appreciate the detailed appendix. While I have seen many of the components employed in similar settings, such as sequential variational autoencoders or Latent-ODEs, the proposed architecture looks like an interesting new combination of all of them. The idea of transfer learning and generalization for dynamics models is an important field of study. I like the approach of using an expressive encoder to inform the dynamics model about the specifics of the observed time series by encoding it in the initial state, and the fact that the obtained model is continuous in time."
                },
                "weaknesses": {
                    "value": "General problem setting:\n\nWhat is meant by invariant? To my understanding the term \u201cinvariant\u201d in this paper mostly refers to the flow of the underlying system (i.e. the equations given in Appendix 1), while the realization-specific components are different initial conditions from which different observations start. This is also the situation primarily studied in the experimental section. The authors claim to show \"generalization well beyond the data distributions seen during training\".\nHowever, the setting they mostly investigate is just the standard setting in dynamical systems reconstruction, where underlying dynamical processes/flow fields are inferred from multiple or even single observations. I don't think this constitutes generalization beyond the data distribution in the usual sense.\n\nMore interesting to my mind is the question of transfer learning e.g. between different dynamical regimes (as studied out in 6.2.) with different parameter settings of the underlying system, or even between different but in some way related systems. Relating this to a more formal language common in dynamical systems theory might be helpful, since for dynamical systems the concept of invariants, such as topological invariants, fixed points, periodic orbits, Lyapunov spectra, invariant measures etc. is well studied. While the abstract mentions the motivation to \u201c enforce certain scientifically-motivated invariances\u201d these invariances are not really made explicit in the related work or in the analysis of the obtained results and remained somewhat elusive to me.\nFurther, while you mention that your approach \u201cis in principle not specific to physical ODE-like models\u201d, these are the only systems you actually study in the experiments, which naturally suggests are framing in these terms.\n\nTransfer learning setting (6.2.)\n\nYour framework is aimed at transfer learning from \u201cpotentially highly heterogeneous parameters and data\u201d, which as mentioned seems the more novel application of your framework. Most of the empirical section however focuses on multiple instantionations of the same underlying system. The results from 6.2., where transfer learning between different model parameters is learned, however indicate that the transfer effect is relatively weka, with advantages over learning from scratch remaining within the error bars, and comparative performance requiring a significant portion of the new training set (if I understood it correctly corresponding to hundreds of tractories from the altered system).\n\nInterventions/nonstationarities in parameters for dynamical systems can also be viewed through the lense of bifurcations. Increasing the Reynolds number can e.g. induce turbulence, as you rightly point out. From your description it is not altogether clear what changes in the dynamics your parameter changes in this section induce, and whether they lead to qualitative differences.\nSince to my understanding this transfer is the primary motivation for introducing your framework, I found this section to be a little too short and and not in-depth enough.\nI also wasn't 100% sure if you indeed jointly trained on different Reynolds numbers for the flow around a blunt body (Re = 100, 250, 500), which would be an interesting extension of your study in 6.2.\n\nComparisons and Experimental Evaluation\n \nAll datasets studied in the comparison methods are based on solutions to synthetic ODE systems, and include several PDEs. The claim that you provided evaluation on real-world datasets I find a little misleading, since none of the benchmarks are based on experimental/real world data, while of course being inspired by real systems.\n\nSelecting comparisons is always a debatable point, but since all benchmark systems chosen are relatively simple/polynomial form, other classes of comparisons could be valuable: approaches by Brunton et al. based on SINDy and its variants are also frequently studies similar fluid flows. Neural Fourier Operators, which you also mention in the related work have been particularly successful for PDEs, so they might be a better choice here.\n \nAlgorithms like ODE-RNN are tailored towards irregularly sampled, but not necessarily spatially extended data. ODE-RNN in general has very poor performance everywhere in the paper (see all figures including ODE-RNN, e.g. Fig 4 or Fig. J6 b), where they seem to have learned nothing meaningful at all, which indicates that there either is a bug in the training, or the method is not well suited as a comparison.\n\nOverall, since I think the general setting studied here is important and the presentation is good, if my concerns can be adressed I am also willing to adjust my final score."
                },
                "questions": {
                    "value": "Model parameters:\n\nHow parameter intense is your approach? Would it struggle to infer meaningful dynamics from only single or few trajectories, compared to approaches with a stronger inductive bias, such as SINDy (Brunton et al., 2015), or experimental settings, where there are often only single or few observed trajectories. The models in Table H2 have between 1 and 10 million parameters, and are trained on observations with 400 initial states each used for training, which hints at a data and parameter intese regime given some of the relatively simple underlying dynamical processes. How does your model behave for sparser data? Figure 6 indicates that for comparable performance on the pendulum data, comparable performance requires hundreds of observed trajectories?\n\nRelated Work: \n\nI understand that it's hard to provide an exhaustive overview over related methods, and I appreciate the extended related work. I however want to point out that there are many RNN based approaches specifically designed for inferring complex dynamics and leveraging inferred models for long-term time series forecasting (Reservoir Computing, see Pathak et al. 2018), LSTMs (Vlachas et al., 2018) or RNNs (see e.g. Hess et al., ICML 2023), and could be mentioned here as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4895/Reviewer_1fGG",
                        "ICLR.cc/2024/Conference/Submission4895/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666494448,
            "cdate": 1698666494448,
            "tmdate": 1700496290403,
            "mdate": 1700496290403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b1FYsNidXw",
                "forum": "EWTFMkTdkT",
                "replyto": "fiYbIbBICg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Meaning of invariances and generalization\\\nThank you for your thoughtful questions and we appreciate the opportunity to provide further clarification. Please see the overall response, which we think addresses these points. Regarding generalization: what we mean is that different instances can lead to very different data distributions and therefore cannot be handled by straightforward supervised learning, for instance.\n\nQ2: Boundaries of transfer learning using concepts of invariant measures\\\nIt would indeed be interesting to study the behaviour of LaDID's data driven approach on data from systems with known, specific, complex invariances. This goes beyond the scope of the present paper. We emphasize also that the invariances that we exploit are very general (and common to most areas of science), rather than rooted in mathematical properties of specific dynamical models.\n\nQ3: Performance of transfer learning / Joint experiment with different Reynolds numbers\\\nThank you very much for your thoughtful comment. However, we respectfully disagree with your conclusion regarding the perceived weakness of the transfer effect. In Section 6.2, we deliberately focus on two distinct types of interventions: (i) those on the parameters of the underlying system leading to a different trajectory and (ii) interventions on the perception of the dynamical system (noise or camera shifts). Our results in Section 6.2 strongly indicate that transfer learning, in both cases, exhibits good performance.\n    \nQualitative interventions on the dynamical system:\\\nFig. 6a illustrates that fine-tuning a pre-trained LaDID with just 8\\% of the data size (32 new trajectories) achieves a performance level that matches the best state-of-the-art competitor, MSVI, which was trained on a dataset 12.5 times larger for the new system (see Fig 3).\\\nTo address your requested transfer learning experiment at different Re-numbers, we now include a series of transfer learning experiments where we jointly train on a dataset with Re-numbers of $Re=[100, 250, 500]$ and present predictions for a set of Re-numbers of $Re=[100, 175, 250, 300, 400, 500]$. Please see Sec. P in the Appendix for detailed results.\n    \nInterventions upon the measurement process:\\\nFig. 6b presents the effects of interventions upon the camera position when training on a dataset with 160 trajectories and Re-numbers of $Re=\\{100, 250, 500\\}$, however the underlying flow are not affected by the interventions.  Furthermore, we added Sec. M to the Appendix to illustrate the effect of perturbations commonly occurring during measurements, e.g. noise or systematic perception errors.\n    \nIn our opinion, the results in Sec. 6.2 of the main text and Sec. M and P demonstrate a robust transfer effect. We hope that these additional details clarify the effectiveness of our framework in handling transfer learning tasks.\n\nQ4: Selection of baselines and experiments\\\nWe appreciate your perspective on the choice of datasets and comparison algorithms, and we are open to exploring new suggestions. It is correct that our datasets are simulations rather than experimental measurements. However, we carefully selected simulations for real-world systems across various disciplines, which play crucial roles in typical engineering tasks. These systems are both nontrivial and allow us to properly explore model behavior.\n\nRegarding the choice of baseline algorithms, we established two key requirements: (i) the algorithm must involve some form of latent representation, and (ii) it must be continuous in time. While requirement (i) is restrictive, we believe it is necessary to focus on approaches that include dimension reduction. For that reason, we intentionally excluded interesting approaches like SINDy, FNO, or GPODE, as they operate directly in the observational space. Nevertheless, we consider our chosen baseline algorithms as strong competitors. With the exception of ODE-RNN, they consistently demonstrate reasonable predictions in most test cases. We agree that the original application of ODE-RNN was not for spatio-temporal systems. Instead, we adapted it by using our encoder module to generate latent input embeddings and utilized the MSVI decoder for a meaningful comparison. While ODE-RNN may not be the ideal benchmark, we included it for its foundational theoretical and practical relevance to the other benchmarks.\n\nQ5: Performance with reduced number of training trajectories\\\nFor a more in-depth analysis, we have updated Section K.7 in the Appendix, introducing a series of experiments involving different numbers of training trajectories. The outcomes depicted in Fig. K7 demonstrate that utilizing only 16\\% of the training trajectories (64 trajectories) attains a performance level comparable to MSVI trained on the complete dataset. \n\nQ6: Extension of related work\\\nWe have added a dedicated section to the extended related work, where we discuss these relevant RNN-based approaches."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263047375,
                "cdate": 1700263047375,
                "tmdate": 1700263047375,
                "mdate": 1700263047375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OAWc5HxWzl",
                "forum": "EWTFMkTdkT",
                "replyto": "fiYbIbBICg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4895/Reviewer_1fGG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4895/Reviewer_1fGG"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed rebuttals and additions to the experimental evaluation and related work.\n \nSeveral referees agreed with the somewhat vague conception of the separation of RI and RS components. While the comments have aimed at clarifying this, I agree with Reviewer Fm8i that this should be elaborated on more clearly and give a concrete example, especially given the close similarity of the experimental section to the distinction \"model parameters\" as RI and \"initial states\" as RI.\n\nAs per Fm8i's comment, I also found the strong performance difference between methods a little hard to understand, since the selected benchmark systems seem fairly standard, and the proposed approach is not specifically tailored to excel at this kind of PDE learning.\n\nThis also connects my question w.r.t. the transfer learning effect between dynamical regimes, which is not so much about comparison to other approaches, as explored in the new figure in K7, but more about how well out-of-domain generalization works without retraining on significant portions of the novel train set, or even with zero-shot predictions.\nI appreciate the new experiments in section P, which go in this direction. I completely understand that new results hard to achieve during the revision under time pressure, but the way they are currently presented I find them a little difficult to interpret. Parts of Fig. P1b) are basically impossible to read, and a) I find a little challenging to interpret. Given as I mentioned I think this might be one of the more important applications of your framework, I would appreciate a clearer treatment and analysis in case the paper is accepted.\n\nIn light of the improvements and detailed feedback I am also happy to adjust my score and tend towards accepting."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496217714,
                "cdate": 1700496217714,
                "tmdate": 1700496390869,
                "mdate": 1700496390869,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ApDmLhti3d",
            "forum": "EWTFMkTdkT",
            "replyto": "EWTFMkTdkT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4895/Reviewer_apHo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4895/Reviewer_apHo"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript introduces a novel deep trajectory inference model, referred to as LaDID. This model designed to predict dynamical trajectories by disentangling invariant and variant dynamical factors. LaDID employs a convolutional autoencoder unit for dimension reduction and a transformer unit for learning temporal structure, eliminating the need for ODE formulations and enabling efficient long trajectory prediction without added computational costs.\n\nIn the study, the authors evaluated LaDID's performance using six simulated datasets. They conducted a comprehensive comparison of LaDID with four ODE-based models, namely ODE-RNN, ODE2VAE, NDP, and MSVI. The results demonstrated LaDID's superiority in all experiments, including challenging few-shot learning tasks.\n\nI appreciate the problem tackled by the author and find the concept of invariant-latent representation learning in dynamical systems intriguing. The paper appears to have a solid metric in place. Nevertheless, there are certain crucial aspects that need to be improved. I am open to potentially revising my assessment following the author's rebuttal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The manuscript tackles an intriguing and challenging problem related to trajectory inference in dynamical systems. The proposed approach involves effectively decomposing the latent factors into variant and invariant components, which is fairly innovative.\n\n- The authors conducted a comprehensive performance evaluation across six complex dynamical systems. \n\n- The proposed model consistently outperforms its related counterparts across a wide range of tasks. \n\n- The proposed model showcases capabilities in handling long trajectories, even when provided with limited training samples. This adaptability to data constraints is a significant strength, as it opens up opportunities for more efficient and practical application in dynamical system modeling."
                },
                "weaknesses": {
                    "value": "**Major comment:**\n\nMy prominent concern in the paper pertains to the insufficient explanation for why their model outperforms others. The main contribution of the paper lacks clear description and justification. As pointed out by the authors, both LaDID and MSVI (Iakovlev et al., 2023) share several similarities, such as using CNN encoders/decoders and the same transformer. The primary distinction between these models lies in replacing neural ODE with MLPs to learn $f_{\\phi_{dyn}}$ and introducing a smoothness loss to discourage abrupt transitions between latent subpatches. It remains unclear how these modifications enhance the process of learning latent dynamics in comparison to ODE-based methods.\n\n**Minor Points:**\n\n- Figure 1 could benefit from a more comprehensive overview of their framework. Clarifying how each process in the top row corresponds to each step in the bottom row would enhance understanding.\n- The paper's writing can be improved, and some content from the Appendix may be integrated into the main text.\n- The descriptions of the experiments are somewhat confusing and could benefit from increased clarity and explanation."
                },
                "questions": {
                    "value": "- If I understand correctly, the paper addresses only the initial state as the specific (variant) trajectory factor, right? What about other sources of variability, like measurement noise?\n\n- In section 4.1, Model, the main text says *\"To obtain a latent trajectory,  ... we choose as a set of different sine and cosine waves with different wave length.\"* Could you explain it? \n\n- Could you please explain the choice of using MSE/RMSE for evaluation? It seems that capturing the overall structure of dynamics (direction, shape, etc.) might be more crucial than the absolute amount of misprediction.\n\n- Can LaDID effectively handle time-variant dynamical systems? If yes, how?\n\n- In the graphical model (Figure A1), $\\psi^r$ is out of the box \"$N$\". Then, how is $\\psi^r$ a function of \"$n$\" in Eq. 3 and Eq. 9?\n\n- The text mentions *\" ... mean-aggregation, which can be changed based on the task at hand.\"* Could you elaborate on the role and task-dependent nature of mean-aggregation?\n\n- In Eq. 9, within the representation prior term, none of the variables appear to be a function of \"$n$.\" Can you clarify this discrepancy?\n\n- What does \"$D$\" represent in Figure A1?\n\n- The authors stated that *\"our models are specifically designed to exploit certain invariances that are important in classical scientific models.\"* Could you provide more detail on these \"certain invariances\"?\n\n- Beyond classical scientific examples, can LaDID effectively learn developmental trajectories involving bifurcations and other complex phenomena?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783206161,
            "cdate": 1698783206161,
            "tmdate": 1699636473896,
            "mdate": 1699636473896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gbB5LvwmYa",
                "forum": "EWTFMkTdkT",
                "replyto": "ApDmLhti3d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4895/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Unclear how modifications enhance learning of latent dynamical systems\\\nPlease find a fuller account of how our approach enhances the learning of latent dynamics in the overall response above. We clarify there why our approach is different from e.g. MSVI, and explain the invariances in more detail. We emphasize that they are general properties that we think hold for a variety of systems rather than specific invariances derived from equations for a particular system.\n\nQ2: Fig 1 - more comprehensive overview\\\nThank you for the suggestion; we have modified figure 1 to improve the visualization of our network.\n\nQ3: Improved writing, add parts from App. to MS\\\nWe are highly motivated to improve the clarity of our work and would appreciate any direct feedback on unclear sections. However, we are constrained by the strict page limit, which limits the amount of additional content we can include.\n\nQ4: Improved Exp. Description\\\nDue to the page limit, we have added the Sec. J in the Appendix to provide a more comprehensive description of the experiments and hope that the updated Appendix addresses your concerns.\n\nQ5: Measurement noise\\\nTo demonstrate the robustness to noise, we provide evaluation metrics for various noise levels added to the mean of our experiments, e.g. Gaussian noise ($\\sigma = [0.1, 0.2, 0.3, 0.4]$) and salt-and-pepper noise ($\\rho = [10\\\\%, 35\\\\%, 50\\\\%, 70\\\\%]$). The results are presented in Sec. M1 and M2 of the Appendix. We note also that within our loss-based framework, measurement and process noise are, in a way, handled under a unified overall objective.\n\nQ6: Encoding of time queries\\\nWe stack the sin/cos of the first 16 integer frequencies contained in the observed time interval similar to a Fourier series yielding a 32-dimensional relative time encoding.\n\nQ7: Why MSE for evaluation?\\\nWe agree that time-averaged RMSE values do not capture all relevant information and can easily yield misleading interpretation. Therefore, we consider three metrics including the time-averaged nMSE, the RMSE over time and the visualization of the pixelwise $L2$-error (see Figs J1-6) allowing us to precisely study the accuracy of the predicted dynamics.\n\nQ8: Can LaDID effectively handle time-variant dynamical systems?\\\nSince we have not conducted experiments for such systems, we are hesitant to make claims about the effectiveness of LaDID in this context. However, we are optimistic that LaDID could be effective for some such systems, provided suitable higher-level regularities exist. Here, a key point is that LaDID does not pre-suppose any particular kind of model but rather the high-level properties detailed above, which can hold for some time-varying systems. The observed generalization performance in Section 6.2 suggests that LaDID may have the potential to handle time-varying systems. Further investigation will be required to address this question in future work.\n\nQ9: Notation in graphical model vs. eq. 3 and 9\\\nThank you for bringing this to our attention. It is important to distinguish between the conceptual representation in Fig. A1 and the implementation of the LaDID model. In Fig. A1, we illustrate the overarching concept, indicating that for each trajectory, we generate a unique $\\psi^r$ and subsequently roll out the latent trajectory. The plate notation $N$ represents the length of the requested trajectory points. In eq. 3 and 9, the variable $N$ denotes the number of subtrajectories, with the understanding that the concept outlined above holds true for each subtrajectory. We recognize the potential confusion and updated the notation in Figure A1 and its caption for clarity.\n\nQ10: Mean-aggregation and possible alternatives\\\nIn the proposed implementation we choose the mean for sake of simplicity, but many different aggregations appear possible. One counterpart which we want to investigate in future work is the replacement of the mean by a singular value decomposition. Since the derived embeddings of the $k$ input samples are low-dimensional, a SVD may be cost efficient in extracting coherent data structures across all embeddings. Another promising replacement may be the introduction of a RNN where we want to use its final hidden state as general initial condition representation. \n\nQ11: Dependence of prior on subtrajectory\\\nAll priors are computed per subtrajectory and thus a function of $n$ which we assumed to be clear from the context, but for clarity, we have added the dependence to the equations.\n\nQ12: Graphical model - plate notation D\\\nIn Fig. A1, D refers to the total number of the training samples. We amended the figure caption to clarify this aspect.\n\nQ13: Application to bifucations and other phenomena\\\nWe appreciate your curiosity in this regard but at the stage, we lack evidence to make statements about LaDID's effectiveness in learning such dynamics. We plan to conduct thorough investigations into the performance of LaDID in such scenarios in future."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262604798,
                "cdate": 1700262604798,
                "tmdate": 1700262604798,
                "mdate": 1700262604798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DudHCjh2xJ",
                "forum": "EWTFMkTdkT",
                "replyto": "gbB5LvwmYa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4895/Reviewer_apHo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4895/Reviewer_apHo"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the author's efforts in presenting a compelling rebuttal alongside additional experiments. The supplementary experiments, particularly section M, offer intriguing insights into the model's capabilities.\n\nHowever, before finalizing my decision, I feel it's crucial to revisit my primary concern regarding the outperformance of the proposed model. Despite the comprehensive responses addressing various aspects in the overall response, I still seek clarification on how LaDID significantly enhances learning ODE/PDE-based dynamics compared to the previous ODE-based approach. I fully acknowledge the potential for isolating invariant structures/mechanisms and the emphasis on the RS-RI decomposition to enhance inference. However, it remains hard to attribute all improvements solely to this factorization without demonstration of the level of decomposition achieved.\n\nEchoing the concerns of other reviewers, I believe the authors should provide quantitative evidence to support the model's superior performance by quantifying the extent of disentanglement between RI and RS. For a given experiment, explicitly defining RS and RI parameters and demonstrating how effectively the model infers these parameters would significantly strengthen the argument for the model's effectiveness."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531967931,
                "cdate": 1700531967931,
                "tmdate": 1700531967931,
                "mdate": 1700531967931,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]