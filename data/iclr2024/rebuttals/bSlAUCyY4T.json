[
    {
        "title": "Knowledge Graph Completion by Intermediate Variables Regularization"
    },
    {
        "review": {
            "id": "v4SwGMb9I9",
            "forum": "bSlAUCyY4T",
            "replyto": "bSlAUCyY4T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9265/Reviewer_YFRf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9265/Reviewer_YFRf"
            ],
            "content": {
                "summary": {
                    "value": "Tensor Decomposition-Based (TDB) models have been quite successful in the field of KGC. Despite the effectiveness of TDB models, they are susceptible to overfitting. Existing regularization methods typically focus on minimizing the norms of embeddings to control the model, resulting in suboptimal performance. To overcome this limitation, the authors introduce a novel regularization method tailored for TDB models. This regularization method can be applied to most TDB models, seamlessly integrating existing regularization techniques while maintaining computational efficiency.\n\nThe core of this method involves minimizing the norms of intermediate variables used in various ways to compute the predicted tensor. To support their proposed regularization approach, the authors provide a theoretical analysis that demonstrates its effectiveness in reducing overfitting by promoting a low trace norm of the predicted tensor."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The article is well-structured with detailed analysis and clear logic. The language is fluent and reader-friendly. The new regularization approach for TDB models demonstrates good performance."
                },
                "weaknesses": {
                    "value": "The first contribution, which is a detailed overview of a wide range of TDB models, is somewhat limited in its actual content.\nThe experimental results could benefit from a more comprehensive comparison involving additional metrics such as efficiency and time-related measures."
                },
                "questions": {
                    "value": "Could the authors further explain the difference between IVR and IVR-3 in Table 3? Need to clarify the metrics in the experiments. What do the results mean, like accuracy, error, or anything else?\n\nUnclear meaning of the notation of (i, j, ?) above Section 3.3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9265/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9265/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9265/Reviewer_YFRf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9265/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634498426,
            "cdate": 1698634498426,
            "tmdate": 1699637167074,
            "mdate": 1699637167074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dCw0GRSzR2",
                "forum": "bSlAUCyY4T",
                "replyto": "v4SwGMb9I9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9265/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your careful and constructive comments. We have addressed the questions that you raised as follows. Please let us know if you have any further concerns.\n\n**Q1: The first contribution, which is a detailed overview of a wide range of TDB models, is somewhat limited in its actual content.**\n\nA1: We propose a general form of tensor decomposition based (TDB) models for KGC, which serves as a foundation for further analysis and exploration of TDB models. The general form presents a unified view of TDB models and helps the researchers understand the relationship between different TDB models. Moreover, the general form motivates the researchers to propose new methods and establish unified theoretical frameworks that are applicable to most TDB models. Our proposed regularization method and the theoretical analysis in Section 3.3 are examples of such contributions.\n\n**Q2: The experimental results could benefit from a more comprehensive comparison involving additional metrics such as efficiency and time-related measures.**\n\nA2: We present more detailed experimental results in Appendix C. Since these regularizations are all composed of $L^p$ norms, which are all efficient, they do not affect the model efficiency significantly. The most important factor that affects the model efficiency is the number of parts $P$. Since models with different regularizations have the same value of $P$, we do not need to compare their efficiency.\n\nAs we discussed in Section 3.1 Paragraph \u201cThe Number of Parameters and Computational Complexity\u201d, the computational complexity is related to the number of parts $P$. We further provide an efficiency analysis in Appendix C Paragraph \u201cThe Number of Parts\u201d. The results show that the model performance generally improves and the running time generally increases as P increases. Thus, the larger the part $P$, the more expressive the model and the more the computation.\n\n**Q3: Could the authors further explain the difference between IVR and IVR-3 in Table 3?**\n\nA3: We defined IVR-3 and IVR as two variants of Eq.(4) with different constraints on the regularization coefficients $\\lambda_{i}$ in Section 3.2. Specifically, IVR is Eq.(4) with the constraint $\\lambda_{1}=\\lambda_{3}, \\lambda_{2}=\\lambda_{4}$, while IVR-3 is Eq.(4) with the constraint $\\lambda_{1}=\\lambda_{2}=\\lambda_{3}=\\lambda_{4}$. Therefore, IVR has two degrees of freedom in choosing the regularization coefficients, while IVR-3 has only one.\n\n**Q4: Need to clarify the metrics in the experiments. What do the results mean, like accuracy, error, or anything else?**\n\nA4: We have provided more detailed experimental results in Appendix C, including the evaluation in the second paragraph of Appendix C. The evaluation metrices is commonly used in KGC. Besides, we analyze the impact of random initialization, the impact of the hyper-parameters and the number of parts in Appendix C.\n\n**Q5: Unclear meaning of the notation of (i, j, ?) above Section 3.3.**\n\nA5: We use the notation $(i, j, ?)$ to represent a query that asks for the most likely tail entities given a head entity $i$ and a relation $j$. This notation is commonly used in knowledge graph completion tasks. We will make the notation clearer in the revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258777081,
                "cdate": 1700258777081,
                "tmdate": 1700258777081,
                "mdate": 1700258777081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I5CRLxp6xo",
                "forum": "bSlAUCyY4T",
                "replyto": "dCw0GRSzR2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9265/Reviewer_YFRf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9265/Reviewer_YFRf"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate your efforts in addressing my concerns."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671423936,
                "cdate": 1700671423936,
                "tmdate": 1700671423936,
                "mdate": 1700671423936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O8iNsGY05h",
            "forum": "bSlAUCyY4T",
            "replyto": "bSlAUCyY4T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9265/Reviewer_e2Qz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9265/Reviewer_e2Qz"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of overfitting in Tensor Decomposition-based (TDB) models used in Knowledge Graph Completion (KGC), where existing regularization methods, focused on minimizing the norms of embeddings, have led to suboptimal performance. A novel regularization method is proposed that minimizes the norms of intermediate variables involved in computing the predicted tensor, enhancing most TDB models by incorporating existing regularization techniques and ensuring tractable computation. Moreover, extensive evaluation verifies the effectiveness and superiority of the model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe authors propose a new KGC framework, establishing a general form to serve as a foundation for further TDB model analysis. which aims to tackle the issue of overfitting in Tensor Decomposition-based models.\n2.\tThe authors provide a detailed theoretical analysis, e.g., showing the ability for its generality and effectiveness, which guarantees the validity of the method."
                },
                "weaknesses": {
                    "value": "1. The paper is not organized clearly, which is not friendly for understanding. For instance, there are lack of an intuitive explanation for how this method can mitigate the overfitting issue.\n2. The regularization approach as shown in Eq.(4) is too complex to use. Moreover, this paper miss some strong baselines such as [1][2]\n[1] Low-Dimensional Hyperbolic Knowledge Graph Embeddings\n[2] ER: equivariance regularizer for knowledge graph completion"
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9265/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678325101,
            "cdate": 1698678325101,
            "tmdate": 1699637166964,
            "mdate": 1699637166964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wDIKfivmro",
                "forum": "bSlAUCyY4T",
                "replyto": "O8iNsGY05h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9265/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your careful and constructive comments. We have addressed the questions that you raised as follows. Please let us know if you have any further concerns.\n\n**Q1: The paper is not organized clearly, which is not friendly for understanding. For instance, there are lack of an intuitive explanation for how this method can mitigate the overfitting issue.**\n\nA1: We have explained the intuition of our regularization from two perspectives. Previous works only minimize the norms of the embeddings to prevent overfitting, while our proposed regularization IVR minimizes the norms of the intermediate variables involved in the processes of computing the predicted tensor, which can further reduce the complexity of the model to prevent overfitting.\n\nFrom the perspective of theoretical analysis, we further propose a theoretical framework to explain how the model mitigate the overfitting issue. We prove that our regularization IVR can minimize the Frobenius norms of the intermediate variables to minimize the overlapped trace norm of the predicted tensor as shown in Section 3.3. The overlapped trace norm can be a measure of the complexity of the predicted tensor, and minimizing it can regularize the model and prevent overfitting. We first prove Lemma 1 in Appendix B, which relates the Frobenius norm and the trace norm of a matrix. Lemma 1 shows that the trace norm of a matrix is an upper bound of a function of several Frobenius norms of intermediate variables. Based on Lemma 1, we then prove that IVR serves as an upper bound for the overlapped trace norm of the predicted tensor, which promotes the low nuclear norm of the predicted tensor to regularize the model, which is known to reduce overfitting.\n\nWe will make this statement clearer in the revision. And we want to highlight that the other three reviewers appreciate the clarity of our paper. We welcome any suggestions to improve the organization of our paper further.\n\n**Q2: The regularization approach as shown in Eq.(4) is too complex to use.**\n\nA2: Our regularization IVR is not complex to use. First, IVR is composed of several Frobenius norms of variables, which are simple to compute. Second, we provide a pseudocode for computing IVR in Appendix C. The pseudocode shows that IVR can be computed incidentally during the process of computing the predicted tensor $X$. Third, we also provide a source code for IVR in the Supplementary Material. The model.py file shows that the implementation of IVR is concise and straightforward. In summary, IVR is not complex to use and implement.\n\n**Q3: This paper misses some strong baselines such as [1][2]. [1] Low-Dimensional Hyperbolic Knowledge Graph Embeddings. [2] ER: equivariance regularizer for knowledge graph completion.**\n\nA3: In this paper, we mainly focus on tensor decomposition-based (TDB) models, which have wide applicability and great performance in knowledge graph completion. Therefore, we mainly include the TDB models as baselines for fair comparison. We will make the baselines more complete in the revision.\n\n**We would like to emphasize the main contributions of our paper as follows:**\n1. We propose a general form for TDB models, which provides a unified view of existing TDB models and serves as a foundation for further analysis and exploration.\n2. We introduce a novel regularization method for TDB models based on our general form to mitigate the overfitting issue. Our regularization method is notable for its generality and effectiveness, as it can be applied to most TDB models and can improve their performance significantly. Previous regularization methods either have poor performance (such as F2) or are not general enough (such as N3 and DURA), while our method overcomes both limitations. Table 1 shows the experimental results that demonstrate the effectiveness and generality of our method.\n3. We provide a general theoretical framework for most TDB models in Section 3.3, which proves that our regularization method can promote low trace norm of the predicted tensor, which is known to reduce overfitting. In contrast, N3 and DURA only provide theoretical analysis for models based on CP decomposition, which limits their applicability."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258396115,
                "cdate": 1700258396115,
                "tmdate": 1700258396115,
                "mdate": 1700258396115,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WRHQaTRvho",
            "forum": "bSlAUCyY4T",
            "replyto": "bSlAUCyY4T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9265/Reviewer_Bhgm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9265/Reviewer_Bhgm"
            ],
            "content": {
                "summary": {
                    "value": "The knowledge graph (KG) can be represented as a 3-way tensor. Hence, the knowledge graph completion can be naturally formed as a tensor completion task. In this paper, the author provided an overview of the tensor decomposition-based (TDB) models and derived a general form which is the summation of $D/P$ Tucker decompositions with shared core tensor $\\boldsymbol{W} \\in \\mathbb{R}^{P \\times P \\times P}$. The main contribution of this paper is the proposed regularization term, which is applicable to the `general form' they proposed, by incorporating the existing regularization methods. The author claims that their novel regularization term minimizes the norms of intermediate variables for promoting low trace norm of predicted tensor. The theoretical analysis and experiments were provided to verify their claim."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n1. Easy to read.\n2. They provided numerical experiments on different datasets and methods."
                },
                "weaknesses": {
                    "value": "Weaknesses:\nThe main focus should be the regularization term they proposed instead of the general form. The weakness is novelty and significance, which is limited by my point of view.\n1. For the theoretical side, the nuclear p-norms and squared F-norm methods were well-studied. It's not a surprise that Equation 4 can generalize these, and this is not new. As the author claimed, they incorporated the N3 norm and DURA, where some of the analysis was established. Their weight IVR norm is not theoretically novel and significant. Could the author clarify what is the unique hardness and novelty of this proposed IVR norm? \n\n\n2. For the experiment side, as the author claimed in the paper, their method promotes low rank. Do you actually see the zero columns in the factors, compared to the existing method?"
                },
                "questions": {
                    "value": "My question was put in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9265/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778960003,
            "cdate": 1698778960003,
            "tmdate": 1699637166843,
            "mdate": 1699637166843,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q7yli5sMFX",
                "forum": "bSlAUCyY4T",
                "replyto": "WRHQaTRvho",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9265/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your careful and constructive comments. We have addressed the questions that you raised as follows. Please let us know if you have any further concerns.\n\n**Q1: The main focus should be the regularization term they proposed instead of the general form.**\n\nA1: We agree that the main contribution of our paper is the regularization term, but we also argue that the general form is necessary, as it serves as a foundation for our further analysis. Without the general form, we cannot guarantee the generality of our regularization term and the theoretical analysis, as they depend on the unified view of the TDB models provided by the general form.\n\n**Q2: Their weight IVR norm is not theoretically novel and significant. Could the author clarify what is the unique hardness and novelty of this proposed IVR norm?**\n\nA2: Our proposed regularization is notable for its generality and effectiveness, as it can be applied to most TDB models and can improve their performance significantly. Although some of the theoretical analysis was established for F2, N3 and DURA, it is still challenging to design a regularization method that has both the properties of generality and effectiveness. As previous regularization methods either have poor performance (such as the F2 norm method, which achieves suboptimal results) or are not general enough (such as the N3 norm method and the DURA method, which are only suitable for CP and ComplEx models), while our method overcomes both limitations. Table 1 shows the experimental results that demonstrate the effectiveness and generality of our method. Our method can be applied to various TDB models, such as CP, ComplEx, SimplE, ANALOGY, QuatE, TuckER, etc., and achieve SOTA results. For example, TuckER with our method achieves new SOTA results on the WN18RR dataset.\n\nMeanwhile, we provide a general theoretical framework for most TDB models in Section 3.3, which proves that our regularization method can promote low trace norm of the predicted tensor, which is known to reduce overfitting. Previous works only provide theoretical analysis for CP and ComplEx models, which limits their applicability. We discover the relationship between the overlapped trace norm and our proposed IVR regularization, and show that IVR serves as an upper bound for the overlapped trace norm. We also verify the validity of our theoretical analysis experimentally in Section 4.4. You can also refer to A3 of the rebuttal for Reviewer TVtg.\n\n**Q3: For the experiment side, as the author claimed in the paper, their method promotes low rank. Do you see the zero columns in the factors, compared to the existing methods?**\n\nA3: We argue that a low rank matrix does not necessarily imply sparse embeddings, as a low rank matrix can have non-zero columns. For example, consider a matrix $X$ whose entries are all 1. The rank of $X$ is 1, but $X$ has no zero columns. Therefore, a better metric to measure the rank of a matrix is the overlapped trace norm, which is a convex surrogate of the rank. We experimentally verify that minimizing our proposed regularization can effectively minimize the overlapped trace norm $L(X)$ in Section 4.4. To make the experiments more detailed, we further show the results of different regularizations in the following Table 1, which demonstrate that our method is better at promoting low rank than other methods.\n\nTable 1: The results on Kinship dataset with different regularizations. Larger MRR and lower L(X) are better.\n| | MRR| L(X)|\n|  ----  | ----  | ----  |\n| CP | 0.894 | 26439 |\n| CP-F2 | 0.900 | 25934 |\n| CP-N3 | 0.900 |21801 |\n| CP-DURA |0.901| 18641 |\n| CP-IVR | 0.907 | 15366 |\n\nWe further analyze the sparsity of embeddings induced by different regularizations as below. Generally, there are few entries of embeddings that are exactly equal to 0 after training, which means that it is hard to obtain sparse embeddings directly. To get sparse embeddings, we can set a proper threshold and set the absolute value of the embedding entries less than the threshold to be exactly 0. Denote the ratio of zero entries as $\\alpha$, then we can compare the performance of different models under the same $\\alpha$. The following Table 2 shows that our regularization CP-IVR can achieve better performance than other regularizations under the same sparsity circumstance.\n\nTable 2: The MRR results of models with different sparsity. Larger MRR is better.\n| $\\alpha$ | 0%| 20%| 40%| 60%| 80%|\n| ---- | ---- | ---- | ----  | ---- | ---- |\n| CP | 0.438 | 0.438 | 0.431 | 0.420 | 0.388 |\n| CP-F2 | 0.449 | 0.449 | 0.447 | 0.445 | 0.424 |\n| CP-N3 | 0.469 | 0.468 | 0.465 | 0.457 | 0.426 |\n| CP-DURA | 0.471 | 0.471 | 0.469 | 0.465 |0.440 |\n| CP-IVR | 0.478 | 0.478 | 0.475 |0.471 | 0.445 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258006292,
                "cdate": 1700258006292,
                "tmdate": 1700258006292,
                "mdate": 1700258006292,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pGh6cZFYES",
                "forum": "bSlAUCyY4T",
                "replyto": "Q7yli5sMFX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9265/Reviewer_Bhgm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9265/Reviewer_Bhgm"
                ],
                "content": {
                    "title": {
                        "value": "Response to the author"
                    },
                    "comment": {
                        "value": "The reviewer thanks the author for the clarification. \nFor the Q3, I believe for the CPD, since it's constraining rank-one tensors, you should see zero columns in the factors if you actually promote low-rank."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494613117,
                "cdate": 1700494613117,
                "tmdate": 1700494613117,
                "mdate": 1700494613117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZqKYfvPwim",
            "forum": "bSlAUCyY4T",
            "replyto": "bSlAUCyY4T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9265/Reviewer_TVtg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9265/Reviewer_TVtg"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the tensor-decomposition based (TDB) methods on knowledge graph completion. It proposes a general form to unify previous TDB methods and proposes a new regularization to serve as a upper bound of previous regularizations. The proposed regularization is evaluated on three knowledge graph completion dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is very well-written. The reviewer enjoys reading the paper.\n\n2. The proposed generic form of TDB unifies previous TDB-based KGC methods, which is not significant but a useful summary of the development of TDB-based methods on KGC. It would make the paper more useful if the authors could explain in detail how this generic form could benefit the community.\n\n3. The reviewer appreciates the extensive evaluation on three datasets."
                },
                "weaknesses": {
                    "value": "1. First, the experimental results look marginal. It might indicate that the new regularization is useful to prevent overfitting since it is a upper bound. However, what if we increase the coefficients of regularization in the baseline models? The experimental results does not show the effectiveness of the proposed new regularization.\n\n2. Second, the proposed new regularization lacks motivation. The paper proves that the proposed new regularization is an upper bound of previous overlapped trace norm. However, the motivation of this new proposal is not well justified. Why an upper bound of the regularization is better in the optimization? It does not make sense to me.\n\n3. The loss function in Section 3.2 is not well explained (Sorry, I do not see the parameters in the loss function). What are the parameters? W, H, T? I will assume $X$ indicates both the ground truth tensor value and the parameterized tensor value from W, H, T? Also, in the KGC applications, the loss function should have tensor mask to indicate the observed entries."
                },
                "questions": {
                    "value": "See above. \n\nThe reviewer likes the paper, however, the contributions and the usefulness of the proposal should be elaborated (the current version does not satisfy ICLR bar). I am willing to raise the score for further explanations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9265/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817301186,
            "cdate": 1698817301186,
            "tmdate": 1699637166718,
            "mdate": 1699637166718,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VALCUhYlaA",
                "forum": "bSlAUCyY4T",
                "replyto": "ZqKYfvPwim",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9265/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your careful and constructive comments. We have addressed the questions that you raised as follows. Please let us know if you have any further concerns.\n\n**Q1: Could the authors explain in detail how the general form benefits the community?**\n\nA1: We propose a general form of tensor decomposition based (TDB) models for KGC, which serves as a foundation for further analysis and exploration of TDB models. The general form presents a unified view of TDB models and helps the researchers understand the relationship between different TDB models. Moreover, the general form motivates the researchers to propose new methods and establish unified theoretical frameworks that are applicable to most TDB models. Our proposed regularization and the theoretical analysis are examples of such contributions.\n\n**Q2: What if we increase the coefficients of regularization in the baseline models?**\n\nA2: We have shown the results of experiments with respect to the hyper-parameters in Appendix C. We analyze the impact of the hyper-parameters, the power of the Frobenius norm $\\alpha$ and the regularization coefficient $\\lambda_i$ in Appendix C (Table 6 to Table 9). The results show that the performance generally increases as $\\alpha$/$\\lambda_i$ increases and then decreases as $\\alpha$/$\\lambda_i$ increases. The results indicate the effectiveness of the hyper-parameters.\n\n**Q3: The proposed regularization lacks motivation. Why an upper bound of the regularization is better in the optimization?**\n\nA3: The motivation of our regularization IVR is that we can minimize the Frobenius norms of the intermediate variables (involved in the processes of computing the predicted tensor) to minimize the overlapped trace norm of the predicted tensor. The overlapped trace norm can be a measure of the complexity of the predicted tensor, and minimizing it can regularize the model and prevent overfitting. To establish a theoretical framework for IVR, we prove Lemma 1 in Appendix B, which relates the Frobenius norm and the trace norm of a matrix. Lemma 1 shows that the trace norm of a matrix is an upper bound of a function of several Frobenius norms of intermediate variables. Based on Lemma 1, we prove that IVR serves as an upper bound for the overlapped trace norm of the predicted tensor.\n\nFrom another view, since computing the trace norm has a high computational complexity of $\\mathcal O(n^3)$, which is impractical for large knowledge graphs, our regularization IVR is composed of several Frobenius norms, which are computationally efficient. We prove that minimizing IVR can effectively minimize the overlapped trace norm, and we also verify this experimentally in Section 4.4. Thus, IVR serves as a good surrogate of the overlapped trace norm.\n\n**Q4: What are the parameters in the loss function?**\n\nA4: We can substitute Eq.(1) into the loss function to see the parameters. The parameters come from two parts, the core tensor $W$ and the embedding matrices $H$, $R$ and $T$. Please refer to Section 3.1 Paragraph \u201cThe Number of Parameters and Computational Complexity\u201d for more details.\n\n**Q5: The loss function should have a mask tensor to indicate the observed entries.**\n\nA5: We do not need a mask tensor to distinguish the observed and unobserved entries. We can treat every observed entry as a positive sample, and every unobserved entry as a negative sample. Then the loss function maximizes the score of the observed entries (i.e., $X_{ijk}$) and minimizes the score of the unobserved entries (i.e., $X_{ijk'}$, where $k'\\neq k$). In other words, the loss function implicitly incorporates the mask tensor. Therefore, we do not need to explicitly use a mask tensor.\n\n**Q6: The contributions and the usefulness of the proposal should be elaborated.**\n\nA6: The main contributions of our paper are in 3 folds:\n\n1. We propose a general form for TDB models, which provides a unified view of existing TDB models and serves as a foundation for further analysis and exploration. You can also refer to A1.\n2. We introduce a novel regularization method for TDB models based on our general form to mitigate the overfitting issue. Our regularization method is notable for its generality and effectiveness, as it can be applied to most TDB models and can improve their performance significantly. Previous regularization methods either have poor performance (such as F2) or are not general enough (such as N3 and DURA), while our method overcomes both limitations. Table 1 shows the experimental results that demonstrate the effectiveness and generality of our method.\n3. We provide a general theoretical framework for most TDB models in Section 3.3, which proves that our regularization method can promote low trace norm of the predicted tensor, which is known to reduce overfitting. In contrast, N3 and DURA only provide theoretical analysis for models based on CP decomposition, which limits their applicability. You can also refer to A3."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257319497,
                "cdate": 1700257319497,
                "tmdate": 1700257319497,
                "mdate": 1700257319497,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]