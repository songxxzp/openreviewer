[
    {
        "title": "Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance"
    },
    {
        "review": {
            "id": "LUNCOZk967",
            "forum": "yAcLwJu9qs",
            "replyto": "yAcLwJu9qs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6223/Reviewer_wHpz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6223/Reviewer_wHpz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a visually-continuous corruption robustness (VCR) metric based on the visual information fidelity (VIF) metric. Furthermore, the authors propose two human-aware metrics HMRI and MRSI. The key message is that the gap between neural network robustness and human robustness is larger than expected. Authors have conducted experiments with 14 different image corruption techniques with 7718 human participants and different SOTA neural networks models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-motivated and the experiments are very extensive\n- The problem discussed is important and interesting\n- Implementation and data was made available by the authors"
                },
                "weaknesses": {
                    "value": "- The paper is very hard to read and necessary background is not introduced. For example, I would have liked an explanation what visual information fidelity is. Overall, there is a lot of content squeezed in the 9 pages which makes the paper mostly incomprehensible.\n- Due to above issue, I strongly suggest to publish the paper in a journal (which usually have no page limits). The quality of the write-up would highly benefit from this.\n- Section 2 (Methods) needs a major rewrite to make it more accessible to readers not familiar with image quality metrics. Here are some points that can be improved:\n    1. The section mentions multiple times that $\\Delta_v \\in [0,1]$. \n    2. The variable c is used before it was defined.\n    3. Authors should stress that $\\Delta_v$ is just an auxiliary quantity that later is used to define the VCR.\n    4. Authors could consider adding a figure to give an overview of the used and introduced metrics. As a reader, I was overwhelmed by all these acronyms. An overview would have been very helpful.\n- I very much appreciate that the authors shared their code, however I find it inappropriate to refer to it as a \"toolbox\". In my opinion a toolbox is an installable Python package that is easily applicable to different models and datasets. Authors have to spend more time on their code repository before calling it a \"toolbox\".\n\nMinor details:\n- page 2: $max$ should be $\\max$\n- Tbl. 2 -> Tab. 2\n- typo page 4: \u201ccoverages= of\u201d and \u201c[0..1]\u201d (should be [0,1])\n- Two paragraphs in the abstract look unusual\n- Suggestions: \"Uniform(0,1)\" -> \"U(0,1)\""
                },
                "questions": {
                    "value": "- What is impulse noise or glass blur?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "details_of_ethics_concerns": {
                    "value": "The paper proposes a study with human participants (Mechanical Turk platform). Whether platforms like this are ethical or not is debatable. It is probably ok, but it would be good if someone familiar with the ICLR ethics guidelines would look into this."
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743899511,
            "cdate": 1698743899511,
            "tmdate": 1699636679360,
            "mdate": 1699636679360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R8NZE8Sy6y",
                "forum": "yAcLwJu9qs",
                "replyto": "LUNCOZk967",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wHpz"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's comments on the difficulty to understand the text. We have revised Section 2 (Methods) as suggested, and replaced \"toolbox\" by \"code\".\n\nMost importantly, we have also created a visual summary of the VCR metrics in Section 8.4, and referred to the figures from Section 2.\n\nFurther, we have added a more precise, but still high-level description of how VIF is computed, using wavelet decomposition and mutual information, in Section 8.5. The detailed formulas could easily fill an entire page but are available in the original paper [Sheikh & Bovik, 2006].\n\nFinally, regarding impulse noise or glass blur: impulse noise is a color analogue of salt-and-pepper noise, and glass blur simulates frosted glass windows or panels. Both corruptions are included in ImageNet-C. All of the corruptions in our study are visualized in Figure 10 in Section 8.6 of the appendix."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695010407,
                "cdate": 1700695010407,
                "tmdate": 1700695010407,
                "mdate": 1700695010407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4mqjFZnImE",
            "forum": "yAcLwJu9qs",
            "replyto": "yAcLwJu9qs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6223/Reviewer_ZhQE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6223/Reviewer_ZhQE"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the visually-continuous corruption robustness of existing Neural Networks (NNs) is examined and compared. In particular, two metrics including the Human-Relative Model Robustness Index (HMRI) and Model Robustness Superiority Index (MRSI) are proposed for the models\u2019 performance evaluation on 14 corruption types. The experiments reveal the high robustness gap between humans and NNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe proposed metrics are reasonable which could mitigate the evaluation bias caused by the quality distribution of the test set.\n2.\tSeveral interesting and meaningful observations are presented. For example, the performance of prediction consistency of different NNs is compared, in addition to the model accuracy.\n3.\tThe authors explore the visually similar transformations, offering opportunities for more cost-effective robustness assessments."
                },
                "weaknesses": {
                    "value": "1.\tIn this paper, The VIF is adopted as the quality measure. However, compared with the advanced full-reference quality measures, such as LPIPS [1], and DISTS [2], the VIF is usually inferior.\n2.\tIn Sec.3, Page 4, the coverage between the IMAGENET-C and VCR Test Set is compared by splitting the full quality range into 40 bins. However, the coverage is highly relevant to the number of bins. In an extreme case, when the bin number is 1, the same coverage the two sets will possess. As such, how to ensure the coverage is reasonable?\n3.\tDuring subjective testing, human decisions are usually highly affected by the memory effect, i.e., a severally corrupted image could still be recognized successfully when humans have observed the same image content but with a high quality. The authors should illustrate how to avoid such effect and provide more details to demonstrate the reliability of the human decision collection.\n4.\tTypos: in De\ufb01nition 1 [Human-Relative Model Robustness Index (HMRI)]: the S^m({\\gamma}^(v)) should be S^m_{\\gamma}^(v)."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6223/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6223/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6223/Reviewer_ZhQE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754107717,
            "cdate": 1698754107717,
            "tmdate": 1699636679248,
            "mdate": 1699636679248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YVFu6zt06x",
                "forum": "yAcLwJu9qs",
                "replyto": "4mqjFZnImE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZhQE"
                    },
                    "comment": {
                        "value": "We very much appreciate the valuable comments and questions. We address each of them individually.\n\n*\"In this paper, The VIF is adopted as the quality measure. However, compared with the advanced full-reference quality measures, such as LPIPS [1], and DISTS [2], the VIF is usually inferior.\"*\n\nWe choose VIF, since it is well-established, computationally efficient, applicable to our transformations, and still performing competitively compared to newer metrics. In fact, according to Table 1 in DISTS [Ding et al., 2022], VIF performs better than LPIPS across all three datasets used in the comparison, and it outperforms DISTS on one of them. Even though DNN-based metrics like DISTS may be applicable to a wider class of transformations than VIF, including those that affect both structure and textures, their scope may depend on the training datasets in potentially unpredictable ways. On the other hand, the scope of VIF is well-defined based on the metric's mathematical definition. In particular, VIF is suitable for evaluating corruption functions that can be locally described as a combination of signal attenuation and additive Gaussian noise in the sub-bands of the wavelet domain. The transformations in our experiments are local corruptions that are well within this scope.  We have updated Section 8.5 to reflect this rationale. However, future work should explore VCR using other IQA metrics.\n\n\n*\"In Sec.3, Page 4, the coverage between the IMAGENET-C and VCR Test Set is compared by splitting the full quality range into 40 bins. However, the coverage is highly relevant to the number of bins. In an extreme case, when the bin number is 1, the same coverage the two sets will possess. As such, how to ensure the coverage is reasonable?\"*\n\nWe follow the established standard for the number of recall positions when calculating average precision, which are also empirical performance curves with a similar shape. For example, the KITTI benchmark has increased this number to 40 from 11, which was originally used in the PASCAL VOC benchmark.  \n\n*\"During subjective testing, human decisions are usually highly affected by the memory effect, i.e., a severely corrupted image could still be recognized successfully when humans have observed the same image content but with a high quality. The authors should illustrate how to avoid such effect and provide more details to demonstrate the reliability of the human decision collection.\"*\n\nThe same original image, corrupted or not, was never shown again to the same participant, for exactly this reason. We\u2019ve added this clarification to the experiment description."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694970553,
                "cdate": 1700694970553,
                "tmdate": 1700694970553,
                "mdate": 1700694970553,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oS5iVHTjmG",
            "forum": "yAcLwJu9qs",
            "replyto": "yAcLwJu9qs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6223/Reviewer_gtM1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6223/Reviewer_gtM1"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a new concept called visually-continuous corruption robustness (VCR), a better alternative to measure corruption robustness than ImageNet-C benchmark. Unlike pre-defined and definite parameters in ImageNet-C, this work creates a benchmark comprising continuous range of image corruption levels and evaluate human performance on the benchmark. Following that, two human aware metrics are introduced to compare neural network performance against humans. Authors demonstrate a notable disparity in robustness between the networks and human performance, despite the improvements seen in ImageNet-C benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tWell written paper.\n\n-\tThoughtful in designing the benchmark with visually-continuous corruptions.\n\n-\tWell detailed and carefully conducted human experiments.\n\n-\tQuantitatively shown that ImageNet-C comprise less coverage of visual corruptions than the proposed benchmark.\n\n-\tVCR is shown to be better robustness estimate than benchmarking on ImageNet-C -> Models having good performance on ImageNet-C shown to be not robust enough on the proposed benchmark.\n\n-\tThis work emphasizes that model robustness is reliable upon verifying across continuous range of image corruption levels, instead of checking at pre-defined parameters.\n\n-\tOpen-sourced with all human data. This benchmark is beneficial and steer the future research on corruption robustness in the right direction."
                },
                "weaknesses": {
                    "value": "I don\u2019t have major concerns about this work. I appreciate authors for considering wide range of models. However, some of the top performing robust models (ImageNet-C leaderboard https://paperswithcode.com/sota/domain-generalization-on-imagenet-c?p=augmix-a-simple-data-processing-method-to) like DINOv2, and MAE are missing in the evaluation. It is helpful to understand behaviour of these models in VCR."
                },
                "questions": {
                    "value": "-\tIn page 3, under Testing VCR, it is mentioned that \u201conly sufficient data in each group but not uniformity\u201d. Why this is the case? How do you define the sufficient data here? What are the drawbacks of considering uniformity. It is mentioned that \u201c this specific design removes the possibility of biased results\u201d, can you clarify what kind of biased distribution of data is referred here?\n\n-\tHumans are presented with one image at a time for 200 ms? Isn\u2019t it too short to notice the image? Are the human participants in an average recognize objects in the image within that time? Would it be safe to assume that human participants do even better job when presented with an image upto 1s? It is mentioned that time was set to ensure fairness. Are the machines classify each image with the same 200 ms?\n\n-\tPlease briefly discuss the qualification tests and sanity checks aimed to filter the participants.\n\n-\tA curious question, What is the total number of participants before the filtration process?\n\n-\tAre same images seen by each human participant?\n\n-\tPlease connect (\\Cref) the text \u201cFig 1\u201d to the Figure 1. Similarly, for other figures and tables."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845068044,
            "cdate": 1698845068044,
            "tmdate": 1699636679144,
            "mdate": 1699636679144,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fFtRGw8p0y",
                "forum": "yAcLwJu9qs",
                "replyto": "oS5iVHTjmG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gtM1"
                    },
                    "comment": {
                        "value": "We very much appreciate the valuable comments and questions. We address each of them individually.\n\n*\"However, some of the top performing robust models (ImageNet-C leaderboard) like DINOv2, and MAE are missing in the evaluation. It is helpful to understand behaviour of these models in VCR.\"*\n\nWe have added DINOv2, the top performer on the ImageNet-C leaderboard, to our evaluation. It also performs best in terms of VCR, along with the other two already-included transformers (TianDeiT-A and -B). The identified robustness gaps compared to humans remain though for all three model, especially for blur corruptions, so the paper conclusion remains unchanged. We\u2019ve run into some problems interfacing the existing MAE models with ImageNet, but should have this resolved for the final version of the paper.\n\n*\"In page 3, under Testing VCR, it is mentioned that \u201conly sufficient data in each group but not uniformity\u201d. Why this is the case? How do you define the sufficient data here? What are the drawbacks of considering uniformity.\"*\n\nAchieving uniformity in VCR requires inverting the mapping from VCR values to parameter values, which would require a costly optimization loop, which is sketched in Sec 8.5. However, since we are fitting a spline, the simple Alg. 1 in Sec 8.5 can tolerate the variation in bin sizes, even if the coverage of the bins is not 100% (see Table 1). This simple algorithm is similar to how average precision for object detection is computed, by first monotonically interpolating the existing datapoints before computing the area under the curve.\n\n*\"It is mentioned that \u201c this specific design removes the possibility of biased results\u201d, can you clarify what kind of biased distribution of data is referred here?\"*\n\nImageNet-C computes robustness by averaging performance over a set of corruption parameter levels, and it does not fit a continuous function before computing the average. As a result, and because of the non-linear (and varying across transformations) relationship between the corruption parameters and their visual effect (as measured by VQA), the ImageNet-C approach leads to big gaps in coverage of the visual change range, such as covering only low or only high visual change levels (see Figure 11). For example, ImageNet-C has no samples with Delta v below 0.5 for Gaussian noise; see Figure 11e in the appendix. As a result, the robustness error reported by ImageNet-C for Gaussian noise is biased towards the far end of the visual change range. Conversely, the robustness error reported by ImageNet-C for Gaussian blur does not consider robustness in the far end (Figure 11o) and is thus biased towards the low end.\n\n*\"Humans are presented with one image at a time for 200 ms? Isn\u2019t it too short to notice the image? Are the human participants in an average recognize objects in the image within that time? Would it be safe to assume that human participants do even better job when presented with an image upto 1s? It is mentioned that time was set to ensure fairness. Are the machines classify each image with the same 200 ms?''*\n\nWe follow an established experimental protocol from Gherios et al. 2019a and Hu et al., 2022. The human visual system (HVS) achieves object recognition with feed-forward processing from a single glance within its early (ventral) stream anywhere between 100 to 200ms (see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3129241/). Above 200ms, recurrent processing starts to play a role, where evidence is integrated over multiple scans of the image (like in test time augmentations), and it may eventually involve functions of the pre-frontal cortex. In that sense, as you point out, human performance will increase with time and the increased recursive processing and the subsequent conscious analysis of the image content in the pre-frontal cortex. We limit the experiment to 200ms to compare the performance of feed-forward DNNs with the feed-forward processing in the brain, where the comparison is fair.  This clearly would not be the case when the human subjects start to involve the full spectrum of their brain functions.\n\n(continued in the next post)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694876905,
                "cdate": 1700694876905,
                "tmdate": 1700694876905,
                "mdate": 1700694876905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tXILbporjW",
            "forum": "yAcLwJu9qs",
            "replyto": "yAcLwJu9qs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6223/Reviewer_zDY4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6223/Reviewer_zDY4"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a comparative study of the robustness of neural networks on visual changes (image corruptions) compared to humans, and perform a larg user-study. This work proposes two measures (HMRI and MRSI) to compare results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ address of the important problem of robustness to image corruptions\n+ contribution of data: the work contributes with data relative to human-based (large number) assessment of image classification robustness, that might be used to further study the problem"
                },
                "weaknesses": {
                    "value": "_difficult to read_\nThe paper is hard-to-follow, and the argumentations or explanations are given in an overworded way. Objectives are not clear, and so possible insights that one should gain from reading this work. This makes difficult also to grasp the conceptual contributions or take-aways expected from the experimental analysis and results. \nIt looks also strange that a paper that proposes a dataset of corruptions applied to images does not show images of such corruptions and how they relate with existing benchmarks.\n\n_poor insights_\nThe paper does not provide insights on how the results should be used: to design new models? train existing architectures differently? or other. WHile the user-study and experimental analysis is large, there is little to none instructive conclusions.\n\n_relation with related work missing or weak_\nNo discussion or comparison with existing work and consider continuous corruptions, such as ImageNet-P and ImageNet-CCC, or other benchmark datasets such as ImageNet-Cbar or ImageNet-3DCC. \n\n_choice of the models_\nthe choice of the tested models is not motivated, neither perspectives on the type of architecture, training data and strategies (e.g. supervised learning, self-supervised, using ImageNet21K or LAION or other datasets, CNN vs transformers) are given."
                },
                "questions": {
                    "value": "How the augmentations proposed in this paper compare with the continuously changing corruptions of ImageNet-CCC? Why the focus is only wrt ImageNet-C?\n\nHow the models are chosen (criteria, comparative perspective, etc.)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698998013993,
            "cdate": 1698998013993,
            "tmdate": 1699636679017,
            "mdate": 1699636679017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J6fv3kUUwU",
                "forum": "yAcLwJu9qs",
                "replyto": "tXILbporjW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zDY4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable comments. We address each weakness and question listed in the review individually.\n\n*\u201cObjectives are not clear, and so possible insights that one should gain from reading this work. This makes difficult also to grasp the conceptual contributions or take-aways expected from the experimental analysis and results.\u201d*\n\nThe key objective is to improve the measurement of robustness and comparison to human performance. The key take-away is that robustness needs to be measured over the full and continuous range of visual change to avoid biased results. The use of a visual quality metric to quantify this range allows standardizing the reference over different transformations, since each transformation has different parameters and effects. We have emphasized the key insights in the introduction and conclusions (see the statements highlighted in boldface in the conclusion). \n\n*\"It looks also strange that a paper that proposes a dataset of corruptions applied to images does not show images of such corruptions and how they relate with existing benchmarks.\"*\n\nThe image corruptions are shown in Fig. 10 in the supplementary appendix. It takes a full page, so there is no space for it in the main paper. We have added a reference to the figure from the main body of the paper. The transformations themselves are from the existing benchmarks, 9 from ImageNet-C, four from Albumentations, and one from the previous study by Gheiros et al., and this is specified on p. 4 of the main paper.\n\n*\"The paper does not provide insights on how the results should be used: to design new models? train existing architectures differently? or other. While the user-study and experimental analysis is large, there is little to none instructive conclusions.\"*\n\nThe main insight is that failing to measure robustness over the full and continuous range, as done in existing benchmarks, leads to biased results that do not fully reflect the actual robustness compared to humans, as supported by the experiments. The key message is: What is not measured properly, cannot be improved. Designing new models or training techniques is not in the scope of this paper. \n\n (continued in the next post)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694229010,
                "cdate": 1700694229010,
                "tmdate": 1700694229010,
                "mdate": 1700694229010,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]