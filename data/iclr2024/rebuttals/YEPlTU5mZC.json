[
    {
        "title": "Implicit Gaussian process representation of vector fields over arbitrary latent manifolds"
    },
    {
        "review": {
            "id": "A8YyzCpqVV",
            "forum": "YEPlTU5mZC",
            "replyto": "YEPlTU5mZC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7490/Reviewer_bAaw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7490/Reviewer_bAaw"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a Riemannian manifold vector field Gaussian process (RVGP), a generalization of Gaussian processes (GPs) for learning vector signals over latent Riemannian manifolds. The core of the idea is to use positional encoding derived from the connection Laplacian. The authors demonstrated the effectiveness of the proposed method via super resolution and inpainting for the vector field on a 3D mesh and EEG analysis."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The theoretical part is mostly well written and technically sound.\n- Related work is well addressed.\n- Practicality is well demonstrated on real data."
                },
                "weaknesses": {
                    "value": "Lack of precision in some statements.\n- V is defined multiply: for vectors and for the nodes of the graph. One of the important statements \"While G approximates M it will not restrict the domain to V\" becomes unclear.\n- The shape of $O_{ij}$ is unclear. It seems to be $m \\times m$ for eq. 11, but it seems to be $d \\times d$ for eq.12. (The rank of $O_{ij}$ will be m.) The shape of $L_{c}$ is also unclear. It is clearly defined as $nd \\times nd$ in eq. 12, but it is inconsistent with $\\Lambda_{c},U_{c} \\in R^{nm\\times nm}$. Maybe something is wrong.\n- A quantitative evaluation and analysis is missing for 5.1. I could not judge whether \"smoothly resolved the singularity by gradually reducing the vector amplitudes to zero\" is okay or not. Does this mean that the result is different from the ground truth? A quantitative evaluation is also appreciated.\n\nDetailed comments:\n- Something wrong: {($x_i$}), $\\hat{v} = \u2225_{i=0}^{n}\\hat{v}_{i}$\n- I could not understand what the authors were doing: \"We then sampled corresponding vectors {$v_{i}$} from a uniform distribution on the sphere\".\n- $S^{3}$ should be $S^{2}$"
                },
                "questions": {
                    "value": "1. The reviewer has a question about the statement \"vector field on latent Riemannian manifolds\". I suppose two interpretations; each of the vector itself should lie on the manifold, or the domain of the field is enclosed on the manifold but the vector can be out of the manifold. In the experimental result (Fig. 2C), the authors point out the \"vectors that protrude the mesh surface\". For the former, this should be an undesirable result, but for the latter, it is okay. An example of the latter case is the normal vector of the Stanford bunny for Figure 2. It is also possible to consider such a problem, but I'm curious if the proposed method can model it. At least I expect the proposed method to work for $m=n-1$ (2D manifold in 3D space), but I doubt it for $m<n-1$ because the Levy-Civita connection is insufficient to address the complementary subspace of the tangent space.\n\n2. The reviewer has some doubts about the term \"unknown manifold\". For 5.1, the training data is dense enough to approximate the manifold well. For 5.2, the authors \"constructed RVGP kernels using the connection Laplacian eigenvectors derived from a k-nearest neighbour graph (k = 5) fit to mesh vertices\". It seems that the Laplacian was computed from 256 vertices, not n=61. If I understand correctly, this violates the prerequisite of \"unknown manifold\". Is there no value in analyzing the relationship between the coverage of the data for the manifold and the accuracy of the vector field prediction? I'm also curious about the analysis of the predicted vector field for out of the manifold to check for regularity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698472905690,
            "cdate": 1698472905690,
            "tmdate": 1699636904159,
            "mdate": 1699636904159,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EJ3ZgulB5p",
                "forum": "YEPlTU5mZC",
                "replyto": "A8YyzCpqVV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7490/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer bAaw part 1"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe thank you for the positive evaluation of our manuscript and for helping us to improve the mathematical clarity and scope of our method. Please find answers to your concerns below in a three-part message.\n\nReviewer: \n\nV is defined multiply: for vectors and for the nodes of the graph. One of the important statements \"While G approximates M it will not restrict the domain to V\" becomes unclear.\n\nAnswer:\n\nThank you for spotting this. We have removed the definition of V relating to vectors as it was unnecessary.\n\nReviewer: \n\nThe shape of O_ij is unclear. It seems to be m x m for eq. 11, but it seems to be d x d for eq.12. (The rank of  will be m.) The shape of L_c is also unclear. It is clearly defined as nd x nd  in eq. 12, but it is inconsistent with \\Lambda_c,U_c\\in R^{nd \\times nd}. Maybe something is wrong.\n\nAnswer:\n\nThank you for spotting this inconsistency. The shape of the matrix Lc is nm x nm, which was a typo. Correspondingly, O_ij is m x m. Both of these operators involve the manifold dimension m because they act intrinsically over the manifold.\n\nReviewer: \n\nA quantitative evaluation and analysis is missing for 5.1. I could not judge whether \"smoothly resolved the singularity by gradually reducing the vector amplitudes to zero\" is okay or not. Does this mean that the result is different from the ground truth? A quantitative evaluation is also appreciated.\n\nAnswer:\n\nThank you for requesting further clarifications on this matter. In Fig 2B of the revised version, we now include a quantitative comparison against a method that uses a channel-wise encoding of the vector signal with a radial basis function kernel. We find out our method provides overall higher accuracy and robustness to variation in the data quantity. Furthermore, this accuracy is achieved with fewer eigenvectors, which confirms the efficiency of our positional encoding.\n\nWith this additional quantification, Fig 2C now provides a pictorial illustration of the errors stemming from an incomplete satisfaction of the tangency and smoothness constraint by the competing channel-wise RBF method.\n\nReviewer: \n\nSomething wrong: {(x_i}), \\hat v = ||_{i=0}^n \\hat v_i \nI could not understand what the authors were doing: \"We then sampled corresponding vectors {v_j} from a uniform distribution on the sphere\".\n\nAnswer:\n\nThank you for spotting the lack of clarity in this paragraph in the previous two bullet points. We have slightly rewritten the paragraph to clarify the construction of the smooth vector field and fixed two minor typos.\n\nReviewer:\n\nS^3 should be S^2.\n\nAnswer:\n\nThank you, corrected."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224706579,
                "cdate": 1700224706579,
                "tmdate": 1700224872025,
                "mdate": 1700224872025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9W9dCTnfxh",
                "forum": "YEPlTU5mZC",
                "replyto": "ehAwnXFHs1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7490/Reviewer_bAaw"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewer_bAaw"
                ],
                "content": {
                    "comment": {
                        "value": "> Reviewer:\n> I'm also curious about the analysis of the predicted vector field for out of the manifold to check for regularity.\n\n> Answer:\n> Our method, by construction, relies on knowing the anchor points over which the vector field is predicted. Predicting vectors over out-of-sample manifold points would be possible by training another GP, such as the one in Borovitsky et al. 2020 (https://arxiv.org/abs/2010.15538) to predict the manifold points first. However, in this example, we were interested in predicting the vector field for specific known locations on the human scalp where we have a ground truth signal. Predicting both location and vector field would result in a compound error, which would not fairly represent the accuracy of our method.\n\nSorry for the misleading comment.\nThe reviewer meant, not specifically for the EEG data, but for a general case as presented in 5.1."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271942570,
                "cdate": 1700271942570,
                "tmdate": 1700271942570,
                "mdate": 1700271942570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R0YCxu7Vgb",
                "forum": "YEPlTU5mZC",
                "replyto": "AESJhpADIA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7490/Reviewer_bAaw"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewer_bAaw"
                ],
                "content": {
                    "title": {
                        "value": "out-of-manifold prediction"
                    },
                    "comment": {
                        "value": "The reviewer recognized that the authors already presented  out-of-sample but on-manifold predictions. However, the review had some doubts about the term \"unknown manifold\" since the training data is dense enough to approximate the manifold well. Therefore, the review requested to check how robust the prediction is for \"out-of-manifold\", which is NOT on-manifold. For Stanford bunny case, it corresponds the vector field in 3D space, not only on the surface."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442184190,
                "cdate": 1700442184190,
                "tmdate": 1700442184190,
                "mdate": 1700442184190,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ELEQaRURRx",
            "forum": "YEPlTU5mZC",
            "replyto": "YEPlTU5mZC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7490/Reviewer_TuVU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7490/Reviewer_TuVU"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at performing Gaussian Process regression over vector fields on unknown manifolds.\n\nThe underlying manifold and tangent space is estimated by combining a proximity graph approach to modelling the underlying manifold, and approximating the tangents space by taking the highest singular values of the matrix of directions to neighbours.\n\nThe discretised connection Laplacian on these tangent spaces is then used to construct a kernel by projecting the spectral decomposition of the connection laplacian onto the estimated tangent spaces.\n\nThis kernel is then used in a number of experiments, from some simple inpainting and super-resolution tasks to superresolution of real EEG data, and show improved diagnostic capabilities using this method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method is clean and simple\n- The method demonstrably works in the single task presented\n- Most of the paper is easy to follow"
                },
                "weaknesses": {
                    "value": "- I found the section \"Vector-field GP on arbitrary latent manifolds\" difficult to follow. For example it is not clear to me what $(U_c)_i$ is. To me this would denote the $i'th$ row, but it clearly is not as it is the wrong shape. Also in equation 15, $\\Phi(\\Lambda_C)^{-2}$ is a $\\mathbb{R}^{mn \\times mn}$ matrix, but is being producted with $P_v$, a $\\mathbb{R}^{d\\times k}$ matrix?"
                },
                "questions": {
                    "value": "- Can you explain the procedure of constructing the kernel in \"Vector-field GP on arbitrary latent manifolds\"? \n- How does this kernel differ from using the method of Hutchinson et. al. more directly? I.e. Using the scalar kernel defined by the graph laplacian, $k(i,j)$, from this creating a diagonal kernel $ K(i,j) = k(i,j) * I_{d\\times d}$, and then restricting this to the estimated tangent spaces, $\\mathbb{K}(i, j) = \\mathbb{T}_i K(i,j)  \\mathbb{T}_j$\n- Presumably one needs to know the dimension of the unknown manifold ahead of time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7490/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7490/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7490/Reviewer_TuVU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698708651352,
            "cdate": 1698708651352,
            "tmdate": 1699636904036,
            "mdate": 1699636904036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ur1ywD48UR",
                "forum": "YEPlTU5mZC",
                "replyto": "ELEQaRURRx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7490/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer TuVU"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe thank you for the positive evaluation of our manuscript and for helping us to improve the mathematical clarity and overall comparison of our method to related works. Please find responses to your questions below.\n\nReviewer:\n\nI found the section \"Vector-field GP on arbitrary latent manifolds\" difficult to follow. For example it is not clear to me what (U_c)_i is. To me this would denote the ith row, but it clearly is not as it is the wrong shape. \n\nAnswer:\n\nWe thank the Reviewer for pointing out the lack of clarity in this section. We believe the Reviewer\u2019s confusion is due to points in the tangent bundle representing vector spaces of dimension m rather than points. Therefore, the positional encoding will be a matrix rather than a vector. The index i can be considered a slice of an n x m x k tensor. We have revised the manuscript to clarify this.\n\nReviewer:\n\nAlso in equation 15, \\Phi(Lambda_c) is a R^{mn \\times mn} matrix, but is being producted with P_v, a R^{d \\times k} matrix?\n\nAnswer:\n\nThank you for pointing out this typo. \\Phi(Lambda_c) should be \\Phi((Lambda_c_{1:k,1:k})) since we are taking the first k eigenvalues. \n\nReviewer:\n\nCan you explain the procedure of constructing the kernel in \"Vector-field GP on arbitrary latent manifolds\"? \n\nAnswer:\n\nWe construct the kernel over the tangent bundle in Equation 15 by direct analogy to the kernel over the manifold in Equation 5-6 by replacing the Laplace Beltrami operator acting on scalar signals on the manifold with the higher-order connection Laplacian operator acting on vector fields over the manifold. This provides a constructive and fully intrinsic definition, i.e., relying on the local Riemannian metric. We will elaborate on this below in the next response. \n\nReviewer:\n\nHow does this kernel differ from using the method of Hutchinson et. al. more directly? I.e. Using the scalar kernel defined by the graph laplacian, k(i,j), from this creating a diagonal kernel K(i,j)=k(i,j)\\ast I_{d\\times d}, and then restricting this to the estimated tangent spaces, K(i,j) = T_iK(i,j)T_j.\n\nAnswer:\n\nThank you for drawing our attention to a more detailed comparison to Hutchinson et al. First, let us point out that our kernel cannot be obtained by the procedure suggested by the reviewer, i.e., expanding the kernel defined by the graph laplacian, k(i,j), and then restricting to the tangent spaces. This is because the graph Laplacian and the connection Laplacian are not related by linear transformations. Instead, they share a non-linear relationship given by the Weitzenbock inequality, L_c - L = A, where A is an operator depending on the Ricci curvature of the manifold (note that we used Lc and L to define continuous operators obtained as limiting objects of our discrete formulation).\n\nRegarding the comparison to Hutchinson et al., while their construction relies on a non-linear but isometric projection to Euclidean space and defining the GP therein, our method builds the GP intrinsically over the manifold. By intrinsically, we mean using the connection Laplacian operator, which is defined in terms of the Riemannian metric.\n\nThus, our method is more robust in practice since the connection Laplacian operator can always be constructed from local connections, whereas a global Euclidean isometric projection may be less reliable to approximate.\n\nWe have added additional explanations in the text.\n\nReviewer:\n\nPresumably one needs to know the dimension of the unknown manifold ahead of time?\n\nAnswer:\n\nThe dimension of the manifold does not need to be known ahead of time. Rather, it can be estimated as the mean of dominant singular values during the tangent space approximation based on a cutoff for the fraction of explained variance. Taking the mean as an estimator is suitable because it minimises the mean-squared error.\n\nWe have now implemented this feature in the code (which we will link after publication) and clarified it in the text."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700223071719,
                "cdate": 1700223071719,
                "tmdate": 1700223071719,
                "mdate": 1700223071719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "afqY5Veluc",
                "forum": "YEPlTU5mZC",
                "replyto": "Ur1ywD48UR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7490/Reviewer_TuVU"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewer_TuVU"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications - I believe all the parts I mentioned have been clarified satisfactorily.\n\nI asked \"How does this kernel differ from using the method of Hutchinson et. al. more directly?\" as I am sure your method is better and provides a more sensible kernel. It would be very nice I think if it were possible to visualise the kernel. \n\nFor example, akin to how one plots a scalar kernel as a line/colormap by evaluating k(?, x) over the domain, it is possible to plot the co-vector field (or the associated vector field) defined by k(?, (x, v)) for a chosen point and tangent vector. Compare this to the kernel from Hutchinson et. al. on e.g. the sphere and I guess that you will end up with a more sensible picture, rather than a kernel that depends on the chosen embedding into euclidean space. I think this would be a very good visual sell for the paper.\n\nOn a related point, does this methodology extend to the continuous setting where one knows the manifold ahead of time, akin  to Borovitskiy et al., 2021? What are the challenges associated?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559169627,
                "cdate": 1700559169627,
                "tmdate": 1700559169627,
                "mdate": 1700559169627,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cyEVkRalU0",
            "forum": "YEPlTU5mZC",
            "replyto": "YEPlTU5mZC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7490/Reviewer_3KnT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7490/Reviewer_3KnT"
            ],
            "content": {
                "summary": {
                    "value": "Gaussian processes is a popular Bayesian method that easily incorporates prior knowledge and provides good uncertainty quantification. GP is originally defined in Euclidean space which limits its application in certain domains. In this paper, the author proposes the Riemannian manifold vector field GP (RVGP) which extends GP to learn vector signals over latent Riemannian manifolds with the use of connection Laplacian operator. Experiment results show RVGP can encode the manifold and vector field's smoothness as inductive biases and have good performances on electroencephalography recordings in the biological domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Extending GP to vector-valued signals is novel. The proposed method also removes the common assumption that the manifold is known in non-Eucleadian GP, which expands GP's applicability.\n- The paper is well written."
                },
                "weaknesses": {
                    "value": "- One advantage of GP is data efficiency. In section 5.1's superresolution experiment RVGP is trained using vectors over 50% of the nodes, I would be interested to see the results when trained with less data.\n- In section 5.2 the authors compare RVGP with interpolation methods, which might be a too simple baseline comparison. The RVGP's performance is also close to linear prediction. I would be interested to see the comparison against a stronger baseline method. \n- A section discussing the limitations of the proposed method would be good."
                },
                "questions": {
                    "value": "See weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7490/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7490/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7490/Reviewer_3KnT"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699282079998,
            "cdate": 1699282079998,
            "tmdate": 1700641912531,
            "mdate": 1700641912531,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1kukxlLwsj",
                "forum": "YEPlTU5mZC",
                "replyto": "cyEVkRalU0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7490/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 3KnT"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe thank you for the positive evaluation and the criticisms of our article. We have now addressed your concerns and believe the article has been substantially improved. Please find individual responses below.\n\nReviewer: \n\nOne advantage of GP is data efficiency. In section 5.1's superresolution experiment RVGP is trained using vectors over 50% of the nodes, I would be interested to see the results when trained with less data.\n\nAnswer:\n\nWe thank the Reviewer for highlighting this point. In Fig 2B, we already vary the density of data points, hence the amount of training data, by changing the resolution of the surface representation. We showed that there is no significant drop in accuracy when we vary the pairwise distance of sample points relative to the object (bunny) diameter. Increasing from 1% to 8% effectively uses less data and results in a smoother bunny surface due to the lower resolution. \n\nTo address the Reviewer\u2019s concern, we have now added a benchmarking of our method, which is based on a vectorial representation using the connection Laplacian, against an alternative GP method based on a channel-wise representation using the graph Laplacian and a radial basis function kernel. Unlike the benchmark, our method shows no significant variation against resolution and can represent the vector field with significantly fewer eigenvectors (Fig 2B). \n\nReviewer:\n\nIn section 5.2 the authors compare RVGP with interpolation methods, which might be a too simple baseline comparison. The RVGP's performance is also close to linear prediction. I would be interested to see the comparison against a stronger baseline method. \n\nAnswer:\n\nThank you for pushing us to perform further benchmarking of RVGP. We chose linear and spline interpolation as benchmarks since there are broadly adopted approaches in current experimental and clinical practice.\n\nAt the request of the reviewer, we now include a comparison of our vector-based method against a channel-wise radial basis function (RBF) kernel. We show in Fig. 3D,E that this method offers higher performance than linear and spline interpolation but is subpar compared to RVGP. \n\nReviewer:\n\nA section discussing the limitations of the proposed method would be good.\n\nAnswer:\n\nIn the revised version, we now include an expanded Discussion section detailing the limitations and future possibilities."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222647132,
                "cdate": 1700222647132,
                "tmdate": 1700222647132,
                "mdate": 1700222647132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u6oSxqAk69",
                "forum": "YEPlTU5mZC",
                "replyto": "1kukxlLwsj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7490/Reviewer_3KnT"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7490/Reviewer_3KnT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications and additional experiments. It cleared my concerns and I decided to raise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641893265,
                "cdate": 1700641893265,
                "tmdate": 1700641893265,
                "mdate": 1700641893265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]