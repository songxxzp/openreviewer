[
    {
        "title": "OneSpike: Ultra-low latency spiking neural networks"
    },
    {
        "review": {
            "id": "0iHiyf2V4r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3309/Reviewer_2hLo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3309/Reviewer_2hLo"
            ],
            "forum": "QRWrvzRU4w",
            "replyto": "QRWrvzRU4w",
            "content": {
                "summary": {
                    "value": "The paper proposes a method to convert rate-encoded spiking neural network into an equivalent OneSpike model with only one timestep. Authors use a parallel spike generation (PSG) method and develop a OneSpike framework. The paper claims that this method can achieve ultra-low latency, high accuracy, and hardware feasibility for SNNs. The paper compares OneSpike with various state-of-the-art SNNs and BNNs, and shows that OneSpike achieves the highest accuracy (81.92% on ImageNet) over other ANN-SNN conversion methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Authors evaluation the OneSpike method on ImageNet with RepVGG-L2pse architecture and achieve an 81.92% accuracy."
                },
                "weaknesses": {
                    "value": "Compared to IF neuron, OneSpike neuron use different group to generate spike output corresponding to different timesteps in IF neuron. Thus, the claim of one timestep neuron is not true.\n\nOneSpike model is mathematically equivalent to an activation quantized model. Compare to the widely used, GPU friendly network quantization technique, I don't think OneSpike has any advantage."
                },
                "questions": {
                    "value": "Please discuss the concerns addressed in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697458482126,
            "cdate": 1697458482126,
            "tmdate": 1699636280199,
            "mdate": 1699636280199,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LFgQUuyesY",
                "forum": "QRWrvzRU4w",
                "replyto": "0iHiyf2V4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**Comparison with IF neuron**\n\nResponse: Please refer to Section 1.3 above. \n\n**Advantage compared to other quantized networks**\n\nResponse: We have conducted comparisons of our models with activation quantized models, such as Binary Neural Networks (BNNs). For further explanation, please refer to Section 1.1 above."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582069130,
                "cdate": 1700582069130,
                "tmdate": 1700582069130,
                "mdate": 1700582069130,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uSvizz2vA5",
            "forum": "QRWrvzRU4w",
            "replyto": "QRWrvzRU4w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3309/Reviewer_t2PL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3309/Reviewer_t2PL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a parallel spike generation (PSG) method that generates all spikes for a network layer within a single timestep."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The authors think that they have achieved superior results on complex datasets under low time latency."
                },
                "weaknesses": {
                    "value": "1. **I think the concept of OneSpike proposed in this paper is actually a gimmick.** As shown in Fig.1, the authors split the same parameters of each layer into $g_l$ groups, in fact speculating whether neurons will fire a spike at $i$-th step ($i=1,...,g_l$) under the condition that the input current in each step is completely the same (i.e. the current is uniformly distributed). Subsequently, they obtain an accurate spike sequence $s_1,...,s_g$ under the condition of uniform input current, then continue to calculate the new average input current $x^{l+1}$ after passing through the next-layer weights $W^l$. Note that in this process $s_1,...,s_g$ are respectively calculated with $W^l$ and the overall number of operations is the same as the number of operations in the previous works that emitted spikes for $g_l$-steps. **That is to say, the overhead of OneSpike mentioned by the authors is actually equivalent to the cost of the previous researchers' $g_l$ time-steps.**\n\n2. Eq.10 involves multiplication and modulus operations, which were usually not allowed in previous SNN related works.\n\n3. The reason why authors can achieve an accuracy >80% is not merely because their algorithm is superior to previous works, but because of the advantages of RepVGG network structure itself. Previous works mainly used VGG-16 and ResNet-34, which is obviously difficult to achieve an accuracy of >75% on ImageNet.\n\nOverall, I think the contribution of this paper is actually very limited. If we switch the order of the weight matrix $W^l$ and summation operations ($\\sum$) in Figure 1, in fact, the operation mechanism of the entire network is completely equivalent to QCFS ANN [1], which is an ANN with quantized activation output.\n\n[1] Tong Bu, Wei Fang, Jianhao Ding, PengLin Dai, Zhaofei Yu, and Tiejun Huang. Optimal ann-snn conversion for high-accuracy and ultra-low-latency spiking neural networks. ICLR 2022."
                },
                "questions": {
                    "value": "See Weakness Section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3309/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3309/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3309/Reviewer_t2PL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697797938573,
            "cdate": 1697797938573,
            "tmdate": 1699636280121,
            "mdate": 1699636280121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zTzTCqLfv6",
                "forum": "QRWrvzRU4w",
                "replyto": "uSvizz2vA5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**Neurons are speculated to fire spikes uniformly across steps, leading to a spike sequence and average input current calculation with the same operational cost as previous multi-step methods**\n\nResponse: Thank you for acknowledging the accuracy of the spike sequence generated by our model. Compared to previous works that operate over multiple $g_l$ timesteps, our model performs all computations in parallel. This has the advantage of a better global picture.\n\nCompared to the traditional IF model, PSG can more accurately approximate the scaled value in the ANNs as shown in Section 5.1 of the main paper.\n\n**The multiplication and modulus operations involved in the Eq.10**\n\nResponse: Eq. 10 is the full precision math that needs to be done, not the actual implementation. We used the power of 2 values, simpler shifts, bitwise operations etc. to approximate it. Details are discussed in Section 4.2 of the main paper.\n\n\n\n**The advantages of RepVGG network structure**\n\nResponse: We chose baselines with high accuracies because they are state-of-the-art. The key is that our method will convert any multi-step SNN into a single-step one with little loss of accuracy.\n\n\n\n**QCFS ANN**\n\nResponse: QCFS primarily focuses on ANN-to-SNN conversion, whereas OneSpike starts with a SNN. Also, QCFS requires a relatively large timestep of 16 to achieve an accuracy of only 50.97% on ImageNet. If we had started the same SNN as QCFS, we expected to achieve a higher accuracy but with a single timestep. Unfortunately, we were unaware of the QCFS work and did not have the time to do that."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582403067,
                "cdate": 1700582403067,
                "tmdate": 1700582403067,
                "mdate": 1700582403067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PYbgQkX1GI",
            "forum": "QRWrvzRU4w",
            "replyto": "QRWrvzRU4w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3309/Reviewer_nUrX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3309/Reviewer_nUrX"
            ],
            "content": {
                "summary": {
                    "value": "The paper represents an interesting step in the efforts to make SNNs live up to the promise of lower energy consumption than there ANN counterparts. The paper proposes an ANN-to-SNN conversion, or more specifically a conversion from N-step SNNs to 1-step SNNs that preserves accuracy. The results on ImageNet getting over 80% accuracy is really strong as this is a much higher accuracy than previous SNN papers. However, in this reviewer's perspective there are several questions unanswered which makes the true benefits of the approach unclear."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The resulting accuracy improvement on ImageNet is impressive."
                },
                "weaknesses": {
                    "value": "The weaknesses of the paper are related to an incomplete energy model and analysis. It does not fully consider the cost of memory access nor the cost of handling the sparsity (compared to ANNs).  Comparisons to state of the art SNNs are focused on accuracy and not energy."
                },
                "questions": {
                    "value": "The paper talks proposes to in parallel create different spike groups changing the traditional IF model significantly. However, this opens the question of proper comparisons. For example, there are many non-multiplier-based implementations of ANNs that should also be considered when doing comparisons. In particular, the fact that their approach involves a module of a power of 2, made me think that their approach must be similar to decomposing the weights of an ANN bit-wise. However, I understand that the power of 2 for each group and each layer is fixed. I had wondered if you considered varying theta for different groups. \n\nMore generally, I think it would be good for the paper to better explain the SNN -> ANN conversion step. Your abstraction mentions this but in your algorithm, you focus on converting a N-step SNN to a 1-step SNN. Also, in figure 1, it seems you are using W for both weights and a dimension of the input feature map. This is confusing. Can you clarify?\n\nI find the analysis of energy consumption based on FLOPs somewhat limiting. In many neuromophic designs the dominant energy consumption is the weight and membrane potential lookup. Can you include an estimate of the memory access cost in your designs and comparisons? There are a number of energy models of SNNs (see e.g., https://arxiv.org/pdf/2309.03388.pdf) that include means of capturing the memory cost of SNNs that would make the results far more reliable.  In particular, my concern is that most of the membrane potentials need to be updated despite the sparsity of activations and this should be captured.\n\nSecondly, I think the paper should at least have a discussion of  the cost of supporting the SNN bit-level sparsity (compared to ANNs that do not do typically have or need to handle this granularity of sparsity).  For example, looking up a 1-bit activation is not 8 times less energy than looking up a 8-bit activation because much of the energy is associated with address decoding.  In designs that are spike centric (like Loihi) the cost of memory lookups and routing data can overshadow the cost of add vs mulitply (which is why they support graded spikes). Numerous hardware designs have been proposed to better manage weight and activation sparsity but they come at a cost. This should be recognized when proposing advanced SNN algorithms.\n\nI also wondered if your constraint on the ANN quantization has an impact on accuracy. This does not seem to be addressed.  It was called \"near-lossless\" but not quantified (from what I can see).  Can you clarify?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3309/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3309/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3309/Reviewer_nUrX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698441924623,
            "cdate": 1698441924623,
            "tmdate": 1699636280048,
            "mdate": 1699636280048,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u1oYCDYrEi",
                "forum": "QRWrvzRU4w",
                "replyto": "PYbgQkX1GI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**ANN comparisons and the possibility of varying theta across groups.**\n\nResponse:\n\n+ We have compared our model with BNNs and listed the advantages of the OneSpike model in the Discussion section. For further explanation, please refer to Section 1.1 above.\n\n+ The modulo operation is needed to ascertain the residual membrane potential for each group. Specifically, this involves taking the modulo of the accumulated membrane potential with the threshold to simulate the firing process post-spike. However, the direct use of it is not compatible with SNNs. Therefore, in our design, all thresholds for OneSpike are powers of two. Modulo 2 is then simply examining the least significant bit of a binary number. However, it is not for decomposing the weights of ANNs, as the reviewer asserted.\n\n+ We did not apply different theta values for each group because they share the same weight and input. We use different groups to represent the same batch of data more precisely compared to the IF model. The original purpose of using a fixed theta for each group is to make the output more consistent, however, using different theta among layers could be a potential way to improve the energy efficiency and the flexibility of our model.\n\n  \n\n**The explanation of ANN-to-SNN conversion and the update of Figure 1**\n\nResponse: Please refer to our opening response. OneSpike will work with any SNN, regardless of how it was obtained.\nThank you for pointing out our mistake in Figure 1, we already updated our figure.\n\n\n\n**FLOPs based energy evaluation**\n\nResponse: We agree with the reviewer and have responded to the issue of energy in the last paragraph of the general response. There are many aspects of hardware that, short of actually building it, are rather hard to quantify. Furthermore, a detailed discussion of these would be outside the scope of ICLR. Hence, our use of FLOPS as an approximation.\n\n**Constraint on the ANN quantization having an impact on accuracy**\n\nResponse: Thank you for pointing this out. As our focus is on SNN-to-SNN conversion, the specifics of ANN-to-SNN conversion do not constitute our primary concern. Any method that develops a multi-step SNN can be adopted. In Section 3.2, we introduced the method for ANN-to-SNN conversion used in our experiments. This method includes processes of clamping and quantization. For details, please refer to the following paper:\n\nZ. Yan, J. Zhou and W. -F. Wong, \"CQ$^{+}$ Training: Minimizing Accuracy Loss in Conversion From Convolutional Neural Networks to Spiking Neural Networks,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 10, pp. 11600-11611, Oct. 2023, doi: 10.1109/TPAMI.2023.3286121."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580991971,
                "cdate": 1700580991971,
                "tmdate": 1700580991971,
                "mdate": 1700580991971,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YMuIk3g1eJ",
                "forum": "QRWrvzRU4w",
                "replyto": "u1oYCDYrEi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3309/Reviewer_nUrX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3309/Reviewer_nUrX"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal. I appreciate the clarification of that the approach can handle any SNNs and the correction to Figure 1. I think my biggest concern is that the benefit of the approach remains unclear - while you do achieve one time step, you incur additional costs that I think cannot be reflected in FLOPs alone.  While I agree that reducing the computation to 1 time step helps,  the addition of groups may make data movement and memory lookups more complex.  it is hard to estimate many of these aspects, several papers have proposed unified energy models that at least take into account memory lookups (relating the energy of a memory lookup to that of something like an 8-bit add). I think other reviewers have similar concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678735821,
                "cdate": 1700678735821,
                "tmdate": 1700678735821,
                "mdate": 1700678735821,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0p57vYVQqU",
            "forum": "QRWrvzRU4w",
            "replyto": "QRWrvzRU4w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3309/Reviewer_yNko"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3309/Reviewer_yNko"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method to convert a classical analog neural network into a spiking neural network. The results are compared with other state-of-the-art methods and show good performance on the imagenet challenge."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clear and well written. The results are interesting and show a significant performance improvement over state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "The parallel spiking generation method could be understood as some kind of quantization of analog numbers in a dyadic format, and this point should be more clearly stated.  As such, such a method seems relatively similar to methods that use quantization in analog networks. In particular, even if this method seems original, the parallelism with existing methods needs to be strengthened. In particular the claim that \"To the best of our knowledge, this study is the first to explore converting multi-timestep SNNs into equivalent single-timestep ones\" should be circonstantied. On the other hand, do you see any analogy between this mechanism and processes that might take place in biological neural networks? It seems, for example, that predictive methods will use residuals, and that these can themselves be quantified, and so on... but to my knowledge, there are no papers exploring this interesting direction of research."
                },
                "questions": {
                    "value": "The method presented in this paper works well on static images. Do you think this method could be extended to dynamic images, such as videos? Do you think this method could be extended to recurrent neural networks?\n\n\nMinor:\n- \"a PyTorch toolkit called OpCounter(Lyk), \" fix reference"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3309/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659730953,
            "cdate": 1698659730953,
            "tmdate": 1699636279978,
            "mdate": 1699636279978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Istz8UIjOa",
                "forum": "QRWrvzRU4w",
                "replyto": "0p57vYVQqU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3309/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3309/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**Difference between PSG and quantized value in analog network and the parallelism with existing methods.**\n\nResponse: All SNNs can be seen as quantization. What makes PSG stand out is its consideration of temporal dynamics.\nThe parallel spiking generation method resembles the quantization of analog signals, particularly in the way it discretizes continuous information. Unlike SNNs, quantized neural networks (particularly binary networks) typically do not consider temporal dynamics. For the comparison with quantized models and the IF model, please refer to Sections 1.1 and 1.3 of the general response above. \n\n**The claim of the first to explore converting multi-timestep SNNs into equivalent single-timestep ones.**\n\nResponse: With all due respect, we stand by this claim as so far we have not found any evidence to the contrary.\n\n\n**Analogies with processes in biological neural networks and extensions to dynamic images and recurrent neural networks.**\n\nResponse: Thank you for pointing out the potential of exploring the analogies between our PSG method and biological neural processes as well as extending to dynamic images or videos, this may be our future direction."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3309/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567989577,
                "cdate": 1700567989577,
                "tmdate": 1700567989577,
                "mdate": 1700567989577,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]