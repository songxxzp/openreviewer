[
    {
        "title": "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models"
    },
    {
        "review": {
            "id": "v1ta4wc4KY",
            "forum": "LzPWWPAdY4",
            "replyto": "LzPWWPAdY4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7536/Reviewer_BzhB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7536/Reviewer_BzhB"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new approach for weight quantisation and parameter-efficient fine-tuning via low-rank adapters termed LoftQ. LoftQ is inspired by QLoRA and aims to improve over it by providing a better quantisation and better initialisation for the low-rank adapter weight matrices.  \n\nFor background: LoRA makes the assumption that the difference between pre-trained and fine-tuned weights can be approximated by a low-rank matrix, i.e. $W_{ft*} = W_{pt} + AB^T$. \n\nThe core contribution of this work relies on the observation that QLoRA quantises $W_{pt}$ but still relies on the default LoRA initialisation which assumes a non-quantised matrix $W_{pt}$.\n\nTo address this shortcoming, the authors propose an iterative LoRA-aware quantisation which jointly improves the quantisation of $W_{pt}$, making it more similar to the pre-trained weight, and the initialisation of $A$ and $B$ (as the authors note, QLoRA is a special case of their proposed algorithm). \n\nThe authors compare their proposed approach to QLoRA and full fine-tuning across several models and datasets, showing that it consistently outperforms QLoRA.\n\nIn addition to their main experiments, the authors provide ablations investigating their proposed approach in more detail.\n\n- Dettmers et al. 2023 - QLoRA: Efficient Finetuning of Quantized LLMs"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The core contribution of this work is well motivated and grounded in the shortcomings of an existing widely used approach.\n- The authors provide sufficient experimental results to demonstrate the usefulness of their approach\n- The authors provide ablation studies, investigating important details of their approach\n- The paper is well written, the structure is clear and easy to follow"
                },
                "weaknesses": {
                    "value": "I couldn't identify serious weaknesses of this work but I have some suggestions and questions for the authors. See below."
                },
                "questions": {
                    "value": "**Questions and suggestions**\n- The result of the LoftQ algorithm is a quantised weight matrix ($Q_T$) as well as the LoRA matrices ($A_T$, $B_T$). An interesting ablation would be to discard $A_T$ and $B_T$ and use the default LoRA initialisation instead. This would tell us more about the importance of initialising $A_T$ and $B_T$ differently.\n- One of the findings in the QLoRA paper is that it is crucial to add LoRA adapters to every linear layer of the model (Figure 2 in the QLoRA paper). It could be interesting to run a similar ablation with your method. Given your improved initialisation, maybe it is sufficient to add LoRA adapters to fewer layers.\n- It could be interesting to study the difference in initialisation of the low-rank matrices more. Does your work provide insights into what makes a good LoRA initialisation and could these insights be potentially applied to non-quantised LoRA as well? \n\n\n**Typos and writing suggestions**\n\n- Introduction, second paragraph: \"It is predicated on the hypothesis ...\" \n    - You might want to use \"based\" instead of predicated\n- Discussion, LoftQ better than full precision LoRA: \"Such zero initialisation could cause the fine-tuning unstable\"\n    - This sentence needs rewriting"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7536/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830605139,
            "cdate": 1698830605139,
            "tmdate": 1699636910307,
            "mdate": 1699636910307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CvS9XfCptt",
                "forum": "LzPWWPAdY4",
                "replyto": "v1ta4wc4KY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7536/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7536/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the questions"
                    },
                    "comment": {
                        "value": "Thank you for pointing our typos and writing suggestions, we will improve it in the next version.\n\n**Response to the questions**\n\n1. LoftQ enables $Q_T + A_T B_T$ as close to the pre-trained weight $W$ as possible. It does not guarantee $Q_T$ alone would be a better initialization than $Q_0$ which is used by QLoRA. We verify this in two ways: (1) we compare the F norm $||W-Q_T||_F$ and $||W-Q_0||_F$; (2) we take experiments on $Q_T$ with default LoRA initialization, as you suggest. The results are listed below:\n\n(1). We measure the fc1 matrix of the 1st encoder layer in BART-large.\n\n|                         | t=0  | t=1  | t=2  | t=3  | t=4  |\n| -------------------------------- | ---- | ---- | ---- | ---- | ---- |\n| $ \\|\\|W - Q_t\\|\\|_F$                  | 10.50 | 10.69 | 10.99 | 11.28 | 11.55 |\n| $ \\|\\|W-(Q_t + A_t B_t^{\\top})\\|\\|_F$ | 10.05   | 9.73   | 9.56   | 9.44  | 9.36   |\n\nFrom the table, we can see  $||W - Q_t||_F$ actually increases throughout more iterations while $||W - (Q_t + L_t R_t^{\\top})||_F$ decreases.\n\n(2). We also take experiments on GSM8K\n|    Model    | $Q_T + A_T B_T$ |  $Q_T$ w/ zero init  |\n| ----------- | --------------  | ----------- |\n| LLAMA-2-7b  |      35.0       |     33.4    |\n| LLAMA-2-13b |      45.0       |     41.6    |\n\n2. This is an interesting question, but it is beyond the scope of our work. It remains open on how to achieve the best trade off between the number of layers with LoRA adapters and the performance. We would suggest referring to [AdaLoRA](https://arxiv.org/abs/2303.10512), which allocates dynamic ranks to different frozen matrices.\n\n3. The insight of LoftQ is that the full low-rank adapters are used to bridge the gap brought by quantizing pre-trained weights. We, unfortunately, cannot apply this logic to non-quantized LoRA because it does not have quantization discrepancy issues."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684018959,
                "cdate": 1700684018959,
                "tmdate": 1700707655572,
                "mdate": 1700707655572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u93H33DD7O",
            "forum": "LzPWWPAdY4",
            "replyto": "LzPWWPAdY4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7536/Reviewer_5E8o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7536/Reviewer_5E8o"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method (LoftQ) to initialize the quantized weights in a transformer based model for future LoRA-based fine-tuning. Different from initializations for Quantized Lora used in prior methods, such as fixup or zero-init, LoftQ initalizes the quantized matrix weights and lora weights together to minimize the Frobenious norm of the difference between the floating point weights and the quantized weights. The initialization process is iterative where the quantized matrix is obtained through a standarized quantization process and the lora quantized weights are obtained from a SVD decomposition.\n\nExperiments on encoder models (classification), encoder-decoder models (summarization), and decoder models (math reasoning, language modeling) are conducted and results are in favor of the LoftQ initialization."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The lack of a proper initialization of quanitzed lora methods intuitively makes sense, the authors identified this problem and proposed a simple but working solution to address this problem. I appreciate this simplicity.\n2. The experiments are well conducted over quite a few domains/datasets, models, and quantization schemas.\n3. The paper is well written."
                },
                "weaknesses": {
                    "value": "1. It might be better to put higher priority and conduct more experiments on decoder-based (or encoder decoder) models for generative tasks. It seems that quantized lora (whether with or without intialization) lacks too much in classification tasks with encoders, to the extent that pratictionars probably won't want to train quantized lora models on these tasks. \n2. Otherwise, I find this paper well rounded without significant weaknesses."
                },
                "questions": {
                    "value": "1. It would be nice to show the memory footprint for 2-bit quantized models during training. \n2. Would the quantized lora initialization in turn help full quantized fine-tuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7536/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7536/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7536/Reviewer_5E8o"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7536/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699653423265,
            "cdate": 1699653423265,
            "tmdate": 1699653470098,
            "mdate": 1699653470098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3lMyMBr3d3",
                "forum": "LzPWWPAdY4",
                "replyto": "u93H33DD7O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7536/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7536/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the weaknesses and questions"
                    },
                    "comment": {
                        "value": "**Response to the weaknesses**\n\nWe appreciate your suggestion on conducting more experiments on decoder-only models. We are currently working on LLAMA-2-70b. Due to the time limit, we may be not able to present comprehensive results in rebuttal within a short response period. However, we will add more experiments in the next version.\n\n**Response to the questions**\n\n1. The training memory footprint varies by task, batch size, and input sequence length. Therefore, we only provide examples. \n| Model       | Bit  | Rank | Task  | Seq length | Batch Size | GPU Memory |\n| ----------- | ---- | ---- | ----- | ---------- | ---------- | ---------- |\n| LLAMA-2-7b  | 4    | 64   | GSM8K | 384        | 1         | 15GB       |\n| LLAMA-2-13b | 4    | 64   | GSM8K | 384        | 1         |  24GB   |\n\n\n2. We are not sure about what you mean by \u201cfull quantized fine-tuning\u201d. If it means quantization-aware full fine-tuning, our method is orthogonal to this setting because quantization-aware full fine-tuning does not have low-rank adapters. However, in terms of the initialization of the quantization-aware full fine-tuning, which also starts with a quantized model, our method could provide valuable insight: simply add the $A_T B_T$ back to $Q_T$, but how to add it back so that it achieves the least quantization discrepancy still remains an open question."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683623493,
                "cdate": 1700683623493,
                "tmdate": 1700683623493,
                "mdate": 1700683623493,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0TKkZiKsvi",
            "forum": "LzPWWPAdY4",
            "replyto": "LzPWWPAdY4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7536/Reviewer_tN6u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7536/Reviewer_tN6u"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes better initialization for LoRA adaptors A and B, and the Quantization of pre-trained weights W_{pt} in a setup where two things are desired:\n1) downstream fine-tuning\n2) quantization of W_{pt}. \n\nThe authors propose an iterative method to find better initializations for these matrices. Through rigorous experiments the work shows that the proposed initialization is better than the vanilla initialization proposed in QLoRA.\nThe authors conduct experiments with almost-extreme quantization (2 bit) to show efficacy of their approach, where the traditional methods (QLoRA) even fail to train. \nThe work also attempts to analyze the impact of number iterations (of the proposed iterative method) and the experiments are conducted well."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This work presents well motivated initialization method for LoRA + Quantization\n* Through extensive experimentation on several architectures and benchmarks, this work clearly elucidates pitfalls of QLoRA and effectiveness of the proposed method"
                },
                "weaknesses": {
                    "value": "None, but a few clarifying questions stated below."
                },
                "questions": {
                    "value": "1) For the XSUM and GSM8k tasks, LoftQ gets better accuracy than full-precision LoRA. I wonder how the FP LoRA was tuned? Maybe 4 bit quantization does implicit regularization, and FP LoRA  was not regularized well enough? This would especially make a difference if the tasks are low dimensional. In other words, if a high capacity LLAMA 13B model is fine-tuned LoRA style on GSM8k, how did the authors ensure that the model was not overfitted?\n\n2) It would be nice to analyze number of epochs, and training steps required for baseline full precision LoRA and LoftQ. \n\n3) LoRA's original motivation stems from \"training efficiency\" while maintaining the inference cost the same as the base model. Conversely quantization's main motivation is inference efficiency. Keeping training efficiency aside, a good baseline maybe quantization aware fine-tuning (i.e. no LoRA), to establish upper bound on accuracy for LoftQ. \n\n4) It wasn't very fully clear but are the LoRA adaptors, A and B, quantized as well in LoftQ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7536/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7536/Reviewer_tN6u",
                        "ICLR.cc/2024/Conference/Submission7536/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7536/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699679615699,
            "cdate": 1699679615699,
            "tmdate": 1699943643676,
            "mdate": 1699943643676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fupUPm3ypN",
                "forum": "LzPWWPAdY4",
                "replyto": "0TKkZiKsvi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7536/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7536/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Response to your questions**\n1. Thanks for the advice of taking experiments on regularized FP LoRA. The FP LoRA in our paper is fine-tuned without any regularization but we have properly tuned the hyper-parameters, including lora_alpha, learning rate, batch size, etc. We did observe overfitting: as the training epoch increases, the accuracy on the test set drops after a certain epoch. We use early stopping to prevent overfitting. We stop the training by the 6th epoch. \n\n2. Adding weight decay to the LoRA adapters does help the LLAMA-2-13b on GSM8K, but it does not help LLAMA-2-7b on GSM8K. Specifically, full precision LLAMA-2-13b with LoRA achieves 46.0% now, instead of 43%, but LLAMA-2-7b drops from 36.9% to 34.4% on GSM8K. We will add the regularization results to the latest paper.\nWe have listed the required epochs in Table 11 and Table 12 in the Appendix. We choose the same epochs for both FP LoRA and LoftQ in all experiments.\nThank you for your insightful comments. Allow me to address a potential misunderstanding regarding the motivation of LoRA and quantization. The original motivation of LoRA is efficient multi-task learning as introduced in the second paragraph of Section 1 in LoRA paper. The motivation of weights-only quantization is model compression, allowing LLMs to run on memory-limited devices.\n\n3. To the best of our knowledge and the NLP literature, it is not clear whether quantization-aware training (QAT) would be an upper bound for LoftQ or QLoRA, given two reasons: (1) the initial weights are quantized and therefore there exists a significant gap between quantized initialization and the pre-trained weights in QAT, while LoftQ can reduce such a gap. (2) it is difficult to update the non-continuous quantized parameters, which is merely possible to achieve full precision fine-tuning performance, let alone given severely degraded initial weights. On the other hand, LoftQ fine-tunes full precision LoRA adapters, making it easy to fine-tune.\nThe LoRA adapters, A and B, are not quantized. They remain full precision throughout the training. Maintaining full precision or for the LoRA adapters cause minor extra memory compared to quantized LoRA adapters."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7536/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683481870,
                "cdate": 1700683481870,
                "tmdate": 1700683481870,
                "mdate": 1700683481870,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]