[
    {
        "title": "Exploring the State and Action Space in Reinforcement Learning with Infinite-Dimensional Confidence Balls"
    },
    {
        "review": {
            "id": "iVdBxuurFD",
            "forum": "U0c2IaQhHk",
            "replyto": "U0c2IaQhHk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7398/Reviewer_u1Ph"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7398/Reviewer_u1Ph"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies online RL where (1) both state and action spaces are assumed to be infinite and (2) the transition kernel is approximated by an RKHS. The paper proposes a model-based algorithm to directly optimize over the space of transition kernels and provides a concentration analysis for the concentration of the transition kernel. The resulting algorithm is shown to have $\\tilde{O}(\\sqrt{T})$ regret. Simulation studies are included."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper proposes a possibly computationally efficient algorithm for RL with function approximation when state and action spaces are infinite.\n- Assuming that the transition kernel itself is drawn from an RKHS seems an interesting idea that has not been explored fully in prior literature."
                },
                "weaknesses": {
                    "value": "1. The proof appears to be incomplete and is hard to follow. It is not clear how the concentration analysis in A.2 helps the regret bound in A.1. It is not clear how these two parts relate to each other and it is further unclear how \"the sum of the right-hand side\" is bounded in Lemma 3, without invoking the later lemmas.\n2. Some related works appear to be missing, such as earlier works on sample-efficient RL for low-rank MDPs or MDPs with bounded Bellman-Eluder dimension, or recent works such as admissible Bellman characterization.\n3. Is it assumed, perhaps implicitly, that the reward function is known beforehand? The paper appears to estimate the transition kernel only and does not discuss how the reward function $r$ is estimated. It would be surprising if the paper can obtain the regret guarantee without any regularity assumptions on the reward function. \n4. The algorithm design appears to be similar to earlier works on RL with general function approximation: it appears to be a specialization of earlier algorithm such as OLIVE for when (1) only the transition is estimated and (2) the function class is known to be an RKHS, which allows the optimization problem to be written directly over the space of $M$.\n5. Algorithm 2 requires $S$ and $A$ to be finite, which is not assumed by Section 3.4.\n6. The remark after Algorithm 1 is a bit misleading. Without any assumption on $r$, I am not sure if $\\max_{a} Q(s, a)$ can be done efficiently."
                },
                "questions": {
                    "value": "1. Can the authors provide more details on the proof?\n2. Can the authors discuss how the paper relates to additional prior works not discussed in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682810937,
            "cdate": 1698682810937,
            "tmdate": 1699636886354,
            "mdate": 1699636886354,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kf52MqtjuC",
                "forum": "U0c2IaQhHk",
                "replyto": "iVdBxuurFD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Rebuttal:**\n\n**Q1:**\nThe proof appears to be incomplete and is hard to follow. It is not clear how the concentration analysis in A.2 helps the regret bound in A.1. It is not clear how these two parts relate to each other and it is further unclear how \"the sum of the right-hand side\" is bounded in Lemma 3, without invoking the later lemmas.\n\n**A1:**\nWe're really sorry if any process of our proof puzzled you. Let us explain it to you.\nAfter we get the Lemma 3, which means $Regret(NH) \\lesssim \\sqrt{\\beta_{N+1}}NH^2+H\\sum_{n=1}^{N}\\mathbb{P}[E_n=0]$, it is natural for us to consider bounding the right side. The first term of the right side is determined by the radius of the confidence ball, while the second term of the right side require the probability that $M^*$ is in the confidence ball. It is actually a trade-off about the confidence ball, so we want to learn more about how \"small\" can the confidence ball be if we want to ensure a high probability that $M^*$ is in the confidence ball, which is the Lemma 6, the final goal of A.2 Concentration. So we start A.2.\n\nThen in the A.3, we use Lemma 6, which shows the relation between the two terms of the right side of Lemma 3, and set optimal values to minimize the order of the sum of the two terms. Finally, by Lemma 3, we complete the bound of the Regret. \n\nWe don't exactly know if we have explained it clearly. So if there is still any problem, feel free to question us. What's more, thanks to your advice, we have added some detailed ideas of proof in our revised version.\n\n**Q2:**\nSome related works appear to be missing, such as earlier works on sample-efficient RL for low-rank MDPs or MDPs with bounded Bellman-Eluder dimension, or recent works such as admissible Bellman characterization.\n\n**A2:**\nRelated work:\n\nThere is a line of work on sample efficient RL in low rank MDPs [1][2]. [3] introduces Bellman Eluder dimension to find the minimal structure assumptions that empower sample efficient learning. [4] proposes an Admissible Bellman Characterization class that subsumes nearly all MDP models in the literature for tractable RL.\n\n[1]Agarwal A, Kakade S, Krishnamurthy A, et al. Flambe: Structural complexity and representation learning of low rank mdps[J]. Advances in neural information processing systems, 2020, 33: 20095-20107.\n\n[2]Shah D, Song D, Xu Z, et al. Sample efficient reinforcement learning via low-rank matrix estimation[J]. Advances in Neural Information Processing Systems, 2020, 33: 12092-12103.\n\n[3]Jin C, Liu Q, Miryoosefi S. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms[J]. Advances in neural information processing systems, 2021, 34: 13406-13418.\n\n[4]Chen Z, Li C J, Yuan A, et al. A general framework for sample-efficient function approximation in reinforcement learning[J]. arXiv preprint arXiv:2209.15634, 2022.\n\n**Q3:**\nIs it assumed, perhaps implicitly, that the reward function is known beforehand? The paper appears to estimate the transition kernel only and does not discuss how the reward function $r$ is estimated. It would be surprising if the paper can obtain the regret guarantee without any regularity assumptions on the reward function.\n\n**A3:**\nIn our paper, we need to know the immediate reward $r_h(s,a)$ after playing $a$ at $s$. This is in fact without loss of generality because learning about the environment $P$ is much harder than learning about $r$. In the case if $r$ is unknown, we can extend our algorithm by adding a step of optimistic reward estimation like in LinUCB. There are also works having the same assumptions on reward.[1][2][3]\n\n[1]Yang L, Wang M. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound[C]//International Conference on Machine Learning. PMLR, 2020: 10746-10756.\n\n[2]Agrawal S, Jia R. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds[J]. Advances in Neural Information Processing Systems, 2017, 30.\n\n[3]Azar M G, Osband I, Munos R. Minimax regret bounds for reinforcement learning[C]//International Conference on Machine Learning. PMLR, 2017: 263-272.\n\n**Q4:**\nAlgorithm 2 requires $S$ and $A$ to be finite, which is not assumed by Section 3.4.\n\n**A4:**\nThe simulation (Algorithm 2) is based on a special case that the state and action space is finite (and thus the kernel space must be finite-dimensional) just as the Section 3.4, where we have defined the setting of the RKHS model in this specific case."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592779863,
                "cdate": 1700592779863,
                "tmdate": 1700592779863,
                "mdate": 1700592779863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zlH1Pdkzav",
                "forum": "U0c2IaQhHk",
                "replyto": "Kf52MqtjuC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7398/Reviewer_u1Ph"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7398/Reviewer_u1Ph"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their response. \n\n- I haven't found the revision that the authors mentioned. Could you double check to see if it has been updated?\n- Details behind the proof is still missing. My original question is on how the bound in Lemma 3 can be *obtained* using the previous results. Usually this is done by some variant of elliptical potential growth lemma, which I cannot find in the proof. Even then, assuming the proof is correct, I still believe the proof to be poorly organized, and much more care need to go into writing and presenting the proof for a top tier conference such as ICLR.\n- I don't think the relationship between the proposed algorithm and earlier results such as GOLF or OLIVE has been discussed sufficiently. How is the proposed approach different from simply adapting these earlier results to the RKHS setting?\n- The assumption that the reward is known is not mentioned in the paper. Earlier works, including those cited, explicitly made this assumption clear. Even then, the assumption is not without loss of generality: we simply do not have polynomial-sample confidence bounds for all possible choices of $r$ (e.g. $r$ is nonparameteric and belongs to a function class with exponential complexity).\n- Algorithm 1 cannot be done efficiently when $r(s, a)$ is nonconcave / nonconvex in $a$ and the action space is continuous."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667862059,
                "cdate": 1700667862059,
                "tmdate": 1700667862059,
                "mdate": 1700667862059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "do4xoQXTUe",
            "forum": "U0c2IaQhHk",
            "replyto": "U0c2IaQhHk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7398/Reviewer_oU2p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7398/Reviewer_oU2p"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a model-based algorithm for episodic MDP whose transition probability is embedded in a given reproducing kernel Hilbert space. The assumption of RKHS-embedding of transition probability can handle non-linear relationships in transition probability. The proposed algorithm (RKHS-RL) estimates the transition core using ridge regression based on the collected data, and constructs an optimistic action-value function based on the infinite dimensional confidence ball. The proposed method achieves a dimension independent regret bound of $O(H \\sqrt{T})$ where $H$ is the horizon length, $T$ is the total number of interaction between the agent and the environment. Furthermore, the authors confirm the performance of finite-dimensional RKHS-RL through experiments in a simple tabular setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The motivation for the problem addressed in this paper is well-explained, drawing from the literature and related work. Additionally, the organization of the paper is appropriately structured to facilitate understanding.\n\n- The RKHS-embedding of transition probability is useful in modeling non-linear transition probabilities for state-action pairs. The author has demonstrated that the regret bound of the proposed method can achieve dimension-free sub-linear regret."
                },
                "weaknesses": {
                    "value": "- The computational complexity of the proposed method has not been addressed. In \u201cIntroduction\u201d, the authors mentioned that the proposed model can handle infinite state and action spaces. However, it seems that the computation in Algorithms 1 and 2 is heavily influenced by the size of the state-action space. It would be helpful to specify how efficient the proposed method is both statistically and computationally compared to previous algorithms, particularly for infinite state spaces.\n\n- The proposed algorithm appears similar to KernelMatrixRL (Yang & Wang, 2020). While the authors mentioned that the RKHS-embedding setting poses significant challenges compared to approach of Yang & Wang (2020), it would be beneficial to explain in detail what specific challenges arise and how they were addressed using mathematical techniques in the paper.\n\n- There is a gap between the settings discussed in the paper and the simulations. The proposed method can handle settings that previous approaches, such as tabular MDPs, linear MDPs, and parametric MDPs, cannot. However the experiments were conducted in a simple tabular setting. Additionally, it is anticipated that Q-learning or SARSA would perform significantly better in the current experiments. It would be valuable to include comparisons with other baseline algorithms."
                },
                "questions": {
                    "value": "1. How can the proposed method be applied to a continuous state-action space? The current planning approach seems challenging to implement in a continuous state-action space.\n\n2. How was it possible to achieve a tight regret bound with respect to H when compared to the regret bound of Yang & Wang (2020)?\n\n3. The regret bound of RKHS-RL does not include the effective dimension of the kernel. In that case, does the regret not depend on dimension even when using a finite-dimensional kernel? What is the key technique to eliminate dimension dependence when compared to KernelMatrixRL in Yang & Wang (2020) ?\n\n4. In the finite-dimensional case of RKHS-RL, how is the regularity bound in Assumption 2 defined? Can we assume a bound independent of the size of the state-action space in finite-dimensional case?\n\n5. Why does Figure 1-(b) show negative values for regret?\n\n6. How does the following inequality in the Proof of Lemma 2 hold? \n: $$ H || \\Phi\\_{n,h} \\tilde{\\circ} (M^* - M'\\_n) ||_1 \\le H || \\Phi\\_{n,h} \\tilde{\\circ} (M^* - M'\\_n) ||_2 $$"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7398/Reviewer_oU2p"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751262866,
            "cdate": 1698751262866,
            "tmdate": 1699636886241,
            "mdate": 1699636886241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RbtssR6KKm",
                "forum": "U0c2IaQhHk",
                "replyto": "do4xoQXTUe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Rebuttal:**\n\n**Q1:**\nThe computational complexity of the proposed method has not been addressed.\n\n**A1:**\nOur algorithm and computation does not scale with the very large dimension of the state and action space, instead it is actually related to the Hilbert space which contains real-valued functions (that we want to include) based on the state and action space. For example, if there is a really large state and action space with a finite-dimensional ($d$) feature space, then our algorithm may only scale with the dimension $d$ hidden in the constant of our RKHS regularity. This case reduces to Yang & Wang. Therefore,  in the case of infinite state spaces, if the feature space is finite-dimensional, our algorithm coincides with Yang & Wang(2020), achieving an algorithm similar to our Algorithm 2.\n\n**Q2:**\nIt would be beneficial to explain in detail what specific challenges arise and how they were addressed using mathematical techniques in the paper compared with Yang & Wang 2020.\n\n**A2:**\nObtaining a similar outcome of regret bound in an infinite-dimensional case is not trivial. On one hand, many of the techniques available in the finite-dimensional setting are difficult to use in our proof. We overcome these problems using Fr$\\acute{e}$chet derivatives, Azuma-type inequality for Banach space-valued martingales, and many other tools. \n\nOn the other hand, generalizing the model to the RKHS framework is not a trivial task. Specifically, instead of naively assuming that the transition probability $P$ belongs to an RKHS, we assume a more delicate structure of $P$ which is that $P$ has a decomposable structure, as illustrated in Assumption 1. Only on this assumption can we derive an efficient algorithm to conduct RL but still make the model quite general as long as $P$ is smooth enough. Naively assuming $P$ belongs to an RKHS will lead to an unattainable outcome. \n\n**Q3:**\nThere is a gap between the settings discussed in the paper and the simulations. \n\n**A3:**\nOur simulation is just a special case of our theory and Algorithm 1. Our theory and Algorithm 1 are much more universal and applicable to more cases since their modeling is infinite-dimensional. We have compared Algorithm 2 with established techniques such as Q-Learning and SARSA and found that our Algorithm 2 reaches a lower regret in this specific case in our latest edition. \n\n**Q4:**\nThe regret bound of RKHS-RL does not include the effective dimension of the kernel. In that case, does the regret not depend on dimension even when using a finite-dimensional kernel? \n\n**A4:**\nThe regret may depend on dimension when using a finite-dimensional kernel since constants in Assumption 2 (RKHS regularity) may depend on the dimension of the kernel. We can see A5 as an example.\n\n**Q5:**\nIn the finite-dimensional case of RKHS-RL, how is the regularity bound in Assumption 2 defined?\n\n**A5:**\nUnder our finite-dimensional case of RKHS-RL, Assumption 2 reduces to $\\sum_{s,a,\\widetilde{s}} M((s,a),\\widetilde{s})^2 \\leq C $, $d_1+d_2 \\leq C$, $d_1\\leq C$, $0\\leq c\\leq 1\\leq C$, where $d_1$ is the dimension of $\\mathcal{S}$ and $d_2$ is the dimension of $\\mathcal{A}$. Our regularity bound isn't related to the dimension of state-action space but may scale with the dimension of the kernel (this case reduces to Yang & Wang). \n\n**Q6:**\nWhy does Figure 1-(b) show negative values for regret?\n\n**A6:**\nThanks for your correction. We made some mistakes in the former edition, mistaking the step-by-step strategy obtained by Q-learning as the best strategy when calculating the regret. We have corrected this mistake in the revised edition and added some comparisons with other algorithms like Q-learning and SARSA.\n\n**Q7:**\nHow does the following inequality in the Proof of Lemma 2 hold? :\n\n**A7:**\nThanks for your question. Under the assumption that the Lebesgue measure of space $S$ is finite, we can use the Jensen's inequality and get that \n\n$H ||$ $\\Phi_{n,h}$ $\\tilde{\\circ}$ $(M^* - M'_n)$ $||_1$ \n\n$\\lesssim$  $H ||$ $\\Phi_{n,h}$ $\\tilde{\\circ}$ $(M^* - M'_n)$ $||_2 $\n\n**Q8.**\nHow was it possible to achieve a tight regret bound with respect to $H$ when compared to the regret bound of Yang & Wang (2020)?\n\n**A8.**\nWe achieve a tight regret bound with respect to $H$ when compared to the regret bound of Yang \\& Wang (2020) because the \"radius\" of our confidence ball $\\beta_n$ converges faster than Yang \\& Wang (2020)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591453921,
                "cdate": 1700591453921,
                "tmdate": 1700671361624,
                "mdate": 1700671361624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oxFo82vwzp",
                "forum": "U0c2IaQhHk",
                "replyto": "RbtssR6KKm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7398/Reviewer_oU2p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7398/Reviewer_oU2p"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response and I have no further questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737235393,
                "cdate": 1700737235393,
                "tmdate": 1700737235393,
                "mdate": 1700737235393,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PJ8DRUbdUD",
            "forum": "U0c2IaQhHk",
            "replyto": "U0c2IaQhHk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7398/Reviewer_3vbX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7398/Reviewer_3vbX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a non-parametric online RL algorithm called RKHS-RL that overcomes the curse of dimensionality in RL by utilizing reproducing kernels and the RKHS-embedding assumption. The proposed algorithm can handle both finite and infinite state and action spaces, as well as nonlinear relationships in transition probabilities. The paper provides theoretical guarantees, demonstrating that RKHS-RL achieves a sublinear regret bound, making it an effective approach for RL problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The key contributions of the paper are:\n\n1. **Theoretical Foundation**: It establishes a solid theoretical foundation for applying RKHS to reinforcement learning, providing a new perspective on how to handle the curse of dimensionality in such problems.\n\n2. **Regret Bounds**: The paper presents a significant theoretical result by proving that RKHS-RL achieves sublinear regret bounds, specifically \\( \\tilde{O}(H\\sqrt{T}) \\), where \\( T \\) is the time step and \\( H \\) is the horizon of the Markov Decision Process (MDP). This indicates that the algorithm is efficient in balancing exploration and exploitation over time.\n\n3. **Experimental evaluation**: The paper evaluates the performance of finite-dimensional RKHS-RL through simulations. The experiment examines the asymptotic property of the average value, which indicates that the solution of the function is stable. Additionally, the regret bound proposed in the paper is evaluated. The results show that the regret is bounded and align with Theorem 1."
                },
                "weaknesses": {
                    "value": "1. **Empirical Evidence**: The paper could be strengthened by including more empirical evidence to support the theoretical findings. This includes detailed comparisons with existing methods, such as those mentioned in the references, to demonstrate the practical effectiveness of RKHS-RL.\n\n2. **Scalability and Computation**: While the theoretical aspects are strong, the paper does not thoroughly address the scalability of the algorithm, especially considering the potential growth of the kernel matrix, which is a known issue in kernel methods as the number of state-action pairs increases. The paper could address potential computational bottlenecks more thoroughly, especially when scaling to very large state and action spaces.\nThe practicality of infinite-dimensional confidence balls in real-world applications is not addressed. The paper could improve by discussing how this aspect of the algorithm translates to practical implementations and what trade-offs might be involved.\n\n3. **Generalization and Application**: The paper would benefit from a discussion on the generalization capabilities of RKHS-RL across different domains and a demonstration of its application to real-world problems, which are areas of interest in the references."
                },
                "questions": {
                    "value": "0. **Typo**: Is it a typo in section 4 simulation?\n> we observe that $\\operatorname{Regret}(T) / N^{2 / 3}$ is bounded.\n1. **Assumptions of RKHS-Embedding**:\nCould you elaborate on the conditions under which the RKHS-embedding of transition probabilities is a valid assumption? Are there known classes of RL problems where this assumption may not hold?\n2. **Algorithmic Scalability**:\nHow does the RKHS-RL algorithm scale with the dimensionality of the state and action spaces in practice? Are there computational constraints that could limit its application to large-scale problems?\n3. **Comparison with Existing Methods**:\nThe paper would benefit from a comparative analysis with other RL algorithms. Have you conducted such comparisons, and if so, could you share these results?\n4. **Hyperparameter Sensitivity**:\nHow sensitive is the RKHS-RL algorithm to the choice of hyperparameters, including the selection of kernels and regularization parameters in ridge regression?\n5. **Practical Implementation**:\nCan you provide insights into the practical implementation of infinite-dimensional confidence balls? How does this concept translate into a computationally feasible algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7398/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7398/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7398/Reviewer_3vbX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7398/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699086028652,
            "cdate": 1699086028652,
            "tmdate": 1699636886128,
            "mdate": 1699636886128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2Rcg8udLND",
                "forum": "U0c2IaQhHk",
                "replyto": "PJ8DRUbdUD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7398/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Rebuttal**:\n\nThanks for your encouraging words and constructive comments. We sincerely appreciate your time in evaluating our work. Our point-to-point responses to your comments are given below.\n\n**Q1:**\nEmpirical Evidence: The paper could be strengthened by including more empirical evidence to support the theoretical findings.\n\n**A1:**\nWe have added the comparison with existing methods like Q-Learning and SARSA in our simulation of finite-dimensional case(Algorithm 2) and concluded that our algorithm achieves a lower regret.\n\n**Q2:**\nScalability and Computation: While the theoretical aspects are strong, the paper does not thoroughly address the scalability of the algorithm. The paper could improve by discussing how this aspect of the algorithm translates to practical implementations and what trade-offs might be involved.\n\n**A2:**\nAbout Scalability: Since our algorithm needs to get the estimated function $M_n$ by the idea of kernel ridge regression, the computational complexity scales with the state-action pairs. However, it does not scale with the very large dimension of the state and action space. Instead, it is related to the Hilbert space which contains real-valued functions (that we want to study) based on the state and action space. For example, if there is a very large state and action space with finite features, then our algorithm may only scale with the feature dimension $d$ hidden in the constant of our RKHS regularity. This case reduces to Yang \\& Wang. \nAbout Practicality: The practical implementation of the infinite-dimensional confidence ball is solving the maximum problem in equation (4). We present a practical approach for addressing the maximum problem. Leveraging the representer theorem, we express the objective function $M_n$ as a linear combination of kernels. Through the application of Lagrange Duality, we effectuate a conversion of the constrained optimization problem into a penalized version. By substituting the linear combination for $M_n$, we successfully reframe the original infinite-dimensional problem as a finite-dimensional quadratic counterpart. Consequently, this transformation enables an efficient solution to the problem.\n\n**Q3:**\nGeneralization and Application: The paper would benefit from a discussion on the generalization capabilities of RKHS-RL across different domains and a demonstration of its application to real-world problems, which are areas of interest in the references.\n\n**A3:**\nOur algorithm can be used in different domain of state by using different kernels. In practice, our infinite-dimensional modelling can capture nonlinearity which is more practical. What's more, RKHS-based model can tackle the curse of dimensionality and it offers greater flexibility in capturing intricate patterns and relationships. Furthermore, kernel methods can proceed with high-dimensional data.\n\n**Q4:**\nIs it a typo in section 4 simulation? (we observe that $\\operatorname{Regret}(T) / N^{2 / 3}$ is bounded.)\n\n**A4:**\nThanks for your correction. We have corrected this typo in our revised version.\n\n**Q5:**\nAssumptions of RKHS-Embedding: Could you elaborate on the conditions under which the RKHS-embedding of transition probabilities is a valid assumption? Are there known classes of RL problems where this assumption may not hold?\n\n**A5:**\nAssumption 1 can be satisfied if the coefficient of basis expansion for $P((s,a),\\tilde{s})$ decays fast enough. In specific, let $\\{a_{ij}\\}$ be the coefficient of basis expansion  $P((s,a),\\tilde{s})=\\sum_{i,j}a_{ij}h_i(s,a)g_j(\\tilde{s})$, where $\\{g_i\\}$ is the orthonormal basis in $\\mathcal{L}^2(\\mathcal{S})$ and $\\{h_i\\}$ is the orthonormal basis in $\\mathcal{L}^2(\\mathcal{S}\\times\\mathcal{A})$. Let $\\{\\gamma_i\\}$ be the eigenvalues of \n$\\mathcal{H}_1$ and $\\{\\mu_j\\}$ be the eigenvalues of $\\mathcal{H}_2$. \n\nThen Assumption 1 can be satisfied as long as $\\sum_{i,j} a_{ij}^2/\\gamma_i^3 \\mu_j < \\infty$. \nFor example, $\\{\\gamma_i\\}$ and $\\{\\mu_j\\}$ are polynomial decay, \ncorresponding to $\\mathcal{H}_1$ and $\\mathcal{H}_2$ are sobolev spaces. \n\n$\\gamma_i\\leq i^{-\\alpha}$ and $\\mu_j\\leq j^{-\\beta}$ for some $\\alpha$ and $\\beta$. If $a_{ij}\\leq i^{-3\\alpha/2-1}j^{-\\beta/2-1}$, then Assumption 1 will be satisfied. This rate of decay can be satisfied by most of the smooth functions. Details will be in the updated paper.\n\n**Q6:**\nHyperparameter Sensitivity: How sensitive is the RKHS-RL algorithm to the choice of hyperparameters, including the selection of kernels and regularization parameters in ridge regression?\n\n**A6:**\nThe choice of some hyperparameters like $\\lambda_n$ is determined in our theoretical proof. Also, we have done some tests on the choice of $\\lambda_n$ in simulation in our updated supplementary material. We can see that in this case, the order of $\\lambda_n$ is insensitive to the regret. Considering the constant coefficient term, we can get it through cross-validation.\n\tThe selection of kernel is determined by real-world setting."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7398/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589498743,
                "cdate": 1700589498743,
                "tmdate": 1700589498743,
                "mdate": 1700589498743,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]