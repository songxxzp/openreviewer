[
    {
        "title": "Incentivized Black-Box Model Sharing"
    },
    {
        "review": {
            "id": "5zDUjGsSC4",
            "forum": "VGLU5N1AD2",
            "replyto": "VGLU5N1AD2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4374/Reviewer_jHme"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4374/Reviewer_jHme"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduced an incentivized black-box model sharing framework that equitably distributes ensemble predictions and rewards parties based on their contributions. The authors (1) introduced a Weighted Ensemble Game to quantify the contribution of black-box models towards predictions; (2) derived a closed-form solution for fair reward allocation based on Weighted Ensemble Game and  Fair Replication Game; (3) theoretically proved that approximate individual rationality is satisfied. Finally, the authors also conduct numerical experiments on real-world data to confirm the efficacy of their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, this paper is well written and clearly addresses the three main questions that the authors proposed to address, each corresponding to (1) how to quantify the contributions made by each model, (2) how to ensure that each party receives a fair payment/reward and (3) how to ensure individual rationality is ensured. It also provides solid theoretical results for each of the aforementioned questions, accompanied by empirical evaluations. \n\nNonetheless, I am not an expert in the field of Black-Box Model Sharing and hence have limited expertise in evaluating the merit/weakness of this work."
                },
                "weaknesses": {
                    "value": "See questions."
                },
                "questions": {
                    "value": "(1) Could you provide one specific example that motivates why individual rationality is chosen as one of your key metrics? \n\n(2) Why do you consider Shapley fairness as your main fairness notion? Any other fairness notions that might fit into your framework?\n\n(3) In Sec 5 you suggested that \"We will later empirically show that the virtual regret $\\epsilon$ is not needed and the strict IR is satisfied\". Is this a purely empirical observation or do you believe stronger theoretical results can be established here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Reviewer_jHme"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4374/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698387140011,
            "cdate": 1698387140011,
            "tmdate": 1699636410055,
            "mdate": 1699636410055,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bcm0tR5aJq",
                "forum": "VGLU5N1AD2",
                "replyto": "5zDUjGsSC4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer jHme for reviewing our paper and for the positive and encouraging feedback!\n\nWe hope to answer your questions as follows.\n***\nQ1:\n> Could you provide one specific example that motivates why individual rationality is chosen as one of your key metrics?\n\nLet's consider a scenario where three private hospitals (A, B, and C) want to share predictions based on their patient data to improve the predictive performance of their model. However, Hospital C has already invested significant resources into collecting a high-quality dataset (with a high data diversity), while Hospitals A and B have datasets that are noisy or of lower quality in comparison. If the ensemble predictions are naively shared among all three hospitals, Hospitals A and B would benefit more because they can significantly improve their models, while it may not benefit Hospital C at all (i.e., **no collaborative gain** in the form of either monetary or improved model) and its competitors are better off. In other words, hospital C did not improve its utility by collaborating (i.e., the individual rationality is *not* satisfied) and would be disincentivized to collaborate. Therefore, enforcing individual rationality can ensure Hospital C's collaboration by ensuring its utility will improve (or positive collaborative gain).\n\nAdditionally, a similar example is used to motivate individual rationality in the case of autonomous driving to achieve a win-win outcome (in the form of legislation that enforces individual rationality) for both consumers and car companies [1].\n\nTherefore, individual rationality is one key incentive that we need to consider in collaboration.\n\n[1] Karimireddy, S. P., Guo, W., & Jordan, M. I. (2022). Mechanisms that incentivize data sharing in federated learning. *arXiv preprint* arXiv:2207.04557.\n\n***\n\nQ2:\n> Why do you consider Shapley fairness as your main fairness notion?\n\nIntuitively, the more one contributes, the more it should receive from collaboration. To formalize this intuition, there are many ways to do so. In our problem, we adopt the Shapley value as an interesting and meaningful way to achieve this because it is the unique solution that satisfies the four fairness properties (*efficiency*, *symmetry*, *dummy party*, and *linearity*) as in App. B.1.\n\n- [**symmetry**] To ensure two equally contributing parties are equally recognized, symmetry ensures that two *parties with equal marginal contributions to any coalition in the collaboration receive the same collaborative gain*;\n\n- [**dummy party**] To prevent *free-riders*, dummy party ensures that *parties with zero marginal contributions to any coalition receive no collaborative gain*.\n    \n    Additionally, it is shown by Sim et al. (2020) that Shapley value also satisfies some other fairness properties: strict desirability and strict monotonicity, which we have newly added to App. B.1. Hence, the Shapley value is a careful **design choice** in our work.\n\nAs additional justification, the Shapley value is a very widely used solution in previous data sharing works (Ghorbani & Zou, 2019; Jia et al., 2019a; Sim et al., 2020; Tay et al., 2022; Karimireddy et al., 2022;) as the notion of fairness.\n\n> Any other fairness notions that might fit into your framework?\n\nOne other common fairness notion, known as *egalitarian*, aims at achieving equitable outcomes for all the parties (e.g., by rewarding all the parties equally). This does not satisfy the dummy party property and can cause the free-rider problem since even the non-contributing party is rewarded equally with the high-contributing parties.\n\nAnother common fairness notion (in the allocation of goods) is *envy-freeness*, which is not applicable here. Because it is typically used for rivalrous goods, but the rewards here (i.e., ensemble predictions) are non-rivalrous (since they can be replicated), and it is the reason for our proposed definition of fair replication game.\n\nFor discussions of different fairness notions, we will incorporate them in the revision.\n\n***\nWe answer the other question in the subsequent comment."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157431182,
                "cdate": 1700157431182,
                "tmdate": 1700157431182,
                "mdate": 1700157431182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OeZ8iJqUqp",
                "forum": "VGLU5N1AD2",
                "replyto": "5zDUjGsSC4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q3:\n>In Sec 5 you suggested that \"We will later empirically show that the virtual regret $\\epsilon$ is not needed and the strict IR is satisfied\". Is this a purely empirical observation or do you believe stronger theoretical results can be established here?\n\nIt is *mainly an empirical observation*, but we think *stronger theoretical results may be possible*. The reason is the derivations for our theoretical result on the virtual regret $\\epsilon$ involve several inequalities (e.g., Hoeffding's inequality, the triangle inequality, the constraint of the growth function by the VC dimension, and the supremum inequality of the domain divergence measure, as in App. C.3.). The improvement of the tightness of one or more of these inequalities can potentially lead to a stronger theoretical result. We will include this comment in our revision.\n***\nWe hope our responses have addressed your concerns and helped you to understand our work better. We are happy to answer any additional questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157472692,
                "cdate": 1700157472692,
                "tmdate": 1700209219439,
                "mdate": 1700209219439,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0qXDDoYG0A",
            "forum": "VGLU5N1AD2",
            "replyto": "VGLU5N1AD2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies how to incentivize different agents to participate in black-box model sharing. \n\nMore specifically, given a set of points S, the host wants each agent to share their predictions on those points, and the host incentivizes them by giving the final ensemble predictions over these points (every agent's predictions are weighted by some weights beta), which can be used to get a new and hopefully improved model h'. The number of these additional points and the ensemble predictions on these points given to each agent is proportional to the contribution of the agent. They show a principled manner of how to measure contribution of each agent. Also, they show how to incentivize each agent to actually participate here: i.e. there's incentive for them to report their predictions because the new model h'trained with the addition of the points and ensemble predictions performs better than the previous model h. \n\nEach agent can make a payment to collect more of those points and their ensemble predictions. And the paper shows how to set up these payment values and reward values so as to guarantee some form of fairness (T1 on pg 5). \n\nThey also evaluate their approach on some datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-The main problem that they study is well-motivated, and the guarantees that they seek seem reasonable as well. It's nice that they can verify the theoretical claims in their experiments."
                },
                "weaknesses": {
                    "value": "-My main complaint of the paper is that the overall presentation was pretty hard to follow, resulting in some confusion over few details of the paper.  For instance, I\u2019m a little confused about how the weights beta_{i,x}\u2019s are set if the true label for point x is unknown. See more detailed question below. And also, it seems that there\u2019s an assumption about the unique of the optimal ensemble weights. Anyway, I think it would be helpful to add more prose to improve the overall presentation of the paper; I think the valuation part in section 6 is not too surprising but can be used as a sanity check and be moved to the appendix, which will allow more room to add more prose throughout the paper."
                },
                "questions": {
                    "value": "-The paper describes once how the ensemble weights are set in 4.1. However, here it\u2019s assumed the host actually knows the ground truth. So, is it just that in the very beginning where the host has access to a data set that\u2019s held off, the host asks the clients to participate and find these weights in the very beginning and use these weights going forward?  But more realistically, the host would want to query each party to provide predictions for points for which the true label is unknown. In those cases, how would want find these weights? Note that the way things are written, the weight beta_{i,x} is set differently for each point x, meaning one can\u2019t estimate these beta_{i,x} differently for each x, if the true label for that y is not known, but rather set a weight beta_i that\u2019s the same across all the points. This should still maintain proposition 1, as all the arguments are always averaged over the entire distribution D anyway. \n\n\n-I think there\u2019s an inherent assumption that the optimal weights beta\u2019s are unique. Consider a following example where every party has the same exact model h. Then, the ensemble model will be the same no matter how the weights beta\u2019s are set.  In this case because everyone has the same model, one should be rewarded the same reward, meaning the beta\u2019s should be uniform across every client. However, setting beta\u2019s such that it places all its weight on a single model is also an optimal solution, which results in only that client receiving all the rewards. I think this is not just an artifact of this toy example, but if the data that each client has is pretty homogenous and resulting in similar overall loss, this can be very possible (assuming that as I described above the weights should be chosen not over (party i, point x) but rather over just the parties)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4374/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775701660,
            "cdate": 1698775701660,
            "tmdate": 1699636409974,
            "mdate": 1699636409974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aslgYMIv1z",
                "forum": "VGLU5N1AD2",
                "replyto": "0qXDDoYG0A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful to the reviewers for their constructive and detailed feedback. We will incorporate this feedback into our revised work. We respond below to their concerns:\n***\nW1:\n>I\u2019m a little confused about how the weights $\\beta_{i,x}$\u2019s are set if the true label for point x is unknown.\n\nPlease kindly refer to our response below in Q1 and **App. B.2** for how $\\beta_{i,x}$\u2019s are set if the true label for point x is unknown.\n\n>add more prose throughout the paper.\n\nTo avoid significant changes (listed below) in the rebuttal as instructed by the guideline in ICLR 2024 \"*Area chairs and reviewers reserve the right to ignore changes that are significantly different from the original paper*\", we will make the following changes in the revision:\n\n(1) move the valuation part in section 6 to Appendix\n\n(2) move some remarks in Appendix to the main paper to add more elaborations\n\n(3) include more experiments with non-optimal (practical) ensemble methods in the main paper\n\n\n***\nQ1:\n>So, is it just that in the very beginning ... the host asks the clients to participate and find these weights in the very beginning and use these weights going forward?\n\nNo. In our setting, no matter what ensemble method is used, the host will query each party for each data point and continually determine the ensemble weights.\n\n>But more realistically, the host would want to query each party to provide predictions for points for which the true label is unknown.\n\nYour realistic consideration is correct. In fact, __in some of our experiments__, the host queries each party for each data point and continually determines the ensemble weights, where the true label is unknown.\n\n\n>...for which the true label is unknown. In those cases, how would the host want find these weights?\n\nWe assume the host knows the ground truth **only when** we use the optimal ensemble method. As we have described in **App. B.2**, ensemble methods from previous works such as average ensemble (AVG), majority vote (MV), knowledge vote (KV), and multiplicative weight update (MWU) do **not** need the ground truth. Those ensemble methods can set the weight $\\beta_{i,x}$ both **differently** for each party and each data $x$.\n\nOn the other hand, if the ground truth is available, we can observe a much stronger correlation (i.e., **-0.72**) between the average ensemble weight $\\mathcal{V}\\_i$ and the generalization error $L\\_{\\mathcal{D}}(h\\_i)$ on MNIST in Table 2, compared with the correlation of MV (i.e., **-0.24**) and MWU (i.e., **-0.24**) on MNIST in table 1.\n\nWe conduct an **additional experiment** (update in App. D.1) and highlight that the requirement on the ground truth is **not** very restrictive, where the labeled data (i.e., ground truth) of size as small as 100 is sufficient to identify high-quality models, as shown in Fig. 6 in App. D.1. The negative correlation of size 100 is as good as that of size 4000. In practice, those weights estimated on the small labeled dataset can be potentially used for other unlabeled data.\n\n***\nWe address the other concern in the subsequent comment."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156512112,
                "cdate": 1700156512112,
                "tmdate": 1700156512112,
                "mdate": 1700156512112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "STM4npN9kz",
                "forum": "VGLU5N1AD2",
                "replyto": "n3TzWlMkH9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications"
                    },
                    "comment": {
                        "value": "Sorry about the late response to the rebuttal, and thanks for the clarifications! \n\nSince there isn't that much time remaining for the rebuttal period, I'll be sure to make my questions brief. I'm still a little hung up on the weights when the ground truth is not known. \n\n\nDon't a lot of the theoretical results hinge upon the weights being chosen according to the optimal weights when the ground truths are known? For instance, I think intuitively that if I just choose the weights arbitrarily (e.g. just always listen to a particular agent for whatever reason), I shouldn't expect to get a lot of the nice properties describe in Section 5.\n\nIt seems to me that the proposed approaches when the ground truth is not known are mostly 'heuristics' to be close to the optimal weights; I do see that for multiplicative weights, you can in fact get a no-regret style closeness to the optimal weights.\n\nIt would be great if my concern here is valid or not.\n\n\nAnd adding a regularization term to enforce a uniqueness of the optimal weights makes sense to me!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674086349,
                "cdate": 1700674086349,
                "tmdate": 1700674086349,
                "mdate": 1700674086349,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qACMS9RZYT",
                "forum": "VGLU5N1AD2",
                "replyto": "0qXDDoYG0A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_hnks"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I'm aware that the results still hold true regardless of what the chosen weights are, but my point is that the results hold only with respect to the given weights. \n\nSo, if I choose arbitrary weights, the contribution determined by these weights would still be arbitrary and the theoretical guarantees would be arbitrary as it's with respect to those arbitrary weights. Therefore, in order to fully capture the contribution of each agent faithfully, we better choose \"good\" weights that truly captures the contribution of each agent because the good theoretical guarantees hinge upon the fact that the weights are faithfully measuring the contribution. Simply said, the quality of the theoretical guarantees hinge upon the quality of the weights.\n\nAnd my concern is that coming up with good weights and verifying that those are good weights is hard to do when there are no ground-truth; the paper is only empirically showing the previous ensemble methods seem to do well, but these are just practical heuristics, and there's no theoretical guarantee that these are good weights. Note that even in the experiments, a hold-out dataset is being used to show that the ensemble methods are performing well. But in the absence of any hold-out dataset with ground truths, how can one even tell whichever chosen ensemble method is actually obtaining good weights or not?\n\nOr am I still misunderstanding the results?"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725964311,
                "cdate": 1700725964311,
                "tmdate": 1700726034503,
                "mdate": 1700726034503,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J4xuBBaJu9",
            "forum": "VGLU5N1AD2",
            "replyto": "VGLU5N1AD2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4374/Reviewer_gZai"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4374/Reviewer_gZai"
            ],
            "content": {
                "summary": {
                    "value": "* This paper proposes a theoretical framework for incentivized black-box model sharing, based on cooperative games.\n* On the first stage of interaction, each party $i\\\\in[n]$ trains a multiclass classifier $h_i(x)$ using distribution $\\\\mathcal{D}_i$, but are interested in maximizing performance on a different distribution $\\\\mathcal{D}$. \n* The trained classifiers are sent to a trusted party, and combined into an ensemble model $h_N(x)=\\\\sum_i \\\\beta_{i,x} h_i(x)$. The trusted party evaluates $h_N$ on a dataset $U\\\\sim\\\\mathcal{D}^T$ from the target distribution, and performance is translated into fair rewards $r_i$ for each party by the weighted ensemble game (WEG) mechanism.\n* The WEG mechanism is based on Shapley values of a fully-additive cooperative game. The contribution of the $i$-th party is assumed to be equal to the average ensemble weight of their predictor ($\\\\sum_{x\\\\in U} \\\\beta_{i,x}/T$).\n* On the second stage, each party is allowed to add $p_i$ monetary funds to increase their reward, and additional rewards $r_i^+$ and payments $p_i^+$ are distributed fairly by the fair replication game (FRG) mechanism, relying on Theorem 1.\n* Once the final reward values are set, rewards ($r_i+r_i^+$) are realized as iid samples from the set $\\\\{(x,h_N(x)\\\\}_{x \\\\in U}$, and offset payments $p_i-p_i^+$ are realized as monetary transfers.\n* Empirical evaluation is performed on MNIST, CIFAR-10 and SVHN, demonstrating accuracy gains in several settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Problem is well-motivated. Two-stage collaborative game structure is an interesting design approach.\n* Makes effort to support key assumptions (e.g for valuation functions).\n* Empirical evaluation supports claims and provides confidence bounds. Documented code is provided."
                },
                "weaknesses": {
                    "value": "* Limitations of the proposed method are not discussed clearly.\n* Unclear applicability for practical ensemble methods: Average ensemble weight is uncorrelated with the objectives of the parties (Table 1), experiments are performed with an \"ideal method\" (Section 4.1).\n* Presentation is dense, and was hard for me to follow. Many remarks which were very helpful to my understanding only appeared in Appendix A."
                },
                "questions": {
                    "value": "* Motivation: Under which conditions is the model incentive structure realistic, and the valuation assumption applicable? In the hospital example mentioned in Appendix A (Q2), it is reasonable to assume that every hospital has access to a data source $\\\\mathcal{D}_i$ based on their local population, however it doesn\u2019t seem intuitive to me that the hospital would desire a classifier that has good performance on a population $\\\\mathcal{D}$ which is different than their own, and common to all other hospitals. Can you clarify this example, or give a different practical example where assumptions intuitively hold?\n* How does the method perform under practical (non-ideal) ensemble methods?\n* Price of fairness: If I understand correctly, it seems that the overall welfare of the parties ($\\\\sum_i L_{\\\\mathcal{D}}(h_i)$) would be maximized by sharing all target-dataset data $\\\\{(x_t,h_N(x_t)\\\\}_{t=1}^T$ with all parties. What are the shortcomings of this approach? How does its welfare compare to the mechanism presented in the paper?\n* What is the relation between the objective $L_\\\\mathcal{D}(h_i)$ and the utility $u_i$ presented in Theorem 1? Also, is it possible to quantify the relation between payment and accuracy increase for a given problem instance?\n* Technical questions: What is the meaning of the notation $\\\\hat{L}_{\\\\mathcal{D}}(h,h_N)$ in Section 5.2? Is there an upper bound on the size of realized reward $T_i$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Reviewer_gZai"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4374/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778238536,
            "cdate": 1698778238536,
            "tmdate": 1699636409890,
            "mdate": 1699636409890,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jJ6WfrtXd9",
                "forum": "VGLU5N1AD2",
                "replyto": "J4xuBBaJu9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer gZai for taking the time to review our paper and for providing a very detailed summary and questions and positive feedback that our problem is **well-motivated**, our design approach is **interesting**, and for appreciating our efforts in supporting key assumptions and empirical evaluations.\n\nWe would like to address the comments as follows.\n\n***\nW1:\n> Limitations of the proposed method are not discussed clearly.\n\nWe have added a section (App. A.1) discussing the limitations and will make this clearer in our revision.\n\nFor example, one limitation of our method is that it is developed for ensemble methods that take the form of weighted sum formulation (in Section 4.1), which does appear in several common ensemble methods (mentioned in Section 4.1).\n\n\n***\nW2:\n> Unclear applicability for practical ensemble methods\n\nWe indeed have experimental *results for non-ideal ensemble methods* (which are more practical) and have also *included additional experiments*.\n\n- [Results for non-ideal ensemble methods.] The results are in Tables 4 and 5 (in App. D.1). We observe a **stronger correlation** between the average ensemble weight $\\mathcal{V}\\_{i}$  and the generalization error $L\\_{\\mathcal{D}}(h\\_{i})$ with the practical ensemble methods (i.e., MV, KV, and MWU) as shown in Table 5 under the non-i.i.d. data setting, which we believe is more important because the setting is more realistic. If the ensemble is bad (e.g., AVG shown in Table 5), our method of valuation and allocation still works, but parties with better models may not be identified as such in the collaboration.\n\n- [Additional experiments.] We included **additional results** in our paper, in Fig. 8 (in App. D.2.1), to demonstrate the accuracy gains w.r.t. different non-ideal (practical) ensemble methods used. In particular, we observe that *strict IR is always satisfied*.\n\n    For the additional experiment on CIFAR-10, we will include it if it is complete before the end of the rebuttal; otherwise, we will include it in the revision.\n\nAs these different (non-ideal) ensemble methods produce predictions of varying qualities, we use the ideal (i.e., optimal ensemble) method to illustrate the effect of an ideal case of collaboration. The non-ideal ensemble methods might not be as effective in achieving fairness or IR as the ideal ensemble, but they can be applied nevertheless.\n\n***\n\nW3:\n> Many remarks which were very helpful to my understanding only appeared in Appendix A.\n\nThank you for taking the time to carefully read our prepared appendix. It would be very helpful if the reviewer would let us know the specific remarks (currently in appendix) that would aid the understanding of the reader, if moved to the main paper. We will improve the presentation accordingly.\n\n***\nWe address the other concerns in the subsequent comment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700155587885,
                "cdate": 1700155587885,
                "tmdate": 1700155587885,
                "mdate": 1700155587885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B9tgKuTWci",
                "forum": "VGLU5N1AD2",
                "replyto": "aIW7yzutkf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_gZai"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_gZai"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response, and for the clarifications made in the revision. I have no further questions. \n\nFor improving clarity (W3), I found Figure 5 and Appendix A.2 Q2 to be helpful, and I believe that a greater emphasis on concise intuitive examples will improve the overall clarity of the exposition."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609849250,
                "cdate": 1700609849250,
                "tmdate": 1700609849250,
                "mdate": 1700609849250,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kqvKqXzZ66",
            "forum": "VGLU5N1AD2",
            "replyto": "VGLU5N1AD2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4374/Reviewer_1Wgz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4374/Reviewer_1Wgz"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a framework for model sharing across parties. In relation to prior work, this paper considers incentives, as well as parties only sharing their model (rather than data which can be sensitive). The framework distributes rewards in proportion to the contribution of each party, and also allows for payments between parties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Tackles an important and practical problem of considering incentives in the context of model sharing\n- Model enforces desirable properties such as fairness and IR, and combines many practical considerations together\n- Analysis is thorough"
                },
                "weaknesses": {
                    "value": "The main weakness is in the exposition - I was not able to understand the model. It seemed like the model and problem formulation were not comprehensively specified. The fact that there is an FAQ section on the model speaks to how the model is not completely clear. Here are my questions that I couldn\u2019t find answers to:\n- How should we compare prediction error to monetary payments to \"rewards\" (samples of ensemble predictions)? (Do they use the same unit of measurement?) \n- Relatedly, what is the formula for the utility of party i? \n- Payments can be made from one party to another. Does each party, decide on their own, how much to pay to each other party, or is this transfer also specified as part of the mechanism? Does each party have a budget?\n\nThe model has two main parts, as described in Figure 5. Can we simply de-couple these two stages and study each part separately, or are there interactions that require studying them together? Just studying one aspect would make the paper simpler and more clear."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4374/Reviewer_1Wgz"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4374/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806548533,
            "cdate": 1698806548533,
            "tmdate": 1699636409808,
            "mdate": 1699636409808,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UMFpYMbW9Z",
                "forum": "VGLU5N1AD2",
                "replyto": "kqvKqXzZ66",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer 1Wgz for taking the time to review our paper and for recognizing that our studied problem is **important and practical**, our model achieves **desirable properties**, and our analysis is **thorough**. We wish to provide the following clarifications and have incorporated some changes in our revision (with highlighted text in the updated pdf).\n\n---\n\n>The fact that there is an FAQ section on the model speaks to how the model is not completely clear.\n\nOur provided Q\\&A section in App. A.2 is meant to further supplement the main paper with more nuanced and specific details. In our revision, we will improve the exposition of our work to specify the problem formulation more clearly.\n\nW1:\n> How should we compare prediction error to monetary payments to \"rewards\" (samples of ensemble predictions)? (Do they use the same unit of measurement?)\n\nThe monetary payments (i.e., $p_i$) and the value $r_i$ of the rewards (of size $T_i$) can be viewed to use the same unit of measurement (i.e., in the same domain). This is by design so that the allocation mechanism (Theorem 1) enables a translation between ensemble predictions (i.e., the realized rewards $T_i$) and monetary payment. We note that this has not been previously achieved by existing works.\nPlease refer to Q4 in App. A where we illustrate how the monetary currency is projected into the positive real domain $\\mathbb{R}_+$ to align with the reward domain.\n\nWhen you say \"compare prediction error to monetary payments\", we assume you mean the relationship between the two. To elaborate more on the relationship: An increase in the monetary payment $p_i$ leads to a higher reward $r_i$, and thus a larger size for $T_i$, which can decrease the upper bound on the prediction error $L_{\\mathcal{D}}(h_i')$:\n\n- From Proposition 2, the upper bound of **prediction error** (i.e., $L_{\\mathcal{D}}(h_i')$) depends on **payment** $p_i$, because $L_{\\mathcal{D}}(h_i') \\leq L_{\\mathcal{D}}(h_i)+\\epsilon_i$ and $\\epsilon_i$ depends on $T_i$. To compare the prediction error to monetary payment with approximation, we investigate \"the change in $\\epsilon_i$\" as a function of $p_i$ in a newly added empirical result (i.e., Fig. 9 in App. D.2.1), which provides an empirical quantification between the accuracy gain w.r.t. the increase in (monetary) payment.\n***\nW2:\n>Relatedly, what is the formula for the utility of party i?\n\nBy **definition**, $u_i$ is the Shapley value to the linearly combined game of $G$ and $G^p$. By **derivation**, the explicit **formula** for the utility $u_i$ of party $i$ is given in Theorem 1: $u_i = r_i + r_i^+ + p_i^+ - p_i $, where \n- $r_i=\\phi_i$,\n- $r_i^+ = \\frac{\\mathcal{V}_N\\times p_i}{\\mathcal{V}_N - \\phi_i}$,\n- $p_i^+=\\sum_{j \\in N \\setminus i }\\frac{\\phi_i \\times p_j}{\\mathcal{V}_N - \\phi_j}$, and\n- $p_i\\in \\mathbb{R}_+$ is determined by the party itself.\n\nThe utility $u_i$ represents the *overall gain* of party $i$ from the collaboration, as a sum of $i$'s total reward $(r_i + r_i^+)$ and payoff/net compensation $(p_i^+-p_i)$.\n\n- $(r_i + r_i^+)$ represents the *value* of ensemble predictions that party $i$ receives from the collaboration,\n- $p_i^+$ represents the value of the received monetary compensation from all other parties, and\n- $p_i$ is the payment that party $i$ made to the host in exchange for additional rewards of the value $r_i^+$.\n\nOur formula of the utility fully specifies all the terms. Note that these interpretations (of the utility) are provided in the main paper (above Theorem 1 and below Remark 1).\n\n---\nW3:\n> Payments can be made from one party to another. Does each party, decide on their own, how much to pay to each other party, or is this transfer also specified as part of the mechanism?\n\nEach party decides on its own how much to pay *in total* (i.e., $p_i$) to the central host; the exact distribution of $p_i$ to each other party is **specified by our mechanism** (and managed by the central host) to achieve fairness: The proposed formula of $p_i^+=\\sum_{j\\in N\\setminus \\{i\\}}\\frac{\\phi_i \\times p_j}{\\mathcal{V}_N - \\phi_j}$ in Theorem 1 specifies this payment distribution from every party $j\\in N\\setminus \\{i\\}$ to party $i$.\n\n> Does each party have a budget?\n\nOur mechanism can achieve fairness whether or not a party has a budget constraint:\n\n- [With budget constraint.] If party $i$ has a budget of $B_i$, it can freely choose payment on its own as long as $p_i \\leq B_i$, and fairness is always achieved by the reward design and realization, as shown in App. C.6.\n\n- [W/o budget constraint.] If party $i$ does not have any budget constraint, then it can make the maximal payment $p_i^\\ast = (\\phi^\\ast -\\phi_i)(\\mathcal{V}_N-\\phi_i)/\\phi^\\ast$ to receive all the reward of value $\\mathcal{V}_N$, which is mentioned in the paragraph below Remark 1. In this case, fairness is achieved by fairly distributing the payments.\n\n---\nWe address the other concerns in the subsequent comment."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153505093,
                "cdate": 1700153505093,
                "tmdate": 1700153505093,
                "mdate": 1700153505093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lZKmbV7Eyd",
                "forum": "VGLU5N1AD2",
                "replyto": "lwgwqBIZJn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_1Wgz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4374/Reviewer_1Wgz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their detailed clarifications. My original questions have been clarified.\n\nI still think that the paper is very dense and difficult to follow. Specifically, section 3 should completely specify the model, which it currently does not do rigorously. I found reviewer gZai's summary of the paper to be a clearer description of the model. Here are a couple of examples of sources of confusion:\n- The reward is not defined - I believe it corresponds to a scalar value, but it is initially introduced as a set of predictions. \n- This section should also clearly delineate which aspects of the process will be specified as the main contributions of the paper (Section 4+5). For example, after the sentence \"parties are allowed to make monetary payments $p_i$ ...\" - the authors should write that this payment mechanism will be detailed in Section 5 (and perhaps the desirable properties of this mechanism should also be written here). Essentially, it was unclear which parts are taken as given / as definition, and which parts represent the main contribution. \n- There is a paragraph about the valuation function, but at this point it is completely unclear why this is relevant and how this relates to the model specified in the previous paragraph. It is written that the shapley value represents the \"fair contribution of party i\", but the fair contribution was never defined. \n\nI don't need the authors to respond individually to the above points since this is late (my apologies). But in summary, I found the paper difficult to follow because there was no roadmap and I felt that many descriptions and terms came out of nowhere. Looking at the paper, I do think much of my concerns can be alleviated if Section 3 was significantly clarified."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4374/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676811976,
                "cdate": 1700676811976,
                "tmdate": 1700676811976,
                "mdate": 1700676811976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]