[
    {
        "title": "An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization"
    },
    {
        "review": {
            "id": "W99mxEMEci",
            "forum": "rpH9FcCEV6",
            "replyto": "rpH9FcCEV6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4729/Reviewer_nuWm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4729/Reviewer_nuWm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for conducting a membership inference attack on a diffusion model. Compared to previous methods (SecMI), the proposed method achieves better results while accelerating the process by 5-10 times. The effectiveness of the method is validated on multiple datasets in the paper. Additionally, this paper also analyzed the MIA robustness of diffusion on speech-to-text task, and the authors suggests using models that directly output audio to enhance the MIA robustness of the model."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposed a method achieved a 5-10 times improvement compared to previous SOTA method (SecMI) and conducted extensive experiments to evaluate the proposed method. Strengths are listed below:\n\n1.\tThis paper conducted extensive experiments, including three image datasets and three audio datasets. In particular, this paper also conducted experiments on stable diffusion, including scenarios where the ground truth text was unknown.\n2.\tExperiments demonstrated that the proposed method achieved a 5-10 times improvement compared to previous SOTA method (SecMI), while also achieving better results.\n3.\tThis paper analyzed the threat model. When the diffusion model is used for some tasks such as inpainting and classification, it requires the use of intermediate results. In such cases, their method can be employed to attack the model. It\u2019s reasonable.\n4.\tExperimental results showed that diffusion models directly outputting audio are more robust compared to models outputting mel-spectrograms. This paper recommended using models that directly output audio to enhance robustness."
                },
                "weaknesses": {
                    "value": "The paper is overall clear and sound, but I still have some minor concerns:\n\n1.\tWhen attack audio, this paper also uses these methods proposed from image attacks (NA, SecMI). The methods for attacking audio should be included as well.\n2.\tIn the loss function of the diffusion model, there is a noise term. The proposed method is based on the loss function, but the method to remove the influence of the random seed is not described clearly.\n3.\tFig. 2 is too small to read."
                },
                "questions": {
                    "value": "1. It is not clear how hyperparameters are selected for other methods.\n2. Why audio data is more robust against the proposed method?\n3. It is better to present the procedure of the proposed method in one table."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4729/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4729/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4729/Reviewer_nuWm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698624648001,
            "cdate": 1698624648001,
            "tmdate": 1699636454730,
            "mdate": 1699636454730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4JZ1BcrjkU",
                "forum": "rpH9FcCEV6",
                "replyto": "W99mxEMEci",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1: When attack audio, this paper also uses these methods proposed from image attacks (NA, SecMI). The methods for attacking audio should be included as well.**\n\nThank you for your comment. To our knowledge, no specific attacks target TTS tasks.\n\nWe conducted a thorough investigation in the Text-to-Speech domain, particularly on diffusion models, and found no TTS-specific attacks. Consequently, we utilized the state-of-the-art diffusion Membership Inference Attack (MIA) method for images. However, we acknowledge the possibility of overlooking certain methods. If you know of any, we'd be glad to include them in our experiments.\n\n**W2: In the loss function of the diffusion model, there is a noise term. The proposed method is based on the loss function, but the method to remove the influence of the random seed is not described clearly.**\n\nThanks for your valuable comment. We believe we have removed the influence of random seed.\n\nIn fact, although the form of Eq. 9 may appear similar to the training loss, the input for our $\\boldsymbol \\epsilon$ is the model output at time $t=0$, which results in a deterministic outcome. Furthermore, in the case of continuous time, as seen in Eq. 14, its form is entirely different from the training loss. In addition, the results for DDPM are obtained from the checkpoint and dataset provided in their official repository [r1], while the result of stable diffusion is obtained from the checkpoint provided by HuggingFace [r2]. We also conduct experiments on different dataset splits (Fig. 2). Consequently, we believe we have eliminated the effects of random seed.\n\n**W3: Fig. 2 is too small to read.**\n\nThanks for your valuable comment. Parts of the subfigures from Figure 2 have been moved to the appendix. They have also been enlarged to improve their readability in the new version.\n\n**Q1: It is not clear how hyperparameters are selected for other methods.**\n\nThank you for the question. In order to ensure fairness, when conducting experiments, for the same experiment, we choose the default settings in the paper of the method, as well as the checkpoints, data splits, etc. provided by them. For different experiments, especially TTS, we utilize grid-search to find the optimal parameters that the method can achieve. We have put this into Appendix A  in the new version.\n\n**Q2: Why audio data is more robust against the proposed method?**\n\nThanks for your question. In fact, Models that directly output audio not only exhibit robustness towards our proposed method, but also towards the baseline we have chosen. The output format of the model could be the reason. For example, a sentence breaks down into finite linguistic units or morphemes, and the model might learn their corresponding audios, becoming attack-resistant once all units are learned. \n\nWe've analyzed various Text-to-Speech (TTS) pipelines using the same dataset. All models directly creating audio exhibited robustness to MIA, suggesting input type and architecture aren't likely causes. Interestingly, mel-spectrograms resemble one-channel images, hinting again that output type could be a factor. However, this is a hypothesis, and as stated in our paper, further investigation is needed to confirm the reasons behind these observations.\n\n**Q3: It is better to present the procedure of the proposed method in one table.**\n\nThank you for your suggestion. We have added a table to the paper to make our methods easier to understand in section B of the new version.\n\n[r1] https://github.com/jinhaoduan/SecMI\n\n[r2] https://huggingface.co/runwayml/stable-diffusion-v1-5"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700438730392,
                "cdate": 1700438730392,
                "tmdate": 1700438730392,
                "mdate": 1700438730392,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4SKXxPD7Ul",
            "forum": "rpH9FcCEV6",
            "replyto": "rpH9FcCEV6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4729/Reviewer_Vi3v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4729/Reviewer_Vi3v"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel membership inference attack, Proximal Initialization Attack (PIA), for diffusion models. By using the output at time $t=0$ and calculating the difference between the model outputs at time $t$ and $t-1$, it is possible to determine whether a sample is present in the training dataset. The proposed method is validated for its effectiveness on both image and audio datasets. The results show that compared to the baseline, this method achieves better performance while achieving a 5-10 times speedup."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper aligns with the scope of the conference.\n2. The paper is easy to understand, and its motivation is clear.\n3. The experiments in the paper are extensive. The proposed method is validated on multiple datasets and multiple models in both image and audio domains, demonstrating its effectiveness. Additionally, the results on the audio dataset indicate that the model with audio output exhibits greater robustness.\n4. They demonstrated the scenarios in which the proposed method can be applied (threat model in other word)."
                },
                "weaknesses": {
                    "value": "1. Some of the figures in the paper are difficult to read, due to the fact that too many datasets and models are included in the same figure.\n2. The pipeline of the TTS tasks is not explained in the paper.\n3. Some details of the experimental setup are not clearly described. Specifically, in the experiment on stable diffusion, the checkpoint of BLIP and the specific prompt used are not mentioned in this paper."
                },
                "questions": {
                    "value": "1. How is the query number for the comparison methods determined? Is it possible that the comparison methods used in the article could yield better performance with a lower query number?\n2. For the TTS task, this article explores models that generate output in the form of audio and mel-spectrogram. Are there any other types of outputs for the TTS task? It is interesting to investigate whether the other outputs is robust for the TTS task."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745270171,
            "cdate": 1698745270171,
            "tmdate": 1699636454597,
            "mdate": 1699636454597,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VjAx0yNlBY",
                "forum": "rpH9FcCEV6",
                "replyto": "4SKXxPD7Ul",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1: Some of the figures in the paper are difficult to read, due to the fact that too many datasets and models are included in the same figure.**\n\nThank you for your suggestion. A subset of the subfigures from Figure 2 has been moved to section B of the appendix. All figures have also been enlarged to improve their readability in the new version.\n\n**W2: The pipeline of the TTS tasks is not explained in the paper.**\n\nThanks for your valuable comment. We have incorporated the pipeline of the Text-to-Speech (TTS) task into the paper for the readers' understanding in section 4.4 of  the new version. \n\nSpecifically, according to reference [r1], the TTS task pipeline can be divided into three parts: text to mel-spectrogram, text to audio, and mel-spectrogram to audio. We have tested models for each pipeline in our experiments. More specifically, Grad-TTS falls under the category of text to mel-spectrogram, DiffWave is categorized under mel-spectrogram to audio, and FastDiff is classified as text to audio.\n\n**W3: Some details of the experimental setup are not clearly described. Specifically, in the experiment on stable diffusion, the checkpoint of BLIP and the specific prompt used are not mentioned in this paper.**\n\nThanks for your valuable comment. The prompt we used in BLIP is \"A picture of \". More specifically, we downloaded the checkpoint from reference [r2], and when inputting each image, we also input the aforementioned prompt.\n\nWe have provided more details, including the prompt above, and organized the presentation to make it more readable in the new version.\n\n**Q1: How is the query number for the comparison methods determined? Is it possible that the comparison methods used in the article could yield better performance with a lower query number?**\n\nThank you for the question. For the NA method, the query number is fixed at 1. However, for SecMI, the query number is directly proportional to the chosen attack parameter $t$. We selected the most effective $t$ through a grid search. Therefore, for SecMI, the performance is worse with the same query number.\n\n**Q2: For the TTS task, this paper explores models that generate output in the form of audio and mel-spectrogram. Are there any other types of outputs for the TTS task? It is interesting to investigate whether the other outputs is robust for the TTS task.**\n\nThank you for your question.  The types of output are categorized into audio and mel-spectrogram according to [r1]. And to the best of our knowledge, there are no specific attacks targeting TTS task.\n\nWe have carried out a meticulous survey in the field of Text-to-Speech focusing on the diffusion model, and as far as we know, there is no specific attacks targeting TTS tasks. Therefore, we have employed the SOTA diffusion Membership Inference Attack (MIA) method used on images. Nevertheless, it is possible that we may have missed some methods. If you are aware of any such methods, we would be delighted to incorporate them into our experiments.\n\n\n[r1] Zhang, C., Zhang, C., Zheng, S., Zhang, M., Qamar, M., Bae, S. H., & Kweon, I. S. (2023). A survey on audio diffusion models: Text to speech synthesis and enhancement in generative ai. arXiv preprint arXiv:2303.13336, 2\n\n[r2] https://huggingface.co/Salesforce/blip-image-captioning-large"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700437921533,
                "cdate": 1700437921533,
                "tmdate": 1700438324444,
                "mdate": 1700438324444,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wy51irDEn9",
            "forum": "rpH9FcCEV6",
            "replyto": "rpH9FcCEV6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4729/Reviewer_7qyp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4729/Reviewer_7qyp"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach to membership inference attacks on diffusion models named Proximal Initialization Attack (PIA), focusing on efficiency. The proposed method utilizes the distance between the true trajectory and the estimated trajectory, and the estimated trajectory is obtained by the output of models at $t=0$. With just 2 queries, this approach achieves superior AUC and TPR@1%FPR compared to previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper is well-written and easy to read.\n* The experiments are conducted extensively across multiple datasets and models in the domains of images and audio, confirming the effectiveness of the proposed method. A thorough ablation study is also conducted to investigate the patterns of various parameters.\n* The results of the experiment are promising, and the experiment is convincing. The proposed method is much faster than the baseline, which is particularly useful for large-scale models."
                },
                "weaknesses": {
                    "value": "* The experimental results are promising. It would be more convincing if you could provide more details, such as the selection of all experimental parameters, including the comparative methods.\n* Eq.9 is similar to the original loss function. Is there any connection between the proposed method and the loss function? Please provide further analysis."
                },
                "questions": {
                    "value": "* Why is mel-spectrogram more robust than directly outputting audio? \n* Does the model that directly outputs audio have any other costs compared to outputting mel-spectrogram?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812254391,
            "cdate": 1698812254391,
            "tmdate": 1699636454516,
            "mdate": 1699636454516,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T3Jrv01JjK",
                "forum": "rpH9FcCEV6",
                "replyto": "Wy51irDEn9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1: The experimental results are promising. It would be more convincing if you could provide more details, such as the selection of all experimental parameters, including the comparative methods.**\n\nThank you for your advice. We have placed most of the experimental settings, including model training details and parameter settings, in Appendix A.1. For comparative methods, the chosen parameters are either the default ones from the original papers or those that produced the best results after grid search. We have provided more details, such as the details for Blip, and organized the presentation to make it more readable in the revision version.\n\n**W2: Eq.9 is similar to the original loss function. Is there any connection between the proposed method and the loss function? Please provide further analysis.**\n\nThanks for your valuable comment. We believe there is little connection between them.\n\nWhile the form of Eq. 9 may appear similar to a loss function, there are some distinctions. In the loss function, the input $\\boldsymbol\\epsilon$ is a Gaussian noise, while in our case, it is the output of the model at time $t=0$. Furthermore, when our method is applied to continuous-time diffusion models, as in Eq. 14, the difference from the training loss becomes evident.\n\n**Q1: Why is mel-spectrogram more robust than directly outputting audio?**\n\nThank you for your question. The output format of the model could be the reason.\n\nWe've analyzed various Text-to-Speech (TTS) pipelines using the same dataset. All models directly creating audio exhibited robustness to MIA, suggesting input type and architecture aren't likely causes. Interestingly, mel-spectrograms resemble one-channel images, hinting again that output type could be a factor. For example, a sentence breaks down into finite linguistic units or morphemes, and the model might learn their corresponding audios, becoming attack-resistant once all units are learned. However, this is a hypothesis, and as stated in our paper, further investigation is needed to confirm the reasons behind these observations.\n\n**Q2: Does the model that directly outputs audio have any other costs compared to outputting mel-spectrogram?**\n\nThank you for your question. As mentioned in [r1], models that directly output audio are still under development. The cost of employing an end-to-end model lies in the difficulty of achieving better text-to-speech performance compared with GradTTS.\n\n\n[r1] Zhang, C., Zhang, C., Zheng, S., Zhang, M., Qamar, M., Bae, S. H., & Kweon, I. S. (2023). A survey on audio diffusion models: Text to speech synthesis and enhancement in generative AI. arXiv preprint arXiv:2303.13336, 2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700437448064,
                "cdate": 1700437448064,
                "tmdate": 1700437448064,
                "mdate": 1700437448064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gV5n2Lw64C",
            "forum": "rpH9FcCEV6",
            "replyto": "rpH9FcCEV6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4729/Reviewer_Jy2D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4729/Reviewer_Jy2D"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), by using groundtruth trajectory obtained by \u03f5 initialized in t = 0 and predicted point to infer memberships. Experimental results demonstrate the effectiveness of the proposed method on vision and text-to-speech tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This paper is well-written and clear.\n2. The idea is very simple and novel.\n3. Very good experimental results."
                },
                "weaknesses": {
                    "value": "1. The motivation of some transformations in the method is not very clear.\n2. The third contribution about the experiments is too long and redundant.\n3. There exists some grammar errors or typos, like \u201cMIA for generation tasks \u2026 has also been extensively researched\u201d in the related work section."
                },
                "questions": {
                    "value": "1. In Eq. (14), the motivation to ignore the high-order infinitesimal term and dt is not clear.\n2. Why the proposed method can obtain better performance than the best competitors in Tables 1-3?\n3. How to apply the proposed method to discrete-time diffusion models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698978093383,
            "cdate": 1698978093383,
            "tmdate": 1699636454427,
            "mdate": 1699636454427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PbmHLcTrxP",
                "forum": "rpH9FcCEV6",
                "replyto": "gV5n2Lw64C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1: The motivation of some transformations in the method is not very clear.**\n\n\nR1: Thank you for your insightful comment. Our motivation primarily stems from our fundamental idea. \n\nThe concept involves obtaining a sample $\\boldsymbol x_t$, using the model to obtain $\\boldsymbol x_{t+\\Delta}$, and subsequently acquiring $\\boldsymbol x_{t}'$. If the model fits this sample better, it is more likely to be within the training set. Simultaneously, as $\\Delta t$ increases, the error also increases. Therefore, we chose the smallest value for $\\Delta t$. With this choice of $\\Delta t$, all our transformations (mainly Eq. 9 and Eq. 13) can be naturally derived.\n\n**W2: The third contribution about the experiments is too long and redundant.**\n\nThank you for your suggestion. We have revised this part of our contribution in a more concise language.\n\nThe modified contribution is below:\nOur evaluations show that PIA matches SecMI's AUC performance and outperforms it in TPR @ 1\\% FPR, while being 5-10 times quicker. Moreover, our data imply that in text-to-speech tasks, models producing audio are more resistant to MIA attacks than those generating image-like mel-spectrograms. We therefore suggest using audio-output generation models to minimize privacy risks in audio creation tasks.\n\n**W3: There exists some grammar errors or typos, like \u201cMIA for generation tasks \u2026 has also been extensively researched\u201d in the related work section.**\n\nThanks for your suggestion. We have carefully checked and corrected the grammar errors or typos.\n\n**Q1: In Eq. (14), the motivation to ignore the high-order infinitesimal term and dt is not clear.**\n\nAnswer: Thank you for your question. Our motivation is to reduce error during solving ordinary differential equation. \n\nSpecifically, our method involves solving the ODE from $\\boldsymbol x_t$ to obtain $\\boldsymbol x_{t+\\Delta t}$ and $\\boldsymbol x_{t}'$ respectively. In this process, there is an error that grows with $\\Delta t$. Therefore, we choose $\\Delta t\\rightarrow 0$. In this case, we can ignore the high-order infinitesimals. $\\Delta t=dt$. When comparing two samples, we need to choose the same $\\Delta t$, so this term can be disregarded.\n\n**Q2: Why the proposed method can obtain better performance than the best competitors in Tables 1-3?**\n\nThank you for your question. NA uses the training function for the attack, but the noise term in diffusion models may hinder its effectiveness. Both SecMI and our approach replace the noise term with the model's output, thereby obtaining better results. Moreover, our method boosts efficiency by using the output at $t=0$ rather than multi-step iterations, simplifying parameter adjustments and enhancing performance. A distinct distance function from SecMI also contributes to better effectiveness.\n\n**Q3: How to apply the proposed method to discrete-time diffusion models?**\n\nThanks for your question. As we described in Eq. 9, $R_{t, p}=\\left\\Vert\\epsilon_{\\boldsymbol{\\theta}}\\left(x_{0}, 0\\right)-\\epsilon_{\\boldsymbol{\\theta}}\\left(\\sqrt{\\bar a_{t}} x_{0}+\\sqrt{1-\\bar a_{t}} \\epsilon_{\\boldsymbol{\\theta}}\\left(x_{0}, 0\\right), t\\right)\\right\\Vert_{p}$, we need to obtain the model's output at time $t=0$. This output is then inserted into $R_{t,p}$. By comparing the value of $R_{t,p}$ with the threshold $\\tau$, we can determine whether it is a training sample."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700437169955,
                "cdate": 1700437169955,
                "tmdate": 1700437169955,
                "mdate": 1700437169955,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]