[
    {
        "title": "Metric Learning for Detection of Large Language Model Generated Texts"
    },
    {
        "review": {
            "id": "T2rxc3PO9g",
            "forum": "LKx4rubqkO",
            "replyto": "LKx4rubqkO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7753/Reviewer_8Zoy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7753/Reviewer_8Zoy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a solution to detecting texts generated by LLMs. It emphasized on computational costs, accessibility, and performance.\n\nSpecifically, the authors propose a metric-based detection framework that evaluates the similarity between a given text and an equivalent example generated by LLMs to determine the text's origination. The framework includes a text embedding model and a metric model, with a focus on designing the metric component. \n\nThe authors also introduce four datasets with over 85,000 prompts and triplets of responses for benchmarking, showing that their best architectures maintain F1 scores between 0.87 to 0.95 across various settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The approach itself is new. Being able to detect this without access to the internal structure is very valuable.\n* The data sets will be very useful for any downstream tasks.\n* The experimental results are impressive.\n* The focus on computational cost and accessibility is especially relevant."
                },
                "weaknesses": {
                    "value": "* This approach still relies heavily on the LLM themselves.\n* The scope of the study is rather small, focusing only on short responses."
                },
                "questions": {
                    "value": "* Could you elaborate on the different types/variations of prompts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727613089,
            "cdate": 1698727613089,
            "tmdate": 1699636946568,
            "mdate": 1699636946568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FshqYCTtH7",
                "forum": "LKx4rubqkO",
                "replyto": "T2rxc3PO9g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you very much for your high regard on our paper. To address your concerns:\n1. We utilize the LLMs themselves as a method to reduce computational costs from the detectors. Having references from the generative LLMs, the detectors can focus more on learning a good metric while still maintaining reasonable complexity.\n2. We are expanding this works to longer text data.\n3. The prompts are mainly questions in the forms what/who/when/where/which/how.\n\nThank you very much again."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715364270,
                "cdate": 1700715364270,
                "tmdate": 1700715364270,
                "mdate": 1700715364270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OWoVX8kvLT",
            "forum": "LKx4rubqkO",
            "replyto": "LKx4rubqkO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7753/Reviewer_CEgw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7753/Reviewer_CEgw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a metric-learning based approach for the detection of LLM-generated text given a known context. The authors propose two metric-based neural architectures trained with triplet loss, one based on embedding the full-text, and another based on embedding individual sentences. The detection decision is then based on thresholding the distance between an input-text and an LLM generated text."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Approach does not require knowing the logits of the LLM or modifying its logits in any way as with watermarking. \n* Focuses on relatively short pieces of text (3-5) sentences. \n* Authors created and will release a set of four datasets totaling over 85,000 generations on a wide variety of topics."
                },
                "weaknesses": {
                    "value": "* There are many typographic errors spread throughout the manuscript. This made the paper overly difficult to understand.\n* The proposed approach requires knowing the context that may have been given to an LLM to generate the text. This seems like too big of a restriction, as it is often the case that we don\u2019t know how the LLM was prompted. Suggestion:\n  * Instead of requiring the context to generate a response by ChatGPT, a set of known ChatGPT generations on arbitrary generations could\u2019ve been kept aside. The detector would then take the distance between the input text and the set of known ChatGPT generations.\n  * The above could be compared to the case where one does know the context of the generation. \n* The only LLM considered is ChatGPT. It would\u2019ve been interesting to include a broader set of LLMs and explore the robustness of the approach to unseen LLM. Suggested Models: Llamav1, Llamav2, OPT, GPT-2, GPT-4, Cohere, Dollyv2, etc.\n* In the \u201csame corpus with paraphraser\u201d experiments the model was trained on the paraphrased LLM detections. This means that all the testing data is still in-domain, and hence it may defeat the purpose of the experiment. Suggestions:\n  * Use a different paraphraser on the testing data.\n  * Do not train on the paraphrased text.\n* It is unclear whether the sentence framework out-performs the full-text framework simply because it has more parameters. Suggestion: To experiment whether this is true, a best-effort could be made at matching the number of parameters between both frameworks by either increasing the number in the case of the full-text framework or decreasing it in the case of the sentence framework.\n* Instead of evaluating the F1 score on the best threshold found in the validation set, it would\u2019ve been better to just plot the ROC curves and show the median and 90th / 10th percentiles. This would give a better understanding of how the detector performs across varying ranges of FPR. \n* There are more up-to-date metric-learning losses that could\u2019ve been explored. Examples:\n  * Supervised Contrastive Loss: https://arxiv.org/abs/2004.11362\n  * InfoNCE: https://papers.nips.cc/paper_files/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf\n  * Tuplet Margin Loss: https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf\n* Not enough baselines compared against. Some recommendations include:\n  * OpenAI Detector: https://huggingface.co/roberta-base-openai-detector\n  * LLMDet: https://github.com/TrustedLLM/LLMDet\n  * A RoBERTa detector fine-tuned on the context and LLM-generation / human-answer for each dataset."
                },
                "questions": {
                    "value": "* In S5, the training time of various methods is brought up as a reason to exclude certain baselines from consideration. However, this is an offline cost, which is not relevant at inference/test-time, so it's not clear why it's relevant. How do various methods compare at inference time?\n\n* In practice, the robustness of machine-text detectors is important for many applications of machine-text detectors. For example, the cost of false positives is high when making false accusations of plagiarism. As a result, it is important to measure the robustness and calibration of such detectors. How does the proposed method fare when faces with various distribution shifts relative to the training data? (new LLMs, new domains, new decoding methods, etc.) Is it more robust than simple alternatives, such as simple supervised classifiers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776979927,
            "cdate": 1698776979927,
            "tmdate": 1699636946457,
            "mdate": 1699636946457,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v3cGyPtx5M",
                "forum": "LKx4rubqkO",
                "replyto": "OWoVX8kvLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you very much for your feedback. We want to address your comments as follows.\n1. We are developing another model to reconstruct prompts from arbitrary texts. These models can be as simple as a finetune LLM to pose a question that capture the topic of a text.\n2. At the time of model training and evaluation of models in this paper, GPT-3.5-TURBO/ChatGPT was the most popular and advance. We will incorporate the newer models into this work.\n3. While the models are trained on paraphrased texts, the question-answer data is not the same between training and testing. Furthermore, the test data only comprises of paraphrased texts. Therefore, we believe this experiment makes sense as-is. Regardless, we will follow your suggestion.\n4. Contrastive loss is fairly older than triplet loss. We will consider the other two.\n5. We eliminate a lot of detectors from comparison due to their complexity. We are aiming at small models that can be maintained in consumer computers.\n\nFinally, to answer your questions:\n1. While computational costs of supervised classifiers are indeed cheaper at inference time, they still require a big enough system to host/maintain. For example, ROBERTA at 1.5 billion parameters is definitely not easy for a regular consumer computer to sustain. As we really want to aim at cheap models, complexity in inference is also of interests.\n2. Expanding this model to general/new LLMs and domains is one future direction of this project.\n\nWe will incorporate your suggestion into the next iteration of this paper.\n\nThank you very much again for your valuable feedback."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715169917,
                "cdate": 1700715169917,
                "tmdate": 1700715169917,
                "mdate": 1700715169917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hmVynsoRfi",
            "forum": "LKx4rubqkO",
            "replyto": "LKx4rubqkO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7753/Reviewer_7NGJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7753/Reviewer_7NGJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of detecting texts generated by Large Language Models(LLMs). The authors propose a method that involves assessing the similarity between a given text and a comparable LLM-generated example to determine the source of the text. The authors conduct experiments on a novel, metric-based detection paradigm, utilizing four extensive datasets comprising over 85,000 prompts and response triplets, to evaluate the effectiveness of their approach in distinguishing between human-written and LLM-generated texts across various corpora and contexts, including paraphrasing scenarios. Experimental results show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "- The paper focuses exclusively on short text responses, with an average length of five sentences and no less than three. This limitation could hinder the generalizability and applicability of the proposed approach, as longer texts or documents might exhibit different characteristics and patterns that are not captured by the model trained on shorter responses.\n\n- Lack of convincing baselines. The authors merely use the distance threshold approach on the embedding generated by the standalone pretrained MPNet as the baseline. \n\n- The authors not evaluate the proposed method on the benchmark such as Human ChatGPT Comparison Corpus (HC3) [4]. \n\n- There is a lack of previous studies in the literature.  \n\n[1] Gehrmann, Sebastian, Hendrik Strobelt, and Alexander Rush. \"GLTR: Statistical Detection and Visualization of Generated Text.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Association for Computational Linguistics, 2019.\n\n[2] Clark, Elizabeth, et al. \"All That\u2019s \u2018Human\u2019Is Not Gold: Evaluating Human Evaluation of Generated Text.\" Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.\n\n[3] Sadasivan, V. S., Kumar, A., Balasubramanian, S., Wang, W., & Feizi, S. (2023). Can ai-generated text be reliably detected?. arXiv preprint arXiv:2303.11156.\n\n[4] Guo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding, Y., ... & Wu, Y. (2023). How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597."
                },
                "questions": {
                    "value": "Please refer to the \"Weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832167117,
            "cdate": 1698832167117,
            "tmdate": 1699636946349,
            "mdate": 1699636946349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7JAIaF7aYv",
                "forum": "LKx4rubqkO",
                "replyto": "hmVynsoRfi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you very much for you feedback. We want to address your second point regarding the baseline. As we eliminate supervised classifiers due to their complexity, and probabilistic models as they require access to output logits, the choice of baselines is limited. We will take your suggestion and incorporate more literature into the next iteration of this paper.\n\nThank you again."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714452648,
                "cdate": 1700714452648,
                "tmdate": 1700714452648,
                "mdate": 1700714452648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qJjV9BUXXE",
            "forum": "LKx4rubqkO",
            "replyto": "LKx4rubqkO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7753/Reviewer_X5RJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7753/Reviewer_X5RJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a neural classifier that detects LLM-generated texts from\nhuman-generated texts. Instead of directly relying internals of LLMs, the \nproposed classifier leverages triplet samples of the same meanings, and builds\na neural classifier on these triplets that uses metric learning internally.\nExperimental results indicate that the proposed classifier works good (but not\nshown better than baselines) on several datasets, and induced metrics are\nsignificantly different between LLM-generated texts and human-generated texts."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "While the rationale and the proposed approach is decent, the crucial drawback\nof this paper is that it is not compared with possible baselines. The basic\nidea that the classifier should base on the sentences of the same semantic\ncontent is of course good, but such a classifier can be built just from a set\nof pairs of texts, not triplets. Also, that central idea of using the\nsame-meaning sentence itself should be validated empirically: how about the\nperformance if we simply build a classifier on two sets of texts, one from LLM\nand one of humans?\n\nWithout such empirical investigations, this paper should not be accepted as a\nmachine learning conference paper."
                },
                "weaknesses": {
                    "value": "See above."
                },
                "questions": {
                    "value": "Nothing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699003573873,
            "cdate": 1699003573873,
            "tmdate": 1699636946201,
            "mdate": 1699636946201,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TmxW4RM9QI",
                "forum": "LKx4rubqkO",
                "replyto": "qJjV9BUXXE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you very much for you feedback. We want to clarify that the reason triplet training was selected is because it had advantages over pairwise training. First, the performance of triplet loss was proved to be very good in multiple research. Second, pairwise training like contrastive loss could be more extreme in that the metric of same-label pairs are learned to be 1, and that of different-label pairs is learned to be 0, which could pose a more difficult training problem.\n\nWe will take your suggestion into consideration and include pairwise models in the next iteration of this paper. \n\nThank you again."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714176051,
                "cdate": 1700714176051,
                "tmdate": 1700714176051,
                "mdate": 1700714176051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]