[
    {
        "title": "Aligning brain functions boosts the decoding of videos in novel subjects"
    },
    {
        "review": {
            "id": "jeHba36ABE",
            "forum": "GYAvwLviup",
            "replyto": "GYAvwLviup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5566/Reviewer_jWwB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5566/Reviewer_jWwB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a aligment method to boost the performance of brain decoding."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposed a simple method for brain decoding."
                },
                "weaknesses": {
                    "value": "1.The loss function has three parameters. How to choose the parmeters is difficult.\n2.The proposed method should be compared with the state-of-the-art methods."
                },
                "questions": {
                    "value": "1. The dataset is very small. Hence, How is the generalization of the model\uff1f\n2. The proposed method don't compared with the state-of-the-art methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698710390760,
            "cdate": 1698710390760,
            "tmdate": 1699636572645,
            "mdate": 1699636572645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5IALux9X7p",
                "forum": "GYAvwLviup",
                "replyto": "jeHba36ABE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5566/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and feedback.\n\nWe want to stress that we respectfully disagree with the reviewer\u2019s summary of our work. Our manuscript does not propose an alignment method. It instead shows that at least one functional alignment method (introduced in a different paper) is suited to transfer semantic decoders trained in some individuals to other, left-out individuals. It is possible that other functional alignment methods could achieve better performance in this regard, although we believe it is out of the scope of this study to compare them. Also, we stress that anatomical alignment (i.e. projecting all data to a common anatomical template like MNI or fsaverage) is the go-to strategy employed in a vast majority of studies. Therefore, since this anatomical alignment is our baseline, our work already includes an insightful comparison to the most commonly used alignment method in the literature. Finally, as stated in Bazeille et al. 2021, it is not clear that any existing functional alignment method outperforms others in the general case (in particular, performance highly depends on the downstream task and input data).\n\n**Weaknesses**\n\n1. Reviewer jWwB is right that the chosen alignment method (FUGW) has 3 hyper-parameters, namely alpha, rho and epsilon. We use FUGW because, unlike alternatives, it comes with a simple python API comparable to that of sklearn and thus facilitates reproducibility. Previous work has already explored the impact of these three hyper-parameters, and showed that alignment is robust to changing values for alpha and rho, although epsilon needed to be carefully chosen. Our goal is a proof-of-existence, i.e. that it is possible to yield significant decoding gains with a standard functional alignment preprocessing.\n2. Since we did not introduce this alignment method, we did not feel compelled to compare it with a lot of other methods. However, note that our baseline (i.e. fsaverage) is the most commonly used alignment method. Moreover, we agree with you that our work could constitute an interesting downstream task, which a future benchmark could use to compare between functional alignment methods.\n\n**Questions**\n\n1. Indeed, fMRI datasets featuring a lot of data per participant are still very rare. To our knowledge, the Wen 2017 featured in our study is among the most extensive datasets in this regard. In order to strengthen our work, we have decided to implement two other experiments, which reproduce our analysis with each possible reference subject. We kindly invite you to read their description in the response we make to reviewers uqYK and HiZ5.\n2. We hope our second point of the previous section addresses your concern.\n\nWe thank jWwB for their helpful remarks, and hope these comments help alleviate their concerns. We would be happy to provide additional information and experiments which could help strengthen our conclusions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699984416046,
                "cdate": 1699984416046,
                "tmdate": 1699984416046,
                "mdate": 1699984416046,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I0fIdIfeBk",
            "forum": "GYAvwLviup",
            "replyto": "GYAvwLviup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5566/Reviewer_tcU3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5566/Reviewer_tcU3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to boost the decoding of videos in a single left-out subject with an alignment model and a decoder. The alignment model includes anatomical alignment and functional alignment. Finally, the pre-trained image encoder is used to obtain the video output."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed ideas are simple and intuitive. It's quite generic and can be applied to different models."
                },
                "weaknesses": {
                    "value": "1. The novelty needs to be further elaborated.\n1. The proposed method lacks a comparison with other models. As a result, the effectiveness of the method is not convincing and requires further validation.\n2. The proposed method in this paper employs one reference subject. How to deal with multiple reference subjects in practice?"
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5566/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5566/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5566/Reviewer_tcU3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740504807,
            "cdate": 1698740504807,
            "tmdate": 1699636572548,
            "mdate": 1699636572548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l830xywHhK",
                "forum": "GYAvwLviup",
                "replyto": "I0fIdIfeBk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5566/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and feedback.\n\n**Weaknesses**\n\n1. We agree with the reviewer that we should clarify the novel contributions of our work. We propose to amend the introduction  with the following paragraph:  \n\u201cOverall, decoding and functional alignment studies have focused on largely disjoint objectives: decoding semantics is typically done at the individual level (e.g. Tang et al. 2023, Scotti et al. 2023, Ozcelik et al. 2023), whereas functional alignment across subjects is evaluated on simple features, such as retinotopy (Feilong et al. 2022), object categorization (Bazeille et al. 2021), and binary contrasts (Thual et al. 2022). With the rise of naturalistic datasets (NSD, Wen2017, Lebel2023, Things), it is pressing to know the conditions necessary for future studies to directly benefit from the high-level representations identified in a small set of high-quality recordings. Here we systematically investigate the data regime in which a set of standard semantic decoders would benefit from functional alignment.\u201d\n2. Reviewer tcU3 is right that we only benchmark the functional alignment method (FUGW) against the most commonly used anatomical alignment method, which is our baseline. FUGW was introduced and benchmarked in a different paper. The goal of the present study is to evaluate the condition in which functional alignment could improve standard  semantic decoders. Moreover, the baseline used in our paper is the alignment method most commonly used in neuroscience.\n3. We agree with tcU3 that the present method is limited to a single reference subject. To mitigate this issue, we propose to generalise our result section, by reproducing the analysis with each possible reference subject. We kindly invite you to read the description of two additional experiments in the response we make to reviewers uqYK and HiZ5. Overall, however, building a multi-subject reference is still an open research question. Thual et al. 2022 introduces a method to build functional barycenters of individuals, which we did not try here because we feel that this way of computing reference subjects is fairly new and would require extensive testing of its own.\n\nWe thank tcU3 for their helpful remarks, and hope these comments help alleviate their concerns. We would be happy to provide additional information and experiments which could help strengthen our conclusions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699984235629,
                "cdate": 1699984235629,
                "tmdate": 1699984235629,
                "mdate": 1699984235629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9QXo6SbwAt",
            "forum": "GYAvwLviup",
            "replyto": "GYAvwLviup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5566/Reviewer_HiZ5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5566/Reviewer_HiZ5"
            ],
            "content": {
                "summary": {
                    "value": "The work deals with the decoding of high-level visual features from fMRI recordings. The authors use functional alignment to align fMRI data across subjects. The work claims that using functional alignment instead of standard structural methods boosts the decoding performance when there is limited data available for the subject in case. Further, training a model with aligned subjects can be used, whereas previous models could only decode responses specific to a subject."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Using functional alignment to improve decoding of visual representations across subjects is novel and noteworthy. The quantification of decoder performance with respect to data size is noteworthy."
                },
                "weaknesses": {
                    "value": "The approach of showing the left-out subject the same video as the reference subject is a substantial weakness. This coupled with the functional alignment could theoretically act as a \u201cleakage\u201d mechanism for the data"
                },
                "questions": {
                    "value": "Does one repetition in line 268 mean the first trail or the second? \n\nLine 281 says the left-out subjects have to watch the same videos, while, line 265 says the subjects are shown the test stimuli for the first time. Does this mean the videos shown themselves are new to the subjects?\n\nWhere is it shown that the approach aligns brain responses in accordance with brain anatomy? ( LIne 12)\n\nPerhaps, adding at least a single subject comparison where the subject is shown different stimuli may help have more robust results"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795319500,
            "cdate": 1698795319500,
            "tmdate": 1699636572450,
            "mdate": 1699636572450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8uk4TVJdRE",
                "forum": "GYAvwLviup",
                "replyto": "9QXo6SbwAt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5566/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and feedback.\n\n**Weaknesses**\n\nIn our setups, the test set is completely separated from the training sets of both the decoder and alignment: the training and test sets contain different movies acquired during different MRI sessions.\n\nHowever, reviewer HiZ5 is right that our alignment (although not the decoder) requires the participants to watch the same stimuli. This limitation applies to the vast majority of alignment methods (Bazeille et al. 2021). \n\nTo clarify this issue we propose to indicate the following in the method section: \n\u201cFor example, in the out-of-subject setup, a decoder is trained on movies 1, 2, 3 seen by subject A. Subjects A and B have both seen movies 4 and 5, which are used to compute an alignment. We refer to these movies as \u201canchors\u201d, because they allow left-out subjects to \u201canchor\u201d themselves onto a decoder that has been trained on another individual. Finally, the decoder is tested on data from subject B, acquired when they were watching movie 6. All aforementioned movies were acquired in different MRI sessions.\u201d\n\nIn our paper, movies used to train the decoder and alignment are the same, which does not need to be true in the general case. However, there is no reason not to include \u201canchor\u201d movies in the training dataset of the decoder, as it makes for more accurate decoders and more powerful \u201canchors\u201d to the decoder.\n\n**Questions**\n\n1. In this case, we used only the first trial (i.e. run). We could try to use only the second run, but we think it is more crucial to measure results based on data which represents the neural response of individuals when they are first shown the stimuli. Indeed, stimuli repetition can lead to unexpected effects (drop of attention, less controlled experiment in general).\n2. We hope that our response to the main weakness answers your question. Please let us know if further clarification would be helpful.\n3. We further expand on our claim that alignments map vertices in accordance with brain anatomy: in Figure 4, the first column from the left shows an atlas displayed on subject 1, without functional alignment (in this case, using only anatomical alignment). Columns 2, 3, and 4 show how vertices of subject 1 are reorganised on the cortical sheet to functionally match that of subject 2, using an increasing amount of movie-watching data to fit the functional alignment. What we observe is that, with more data, computed reorganisations look more anatomically coherent. For instance, the parietal and temporal lobe look very scrambled in column 2 (i.e. some of their vertices get matched with vertices coming from very different cortical areas), but much more consistent in column 4 (based on colour, matched vertices are much more coherent). One can observe a similar effect even in premotor areas, and the occipital lobe.  \nWe are working on a refined version of this Figure to make these points clearer.\n4. We hope that our response to the main weakness answers your question. Please let us know if further clarification would be helpful.\n\nAgain, we are thankful to HiZ5 for these valuable comments."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699984002476,
                "cdate": 1699984002476,
                "tmdate": 1699984002476,
                "mdate": 1699984002476,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "heYBUmxziF",
            "forum": "GYAvwLviup",
            "replyto": "GYAvwLviup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5566/Reviewer_uqYK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5566/Reviewer_uqYK"
            ],
            "content": {
                "summary": {
                    "value": "This paper is aimed at improving subject video decoding from BOLD functional responses obtained during a movie watching paradigm. To this end, the authors perform brain response alignment via an optimal transport methodology, followed by a linear regression to predict latent representations of video frames (obtained by standard encoder models such as CLIP or VD-VAE).  They find that this method improves out of subject video decoding performance when compared to a purely anatomical alignment approach. They also examine multi-subject alignment in comparison to single subject approaches when limited number of paired recordings are available"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper provides some interesting insights into the use of functional alignment models for studying BOLD responses in the context of naturalistic stimuli. Although the alignment methodology is not entirely a novel contribution in and of itself, the experimental setup is well deigned to examine the hypotheses being tested. The findings are clearly presented and motivated"
                },
                "weaknesses": {
                    "value": "The major weakness of this paper is the limited number of subjects that are available for testing. Given that only three different subjects are used, it is unclear whether the findings of the paper and insights will generalize for a larger population."
                },
                "questions": {
                    "value": "1. It is not clear how sensitive the method to the choice of image latent representation/granuarity of features? How was the choice of representational models (such as CLIP or VD-VAE etc) made, is one or the other more suitable for this evaluation setup? Additionally, is there a reason the video frame decoding is restricted to a linear regression parameterization\n\n2. How was the choice of retrieval metric made? Is there a reason standard metrics such as mean average precision, or NDCG are not appropriate for evaluating this task?\n\n3. It would be nice to provide more context to explaining the design and modeling strategy in Eq. (1). The way it is currently presented requires the reader to flip back and forth between this manuscript and Thual et al 2022 to understand the methodology properly.\n\n4. It would be nice if a higher resolution image for Figure 2 could be made available to understand the false retrieval cases. Additionally, it would be nice to provide more description in the appendix to help the reader interpret the extended experimental observations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5566/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5566/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5566/Reviewer_uqYK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699566123218,
            "cdate": 1699566123218,
            "tmdate": 1699636572366,
            "mdate": 1699636572366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wNxzpgoePb",
                "forum": "GYAvwLviup",
                "replyto": "heYBUmxziF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5566/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and valuable feedback.\n\n**Weaknesses**\n\nWe agree that it is unclear whether our multi-subject alignment will scale to larger numbers of individuals.\nOur decision to focus on a dataset with few individuals with a lot of natural stimuli stems from a simple question: under what data regime can functional alignment be beneficial to semantic decoding. \nIn this regard, our contribution is a proof-of-concept: if decoders trained on a single participant can generalize to other unseen individuals for whom little data has been acquired, then it constitutes (1) a strong incentive to acquire very high amounts of data in a limited number of participants and (2) an additional legitimation of recent initiatives (e.g. Natural Scene Dataset, Kay, Naselaris et al. 2023; deep-fMRI-dataset, Lebel, Huth et al. 2023). \n\nTo further address this issue, we propose to add two more experiments:\n- Experiment 1: evaluate decoding gains on each possible reference subject of the Wen2017 dataset. This will allow us to triple the number of experiments, hopefully strengthening our original conclusions.\n- Experiment 2: evaluate decoding gains for out-of-subject setups in which the decoder has been trained on multiple aligned subjects.\n\nWe will test all possible combinations to measure the effect of multi-subject training on this cohort.\nWe will post results for both these experiments as soon as we get them.\n\n**Questions**\n\n1. We chose to decode CLIP Vision 257 x 768 (high-level, semantic features) and VD-VAE (low-level features) latents to compare our results to that of Ozcelik et al. 2023 (in which the authors trained single-subject linear decoders on fMRI data from the Natural Scenes Dataset with much higher signal-to-noise ratio). We also included CLIP Vision CLS and AutoKL latents because they are comparable to CLIP Vision 257 x 768 and VD-VAE latents respectively, while being much smaller. We propose to clarify these choices in the \u201cVideo output\u201d section.  \nSupplementary figures S5-S7 illustrate the differences in decoding performance between these latents. In particular, we see that CLIP-based features (i.e. high level) are much more easily decoded than other, low-level features.\nDo you wish for us to make these results more clearly accessible in the main paper?  \nThe decision  to use linear decoder follows a classic approach in fMRI (e.g. Naseralis et al 2011, Huth et al Nature 2016, Ozcelik et al. 2023) which present multiple advantages: (1) simplicity, (2) interpretability and (3) robustness. Furthermore, our internal testing did not show significant improvement of deep-learning pipelines over linear approaches. We propose to clarify this motivation in the method section. \n2. We chose to report retrieval metrics to be comparable with other recent papers (e.g. Scotti et al. 2023) and because image reconstruction metrics (like used in Ozcelik et al. 2023) depend on the generative models used, but are also difficult to assess. Also, top-k accuracy is used in similar work (Scotti et al. 2023; Defossez et al. 2023; Chen et al 2022). Finally, we think that relative median ranks are easier to understand intuitively and to compare between settings.\n3. We agree that this part of the paper is confusing. We propose to rearrange the method sections to avoid this back-and-forth. We agree and will make a higher-resolution version of Figure 2 in the appendix and add more comprehensive captions and paragraphs to introduce supplementary results.\n\nAgain, we are thankful to uqYK for these valuable comments."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699983923764,
                "cdate": 1699983923764,
                "tmdate": 1699983923764,
                "mdate": 1699983923764,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]