[
    {
        "title": "Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning"
    },
    {
        "review": {
            "id": "kCnZU64274",
            "forum": "pRpMAD3udW",
            "replyto": "pRpMAD3udW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5152/Reviewer_yFyT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5152/Reviewer_yFyT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes new learning methods to master simulated tabletop robot manipulation from multi-modal prompts. Specifically, their method involves two stages, first inverse-dynamics pretraining then multi-task finetuning. State-of-the-art results are demonstrated on the multimodal prompt benchmark VIMA-BENCH. Furthermore, authors conducted ablation studies to justify the effectiveness of design choices and showcase in-context learning ability achieved by the trained model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is effective, as demonstrated by its new SOTA performance on VIMA-BENCH.\n- Comprehensive ablation studies draw insights into the effectiveness of proposed method.\n- Demonstrated in-context learning ability described in Section 4.3 is interesting and impressive.\n- The paper is well-written and presented."
                },
                "weaknesses": {
                    "value": "- Albeit the method is interesting and demonstrated improvement is impressive, the proposed method is only evaluated on a single benchmark. It would be more solid if authors cloud show similar improvement on other robot learning benchmarks such as RLBench (James et al., 2020).\n- It's totally legitimate for the authors to argue other benchmarks do not support multimodal prompts. In that case, I would encourage authors to extend existing VIMA-BENCH by adding more representative tasks to show the in-context learning ability of models trained with the proposed method.\n- Although this paper is not designed to address real-robot manipulation, showing proof-of-concept demos would justify the feasibility of applying this method on real hardware.\n- Missing citations. Authors are encouraged to discuss the following recent related work:\n\nRadosavovic et al., Robot Learning with Sensorimotor Pre-training, arXiv 2023.\n\nShah et al., MUTEX: Learning Unified Policies from Multimodal Task Specifications, CoRL 2023.\n\n## References\nJames et al., RLBench: The Robot Learning Benchmark & Learning Environment, IEEE Robotics and Automation Letters 2020."
                },
                "questions": {
                    "value": "To encode multimodal prompts, the introduced RC provides a direct connection between input embeddings and LM's output embeddings. With this shortcut, is there any performance difference between LMs with varying depth?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698465472726,
            "cdate": 1698465472726,
            "tmdate": 1699636509728,
            "mdate": 1699636509728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qj1IjgqnmZ",
                "forum": "pRpMAD3udW",
                "replyto": "kCnZU64274",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer yFyT"
                    },
                    "comment": {
                        "value": "We are grateful for the positive and detailed feedback from the reviewer. Please find our responses below. We hope that the following response will further clarify any concerns and assist in advocating for the acceptance of our paper.\n\n> It would be more solid if authors cloud show similar improvement on other robot learning benchmarks such as RLBench (James et al., 2020).\n\nAs mentioned by the reviewer, RLBench does not support multimodal prompts. Thus, we choose to extend the existing VIMA-BENCH by adding more tasks to showcase the in-context learning ability of our models.\n\n> I would encourage authors to extend existing VIMA-BENCH by adding more representative tasks to show the in-context learning ability of models trained with the proposed method.\n\nWe thank the reviewers for the constructive comments! We designed 4 new tasks with in-context learning examples in the task prompt. We evaluate our policy that is trained on the full VIMA-BENCH data on these 4 tasks and compare it with the VIMA policy. The table below shows that our method demonstrates superior performance over all baseline methods. On the other hand, the VIMA policy struggles with these tasks, revealing its inability to learn from the in-context demonstration. More experiment details, including task definitions, can be found in Appendix D.\n\n| Method                        | Move Then Rotate | Rotate Then Move | Move Then Stack | Stack Then Move | Average Success Rate |\n|-------------------------------|------------------|------------------|-----------------|-----------------|----------------------|\n| Our Method                    | **13.5%**         | **14.5%**        | **22.0%**       | **10.0%**       | **15.0%**            |\n| Our Method w/ Pretrain Only   | 4.5%             | 0.5%             | 20.5%           | 1.5%            | 5.3%                 |\n| Our Method w/o Pretrain       | 0.0%             | 0.0%             | 0.0%            | 0.5%            | 0.1%                 |\n| VIMA                          | 0.0%             | 0.0%             | 0.0%            | 0.5%            | 0.1%                 |\n\n> Missing citations. Authors are encouraged to discuss the following recent related work\n> \n\nThank you for bringing up these two works! We have updated our manuscript to cite and discuss these works in the related work section.\n\n> Although this paper is not designed to address real-robot manipulation, showing proof-of-concept demos would justify the feasibility of applying this method on real hardware.\n\nWe appreciate the reviewer's suggestion. We currently do not have access to a real robot and thus cannot conduct a demonstration. Nevertheless, we assume the same action space as in Transporter [1] and CLIPort [2] and share similar settings except for tackling multimodal prompts. These alignments suggest strong potential for real-world deployment of our method. We believe our contributions should still be valid in the absence of a real-robot demonstration.\n\n> With this shortcut, is there any performance difference between LMs with varying depth?\n\nWe evaluate our methods by replacing the T5-base with T5-small. The experiment result is shown below. We can see that the performance difference is minimal.\n\n| Model      | L1    | L2    | L3    | L4    |\n|------------|-------|-------|-------|-------|\n| T5-base    | 97.8% | 97.9% | 93.4% | 59.1% |\n| T5-small   | 97.9% | 98.0% | 92.7% | 50.0% |\n\n[1] Zeng et al., Transporter Networks: Rearranging the Visual World for Robotic Manipulation, CoRL 2020.\n\n[2] Shridhar et al., CLIPORT: What and Where Pathways for Robotic Manipulation, CoRL 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700130691475,
                "cdate": 1700130691475,
                "tmdate": 1700130691475,
                "mdate": 1700130691475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BplO2CT3Ry",
                "forum": "pRpMAD3udW",
                "replyto": "kCnZU64274",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up the disccusion"
                    },
                    "comment": {
                        "value": "Dear Reviewer yFyT,\n\nThank you again for your time and effort. Your feedback has been valuable in helping us clarify, improve, and refine our work. We have carefully addressed your comments in our authors' responses to improve the quality of our paper. We thus kindly request that you take a moment to revisit our paper and consider the changes we have made. We hope our clarifications can help you better advocate for our paper's acceptance.\n\nBest regards,\n\nThe authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526191281,
                "cdate": 1700526191281,
                "tmdate": 1700526191281,
                "mdate": 1700526191281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h8STGZm0vL",
                "forum": "pRpMAD3udW",
                "replyto": "Qj1IjgqnmZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Reviewer_yFyT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Reviewer_yFyT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for conducting extra experiments to address my questions. I would like to keep my original rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630794063,
                "cdate": 1700630794063,
                "tmdate": 1700630794063,
                "mdate": 1700630794063,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q578pdHu5L",
            "forum": "pRpMAD3udW",
            "replyto": "pRpMAD3udW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5152/Reviewer_Y3nY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5152/Reviewer_Y3nY"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new method to learn multi-model prompt robot policy. The  main differences from a prior work, VIMA, are the following:\n\n1. Have a pre-training phrase that pretrains on prompts asking the robot to follow a certain motion.\n2. A new encoding method to encode multi-model prompts.\n3. A method to model the dependency among action dimensions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The pretraining method makes sense in that it uses the implicit motion data in each trajectory as the training signal.\n- The new prompt encoding and action dependency modeling are valid.\n- Presentation of the experiment results are comprehensive, and extensive details are given for the method explanation. \n- Experimentation is rigorous and follows prior benchmarking."
                },
                "weaknesses": {
                    "value": "1. The pretraining method is not general enough: it only concern about instruction of \"follow motion for ...\" for a particular motion trajectory, and therefore it mainly tackles the tasks with prompts given a certain motion of a certain trajectory. This means it assumes the task at hand is always similar to follow motion, which is not true.\n\n- An example to illustrate this: it can do well for task T10, but for task T13, when it sweeps something without touching an object, it cannot generalize. \n\n2. For the pretraining method to work, this method also assumes that the prompts contains the motion trajectory keypoint, which is a very narrow assumption and might not always hold. The end users would not be expected to provide the entire trajectories all the time. Therefore the pretraining on motion following is a bit overfitting to the tasks that VIMA designed. \n\n- related to this point: the work advocates for inverse dynamics modeling, but I think this is quite specific to the VIMA-bench task setting with algorithmic oracle. It would be hard to model inverse dynamics in real world.\n\n3. Effectiveness of proposed method: in terms of experiment results (for the full results in Appendix A), there are not significant improvement over VIMA; for those tasks (T10) that has significant improvement, it seems to because the pretraining phase overfits to \"Follow Motion\" task."
                },
                "questions": {
                    "value": "1. In Appendix  A page 14, two variations of \"Ours\" are:\n- w/ pretrain\n- w/ Encoder-Decoder\nIs \"w/ Encoder-Decoder\" with or without pretraining? Are these two variations adding \"pretrain\" and \"Encoder-Decoder\" on top of some common method or is one of them adding on top of another?\n\n3. For task T13, could you provide more details on the failure cases of VIMA and this method respectively? Providing some video rollouts of the two methods would be great. \n\n4. The authors are encouraged to provide full experiment results in the main text rather than a portion of it.\n\n5. The conclusion mentioned that the work \"demonstrate the in-context learning capability\". Could the authors elaborate more on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Reviewer_Y3nY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781031374,
            "cdate": 1698781031374,
            "tmdate": 1699636509629,
            "mdate": 1699636509629,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oNDrzihNH3",
                "forum": "pRpMAD3udW",
                "replyto": "Q578pdHu5L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Y3nY"
                    },
                    "comment": {
                        "value": "We appreciate the detailed feedback from the reviewer. Please find our responses below.\n\n> The pretraining method is not general enough: it only concern about instruction of \"follow motion for ...\" for a particular motion trajectory, and therefore it mainly tackles the tasks with prompts given a certain motion of a certain trajectory.\n> \n\nWe would like to clarify an important misunderstanding: The goal of our pretraining is not to achieve zero-shot generalization for any unseen task. Instead, we aim to endow robots with an intuitive, human-like ability to learn and adapt to new tasks through in-context demonstrations. This focus is the cornerstone of our inverse dynamic pretraining design, facilitating the robot's understanding of the underlying transition dynamics suggested by the multimodal prompts.\n\nNevertheless, our pretraining can still improve our model's performance on tasks without trajectory instructions in the prompt. Evidence of this can be found in Table 1, which shows that our pretraining can improve the performance of Task 5 (Rearrange then restore) consistently on L1, L2, and L3 levels, despite Task 5 only containing subgoal images in its prompt, demonstrating the versatility of our pretraining approach.\n\n> This means it assumes the task at hand is always similar to follow motion.... it can do well for task T10, but for task T13... it cannot generalize.\n\nWe contend that the challenges in solving Task 13 (Sweep without Touching) are not indicative of shortcomings in our algorithm design. And thus, tackling Task 13 is outside the scope of our paper.\n\nTask 13 is an L4 task in the VIMA-BENCH, which is out of the training distribution. Our analysis of the VIMA-BENCH training tasks and trajectories reveals two primary factors hindering generalization to Task 13:\n\n1. **End Effector Utilization**: T13 employs a \"spatula\" end effector, a tool only used in Task T12 (Sweep without Exceeding) among all training tasks. 1. The majority (12 out of 13) of training tasks utilize a \"suction cup\" end effector. This distinction is critical as the suction cup is associated with \"pick and place\" motor skills, while the spatula is linked to \"wipe\" actions, as detailed in Section 2.\n2. **Layout and Prompt Specificity**: The workspace layouts of Task 12 and Task 13 are nearly identical, yet they diverge significantly from the other training tasks. Task 13's prompt only differs from T12's by the term touching. However, the training data lacks the necessary signal to effectively conceptualize and respond to touching, impacting the model's performance on Task 13.\n\nConsequently, policies trained via our methods and VIMA tend to replicate Task 12's action sequence in Task 13, leading to failure.\n\nHowever, we kindly ask the reviewer to pay attention to our method's success in significantly improving performance on Task 5, 9, and 17. This improvement is not limited to motion-following tasks. Furthermore, while Task 10 (Follow Motion) shares similarities with our pretraining prompts, the demonstration image sequence in Task 10 includes distractors absent from the robot's workspace (see Figure 2). Conversely, our pretraining tasks incorporate the robot's observational sequence directly into the task prompt, as illustrated in Figure 4.\n\n> For the pretraining method to work, this method also assumes that the prompts contains the motion trajectory keypoint, which is a very narrow assumption and might not always hold.\n\nFirst, We would like to clarify the misunderstanding that our pretraining method does not depend on the motion trajectory key point. Our method only requires a sequence of state-action pairs of the robot trajectory, which is always satisfied in the multi-task imitation settings. Second, the key points are a natural consequence of our action space $\\mathcal{A} = (\\mathcal{T}\\_{intial}, \\mathcal{T}\\_{target})$, as each observation is gathered at the start/end of each action. Moreover, we emphasize that this versatile action space has been widely adopted in various robotics system, including VIMA, Transporter Network[1], and CLIPort [2].\n\n> The end users would not be expected to provide the entire trajectories all the time. Therefore the pretraining on motion following is a bit overfitting to the tasks that VIMA designed.\n\nWe are considering multi-task imitation learning where demonstration trajectories for each training task are provided. We argue that multi-task imitation learning is a general setting adopted in various works [1, 2]. We can always conduct our pretraining for any robot trajectory with a sequence state-action of pair [3]. Therefore, our pretraining strategies are NOT overfitting to the VIMA designed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700129762043,
                "cdate": 1700129762043,
                "tmdate": 1700129762043,
                "mdate": 1700129762043,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xhAt7L4dnw",
                "forum": "pRpMAD3udW",
                "replyto": "Q578pdHu5L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up the discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer Y3nY,\n\nThank you again for your time and effort. Your feedback has been valuable in helping us clarify, improve, and refine our work. We have carefully addressed your comments in our authors' responses to improve the quality of our paper. We thus kindly request that you take a moment to revisit our paper and consider the changes we have made. We hope our clarifications warrant a more positive evaluation of our work.\n\nBest regards,\n\nThe authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525941813,
                "cdate": 1700525941813,
                "tmdate": 1700525941813,
                "mdate": 1700525941813,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rTDCwS5Tez",
                "forum": "pRpMAD3udW",
                "replyto": "xhAt7L4dnw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Reviewer_Y3nY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Reviewer_Y3nY"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response!"
                    },
                    "comment": {
                        "value": "I thank the authors for their responses and I acknowledge the clarifications by the authors. Thank you and the comments will be taken into considerations."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602506085,
                "cdate": 1700602506085,
                "tmdate": 1700602506085,
                "mdate": 1700602506085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "33jWYobUAk",
            "forum": "pRpMAD3udW",
            "replyto": "pRpMAD3udW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5152/Reviewer_YMJx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5152/Reviewer_YMJx"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of multi-modal prompting in \"embodied tasks\", i.e., the combination of language and image to train a model to be capable of multi-tasks. The authors introduced a two-stage training pipeline, in pretraining, using the inverse dynamic modelling loss, and in fine-tuning, using a multi-task imitation loss.\n\nOverall, this paper can be seen as a follow-up of the VIMA[1] paper. Results show a 10% success rate gain in the VIMA benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written. I am glad to read the detailed analysis of the ablation studies. The introduction and related works sections indicate the authors are very familiar with relevant literature."
                },
                "weaknesses": {
                    "value": "While this paper looks technically sound to me, I found the small improvements based on the VIMA paper can not be viewed as a significant contribution that is sufficient to be accepted in ICLR. The claimed contributions include (1) a MIDAS training framework, i.e., introducing inverse dynamic modelling loss, page 5 Eq(3) in pretraining + multi-task imitation loss; (2) residual connections in the visual layers; \n(3) a small performance gain (10%) compared to the VIMA paper. However, using inverse dynamic modelling loss and multi-task supervision loss are all intuitive and an easy follow-up step after the VIMA paper. Therefore, the reviewer found the contributions are not sufficient to be published as a long paper in ICLR.\n\nNov 23 update: regarding (3), after reviewing the additional experiments the authors submitted, I think the performance looks good for me. I will raise my score to 5 accordingly."
                },
                "questions": {
                    "value": "- The authors add the appendix pages in the main paper, which exceeds the page limits. Please remove the appendix in the revision.\nNov 23 update: Have no concerns after rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Reviewer_YMJx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698869302092,
            "cdate": 1698869302092,
            "tmdate": 1700756101683,
            "mdate": 1700756101683,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VZkWyJ4gss",
                "forum": "pRpMAD3udW",
                "replyto": "33jWYobUAk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YMJx"
                    },
                    "comment": {
                        "value": "We would like to clarify several major misunderstandings on the contributions of our work.\n\n> using inverse dynamic modeling loss and multi-task supervision loss are all intuitive and an easy follow-up step after the VIMA paper\n\nWe would like to emphasize that our core novelty does not lie in the specific training losses employed. It lies in equipping a multi-task policy with the capacity for in-context learning, which is achieved by applying a two-stage training pipeline. Our goal is to endow a multi-task robot with an intuitive, human-like ability to learn and adapt to new tasks through in-context demonstrations rather than striving for zero-shot generalization to entirely unseen tasks. Table 1, 2, and Appendix D shows that our method establishes SOTA performances across all four evaluation protocols while exhibiting a superior in-context learning ability. To the best of our knowledge, **simultaneously equipping a robot with multi-task and in-context learning abilities** has not been extensively explored in robotics, making our contribution a significant step forward in this domain.\n\n> (2) residual connections in the visual layers\n\nWe would like to clarify that our contribution is **an effective multimodal prompt encoder that can capture visual and textual details**. Specifically, it is constructed by adding a residual connection (RC) from the input visual tokens to the encoded embeddings, assisting the encoding process to retain more detailed visual information. Notably, this simple yet effective design is not limited to the task of robotics control. It can be incorporated into any multimodal LLM (MLLM), e.g., LLaVA [1] and AnyMAL [2],  to facilitate general multimodal understanding.\n\n> (3) a small performance gain (10%) compared to the VIMA paper\n\nOur method achieves average success rates of 97.8% (+10.6% over VIMA) on L1, 97.9% (+10.9% over VIMA) on L2,  93.4% (+9.4% over VIMA) on L3,  and 59.1% (+9.5% over VIMA) on L4 on the standard VIMA-BENCH. Notably, we highlight our exceptional gains on Task 5 (Rearrange the restore, +31.8%), Task 9 (Twist, +86.3%), Task 10 (Follow Motion, + 41.0%) and  Task17 (Pick then restore, +19.3%). These results clearly indicate a major advancement over the VIMA model, not just a small performance gain.\n\n> The reviewer found the contributions are not sufficient to be published as a long paper in ICLR.\n\nWe kindly remind that the reviewer has overlooked our policy's superior in-context learning ability, as empirically supported in Section 4.3 and Appendix D. Our model can learn from in-context examples for novel tasks, a feature where the baseline VIMA model falls short. We highlight the in-context learning ability is crucial for a generalist robot.\n\nWe assert the novelty of our research. Prior works have primarily focused on developing either a multi-task robot or one capable of learning from demonstrations or in-context examples. However, integrating in-context learning into a multi-task policy remains under-explored in robotics.\n\nMoreover, our work sheds light on the emergence of the in-context learning ability in our model, a topic of significant interest in NLP. While one might initially credit inverse dynamic pretraining for this ability, Table 2 reveals that policies developed through our two-stage pipeline (both `Our Method` and `Our Method w/o Modified FT`) outperform those from inverse dynamic pretraining alone (`Our Method w/ Pretrain Only`). This phenomenon echoes the findings in FLAN [3], which indicate that a finetuned language model excels in in-context learning.\n\nWe thank the reviewer's invaluable feedback, which helped us clarify our contributions. We kindly ask the reviewer to revisit our paper in light of our response and consider whether the clarifications we have provided might warrant a reconsideration of the rating.\n\n> The authors add the appendix pages in the main paper, which exceeds the page limits. Please remove the appendix in revision.\n\nWe follow the official instructions of ICLR https://iclr.cc/Conferences/2024/CallForPapers. We make sure our main text is within the 9 pages limit.\n**Paper length**\n\nThere will be a strict upper limit of 9 pages for the main text of the submission, with unlimited additional pages for citations. This page limit applies to both the initial and final camera ready version.\n\n- Authors may use as many pages of appendices (after the bibliography) as they wish, but reviewers are not required to read the appendix.\n\n[1] Liu et al., Visual Instruction Tuning. NeurIPS 2023\n\n[2] Moon et al., AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model. arXiv:2309.16058\n\n[3] Wei et al. \"Finetuned language models are zero-shot learners.\" ICLR 2022"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700129749636,
                "cdate": 1700129749636,
                "tmdate": 1700129749636,
                "mdate": 1700129749636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g9besq0jwZ",
                "forum": "pRpMAD3udW",
                "replyto": "33jWYobUAk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up the discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer YMJx,\n\nThank you again for your time and effort. Your feedback has been valuable in helping us clarify, improve, and refine our work. We have carefully addressed your comments in our authors' responses to improve the quality of our paper. We thus kindly request that you take a moment to revisit our paper and consider the changes we have made. We hope our clarifications warrant a more positive evaluation of our work.\n\nBest regards,\n\nThe authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525860047,
                "cdate": 1700525860047,
                "tmdate": 1700525860047,
                "mdate": 1700525860047,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fJupeDDbhh",
            "forum": "pRpMAD3udW",
            "replyto": "pRpMAD3udW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5152/Reviewer_t7wu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5152/Reviewer_t7wu"
            ],
            "content": {
                "summary": {
                    "value": "Good paper! This paper proposes a new method called MIDAS for robot manipulation with multimodal prompts. The key ideas are:\nA two-stage training pipeline with inverse dynamics pretraining and multi-task finetuning\nAn effective multimodal prompt encoder that augments a pretrained language model with a residual connection to visual features\nModeling action dimensions as individual tokens and decoding them autoregressively\nThe method is evaluated on the VIMA-BENCH benchmark and establishes a new state-of-the-art, improving success rate by around 10%. The ablation studies demonstrate the benefits of the proposed training strategy and prompt encoder design."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The inverse dynamics pretraining is an interesting idea to enable the model to infer actions from visual observations. This facilitates in-context learning from demonstration examples in the prompts.\n\n- Modeling action dimensions independently and decoding them autoregressively is intuitive and shows improved performance.\n\n- Comprehensive experiments on the challenging VIMA-BENCH benchmark with clear improvements over prior state-of-the-art.\n\n- Ablation studies provide useful insights into the contribution of different components."
                },
                "weaknesses": {
                    "value": "The prompts are quite controlled during pretraining versus the more complex prompts at test time. It is unclear if the pretraining fully transfers to the downstream tasks."
                },
                "questions": {
                    "value": "- For the inverse dynamics pretraining, were other self-supervised objectives explored besides simply reconstructing the actions?\n\n- What stopped the baseline VIMA model from reaching the same performance with just more compute/data?\n\n- Is there other complementary information like force sensors that could augment the visual observations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5152/Reviewer_t7wu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699555637041,
            "cdate": 1699555637041,
            "tmdate": 1699640984454,
            "mdate": 1699640984454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jiI89AQx80",
                "forum": "pRpMAD3udW",
                "replyto": "fJupeDDbhh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t7wu"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's positive feedback on our work. We hope the following response will further clarify any concerns and assist in advocating for the acceptance of our paper.\n\n> The prompts are quite controlled during pretraining versus the more complex prompts at test time. It is unclear if the pretraining fully transfers to the downstream tasks.\n\nOur pretraining mainly focuses on enabling the agent to reason the inverse dynamics from the task prompt, and thus does not incorporate a wide range of language. Our approach can still benefit the downstream tasks without demonstration trajectory in the prompt. For example, it can improve the performance of Task 5 (Rearrange then restore) consistently across L1, L2, and L3, despite Task 5 only containing subgoal images in the prompt.\n\nWe acknowledge the potential benefits of a more varied language in our pretraining prompts and consider its inclusion as a valuable future extension of our research.\n\n> For the inverse dynamics pretraining, were other self-supervised objectives explored besides simply reconstructing the actions?\n\nIn this project, we only tried the inverse dynamic prediction as the pretraining tasks given limited resources. However, it might also be possible to reconstruct the observation images during pretraining, as shown by Radosavovic et al. [1]\n\n> What stopped the baseline VIMA model from reaching the same performance with just more compute/data?\n \n1. The VIMA models each action dimension independently, which can lead to task failure when tackling tasks requiring coordination between the initial and target pose of the action.\n2. The training data provided in VIMA-BENCH is imbalanced. We reveal this problem by finetuning a VIMA policy trained on the full multi-task data on only the trajectories of Task 5 (Twist). Its success rate increased from ~13% to ~92% on L1, L2, and L3 evaluations.\n3. Performing multi-task imitation alone is insufficient to enable the VIMA policy to reason inverse dynamics given novel tasks with in-context examples in the prompt, which leads to the VIMA policy's inability to perform, e.g., Task 9 (Twist) and Task 10 (Follow Motion).\n\n> Is there other complementary information like force sensors that could augment the visual observations?\n\nThe current VIMA-BENCH only provides visual observations. However, in general, we can always include complementary information to augment the observation space, and this state-based information, like force sensors, can often improve the sample efficiency during training. For example, Radosavovic et al. incorporate proprioceptive robot states to assist in action prediction.\n\nHowever, in-context examples/demonstrations are more accessible to gather as a sequence of images/video when tackling novel unseen tasks. Therefore, we consider pure image observations in this paper.\n\n\n[1] Radosavovic et al., Robot Learning with Sensorimotor Pre-training, arXiv 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700129727047,
                "cdate": 1700129727047,
                "tmdate": 1700129727047,
                "mdate": 1700129727047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mdu0WXURhV",
                "forum": "pRpMAD3udW",
                "replyto": "fJupeDDbhh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up the discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer t7wu,\n\nThank you again for your time and effort. Your feedback has been valuable in helping us clarify, improve, and refine our work. We have carefully addressed your comments in our authors' responses to improve the quality of our paper. We thus kindly request that you take a moment to revisit our paper and consider the changes we have made. We hope our clarifications warrant a more positive evaluation of our work.\n\nBest regards,\n\nThe authors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525743988,
                "cdate": 1700525743988,
                "tmdate": 1700525743988,
                "mdate": 1700525743988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]