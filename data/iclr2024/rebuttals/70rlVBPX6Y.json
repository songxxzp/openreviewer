[
    {
        "title": "Neural Architecture Search for TinyML with Reinforcement Learning"
    },
    {
        "review": {
            "id": "GdjmQADzTI",
            "forum": "70rlVBPX6Y",
            "replyto": "70rlVBPX6Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_KEWJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_KEWJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new NAS strategy based on multi-objective Bayesian optimization and an ensemble of competing parametric ARS RL agents. Experiments are conducted on two TinyML use cases, including image classification and time-series classification."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Multi-objective optimization is critical for improving the performance/efficiency of deep learning on microcontrollers. \n2. The studied problem has many practical applications."
                },
                "weaknesses": {
                    "value": "1. The technical contribution of the proposed method is a bit limited. MOBOpt and ARS RL are existing techniques. Combining them and applying them to TinyML is a bit straightforward. \n\n2. This paper lacks experiments on more challenging tasks, such as ImageNet classification on MCU [1] and Tiny Object Detection [2]. \n\n3. In addition, this paper also lacks direct comparisons with stronger baselines [1,2].\n\n[1] Lin, Ji, et al. \"Mcunet: Tiny deep learning on iot devices.\" Advances in Neural Information Processing Systems 33 (2020): 11711-11722.\n\n[2] Lin, Ji, et al. \"Memory-efficient patch-based inference for tiny deep learning.\" Advances in Neural Information Processing Systems 34 (2021): 2346-2358."
                },
                "questions": {
                    "value": "See comments above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698282919321,
            "cdate": 1698282919321,
            "tmdate": 1699636454691,
            "mdate": 1699636454691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gjGPoj56vO",
                "forum": "70rlVBPX6Y",
                "replyto": "GdjmQADzTI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for his time and effort in reviewing our paper and are pleased to hear that the reviewer considers our approach to be practical. \n\n> 1. The technical contribution of the proposed method is a bit limited. MOBOpt and ARS RL are existing techniques. Combining them and applying them to TinyML is a bit straightforward.\n\nWe agree with the reviewer that both MOBOpt and ARS are existing techniques that are studied for years. However, we want to highlight that a combination of them (and how we did it) is anything but straightforward. We believe that the novelty lies in how we integrated both techniques and how they both play their strengths without being computationally infeasible.\n\n> 2. This paper lacks experiments on more challenging tasks, such as ImageNet classification on MCU [1] and Tiny Object Detection [2].\n\nWe believe that the main application of edge computing is to perform data analysis close to where data is generated, i.e., on sensor nodes. As a result, we argue that most practical use cases will focus on time series analysis rather than image-based classification, although this is not excluded. Therefore, we focus our analysis on time series classification tasks using the UCA&UCR time series analysis benchmark. To keep the tasks challenging, we only consider multivariate problems. To demonstrate the flexibility of our approach, we also applied it to an image-based task, i.e., CIFAR10, see Figure 1.\n\n> 3. In addition, this paper also lacks direct comparisons with stronger baselines [1,2].\n\nWe have added comparative results of our approach to MCUNet. As this was requested by several reviewers, you can find it in the general rebuttal above. We hope that these additional results are of interest.\n\nWe hope we managed to address all your concerns and are happy to discuss further."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982854040,
                "cdate": 1699982854040,
                "tmdate": 1699982854040,
                "mdate": 1699982854040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SMolxOkeqm",
            "forum": "70rlVBPX6Y",
            "replyto": "70rlVBPX6Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_CC3e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_CC3e"
            ],
            "content": {
                "summary": {
                    "value": "To optimize DNN for deployment on microcontrollers, the paper proposes a NAS approach that combines Augmented Random Search with Reinforcement learning for Bayesian Optimization. It employs an ensemble of competing polices to identify optimal DNN architectures and also strike a balance among accuracy, memory and computational complexity. Experimental results demonstrate the proposed approach outperforms traditional multi-objective Bayesian optimization methods across various datasets and architectures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of combining ARS and RL for Bayesian Optimization is novel and interesting.\n2. The paper is well-written and clearly presented."
                },
                "weaknesses": {
                    "value": "1. The superiority of the proposed method is not convincing based on the existing experiments. There lacks comparison with NAS methods.\n2. It is hard to duplicate the results based on the details provided in the paper."
                },
                "questions": {
                    "value": "1. As a NAS strategy for microcontrollers, why is the proposed method only compared to Bayesian Optimization methods instead of related NAS methods (especially those for microcontrollers)? Also, more recent methods should be compared with instead of those quite traditional ones.\n2. Why choose only ParEGO (proposed in 2006) for comparison in Table 1? Also, it seems that the superiority of the proposed method is not quite impressive compared to ParEGO."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741434901,
            "cdate": 1698741434901,
            "tmdate": 1699636454547,
            "mdate": 1699636454547,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gY2zzUrMJi",
                "forum": "70rlVBPX6Y",
                "replyto": "SMolxOkeqm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for working through our paper and are happy to hear that the reviewer found it interesting.\n\n> The superiority of the proposed method is not convincing based on the existing experiments. There lacks comparison with NAS methods.\n\n> As a NAS strategy for microcontrollers, why is the proposed method only compared to Bayesian Optimization methods instead of related NAS methods (especially those for microcontrollers)? Also, more recent methods should be compared with instead of those quite traditional ones.\n\nWe agree with the reviewer and share the concern. As this was requested by multiple reviewers, we added results for MCUNet and ProxylessNAS, a zero-shot hardware aware NAS strategy, please see the general rebuttal above. We hope that these additional results are of interest.\n\n> It is hard to duplicate the results based on the details provided in the paper.\n\nWe are checking whether it is possible to publish source code and think that we will come to a solution. Furthermore, we evaluate the possibility to let others reproduce our algorithms and experiments.\n\n> Why choose only ParEGO (proposed in 2006) for comparison in Table 1? Also, it seems that the superiority of the proposed method is not quite impressive compared to ParEGO.\n\nWe chose ParEGO as the main comparison in Table 1 because it is the closest competitor to our approach in the experiments we present in Figure 1. Unfortunately, we were not able to obtain additional results for MCUNet, as it only supports image-based datasets and not datasets with only a single spatial dimension.\n\nWe hope we managed to address all your concerns and are happy to discuss further."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982810299,
                "cdate": 1699982810299,
                "tmdate": 1699982810299,
                "mdate": 1699982810299,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xC7D65CnG7",
                "forum": "70rlVBPX6Y",
                "replyto": "gY2zzUrMJi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4726/Reviewer_CC3e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4726/Reviewer_CC3e"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification and experiments. I have read the author's responses and insist my initial rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708000372,
                "cdate": 1700708000372,
                "tmdate": 1700708000372,
                "mdate": 1700708000372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lV53lBRMpR",
            "forum": "70rlVBPX6Y",
            "replyto": "70rlVBPX6Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_FN6N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_FN6N"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses reinforcement learning to perform multi-objective hyper parameter optimization for pruning a given architecture. The hyperparameters identified by the search algorithm are then used by the pruning pipeline to prune and then evaluate the model on the various objectives specified.\n The Augmented Random Search (ARS) agents are trained to identify the next most promising candidates. They circumvent the computationally expensive step of training the candidates to obtain the reward model by employing a gaussian process as a surrogate model to predict the reward. The GP prior is initialized by evaluating candidates sampled using Latin-Hypercube sampling. During every iteration of the search, multiple ARS agents are trained and the GP's posterior is updated by evaluating the best candidates proposed by all the agents. The various objectives used are accuracy, RAM and ROM memory consumption and FLOPS.\n  \n  They performed pruning on MobileNetV3 trained on DaLiac , ResNet trained on Cifar10. They also evaluated their algorithm on timeseries classification where the architecture used is CNN. In addition to this, on a synthetic dataset, they also showed that their algorithm is able to find a good minima when compared to ParEgo even during poor initialization where all the samples are in a region with no gradients."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It outperforms all the ParEgo multi-objective bayesian optimization baseline.\n2. ARS agent in conjunction with GP as a surrogate avoids expensive evaluations to obtain the reward model. It would be good to highlight this and report the time taken by all the algorithms."
                },
                "weaknesses": {
                    "value": "1. In the past, other multi-objective algorithms have used Knowledge distillation (KD) rather than pruning to obtain smaller well performing architectures. Please adapt yours to perform KD and compare against the following baselines [5], [6], [7]. It is also essential to indicate the drop in accuracy of the searched architecture when compared to the original architecture. Given that these algorithms also employ reinforcement learning and Bayesian optimization, please highlight how your algorithm is different from theirs and also demonstrate that yours performs better than these baselines.\n\n2. Please cite other multi-objective NAS algorithms such as  MnasNet [1], once-for-all [2], NSGA-NET [3], Dppnet [4] in the related work. \n3. Given that your algorithm is performing HPO, please include other HPO solvers that use multi-objective algorithms such as NSGA-II, Multi-Objective Particle Swarm Optimization etc.\n\n[1] Once-for-All: Train One Network and Specialize it for Efficient Deployment, Cai et al.\n[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile, Tan et al.\n[3] NSGA-Net: Neural Architecture Search using Multi-Objective Genetic Algorithm, Lu et al.\n[4] DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures, Dong et al.\n[5] Learnable embedding space for efficient neural architecture compression., Can et al.\n[6] AMC: automl for model compression and acceleration on mobile devices., He et al.\n[7] N2N learning: Network to network compression via policy gradient reinforcement learning., Ashok et al."
                },
                "questions": {
                    "value": "1. How can a weighted sum reward yield various Pareto fronts? Given that the final objective is a weighted sum, one must vary the weights to obtain various fronts. This is not a Pareto-optimal solution. Algorithms such as NSGA-II yield pareto-optimal solution. \n2. How can k-means ensure the diversity of the solutions in Pareto-front? K-means selects solutions from each centroid which would imply that it is the most representative sample in that cluster. To ensure diversity, we want to sample solutions from region where the number of solutions in the neighborhood is sparse.  NSGA-II uses crowding distance, Lemonade uses kernel density estimator and in both cases they select solutions from less crowded region."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761799483,
            "cdate": 1698761799483,
            "tmdate": 1699636454465,
            "mdate": 1699636454465,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "78HOEcHHpx",
                "forum": "70rlVBPX6Y",
                "replyto": "lV53lBRMpR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time and effort to thoroughly review our paper. We are glad that the effectiveness of our approach is recognized. As with all black-box NAS techniques, the biggest influence on the optimization time is the time needed to train the DNNs. Since we use small, inexpensive RL agents, their additional overhead has proven to be negligible compared to other black-box solvers. \n\n> In the past, other multi-objective algorithms have used Knowledge distillation (KD) rather than pruning to obtain smaller well performing architectures. Please adapt yours to perform KD and compare against the following baselines [5], [6], [7]. It is also essential to indicate the drop in accuracy of the searched architecture when compared to the original architecture. Given that these algorithms also employ reinforcement learning and Bayesian optimization, please highlight how your algorithm is different from theirs and also demonstrate that yours performs better than these baselines.\n\n> Please cite other multi-objective NAS algorithms such as MnasNet [1], once-for-all [2], NSGA-NET [3], Dppnet [4] in the related work.\n\nWe thank the reviewer for highlighting important additional related work which we indeed did not properly include. We made sure to update the related work section of our work accordingly (see updated paper). We also extended our related work section to reflect advances in the usage of knowledge distillation techniques to achieve model compression besides other well-known compression techniques like pruning and quantization.\n\n> Given that your algorithm is performing HPO, please include other HPO solvers that use multi-objective algorithms such as NSGA-II, Multi-Objective Particle Swarm Optimization etc.\n\nWe provide comparative results for two datasets and model architectures using NSGA-II in Figure 1. In general, the reason we decided to focus on Bayesian optimization rather than evolutionary or swarm-based strategies is that they tend to have a high sampling complexity, making them underperform in scenarios where very little sampling can be done, we which believe can also be seen in the results provided in Fig. 1.\n\n> How can a weighted sum reward yield various Pareto fronts? Given that the final objective is a weighted sum, one must vary the weights to obtain various fronts. This is not a Pareto-optimal solution. Algorithms such as NSGA-II yield pareto-optimal solution.\n\nWe perform multi-objective optimization, i.e., we have multiple objective metrics, 4 in your case. As a result, the result of the optimization is a 4-dimensional Pareto front. Since this front cannot be visualized directly in 2D or 3D space, we decided to use a sliced representation, i.e., all-versus-accuracy in Figure 1. More precisely, the Pareto fronts provided in Figure 1 are the same front for each row, just from three different \"perspectives\". To provide a single reward signal to the RL agents, we compute a weighted sum of the 4D target vector. The reward is not visualized in the Pareto fronts. The weights for the sum can either be provided by the user based on the subjective importance of the goals, or the algorithm randomly chooses them for each trial to provide a diverse set of tradeoffs.\n\n> How can k-means ensure the diversity of the solutions in Pareto-front? K-means selects solutions from each centroid which would imply that it is the most representative sample in that cluster. To ensure diversity, we want to sample solutions from region where the number of solutions in the neighborhood is sparse. NSGA-II uses crowding distance, Lemonade uses kernel density estimator and in both cases they select solutions from less crowded region.\n\nWe use K-means to find representative samples that are spread far apart as good starting points for our RL agents to search for the next candidate during optimization. Since we are using multiple local agents searching in parallel, we wanted to make sure that they are distributed throughout the search space to ensure diversity in their proposed solutions.\n\n\nWe like to thank the reviewer again for his excellent points in helping us clarify the paper's contents. We would be happy to discuss further in case additional questions arise."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982752670,
                "cdate": 1699982752670,
                "tmdate": 1699982752670,
                "mdate": 1699982752670,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7VRpWFYlEj",
            "forum": "70rlVBPX6Y",
            "replyto": "70rlVBPX6Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_4rfG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_4rfG"
            ],
            "content": {
                "summary": {
                    "value": "This study merges the efficiency of Gaussian Processes (GPs) with the effectiveness of Reinforcement Learning (RL) agents to facilitate the efficient exploration of Deep Neural Network (DNN) architectures that can be directly implemented on microcontrollers. Unlike previous research on RL for Multi-Objective Optimization (MOOpt), it doesn't apply RL directly to the MOOpt problem's search space. Instead, it employs RL to enhance the optimization process conducted on GPs. To tackle the issue of resource-intensive RL agent training, it suggests training a group of competing policies within MOBOpt. These policies utilize a straightforward Augmented Random Search (ARS) method for candidate sampling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The research introduces an innovative approach by uniting the sampling efficiency of Gaussian Processes (GPs) with the expressive capabilities of Reinforcement Learning (RL) agents. Additionally, the incorporation of Augmented Random Search represents a novel enhancement.\n\n2. The study demonstrates performance improvements when compared to alternative Bayesian optimization strategies."
                },
                "weaknesses": {
                    "value": "1. From a technical standpoint, I haven't identified a direct link between the proposed method and TinyML, and this study hasn't delved deeply into the realm of TinyML models, and some important related works [1] [2]  are missing. \n\n2. The comparative methods in this study are restricted to Bayesian optimization strategies, omitting comparisons with other Neural Architecture Search (NAS) approaches such as zero-shot methods, optimization-based methods, or predictor-based methods.\n\n3. The mentioned validated architectures are limited to ResNet18 and MobileNetV3, which are somewhat dated in the context of contemporary developments.\n\n[1] MCUNet: Tiny Deep Learning on IoT Devices\n[2] MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning"
                },
                "questions": {
                    "value": "1. The main performance comparison didn't conclude for some architectural performance but mainly focused on the comparison with other optimization methods. How about the performance comparison with MCUNetV1[1], and MCUNetV2 [2]?\n2. What is the search cost and efficiency compared with other types of NAS methods? For example, it would be fair to compare with other zero-shot NAS methods or preditor-based methods\uff1f"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774363051,
            "cdate": 1698774363051,
            "tmdate": 1699636454364,
            "mdate": 1699636454364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ILvEI95z1g",
                "forum": "70rlVBPX6Y",
                "replyto": "7VRpWFYlEj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "First and foremost we want to thank the reviewer for the time and effort spent on reviewing our paper. We are happy to hear that our approach is considered innovative and novel.\n\n> From a technical standpoint, I haven't identified a direct link between the proposed method and TinyML, and this study hasn't delved deeply into the realm of TinyML models, and some important related works [1] [2] are missing.\n\nWe thank the reviewer for highlighting important related work and have updated our work accordingly (see updated paper). The challenge of architecture design for TinyML lies in finding a trade-off between hardware constraints of a given edge system and DNN accuracy. We tried to automate the hardware aware architectural search for a given target platforms, represented by a set of constraints, e.g., <256 Kb RAM, <1MB Flash, on the target metrics, i.e., ROM, RAM, FLOPs, accuracy, by combing multi-objective NAS (black-box Bayesian optimization) with RL and comparing it to other optimization strategies. \n\n> The comparative methods in this study are restricted to Bayesian optimization strategies, omitting comparisons with other Neural Architecture Search (NAS) approaches such as zero-shot methods, optimization-based methods, or predictor-based methods.\n\nThank you for bringing up that point! As also requested by other reviewers, we now provide more results for MCUNet and ProxylessNAS, a zero-shot optimization framework, (see general rebuttal response above). We think that these additional results and especially the comparison with ProxylessNAS are of interest to the reviewers and readers of the paper.\n\n> The mentioned validated architectures are limited to ResNet18 and MobileNetV3, which are somewhat dated in the context of contemporary developments.\n\nWe selected ResNet18 and MobileNetV3 as they are architectures commonly used in TinyML benchmarks like MLPerf Tiny [1]. Furthermore, these architectures cover all relevant layer types and network structures that are also used in other TinyML architectures such as MCUNet.\n\n[1] Banbury, Colby, et al. \"MLPerf Tiny Benchmark.\" Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1). 2021.\n\n> The main performance comparison didn't conclude for some architectural performance but mainly focused on the comparison with other optimization methods. How about the performance comparison with MCUNetV1[1], and MCUNetV2 [2]?\n\n> What is the search cost and efficiency compared with other types of NAS methods? For example, it would be fair to compare with other zero-shot NAS methods or preditor-based methods?\n\nAs with all black-box optimization approaches, the main factor influencing the search cost of our approach is the time required to train DNN architecture candidates during optimization. We found that the additional overhead of the RL agents we propose to use in our work is negligible compared to other black-box solvers, e.g., ParEGO. \n\nWe agree that the comparison between slower but accurate black-box optimization and faster but imprecise zero-shot NAS methods is interesting. As a result, we provide additional results for MCUNet and ProxylessNAS for CIFAR10 and compare them with the results we report in our work (see general rebuttal response).\n\nWe hope we managed to address all your concerns and are happy to discuss further."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982647594,
                "cdate": 1699982647594,
                "tmdate": 1699982647594,
                "mdate": 1699982647594,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YGbpXlGduI",
            "forum": "70rlVBPX6Y",
            "replyto": "70rlVBPX6Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_obe6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4726/Reviewer_obe6"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a NAS strategy specifically for TinyML using multi-objective Bayesian optimization (MOBOpt) and an ensemble of competing parametric policies trained using Augmented Random Search (ARS) Reinforcement Learning (RL) agents. This approach can be utilized to explore the design tradeoffs between a DNN\u2019s predictive accuracy, memory consumption on a given target system, and computational complexity. The experiments show competitive results compared to prior optimization strategies. The paper is well organized in general, but my major concern is that the proposed optimization approaches such as MOBOpt are generic and not specific to TinyML. Particularly, the authors argue that it is expensive to evaluate the accuracy and memory consumption for TinyML system, but accuracy evaluation is required for NAS despite the targeted computing platform. Although memory (RAM and ROM) consumption is a specific requirement for TinyML system, the evaluation is similar or even less expensive than accuracy evaluation. Hence, more discussion about why tinyML makes NAS challenging and necessary is expected."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed NAS approach produces competitive models for MCU platform. The paper is well organized."
                },
                "weaknesses": {
                    "value": "The proposed NAS optimization seems to be generic and not specific for TinyML platform, which makes the research problem not quite convincing."
                },
                "questions": {
                    "value": "Could you illustrate why tinyML makes NAS challenging and unqiue?\nHow the proposed NAS optimization applies specifically for tinyML rather than general network design?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4726/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4726/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4726/Reviewer_obe6"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840652935,
            "cdate": 1698840652935,
            "tmdate": 1699636454296,
            "mdate": 1699636454296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vHMS5UDuLc",
                "forum": "70rlVBPX6Y",
                "replyto": "YGbpXlGduI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and effort working through our paper. We are happy to hear that our approach is competitive and meaningful. We also now provide more results compared to MCUnet, which has been suggested by some reviewers (see the general response). \n\nThe main challenge in deploying DNNs on the edge, i.e., tinyML, are the strict resource constraints, e.g., <256 Kb RAM and <1Mb Flash, in comparison to the high computational complexity and memory demands of most used DNN architectures. This makes finding DNN architectures that can achieve sufficient accuracy on relevant ML problems while being small enough to be feasibly deployable on edge systems a complicated and hardware-dependent process. NAS provides a great way to search for DNN architectures, but usually only focuses on network accuracy and not so deeply on the tradeoff between hardware constraints and accuracy. Furthermore, many existing NAS techniques that do consider the target hardware focus on zero-shot techniques that rely on fast but often imprecise predictive models instead of training DNN candidates. As a result, our work aims to explore the feasibility and explore the effectiveness of considering hardware aware NAS as a multi-objective optimization process in combination with reinforcement learning.\n\nWe hope that we have been able to explain sufficiently why tinyml places special demands on the optimization of neural networks. If there are any remaining or additional questions we are happy to further discuss them with you."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982559968,
                "cdate": 1699982559968,
                "tmdate": 1699982559968,
                "mdate": 1699982559968,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IVB01sehG2",
                "forum": "70rlVBPX6Y",
                "replyto": "vHMS5UDuLc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4726/Reviewer_obe6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4726/Reviewer_obe6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification and it is good to see the comparison with MCUNet.\nI agree with other reviewers that the major contribution is a combination of existing techniques, which makes the novelty and contribution insufficient. So I insist my prior rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620465081,
                "cdate": 1700620465081,
                "tmdate": 1700620465081,
                "mdate": 1700620465081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]