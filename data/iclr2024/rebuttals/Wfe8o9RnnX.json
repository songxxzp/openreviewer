[
    {
        "title": "A Generative Augmentation Framework for Contrastive Learning"
    },
    {
        "review": {
            "id": "IG0cUmIoBH",
            "forum": "Wfe8o9RnnX",
            "replyto": "Wfe8o9RnnX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_7GZG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_7GZG"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed GenCL, which combines modern generative models with contrastive learning. Specifically, GenCL uses generative models to create better-augmented images when compared with heuristic augmentations for contrastive learning. The empirical results show that GenCL improves the downstream classification performance on ImageNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The experimental results seem promising, as shown in Table 1.\n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "(1) The technical novelty of the proposed method is quite limited. The authors used existing well-performing generative models (ICGAN and Mask GIT) as the augmentation module in the standard contrastive learning framework. Using noised representation and in-painting methods to generate augmented images is not new. \n\n(2) The method is only tested on downstream ImageNet classification. Other CL literature evaluates their methods on more CV tasks, like object detection and segmentation."
                },
                "questions": {
                    "value": "- GenCL relies heavily on pre-trained generative models. If the target domain differs from the pre-trained domain, the generative models may fail to produce valid augmented images. This limits the use of GenCL in practice.\n\n- The paper is a good application of generative models and contrastive learning methods. However, it may not be suitable for ICLR due to the lack of novelty."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697613423323,
            "cdate": 1697613423323,
            "tmdate": 1699636133761,
            "mdate": 1699636133761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "kZLpUdq6qk",
            "forum": "Wfe8o9RnnX",
            "replyto": "Wfe8o9RnnX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_4FGw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_4FGw"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces GenCL, a Generative Augmentation Framework for Contrastive Learning, aiming to improve upon the standard image augmentation methods typically used in contrastive learning. The authors outline the GenCL neural network architecture, detail the two proposed generative augmentation techniques (noise-based and mask-based), and provide a performance analysis of their method on standard self-supervised learning benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1 - Generative Augmentations: \n\nUnlike traditional geometric and color augmentations, which predominantly focus on pixel-level modifications, GenCL's generative approach can modify high-level visual features, offering richer and more diverse augmentations.\n\nS2 - Performance Boost: \n\nThe results demonstrate that incorporating generative augmentations in contrastive learning can lead to significant improvements in self-supervised learning benchmarks.\n\nS3 - Comprehensive Analysis: \n\nThe paper thoroughly evaluates the GenCL approach, detailing the neural network architecture and providing in-depth ablation studies."
                },
                "weaknesses": {
                    "value": "W1 - Effectiveness over contrastive learning framework and downstream tasks:\nWhile the proposed augmentation method is effective, further experiments are expected across standard contrastive learning frameworks such as MoCo, SimCLR, or BYOL[1, 2, 3] to prove the generalization capacity. Meanwhile, I'm also curious whether the augmentation methods boost the performance of tasks like object detection and segmentation. \n\nW2 - Computation cost:\nGenerative model-based augmentation will unavoidably introduce additional computation. However, how is the computation when compared with native multi-crops methods from InfoMin[4]? If the proposed method needs more computation or the improvements become marginal with multiple crops, then the contribution of this method would be less salient.  \n\nW3 - Discussion about related work (minor).\nThere is a series of works that aims to boost the performance of contrastive learning in the feature space. Either of them aims to generate better positive pairs or provide additional contrast. I'd suggest the author give them sufficient discussion and comparison[5,6,7].\n\n[1] Momentum Contrast for Unsupervised Visual Representation Learning\n\n[2] A Simple Framework for Contrastive Learning of Visual Representations\n\n[3] Bootstrap your own latent: A new approach to self-supervised Learning\n\n[4] What Makes for Good Views for Contrastive Learning?\n\n[5] Towards domain-agnostic contrastive learning\n\n[6] Hallucination Improves the Performance of Unsupervised Visual Representation Learning\n\n[7] Metaug: Contrastive learning via meta feature augmentation"
                },
                "questions": {
                    "value": "The main questions are listed in Weaknesses. I'd raise my score if they were appropriately addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2021/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2021/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2021/Reviewer_4FGw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698518294808,
            "cdate": 1698518294808,
            "tmdate": 1699636133680,
            "mdate": 1699636133680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "yydla3APUz",
            "forum": "Wfe8o9RnnX",
            "replyto": "Wfe8o9RnnX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_NnqA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_NnqA"
            ],
            "content": {
                "summary": {
                    "value": "The work focuses on utilizing the power of generative models for augmentations (or views) in contrastive learning. The authors propose two methods: noise-based and masked-based augmentations in contrastive learning. The noise-based augmentations perturbs the feature map of the original by adding some Gaussian noise and this noisy feature map is fed into the generative model to synthesize a semantically similar image. The mask-based augmentations masks a portion of the feature map and then uses an inpainting model to recreate the original image. In this work, the authors use MaskGIT for the mask-based augmentations and Instance-Conditioned GAN for the noise-based augmentations. The authors show results on ImageNet-1k dataset with VICReg (a contrastive learning algorithm) where the mask-based augmentations outperforms the baseline  and noise-based augmentations. A comprehensive analysis of the computation cost is also mentioned in the results section."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The concept of replacing hand-crafted augmentations with generative augmentations is intuitive, logical and interesting direction.\n- The computation cost benefit analysis is comprehensive. The authors address the biggest problem with generative data augmentation i.e computing data augmentation using a generative model is extremely time and compute-heavy.\n- Overall, the paper is well-written. In the related work section, the authors clearly mention the difference between the proposed methods and previous related research: GenRep and COP-Gen.\n- Results with Mask-based augmentations with VICReg on ImageNet-1k show better performance than the baseline."
                },
                "weaknesses": {
                    "value": "- The idea of IC-GAN as data augmentation or views in Self Supervised learning has been previously explored in [1]. In Astolfi et. al [1], they integrate IC-GAN with SwAV and show results on ImageNet (Refer Section 5.2 in the paper). The authors should discuss this paper and also compare the results. Particularly, the authors should elaborate on how the proposed noise-based augmentation is different from the one in [1].\n- These are my concerns with the results section.\n    - The VICReg [2] paper reports a 68.7% Top-1 Accuracy with 100 epochs (Table 4 of VICReg) The authors also run the VICReg for 100 epochs (due to computation constraints). The baseline reported by the authors (0% sampling rate corresponds to the VICReg baseline) in Table 2 and Table 3 is 65.8% Top-1 Accuracy on ImageNet. This is an important point given that the gains with the generative augmentation is less than 2% in most cases. Can the authors clarify the reason behind the same?\n    - Previous work [3, 4, 5] has shown that generative data augmentation usually leads to benefit in OOD tasks. The results may be more compelling if the authors can analyse the results on some of the OOD datasets like ImageNet-C, ImageNet-R, ImageNet-Sketch, ImageNet-V2 and ObjectNet to name a few.\n    - The proposed noise-based augmentation and mask-based augmentation are not specific to any CL method. From that point of view, the authors should ideally show results with atleast one more CL method apart from VICReg.\n    - Some baselines are missing from the tables. Given that GenRep and COP-Gen are closely related, the authors should compare with these methods. In Table 1, the authors should mention the performance of VICReg baseline (It looks like the VICReg baseline outperforms all the generative augmentations in Table 1). Similarly, in Table 3, the authors should also mention MaskGIT baseline performance (Classification Accuracy Score) for reference.\n    - Some important ablations missing: Similar to GenRep, the authors can also consider steered latent views as an alternative to Gaussian noise in the noise-based augmentations. Similarly, random masking certain percentage of discrete values can be chosen as an ablation in the mask-based augmentations.\n    \n    [1] Astolfi, Pietro, et al. \"Instance-Conditioned GAN Data Augmentation for Representation Learning.\" *arXiv preprint arXiv:2303.09677* (2023) TMLR\n\n    [2] Bardes, Adrien, Jean Ponce, and Yann LeCun. \"Vicreg: Variance-invariance-covariance regularization for self-supervised learning.\" arXiv preprint arXiv:2105.04906 (2021).\n    \n    [3] Sariyildiz, Mert Bulent, et al. \"Fake it till you make it: Learning transferable representations from synthetic ImageNet clones.\" CVPR 2023\u2013IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n    [4] Bansal, Hritik, and Aditya Grover. \"Leaving reality to imagination: Robust classification via generated datasets.\" arXiv preprint arXiv:2302.02503 (2023).\n\n    [5] He, Ruifei, et al. \"Is synthetic data from generative models ready for image recognition?.\" arXiv preprint arXiv:2210.07574 (2022)."
                },
                "questions": {
                    "value": "1. In Section 5: \u201cThis is evidenced by their highest linear top-1 accuracy on the ImageNet dataset being 53.3%, 15.2% less than our highest-performing GenCL model.\u201d Have the authors run GenRep with VICreg? Otherwise, it is not fair to compare the numbers of GenRep and GenCL given that GenRep uses a different generative model and a different contrastive method.\n2. The authors mention the below statement in Section 4.3\n    \n    \"In comparison, training a VICReg CL model with geometry and color augmentations\n    for 100 epochs with a batch size of one would take approximately 1,790 hours, which is 51 times longer than augmenting all the images with the MaskGIT model and 255 times longer than augmenting all the images with the ICGAN model.\"\n    I am not sure I understand why the authors are comparing inference time of a generative model with training of VICReg CL model with 1 epoch.\n    \n3. The authors can definitely try exploring a combination of mask-based and noise based augmentations to see if they further improve performance.\n4. I just wanted to clarify that I do not see any supplementary material. There are a lot of implementation details missing in the paper which are necessary to reproduce the results obtained in the paper. For instance, what is the strength of Gaussian noise used for noise based augmentation? Also additional details about the implementation of mask-based augmentations would make things clearer."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2021/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2021/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2021/Reviewer_NnqA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698527375363,
            "cdate": 1698527375363,
            "tmdate": 1699636133588,
            "mdate": 1699636133588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "gU1GNp7t3D",
            "forum": "Wfe8o9RnnX",
            "replyto": "Wfe8o9RnnX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_8vFk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_8vFk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to increase the common set of augmentations used in self-supervised learning with a generative model producing new augmentations, which allows to augment the images with new factors of variations. A study on the augmentation parameters using the VICReg method is proposed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1) The idea of using generative models in self-supervised learning is a good idea that has not been explored too much. This paper proposes a first approach and the generated samples shown in Figure 1 look reasonable.\n\n2) The proposed approach is generic to the underlying self-supervised learning method. The authors use VICReg, but any other more recent method could have been used. This general idea could also have application beyond self-supervised learning, anywhere data augmentation is used."
                },
                "weaknesses": {
                    "value": "1) The experimental results are extremely poor. There is no comparison with other methods.\n\n2) The comparison between a setting with and without generative data augmentations is not convincing. The authors choose the VICReg method as the baseline, but the number reported are very low compared to the number reported in the VICReg paper.\n\n3) The gain of using generative augmentation is very limited and might not be worth the burden. An ablation in terms of running time and memory usage compared to the VICReg baseline would be helpful to draw a conclusion. The one proposed in Table 4 is very unclear and doesn\u2019t seem to compare to the baseline."
                },
                "questions": {
                    "value": "Table 2 and 3 are the same table."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772656169,
            "cdate": 1698772656169,
            "tmdate": 1699636133517,
            "mdate": 1699636133517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "57TTYSpLha",
            "forum": "Wfe8o9RnnX",
            "replyto": "Wfe8o9RnnX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_KP3v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2021/Reviewer_KP3v"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a generative augmentation framework to produce positive views for contrastive learning. Two type of approaches are discussed: noise-based augmentation and mask-based augmentation. The former uses ICGAN and the latter uses MaskGIT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper studies an important problem of constructing views for contrastive learning using generative models.\n- The presented results are promising and the experiments are extensive.\n- The authors promise to open source the code which will facilitate future research."
                },
                "weaknesses": {
                    "value": "- Clarity: 1) the paper has several long paragraphs of text description that is not super friendly to readers. The presentation might be more clear if formal definitions are given. e.g. when using a GAN generator g, does noise-based augmentation mean g(z+N) where z is the noise vector for image I?\n- Clarification: 1) in Table 1, what are the accuracies for baseline (using default augmentations) and what gamma value is used? 2) for mask-based augmentations, do is the ground-truth label used as input to the MaskGIT model?\n- It would be interesting to also study other contrastive learning methods like SimCLR. It might also be interesting to discuss other related work e.g. [1-3].\n- Problem setting: the mask-based augmentation method utilizes MaskGIT which is trained in a fully-supervised manner and also accepts label as input at inference time. My concern is there might be information leakage during training, which would make the unsupervised or even semi-supervised setting unfair. Also, the method would highly depend on the generative model itself, suppose the generative model is a retrieval model that queries a random image of the same class from training set, then the setting becomes SupCon.\n\n[1] Tamkin, A., Wu, M. and Goodman, N., 2020. Viewmaker networks: Learning views for unsupervised representation learning. arXiv preprint arXiv:2010.07432.\n[2] Han, L., Han, S., Sudalairaj, S., Loh, C., Dangovski, R., Deng, F., Agrawal, P., Metaxas, D., Karlinsky, L., Weng, T.W. and Srivastava, A., 2023. Constructive Assimilation: Boosting Contrastive Learning Performance through View Generation Strategies. arXiv preprint arXiv:2304.00601.\n[3] Jahanian, A., Puig, X., Tian, Y. and Isola, P., 2021. Generative models as a data source for multiview representation learning. arXiv preprint arXiv:2106.05258."
                },
                "questions": {
                    "value": "please see my questions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698952898185,
            "cdate": 1698952898185,
            "tmdate": 1699636133435,
            "mdate": 1699636133435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]