[
    {
        "title": "From generalization analysis to optimization designs for state space models"
    },
    {
        "review": {
            "id": "0Vr0aeNF6l",
            "forum": "EGjvMcKrrl",
            "replyto": "EGjvMcKrrl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1640/Reviewer_SPHH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1640/Reviewer_SPHH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proves a generalization error bound for SSMs, where the input data are assumed to be sampled from a Gaussian process, which incorporates temporal dependency. The error bound motivates both a new initialization scaling strategy and a regularized loss function in training. The effect of the new initialization and regularization are evaluated using both a synthetic dataset and the Long-Range Arena benchmark collection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The error bound in Theorem 1 incorporates the temporal dependency of the input data. It is nicely justified in the `Comparison` section why this is important and how it is missing from the previous work.\n* As far as I know, the scaling and the regularization strategies are new to the SSM society. The paper demonstrates their potential promises using experiments.\n* Overall, the paper is clearly written, and the mathematical statements are mostly properly made. (See `Questions` below for a couple of clarification questions.)"
                },
                "weaknesses": {
                    "value": "* My biggest concern is the assumption that the inputs in `Theorem 1` are sampled from a Gaussian process. Perhaps the Gaussian assumption is more reasonable in a non-temporal setting, such as linear regression. However, most time series inputs that we encounter in practice cannot be ''sampled'' from a Gaussian process. For example, you cannot find a single GP that accounts for the flattened MNIST images, because images representing different numbers may have their own unique features, and such distinct features cannot be captured solely by the randomness in your GP. If you fix a GP and randomly sample your sequential pixels, then most figures you obtain won't represent any number. I understand that this Gaussian assumption is crucial in proving your generalization error bound and there is perhaps no way out, but this indeed results in a gap between your theory and the methodologies you proposed.\n\n* The proposed regularization method (9) combines a normalized $\\ell^2$ loss and a regularization term. In an SSM, one usually chains multiple LTI systems; however, only the target output of the entire SSM is known (e.g., whether the maze is solvable, what the number in the MNIST figure is). In that case, it is unclear how the ''target outputs'' $y_i$ of the intermediate LTI systems are defined.\n\n* The evaluation of the model does not show clear evidence of why the scaling of the initialization makes the model more robust. For example, in `Table 1`, comparing the cases `w/o (8), (9)` to `w/ (8)`, it seems that adding the scaling improves the training accuracy but makes the generalization accuracy even worse. This actually contradicts the claim that the model is made more robust by scaling the initialization.\n\n* Since the regularization involves a hyperparameter $\\lambda$, it is a good practice to perform an ablation study to demonstrate the effect of changing $\\lambda$."
                },
                "questions": {
                    "value": "* The setup of this paper does not consider the matrix $\\mathbf{D}$ in an LTI system. How easy is it to incorporate that matrix and do you need to scale $\\mathbf{D}$ in initialization?\n\n* In `Theorem 1`, can you show the explicit dependency of $C_T$ on $T$? This is important because in training an SSM, the discretization size $\\Delta$ is usually trainable, making the final time $T$ in the continuous world change from time to time. Hence, in order to apply your theory, it is better if we can understand the role of $T$.\n\n* In `Theorem 1`, when you say $\\tilde{\\mathcal{O}}(\\cdot)$ hides the logarithmic factor, which variables are considered? For example, it clearly does not hide $\\log(1/\\delta)$.\n\n* The presence of `Proposition 1` seems a bit abrupt. How does that relate to your ''robustness'' discussion? In addition, what kind of ''stability'' are you referring to? This is a fairly ambiguous term, which can represent the stability of a numerical algorithm, the asymptotic stability of your LTI system (i.e., if your eigenvalues of $\\mathbf{A}$ are all in the left half-plane), or something else.\n\n* In your experiments, it is shown that the regularizer improves the training accuracy, which is a bit counter-intuitive. Do you have a justification for that?\n\n* Not a question but a side note: in order to comply with the ICLR formatting guide, all matrices and vectors should be made boldface."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1640/Reviewer_SPHH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697590332143,
            "cdate": 1697590332143,
            "tmdate": 1700747249073,
            "mdate": 1700747249073,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HykvNwlT93",
                "forum": "EGjvMcKrrl",
                "replyto": "0Vr0aeNF6l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions. In the following, we address the 4 weaknesses from Weakness 1 to Weakness 4 and the 5 questions from Q1 to Q5.\n\nWeakness 1. *My biggest concern is the assumption that the inputs in Theorem 1 are sampled from a Gaussian process.*\n* We agree with the reviewer that the Gaussian process assumption is a strong assumption. To address this concern, we have relaxed the Gaussian process assumption to a more general class of random processes that are sub-Gaussian and H\u00f6lder continuous.\nThis is a more general assumption than the Gaussian process assumption in the sense that it covers a larger class of random processes including Gaussian processes, H\u00f6lder continuous bounded processes.\nIn particular, as we mentioned in the first point of the response *To all reviewers*, when the range of the random process is a finite set, then the H\u00f6lder continuity condition can be dropped.\nThis relaxation includes the case of image classification tasks, where the input image is flattened as a sequence and the range for each pixel value is a finite set, e.g., $\\{0,1,\\ldots,255\\}$.\n**In the revised paper, we have substantially improved the generality of our theorem (see the revised statement in Theorem 1 and proof in Appendix E) by dropping the Gaussian process assumption and replacing it with a much weaker one in Assumption 1.**\n\nWeakness 2. *The proposed regularization method (9) combines a normalized $\\ell^2$ loss and a regularization term. In an SSM, one usually chains multiple LTI systems; however, only the target output of the entire SSM is known (e.g., whether the maze is solvable, what the number in the MNIST figure is). In that case, it is unclear how the ''target outputs'' $y_i$ of the intermediate LTI systems are defined.*\n* We would like to mention that the regularized loss by the regularization method (9) is given by the original loss plus the regularization measure $\\tau(\\theta)$.\nWhen training multi-layer SSMs, we calculate the complexity $\\tau(\\theta)$ at each layer and add them together as a total regularization measure.\nNotice that the complexity for each single layer only depends on the input sequence statistics and the model parameters of that layer.\nTherefore, we do not need to know how the target outputs $y_i$ of the intermediate systems are defined.\n**To make the statement more clear, we have changed the notation for the regularized loss (9) to $\\tilde{R}_n(\\theta) := R_n(\\theta) +  \\lambda  \\cdot \\tau (\\theta)$ and added the above explanations in Section 4.3.**\n\nWeakness 3. *The evaluation of the model does not show clear evidence of why the scaling of the initialization makes the model more robust. For example, in Table 1, comparing the cases w/o (8), (9) to w/ (8), it seems that adding the scaling improves the training accuracy but makes the generalization accuracy even worse. This actually contradicts the claim that the model is made more robust by scaling the initialization.*\n* There is a misunderstanding here due to the ambiguity of the term `robustness`.\nWe would like to emphasize that 'robustness' in the paper refers to the robustness of the *output value scales* (instead of the test performance) on SSMs across different temporal dependencies of the input sequence.\nThis is desirable because one does not a priori have a good sense of the input sequence and the temporal dependency of the input sequence may vary a lot.\nTherefore, a good way to initialize SSMs should be one that is not sensitive to the variations of the temporal structures.\nThis is not true for the original HiPPO based initialization scheme, which is independent of the input sequence.\nBy looking at the left-hand side of Figure 2, we can see that the SSM output scales at initialization without rescaling (dashed lines) are very sensitive to the variations of temporal dependencies, while the SSM output scales at initialization with rescaling  (solid lines) are much more robust.\nIn Table 1, we can also see that adding the scaling improves the training performance but the generalization performance is not guaranteed to be improved.\nThe above empirical findings also match Proposition 1, which shows that the scale of the SSM output scales after rescaling does not depend on the variations of the temporal structures.\n**To make the statement more clear, we have added the above discussions in Section 4.4.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668479944,
                "cdate": 1700668479944,
                "tmdate": 1700668479944,
                "mdate": 1700668479944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RnX7s6Ub1p",
            "forum": "EGjvMcKrrl",
            "replyto": "EGjvMcKrrl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1640/Reviewer_nrdv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1640/Reviewer_nrdv"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the generalization performance of state space model, in which the data-dependent generalization bound are established. Motivated by the the theoretical findings, the authors design a scaling rule for model initialization and introduce a new regularization mechanism to improve both the robustness and generalization performance of SSM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Each section of the paper is clear presented and motivates the paper well.\n2. The generalization results appear to be interesting, and the experimental results support the theoretical claims."
                },
                "weaknesses": {
                    "value": "The current theoretical results could be more plentiful, e.g. replenish the generalization analysis on  regularized model (9), which may help to answer the question raised below."
                },
                "questions": {
                    "value": "1. In Theorem 1 , the authors claim that the SSM generalization is characterized by the temporal dependencies of the sequential data. More details on how does the dependency of the sequential data affect the generalization error should be included. Moreover, in order to achieve small generalization error, the mean and variance of the GP should remain a small level. While these two key parameters rely on the GP assumption, independent  of data. This seems inconsistent with data-dependent generalization error bounds, as claimed in the paper.\n\n2. In speak of enhancing the robustness of SSMs on different temporal dependencies, the authors take   $1/\\sqrt{\\tau(\\theta)}$  as a rescaling factor for initialization. Any theoretical guarantees (e.g. variance analysis)  on the robustness comparing with the HiPPO framework?\n\n3. The main techniques adopted in the proof are sub-exponential property of r.v. and Borell-TIS inequality, how did they yield to  temporal dependency generalization bounds since both of them are temporal independent."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698557009921,
            "cdate": 1698557009921,
            "tmdate": 1699636092459,
            "mdate": 1699636092459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f73UgiTUfC",
                "forum": "EGjvMcKrrl",
                "replyto": "RnX7s6Ub1p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions. In the following, we address questions from Q1 to Q3.\n\nQ1. *In Theorem 1 , the authors claim that the SSM generalization is characterized by the temporal dependencies of the sequential data. More details on how does the dependency of the sequential data affect the generalization error should be included. Moreover, in order to achieve small generalization error, the mean and variance of the GP should remain a small level. While these two key parameters rely on the GP assumption, independent of data. This seems inconsistent with data-dependent generalization error bounds, as claimed in the paper.*\n* Our generalization bound is data-dependent in the sense that it depends on the statistics of the input sequence and we can only calculate the bound after observing the input sequence.\nIn particular, we can see that the dominant term is given by\n$$\n\\psi(\\theta) := \\int_0^T \\left|\\rho_{\\theta} (T-s)\\right| \\sqrt{K(s, s)}ds +\\left|\\int_0^T  \\rho_\\theta(T-s) \\mu(s) d s\\right|.\n$$\nFor the SSM basis function $\\rho_\\theta(s) = C e^{As}B$, it is exponentially decay with respect to $s$ because the eigenvalues of $A$ are negative.\nTherefore, we do not require the mean and variance to be uniformly small in $[0, T]$ to get a small generalization gap.\nWe also relax the GP assumption to a more general class of random processes that are sub-Gaussian and H\u00f6lder continuous, and we refer the reviewer to the response to the comment 5 from Reviewer 2971 for more details.\n**We have added the above discussions to make the generalization bound more clear after Theorem 1.**\n\nQ2. *In speak of enhancing the robustness of SSMs on different temporal dependencies, the authors take $1/\\sqrt{\\tau(\\theta)}$\n as a rescaling factor for initialization. Any theoretical guarantees (e.g. variance analysis) on the robustness comparing with the HiPPO framework?*\n * We would like to emphasize that the rescaling method is built on the HiPPO based initialization, and it is applied before training.\n The HiPPO based initialization method is independent of the input sequence while we rescale the initialization based on the input sequence.\n This rescaling method improves the robustness of the initial \n output value scales of SSMs across different temporal dependencies of the input sequence.\n As Proposition 1 shows, the scale of the SSM output values after rescaling does not depend on the variations of the temporal structures.\n **To make the statement more clear, we have added the above discussions in Section 4.2.**\n\n Q3. *The main techniques adopted in the proof are sub-exponential property of r.v. and Borell-TIS inequality, how did they yield to temporal dependency generalization bounds since both of them are temporal independent.*\n * We would like to emphasize that the sub-Gaussian property and the H\u00f6lder continuity condition in Assumption 1 are applied to sequences in a time interval $[0, T]$ instead of a single time point (see equation (14) for the terms that we need to bound) in our proof of Theorem 1. This is where the temporal dependency comes from.\n Combining the $\\varepsilon$-net argument and the covering number tools, we can apply the sub-Gaussian property to a sequence in a time interval $[0, T]$.\n Since we have relaxed the Gaussian process assumption to a more general class of random processes that are sub-Gaussian and H\u00f6lder continuous, the proof of Theorem 1 has also been revised accordingly.\n **In light of the question, we added more details in the proof sketch of Theorem 1.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668407674,
                "cdate": 1700668407674,
                "tmdate": 1700668407674,
                "mdate": 1700668407674,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1ETb81MJZa",
            "forum": "EGjvMcKrrl",
            "replyto": "EGjvMcKrrl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1640/Reviewer_2971"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1640/Reviewer_2971"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an analysis of a generalization bound for linear SSMs.  These SSMs are the building blocks of a new class of deep sequence models.  The authors posit that understanding this bound promises to inform the design of initialization and regularization schemes.  Networks trained using these techniques are shown to have better performance or favorable training characteristics on two simple examples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Studying the scaling properties of different initialization schemes and regularizers is clearly important.  Other papers have started to study this also in a bid to understand if HiPPO is simply an example of a wider family of options.  Furthermore, regularizers arising naturally from generalization (which is what a regularizer is trying to target!) is intuitively appealing.  The detail the authors go through their derivations is (for better or for worse) incredible.  I have not gone through the derivations line by line, but I think I understand the general gist of them.  The intuitions given are enough to allow most readers to grasp the core concepts.  The experimental results show promise.  Overall, the paper is fairly well written and prepared."
                },
                "weaknesses": {
                    "value": "I am very on the fence on this work.  I think the work is sound, and the authors are to be commended for the detail they go into, but I am not quite convinced that it is at the requisite threshold for acceptance.  Ironically, I am actually left wanting slightly more.  As someone who uses SSM models, I am not yet convinced to integrate this into our workflow, and would need to see more evidence that it is worth incorporating.  Furthermore, I think there are some disconnects between the theory and the practice.\n\nMy main comment is that the experimental evaluation isn\u2019t quite complete enough to convince me:\n- There is quite a lot of work here to just get better generalization as shown on a near-pathological synthetic example, and marginal improvement on LRA (see Q.1. as well). \n- I would have also liked to have seen more evaluation of the initialization across different sizes of models, sensitivity to hyperparameters etc.  Experimental repeats are also important to ensure that the results are reliable.  \n- The additional time complexity is also theoretically commensurate with the original S4 model, but I would like to see a concrete comparison of the runtimes to confirm this.  \n- I would also like to see a more thorough comparison to, e.g., the initialization and metrics suggested by Orvieto et al. [2023], or evaluation of whether this initialization/regularization scheme can be applied to methods adjacent to S4 (e.g. S4D, S5, Liquid-S4, MEGA).  \n- How reasonable are the assumptions, and how tight are the bounds in practice?  I do not have a great understanding of whether the GP assumption is sensible in practice, and there doesn\u2019t appear to be any validation of this.  How does the fidelity of the GP approximation impact the performance of the regularizer?  \n- It would be interesting to try and establish exactly how the initialization and regularization terms affect the learned model.  I understand that L2 regularization reduces the magnitude of the parameters (controlling a notion of complexity), but what does the regularizer in (9) actually encourage in the learned models?  How are the regularized models different from regular S4 models?  This analysis might enable the design of even better SSM structures.  \n- It is also a shame that Path-X wasn\u2019t included in the paper.  My understanding is that Path-X is the only really challenging LRA benchmark.  While I am willing to overlook this in this evaluation, I encourage the authors to complete the benchmarks.  \n\nThere are additional results in the supplement that are basically not commented on, and seem important (e.g. Figure 3 and Figure 4).  These should be explained more thoroughly, and brought up to the main if they are truly important.  I think these extra experiments that probe the method are super important to verifying that the method is working as expected.\n\n**Summary**:  Right now I am just about convinced that the method just about works, but I think some arguments and opportunities aren\u2019t fully explored.  There is clearly an opportunity for this line of work to become very impactful, but I think it would benefit from a round of revisions, and expanding the breadth and depth of the experimental evaluation.  That said, I am very open to revising my review score should the authors remedy some of my concerns.  \n\n## Minor comments\n- Figures, tables etc should be floated to the top or bottom of pages, as opposed to inserted in the middle of the text. \n- Table 1 should be prepared using the same (and correct) style as used by Table 2.\n- Only proper nouns should be capitalized."
                },
                "questions": {
                    "value": "**Q.1.**:  Can the authors clarify whether, in Table 2, w/o (8, 9) corresponds to the original S4 model?  The numbers are slightly lower than in the original paper, and I am trying to clarify whether these numbers are like-for-like within the table, and how comparable to S4 they are.  \n\n**Q.2.**:  The theories and algorithms presented are for one-layer networks, but then in Section 4.4 you use multilayer networks.  Can the authors comment how the theories translate to multi-layer networks, where, presumably, the statistics of the input to each layer are not constant.  \n\n**Q.3.**:  Can you clarify how the rescaling in Line 7 of Algorithm 1 works: (a) across epochs and (b) extends to multiple layers.  R.e. (a): is the value of $\\tilde{C}$ rescaled at the beginning of every epoch?  I.e. it is being \u201creinitialized\u201d by rescaling its previous values.  R.e. (b): does rescaling $\\tilde{C}$ at each layer disrupt the action of other layers?  Or is there a different method for rescaling between layers?  \n\n**Q.4.**:  The experiment in Figure 2, is it really Gaussian white noise?  Or is it more like Brownian motion?\n\n**Q.5.**:  Does the training loss in Figure 2 (right) include the regularization term?  I believe it should actually be labeled as \u201cTraining set MSE\u201d.  \n\n**Q.6.**:  An appealing benefit of S4 is the zero-shot transfer to other sampling frequencies.  However, this might change the scale of the time-dependencies.  Can the authors clarify whether there are drawbacks to this method with respect to zero-shot transfer? \n\n**Q.7.**:  The theory is presented for linear SSMs, but in practice, multi-layer S4 models are interleaved with position-wise nonlinearities.  I cannot see any discussion of how these nonlinearities (and the parameters in these nonlinearities, e.g. GLU) interact with the regularization of the parameters in the SSM, or, how the warping effect of the nonlinearity affects/interacts with the theoretical results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751354224,
            "cdate": 1698751354224,
            "tmdate": 1699636092387,
            "mdate": 1699636092387,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eWRltAW3XY",
                "forum": "EGjvMcKrrl",
                "replyto": "1ETb81MJZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions. In the following, we address the comments for the 7 comments and questions from Q1 to Q7.\n\n Comment 1. *There is quite a lot of work here to just get better generalization as shown on a near-pathological synthetic example, and marginal improvement on LRA (see Q.1. as well).*\n\n Q1. *Can the authors clarify whether, in Table 2, w/o (8, 9) corresponds to the original S4 model? The numbers are slightly lower than in the original paper, and I am trying to clarify whether these numbers are like-for-like within the table, and how comparable to S4 they are.*\n  * We would like to emphasize that this paper is not just about getting better generalization. The theory is interesting by itself as it is the first data-dependent generalization bound for SSMs that is based on the temporal dependency of the input sequence.\n  For the improvement on LRA, **we have added more numerical results for bidirectional models such as bidirectional S4-Legs and bidirectional S4D-Legs in Table 2 and Appendix A.2.**\n  The results in the updated Table 2 show that our proposed regularization method consistently improves the generalization performance in most tasks for all three models. \n  In that sense, the improvement on LRA is not marginal.\n  * Our original version used the unidirectional S4-Legs model (Gu 2022a), which processes a sequence in one direction from past to future.\n  Compared to recent commonly used bidirectional models (Gu 2023), the unidirectional model is closest to our problem setting even though it is not the state-of-the-art model.\n  Therefore, the numbers in Table 2 should be compared with the results in the earliest paper (Gu 2022a), and we can see that our numbers match with the results in (Gu 2022a).\n\n Comment 2. *I would have also liked to have seen more evaluation of the initialization across different sizes of models, sensitivity to hyperparameters etc. Experimental repeats are also important to ensure that the results are reliable.*\n * **We have added more numerical results on the sensitivity of the regularization coefficient $\\lambda$ for unidirectional S4-Legs, bidirectional S4-Legs and bidirectional S4D-Legs in Section A.2**.\n The results in **Table 6, 7, 8** show that the test performance is much more sensitive to $\\lambda$ for the Pathfinder and PathX tasks compared to other tasks.\n This is because the generalization measure for the Pathfinder and PathX tasks are much larger than other tasks (as shown in **Figure 3, 4, and 5**), making the generalization measure very sensitive to the magnitude of $\\lambda$.\n The ablations study on the sensitivity of $\\lambda$ matches the intuition that the Pathfinder and PathX tasks are more challenging than the other tasks.\n For the repeated experiments, we have added the standard deviation (in 3 independent runs) of the test accuracy for the PathX task in **Table 2** for the bidirectional models.\n\nComment 3. *The additional time complexity is also theoretically commensurate with the original S4 model, but I would like to see a concrete comparison of the runtimes to confirm this.*\n* **We have added the running time per epoch in the updated Table 2 in Section 4.4.**\nThe running time comparison shows that the proposed regularization method brings a little extra computational cost compared to the original training method.\n\nComment 4. *I would also like to see a more thorough comparison to, e.g., the initialization and metrics suggested by Orvieto et al. [2023], or evaluation of whether this initialization/regularization scheme can be applied to methods adjacent to S4 (e.g. S4D, S5, Liquid-S4, MEGA).*\n* **We have added more evaluation on the bidirectional S4D-Legs model in Table 2 and Appendix A.2.**\nThe results in **Table 2** show that our proposed regularization method and initialization scheme can improve the generalization performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667835750,
                "cdate": 1700667835750,
                "tmdate": 1700667835750,
                "mdate": 1700667835750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dqmeZdGbXf",
                "forum": "EGjvMcKrrl",
                "replyto": "1ETb81MJZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's response (continue)"
                    },
                    "comment": {
                        "value": "Q3. *Can you clarify how the rescaling in Line 7 of Algorithm 1 works: (a) across epochs and (b) extends to multiple layers. R.e. (a): is the value of $\\tilde{C}$\nrescaled at the beginning of every epoch? I.e. it is being \u201creinitialized\u201d by rescaling its previous values. R.e. (b): does rescaling $\\tilde{C}$ at each layer disrupt the action of other layers? Or is there a different method for rescaling between layers?*\n* The rescaling method is only applied for SSMs before training. We calculate the complexity measure layer by layer and then rescale the matrix $C$ from the first layer to the last layer sequentially.\nFor example, for a $k$-layer SSM with the initial matrix $C_1,\\ldots,C_k$ at each layer, we first calculate the complexity measure $\\tau_1(\\theta)$ for the first layer and rescale $C_1$ by $C_1/\\sqrt{\\tau_1(\\theta)}$.\nThen we calculate the complexity measure $\\tau_2(\\theta)$ for the second layer for the updated input sequence of layer 2 and rescale $C_2$ by $C_2/\\sqrt{\\tau_2(\\theta)}$. We repeat this process until the last layer.\n**We have added the above discussions in Section 4.2.**\n\nQ4. *The experiment in Figure 2, is it really Gaussian white noise? Or is it more like Brownian motion?*\n* It is Gaussian white noise. The autocovariance function for Brownian motion is given by $K(s,t) = \\min(s,t)$ rather than $\\frac{1}{|b| \\sqrt{\\pi}} e^{-((s-t)/b)^2}$.\n\nQ5. *Does the training loss in Figure 2 (right) include the regularization term? I believe it should actually be labeled as \u201cTraining set MSE\u201d.*\n* Figure 2 (right) shows the training loss when training with the initialization scheme but without the regularization method.\n**We have highlighted it in Section 4.2.**\n\nQ6. *An appealing benefit of S4 is the zero-shot transfer to other sampling frequencies. However, this might change the scale of the time-dependencies. Can the authors clarify whether there are drawbacks to this method with respect to zero-shot transfer?*\n* This is an interesting question. In fact, our generalization measure is zero shot transerable because the input sequence statistics $\\sqrt{K(s,s)}, \\mu(s)$ in our generalization bound both have the same scaling as the input sequence $x(s)$, i.e., by changing sequence $x(s)$ to $x(k s)$ for $k>0$, the standard deviation and the mean of $x(ks)$ is changed to $\\sqrt{K(ks,ks)}, \\mu(ks)$, respectively.\nThen the generalization measure is invariant after changing the sampling frequency if the SSM output value is also invariant.\nTherefore, the zero shot transferability for SSMs is automatically inherited by our generalization bound.\n**In line with the question, we have added a thorough discussion at the end of Section 4.1.**\n\nQ7. *The theory is presented for linear SSMs, but in practice, multi-layer S4 models are interleaved with position-wise nonlinearities. I cannot see any discussion of how these nonlinearities (and the parameters in these nonlinearities, e.g. GLU) interact with the regularization of the parameters in the SSM, or, how the warping effect of the nonlinearity affects/interacts with the theoretical results.*\n* Since we have relaxed the original assumption of Gaussian process to a more general class of random processes that are sub-Gaussian and H\u00f6lder continuous, the theory can be extended if the nonlinearities do not affect the H\u00f6lder condition and the sub-Gaussian property.\nIn fact, the assumption is to ensure that the input sequence is bounded (with high probability) in a given time interval.\nTherefore, intuitively, if the nonlinearity function has a controllable growth rate, then the boundness of the input sequence with the nonlinearity is still satisfied and our theory works if one considers a single layer SSM.\nFor example, for Lipschitz nonlinearities, we list some examples in **Appendix C** to show that the H\u00f6lder condition and the sub-Gaussian property are still satisfied.\n**We have added the above discussions in Section 5.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667975436,
                "cdate": 1700667975436,
                "tmdate": 1700725073918,
                "mdate": 1700725073918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]