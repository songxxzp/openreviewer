[
    {
        "title": "Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning"
    },
    {
        "review": {
            "id": "h1eJCU5SgU",
            "forum": "LWEqTLCHrw",
            "replyto": "LWEqTLCHrw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8006/Reviewer_TEpF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8006/Reviewer_TEpF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a FL anomaly detection method with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuring honest execution of defense mechanisms at the server by leveraging a zero-knowledge-proof mechanism."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors proposed a new method for FL anomaly detection."
                },
                "weaknesses": {
                    "value": "1. The authors did not compare with FL anomaly detection methods such as FLDetector in experiments.\n2. The authors did not test their methods with some strong attacks such as [1], [2] , and [3].\n\n[1] Gilad Baruch, Moran Baruch, and Yoav Goldberg. 2019. A Little Is Enough: Circumventing Defenses For Distributed Learning. In NeurIPS      \n[2] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2020. How to backdoor federated learning. In AISTATS\n[3] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. 2019. Dba: Distributed backdoor attacks against federated learning. In ICLR\n\n3. The authors did not explore the influence of the number of malicious clients."
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Reviewer_TEpF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698451161264,
            "cdate": 1698451161264,
            "tmdate": 1699636986884,
            "mdate": 1699636986884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Watr34kWsT",
                "forum": "LWEqTLCHrw",
                "replyto": "h1eJCU5SgU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8006/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8006/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8006/Reviewers",
                    "ICLR.cc/2024/Conference/Submission8006/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Chairs and Reviewers"
                    },
                    "comment": {
                        "value": "Dear Chairs and Reviewers, \n\nWe do appreciate your effort in reviewing our paper. We got an overall rating of 3 (reject) and a confidence of 4 (confident in their assessment) from Reviewer TEpF, however, we would like to point out that the review is problematic and full of mistakes contradicting our work. In what follows, we expand our concerns in the following aspects: \n\n## Nearly copy-paste summary\n\nReviewer TEpF summarized the paper as follows:\n\n```\nThe paper proposed a FL anomaly detection method with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuring honest execution of defense mechanisms at the server by leveraging a zero-knowledge-proof mechanism.\n```\n\nWhile, in the abstract of our submission, we wrote: \n\n```\nTo address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuring honest execution of defense mechanisms at the server by leveraging a zero-knowledge proof mechanism. \n```\n\nBy comparing our original text with Reviewer TEpF\u2019s summary, we can easily see that Reviewer TEpF almost copied-pasted part of our abstract with little modification. This makes us think that the Reviewer TEpF has spent little to no time reviewing our paper, as evidenced by their other comments, which we discuss next.\n\n\n## Comments with factual mistakes regarding a \u201cmissing\u201d experiment\n\nIn Weakness 3 (W3), the reviewer said \u201cThe authors did not explore the influence of the number of malicious clients.\u201d However, in the original submitted version, we **exactly included** the experiment as Exp 5 (Varying the number of malicious clients) in the experiment section, \u2013 not the appendix. We varied the percentage of malicious clients to be 0, 20%, and 40%, and our approach works pretty well, \u2013 in both cases of 20% and 40% malicious clients, the test accuracy remains very close to the benign case.\n\n\n## Controversial comments with factual mistakes regarding general experiments\n\nIn W2, the reviewer said \u201cThe authors did not test their approach with some strong attacks\u201d and then the reviewer listed 3 papers ([1][2][3]). However, we have already utilized the attack in [2] in our experiments. Moreover, [1] is in fact a famous defense paper, \u2013 even its title clearly reflects that it is a defense paper. Though that paper includes an attack, it is impractical in real-world scenarios (in the next section, we describe why it is impractical). Based on our survey so far, in our humble opinion, we have not seen any paper that utilizes the attack described in [1]. On the other hand, in our experiments, we have already utilized attacks that are both practical and stronger than the attacks suggested by the reviewer. We will explain them in detail in the following section."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515829474,
                "cdate": 1700515829474,
                "tmdate": 1700515829474,
                "mdate": 1700515829474,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8gAu1I9hRE",
                "forum": "LWEqTLCHrw",
                "replyto": "h1eJCU5SgU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8006/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8006/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8006/Reviewers",
                    "ICLR.cc/2024/Conference/Submission8006/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Chairs and Reviewers and Detailed Responses to Reviewer TEpF"
                    },
                    "comment": {
                        "value": "As we responded to W3 in the section above, we would like to address the remaining 2 weaknesses mentioned by Reviewer TEpF. \n\n### W1: The authors did not compare with FL anomaly detection methods such as FLDetector in experiments.\n\nAs already mentioned in our paper, FLDetector is not practical in real-world scenarios. It relies heavily on historical client models from previous training rounds. By examining their detailed implementation, we found that the number of pre-training rounds may vary based on the datasets. For example, the number of pre-training rounds is set to 50 when the datasets are MNIST and FEMNIST, and 20 when the dataset is CIFAR10. Since our approach starts with zero pre-training rounds, it is more generalized, and we do not think there is need in the experiment section to compare our approach with FLDetector which has the practical design limitations above. Thus, in our paper, we decided to mention this work in the Related Works section. \n\n### W2: The authors did not test their methods with some strong attacks\n\nReviewer TEpF suggested 3 attacks (paper [1][2][3]) to us. While we have already included one of the three attacks in our experiments ([2]), we would like to point out that our work has already leveraged stronger and more practical attacks in our paper, compared to those suggested by Reviewer TEpF. \n\nAs addressed in the whole paper, our anomaly detection solution handles the challenges faced by real-world FL systems. The basic rule to select attacks in our experiments is that the attack must be practical to be utilized by malicious clients in real-world cases. \n\nAs for the other two attacks suggested by Reviewer TEpF, [1] is a famous defense paper in fact, \u2013 even its title clearly reflects that it is a defense paper. Though they design an attack, to the best of our knowledge, there is neither an existing work leveraging this attack nor it is practical in real-world scenarios. As shown in their open-sourced implementation (https://github.com/moranant/attacking_distributed_learning/blob/master/malicious.py), the method requires the malicious clients to modify their local models based on the \u201cdistribution\u201d of all local models in the current FL training round, using a mean and a standard deviation. Implementing this in real-world FL applications requires the server (or a malicious third party) to compute the real mean (i.e., the averaged model) and the standard deviation, then send them back to the malicious clients for them to compute the poisoned local model. If the mean (averaged model) is computed by the server, the server has no reason to take the poisoned local model again from the local clients, as it has already obtained the averaged model for the current FL round. If the mean is computed by a malicious third party, then the assumption is that the third party attacker has taken control over the whole FL system, which is too strong and almost impractical in real-world scenarios (remember we have an honest majority). \n\nRegarding the attack in [3], in fact, in our experiments, we have already considered two practical attacks ([2] in the reviewer\u2019s suggested attacks and [4]) that are stronger than [3]. The stronger attack in our experiments can even induce the global model to be the local model submitted by a malicious client. Even under this scenario, we show that our proposed method works effectively.\n\n[4] Chen, Yudong, Lili Su, and Jiaming Xu. \"Distributed statistical machine learning in adversarial settings: Byzantine gradient descent.\" Proceedings of the ACM on Measurement and Analysis of Computing Systems 1.2 (2017): 1-25."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515925531,
                "cdate": 1700515925531,
                "tmdate": 1700620952775,
                "mdate": 1700620952775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FsKv3g8Opt",
                "forum": "LWEqTLCHrw",
                "replyto": "h1eJCU5SgU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8006/Reviewer_TEpF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8006/Reviewer_TEpF"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Dear Chair, reviewers, and authors\n\nThanks to the authors for the detailed reply! However, most of my concerns still exist. \n\n1. The authors still have not included FLDetector in experiments, which is a popular method for FL anomaly detection. The authors just said the historical information is required in FLDetector, and then the method is impractical. This is not quite convincing. \n\n2. [1] and [3] are still not compared. [1] A Little Is Enough: Circumventing Defenses For Distributed Learning is an attack paper, even from the title.\n\n3. Systematic exploration of the influence of the number of malicious clients is expected. In Exp5, only 2 and 4 malicious clients for a single attack are considered.\n\nTherefore, I would like to keep my score. Finally, I suggest paying more attention to the paper itself, which will be more helpful."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706812746,
                "cdate": 1700706812746,
                "tmdate": 1700706812746,
                "mdate": 1700706812746,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bVNiXfeOyi",
            "forum": "LWEqTLCHrw",
            "replyto": "LWEqTLCHrw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8006/Reviewer_zU9y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8006/Reviewer_zU9y"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel anomaly detection approach for addressing the vulnerability of federated learning (FL) systems to malicious clients. It focuses on detecting and eliminating malicious client models without harming benign ones, using a zero-knowledge proof mechanism to ensure honest execution of defense mechanisms at the server."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed approach detects attacks and performs defense operations only when attacks occur, and further identifies and removes malicious client models, therefore being harmless on benign ones."
                },
                "weaknesses": {
                    "value": "1. The mechanism relies heavily on the assumption that malicious clients will remain below 50% of the total. However, its effectiveness may be limited if this assumption does not hold, as adversarial clients constituting over half the clients could possibly sabotage the defense. The authors did not sufficiently discuss limitations to the mechanism or potential strategies if this threshold is exceeded.\n\n2. The proposed method relies on applying the **Three Sigma Rule** to identify outlier clients, but does not adequately justify the underlying assumption that client behavior will follow a **Gaussian/normal distribution**. Without evidence or validation that the models meet this statistical requirement, the correctness and reliability of using this rule is unclear. The authors need to provide empirical or theoretical support for why these distributions were expected in this context, otherwise the key thresholding technique lacks rigorous foundation. \n\n3. The paper claims to propose a new defensive mechanism, but its core algorithms - Krum, Three Sigma Rule and Zero-knowledge proofs - are already well-established in prior work. While integrating existing techniques can still yield new systems, the paper does not provide sufficient insight into how this combination offers meaningful advantage. Without novel algorithmic or analytical insights, the technical value of merely assembling known pieces is limited. To strengthen the paper, the authors should demonstrate deeper understanding of how this specific integration improves upon the state-of-the-art.\n\n4. The evaluation of the proposed method's scalability and performance is limited by the small-scale datasets used (e.g. FEMNIST, CIFAR). While useful for proof-of-concept, these datasets do not adequately represent the data and computational heterogeneity of modern large-scale Federated Learning systems. To fully demonstrate the practicality and effectiveness of this defense, it will be important for the authors to test its performance and overhead when applied to real-world FL scenarios at a larger scale. Without such experimentation on industry-grade datasets and systems, the approach's scalability and real-world viability remain uncertain."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Reviewer_zU9y"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698476305977,
            "cdate": 1698476305977,
            "tmdate": 1699636986767,
            "mdate": 1699636986767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eCYayIkVWn",
                "forum": "LWEqTLCHrw",
                "replyto": "bVNiXfeOyi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zU9y"
                    },
                    "comment": {
                        "value": "We appreciated your constructed feedback that helped enhance our paper. Below are our responses.\n\n**W1: Assumption of Malicious Clients Remaining Below 50%**\n\nThank you for highlighting the importance of discussing the limitations of our mechanism under different proportions of malicious clients. First, we would like to point out that security solutions often address scenarios that are less frequently encountered in the real world \u2013 those that may rarely occur, but when they do, they can lead to serious consequences. This holds true for security issues in Federated Learning (FL) systems as well. In practical FL systems, the proportion of malicious clients is typically no more than 10%. Consistent with this, most literature, to the best of our knowledge, evaluates scenarios with a maximum of 50% adversarial clients, as referenced in [1][2][3][4]. Some methods impose even stricter requirements on the percentage of malicious clients. For example, the well-known m-Krum approach [1] selects n out of N clients in each FL iteration, but only m out of these n local models contribute to the aggregation. The percentage of malicious clients must be less than 1/2 - (m+2)/n as required by the method, which is usually significantly lower than 1/2. For example, in the optimal scenario with 10 participating clients and only 1 local model contributing to the aggregation, which directly leads to an upper limit on the percentage of malicious adversaries when n is 10, the requirement is that the percentage of malicious clients is less than 1/2 - 3/10 = 20%.\nYour comment, however, has inspired us to explore further. If given the opportunity to enhance our paper, we plan to expand Experiment 5 (Varying the number of malicious clients) to include scenarios with over 50% malicious clients. This will allow us to test the limits of our approach in more extreme conditions. It is also noteworthy that our approach has performed well so far. In Experiment 5, where we varied the percentage of malicious clients to 0%, 20%, and 40%, the test accuracy remained very close to that of the benign scenarios, even with 20% or 40% malicious clients.\n\n\n[1] Blanchard, Peva, et al. \"Machine learning with adversaries: Byzantine tolerant gradient descent.\" Advances in neural information processing systems 30 (2017).\n\n[2] Guerraoui, Rachid, and S\u00e9bastien Rouault. \"The hidden vulnerability of distributed learning in byzantium.\" International Conference on Machine Learning. PMLR, 2018.\n\n[3] Karimireddy, Sai Praneeth, Lie He, and Martin Jaggi. \"Byzantine-robust learning on heterogeneous datasets via bucketing.\" ICLR2022.\n\n[4] Xie, Chulin, et al. \"Crfl: Certifiably robust federated learning against backdoor attacks.\" International Conference on Machine Learning. PMLR, 2021.\n\n**W2: Justification for Assuming Gaussian/Normal Distribution**\n\nWe appreciate your feedback on the necessity of justifying the Gaussian distribution assumption for client models. As studied extensively in the distributed learning literature, if the local data of the clients are i.i.d., parameters of the local models follow a Gaussian distribution, see  [5][6][7]. Further, even if the client datasets are non-i.i.d., based on the Central Limit Theorem, the distribution of the local models tends toward a normal distribution.\n\nWe included the following description in Section 3.2.\n```\nThe 3 Sigma Rule is pivotal for two reasons: i) in case the client datasets are i.i.d., parameters of local models follow a Gaussian distribution [5][6][7] ; and ii) even when client datasets are non-i.i.d., the Central Limit Theorem indicates that local models tend toward a normal distribution.\n```\n[5] Baruch, Gilad, Moran Baruch, and Yoav Goldberg. \"A little is enough: Circumventing defenses for distributed learning.\" Advances in Neural Information Processing Systems 32 (2019).\n\n[6] Chen, Yudong, Lili Su, and Jiaming Xu. \"Distributed statistical machine learning in adversarial settings: Byzantine gradient descent.\" Proceedings of the ACM on Measurement and Analysis of Computing Systems 1.2 (2017): 1-25.\n\n[7] Yin, Dong, et al. \"Byzantine-robust distributed learning: Towards optimal statistical rates.\" International Conference on Machine Learning. PMLR, 2018."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515491119,
                "cdate": 1700515491119,
                "tmdate": 1700515491119,
                "mdate": 1700515491119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "El5XcHa5nZ",
                "forum": "LWEqTLCHrw",
                "replyto": "Kf8Qd7fUoX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8006/Reviewer_zU9y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8006/Reviewer_zU9y"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the authors\u2019 response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714083282,
                "cdate": 1700714083282,
                "tmdate": 1700714083282,
                "mdate": 1700714083282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9kProMnf49",
            "forum": "LWEqTLCHrw",
            "replyto": "LWEqTLCHrw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8006/Reviewer_qLTB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8006/Reviewer_qLTB"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an anomaly detection approach for Federated Learning. Their method eliminates the need for prior knowledge of the number of malicious clients and avoids reliance on re-weighting or modifying submissions. Experimental results demonstrate the efficacy of their approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well written and generally easy to follow.  \n2. The method has been compared to a significant amount of related research.\n3. A well-structured and clear presentation."
                },
                "weaknesses": {
                    "value": "1.The explanations of some experimental results are not entirely convincing.\n2.The description of the threat model needs to be more accurate."
                },
                "questions": {
                    "value": "1.\tIt is preferable to describe the threat model based on adversary goals, knowledge, and capabilities.\n2.\tThe justification provided for selecting the 'second-to-the-last layer' in Exp1 is not sufficient.\n3.\tI didn't fully grasp the significance of \"VERIFIABLE ANOMALY DETECTION USING ZKP,\" and the authors should emphasize the research objectives of this section more prominently.\n4.\tThe paper is very well structured and. There are occasional grammar hiccups and typos, so I recommend a light editing pass (below are a few of the mistakes I\u2019ve collected, but there are more).\nPage 5 In this ppaer \u2013>in this paper"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8006/Reviewer_qLTB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651143293,
            "cdate": 1698651143293,
            "tmdate": 1699636986631,
            "mdate": 1699636986631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i62534Z5x8",
                "forum": "LWEqTLCHrw",
                "replyto": "9kProMnf49",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qLTB"
                    },
                    "comment": {
                        "value": "We would like to first express our gratitude for your reviews. Your insightful comments and constructive suggestions are invaluable in enhancing our work. \n\nQ1 & W2:The description of the threat model needs to be more accurate. It is preferable to describe the threat model based on adversary goals, knowledge, and capabilities. \n\nThank you for your constructive insights. Based on your feedback, we modified the description of the threat model as follows:\n\n```\nWe consider an FL system where some clients are malicious, while most clients are honest. The clients have full access to their local data and can train models using their data. They also would like to collaboratively train a model without sharing their data. However, the malicious clients among them may conduct attacks to achieve some adversarial goals, including: i) planting a backdoor to misclassify a specific set of samples while minimally impacting the overall performance of the global model, i.e., backdoor attacks; ii) altering the local models to prevent the global model from converging, i.e., Byzantine attacks; and iii) randomly submitting contrived models without actual training, i.e., free riders. Clients are aware that the server can take some defensive methods to remove potential malicious local models, and they want to verify the mechanism is processed correctly and honestly at the server.\n```\n\nQ2 & W1: The explanations of some experimental results are not entirely convincing. The justification provided for selecting the 'second-to-the-last layer' in Exp1 is not sufficient.\n\nThank you for your suggestion. For the selection of the importance layer, our aim is to choose a general case that is suitable for real-world scenarios that involve various models and datasets. Intuitively, we selected the second-to-the-last layer,  as it contains more information and can represent the whole model. In Experiment 1, we observed that this layer exhibited higher sensitivity compared to most other layers across all scenarios, thus can be utilized to represent the whole model. We acknowledge that our initial presentation for Experiment 1 was unclear. To clarify this, we have revised our descriptions as follows.\n```\nWe utilize the norm of gradients to evaluate the \"sensitivity\" of each layer. A layer with a norm higher than most of the other layers indicates higher sensitivity compared with most of the other layers, thus can be utilized to represent the whole model. We evaluate the sensitivity of layers of CNN, RNN, and ResNet56. The results for RNN are shown in Figure 4, and the results for ResNet56 and CNN are deferred to Figure 14 and Figure 13  in Appendix B.2, respectively. The results show the sensitivity of the second-to-the-last layer is always higher than most of the other layers, which includes adequate information of the whole model, thus can be selected as the importance layer.\n```\n\nQ3: I didn't fully grasp the significance of \"VERIFIABLE ANOMALY DETECTION USING ZKP,\" and the authors should emphasize the research objectives of this section more prominently.\n\nWe appreciate your feedback regarding the motivation of using ZKP. The intuition is that, while most of the clients are benign and wish to collaboratively train machine learning models, they may also have concerns about the server's procedure of removing malicious client models, as this procedure may modify the original aggregation result. Considering this, we deployed a ZKP-based verification procedure for the anomaly detection mechanism so that the benign clients can ensure the correct execution of the mechanism at the server. \n\nBased on your feedback, we included the following description at the beginning of Section 4 (Verifiable Anomaly Detection Using ZKP). We also included corresponding explanations in the introduction.\n```\nThis section introduces a ZKP-based verification procedure so that the benign clients can ensure the correct execution of the anomaly detection mechanism at the server. The intuition is that clients may be skeptical about the removal of some client models during the anomaly detection process at the server, as it may change the aggregation results.\n```\nQ4: The paper is very well structured and. There are occasional grammar hiccups and typos, so I recommend a light editing pass (below are a few of the mistakes I\u2019ve collected, but there are more). Page 5 In this ppaer \u2013>in this paper\n\nThanks for your feedback! We have proofread the paper multiple times and corrected the typos."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456639568,
                "cdate": 1700456639568,
                "tmdate": 1700459076924,
                "mdate": 1700459076924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]