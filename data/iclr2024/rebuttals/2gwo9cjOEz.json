[
    {
        "title": "Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs"
    },
    {
        "review": {
            "id": "OJu9E16Ua2",
            "forum": "2gwo9cjOEz",
            "replyto": "2gwo9cjOEz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6389/Reviewer_bUqG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6389/Reviewer_bUqG"
            ],
            "content": {
                "summary": {
                    "value": "This study offers a theoretical analysis of Graph Neural Networks (GNNs), emphasizing their infinite-width limit behavior through the lens of the Neural Tangent Kernel (NTK). The research illuminates the constancy of the NTK in such scenarios, advancing the understanding of neural network learning dynamics. \n\nFurthermore, the paper explores the implications of varying training intensities across GNN layers, enhancing interpretability in a field often perceived as a 'black box.' The authors validate their theoretical assertions with detailed experiments, utilizing public datasets to ensure reproducibility and credibility.\n\nThis paper bridges complex theoretical insights with practical machine-learning applications, contributing substantially to the discourse on neural network training, optimization, and generalization. Its rigorous approach and novel findings mark a significant stride in understanding and leveraging the full potential of GNNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. By exploring the constancy of the NTK in the infinite-width limit, the study sheds light on complex learning dynamics, enhancing the scientific understanding of how GNNs train and generalize. This rigorous theoretical foundation pushes the boundaries of existing knowledge in neural network behavior.\n2. By dissecting layer-specific training implications, the paper contributes to the interpretability of neural networks, helping researchers to better understand and optimize their GNN models, which is particularly valuable in the 'black box' context of deep learning."
                },
                "weaknesses": {
                    "value": "1. While the paper is strong in theoretical analysis, it may not provide extensive insight into the practical applications of the findings. The implications for real-world scenarios, particularly how these theoretical insights into GNNs and NTK behavior could be harnessed for tangible improvements in specific use cases, might not be thoroughly discussed. \n2. If the experiments were conducted within a narrow set of conditions or datasets, they might not fully represent the complexities of real-world data and scenarios. This limitation could raise questions about the generalizability of the findings and their robustness when applied to diverse, practical challenges in the field of machine learning."
                },
                "questions": {
                    "value": "1. Given the depth of the research, what do the authors see as the next steps or future directions in this domain? Additionally, are there any inherent limitations or challenges in the proposed methods or findings that might need to be addressed in subsequent research?\n2. Can the analysis be extended to deeper (more than 2 layers) GNNs where the aggregation operation is used in internal layers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6389/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6389/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6389/Reviewer_bUqG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6389/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645442932,
            "cdate": 1698645442932,
            "tmdate": 1699636707930,
            "mdate": 1699636707930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hwQTanf6sC",
                "forum": "2gwo9cjOEz",
                "replyto": "OJu9E16Ua2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bUqG"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their appreciation of the key theoretical contributions in our paper. Specific concerns have been addressed below. \n\n\n**Practical applications for specific use-cases in real-world scenarios.** Our experiments originally considered the task of predicting brain functional activity in a future time step based on that in the current time step. This task forms the foundation of the time series forecasting problem in neuroimaging data. Thus, our findings suggest that the choice of cross-covariance graphs in GNNs will result in better prediction performance as compared to the GNNs that rely on the covariance graph. We expanded the scope of our experiments to test this intuition. Specifically, in revised **Appendix H.5**, we have provided a design of a time series prediction model that leverages information from an arbitrary length of data in the past to predict the future. For this specific application, the time series prediction improved by incorporating historical data and more importantly, GNNs with cross-covariance graphs indeed outperformed GNNs with input-based graphs. \n\n\n**Additional datasets.** We have updated the **Appendix H.7** with preliminary results on two other datasets (the PEMS07 traffic flow dataset and an ECG dataset.). For these settings, the cross-covariance based GNN models still retain a performance advantage over the alternatives. \n\nThe limits of our theoretical framework (primarily the fact that the input and output are vectors with the same dimension) certainly impose limits regarding the scope of practical tasks on which we could investigate our theoretical results. In future work, generalizing our theory beyond the setting considered in the paper will let us investigate the generalizability of our findings on a more holistic set of inference problems. \n\n\n**GNNs deeper than 2 layers.** Our current experimental results suggest that our theoretical findings extend to GNNs deeper than two layers (see Fig. 4. that shows the effect of increasing the depth of the model). Even as the depth increases beyond two layers, the cross-covariance based GNNs consistently converge faster and achieve a smaller loss). Additionally, the fact that NTK-based results in the literature are usually generalizable to deeper models makes us optimistic that our results can also be generalized to deeper models. While this was not a focus of the current paper, it is an avenue we aim to explore theoretically in future work.\n\n\n**Future directions, limitations, and challenges.** As discussed previously, extending the theoretical analyses to GNNs deeper than 2 layers is an immediate direction of interest. We expect this analysis to have novel theoretical challenges pertaining to intermediate GNN layers. Furthermore, we will aim to expand the scope of our results for inference tasks beyond the considered setting of the multi-variate regression problem  (for instance, to node-level inference tasks and classification). Another limitation of our theoretical contributions is the lack of thorough analysis of the tightness of the lower bounds on alignment, i.e., the gap between ${\\cal A}$ and ${\\cal A}_L$. \n\nWe hope that we addressed the reviewer's concerns sufficiently, in which case, we would be grateful if your rating of our paper could be re-evaluated. We would be happy to clarify any additional concerns."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189382239,
                "cdate": 1700189382239,
                "tmdate": 1700513091571,
                "mdate": 1700513091571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cvwo40F2WF",
            "forum": "2gwo9cjOEz",
            "replyto": "2gwo9cjOEz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6389/Reviewer_TKzH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6389/Reviewer_TKzH"
            ],
            "content": {
                "summary": {
                    "value": "The authors delve into the theoretical aspects of Neural Tangent Kernels (NTKs) and their influence on the learning and generalization behaviors of over-parameterized neural networks in supervised learning tasks. They introduce the concept of \"alignment\" between the eigenvectors of the NTK kernel and the given data, which appears to play a significant role in governing the rate of convergence of gradient descent and the generalization to unseen data. The paper specifically explores NTKs and alignment in the context of Graph Neural Networks (GNNs). The authors' analysis reveals that optimizing alignment corresponds to optimizing the graph representation or the graph shift operator within a GNN. This investigation leads to the establishment of theoretical guarantees concerning the optimality of certain design choices in GNNs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is well-organized, with a clear delineation of concepts such as NTKs, alignment, and their relevance in the context of GNNs.\n\n* The paper's findings have the potential to advance the understanding and analysis of Graph Neural Networks, providing theoretical insights that could be valuable for the community."
                },
                "weaknesses": {
                    "value": "* The paper seems to miss a crucial related work: Huang, W., Li, Y., Du, W., Yin, J., Da Xu, R. Y., Chen, L., & Zhang, M. (2021). \"Towards deepening graph neural networks: A GNTK-based optimization perspective,\" ICLR 2022. Including and discussing this work could provide a more comprehensive perspective and strengthen the literature review section.\n\n* The theoretical framework primarily relies on the existing NTK theory regarding optimization and generalization. While the authors have cited relevant works, a more distinct and innovative theoretical contribution that extends beyond the current NTK theories would enhance the paper's novelty and impact.\n\n* The paper's discussion on alignment seems closely related to node classification and graph classification tasks. However, there appears to be a lack of relevant examinations or experiments to empirically validate the proposed concepts and theories in these tasks, making it difficult to assess their practical relevance and effectiveness.\n\n* The paper could significantly benefit from a more robust and comprehensive experimental section. Ensuring that the experiments thoroughly validate the theoretical findings, involve extensive comparisons with baseline methods, and are evaluated across various datasets and tasks is essential for demonstrating the approach's practical significance and effectiveness.\n\n* A more detailed and thorough comparison with existing NTK and GNN methods is necessary. The paper should highlight the proposed approach's novelty and advantages, supported by theoretical or empirical evidence, to clearly showcase the contributions and distinguish the work from existing literature."
                },
                "questions": {
                    "value": "* Could the authors elaborate on how the alignment concept is related to node and graph classification tasks? Are there any practical insights or guidelines on how to effectively apply the proposed theories to these tasks?\n\n* Can the authors highlight the novel aspects of their theoretical framework that go beyond the existing Neural Tangent Kernel (NTK) theories? What are the unique contributions that differentiate this work from existing NTK-based studies?\n\n* Are there plans to include more comprehensive experiments to validate the proposed theories and concepts? What datasets, tasks, and baselines are considered for these experiments?\n\n*Will there be experiments conducted specifically to verify the proposed theorems, such as theorem 2 and theorem 3? If so, could you provide insights into how these verifications will be carried out empirically?\n\n* Why the size of $\\mathbf{y}_i$ is $\\mathbb{R}^{n \\times 1}$, given the size of $\\mathbf{x}_i$ is $\\mathbb{R}^{n \\times 1}$? Is there a specific rationale behind this choice of dimensionality?\n\n* Could you elucidate the rationale behind choosing the HCP-YA dataset for your experiments? How does this dataset align with the objectives and hypotheses of your study, especially considering that common node classification or graph classification tasks are typically used in related works?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The ACKNOWLEDGEMENTS on page 13 might violate the anonymity."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6389/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754958868,
            "cdate": 1698754958868,
            "tmdate": 1699636707787,
            "mdate": 1699636707787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7HVoA2l3iG",
                "forum": "2gwo9cjOEz",
                "replyto": "Cvwo40F2WF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TKzH (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments. Specific concerns have been addressed below. \n\n\n**Alignment, NTKs, and GNNs.** We clarify that the notion of alignment (defined as $y^{\\sf T} \\Theta y$ for NTK $\\Theta$ in equation (2) and Definition 1, where $y$ is the output) is a generic property of learning with gradient descent and *not specific to only the GNN architecture or related inference tasks like node classification*. Alignment dictates the convergence of gradient descent (as established in equation (9)). Our objective was to leverage (i) these facts about alignment and gradient descent, and (ii)  specific analytical form of the NTKs for a GNN architecture; to *demonstrate that optimizing alignment for a GNN is equivalent to optimizing the choice of graph shift operator* for the supervised learning task. **To the best of our knowledge, no existing work has motivated the practical choice of graph shift operator in GNNs using either these arguments or the theoretical analyses presented in our paper.** \n\n\n**Same input and output dimensionality.** The input and the output dimensionality were assumed to be the same for analytical tractability. Such settings are of practical relevance in spatio-temporal datasets, where the multiple correlated features evolve over time (for instance, in datasets that describe brain activity, traffic flow, stock prices, etc.). In these settings, the use of past data to predict the future data is a learning task of interest. \n\n\n**Experiments.** Our theoretical results (Theorem 2 and 3) explicitly establish the relationship between the graph shift operator and cross-covariance graphs in the context of optimizing the alignment function. In this respect, our experiments were focused on validating this insight on a real dataset for a suitable practical learning task. \n\n\n**Choice of HCP-YA dataset.** HCP-YA dataset is one of the larger datasets available for brain functional activity in terms of sample size and has been used extensively by the research community. Thus, this dataset was chosen for both its suitability to the considered setting in our theoretical analyses and in the spirit of reproducibility of our findings. \n\n**Validation of theoretical results.** To validate the insights drawn from theoretical results, we considered the inference task of predicting the brain activity at the future using the current time step. This inference task forms the foundation for an algorithm that addresses a typical time series forecasting problem. Here, we compared the performance of GNNs modeled by cross-covariance graphs with GNNs that leveraged only the covariance graph estimated from the input data. As predicted by our theoretical results, the GNNs with cross-covariance graphs indeed outperformed those with covariance graphs. \n\n**Rigor of the Experiments.** We note that we validated these findings across more than 1000 individual subjects in the dataset and a noticeable performance gain can be seen on average and has been reported in our results. The effects of changing different architectural parameters such as the number of layers, number of filter taps and the nonlinearity used in the GNNs were considered in the experiments reported in the appendices, where we observed that the benefit of using cross-covariance graphs remained consistent throughout.  \n\n**Additional Experiments.**  We have further expanded our empirical evaluations (see Appendix H.6) by adding two more datasets and comparing GNNs with cross-covariance graphs with additional baselines.\n\n\n- **Additional baselines.** These additional baselines include a graph based on the Pearson Correlation between the features at different nodes (which is commonly used in tasks pertaining to brain data), and another graph with edges based on the Euclidean distance between node features passed through a Gaussian kernel. (See \u2018Data-driven graph construction and graph learning: A review\u2019 by Qiao et al. for a comprehensive review of many such input-based graph construction methods). We observed that in general, the models with graphs constructed using only the input features with both linear and non-linear affinities exhibit similar performance and the models using the cross-covariance, consistently outperform the input-based graphs.\n\n\n- **More datasets.** To ensure that these results don\u2019t just pertain to one particular dataset, we have updated the Appendix (H7) with preliminary results on two other datasets (the PEMS07 traffic flow dataset and an ECG dataset.). While the performance gap between the different models isn\u2019t as pronounced in these other datasets compared to HCP, the cross-covariance based models still retain an advantage."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189429790,
                "cdate": 1700189429790,
                "tmdate": 1700189429790,
                "mdate": 1700189429790,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VnwP3i7r7r",
                "forum": "2gwo9cjOEz",
                "replyto": "Cvwo40F2WF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TKzH (2/2)"
                    },
                    "comment": {
                        "value": "**Missing literature.** We thank the reviewer for suggesting the work in Huang et. al, 2021 as a relevant study to be discussed in our paper. We note that this study leverages the analyses of NTKs to investigate the impact of number of layers in GNNs on their inference. Broadly, this work leverages NTK of a GNN to motivate various architectural choices and in this respect, is similar in spirit to our work. We have discussed such works in our literature review and will add this paper to the discussion as well. However, we firmly believe that the analytical approaches and motivations relevant to our work and those in Huang et. al, 2021 are very distinct and do not overlap in scope. \n\n\n**Node classification task.**  While the extension of the definition of Alignment to tasks like node classification is straightforward, generalizing our results to such cases is not trivial and attempting to do so would vastly expand the scope of the paper. We have therefore left attempting such an extension for future work. \n\n\n**Ethical concern.** We have provided the acknowledgement for HCP-YA dataset as per the data use agreement. The authors are not among the PI names listed there and have no collaboration with them for this work. \n\n\nWe hope that we addressed the reviewer's concerns sufficiently, in which case, we would be grateful if your rating of our paper could be re-evaluated. We would be happy to clarify any additional concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189480921,
                "cdate": 1700189480921,
                "tmdate": 1700512941073,
                "mdate": 1700512941073,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PTK1kL4GQ4",
            "forum": "2gwo9cjOEz",
            "replyto": "2gwo9cjOEz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6389/Reviewer_GUVA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6389/Reviewer_GUVA"
            ],
            "content": {
                "summary": {
                    "value": "This paper draws on the idea that NTK based generalization is based on the associations of eigenvectors of the NTK with the data. Drawing on this alignment in the case of a GNN is used to derive the graph shift operator (i.e. equivalent of a graph laplacian) that is different from the input graph. To do this they solve an optimization to derive the graph as being the cross-covariance matrix of the input with the output."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Generalizing the NTK to GNNs expands the theory associated with this area, and the suggestion of using a cross covariance matrix involves a graph that uses both input and output variables as nodes, which is not commonly done in current GNNs. The theory could be useful in cases where the graph is not given but constructed as an affinity matrix from data as well."
                },
                "weaknesses": {
                    "value": "The key weakness is that empirically VNNs and graphs that are based on covariance matrices rather than non-linear affinities have fared worse in practice. This may be an instance of the NTK not explaining the entire behavior of neural networks. Moreover the experimental validations seem fairy limited without comparison to GCNs and Graphormers and other modern graph neural network architecture."
                },
                "questions": {
                    "value": "Is this a case of the theory not explaining the entire empirical phenomenon? Can further experiments be performed on a variety of kernels to see what works better in practice on common datasts/"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6389/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801981132,
            "cdate": 1698801981132,
            "tmdate": 1699636707654,
            "mdate": 1699636707654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xS19YHtcd4",
                "forum": "2gwo9cjOEz",
                "replyto": "PTK1kL4GQ4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6389/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6389/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GUVA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments. Specific concerns are addressed below. \n\n\n**Theoretical analysis.** Here, we emphasize that *our theory has a bottom-up characteristic*, where we have begun with the analysis of a simple graph convolutional filter$^1$ as the predictor. Due to inherent linearities in the graph filter, the NTK for a graph filter has, by default, a constant behavior with respect to learnable parameters. The analysis of alignment using NTK in this setting leads naturally to the conclusion of cross-covariance graphs being the optimal choice. Our subsequent analyses adds various different complexities, such as, non-linearities and extension to two layers for a GNN in practice. For the analysis of the GNN model, we operate under the standard assumptions of infinite width associated with NTKs and obtain conclusions similar to that for the simple graph filter setting. Therefore, due to the bottom up characteristic of our analysis and consistency of the conclusions for graph filters and GNNs, we do not believe that our theory overlooked any missing phenomenon that could have led to a drastically different choice of kernel for the considered regression task. \n\n$^1$ *This graph convolutional filter is the fundamental information processing block in the general GNN (or specifically, GCN) architecture considered in our paper.* \n\n**Cross-covariance graphs versus graphs with non-linear affinities.** Regarding the concern that graphs with non-linear affinities may outperform covariance-based graphs, we have performed additional experiments. We refer the reviewer to Appendix H.6 in the revised paper, where we have added the performance comparison between models with different graphs. These include a graph based on the Pearson Correlation between the features at different nodes (which is commonly used in tasks pertaining to brain data), and another graph with edges based on the Euclidean distance between node features passed through a Gaussian kernel. (See \u2018Data-driven graph construction and graph learning: A review\u2019 by Qiao et al. for a comprehensive review of many such input-based graph construction methods). We observed that in general, the models with graphs constructed using only the input features with both linear and non-linear affinities exhibit similar performance and the models using the cross-covariance consistently outperform the input-based graphs. As another baseline, we have also added comparison of  the performance of our models to a Fully-connected Neural Network (FCNN; which has roughly 100 times more number of learnable parameters as compared with GNN models) and we observe that the two-layer GNN with cross-covariance graph exhibits comparable performance to the FCNN.\n\n\n**Choice of GNN architecture.** We clarify that our analysis focuses on graph convolutional networks (GCNs) among the different variants of GNNs that exist in the literature. Using this setting, we have demonstrated how to leverage the theory-inspired insights into the construction for a graph from the data.  Hence, our experiments have focused on the setting consistent with that studied in the theoretical analysis to validate our results. We conjecture that a similar performance advantage will appear when using the cross-covariance matrix as a graph in more complex graph-based architectures. Expanding our theory and experiments to show the advantage of using cross-covariance in more complex models is certainly a direction that we will pursue in future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6389/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189395542,
                "cdate": 1700189395542,
                "tmdate": 1700675524414,
                "mdate": 1700675524414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]