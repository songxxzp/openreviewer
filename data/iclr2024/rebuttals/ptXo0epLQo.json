[
    {
        "title": "$\\alpha$TC-VAE: On the relationship between Disentanglement and Diversity"
    },
    {
        "review": {
            "id": "DfPb7MjemU",
            "forum": "ptXo0epLQo",
            "replyto": "ptXo0epLQo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3812/Reviewer_4hmE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3812/Reviewer_4hmE"
            ],
            "content": {
                "summary": {
                    "value": "From the perspective of information theory, this paper decomposes the Total Correlation term into two distinct bounds: the information bottleneck and the conditional entropy bottleneck. It assigns specific weights to these bounds to balance their influence. Furthermore, the authors evaluate their learning objective across multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- From an information theory perspective, the authors derive a lower bound of total correlation. They simultaneously address both the disentanglement and informativeness of the latent variable.\n- The writing throughout the paper is lucid and well-structured, making it accessible for readers.\n- The research showcases effectiveness by employing diverse evaluation metrics and testing their approach across various datasets."
                },
                "weaknesses": {
                    "value": "- The novelty is limited. Earlier research, referenced by the authors, already explored the decomposition of Total Correlation. The primary contribution seems to be the weighted combination of these two decompositions, which might appear incremental.\n- They claim that the VIB term promotes compression of the latent representation, and the CEB term promotes balance between the information contained in each latent dimension. However, the paper lacks a clear illustration of:\n  - How the information bottleneck correlates with disentanglement.\n  - The connection between diversity and the conditional entropy bottleneck.\n- A more profound point of contention is the origin of the VIB and the CEB terms. Both terms are derived from the same TC term, yet they supposedly represent different characteristics. The paper doesn't sufficiently elucidate why this is the case, which could have made for a compelling discussion."
                },
                "questions": {
                    "value": "- How does the parameter alpha influence the results?\n- What about the traversal outcomes for the CelebA dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Reviewer_4hmE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3812/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698398094778,
            "cdate": 1698398094778,
            "tmdate": 1700723767235,
            "mdate": 1700723767235,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OIwRALEh0q",
                "forum": "ptXo0epLQo",
                "replyto": "DfPb7MjemU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Weakness 1"
                    },
                    "comment": {
                        "value": "Q3.1: The novelty is limited. Earlier research, referenced by the authors, already explored the decomposition of Total Correlation. The primary contribution seems to be the weighted combination of these two decompositions, which might appear incremental.\n\nA3.1: Although the decomposition of Total Correlation has been already explored and the final loss is incremental with respect to Beta-VAE loss, no convex lower bounds that can be computed without using extra networks have been proposed so far for the Total Correlation. Not only the proposed bound is grounded on information theory constructs (VIB and CEB terms), but since it doesn\u2019t require the usage of other networks, as in FactorVAE (H. Kim & A. Mnih, 2019) where a discriminator network is required, it can easily be applied on top of other models. \n\nMoreover, while beta-TCVAE (R. Chen et al. 2019) uses a Total Correlation bound as well, they make use of sampling-based strategies to compute the mutual information component used in their loss. However, such sampling-based strategies have several drawbacks. One of the primary disadvantages is that the accuracy of entropy estimation heavily relies on the size of the sample. When the sample size is too small, especially in high-dimensional spaces, it may not represent the true distribution of the data well. This leads to biased or inaccurate estimates of entropy. Furthermore, the accuracy of entropy estimates can be significantly influenced by the sampling method used. If the sampling method introduces biases or fails to capture certain aspects of the underlying distribution, the entropy estimate will be affected. Finally, in high-dimensional spaces, the amount of data required to accurately estimate entropy grows exponentially. This is known as the curse of dimensionality. Sampling-based methods struggle in these scenarios as obtaining a representative sample that captures all the nuances of a high-dimensional distribution is extremely challenging. We would like to clarify that in our case no sampling is required to compute the TC bound, overcoming all the limitations related to sampling-based approaches."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241531986,
                "cdate": 1700241531986,
                "tmdate": 1700241531986,
                "mdate": 1700241531986,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wJVGce11MY",
                "forum": "ptXo0epLQo",
                "replyto": "DfPb7MjemU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Weakness 2"
                    },
                    "comment": {
                        "value": "Q3.2: They claim that the VIB term promotes compression of the latent representation, and the CEB term promotes balance between the information contained in each latent dimension. However, the paper lacks a clear illustration of:\nHow the information bottleneck correlates with disentanglement.\nThe connection between diversity and the conditional entropy bottleneck.\nA3.2:\n\nRELATIONSHIP BETWEEN IB AND Disentanglement\n\nDisentanglement in VAEs, following Higgings\u2019 \u03b2-VAE framework, seeks to learn representations where individual latent variables capture distinct, independent factors of variation in the data. This is achieved by modifying the traditional VAE objective to apply a stronger constraint on the latent space information bottleneck, controlled by a hyperparameter \u03b2. The \u03b2-VAE, introduced by Higgins\net al. (2016), represents a seminal approach to disentanglement, promoting the learning of factorized and interpretable latent representations.\nOn a related front, the Variational Information Bottleneck (VIB) method, formulated by Alemi et al. (2017), extends the Information Bottleneck principle to deep learning. The VIB approach seeks to find an optimal trade-off between the compression of input data and the preservation of relevant information for prediction tasks. By employing a variational approximation, VIB efficiently learns compressed representations that are predictive of desired outcomes. Interestingly, Alemi formulates a VIB objective that is equivalent to Higgins\u2019 \u03b2-VAE one. Such a result makes clear the relationship between Information Bottleneck and Disentanglement, concluding that imposing a higher information bottleneck leads to higher disentanglement.\n\n\n\nRELATIONSHIP BETWEEN CEB AND DIVERSITY\n\nFISHER\u2019S DEFINITION OF CONDITIONAL ENTROPY BOTTLENECK\nFisher\u2019s approach to the Conditional Entropy Bottleneck Fischer & Alemi (2020) is an extension\nof the Information Bottleneck (IB) principle Alemi et al. (2017), aimed at finding an optimally\ncompressed representation of a variable X that remains highly informative about another variable\nY , under the influence of a conditioning variable Z. The CEB objective, according to Fisher, is\nformalized as a trade-off between two competing conditional mutual information terms:\n\nmin_q(z|x) [I(X; Z|C) \u2212 \u03b2I(Y ; Z|C)]\n\nHere, I(X; Z|C) quantifies the amount of information that the representation Z shares with X,\nconditioned on C. Simultaneously, I(Y ; Z|C) measures how much information Z retains about Y ,\nalso under the condition of C. The parameter \u03b2 serves as a crucial tuning parameter, balancing these two aspects.\n\n\nADAPTING CEB TO VAES WITHOUT CONDITIONING VARIABLES\nIn the realm of Variational Autoencoders, where the training strategy is to reconstruct the input\ndata X using a latent representation Z without any external conditioning C, the CEB framework\nundergoes a significant simplification. Given that X = Y in a typical VAE setup, the CEB objective\nreduces to a form where the focus shifts to optimizing the mutual information between X and it's\nlatent representation Z:\n\nmin_q(z|x) [(1 \u2212 \u03b2)I(X; Z)]\n\nThis objective can be further broken down as (1 \u2212 \u03b2)(H(X) \u2212 H(X|Z)), where H(X) represents\nthe entropy of the input data, and H(X|Z) is the conditional entropy of the input given its latent\nrepresentation. This formulation underscores the trade-off between compressing the input data in\nthe latent space and retaining essential information for accurate reconstruction.\n\nOn the relationship between Diversity and CEB\nFollowing Friedman & Dieng (2022), Diversity can be quantitatively expressed as the exponential\nof the entropy of the latent space distribution q(Z|X):\nDiversity = exp(H(Z|X))\nTo understand how the CEB framework relates to this notion of diversity, we utilize the entropy\nchain rule H(Y |X) = H(X, Y ) \u2212 H(X) , which allows to decompose H(X|Z) in terms of the\njoint entropy H(X, Z) and the conditional entropy H(Z). Consequently, the CEB objective evolves\ninto a more comprehensive form that explicitly accounts for the diversity of the latent space:\n\nmin_q(z|x) [(1 \u2212 \u03b2)(H(X) \u2212 H(X, Z) + H(Z))]\n\nmin_q(z|x) [(1 \u2212 \u03b2)(\u2212H(Z|X) + H(Z))]\n\nThe latter one, makes clear the connection between the CEB term and Diversity as defined in Fried-\nman & Dieng (2022). Indeed, we can see that when minimizing the CEB term the Diversity term is\nMaximized."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241601522,
                "cdate": 1700241601522,
                "tmdate": 1700241601522,
                "mdate": 1700241601522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "35Kc6AaW0R",
                "forum": "ptXo0epLQo",
                "replyto": "DfPb7MjemU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Weakness 3"
                    },
                    "comment": {
                        "value": "A3.3:\nAppendix A.1 presents the derivation of the proposed TC bound. In order to derive it, we used the same trick - making explicit different quantities from the starting TC expression and upper bounding differently the two expressions - such an approach produces two different upper bounds and two related gaps that will be minimized when optimizing the bound. In appendix A.1 we write: \n\n\u201cMaximizing L(z, x) not only maximizes the original objective TC(z, x), but at the same time\nminimize the gap produced by upper bounding equation 10. As a result, \u2211k=1[Eq\u03b8 (zk )[DKL(q\u03b8 (x|zk)\u2225p\u03c6(x|zk))]] + Eq\u03b8 (x|z)[[DKL(q\u03b8 (z)\u2225r(z))], (11)\nwill be minimized, leading to: r(z) \u2248 q\u03b8 (z) and p\u03c6(x|zk) \u2248 q\u03b8 (x|zk).\u201d\n\n\u201cMaximizing Eq. 18 not only maximizes the original objective T C(z, x) but at the same time min-\nimizes the gap produced by upper bounding Eq. equation 17, leading to: rp(zk|x) \u2248 q\u03b8 (zk|x),\nq\u03b8 (z\u0338=k) \u2248 rp(z\u0338=k|x) and q\u03b8 (x|zk) \u2248 p\u03c6(x|zk)\u201d\n\nHence, it is important to clarify that the two terms are substantially different because of 1) the expressions used to define the TC and 2) the components dropped to upper bound the Total Correlation.  The gap minimized in the two terms is different, which is why the two bounds end up being different as well."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241697559,
                "cdate": 1700241697559,
                "tmdate": 1700241697559,
                "mdate": 1700241697559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vtkUgbVMaA",
                "forum": "ptXo0epLQo",
                "replyto": "DfPb7MjemU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Q2"
                    },
                    "comment": {
                        "value": "Q3.4: What about the traversal outcomes for the CelebA dataset?\n\nA3.4: We did not add CelebA traversals because we used 48 dimensional latent space, which makes it a bit impractical producing the traversals and including them in the paper.  Here you can find a link to an anonymous drive that contains the related latent traversals: \nhttps://drive.google.com/file/d/1V-9bM70z0YwrzpRpiqP8rHk8GGXtx8NY/view?usp=sharing . The presented traversals show more than 15 uncovered generative factors ( which is the highest number of uncovered generative factors in the literature (R. Chen et al. 2019)  for the Celeba dataset). Moreover some of the uncovered generative factors are not present in the training set, for instance the background color (18th row of the traversal)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241779780,
                "cdate": 1700241779780,
                "tmdate": 1700241779780,
                "mdate": 1700241779780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xUTyTa4pXR",
                "forum": "ptXo0epLQo",
                "replyto": "DfPb7MjemU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the outlined weaknesses. We were aware of some of them, which is why we prepared some of the answers beforehand (i.e., relationship between CEB and Diversity). We are currently preparing the plots for some experiments we perform with the goal of clarifying how alpha influences the results. We hope that together with the previous questions, these answers can lead the reviewer to increase the grade of the review. If there is any other question or point that the reviewer would like to expand on, we are more than available on discussing and answering any kind of question. \n\nThank you again for your availability!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242005095,
                "cdate": 1700242005095,
                "tmdate": 1700242005095,
                "mdate": 1700242005095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B6fbymxjoS",
                "forum": "ptXo0epLQo",
                "replyto": "35Kc6AaW0R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Reviewer_4hmE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Reviewer_4hmE"
                ],
                "content": {
                    "comment": {
                        "value": "Both of these upper bounds are derived from the following term:$ TC_{\\theta}({z},{x}) = \\sum_{k=1}^{K}I_\\theta (z_k, x) - I_\\theta (z, x) $. As stated by the authors in the text:\n   - VIB makes $ r(z) \\approx q_\\theta (z) $ and $p_\\phi(x|z_k) \\approx q_\\theta (x|z_k) $,\n   - CEB results in: $r_p(z_k|x) \\approx q_\\theta (z_k|x)$, $q_\\theta (\\bar{z}=k) \\approx r_p(\\bar{z}=k|x) $and$q_\\theta (x|z_k) \\approx p_\\phi(x|z_k)$.\n\nThe VIB and CEB upper bounds are derived from the same underlying term and share similarities in their optimization outcomes. These shared outcomes imply that effectively optimizing these bounds requires a deep understanding of their interaction and the distinct characteristics each represents with respect to TC. For instance, this may involve sensitive analysis of hyperparameters like $\\alpha$, which plays a critical role in balancing the trade-offs inherent in these models."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534048462,
                "cdate": 1700534048462,
                "tmdate": 1700534048462,
                "mdate": 1700534048462,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9smsCAS4Er",
                "forum": "ptXo0epLQo",
                "replyto": "cnJ76APWbm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Reviewer_4hmE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Reviewer_4hmE"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for your response. My comments have been addressed, so I'll bump up the score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723750335,
                "cdate": 1700723750335,
                "tmdate": 1700723750335,
                "mdate": 1700723750335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cT5mrWm4mc",
            "forum": "ptXo0epLQo",
            "replyto": "ptXo0epLQo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3812/Reviewer_hWoC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3812/Reviewer_hWoC"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author introduce \u03b1-TCVAE, a variational autoencoder optimized using a new total correlation (TC) lower bound that both maximizes disentanglement and latent variables informativeness. The new TC bound is grounded in information theory and can be reduced to a convex combination of the known VIB and CEB terms. Moreover, the paper also presents quantitative analyses and correlation studies which support the idea that smaller latent domains lead to better generative capabilities and diversity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The total correlation between x and z has not been previously used in disentangled representation learning, although similar concepts exist, such as mutual information in InfoGAN.\n- The paper  first paper propose to discuss diversity in the context of disentangled representation learning, drawing from previous research in GANs (e.g., FID).\n- The paper considers reinforcement learning as a downstream task for disentangled representations."
                },
                "weaknesses": {
                    "value": "- The paper proposes maximizing an upper bound of the correlation between x and z for disentanglement, but \u03b2-TCVAE penalizes total correlation between different z's. The connection between these two approaches is unclear, and the use of \u03b1-TCVAE may be misleading. If there is a connection, it should be further clarified.\n- The authors should explain why maximizing the upper bound of the correlation between x and z promotes disentanglement, and why increasing variable informativeness enhances diversity.\n- The traversal results shown in Appendix Figure 12 are poor in terms of both disentanglement and generation quality. Additionally, the paper lacks traversal images results for real world natural images (not MPI3D-real), such as CelebA.\n- The paper should provide an analysis of the sensitivity of the $\\alpha$ parameter.\n- If the answer to weakness 1 is the latter, it seems that this work is relatively incremental, as it only adds a conditional TC term to the \u03b2-TCVAE. Considering this point, the authors should clarify it in the rebuttal stage."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Reviewer_hWoC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3812/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589477267,
            "cdate": 1698589477267,
            "tmdate": 1699636338548,
            "mdate": 1699636338548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZPSHvXwq79",
                "forum": "ptXo0epLQo",
                "replyto": "cT5mrWm4mc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback. We updated the manuscript and particularly the appendix to include the material related to these questions. \n\nQ1: The paper proposes maximizing an upper bound of the correlation between x and z for disentanglement, but \u03b2-TCVAE penalizes total correlation between different z's. The connection between these two approaches is unclear, and the use of \u03b1-TCVAE may be misleading. If there is a connection, it should be further clarified.\n\nA1: The connection between \u03b2-TCVAE and \u03b1-TCVAE can be explained looking at Equation 2 in the paper:  $TC_{\\theta}(\\boldsymbol{z}, \\boldsymbol{x}) = TC_{\\theta}(\\boldsymbol{z}) - TC_{\\theta} (\\boldsymbol{z} | \\boldsymbol{x})$.\nThere are 2 main differences between \u03b2-TCVAE and \u03b1-TCVAE: \n1) As highlighted by the reviewer, while \u03b2-TCVAE  tries to optimize $TC_{\\theta}(\\boldsymbol{z})$, we upper bound  $TC_{\\theta}(\\boldsymbol{z}, \\boldsymbol{x})$ which also takes into account the mutual information shared between latent representation and input space. Indeed, one of the main problems of most of disentangled models is trying to make as much independent as possible the latent representations without taking into account the information that flows from the input space to the latent representation. By contrast, upper bounding  $TC_{\\theta}(\\boldsymbol{z}, \\boldsymbol{x})$ allows our model to better represents and discover more factors of variations in  complex datasets (e.g., MPI3D_Real and CelebA). For instance, while \u03b2-TCVAE discovers 15 generative factors in CelebA, we are able to count 25 distinct generative factors in  \u03b1-TCVAE (here there is an anonymous link with the generated traversals: https://drive.google.com/file/d/1V-9bM70z0YwrzpRpiqP8rHk8GGXtx8NY/view?usp=sharing ). \n\n2) While \u03b2-TCVAE uses a Monte Carlo sampling approach to compute the mutual information term used in \u03b2-TCVAE loss function, we do not need to use any sampling strategy to compute the derived bound. However, such sampling-based strategies have several drawbacks. One of the primary disadvantages is that the accuracy of entropy estimation heavily relies on the size of the sample. When the sample size is too small, especially in high-dimensional spaces, it may not represent the true distribution of the data well. This leads to biased or inaccurate estimates of entropy. Furthermore, the accuracy of entropy estimates can be significantly influenced by the sampling method used. If the sampling method introduces biases or fails to capture certain aspects of the underlying distribution, the entropy estimate will be affected. Finally, in high-dimensional spaces, the amount of data required to accurately estimate entropy grows exponentially. This is known as the curse of dimensionality. Sampling-based methods struggle in these scenarios as obtaining a representative sample that captures all the nuances of a high-dimensional distribution is extremely challenging. We would like to clarify that in our case no sampling is required to compute the TC bound, overcoming all the limitations related to sampling-based approaches."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682061190,
                "cdate": 1700682061190,
                "tmdate": 1700682061190,
                "mdate": 1700682061190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xeMtp0gT9A",
                "forum": "ptXo0epLQo",
                "replyto": "cT5mrWm4mc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q2: The authors should explain why maximizing the upper bound of the correlation between x and z promotes disentanglement, and why increasing variable informativeness enhances diversity. \n\nWe added a section in the Appendix which explains and formally defines the connection between the CEB term and diversity, and the VIB term and disentanglement. \n\nQ3: The traversal results shown in Appendix Figure 12 are poor in terms of both disentanglement and generation quality. Additionally, the paper lacks traversal images results for real world natural images (not MPI3D-real), such as CelebA.\n\nHere there is an anonymous link with the generated traversals for CelebA: https://drive.google.com/file/d/1V-9bM70z0YwrzpRpiqP8rHk8GGXtx8NY/view?usp=sharing . The reason why we did not include them is that since we are using a latent space with 48 dimensions, fitting the image in one page is very unhandy and leads to a very poor image quality. \n\n\nQ4: The paper should provide an analysis of the sensitivity of the parameter.\n\nWe added a section in the appendix with the sensitivity analysis of the alpha parameter. \n\nQ5 is explained in the answer we provided for Q1. \n\nWe thank the reviewer for the provided feedback. We hope that the sections added in the appendix fully answer the provided feedback and address the highlighted weaknesses, leading to an increase of the rating. We are open to discuss for any further question."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682526037,
                "cdate": 1700682526037,
                "tmdate": 1700682526037,
                "mdate": 1700682526037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XfmWmqP1DF",
                "forum": "ptXo0epLQo",
                "replyto": "cT5mrWm4mc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Reviewer_hWoC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Reviewer_hWoC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank yor for your response. Some of the concerns are addressed. However, based on your response, I still have the follow two questions:\n1. What is the difference in KL divergence used in VIB and VAE?\n2. If I understand correctly, CEB is a diversity regularization term, and the disentangling term is VIB. If there is no essential difference in KL divergence in $\\beta$-VAE between VIB , then \\alpha-TCVAE is just a modifiication of $\\beta$-VAE by adding a diversity regularization term on loss function?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710116776,
                "cdate": 1700710116776,
                "tmdate": 1700710214740,
                "mdate": 1700710214740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "27253aSOSj",
            "forum": "ptXo0epLQo",
            "replyto": "ptXo0epLQo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3812/Reviewer_wnca"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3812/Reviewer_wnca"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for learning disentangled representations, alpha-TCVAE. \nIt is based on a new lower bound for TC loss, that convexly combines VIB and CEB terms.\nFor alpha = 0, the alpha-TCVAE reduces to beta-VAE.\nExperiments show that alpha-TCVAE brings some improvement in disentanglement quality measures and diversity of generated samples. \nComparisons with beta-TCVAE, beta-TCVAE, FactorVAE, beta-VAE+HFS, VAE and StyleGAN are provided.\nAn additional experiment shows usefullness of alpha-TCVAE for reinforcement learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A new approximation to the TC loss is provided.\n2. In Appendix E, authors show that \u03b1-TCVAE can learn and represent generative factors that are not present in the ground-truth dataset. This is a very surprising observation, probably authors shoud consider making it more visible by moving it to the main part of the manuscript.\n3. Have you used augmentations during training? Because new factors of variations (like camera elevation) mights be side effect of some augmentations like crop. \n4. A comparison with the recent model \u03b2-VAE+HFS is provided.\n5. The paper contains interesting insights about connections between disentanglement and diversity.\n6. An application to RL is provided.\n7. The paper is well written and easy to follow, the language is fine."
                },
                "weaknesses": {
                    "value": "1. The improvements over other VAE-based methods are very moderate, sometimes lower than std.\n2. The popular dSprites dataset is missing; also popular measures like MIG, DCI-C, DCI-I are missing.\n3. The term I\u03b8(z, x) is not defined in Section 3.\n4. The experimental setting, when a diversity is evaluated by doing traversals by +-6,8,10 std. looks strange for me.\nSuch points will be never sampled from the noise. \n5. Where did hyperparameters from Appendix B come from? For example, Roth et al, 2023 provided a grid search for them. As far as rival methods are concerned, you can take hyperparameters from original papers (but in this case original training pipeline and architectures must be also used). If the hyperparameters for rival methods are selected arbitrary, experimental results are not valid!\n6. Figure 1 is not convincing. For example, row 3 for alpha-TCVAE, contains an image with a defect: non-round violet border. \nHigh diversity obviously can be achieved by generating wrong images."
                },
                "questions": {
                    "value": "1. In Figure 2, what does green horizontal line mean?\n2. In Figure 3, you write that \"The scores for the images of our model, \u03b1-TCVAE, are consistently better than baseline VAE models (lower FID is better), and only slightly worse than StyleGAN.\"\nBut from Figure 3, I see that blue circles (sampling from \"noise\") for all VAE-based models have significantly higher FID than StyleGAN. \nProbably, you compare traversal from VAE and \"noise\" samples from StyleGAN, but it seems not fair to me.\n3. How the matrix for the Vendi score is formed?\n4. FID from \"traversals\" image generation is much lower than from straightforward generation from noise. Do you have an explanation?\nAlso, it is interesting to compare VAE models via Precision/Recall (Kynk\u00e4\u00e4nniemi, T., Karras, T., Laine, S., Lehtinen, J., & Aila, T. (2019). Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32.)\n\n**post rebuttal**. Most of my questions have been addressed, authors even did some additional computations. So, I am raising my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3812/Reviewer_wnca"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3812/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754821554,
            "cdate": 1698754821554,
            "tmdate": 1700839653954,
            "mdate": 1700839653954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vJWBStqKyp",
                "forum": "ptXo0epLQo",
                "replyto": "27253aSOSj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, \n\nThank you for your feedback, we tried to address all the points stated in the review. We hope that these answers can help the reviewer on elucidating the reviewer on some confusing parts. \nQ1: In Figure 2, what does green horizontal line mean?\nA1: The green horizontal line represents the diversity score computed using the ground truth images of the dataset. It is important to note that Table 2 in (Friedman et al, 2022) highlights how all the generative models they considered are not able to achieve a higher diversity score than the one obtained using the original dataset. By contrast, using disentangled models and for the considered datasets, this is not the case.  \n\nA2: Question 2 raises a compelling point. While it's acknowledged that Style-GAN and VAEs employ distinct sampling methods, our comparison of VAE models and Style-Gan isn't intended to establish superiority of one over the other. The inclusion of a GAN-based model in our study serves a specific purpose: to demonstrate that using traversal sampling techniques can yield comparable or marginally lower visual fidelity (as measured by FID) and similar or slightly enhanced diversity (as indicated by Vendi). This finding is significant because GANs typically present greater challenges in training, including instability issues, and demand more computational resources and time. Additionally, GANs are not suitable for inferential purposes, highlighting the appeal of VAE-based models when inference capabilities are necessary.\n\nOur assessment aimed at fairness, considering the distinct training goals of GANs and VAEs. GANs are trained to sample from white noise, whereas VAEs reconstruct images from a structured latent space, sampling latents from specific points during each forward pass. Our balanced approach in sampling strategies was an effort to ensure impartiality towards both GANs and VAE-based models. Sampling with white noise inherently disadvantages VAE-based models, as it presupposes a latent space defined by a multidimensional standard Gaussian \u2013 a characteristic typical of GANs. Conversely, if one were to sample images from the VAE latent space using Style-GAN model, Style-GAN would likely exhibit poor performance as well.\n\nIn summary, the reviewer's observation about the inherent unfairness in directly comparing results from these two sampling strategies is valid. However, our approach of presenting both methodologies aimed to provide an equitable evaluation of both model types."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679301833,
                "cdate": 1700679301833,
                "tmdate": 1700679301833,
                "mdate": 1700679301833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TaNGKGcVBp",
                "forum": "ptXo0epLQo",
                "replyto": "27253aSOSj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "A3: The i,jth entry of the matrix for the Vendi score is computed as the cosine similarity between the InceptionNet feature vectors for the ith and jth images. The use of InceptionNet to extract features is specified on the top of page 6. We will add that the distance function is defined as cosine similarity. \n\nA4: This is a very interesting question. The reason why FID from 'traversals\" is much lower is that using the traversals to generate images is actually the correct way of sampling images when using a VAE model. The idea is that while GAN models don't have a latent space and are trained to sample from white noise, VAE-based models have a defined latent space, which is a specific manifold with its own topology. In other words, when sampling using VAE-based models we need to be sure that the sample relies on the latent space, otherwise the image generation will not be meaningful or will be out-of-distribution (i.e., very bad FID scores).  To ensure that the samples relies on the VAE latent space, we used traversals sampling, which means that we first select a point in the latent space manifold (e.g., which basically is the encoded representation) and then we sample points close to the selected one. As a result, we make sure that the samples rely on the VAE latent space and can be used to generate meaningful images and, hence, much lower FID scores.  \n\nRegarding the use of Precision and Recall as metrics, the reason why we did not use them in the first place is that, as stated in Friedman et al. 2022 -  \"Compared to these approaches (i.e., Precision and Recall as metrics), the Vendi Score is a reference-free metric, measuring the intrinsic diversity of a set rather than the relationship to a reference distribution. This means that the Vendi Score should be used along side a quality metric, but can be applied in settings where there is no reference distribution.\" -  In other words, we were not sure of which one should be the reference distribution in our case and instead we presented Vendi Score next to FID. In these days we tried to see if we could compute them and present the results on time, but the other reviewers were interested on the sensitivity analyses of alpha, so for the sake of time we focused on that rather than on trying to understand how to compute these extra metrics. If computing them is possible, we will add these metrics on the arxiv version upon publication.  You can find the Alpha sensitivity analyses in the appendix of the updated paper. \n\nWe hope that these answers, together with the new section in the appendix (Sensitivity Analyses, Relationship between CEB and Diversity,  and MIG score comparisons) can lead the reviewer to increase the grades. Thank you so much for your availability and the useful feedback."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680650011,
                "cdate": 1700680650011,
                "tmdate": 1700680650011,
                "mdate": 1700680650011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q7hFLkX41Z",
                "forum": "ptXo0epLQo",
                "replyto": "27253aSOSj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3812/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments to Weaknesses"
                    },
                    "comment": {
                        "value": "W2: \nWe added MIG, DCI-C, DCI-I and DCI-D in the appendix. The presents a similar trend as the one showed by the overall DCI reported in the main paper, which is computed as geometric mean of the three DCI scores, as in Rotman et al, 2022. \n\nW3: \nI\u03b8(z, x) is the mutual information between z and x, we updated the main paper and added the definition. \n\nW4: \nThe reason why we used only 6, 8 and 10 std is because we were running out of time and using only 3 values was faster to compute the scores. Moreover, back then we noticed that it didn't make a lot of difference in terms of results between using 1,2,4,6,8,10 and using only 6,8 and 10. We reevaluated both Vendi and FID using 1,2,4,6,8 and 10. We updated the manuscript with the new plots. As expected, the new plots do not differ that much between the one presented before. \n\nW5: \nWe used the folder from Roth et al. 2023 as main codebase to run all of our experiments. For every model we used the hyperparameters used by default in their folder. Although we do not present a grid search for all the hyperparameters of all models, we believe that using the default hyperparameters from Roth codebase, which are also the one that performs the best according their gridsearch, should be a correct and fair approach. \n\nW6: \nIt is true that diversity can be achieved generating wrong images, which is why we present Vendi Scores next to FID scores, that contextualize the provided Vendi Scores and makes sure that high values of Diversity are not obtained because the model generates wrong images."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3812/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684662151,
                "cdate": 1700684662151,
                "tmdate": 1700684662151,
                "mdate": 1700684662151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]