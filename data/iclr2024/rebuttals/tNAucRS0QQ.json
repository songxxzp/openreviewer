[
    {
        "title": "General-purpose Pre-trained Model Towards Cross-domain Molecule Learning"
    },
    {
        "review": {
            "id": "bdriYvFbdq",
            "forum": "tNAucRS0QQ",
            "replyto": "tNAucRS0QQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission942/Reviewer_x6ZB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission942/Reviewer_x6ZB"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a general protein pre-training model, i.e., Biomolecular Interaction Transformer (BIT) to process molecules and pocket-ligand complexes in a unified style. Specifically, the main block of BIT is based on Transformer-M, a previous pre-training model. To enhance the model\u2019s ability of capturing multi- and inter-domain relationships, authors further incorporate Mixture-of-Domain-Experts (MoDE), i.e., separate feed-forward layers, for fusing molecule and pocket information better. Experiment results show that BIT achieves state-of-the-art performance in various downstream tasks, including binding affinity prediction, virtual screening, and molecular property prediction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- BIT shows great performance in all downstream tasks listed in the paper.\n- BIT can handle molecules and protein pockets in a unified way.\n- BIT can work well with both 2D and 3D molecules."
                },
                "weaknesses": {
                    "value": "- Contribution is minor. BIT combines the architecture of Transformer-M and Mixture-of-Domain-Experts technique, which both are from existing methods [1, 2, 3]. In addition, the way of combining the two is also a simple adaption. In summary, I appreciate that authors provide a strong method but I also believe that the contribution of this paper is not enough for acceptance.\n- Experiments are not convincing enough:\n1) Authors did not provide ablation studies to directly show the effectiveness of the main components, e.g., MoDE, of BIT. \n2) The comparison between BIT and the main baseline, i.e., Transformer-M, may not be fair enough. As BIT adopts MoDE technique, it has more trainable parameters than the vanilla Transformer-M. Moreover, BIT uses not only small molecule data but also protein-ligand complex data in the pre-training stage, while Transformer-M only uses small molecule data.\n3) The results of Transformer-M are not included in virtual screening and molecular property prediction benchmarks.\n- Some important details are missing, e.g.,\n1) In section 3.1, authors mentioned domain type embedding but without further description.\n2) Also in section 3.1, authors introduce two special nodes, i.e., [M_VNode] and [P_VNodes]. It is unclear how to build up the input representation with these two nodes.\n3) What is the specific value of noise scale controlling hyperparameter $\\sigma$?\n\n[1] Luo, S., Chen, T., Xu, Y., Zheng, S., Liu, T. Y., Wang, L., & He, D. (2022). One transformer can understand both 2d & 3d molecular data. arXiv preprint arXiv:2210.01765.\n\n[2] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.\n\n[3] Fan, Z., Sarkar, R., Jiang, Z., Chen, T., Zou, K., Cheng, Y., ... & Wang, Z. (2022). M\u00b3vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design. Advances in Neural Information Processing Systems, 35, 28441-28457."
                },
                "questions": {
                    "value": "- In section 3.3, why only add noise to molecules when doing pre-training?\n- In section 3.4.1, Why not use [P_VNode] or the combination of [M_VNode] and [P_VNode] as the representation of protein-ligand complexes?\n- In table 3, why the PCBA dataset is not included in the table?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission942/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission942/Reviewer_x6ZB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698190989276,
            "cdate": 1698190989276,
            "tmdate": 1699636020943,
            "mdate": 1699636020943,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eAK0mq5Utw",
                "forum": "tNAucRS0QQ",
                "replyto": "bdriYvFbdq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x6ZB (1/2)"
                    },
                    "comment": {
                        "value": "We deeply appreciate the reviewer for the insightful and constructive comments!\n\n> Contribution is minor. BIT combines the architecture of Transformer-M and Mixture-of-Domain-Experts technique, which both are from existing methods [1, 2, 3]. In addition, the way of combining the two is also a simple adaption. \n\nWe respectfully disagree with the reviewer that our contribution is minor. We acknowledge that the Transformer-M architecture and Mixture-of-experts technique have been verified in previous works, which we have extensively cited in section 3.2. We did not claim these or their combination as our contribution. \n\nWe want to clarify that our main contribution is designing a general-purpose self-supervised framework for biomolecule modeling, as summarized in the Introduction. To the best of our knowledge, we are the first to propose a viable solution for this objective, and to learn fine-grained inter-molecular interactions with a simple backbone. Along the path to unified self-supervised pre-training, we make a series of key contributions:\n\n- A unified tokenization approach across diverse molecular domains, enabling the straightforward combination architecture of Transformer-M and MoDE. Existing works either train in one particular molecule domain, limiting the model robustness, or train separate specialized backbones for different data and token types, restricting model scalability. \n- Unified self-supervised pre-training objectives applicable to diverse pre-training data sources and downstream tasks. This enhances model versatility and performance across tasks. \n- A general methodology to unify the diverse structural and domain-specific information of biomolecules, enriching the training dataset to enhance model performance.\n- Reformulating virtual screening as a retrieval task, achieving state-of-the-art performance at ultra-fast inference speeds.\n\nIn summary, we are the first to demonstrate the feasibility and potential of cross-domain molecular self-supervised learning with a single backbone. The efficiency and broad applicability of our framework allow it to outperform existing approaches. We believe this self-supervised pre-training framework is novel and valuable to the community.\n\n> 1. Authors did not provide ablation studies to directly show the effectiveness of the main components, e.g., MoDE, of BIT.\n> 2. The comparison between BIT and the main baseline, i.e., Transformer-M, may not be fair enough. As BIT adopts MoDE technique, it has more trainable parameters than the vanilla Transformer-M. Moreover, BIT uses not only small molecule data but also protein-ligand complex data in the pre-training stage, while Transformer-M only uses small molecule data.\n> 3. The results of Transformer-M are not included in virtual screening and molecular property prediction benchmarks.\n\nThanks for your suggestion. We have conducted further experiments to investigate the impact of MoDE following the suggestion. We compare three backbones, all trained on the same samples including both small molecules and protein-ligand complexes. One backbone is the vanilla Transformer-M. The second is Transformer-M with a larger FFN (2 $\\times$ hidden size), having the same parameters as BIT. The third is our proposed BIT. Note that we selected two representative datasets (i.e., a small Tox21 dataset and a large HIV dataset) from MoleculeNet for the ablation study on molecular property prediction, due to time constraints. The results are presented in the table below. We found that introducing MoDE significantly improves the performance, especially on PDBbind where both small molecules and proteins could be encoded simultaneously. This verifies the benefit of introducing MoDE. \n\nAs for the results of Transformer-M not being included in virtual screening and molecular property prediction benchmarks, this is because Transformer-M did not provide the specific experimental results mentioned above, so we were unable to make a direct fair comparison with it.\n\n| Backbone | HIV (AUC) $\\uparrow$ | Tox21 (AUC) $\\uparrow$ | PDBbind (MAE) $\\downarrow$ | DUD-E (AUC) $\\uparrow$ |\n|---------|---------|---------|--------|--------|\n| Transformer-M | 78.3 | 76.9 | 0.977 | 98.0 |\n| Transformer-M (2 $\\times$ hidden size) | 77.8  | 77.5 | 0.952 | 97.8 |\n| BIT | **80.2** | **78.1** | **0.927** | **98.4** |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562453455,
                "cdate": 1700562453455,
                "tmdate": 1700576569537,
                "mdate": 1700576569537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bq4MShv3Nj",
                "forum": "tNAucRS0QQ",
                "replyto": "bdriYvFbdq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x6ZB (2/2)"
                    },
                    "comment": {
                        "value": "Thank you for pointing out our unclear description. We will revise the writing to provide a more comprehensive description.\n\n> In section 3.1, authors mentioned domain type embedding but without further description.\n\nWe apologize for our unclear description. The domain type embedding, i.e. the molecule domain embedding and protein domain embedding $M_{type}, P_{type} \\in R^d$, are two learnable vectors used to indicate the domain type of each atom token. They have the same length as atom feature embedding and structural positional embedding. These three embeddings are summed up as input to the self-attention module. \n\n> Also in section 3.1, authors introduce two special nodes, i.e., [M_VNode] and [P_VNode]. It is unclear how to build up the input representation with these two nodes.\n\nWe apologize for our unclear description. These two special node representations function similarly to the [CLS] token in BERT. However, unlike BERT, each domain expert in our model has its own distinct indicator token. [M_VNode] is inserted at the beginning of the molecule representation sequence, while [P_VNode] is inserted at the beginning of the protein representation sequence. We make the connection between [M_VNode] and each molecule atom individually, and also make the connection between [P_VNode] and each protein atom individually. Both special node embeddings are randomly initialized and updated during training like regular node representation.\n\n> What is the specific value of noise scale controlling hyperparameter $\\sigma$ \n\n$\\sigma$ is set to 0.2, the same value used in Transformer-M.\n\n> In section 3.3, why only add noise to molecules when doing pre-training?\n\nSince proteins tend to be relatively rigid in most cases, generative models typically assume knowledge of the bound protein structure [1]. This is a widely accepted assumption in current molecular docking research [2]. Therefore, we adopt this approach in our work and maintain a fixed protein pocket during ligand recovery, as done in related studies [2,3].\n\n[1] Pagadala, Nataraj S., Khajamohiddin Syed, and Jack Tuszynski. \"Software for molecular docking: a review.\" Biophysical reviews 9 (2017): 91-102.\n\n[2] Corso, Gabriele, et al. \"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking.\" The Eleventh International Conference on Learning Representations. 2023.\n\n[3] Guan, Jiaqi, et al. \"3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction.\" The Eleventh International Conference on Learning Representations. 2023.\n\n> In section 3.4.1, Why not use [P_VNode] or the combination of [M_VNode] and [P_VNode] as the representation of protein-ligand complexes?\n\nIn the representation of the protein-ligand complex, [M_VNode] is positioned as the initial token, whereas the placement of [P_VNode] is more flexible. Therefore, using [M_VNode] simplifies engineering implementation. Additionally, early empirical studies demonstrated that using [M_VNode] either individually or in combination with [P_VNode] resulted in similar learning performance.\n\n> In table 3, why the PCBA dataset is not included in the table?\n\nFor molecular property prediction, we compare against the most representative 2D and 3D pretraining baselines, and report their official results from [4]. We exclude PCBA since most baselines have not reported results on this dataset. However, BIT still achieves the state-of-the-art performance among available records, as shown below: \n\n| Backbone | PCBA (AUC) $\\uparrow$ |\n|---------|---------|\n| PretrainGNN | 86.0 |\n| GROVER | 83.0 |\n| BIT | **88.3** |\n\n[4] Yu, Qiying, et al. \"Unified Molecular Modeling via Modality Blending.\" arXiv preprint arXiv:2307.06235v1 (2023).\n\n**We hope our response can alleviate your concerns. Please let us know if you have any additional questions.**"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562684161,
                "cdate": 1700562684161,
                "tmdate": 1700574118432,
                "mdate": 1700574118432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9ZEQOzvRx2",
                "forum": "tNAucRS0QQ",
                "replyto": "bdriYvFbdq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission942/Reviewer_x6ZB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission942/Reviewer_x6ZB"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate your detailed responses. The additional experiments have alleviated my concerns that the model's performance is solely due to increased parameters and data. Furthermore, I concur that developing a general-purpose model for both protein pockets and small molecules is an innovative concept. However, I still maintain that BIT lacks technical novelty due to the following reasons:\n\n- **Unified Tokenization Approach**: The tokenization method used in BIT is simply a standard approach that involves treating molecules at the atom level. This approach has been employed in previous studies, such as Uni-Mol and Transformer-M. Therefore, I do not consider it to be a novel technique. Additionally, I was unable to locate a detailed explanation of the tokenization approach in the paper.\n\n- **Unified Self-Supervised Pre-training Objectives**: Similar to the tokenization approach, the pre-training objectives of BIT are aligned to Transformer-M. It is hard to say the objectives are novel.\n\n- **General Methodology to Unify Domain-specific Information**: Because MoDE is very similar to MOE, the general methodology should be credited to MOE, which was proposed earlier.\n\n- **Reformulating Virtual Screening as a Retrieval Task**: DrugCLIP [1] has previously reformulated virtual screening as a retrieval task, and has provided a more comprehensive evaluation of this approach.\n\nIn conclusion, I believe that BIT is a well-implemented model, but it lacks a deep understanding. I therefore maintain my original score.\n\n[1] Gao B, Qiang B, Tan H, et al. DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening. NeurIPS 2023"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629211874,
                "cdate": 1700629211874,
                "tmdate": 1700629211874,
                "mdate": 1700629211874,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I4jDAyMwRt",
            "forum": "tNAucRS0QQ",
            "replyto": "tNAucRS0QQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission942/Reviewer_uSoZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission942/Reviewer_uSoZ"
            ],
            "content": {
                "summary": {
                    "value": "The \u201cGeneral purpose pre-trained model\u2026\u201d paper proposes a Biomolecular Interaction Transformer BIT, which is to have a multi-modal training on molecules, together with protein\u2014ligand matching, with 2-D and 3-D structures. The model includes a pre-training.\n\nIn my opinion, this is a valuable paper, presenting a well-defined model, together with well-designed experiments. The BIT model might not be revolutionary, but is a piece of a very solid work, and well performing. I opt for accepting this paper for the conference."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. An advanced model, encompassing 2-D and 3-D structures for molecules, proteins, and ligand\u2014protein interaction modelling, with multi-modal training. The model can be tuned.\n2. Well-defined multi-modal representation learning employing a Transformer model (a Transformer-M model of Luo et al.) with independent tuning of proposed BIT for different knowledge domains.\n3. A very good graphical abstract is given on page 2, showing in detail the proposed architecture. Clear presentation. All this increase the paper readability greatly."
                },
                "weaknesses": {
                    "value": "1. Some generalization of the model to other areas would be welcome."
                },
                "questions": {
                    "value": "1. In the comparison tables, the models (usually proposed BIT) with the best mean have values given in boldface. Are all the models trained on the same data-sets? For some predicted features, the differences between BIT and some other models are large, even though some of them are Transformers too. Are the optimal values for BIT the result of the proposed BIT architecture, the fine-tuning on different modes (the multi-modality), different data sets, better pre-training, or something other? The discussion on the comparison to other approaches is needed in the conclusions/discussion section.\n2. Please correct the spelling of some words. Just as well, please correct the editing of mathematical expressions. E.g. in equations (1) and (2) the equal signs = should be aligned ;-)\n3. Your paper and model is strictly molecule learning oriented. Do you think that the general approach can be used in other sciences? E.g. in biological experiments on cancerous cells and the impact of some sort of certain treatments, which would imply modifications in the course of the operation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission942/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission942/Reviewer_uSoZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672124366,
            "cdate": 1698672124366,
            "tmdate": 1699636020841,
            "mdate": 1699636020841,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0bt798vii5",
                "forum": "tNAucRS0QQ",
                "replyto": "I4jDAyMwRt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uSoZ"
                    },
                    "comment": {
                        "value": "We deeply appreciate the reviewer for the insightful and constructive comments!\n\n> In the comparison tables, the models (usually proposed BIT) with the best mean have values given in boldface. Are all the models trained on the same data-sets? For some predicted features, the differences between BIT and some other models are large, even though some of them are Transformers too. Are the optimal values for BIT the result of the proposed BIT architecture, the fine-tuning on different modes (the multi-modality), different data sets, better pre-training, or something other? The discussion on the comparison to other approaches is needed in the conclusions/discussion section.\n\nThanks for pointing this out. You are right that our method and other pre-training baselines are inconsistent in terms of the pre-training data and pre-training objectives. Being able to leverage more cross-domain data is an advantage of our proposed BIT. For downstream tasks, we report the official results of baselines and follow the same experimental settings as baselines to ensure a fair comparison. We will discuss this issue in the revised paper. \n\nYou provided us with an excellent suggestion to better isolate the benefit of our backbone modeling approach through controlled experiments. Thus, we introduce ablation experiments to investigate the factors influencing performance. Using the same pre-training data, we remove select pre-training objectives or train different backbones with identical procedures to quantify their impact. Note that we selected two representative datasets (i.e., a small Tox21 dataset and a large HIV dataset) from MoleculeNet for the ablation study on molecular property prediction, due to time constraints. As summarized in the following tables, we find 1) our two pre-training objectives are necessary and effective, and 2) introducing MoDE significantly improves performance. In the revision, we will incorporate these ablation studies to provide a controlled comparison.\n\n| Pre-training objectives | HIV (AUC) $\\uparrow$ | Tox21 (AUC) $\\uparrow$ | PDBbind (MAE) $\\downarrow$ | DUD-E (AUC) $\\uparrow$ | \n|---------|---------|--------|--------| --------|\n| BIT | **80.2** | **78.1** | **0.927** | **98.4** |\n| w/o Masked token denoising | 78.2 | 77.6 | 0.939 | 96.5 |\n| w/o Coordinate denoising | 78.5 | 76.1 | 1.016 | 97.1 |\n| w/o pre-training | 70.9 | 75.1 | 1.114 | 95.7 |\n\n| Backbone | HIV (AUC) $\\uparrow$ | Tox21 (AUC) $\\uparrow$ | PDBbind (MAE) $\\downarrow$ | DUD-E (AUC) $\\uparrow$ | \n|---------|---------|---------|--------|--------|\n| BIT | **80.2** | **78.1** | **0.927** | **98.4** |\n| w/o MoDE | 78.3 | 76.9 | 0.977 | 98.0 |\n| w/o MoDE (2 $\\times$ hidden size) | 77.8  | 77.5 | 0.952 | 97.8 |\n\n> Please correct the spelling of some words. Just as well, please correct the editing of mathematical expressions. E.g. in equations (1) and (2) the equal signs = should be aligned ;-)\n\nThanks for your patience and helpful reminder. We will carefully revise our writing.\n\n> - Some generalization of the model to other areas would be welcome.\n>\n> - Your paper and model is strictly molecule learning oriented. Do you think that the general approach can be used in other sciences? E.g. in biological experiments on cancerous cells and the impact of some sort of certain treatments, which would imply modifications in the course of the operation?\n\nThank you for acknowledging and supporting our work. Our current model focuses on general-purpose biomolecule learning. It operates at the molecular resolution level and could potentially be adopted for problems determined by physical structure. However, progress remains to be made in order to handle more general biological problems that cannot be fully explained through molecule interactions.\n\nThe idea of multi-domain modeling is powerful and could be applied to other scientific domains to capture correlations between different data domains, such as precision therapy (cell lines and molecules) and text-guided protein design (text and proteins). \n \nWe plan to pursue more general AI capabilities step by step. The first step is building a foundation model for biomolecules and biochemical reactions. After this, we intend to integrate additional multi-domain data including images and text, which could enable solving more general biomedical problems. This feedback has been invaluable for identifying opportunities and challenges moving forward."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551260183,
                "cdate": 1700551260183,
                "tmdate": 1700564592271,
                "mdate": 1700564592271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ariViumA1p",
            "forum": "tNAucRS0QQ",
            "replyto": "tNAucRS0QQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission942/Reviewer_dgYT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission942/Reviewer_dgYT"
            ],
            "content": {
                "summary": {
                    "value": "In this study, a new pre-trained transformer, BIT, is introduced for processing small molecules, proteins, and ligand-protein complexes. The architecture is based on Transformer-M, which is a transformer architecture that enables the processing of 2D and 3D structures. The main architectural innovation introduced in this work is the use of Mixture-of-Domain-Experts (MoDE) that replaces feed-forward layers, allowing for different processing of small molecules and macromolecules. Two pre-training methods, masking and coordinate denoising, are used to improve the performance of this model. BIT can be utilized for various molecular tasks, including molecular property prediction, structure-based virtual screening, and binding affinity prediction. The experimental section shows that BIT outperforms similar approaches in all three tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposition to use Mixture-of-Domain-Experts in order to process both small molecules and proteins and protein-ligand complexes is interesting. This way, the transformer can be (pre)trained using more data with high diversity.\n- Two pre-training methods are implemented, and the strong performance of the pre-trained transformer is demonstrated in the experiments.\n- The motivation of the paper is clear, and the methodology and experiments are easy to follow.\n- This work has some significant applications in the molecular modeling domain, especially in structure-based drug design. Because BIT can process both small molecules and proteins, the application domain is very broad. The significance of the study is corroborated by the strong performance in the molecular property prediction, binding affinity prediction, and structure-based virtual screening tasks."
                },
                "weaknesses": {
                    "value": "- The main novelty of the paper is the introduction of MoDE in order to process data from diverse molecular domains. In my opinion, the paper lacks a proper evaluation of what these experts learn. For example, do molecule and pocket experts learn similar weights, or are there significant differences? What is the performance of this model when only one type of feed-forward layer is used (like in Transformer-M), but the same pretraining procedure is applied?\n- The Authors propose to use two pre-training objectives. It would be interesting to see what is the impact of each of them. Why did the Authors decide to use a different pre-training procedure than used in Transformer-M? The experimental tables are missing the results achieved for the non-pretrained model.\n- The choice of the models used in the experiments seems arbitrary. For example, why are different models used for binding affinity prediction and molecular property prediction? GROVER could be used in both scenarios. Why are some of these models not pre-trained, while the original works provide pre-training procedures (and sometimes also the pre-trained weights), e.g. GROVER and MAT?\n- Finally, another benchmark for structure-based virtual screening would be helpful in assessing the performance of the proposed method. It has been shown, that decoys contain hidden biases that can impact the performance of deep learning methods [1]. \n\n[1] Chen, Lieyang, et al. \"Hidden bias in the DUD-E dataset leads to misleading performance of deep learning in structure-based virtual screening.\" PloS one 14.8 (2019): e0220113.\n\nIn conclusion, the paper introduces some new ideas (mixing of domains and a different pre-training procedure), but the presented results do not support these design choices. It is not clear which novelties contribute most to the strong performance of BIT.\n\nMinor comments (not impacting my evaluation):\n- The first paragraph of the Introduction section: \"depend on these information\""
                },
                "questions": {
                    "value": "1. For binding affinity prediction, did you consider measuring non-linear correlation, e.g. using the Spearman correlation coefficient? This evaluation metric could be better at showing which methods can correctly prioritize compounds with strong affinity.\n2. Do you plan to publish the code for better reproducibility of your method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838294779,
            "cdate": 1698838294779,
            "tmdate": 1699636020765,
            "mdate": 1699636020765,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8QeATPMscY",
                "forum": "tNAucRS0QQ",
                "replyto": "ariViumA1p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dgYT (1/2)"
                    },
                    "comment": {
                        "value": "We deeply appreciate the reviewer for the insightful and constructive comments!\n\n> The main novelty of the paper is the introduction of MoDE in order to process data from diverse molecular domains. In my opinion, the paper lacks a proper evaluation of what these experts learn. For example, do molecule and pocket experts learn similar weights, or are there significant differences? What is the performance of this model when only one type of feed-forward layer is used (like in Transformer-M), but the same pretraining procedure is applied?\n\nThanks for pointing this out. To further investigate the impact of MoDE, we introduce additional experiments. In these experiments, we use the same pre-training procedure to train two additional backbones. One backbone omits MoDE and uses a shared FFN, while the other also omits MoDE but utilizes a larger FFN (2 $\\times$ hidden size) to match BIT in parameter scale. Note that we selected two representative datasets (i.e., a small Tox21 dataset and a large HIV dataset) from MoleculeNet for the ablation study on molecular property prediction, due to time constraints. The results are presented in the table below. We find that introducing MoDE significantly improves performance, especially on PDBbind where both small molecules and proteins could be encoded simultaneously. This aligns with our motivation for introducing MoDE. Furthermore, the performance gains cannot be attributed solely to increased parameters, as using a larger FFN alone did not consistently improve performance across tasks.\n\n| Backbone | HIV (AUC) $\\uparrow$ | Tox21 (AUC) $\\uparrow$ | PDBbind (MAE) $\\downarrow$ | DUD-E (AUC) $\\uparrow$ | \n|---------|---------|---------|--------|--------|\n| BIT | **80.2** | **78.1** | **0.927** | **98.4** |\n| w/o MoDE | 78.3 | 76.9 | 0.977 | 98.0 |\n| w/o MoDE (2 $\\times$ hidden size) | 77.8  | 77.5 | 0.952 | 97.8 |\n\n> The Authors propose to use two pre-training objectives. It would be interesting to see what is the impact of each of them. Why did the Authors decide to use a different pre-training procedure than used in Transformer-M? The experimental tables are missing the results achieved for the non-pretrained model.\n\nThanks for your suggestion. To examine the impact of the two pre-training objectives, we introduce additional experiments by eliminating select objectives to study their effectiveness. As shown in the table below, the pretrain-then-finetune paradigm is significantly superior to direct supervised training for a Transformer model. We also observe a notable performance decrease when removing either pre-training objective, particularly coordinate denoising for PDBbind (2D+3D) and masked token denoising for molecular property prediction (2D). These results indicate that both pre-training objectives are crucial and yield positive outcomes.\n\nWe did not use the pre-training procedure from Transformer-M for two reasons: 1) supervised pre-training based on the homo-lumo gap prediction may lead to negative transfer, and 2) there is extremely limited data with both 3D co-crystal structures and experimentally determined binding affinity values (approximately 23,000 in PDBbind). Our aim is to build a more versatile model using more unsupervised data, so we adopt a purely self-supervised pre-training approach.\n\n| Pre-training objectives | HIV (AUC) $\\uparrow$ | Tox21 (AUC) $\\uparrow$ | PDBbind (MAE) $\\downarrow$ | DUD-E (AUC) $\\uparrow$ | \n|---------|---------|--------|--------| --------|\n| BIT | **80.2** | **78.1** | **0.927** | **98.4** |\n| w/o Masked token denoising | 78.2 | 77.6 | 0.939 | 96.5 |\n| w/o Coordinate denoising | 78.5 | 76.1 | 1.016 | 97.1 |\n| w/o pre-training | 70.9 | 75.1 | 1.114 | 95.7 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555864650,
                "cdate": 1700555864650,
                "tmdate": 1700555864650,
                "mdate": 1700555864650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hhwD0QVAC3",
            "forum": "tNAucRS0QQ",
            "replyto": "tNAucRS0QQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission942/Reviewer_2Mm5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission942/Reviewer_2Mm5"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a multitask Transformer architecture for molecular interaction learning called Biomolecular Interaction Transformer (BIT)  and a self-supervised learning objective of coordinate denoising and masked token denoising. This approach allows the model to learn representations of biomolecules of both 2D and 3D structures. Moreover, the paper introduces the Mixture-of-Domain-Experts (MoDE) module, which enables the model to learn from biomolecules belonging to different chemical domains (proteins and ligands)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well motivated and the writing is clear.\n+ The proposed approach focuses on protein-ligand interactions, which is of paramount importance for drug discovery and can potentially lead to more efficient and effective therapeutic solutions."
                },
                "weaknesses": {
                    "value": "- The pretraining process appears to be confined to proteins and small molecules. This limited scope raises questions about the model's applicability to a broader range of biomolecules and interactions. \n- The proposed Transformer backbone is invariant to geometric transformations, which may limit its expressive power compared to SE(3) or E(3) equivariant architectures.\n- The model only considers the binding pocket segment of proteins\u2014while computationally efficient, may not be practically feasible without costly simulations to identify these segments. This reliance on prior knowledge or expensive computations could limit the accessibility and scalability in practical applications.\n- The strategy of pre-training the model on both equilibrium structures of molecules and higher-energy protein-ligand complexes may be conceptually problematic. These two types of data represent vastly different energy states, and it is unclear what meaningful semantic learning can be achieved by pretraining them together. This may lead to a model that does not adequately distinguish between the distinct energetic landscapes of the two systems.  Also, the scales of the two training sources are not balanced, where PCQM4Mv2 is significantly larger than Q-BioLiP, but the model performance on molecular property prediction is not very significant compared to the baselines, which warrants a further examination.\n- There is a lack of a detailed explanation for how positional encodings of 2D and 3D structures are integrated into the model, which leaves a gap in understanding the full architecture and mechanics of the model."
                },
                "questions": {
                    "value": "Please see the above weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699059018458,
            "cdate": 1699059018458,
            "tmdate": 1699636020665,
            "mdate": 1699636020665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T4Jgin4tmw",
                "forum": "tNAucRS0QQ",
                "replyto": "hhwD0QVAC3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Mm5 (1/2)"
                    },
                    "comment": {
                        "value": "We deeply appreciate the reviewer for the insightful and constructive comments!\n\n> The pretraining process appears to be confined to proteins and small molecules. This limited scope raises questions about the model's applicability to a broader range of biomolecules and interactions.\n\nThanks for pointing this out. A key advantage of our proposed method over existing works is indeed its broader applicability. Existing methods typically use different tokens for different biomolecules, such as amino acids for proteins and atoms for small molecules. This requires designing specialized encoders for each type of biomolecule, limiting the applicability. Our method instead employs the same token (i.e., atom) for all biomolecule types and uses Mixture-of-Domain-Experts (MoDE) to capture domain specificity. By simply adding corresponding domain expert FFNs, our approach can be readily applied to any biomolecule and interaction.\n\nThe reason we focus on proteins and small molecules in this paper is that protein-molecule binding-related tasks are typical applications with sufficient structural data. This aligns with the main focus of most related works in structure-based drug design and enables a direct comparison to the previous methods in the field. However, as the reviewer points out, investigating more diverse, high-quality biomolecules for pre-training would be valuable future work to further boost the performance and applicability of our model.\n\n> The proposed Transformer backbone is invariant to geometric transformations, which may limit its expressive power compared to SE(3) or E(3) equivariant architectures.\n\nThanks for your thoughtful reminder. We have exactly the same question ourselves when designing the backbone structures. Ultimately, we choose our Transformer backbone over the equivariant architectures for the following reasons: While equivariant architectures efficiently encode rotational and translational symmetries for molecules, their computational overhead becomes prohibitive for large-scale pre-training. For our goal of pre-training a sizable model using atom tokens at scale, training scalability and stability are more crucial. Furthermore, sufficient training data equipped with augmentation techniques can compensate for the lack of built-in equivariance for the Transformer. Empirical results across tasks confirm our backbone's superior performance as expected. Though promising, equivariant networks' current challenges make our backbone a better fit for now.\n\n> The model only considers the binding pocket segment of proteins\u2014while computationally efficient, may not be practically feasible without costly simulations to identify these segments. This reliance on prior knowledge or expensive computations could limit the accessibility and scalability in practical applications.\n\nThis is true. Pocket searching falls outside the scope of this paper, which is focused on training a cross-domain pre-trained model for biomolecules. For the binding-related task, we follow the standard protocol from mainstream approaches [1,2] and the competing methods [3], which assume the binding pocket as given. Pocket searching remains an active research area that may require distinct methodological considerations. Machine learning also has demonstrated potential in this domain, as evidenced by recent approaches [4,5]. \n\n[1] Trott, Oleg, and Arthur J. Olson. \"AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading.\" Journal of computational chemistry 31.2 (2010): 455-461.\n\n[2] Koes, David Ryan, Matthew P. Baumgartner, and Carlos J. Camacho. \"Lessons learned in empirical scoring with smina from the CSAR 2011 benchmarking exercise.\" Journal of chemical information and modeling 53.8 (2013): 1893-1904.\n\n[3] Li, Shuangli, et al. \"Structure-aware interactive graph neural networks for the prediction of protein-ligand binding affinity.\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 2021.\n\n[4] Kriv\u00e1k, Radoslav, and David Hoksza. \"P2Rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure.\" Journal of cheminformatics 10 (2018): 1-12.\n\n[5] Gainza, Pablo, et al. \"Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning.\" Nature Methods 17.2 (2020): 184-192."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552950791,
                "cdate": 1700552950791,
                "tmdate": 1700552950791,
                "mdate": 1700552950791,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]