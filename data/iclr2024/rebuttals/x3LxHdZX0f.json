[
    {
        "title": "PUMA: Secure Inference of LLaMA-7B in Five Minutes"
    },
    {
        "review": {
            "id": "TXmjtgC6z0",
            "forum": "x3LxHdZX0f",
            "replyto": "x3LxHdZX0f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4557/Reviewer_tTd1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4557/Reviewer_tTd1"
            ],
            "content": {
                "summary": {
                    "value": "This paper brings together a bunch of recent techniques for semi-honest, honest majority 3 server multi party computation and supplements them with a few custom designed gadgets. These are then used to approximate GeLU based neural networks in MPC. They have an experimental section analysing how quickly it runs and the accuracy/precision of the resulting protocol."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper brings together a bunch of SOTA work well.\nIt does provide some new approximations, e.g. of GeLU, which seem like useful components for future work.\nThe paper is well laid out and easy to follow.\nThe resulting protocol is runnable with fairly large models and gets good approximations."
                },
                "weaknesses": {
                    "value": "The paper seems some what incremental in nature with the new contributions being slightly limited in scope.\nIt is dubious whether the ability to generate one token from a language model in 5 minutes between three parties in the semi-honest honest majority model is likely to have any applications in the imminent future. But this is a step closer to something practical being possible."
                },
                "questions": {
                    "value": "Are you aware of any plausible near term application that this technology is close to good enough to be deployed for?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749662623,
            "cdate": 1698749662623,
            "tmdate": 1699636433439,
            "mdate": 1699636433439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b49VkRcDiO",
                "forum": "x3LxHdZX0f",
                "replyto": "TXmjtgC6z0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tTd1"
                    },
                    "comment": {
                        "value": "Thank you for your reviewing efforts and comments. \n\nYes practical general-purpose privacy-preserving LLMs is like a \"holy grail\" and hard to achieve, we are still far from that.\nBut PUMA does not use any GPUs or quantization methods, so it is very likely that we could benefit from powerful GPUs and ML algorithm optimizations in the future. Once PUMA could be accelarated by one or two magnitudes, it could be possible to support analysis tasks that are not time-sensitive but require private data, e.g., business document or personal chat history processing."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699860150424,
                "cdate": 1699860150424,
                "tmdate": 1699860150424,
                "mdate": 1699860150424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mOBHHhUoqA",
            "forum": "x3LxHdZX0f",
            "replyto": "x3LxHdZX0f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4557/Reviewer_bbdZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4557/Reviewer_bbdZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes PUMA, a 3PC inference protocol for Transformers."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The 3PC setting for LLM is timely."
                },
                "weaknesses": {
                    "value": "1. Limited novelty. The provided protocols are straightforward and contain no novel design or construction.\n2. Compared with the baseline, the protocol advantages seem to all come from RSS. \n2. Overclaim the experimental performance. The title of this paper is Secure Inference of LLaMA-7B in Five Minutes. However, it is overclaimed. 5 minutes is only when the input and output are 4 and 1 token respectively, which is obviously not in line with the practical service setting."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834242103,
            "cdate": 1698834242103,
            "tmdate": 1699636433373,
            "mdate": 1699636433373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IyjDmlUGhf",
                "forum": "x3LxHdZX0f",
                "replyto": "mOBHHhUoqA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bbdZ"
                    },
                    "comment": {
                        "value": "Thank you for your reviewing efforts and comments.\n\n## For Q1:\nPlease check our general response for \"lack of novelty\".\n\n## For Q2:\nNo, it is not true that \"the protocol advantages seem to all come from RSS\". RSS-based 3PC could save some communication at the **linear** layers compared to MPCFormer's dealer-mode 3PC protocol, but our major advantages come from the lightweight and accurate **non-linear** approximations. We did an experiment on Llama-7B (8-token input) to replace our GeLU(SiLU) optimization with protocols built on faithful exponent operations (like CrypTen in MPCFormer), and it will increase the overall communication cost drastically (from 1.794GB to 4.303GB).\n\n## For Q3:\nDescribing the cost of LLM in a  \"per token\" way is a common practice. We argue that a paper should be judged by its merits, rather than whether it's in line with the practical service."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699868223478,
                "cdate": 1699868223478,
                "tmdate": 1699868223478,
                "mdate": 1699868223478,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mi2BKIZ84H",
            "forum": "x3LxHdZX0f",
            "replyto": "x3LxHdZX0f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4557/Reviewer_DyAw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4557/Reviewer_DyAw"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a secure Transformer inference framework in 3PC."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Simple but effective approximations for GELUs.\n+ End-to-end framework for Secure LLM Inference.\n+ Extensive evaluations."
                },
                "weaknesses": {
                    "value": "The protocols in this work seem limited contributions and are mainly taken from prior works."
                },
                "questions": {
                    "value": "1. What is the cost of secure inference on LLaMA-7B when extending it to a common input length e.g., 128?\n2. Does the polynomial approximation of GELU affect the accuracy of large models such as LLaMA-7B because it seems to cause a relatively large error?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834357321,
            "cdate": 1698834357321,
            "tmdate": 1699636433269,
            "mdate": 1699636433269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rTae1i9sVT",
                "forum": "x3LxHdZX0f",
                "replyto": "mi2BKIZ84H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DyAw"
                    },
                    "comment": {
                        "value": "Thank you for your reviewing efforts and comments.\n## For Q1:\nWe did an experiment running on LLaMA-7B for input with 128 tokens, and the time cost is about 10 minutes. The cost does not grow linearly with the input size (which is also indicated in Table 5).\n## For Q2:\nYes even a small perturbation in the activation function could lead to different output. As mentioned in the general response, if we replace our GeLU/SiLU approximation with Quad polynomial approximation like MPCFormer(ICLR23) , the result would be like this:\n\n- Q: What is the largest animal?\n- A: THER grape contend Iraves Grahaminthshireilton ChurchillTHER grape contendinthshireinthshireinthshireinthshireinthshireinthshireinthshireinthshireinth\n\nIt serves as an evidence that finding proper approximations is a non-trivial task."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699867624418,
                "cdate": 1699867624418,
                "tmdate": 1699867624418,
                "mdate": 1699867624418,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rUZdhdT0Ph",
            "forum": "x3LxHdZX0f",
            "replyto": "x3LxHdZX0f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4557/Reviewer_G7JD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4557/Reviewer_G7JD"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a model that utilizes multi-party computation techniques to perform the LLAMA-7B model while preserving the privacy of the client's data. In this process, an optimization of the approximation method for the GeLU function was carried out, and softmax, embedding, and layer normalization methods were all implemented using MPC, achieving an end-to-end implementation. Through these techniques, the computational time has been reduced by approximately 2 times compared to the existing implementation, MPCformer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Completion of end-to-end implementation of a large language model using multiparty computation techniques.\n2. Achieving inference that is twice as fast as the previously published MPCformer model.\n3. The operation of the GeLU function in a different manner compared to the conventional approach."
                },
                "weaknesses": {
                    "value": "1. Most of the methods appear to be simple adaptations of existing techniques, lacking any distinctive novel approach. While the results are impressive from an industrial perspective, it raises doubts about whether they are suitable for ICLR, which places a strong emphasis on academic contributions.\n\n2. While it claims to perform twice as fast as MPCformer, the paper lacks precise explanations of why each technique is superior to the existing ones, making it challenging to assess their effectiveness. \n\n3. The primary technical contribution seems to be the approximation of the GELU function. However, without a clear comparison to existing approximations, it is challenging to assess the value of this technique. The paper introduces variations in polynomial computation based on the range of x, but it is not evident how this approach is superior to the conventional method of approximating the GELU function in terms of computational efficiency.\n\n4. Regarding the meaningful academic contributions in softmax, embedding, and layerwise normalization, it is not clear where they lie, making it difficult to discern the significance of these contributions."
                },
                "questions": {
                    "value": "1. Provide numerical evidence of how the computations involved in our method for approximating the GELU function offer advantages in terms of computational and communication costs compared to existing techniques that achieve the same level of accuracy.\n\n2. Convince that the techniques employed in softmax, embedding, and layerwise normalization go beyond mere combinations of existing methods and provide non-trivial technical contributions.\n\n3. Explain what specific factors contributed to the 2x performance improvement compared to MPCformer and quantitatively specify the performance gains achieved by each factor."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4557/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4557/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4557/Reviewer_G7JD"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699271155257,
            "cdate": 1699271155257,
            "tmdate": 1699636433202,
            "mdate": 1699636433202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KPWUpZdsia",
                "forum": "x3LxHdZX0f",
                "replyto": "rUZdhdT0Ph",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer G7JD"
                    },
                    "comment": {
                        "value": "Thank you for your reviewing efforts and comments.\n\n## For Q1 and Q3: \n\nThere are many optimization details under PUMA, roughly speaking, GeLU and softmax contribute to the efficiency gains, while embedding, and layernorm are vital to usability. In fact embedding and layernorm decrease efficiency because they are absent in previous works, but we cannot output meaningful results without them. The potion of cost for each module varies with different models. Due to the space limit, we cannot afford to discuss these costs one by one, but GeLU usually contributes the major part in efficiency gains.\n\nWe did an experiment on Llama-7B (8-token input) to replace our GeLU(SiLU) optimization with protocols built on faithful exponent operations (like CrypTen in MPCFormer), and it will increase the overall communication drastically (from 1.794GB to 4.303GB). \n\n## For Q2: \nBringing all the simple and efficient submodules altogether to achieve an end-to-end solution is a non-trivial task. An example is [R1]: it is also made of several building blocks built from existing works.  Each building block might seem to be \"incremental\", but it does not affect the overall contribution. In fact, it serves as the state-of-the-art in the area of MPC machine learning training. \n\n_[R1] Keller M, Sun K. Secure quantized training for deep learning[C]//International Conference on Machine Learning. PMLR, 2022: 10912-10938. (ICML22 spotlight)_"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699866733394,
                "cdate": 1699866733394,
                "tmdate": 1699868386471,
                "mdate": 1699868386471,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]