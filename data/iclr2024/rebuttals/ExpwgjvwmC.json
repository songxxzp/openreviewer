[
    {
        "title": "OmniInput: A Model-centric Evaluation Framework through Output Distribution"
    },
    {
        "review": {
            "id": "8HWktDDhik",
            "forum": "ExpwgjvwmC",
            "replyto": "ExpwgjvwmC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission748/Reviewer_8n4h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission748/Reviewer_8n4h"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model-centric evaluation framework to evaluate the quality of a model's predictions on **all possible inputs**.\nIt first uses a sampler to sample in the input space to obtain the output distribution.\nThen, it annotates the representative inputs with human annotators, results in a human-annotated confidence score for each representative input.\nBased on the confidence scores and model predictions, OmniInput generates a precision-recall curve and report AUC as the metric."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper first proposes a model-centric evaluation approach to evaluate the quality of a model's predictions on **all possible inputs**."
                },
                "weaknesses": {
                    "value": "1. The authors didn't provide enough details for OmniInput (see questions). \n2. The proposed OmniInput was not applied to more challenging  / real world scenarios (large scale image classification, face recognition, etc.)"
                },
                "questions": {
                    "value": "1. More details of OmniInput need to be provided:\n    1. How many samples do OmniInput samples for each experiment? Does it need to sample many data points to ensure at least 50 samples in each bin?\n    2. The scales of log(recall) on precision-recall graphs need to be provided?\n    3. Can this approach be applied to problems with larger scales, i.e., ImageNet classification (1000 classes)? \n2. Does the proposed metric have more advantages besides improved efficiency? For example, if a classification model achieves a higher AUC under OmniInput, can we say the model is less vulnerable to adversarial attacks (black box or white box)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission748/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission748/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission748/Reviewer_8n4h"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission748/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698501150072,
            "cdate": 1698501150072,
            "tmdate": 1699636001969,
            "mdate": 1699636001969,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qQEAmgi5vS",
                "forum": "ExpwgjvwmC",
                "replyto": "8HWktDDhik",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reviews!"
                    },
                    "comment": {
                        "value": "Q1: \n  1. Estimate is provided in caption of Figure 4 with 40 bins, each containing at least 300 samples (at least 12000 samples). GWL has a gradient component to encourage the sampler to visit under-visited output values and it does not need many data points to ensure 50 samples. \n  2. The scale of log(recall) in Figs. 2 and 4 is in natural log scale. \n  3. (W2 and Q1.3) Yes, we believe our approach can be applied to problems with larger scales such as ImageNet classification directly. Please also refer to the general discussion at the beginning of the response \"Common concerns on representative dataset and multi-class classification\" about the scalability where we  discuss how to interpret our results in the experimental section for other datasets and how to extend our method to multi-class classification (applied for CIFAR100). Additional results of CIFAR100 are presented.\n\nQ2: Yes. Our method introduces a paradigm shift in the methodology for evaluating models, suggesting a novel approach that prioritizes a model-centric perspective over a data-centric one. Unlike the traditional practice of hypothesizing potential model failures then conducting experiments for validation, our model-centric approach aims to comprehensively understand a model's intrinsic biases, rather than relying on artificially constructed datasets to assess model performance. By doing so, our method mitigates dataset biases, with improved efficiency resulting as a byproduct, as visual diversity is notably limited across various models.\nIn the context of adversarial attacks, it is noteworthy that existing models, particularly classifiers, often lack the capability to propose informative samples. We leave this research focusing on the generation of informative samples to gain a model-centric understanding of adversarial samples as future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184913437,
                "cdate": 1700184913437,
                "tmdate": 1700185122445,
                "mdate": 1700185122445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W21pK2kMyA",
                "forum": "ExpwgjvwmC",
                "replyto": "8HWktDDhik",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please check if our revision will make our work meet the ICLR bar?"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and feedback! We replied by adding the details to OmniInput (number of samples and scale of recall). We added experiments for representative datasets (CIFAR100 and CIFAR10) with multi-class classification for the larger-scale question. We also clarified the advantage of our approach besides efficiency. Could you please check and see if our revision will make our work meet the ICLR bar? Please let us know if you have any questions. Really appreciate that!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457477133,
                "cdate": 1700457477133,
                "tmdate": 1700457477133,
                "mdate": 1700457477133,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8y0qsY6G9w",
                "forum": "ExpwgjvwmC",
                "replyto": "W21pK2kMyA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_8n4h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_8n4h"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors, \nThanks for the efforts you made in this rebuttal. After reading the rebuttal, I still tend to keep my original rating. One major reason is that I do not get what the metric (AUPR, etc.) inflects in real world application. For an AI/ML model, what's the advantage of  performing good on all possible inputs with equal probability? Will it make the model less vulnerable to attacks (if so, you need to present some quantitative results). \nI'm not a theory guy and it might be difficult for me to get the merit of this work. It would be better if the paper decision can be made based on other reviews."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622842221,
                "cdate": 1700622842221,
                "tmdate": 1700622842221,
                "mdate": 1700622842221,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ho3wmSuQe5",
                "forum": "ExpwgjvwmC",
                "replyto": "8HWktDDhik",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. The metric applied in the entire input space inflects that we can trust a model that have larger AUPR compared to the ones with smaller AUPR, assuming **we do not know what could be the input the model**. This is practical since in the real world, especially in out-of-distribution detection, if we cannot guarantee what the users will input the model. If we test on a predefined set of datasets but the users tend to use the model in an unexpected way, we need such a metric to compare models to know what model can trust more than the others. For example, we might have checked the model can detect Gaussian noise, but what if we have a different Gaussian noise of input with different mean and variance or uniform distribution of the noise as input? We cannot enumerate all the possibility, and the value of our work is we can still estimate the errors a model can make in the most general testing set (entire input space). \n\n**Thus, the model with higher precision is expected to be less vulnerable to overconfident prediction, and a model with higher AUPR is expected to approach closer to the desired input distribution of training (testing) set.** Our work does not target specifically for adversarial attacks at this moment which requires modification to focus on data (informative samples) distribution."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623714653,
                "cdate": 1700623714653,
                "tmdate": 1700624604841,
                "mdate": 1700624604841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "caqszdaRde",
            "forum": "ExpwgjvwmC",
            "replyto": "ExpwgjvwmC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission748/Reviewer_HAeb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission748/Reviewer_HAeb"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a new benchmark that uses the output distribution for model evaluation over the entire input space. Existing sampling algorithms such as GWL are used to sample representative inputs from the output distribution. Then the (human) evaluators annotate representative inputs. Finally, the authors compute the precision-recall curve to compare different machine learning models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Compared with the traditional data-centric evaluations, the authors present a different evaluation benchmark based on model-centric evaluation.\n2. The authors present a detailed analysis on the proposed benchmark. It is a fun read."
                },
                "weaknesses": {
                    "value": "1. Over-fitting Concern: The benchmark, based on binary MNIST or the initial two classes of CIFAR10, leverages low-resolution images, making them relatively easy to recognize. Consequently, models tend to over-fit on this training set, as evident from the near-perfect accuracy rates in Table 1. Advanced CNN architectures might face the over-fitting issue, potentially leading to a subpar performance on the proposed benchmark.\n2. Scalability Concerns: The authors may consider using a more diverse or challenging dataset to truly evaluate and validate model capabilities. However, the current approach may face challenges when extended to more complex, real-world datasets. The input space becomes considerably vast for such datasets, and there appears to be an absence of efficient sampling techniques in the current framework. It would be valuable to address how the proposed method plans to tackle these scalability issues."
                },
                "questions": {
                    "value": "1. Benchmark Limitation: The proposed benchmark is currently restricted to binary classification. How does this benchmark be extended to handle multi-category settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission748/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637335610,
            "cdate": 1698637335610,
            "tmdate": 1699636001898,
            "mdate": 1699636001898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oJkXd6QaZm",
                "forum": "ExpwgjvwmC",
                "replyto": "caqszdaRde",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reviews!"
                    },
                    "comment": {
                        "value": "(W1) Our setting does not assume we know how the model is trained. Thus whether the model simply memorizes the training set (overfitting) does affect our methodology. Overfitting only affects the results we are about to analyze to understand model performance.\n\n(W2, Q1) We discussed how to extend the framework to multi-category settings and more advanced methods of sampling in \u201cCommon concerns on representative dataset and multi-class classification\u201d above in the general discussion. To verify the scalability of multi-classes and larger inputs (CIFAR10 and CIFAR100), we add a new Section I in the Appendix with a detailed discussion of the output distribution and representative samples."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184605693,
                "cdate": 1700184605693,
                "tmdate": 1700184676491,
                "mdate": 1700184676491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "razAazLiCT",
                "forum": "ExpwgjvwmC",
                "replyto": "caqszdaRde",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please check if our revision will make our work meet the ICLR bar?"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and feedback! We added experiments for representative datasets (CIFAR100 and CIFAR10 with multi-class classification). We also replied to the scalability problem of sampling for larger input (though this is not the focus of OmniInput), and about the overfitting concern. Could you please check and see if our revision will make our work meet the ICLR bar? Please let us know if you have any questions. Really appreciate that!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457224842,
                "cdate": 1700457224842,
                "tmdate": 1700457224842,
                "mdate": 1700457224842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "poZEwse26E",
            "forum": "ExpwgjvwmC",
            "replyto": "ExpwgjvwmC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission748/Reviewer_XWBZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission748/Reviewer_XWBZ"
            ],
            "content": {
                "summary": {
                    "value": "Different from traditional data-centric evaluation methods based on the pre-defined test-set, this paper delves into model-centric evaluation, where the test-set is self-constructed by the model itself and the model performance is evaluated using the output distribution. Different from other model-centric evaluation methods, this paper leverages the output distribution as a bridge to generalize model evaluation from representative inputs to the entire input spaces."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A sampler is known to estimate the output distribution over the entire input space given a trained model. This paper demonstrates the importance of the sampler in model-centric evaluation frameworks, which is meaningful and inspirable."
                },
                "weaknesses": {
                    "value": "- The core component of the proposed framework, the sampler to estimate the output distribution over the entire input space, just simply follows the existing work [1], which makes the technique contribution not good enough. As mentioned by the authors in the paper, the proposed method is heavily relied on the sampler, while the sampler is simply borrowed from the existing works. To some extents, this paper can be viewed as an application of the sampler in the field of model evaluation. I realize the meaning of the proposed evaluation framework, but the technique contribution is not good enough to reach the bar of ICLR.\n\n- This paper mainly focuses on a simple binary classification task to demonstrate the effectiveness of the proposed evaluation method.\n\n[1] Gradient-based wang-landau algorithm: A novel sampler for output distribution of neural networks over the input space. ICML 2023."
                },
                "questions": {
                    "value": "I will make my final rating after reading the rebuttal from the authors and the reviews from other reviewers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission748/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission748/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission748/Reviewer_XWBZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission748/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699329360774,
            "cdate": 1699329360774,
            "tmdate": 1700657922128,
            "mdate": 1700657922128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7R0lEPhVqr",
                "forum": "ExpwgjvwmC",
                "replyto": "poZEwse26E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reviews!"
                    },
                    "comment": {
                        "value": "(W1) The major technical contribution of this work is a novel approach to quantify model performance across the entire input space, as opposed to a novel sampler. This work is enabled by a good sampler, but the framework proposed in this work is completely new, and its implication transcends a mere application of the sampler. Specifically, the new contributions of this work include:  \n  1. **Model-centric performance evaluation**: different from the conventional practice of hypothesizing potential model failures and subsequently conducting experiments for validation, our methodology revolves around the concept of model-centric evaluations and samples directly from the model under evaluation. The objective is to comprehend the intrinsic biases of the model self-sufficiently, eschewing the conventional approach of relying on human-probed datasets or other models to estimate model performance. This methodological renovation marks a substantial difference from prior works in the field of model evaluation.\n  2. **Model performance available for the entire input space**: the utilization of the model\u2019s output distribution represents an advancement of performance evaluation that extends from representative samples (only for generative models) to the entire input space (for both generative models and classifiers). This is the first time where model performance is estimated for the entire input space of a dataset.\n  3. **Ability to quantify overconfident predictions**: the issue of overconfident predictions has been a longstanding concern in out-of-distribution (OOD) detection. Our contribution is that we are the first to quantitatively assess overconfident predictions by harnessing the output distribution. This novel approach allows for a quantitative comparison between two models, even when overconfident predictions are observed in both models. This is a significant advance in the context of OOD detection. Our framework not only provides a viable approach to quantify overconfident predictions; more importantly, it provides a comprehensive understanding of models\u2019 performance even when overconfident predictions exist for both models.\n\n(W2) We extended the validation of our framework to CIFAR-100 to provide more evidence that our framework is generic to different applications. Please also refer to \u201cCommon concerns on representative dataset and multi-class classification.\u201d To verify the scalability of multi-classes and larger inputs (CIFAR10 and CIFAR100), we add a new Section I in the Appendix with detailed discussion of the output distribution and representative samples."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184370317,
                "cdate": 1700184370317,
                "tmdate": 1700184370317,
                "mdate": 1700184370317,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1UOQYPMuec",
                "forum": "ExpwgjvwmC",
                "replyto": "poZEwse26E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you please check if our revision will make our work meet the ICLR bar?"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and feedback! We added experiments for **representative datasets** (CIFAR100 and CIFAR10) with **multi-class classification**. We also clarified our contribution compared to the previous works. Could you please check and see if our revision will make our work meet the ICLR bar? Please let us know if you have any questions. Really appreciate that!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456873871,
                "cdate": 1700456873871,
                "tmdate": 1700456873871,
                "mdate": 1700456873871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qtsjrJUMw4",
                "forum": "ExpwgjvwmC",
                "replyto": "1UOQYPMuec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_XWBZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_XWBZ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your rebuttal, which addresses parts of my concerns. I would like raise my rating from 5 to 6, and let AC to make the final decision. One more question. This method assumes that all samples in the entire input space appear with equal probability, since we dont know what could be the input for the model. However, most of samples in the entire input space are uninformative and meaningless. The evaluation should bias to those samples with meaningful semantic and structure, although they may appear in different styles, such as a car in agnostic weather or a car drawn in sketch, which appear with higher probability in real world than uninformative noisy samples. Also, I recommend the authors to test their method on more domain generalization datasets, like PACS and DomainNet, since the setting of this paper is more suitable for domain generalization tasks, which assumes the distributino of test data is agnostic."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657890791,
                "cdate": 1700657890791,
                "tmdate": 1700657890791,
                "mdate": 1700657890791,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dmUAMPeKdx",
                "forum": "ExpwgjvwmC",
                "replyto": "poZEwse26E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comment and suggestion. \n\nWe expect the safety issues to be able to deal with unexpectedness of the testing cases, and our framework is able estimate the errors in these applications. While it is hard to say how likely \"a car drawn in sketch\" will appear in the real world application, the model should be able to handle **obvious** issues such as uninformative noise. **This can happen as glitch deal to hardware failures when autonomous car is deployed**. If we are confident that the typical \"styles\" appear often in the applications and the unexpected glitch won't lead to fatal accidents, we can just test on what we believe the model might fail. If that's the application and we can model these typical scenarios using generative models etc, our framework which uses propose samples and output distribution to compute precision and recall still works. The only difference is the entire input space becomes the data space proposed by the generative model. \n\nWe thank the reviewer's recommendation on the test sets. Testing domain generalization datasets will limit our conclusions on those dataset samples, similar to the data space proposed by the generative models and the conclusions will become limited to typical precision/recall on a predefined dataset. Would you like to elaborate what type of conclusions you expect when we test those datasets, as our setting and their setting both seem be \"agnostic\" but there is a signficant motivational difference? \n\nThank you again for your comment and suggestions."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675559679,
                "cdate": 1700675559679,
                "tmdate": 1700675717444,
                "mdate": 1700675717444,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AOmNUmqwRU",
            "forum": "ExpwgjvwmC",
            "replyto": "ExpwgjvwmC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes OMNIINPUT which utilizes the Gradient Wang\u2013Landau sampler to sample representative data and annotate them for model evaluation. Authors validate the framework on MNIST variants. The metrics of mode evaluation involve precision and recall on the representative subpopulation."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-presented with a fair storyline that motivates the work.\n2. The topic of model evaluation with a neural sampler is important.\n3. The experiment design covers a wide aspect of considerations."
                },
                "weaknesses": {
                    "value": "1. Authors should show sufficient validation of the framework. The paper demonstrates validations from original MNIST as in-distribution samples and MNIST variants as out-distribution ones. Limited discussions on CIFAR-10 are shown in the appendix. The paper should conduct more convincing results from representative datasets (e.g., CIFAR-100, Tiny-ImageNet) to validate the framework.\n\n2. How to generalize the framework to the state-of-the-art vision models remains a question. The paper only evaluates ResNet variants and should further involve ViT variants.\n\n3. Existing methods of model-centric evaluations have utilized large generative models to sample OOD instances [1,2,3,4] by optimizing targeted objectives. The paper should discuss the uniqueness/effectiveness of the proposed approach compared to these baselines.\n\n[1] (ECCV 2020) SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing.\n\n[2] (ICCV 2021) Explaining in Style: Training a GAN to explain a classifier in StyleSpace.\n\n[3] (CVPR 2023) Zero-Shot Model Diagnosis.\n\n[4] (NeurIPS 2023) LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images."
                },
                "questions": {
                    "value": "Please address the issues in the weakness section. I will consider revising the rating based on further responses from the authors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission748/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK",
                        "ICLR.cc/2024/Conference/Submission748/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission748/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699491427951,
            "cdate": 1699491427951,
            "tmdate": 1700617338617,
            "mdate": 1700617338617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RqoCAuO2G5",
                "forum": "ExpwgjvwmC",
                "replyto": "AOmNUmqwRU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reviews!"
                    },
                    "comment": {
                        "value": "(W1) This is a good suggestion. We extended the validation of our framework to CIFAR-10 (and SST2) to provide more evidence that our framework is generic to different ML models. Please also refer to the \"Common concerns on representative dataset and multi-class classification\" for how to interpret our results in the experimental section for other datasets, as well as how our method is extended to multi-class classification (applied to CIFAR100). Additional results of CIFAR100 are presented in Section I in the appendix.\n\n(W2) The generalization to state-of-the-art vision models amounts to scaling to large models, which is linearly proportional to the number of parameters, as both forward and backward functions are called during the sampling process. The ViT model and its variants are transformer-based models, which we expect to have similarities to the transformer models we have tested in the NLP application. We believe our framework would be applicable to other models with similar architectures as we treat the middle layers as a complete black-box. \n\n(W3) We added the following discussion in the \u201cRelated works\u201d section in the manuscript. These previous studies have similarly used generated samples for model evaluation and [3] also asserts the ability to assess models without reliance on a designated test set. The rationale of our work is based on a similar model-centric principle; however, our work brings about several novel contributions to existing work:\n  1. Our primary objective is to estimate precisions and recalls for the entire input space instead of only deriving the performance based on representative samples. This necessitates an unbiased selection of samples that are mapped to the same output value by the model to be evaluated, and the output distribution for generalization. In contrast to prior works which perturb attributes to expand samples beyond designated test sets, our study is different in two key aspects: first, the cited papers did not investigate the distribution of representative samples mapped to the same output in the entire input space; second, they did not recognize the implication of the output distribution for evaluating model performance.\n  2. Our approach is different from a perturb-and-evaluate framework, wherein sample generation is the major objective. On the contrary, our emphasis lies in the identification of major biases that models may potentially harbor,  thereby gaining an understanding of the tasks that the models might perform inadequately. This transcends the narrow confines of scrutinizing isolated properties or attributes in other existing works.\n  3. Our approach goes beyond established methods and settings: we are the first to advocate **the consideration of output distribution of a ML model** for assessing its performance and gaining a comprehensive understanding and quantification of the predominant biases inherent in a model. As the intrinsic complexity of our new method far surpasses conventional 'perturb-and-evaluate' methods, we made a deliberate choice to compromise scalability in the current manuscript for the sake of establishing a proof-of-principle of the capability and implication of our framework. Nevertheless, the methodology is generic, generalizable, and scalable to other scenarios when computational resources permit.  \n  4. All the four cited papers used other generators to generate samples for evaluating a model. On the contrary, we used a sampler to sample the model to be evaluated. Sampling is transparent with convergence estimates, but other generators are still considered as black boxes. Given the inherently unknown biases in models such as CLIP[3], StyleGAN[2], LLM[4], and generative models [1,4], utilizing other models to evaluate a model (as explained in the Discussions section in our manuscript) carries the risk of yielding unfair and potentially incorrect conclusions. Our method brings the focus back to the model to be tested, tasking it with generating samples by itself for scrutiny, rather than relying on external agents such as human or other models to come up with testing data. An additional benefit is that this approach offers a novel framework for estimating errors **in the entire input space** when comparing different models."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183034903,
                "cdate": 1700183034903,
                "tmdate": 1700183961397,
                "mdate": 1700183961397,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lGgVpe7A0p",
                "forum": "ExpwgjvwmC",
                "replyto": "7htUtbdCmJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK"
                ],
                "content": {
                    "comment": {
                        "value": "Learning a sufficiently capable generative model is a well-recognized way to model the input data distribution. It is more intriguing to apply fair samplers on the generative models to sample representative subpopulations. However, I appreciate the effort of this detailed response and will raise the rating from reject to borderline reject."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617300415,
                "cdate": 1700617300415,
                "tmdate": 1700617300415,
                "mdate": 1700617300415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FFy4aB3YFB",
                "forum": "ExpwgjvwmC",
                "replyto": "xq9iBRNaPr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK"
                ],
                "content": {
                    "comment": {
                        "value": "To be specific, I would be interested in the evidence of the claim that \"models are hard to be trusted to model the distribution as we might expected, even for the generative models\". Modern large-scale generative models (i.e., Foundation Models) have been empirically shown capable of modeling the data distribution."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619383045,
                "cdate": 1700619383045,
                "tmdate": 1700619383045,
                "mdate": 1700619383045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rvm1dCmj25",
                "forum": "ExpwgjvwmC",
                "replyto": "AOmNUmqwRU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission748/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. Our evaluation framework focuses on the settings where we cannot guarantee any input distribution when the model is deployed. This is extremely important for model safety in the open-world setting where we do not have prior knowledge of what usage cases we might encounter that have not yet been tested. Assuming the data to be tested coming from some input distribution modeled by a generative model limits the generalization of our conclusion. Even if the task is specified and task-specific data distribution is modeled by a generative model, our framework won't change. What is going to change is the sampler that needs to sample according to that distribution, but this is future work not the focus of our framework. \n\nLastly, previous work [1] has shown even (modern?) generative models like VAE, pixelCNN or flow-based models are not (completely) capable of modeling data distribution, \u201cdensity learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.\u201d\n\n[1] Nalisnick, E., Matsukawa, A., Teh, Y. W., Gorur, D., & Lakshminarayanan, B. (2018). Do deep generative models know what they don\u2019t know?. arXiv preprint arXiv:1810.09136."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630284417,
                "cdate": 1700630284417,
                "tmdate": 1700630368176,
                "mdate": 1700630368176,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]