[
    {
        "title": "MaskCLR: Multi-Level Contrastive Learning for Robust Skeletal Action Recognition"
    },
    {
        "review": {
            "id": "A2V1Cu6kQr",
            "forum": "g0KxyULAun",
            "replyto": "g0KxyULAun",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_WKXM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_WKXM"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a new supervised contrastive-learning method via key-point masking to explore richer features from inactivated key-points. The results demonstrate its better performance compared to SoTA skeleton-based action recognition approaches, especially in the robustness."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Main strength points:\n- Better performance, especially in the robustness, which makes the skeleton-based action recognition algorithms more deployable.\n- Comprehensive ablations and experiments for the masking method.\n- Good writing.\n- Codes are available."
                },
                "weaknesses": {
                    "value": "### Main weak point:\n- The paper combine several **existing** methods to achieve better performance. Specifically, an additional path is added to improve the performance and extract more features. However, this duplicates the computational cost. How much performance gain can we get if we **simply use the upper path and make the ST-MSHA block larger**? Fairer comparison should be added under the scenario of **similar computational cost**.\n\nI will be very willing to raise the score if the authors can address my concerns."
                },
                "questions": {
                    "value": "Similar methods can also be applied to non-skeleton based methods. We know the skeleton-based methods are more robust than non-skeleton-based methods. But will that conclusion still hold when we add masking as mentioned in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Reviewer_WKXM"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697493566057,
            "cdate": 1697493566057,
            "tmdate": 1699636128928,
            "mdate": 1699636128928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Bmtzmdfsy",
                "forum": "g0KxyULAun",
                "replyto": "A2V1Cu6kQr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WKXM"
                    },
                    "comment": {
                        "value": "Thank you very much for these informative and constructive comments. We are delighted that you found our work valuable to the community in terms of making skeletal action recognition models more robust and more deployable.\n\nWe would like to clarify some points regarding our method.  \n\n> **P1:** How much performance gain can we get if we simply use the upper path and make the ST-MSHA block larger? \n\n**R:** Making ST-MSHA blocks larger could have multiple interpretations. This includes increasing the number of blocks (depth), increasing the input and output feature dimensions, increasing the number of heads, MLP ratio, etc. Such experimentation has already been conducted by the authors of MotionBERT [1] and is provided in their ablation and appendix sections. The reported accuracy is the one obtained after optimizing all of these parameters to obtain the highest accuracies on the different datasets. We recognize the importance of this point because optimizing these parameters does indeed significantly improve the model performance. Yet, we further improve the model performance using our training approach. Moreover, we boost the model robustness to perturbations such as occlusion and noisy and we improve generalization to pose estimators of different levels of quality. \n\n\n> **P2:** Fairer comparison should be added under the scenario of similar computational cost.\n\n**R:** Thank you for raising this point. As we mention in the paper, our method does not change the number of parameters of the backbone transformer model and does not change the number of FLOPs. Therefore, our comparisons are by default under the same computational cost at test time. However, at training time, our approach requires an extra amount of computation which varies from a model to the other. For MotionBERT, we train for twice the number of epochs of the original model, first training using cross entropy loss then with the combined contrastive approach discussed in the paper. We note that, generally speaking, the training time depends on the backbone model, and is less important than the inference cost, which does not change under our approach. \n\n> **P3:** I will be very willing to raise the score if the authors can address my concerns.\n\n**R:** Thank you very much for the kind gesture. We hope we addressed your concerns. \n\n> **P4:** Similar methods can also be applied to non-skeleton based methods. We know the skeleton-based methods are more robust than non-skeleton-based methods. But will that conclusion still hold when we add masking as mentioned in the paper?\n\n**R:** Indeed, this is a very interesting line of thought. Especially, for RGB-based method transformer-based methods work similarity, which could inspire future work in this domain. We hope that our work would be valuable for the community in other domains as well such as image classification, object detection, and trajectory prediction.\n\n[1] MotionBERT: A Unified Perspective on Learning Human Motion Representations. ICCV'23"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518994782,
                "cdate": 1700518994782,
                "tmdate": 1700518994782,
                "mdate": 1700518994782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dCicmL2xNd",
            "forum": "g0KxyULAun",
            "replyto": "g0KxyULAun",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_vH9p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_vH9p"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the task of skeleton action recognition. Through an analysis of the attention weights of MotionBERT, the authors highlight the model's concentration on a limited set of discriminative joints for action recognition. As a response to this observation, they introduce a novel approach, which involves masking out the most highly activated joint to encourage the model to explore a broader array of informative joints. The proposed method leverages two distinct contrastive loss functions at both sample and class levels. To assess its efficacy, the approach is evaluated on three widely recognized benchmark datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well organized and easy to follow.\n2. The paper presents a commendable analysis and visualization of the attention weights of MotionBERT, offering a deeper insight into the model's inner workings.\n3. Comprehensive experiments are conducted on three benchmark datasets, also with the results on the robustness against skeleton perturbations."
                },
                "weaknesses": {
                    "value": "1. In regard to motivation, the inspiration for this paper stemmed from an in-depth examination of the attention weights employed by MotionBERT. This analysis revealed that MotionBERT focuses on a restricted set of joints for recognition. However, this gave rise to two questions:\n\n(a) The paper primarily centers its analysis and experiments on MotionBERT, leading to doubts about whether the identified issues are prevalent in various backbone architectures rooted in self-attention mechanisms.\n\n(b) It is noteworthy that the proposed method can exclusively be applied to models built upon self-attention. This constraint, in turn, restricts the method's applicability and generalizability to non-transformer backbones.\n\n2. In terms of novelty and contributions, the primary innovation presented in this paper involves the process of masking out the most highly activated joint and employing a contrastive loss function to reduce the dissimilarity between the original joint embedding and the masked counterpart. Although the idea is conceptually straightforward, in my view, its novelty might be somewhat limited for publication at ICLR.\n\n3. About literature review: it's worth noting that certain recent works on skeleton action recognition have been omitted, such as [1-5].\n\n4. About experimental evaluation: it's notable that the authors only present the results for the joint modality. However, it is a standard practice in the field to present results for the multi-stream fusion approach, which encompasses joint, bone, joint motion, and bone motion modalities, as demonstrated in previous studies [1-5]. Furthermore, it's essential to acknowledge that the performance of the proposed method falls short when compared to PoseConv3D on NTU120-XSet and Kinetics400 datasets. On NTU60-XView, the improvement is only marginal.\n\n[1] Huang, et al. \"Graph contrastive learning for skeleton-based action recognition.\" ICLR 2023.\n\n[2] Lee, et al. \"Hierarchically decomposed graph convolutional networks for skeleton-based action recognition.\" ICCV 2023.\n\n[3] Lee, et al. \"Leveraging spatio-temporal dependency for skeleton-based action recognition.\" ICCV 2023.\n\n[4] Wang, et al. \"Neural Koopman Pooling: Control-Inspired Temporal Dynamics Encoding for Skeleton-Based Action Recognition.\" CVPR 2023.\n\n[5] Foo, et al. \"Unified pose sequence modeling.\" CVPR 2023."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698297571930,
            "cdate": 1698297571930,
            "tmdate": 1699636128863,
            "mdate": 1699636128863,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cSW96O1mUa",
                "forum": "g0KxyULAun",
                "replyto": "dCicmL2xNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vH9p (Part 1/2)"
                    },
                    "comment": {
                        "value": "We appreciate your feedback and comments which are paramount in the process of refining and polishing this work. We find your reviews very insightful and speak to the core of this work. Thank you very much for the time and effort you put into reading this paper carefully.\n\nBelow we discuss your concerns on our proposed framework.\n\n> **P1:** (a) The paper primarily centers its analysis and experiments on MotionBERT, leading to doubts about whether the identified issues are prevalent in various backbone architectures rooted in self-attention mechanisms.\n\n**R:** Thank you for this important observation. The reason why we chose MotionBERT is that it\u2019s the best-performing transformer-based model whose implementation is \u2018publicly available\u2019 at the time of the start of this work. MotionBERT core design is very similar to other transformer-based methods in the sense that it uses spatial and temporal MHSA blocks. How these blocks are ordered and fused together is different from method to another, but virtually in all of them you can conveniently compute the attention scores given to every keypoint and hence, you can apply the idea of targeted masking and MLCL quite easily. In support of this statement, and to address your concern about the applicability of our method to other transformer-based backbones, we conduct further experiments on two new transformers: the vanilla transformer [1] and STTFormer [2]. On the vanilla transformer, we apply SkeletonMAE framework (originally proposed for GCN-based methods) using the vanilla transformer backbone. We provide the results below:\n\n**Table 1: Performance improvement (%) compared to transformer-based backbones. Parentheses indicate improvement over the second best method.** \n\n| Method          | Backbone      | NTU60-XSub | NTU60-XView | NTU120-XSub | NTU120-XSet |\n|-----------------|---------------|------------|-------------|-------------|-------------|\n| AimCLR [3]      | STTFormer     | 83.9       | 90.4        | 74.6        | 77.2        |\n| CrossCLR [4]    | STTFormer     | 84.6       | 90.5        | 75.0        | 77.9        |\n| SkeletonMAE [5] | STTFormer     | 86.6       | 92.9        | 76.8        | 79.1        |\n| MaskCLR (Ours)  | STTFormer     | 90.1 (**+ 3.5**)       | 95.4 (**+ 2.5**)       | 79.0      (**+ 2.2**)  | 80.5   (**+ 1.4**)     |\n| SkeletonMAE [5] | Transformer   | 88.5       | 94.7        | 87.0        | 88.9        |\n| MAMP [6]        | Transformer   | 93.1       | 97.5        | 90.0        | 91.3        |\n| MaskCLR (Ours)  | Transformer   | 93.5    (**+ 0.4**)   | 97.5        | 90.5   (**+ 0.5**)     | 91.9    (**+ 0.6**)    |\n\nAs shown, MaskCLR consistently improves the performance of the other transformer-based methods, showing the generalization and efficacy of our approach. We conclude the repones to this point by emphasizing that our focus is not on MotionBERT as an architecture, but on the idea of self-attention which is used in virtually all transformer-based methods. \n\n> **P2:** (b) It is noteworthy that the proposed method can exclusively be applied to models built upon self-attention. This constraint, in turn, restricts the method's applicability and generalizability to non-transformer backbones.\n\n**R:** That\u2019s exactly correct. We present this approach for transformer-based methods as discussed in the paper. The reason is that we take advantage of the attention weights computed by the self-attention mechanism to empirically find the most important joints for targeted masking. However, it\u2019s important to note that the same idea could be applied to other backbones with a slightly different approach. For example, for convolution-based methods such as GCNs and 3D-CNNs, different other techniques could be employed to find the most important joints. CAM [7] and Grad-CAM [8] are some quick and easy-to-apply methods that are proposed to find the saliency regions in images but could be slightly tweaked to be used in the skeleton domain. In doing so, the rest of our approach, i.e., TM and MLCL, could be conveniently applied for convolution-based methods. We leave this for future work.\n\n[1] An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv'20\n\n[2] Spatiotemporal tuples transformer for skeleton-based action recognition. arXiv'22\n\n[3] Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition. AAAI'22\n\n[4] CrossCLR: Cross-modal contrastive learning for multi-modal video representations. ICCV'21\n\n[5] SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training, ICCV'23\n\n[6] Masked motion predictors are strong 3d action representation learners. ICCV'23\n\n[7] Learning Deep Features for Discriminative Localization. CVPR'16\n\n[8] Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. ICCV'17"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518507857,
                "cdate": 1700518507857,
                "tmdate": 1700518551560,
                "mdate": 1700518551560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zleaNAhTrR",
            "forum": "g0KxyULAun",
            "replyto": "g0KxyULAun",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_8Cc6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_8Cc6"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript observes that current transformer-based skeletal action recognition methods tend to focus on a limited set of joints and low-level motion patterns, potentially resulting in performance degradation because of the action perturbation or ambiguousness shown in Fig.1. Therefore, the authors propose a Targeted Masking strategy to occlude the joints with highest activations, and a Multi-Level Contrastive Learning framework to push the original and altered feature embeddings (positive pair) together. Extensive experiments are conducted on NTU60, NTU120, and Kinetics datasets to verify the effectiveness, generalization, and robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "a. The motivation is well justified. \nb. Experiments are sufficient to demonstrate the effectiveness and robustness. \nc. The proposed MaskCLR reaches a new state-of-the-art on the benchmarks in use."
                },
                "weaknesses": {
                    "value": "a.There is potential room for improvement in the writing skills. For example, in the first sentence of the Abstract, it may be more suitable to use \"tend to focus on\" instead of \"focus on\" when discussing existing methods. Given that the critique of previous methods largely stems from qualitative investigations presented in Fig. 1, it would be advantageous to acknowledge the potential influence of individual bias on these findings. Addressing this issue has the potential to enhance the overall quality of the paper.\nb. Activation-guided augmentation is a concept relatively familiar within the vision community. However, it would be valuable to provide a more direct and explicit explanation of this point. Moreover, it may be better to probe some related works in this field in Sec.2, such as [1-2].\n\n[1] He J, Li P, Geng Y, et al. FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 23663-23672.\n[2] Choe J, Shim H. Attention-based dropout layer for weakly supervised object localization[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 2219-2228."
                },
                "questions": {
                    "value": "Consider replacing the term 'multi-level' in the title with 'activation-guided' or similar terminology to better align with the core theme of this manuscript.\nAdditionally, it's worth noting that most related works in the field of contrastive action recognition are self-supervised and primarily evaluated using unsupervised metrics. However, it's important to point out that despite the reviewer's familiarity with supervised contrastive learning, there was still some confusion experienced until the end of Section 2. This suggests that there may be a need for the authors to dedicate more time to refining and clarifying their paper.\nIf the reviewer's understanding is generally accurate, it underscores the importance of improving the paper's clarity and readability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Reviewer_8Cc6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698324760888,
            "cdate": 1698324760888,
            "tmdate": 1699636128801,
            "mdate": 1699636128801,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L9aB2HDuUV",
                "forum": "g0KxyULAun",
                "replyto": "zleaNAhTrR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Reviewer 8Cc6"
                    },
                    "comment": {
                        "value": "We appreciate the time you invested and reading our paper and providing valuable insights that further contribute to the strength of our work. We are glad that you found our work good in terms of soundness, presentation, and contribution. \n\nBelow we address some of your concerns, which we take into consideration to update our paper.\n\n> **P1:** There is potential room for improvement in the writing skills. For example, in the first sentence of the Abstract, it may be more suitable to use \"tend to focus on\" instead of \"focus on\" when discussing existing methods.\n\n**R:** Thank you very much. Indeed \"tend to focus on\" is a better expression in this case. We will update the paper accordingly. \n\n> **P2:** Given that the critique of previous methods largely stems from qualitative investigations presented in Fig. 1, it would be advantageous to acknowledge the potential influence of individual bias on these findings. Addressing this issue has the potential to enhance the overall quality of the paper. \n\n**R:** We appreciate you raising this point. Indeed, it\u2019s an important aspect that need to be discussed and clarified. In the introduction section, we provide some visualizations and observations to motivate our idea. However, \u2018individual bias\u2019 is not the basis on which we develop our work. In the appendix section, particularly in sections A2 and A4, we offer quantitative examination that further validates our observations by reporting the insertion and deletion scores, proposed in RISE [1]. More specifically, we conduct an in-depth experimentation to study the effect of having big dependency on a few set of joints on the performance of the model on the overall NTU60-XSub dataset. For example, in Figure 9 in the appendix, we can see that starting from standard accuracy of 92.8%, removing just 3/17 of the most important joints results in dropping the accuracy to 75.4% and removing 5/17 damages the performance further to 42.3%, which reflects the huge dependency on a few number of joints. In comparison, we see more stable performance in MaskCLR (figure 9 in the appendix), where removing 3/17 and 5/17 joints results in 90.2% and 85.4% in accuracy consecutively. Our motivation is therefore, not just based on qualitative inspection but a quantitative examination of the model using different attention layers and performance evaluations on the entire dataset. \n\n\n> **P3:** b. Activation-guided augmentation is a concept relatively familiar within the vision community. However, it would be valuable to provide a more direct and explicit explanation of this point. Moreover, it may be better to probe some related works in this field in Sec.2, such as [1-2].\n\n**R:** Thank you for this valuable addition. We will take this into consideration in updating our manuscript. \n\n> **P4:** Consider replacing the term 'multi-level' in the title with 'activation-guided' or similar terminology to better align with the core theme of this manuscript. \n\n**R:** This an interesting suggestion. The reason why we termed our technique \u2018muli-level\u2019 is because we use contrastive learning at different levels (sample- and class- levels) as described in the paper.  We think that 'activation-guided' would be an accurate term to describe our targeted masking strategy since it is indeed \u2018activation-guided.\u2019 \n\n\n> **P5:** it's important to point out that despite the reviewer's familiarity with supervised contrastive learning, there was still some confusion experienced until the end of Section 2. \n\n**R:** Your feedback is quite valuable to us. We will revise the paper with more focus on section 2 and before. \n\n[1] Rise: Randomized input sampling for explanation of black-box models. arXiv, 2018."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517804110,
                "cdate": 1700517804110,
                "tmdate": 1700517804110,
                "mdate": 1700517804110,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S9a15Fgfpt",
                "forum": "g0KxyULAun",
                "replyto": "L9aB2HDuUV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Reviewer_8Cc6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Reviewer_8Cc6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful response, which has addressed some of my concerns. \nFrom the reviewer's standpoint, the multi-level design appears to be an incremental contribution, whereas the empirical investigation depicted in Fig. 1, along with the resulting activation-guided contrastive learning, presents a more novel aspect in the realm of skeletal action recognition.\nFor this reason, the reviewer would like to suggest discussing more related works about activation-guided augmentation (not limited to the field of action recognition)  and reorganizing the title. \nIt is acknowledged that the author may hold a different perspective, and the reviewer is open to considering alternative viewpoints.\nThe reviewer is inclined to assign a rating marginally above the borderline for this paper. \nGiven the contribution of introducing novel aspects to action recognition, the reviewer maintains the original rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539456184,
                "cdate": 1700539456184,
                "tmdate": 1700539456184,
                "mdate": 1700539456184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AfC7YAcoYf",
            "forum": "g0KxyULAun",
            "replyto": "g0KxyULAun",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_VBCi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_VBCi"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a masked contrastive learning approach for skeleton-based action recognition.\n\nThe approach employs a targeted masking strategy to obscure significant joints, thereby encouraging the model to learn a broader set of discriminative joints.\n\nAdditionally, a multi-level contrastive learning framework is proposed to enhance the feature embeddings of skeletons. This serves the dual purpose of ensuring compactness within each class and increasing dispersion among different classes.\n\nExtensive experiments are conducted on various benchmarks to demonstrate the improvements over existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Overall the paper is technical sound.\n\n+ Some nice visualizations presented in the paper.\n\n+ Extensive experimental results and comparisons, ablation studies are well presented."
                },
                "weaknesses": {
                    "value": "Major:\n\n- The novelty of this work is limited.\n\n(i) Transformer-based models are not thoroughly discussed and analyzed (which are closely related to the argument 'seamlessly incorporated into transformer-based models' as mentioned in abstract section. For example, [A].\n\n[A] Focal and Global Spatial-Temporal Transformer for Skeleton-based Action Recognition, ACCV'22\n\n(ii) Why only choose MotionBERT? The reviewer noticed that recent works, e.g., [B] and [C], are not discussed in the paper. There are only 3 works referenced from 2023, why?\n\n[B] 3Mformer: Multi-Order Multi-Mode Transformer for Skeletal Action Recognition, CVPR'23\n\n[C] SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training, ICCV'23\n\n(iii) The reviewer noticed that [B] also uses a very similar two pathways setup, the pipeline looks quite similar? (why it is not discussed and it seems that they also use the same dataset for evaluation, and the differences are (1) they use skeletal hypergraph whereas this work uses skeletal graph (2) they use standard classification loss whereas this work applies contrastive learning, so the novelty is the introduced targeted masking strategy?) Also it is a transformer-based model that uses hypergraph? The reviewer noticed that [B] also considers unactivated but informative joints?\n\n(iv) Furthermore, [B] is based on a transformer architecture that utilizes hypergraphs. The question arises as to whether the proposed strategy can be seamlessly integrated into their transformer-based model. \n\n- The justifications of design choice/principle and rationale are not properly illustrated. Why the model are arranged/designed in this way, and its links/relationships to related works are not provided and discussed.\n\n- The maths presented in the paper is confusing and some terms are not explained clearly and properly.\n\n(i) It is suggested to have a notation section for the maths symbols. Why the $_b$ in Eq (1) and (2) sometimes in bold face, it is quite confusing.\n\n(ii) What is the relationship among $\\textbf{A}$, $\\textbf{A}\\_b$, $\\textbf{A}\\_{\\mathcal{S}}$ and $\\textbf{A}\\_{\\mathcal{T}}$. Why $\\textbf{A} = 1/2 * (\\textbf{A}\\_{\\mathcal{S}} + \\textbf{A}\\_{\\mathcal{T}})$, and '*' denotes multiplication? The design choice/principle and rationale are not properly illustrated.\n\n(iii) In Eq. (4) $\\bigotimes$ is not explained. The reviewer noticed that in Fig. 2, it is mentioned but it would be much clearer to have that in main texts.\n\n- Table 2, the comparisons w.r.t. transformer-based (only 1 method is compared) are very limited. Table 4, the red highlighted results are not discussed and analyzed.\n\n- The authors show Fig. 3 and 4, but the discussions and analysis w.r.t. different perspectives / new insights are very limited. It is suggested to add more discussions and analysis.\n\n- The experiments w.r.t. whether the proposed strategy can be applied to transformer-based models are very limited (only 1 model is experimented). It is suggested to explore more transformer-based models.\n\n\nMinor:\n\n- Figure 1 is not very clear to reviewer. What dataset does these videos come from? The color is also not very visible and clearer enough to reviewer. The explanations are very limited.\n\n- Figure 2 some fonts are too smaller to read."
                },
                "questions": {
                    "value": "Please refer to weakness section.\n\nMore questions:\n\n- Page 2, 'multi-level contrastive learning' part, what does this 'class-averaged features' mean?\n\n- In contribution section of Introduction, how 'unactivated' and 'informative' joints are determined and to what extent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698592637263,
            "cdate": 1698592637263,
            "tmdate": 1699636128716,
            "mdate": 1699636128716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JofMQY9ZdF",
                "forum": "g0KxyULAun",
                "replyto": "AfC7YAcoYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VBCi (Part 1/4)"
                    },
                    "comment": {
                        "value": "Thank you for your precious time that you put in reading our paper carefully. We are delighted that you understood the core of our work, and you found our contributions technically sound and the experiments and ablations extensive and demonstrative of performance improvements.\n\nWe find your feedback deep and constructive, and below we address your concerns on our work:\n\n>**P1:** The novelty of this work is limited.\n\n**R:** We respectfully disagree with you in this point. To the best of our knowledge, MaskCLR is the first work to tackle the issue of robustness in transformer-based methods, and the first to address the issue of generalization to pose estimators in all skeletal action recognition methods in general. Our work proposes a new masking technique that is guided by the attention weights from transformer models, which has empirically been shown to result in performance improvements on all benchmarks. This is the first work to highlight this observation and effectively take advantage of it with the MLCL approach to learn the high-level actions semantics instead of low-level joint variations. Finally, we validate our approach with plenty of experiments and ablation studies on three different benchmarks, with three different pose estimators, and under different forms of skeleton perturbations. Our method shows consistent improvements under almost all of the aforementioned cases, which demonstrates and efficacy and generalization of our idea.\n\n>**P2:** Transformer-based models are not thoroughly discussed and analyzed (which are closely related to the argument 'seamlessly incorporated into transformer-based models' as mentioned in abstract section. For example, [A].\n\n**R:** We appreciate providing the reference for FG-STFormer. We believe it\u2019s a relevant and recent transformer-based work that will further contribute to the comprehensiveness of our related works section. We do provide a discussion of the core idea of transformer-based methods in the methods section of our paper, which mainly targets Multi-Head Self-Attention (MHSA) that is used in virtually all transformer methods. The reason why we did not want to delve deeper in the specific design choices of different transformers is that this is not the main focus of our contribution. While our method mainly targets transformer-based methods, it is agnostic to the specific design choices of such method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514111375,
                "cdate": 1700514111375,
                "tmdate": 1700514111375,
                "mdate": 1700514111375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0A4HtKBfX2",
                "forum": "g0KxyULAun",
                "replyto": "AfC7YAcoYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VBCi (Part 2/4)"
                    },
                    "comment": {
                        "value": ">**P3:** (ii) Why only choose MotionBERT? The reviewer noticed that recent works, e.g., [B] and [C], are not discussed in the paper. There are only 3 works referenced from 2023, why?\n\n**R:** Thank you for this important observation. The reason why we chose MotionBERT is that it\u2019s the best-performing transformer-based model whose implementation is \u2018publicly available\u2019 at the time. MotionBERT core design is very similar to other transformer-based methods in the sense that it uses spatial and temporal MHSA blocks. How these blocks are ordered and fused together is different from method to another, but virtually in all of them you can conveniently compute the attention scores given to every keypoint and hence, you can apply the idea of targeted masking and MLCL quite easily. In support of this statement, and to address your concern about the applicability of our method to other transformer-based backbones, we conduct further experiments on two other transformer backbones: the vanilla transformer [1] and STTFormer [2]. On the vanilla transformer, we apply SkeletonMAE framework (originally proposed for GCN-based methods). We could not, however, experiment with 3Mformer because it does not have an open-source implementation. We provide the results below:\n\n**Table 1: Performance improvement (%) compared to transformer-based backbones. Parentheses indicate improvement over the second best method sharing the same backbone.** \n\n| Method          | Backbone      | NTU60-XSub | NTU60-XView | NTU120-XSub | NTU120-XSet |\n|-----------------|---------------|------------|-------------|-------------|-------------|\n| AimCLR [3]      | STTFormer     | 83.9       | 90.4        | 74.6        | 77.2        |\n| CrossCLR [4]    | STTFormer     | 84.6       | 90.5        | 75.0        | 77.9        |\n| SkeletonMAE [5] | STTFormer     | 86.6       | 92.9        | 76.8        | 79.1        |\n| MaskCLR (Ours)  | STTFormer     | 90.1 (**+ 3.5**)       | 95.4 (**+ 2.5**)       | 79.0      (**+ 2.2**)  | 80.5   (**+ 1.4**)     |\n| SkeletonMAE [5] | Transformer   | 88.5       | 94.7        | 87.0        | 88.9        |\n| MAMP [6]        | Transformer   | 93.1       | 97.5        | 90.0        | 91.3        |\n| MaskCLR (Ours)  | Transformer   | 93.5    (**+ 0.4**)   | 97.5        | 90.5   (**+ 0.5**)     | 91.9    (**+ 0.6**)    |\n\nAs shown, MaskCLR consistently improves the performance of the other transformer-based methods, showing the generalization and efficacy of our approach. We conclude the repones to this point by emphasizing that our focus is not on MotionBERT as an architecture, but on the idea of self-attention which is used in virtually all transformer-based methods. \n\n\n[1] An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv'20\n\n[2] Spatiotemporal tuples transformer for skeleton-based action recognition. arXiv'22\n\n[3] Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition. AAAI'22\n\n[4] CrossCLR: Cross-modal contrastive learning for multi-modal video representations. ICCV'21\n\n[5] SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training, ICCV'23\n\n[6] Masked motion predictors are strong 3d action representation learners. ICCV'23"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514442412,
                "cdate": 1700514442412,
                "tmdate": 1700519419136,
                "mdate": 1700519419136,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fn8WRRm1C4",
                "forum": "g0KxyULAun",
                "replyto": "AfC7YAcoYf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VBCi (Part 3/4)"
                    },
                    "comment": {
                        "value": ">**P4:** (iii) The reviewer noticed that [B] also uses a very similar two pathways setup, the pipeline looks quite similar? (why it is not discussed and it seems that they also use the same dataset for evaluation, and the differences are (1) they use skeletal hypergraph whereas this work uses skeletal graph (2) they use standard classification loss whereas this work applies contrastive learning, so the novelty is the introduced targeted masking strategy?) Also it is a transformer-based model that uses hypergraph? The reviewer noticed that [B] also considers unactivated but informative joints?\n\n**R:**  The 3Mformer paper is a very interesting contribution and offers an excellent opportunity to be combined with our approach. We would like to highlight the differences between the contributions of 3Mformer and MaskCLR:\n1. We would like to note that the contribution of 3Mformer is basically a new transformer architecture, unlike our proposed method which is a new framework for training existing transformer-based methods. Consequently, while they use a two-pathway approach in one part of their model, it is quite different from the one we propose in our work. In our framework, the two pathways share the same weights (basically the same model) with different inputs that effectively enrich the learning of information from the input skeletons. In 3Mformer, the two pathways have different weights but the same input (which is the opposite of what we propose.)\n2. We propose to use the attention weights to guide the masking strategy. 3Mformer is employing a significantly different approach with no masking.\n3. We propose a contrastive learning approach, as you indicated, which is quite different from 3Mformer. \n4. It is not clear to us how 3Mformer considers the unactivated joints. While it is reasonable to assume that the usage of hypergraphs would cover more joints and higher order correlations, our work is different in the explicit targeting of the unactivated joints. To the best of our knowledge, our work is the first to take advantage of this technique for skeletal action recognition.\n\n>**P5:** (iv) Furthermore, [B] is based on a transformer architecture that utilizes hypergraphs. The question arises as to whether the proposed strategy can be seamlessly integrated into their transformer-based model.\n\n**R:** The 3Mformer is based on HoT (Higher Order Transformers) which essentially adopts higher-order self-attention to compute attentions weights of joints and hyper-edges at different orders 1,2, \u2026, r. Since the higher-order attention maps reflect the importance of each joint (or group of joints in a hyperedge), our framework can be incorporated on top of 3Mformer in specific and hypergraph-based transformers in general by the targeted masking of the most important joints. Although the implementation of 3Mformer is not available, we will make sure to include hypergraph-based methods as part of our future research and experiments. \n\n>**P6:** The justifications of design choice/principle and rationale are not properly illustrated. Why the model are arranged/designed in this way, and its links/relationships to related works are not provided and discussed.\n\n**R:** We would like to reiterate that the design of the model itself is not the contribution of our work. Our contribution is the proposed framework of training the existing transformer-based models. For further details about the design choices of the model, please refer to the original paper of MotionBERT: https://arxiv.org/pdf/2210.06551.pdf\n\n>**P7:** (i) It is suggested to have a notation section for the maths symbols. Why the b in Eq (1) and (2) sometimes in bold face, it is quite confusing.\n\n**R:** Thank you for highlighting this. We will take your feedback into consideration and fix some of the notations for easier readability. \n\n**P8:** (ii) What is the relationship among, \\( \\mathbf{A} \\), \\( \\mathbf{A}_{b} \\), \\( \\mathbf{A}_{\\mathcal{S}} \\), and \\( \\mathbf{A}_{\\mathcal{T}} \\)? Why \\( \\mathbf{A} = \\frac{1}{2} \\cdot (\\mathbf{A}_{\\mathcal{S}} + \\mathbf{A}_{\\mathcal{T}}) \\), and '*' denotes multiplication? The design choice/principle and rationale are not properly illustrated.\n\n**R:** Here, the idea is to average the spatial and temporal attention weights from the spatial and temporal MHSA blocks. \\( \\mathbf{A}_{\\mathcal{S}} \\) is the spatial attention scores, \\( \\mathbf{A}_{\\mathcal{T}} \\) is temporal attention scores, \\( \\mathbf{A}_{b} \\) refers to both spatial and temporal scores, \\( \\mathbf{A} \\) is the average of spatial and temporal scores. Thank you for pointing this out. While such definitions are all in the paper next to corresponding notations, we will consider having a notations section to further avoid any confusion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515308266,
                "cdate": 1700515308266,
                "tmdate": 1700517182129,
                "mdate": 1700517182129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AURppmtjwI",
            "forum": "g0KxyULAun",
            "replyto": "g0KxyULAun",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_jVKN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1973/Reviewer_jVKN"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript introduces MaskCLR, a training paradigm tailored for skeleton-based action recognition. This approach applies targeted masking to intentionally occlude predominantly activated joints in skeletal sequences. The authors claim that such a strategy facilitates a more comprehensive extraction of pertinent information from input skeleton joints. Complementing this, a multi-tiered contrastive learning architecture is articulated, contrasting skeleton representations at individual sample and class stratifications, culminating in a class-dissociated feature space. The implications of such a design are posited to be enhancements in model accuracy, resilience to noise perturbations, and adaptability across disparate pose estimators. Empirical results suggest that the MaskCLR paradigm exhibits superior performance metrics relative to extant methodologies on specified benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The introduction of MaskCLR offers a distinct approach in skeleton-based action recognition. This work differentiates itself by emphasizing targeted masking, as opposed to the random masking strategies referenced in prior research by Zhu et al., 2023 and Lin et al., 2023. Ablation studies on the NTU60-XSub dataset support the claims, with an in-depth exploration of hyperparameters and a comprehensive evaluation of both Lsc and Lcc losses under varying masking strategies. The approach adopted by MaskCLR, which encodes detailed representations from input skeleton joints, demonstrates robustness against perturbations, contributing to advancements in skeleton-based action recognition."
                },
                "weaknesses": {
                    "value": "The paper's comparative analysis with existing random masking methodologies seems limited in depth, particularly when referencing works like Zhu et al., 2023 and Lin et al., 2023; A more detailed juxtaposition detailing operational and performance nuances would be insightful. Another constraining factor is the exclusive reliance on attention weights from the final ST blocks, with Chefer et al., 2021 suggesting potential benefits from multi-layer attention scores. Furthermore, while the paper claims robustness and generalization, expanded empirical validation across diverse datasets might solidify these assertions. The current exploration of hyperparameters, especially the effects of Lsc weight \u03b1 and Lcc weight \u03b2, could be augmented by examining their combined effects. A deeper theoretical exposition on choices made, especially regarding contrastive losses, could enhance comprehension. Lastly, a rigorous analysis detailing the model's performance under varying noise conditions would strengthen claims about its robustness against noise."
                },
                "questions": {
                    "value": "1.The paper mentions a \"class-dissociated feature space\" as a result of the multi-level contrastive learning framework. Could the authors delve deeper into the tangible benefits of this feature space in comparison to more traditional feature spaces, especially in terms of model interpretability and robustness?\n2.Given the paper's focus on robustness to perturbations, could the authors provide insights into specific types of perturbations where the model excels and where it might face challenges?\n3.The robustness of MaskCLR, especially against varying degrees of noise, is paramount for real-world applicability. How does the model's performance degrade or vary with incremental noise levels in the input data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no Ethics Concerns"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1973/Reviewer_jVKN"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737987026,
            "cdate": 1698737987026,
            "tmdate": 1699636128647,
            "mdate": 1699636128647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jTtYA32ysz",
                "forum": "g0KxyULAun",
                "replyto": "AURppmtjwI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer jVKN (part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read our paper carefully, and for providing valuable feedback. We deeply appreciate your time and efforts, which will greatly contribute to improving this paper.\n\n> **P1:** The paper's comparative analysis with existing random masking methodologies seems limited in depth, particularly when referencing works like Zhu et al., 2023 and Lin et al., 2023; A more detailed juxtaposition detailing operational and performance nuances would be insightful\n\n**R:** We are not sure what you mean in this point. It would be greatly appreciated if you can clarify specific analysis that can contribute to evaluating the masking strategies. We do not only compare against random masking. In the appendix section A4 we study the effect on performance vs the targeted masking of different numbers of joints. The results confirm the validity of our approach in diluting the focus on a small number of joints, exploiting the information carried in other joints. \n\n> **P2:** Another constraining factor is the exclusive reliance on attention weights from the final ST blocks, with Chefer et al., 2021 suggesting potential benefits from multi-layer attention scores.\n\n**R:** Indeed, we mention this aspect as part of the limitations section of the paper. Certainly, more sophisticated saliency methods could be adopted in our framework. For simplicity, and for validating the efficacy of the approach, we experiment with taking the attention weights directly from the final MHSA block. In the Appendix section A2, we study the effect of using the attention weights from different layers and we find that the last MHSA block attention weights are indeed the best to use. We leave the more sophisticated saliency techniques to future work.\n\n> **P3:** Furthermore, while the paper claims robustness and generalization, expanded empirical validation across diverse datasets might solidify these assertions.\n\n**R:** We appreciate your feedback on this point. We want to clarify that we conduct robustness and generalization experiments under three different cases: 1) perturbed skeletons 2) skeletons from different pose estimator between training and testing and 3) perturbed skeletons from different pose estimators. In each case, we report the results under the most popular action recognition datasets, NTU60 and NTU120, under varying degrees of noise, masking, and pose estimator quality levels. Our approach consistently improves the robustness of the backbone model and skeleton perturbations as well as generalization to different pose estimators. We provide even more experiments in the appendix. Kindly let us know if there are specific datasets or different validations that would further confirm the efficacy of our approach. \n\n\n> **P4:** The current exploration of hyperparameters, especially the effects of Lsc weight \u03b1 and Lcc weight \u03b2, could be augmented by examining their combined effects.\n\n**R:** Thank you for raising this point. The current model is tested with different values for \u03b1 and \u03b2 while applying the two losses separately. The search for better hyperparameters could indeed be improved by combining the losses under different weights and other parameters. We will conduct a more exhaustive search in our future work.\n\n> **P5:** A deeper theoretical exposition on choices made, especially regarding contrastive losses, could enhance comprehension.\n\n**R:** Thank you for pointing this out. Our proposed contrastive learning approach is an extension on the previous works on this field, especially on the self-supervised methods that are described in related works section. Due to the space limitation, we cannot delve deep into the details of contrastive learning. However, given your feedback, we will consider extending our work with more discussion on the theoretical background as part of the appendix of our paper. \n\n> **P6:** Lastly, a rigorous analysis detailing the model's performance under varying noise conditions would strengthen claims about its robustness against noise.\n\n**R:** We test our model under different types and degrees of noise from different pose estimators. As shown in section 4.3, our experiments include spatial Gaussian noise, spatiotemporal Gaussian noise, joint masking, part masking, and frame masking. Further, in the appendix section A4, we further evaluate our approach on targeted masking and shifted joints. The results empirically verify the efficacy of our method in improving robustness against different types of noise. We are open to providing further rigorous analysis under varying degrees of methods. We would greatly appreciate it if you point our specific studies or analysis that would further strengthen this work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512194647,
                "cdate": 1700512194647,
                "tmdate": 1700519273384,
                "mdate": 1700519273384,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yVjeg2V8P5",
                "forum": "g0KxyULAun",
                "replyto": "AURppmtjwI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer jVKN (part 2/2)"
                    },
                    "comment": {
                        "value": "> **P7:** The paper mentions a \"class-dissociated feature space\" as a result of the multi-level contrastive learning framework. Could the authors delve deeper into the tangible benefits of this feature space in comparison to more traditional feature spaces, especially in terms of model interpretability and robustness?\n\n**R:** Thank you very much, indeed that\u2019s a very good question. We summarize the benefits of a class-dissociated feature space as follows: 1) a better-clustered feature space contributes to having a better decision boundary between different classes. Hence, this leads to faster convergence by the classification head and better classification performance in terms of top-1 accuracy. 2) Improved Model Interpretability: The \"class-dissociated feature space\" resulting from the multi-level contrastive learning framework signifies a representation where features are disentangled across different classes. This disentanglement can lead to enhanced model interpretability as the learned features may be more semantically meaningful and class-specific. Traditional feature spaces may lack such disentanglement, making it challenging to attribute specific features to particular classes. 3) Enhanced Robustness: The disentanglement of features in the class-dissociated space can contribute to increased robustness. Traditional feature spaces might encode information in a more intertwined manner, making models susceptible to perturbations or noise in the input data. In contrast, a class-dissociated feature space may offer a more resilient representation, where changes in one class do not unduly affect the features relevant to other classes. This characteristic can improve the model's robustness, especially when dealing with noisy or ambiguous data. \n\n> **P8:** Given the paper's focus on robustness to perturbations, could the authors provide insights into specific types of perturbations where the model excels and where it might face challenges? \n\n**R:** According to our experiments, our model performs well under spatial noise (Fig. 3), spatiotemporal noise (Fig. 3), joint masking (Fig. 4), part masking (Fig. 4), Targeted masking (Fig. 9), and shifted joints (Fig. 10). Semi-optimal performance is achieved when evaluating under less than 30% of last frames from the input skeleton sequence. \n\n> **P9:** 3.The robustness of MaskCLR, especially against varying degrees of noise, is paramount for real-world applicability. How does the model's performance degrade or vary with incremental noise levels in the input data?\n\n**R:** As shown in Figures 3, 4, 9, and 10, we evaluate MaskCLR as well as previous SOTA methods under varying degrees of noise. While the performance of all methods rapidly drops with more noise, MaskCLR shows the lowest drop in accuracy. We extend these evaluations with perturbing skeletons from different pose estimators, on which MaskCLR consistently outperforms the previous methods (see Table 4)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512269193,
                "cdate": 1700512269193,
                "tmdate": 1700519315001,
                "mdate": 1700519315001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]