[
    {
        "title": "Exploiting Open-World Data for Adaptive Continual Learning"
    },
    {
        "review": {
            "id": "qmyFY4eild",
            "forum": "3aSbJhaVDi",
            "replyto": "3aSbJhaVDi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2066/Reviewer_58yd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2066/Reviewer_58yd"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies continual learning in open-set scenarios and proposes a prototype-based approach, OpenACL, to leverage open-world data to enhance model performance on new tasks while mitigating catastrophic forgetting. The authors also demonstrate the effectiveness of the proposed method in online continual learning setting through experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- To the best of my knowledge, this paper is the first to combine open semi-supervised learning with continual learning, so the work is novel.\n- The motivation to improve the model's generalization ability to new tasks by leveraging unlabeled OOD data rather than eliminating it directly is reasonable. \n- The paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. My biggest concern is that the problem definition of Open SSCL seems unreasonable. The authors assume that large amounts of unlabeled data can be accessed at any time and are not affected by task switching. Furthermore, the input x for unlabeled and labeled samples comes from the same distribution. Such a setting is incompatible with traditional continual learning to a certain extent, or reduces the difficulty of its core problem, catastrophic forgetting. In particular, the ablation experiments provided by the appendix also show that without unlabeled data, the proposed method is inferior to ER in terms of accuracy.\nIt is recommended that the authors provide more examples or explanations on the practicality and importance of the problem definition, rather than simply combining Open SSL and CL settings.\n\n \n2. The experiments only focus on the setting of online continual learning (OCL), but the selected baselines are not specifically designed for OCL. Could the authors compare the results of offline continual learning? Or Could it be compared with the OCL methods mentioned in [1]? \n\n3. The authors mention that the hyperparameters for baselines are set to the suggested value in their original implementation, but when the experimental settings are different from the original paper, how can the authors ensure that the setting of the hyperparameters is fair?\n\n[1] Wang L, Zhang X, Su H, et al. A comprehensive survey of continual learning: Theory, method and application[J]. arXiv preprint arXiv:2302.00487, 2023."
                },
                "questions": {
                    "value": "1. In Table 1, why are the results worse for Independent using SimCLR/FixMatch?  Could the authors provide further analysis?\n2. The baseline Single denotes training a single network on data from all tasks. Why is it affected by the new task in Figure 2 & 3? (It is recommended that the legend not block the image content)\n3. Although the paper focuses on OCL settings, it seems to be limited to labeled datasets only. Because unlabeled data can be accessed at any time, as more and more tasks are learned, unlabeled data is learned more and more times. Have the authors considered that the unlabeled dataset will also change as the task changes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Reviewer_58yd"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2066/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698238036238,
            "cdate": 1698238036238,
            "tmdate": 1699636138542,
            "mdate": 1699636138542,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8e0kWZk9QO",
                "forum": "3aSbJhaVDi",
                "replyto": "qmyFY4eild",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Review 58yd"
                    },
                    "comment": {
                        "value": "We appreciate the helpful and concrete suggestions. We address your concerns below.\n\n### RE: Concern of the problem definition\n\nIn real applications, especially those involving continual learning, obtaining a steady stream of labeled data can be difficult. This is particularly hard for new or rapidly evolving domains. Continually labeling large datasets can be very expensive and time-consuming, requiring significant human effort, especially for complex tasks or specialized domains. However, it is not as expensive and difficult as labeled data to obtain large amounts of unlabeled data. More importantly, the acquired unlabeled data in practice often come from different (possibly never-before-seen) classes. Therefore, we believe Open SSCL is reasonable and well-motivated by real applications. Also, we argue that these unlabeled data can be effectively utilized to improve model adaptability on future tasks while tackling the catastrophic forgetting issue in continual learning.\n\nRegarding your concerns about the assumption imposed on unlabeled data. We want to clarify that our method does **not** have any restriction on unlabeled data, i.e., we allow unlabeled data to change as task switches and the unlabeled data actually comes from **different** distributions of labeled data. In our experiments, each unlabeled sample is only used **once** throughout the learning process, so the unlabeled dataset used for each task is different. Because unlabeled data includes data from all classes (including known classes, novel classes in future tasks, and OOD classes that never appear in continual learning tasks) while the labeled data do not contain novel/OOD classes, the input distributions for labeled and unlabeled samples can be significantly different.\n\n### RE: without unlabeled data, the proposed method is inferior to ER\n\nWithout unlabeled data, our proposed method (OpenACL) just uses Eq.(2) to optimize the model, and the problem is reduced to conventional continual learning. In this case, OpenACL and ER can attain similar results. The difference is that ER uses the linear transformation to compute the probability for each class while OpenACL uses the cosine similarity to the prototypes to get the probability. However, note that the key novelty of our work is to leverage unlabeled data to improve model adaptability on future tasks while mitigating catastrophic forgetting in continual learning. In the presence of unlabeled data, OpenACL consistently outperforms other algorithms across various datasets.\n\n### RE: Provide more examples or explanations on the practicality and importance of the problem definition\n\nThanks for your suggestion, we have updated a new version and added more explanations in the introduction section."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330342961,
                "cdate": 1700330342961,
                "tmdate": 1700330342961,
                "mdate": 1700330342961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eIIvKUj6hl",
                "forum": "3aSbJhaVDi",
                "replyto": "8e0kWZk9QO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_58yd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_58yd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. The new experiments can further illustrate the effectiveness of the proposed method for the problem Open SSCL raised by the authors. However, I think there are still some unclear points in the definition of the Open SSCL problem and the proposed method, as follows:\n\n1. The authors mentioned that \"continually labeling large datasets can be very expensive and time-consuming, requiring significant human effort, especially for complex tasks or specialized domains\", I think this is the core problem that Open SSL wants to solve. For Open SSCL, in addition to the above issues, we also need to pay attention to the catastrophic forgetting caused by changes in the data flow (including labeled and unlabeled). Although the authors emphasize that they have no restrictions on unlabeled samples, such a setting is too vague and does not allow for further analysis of the degree of forgetting. Moreover, the authors did not provide some actual scenarios to support the practicality and rationality of their settings.\n\t\n2. The authors mentioned that \"each unlabeled sample is only used once throughout the learning process\". Does this mean that each unlabeled sample will only be used once during the learning process of all tasks? If so, how many unlabeled samples will be used in the learning process of each task? How do you select samples? What to do if the task has not been learned yet, but all unlabeled samples have been used? Will the algorithm's effectiveness be affected if different strategies are used for the above questions?\n\t\nBased on the above considerations, I am currently maintaining my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555297746,
                "cdate": 1700555297746,
                "tmdate": 1700555297746,
                "mdate": 1700555297746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lAwwFvoTMT",
                "forum": "3aSbJhaVDi",
                "replyto": "ElsQMV45Uk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_58yd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_58yd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their detailed responses.\n\nFor response 2, can I understand that the 2,625 unlabeled instances accessed when learning for each task include all 20 OOD classes? In other words, is at least one sample of the 20 OOD classes be accessed during each task learning? If so, I'm wondering if the prototype adaptation module can handle the case of unlabeled classes change. For example, the unlabeled samples accessed when learning different tasks are from different classes.\n\nI strongly recommend that the authors describe more clearly how to sample unlabeled data in Section 5.1."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662922688,
                "cdate": 1700662922688,
                "tmdate": 1700662922688,
                "mdate": 1700662922688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HQPrl4RSNq",
            "forum": "3aSbJhaVDi",
            "replyto": "3aSbJhaVDi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2066/Reviewer_rvMJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2066/Reviewer_rvMJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper is concerned with open-world continual learning. The proposed approach (called OpenACL) is based on the assumption that the novel classes in the out-of-distribution (OOD) data for the current task may become training data in future tasks. Instead of identifying and rejecting OOD samples, the authors use them to adapt a model to a new task and improve the model performance in CL. The proposed approach maintains multiple prototypes for seen tasks and reserves extra prototypes for unseen tasks. Both labeled and unlabeled\ndata are learned to improve the adaptation ability and tackle catastrophic forgetting for prototypes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is, in general, clearly written, well-documented and easy to follow. The review of the state of the art covers almost the relevant literature. The experimental results are extensive and demonstrate the superiority of the proposed approach."
                },
                "weaknesses": {
                    "value": "There is a confusion of the concepts and terminology being used (see the Questions section)"
                },
                "questions": {
                    "value": "The authors need to clarify the following aspects:\n- They claim (first paragraph, page 2, middle): \"Instead of identifying and eliminating OOD samples during training, we may leverage...\". The problem of OOD detection is addressed at inference time, not during training. You could simply refer as: unlabeled training data or semi-supervised learning. Please correct this aspect through the paper and clarify which problem you want to address.\n- Since you want to identify and label unknown samples, your problem is related to Novel Class Discovery problem, not OOD detection.\nSee for instance references [1, 2] (below). Please discuss and clarify these aspects in the paper and update accordingly the related work section.\n- The idea of reserving new prototypes for future classes is not new. It was addressed in [3] (see below). However, it limits considerably the generalization capability of your approach. In a real-world problem, you do not know how the distribution of novel classes will look like, so it is not posible to 'pre-define' prototypes. Some other approaches such as self-supervised learning might be preferred instead to discover new clusters in the unlabeled data.\n- Compare your approach with some methods for Novel Class Discovery\n\n\nReferences:\n[1] Subhankar Roy, Mingxuan Liu, Zhun Zhong, Nicu Sebe, Elisa Ricci. Class-incremental Novel Class Discovery (ECCV 2022)\n[2] K J Joseph, Sujoy Paul, Gaurav Aggarwal, Soma Biswas, Piyush Rai, Kai Han, Vineeth N Balasubramanian. Novel Class Discovery without Forgetting (ECCV 2022)\n[3] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, De-Chuan Zhan. Forward Compatible Few-Shot Class-Incremental Learning. (CVPR 2022)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Reviewer_rvMJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2066/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698312897715,
            "cdate": 1698312897715,
            "tmdate": 1700649424426,
            "mdate": 1700649424426,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hcA2eGGsqe",
                "forum": "3aSbJhaVDi",
                "replyto": "HQPrl4RSNq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Review rvMJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and suggestions. We address each of your concerns in the subsequent response.\n\n### RE: The problem of OOD detection is addressed at inference time, not during training.\n\nWe acknowledge the problem of classic OOD detection is addressed at inference time, not during training. However, in a **semi-supervised** open-set problem, it is necessary to consider OOD data during training because unseen OOD data in the unlabeled dataset would harm model training [A]. The problem of *Open-Set/World Semi-Supervised Learning* has been studied in the literature and we have discussed it in the related work section. For example, [A] assigns lower weights for outliers during training to keep the model safety. [B] adds an additional OOD filtering process into the semi-supervised object detection training pipeline to remove OOD instances.\n\nIn this work, we extend the problem to continual learning (CL) and formulate our problem as open semi-supervised continual learning. Similar to [A,B], we also use unlabeled OOD data during training but the main novelty is to exploit these unlabeled OOD data to improve model adaptability on future tasks while simultaneously tackling catastrophic forgetting in CL by using unlabeled ID data.\n\n### RE: Relationship to Novel Class Discovery Problem\n\nOur work is motivated by [C] which also studies the open-world semi-supervised learning problem. Unlike classic novel class discovery that only considers novel classes in unlabeled datasets and often involves a separate training procedure (i.e., it only discovers novel classes \\textit{after} training on labeled datasets), open-world SSL includes both seen classes and unseen classes in unlabeled datasets and does not require separate training.\n\nIn this paper, we extend the open-world semi-supervised learning problem to the continual learning setting and our key novelty is to continually leverage novel classes (OOD data) to enhance the model adaptability during one-time training. Note that other semi-supervised continual learning works with novel classes (e.g., novel class discovery in [1,2], open-set in [D,E]) don't exploit these data to adapt to the task stream. We will add these works you mentioned to the manuscript.\n\nAlthough our work is related to novel class discovery, we want to clarify that the fundamental setting of OpenACL is *Open-world SSL*, **not** novel class discovery. Thus, our work is fundamentally different from [1,2]. Specifically, [1] and [2] explicitly specified the classes in labeled and unlabeled datasets, and assume they are disjoint, i.e., $Y_{lab} \\cap Y_{unlab}= \\emptyset $ . Using the notations in our paper, it would be $C_u \\cap C_l= \\emptyset$ which is different from our setting where $C_u = C_l \\cup  C_n$. Since there is no risk of misclassifying novel classes as known classes when $C_u \\cap C_l= \\emptyset$, the novel class discovery in [1,2] is easier than open-world setting we considered in this work.\n\nIn addition, the novel classes $C_n$ considered in our work consist of both classes in upcoming tasks and OOD classes that never appear in continual tasks. Because the classes of OOD data may be labeled ID classes in upcoming tasks, our method leverages OOD data instead of rejecting them.  Furthermore, under open-world SSL, we can access data for previous tasks so that our prototype learning mechanism can be designed accordingly to mitigate catastrophic forgetting. These significantly differ from novel class discovery."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330172476,
                "cdate": 1700330172476,
                "tmdate": 1700330172476,
                "mdate": 1700330172476,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yar2vtL5XW",
                "forum": "3aSbJhaVDi",
                "replyto": "4AtcE6jdf8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_rvMJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_rvMJ"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer rvMJ"
                    },
                    "comment": {
                        "value": "After carefully considering authors' responses, I can say that they satisfactorily addressed my concerns. Based on this fact, and taking into account also the other reviewers' comments, I have decided to change my initial rating to: 6. marginally above the acceptance threshold."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649571961,
                "cdate": 1700649571961,
                "tmdate": 1700649571961,
                "mdate": 1700649571961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7J9GVFU6qn",
            "forum": "3aSbJhaVDi",
            "replyto": "3aSbJhaVDi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2066/Reviewer_66AU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2066/Reviewer_66AU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the Open SSCL problem, which aims to leverage unlabeled out-of-distribution (OOD) data to assist continual learning, because these data may become in-distribution (ID) at a future task. To tackle this problem, an algorithm, namely Open ACL is discussed. Specifically, Open ACL consists of three steps. First, the cosine distances between data in seen classes and their prototypes are minimized, to improve the perceived similarity on data points in the same seen class. Second, dissimilarity among data in different classes is improved by contrastive learning. Third, novel prototypes are adapted from centroids of K-means. Experiments show that Open ACL is able to defeat baselines in standard image classification SSCL benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper has precisely identified the problem of data shortage in open world continual learning, and proposes to improve learning with additional OOD data. It bridges a gap in state-of-the-art class-incremental continual learning, where OOD data are generally excluded and only ID data is used.\n2. The proposed method, Open ACL aims to group data points in the same class with similar representations, while separating data points in different classes with dissimilar ones. This technique is able to handle OOD data from unseen classes, preparing knowledge for encountering them in future.\n3. The presentation is easy to follow."
                },
                "weaknesses": {
                    "value": "The experiment design seems to be inconsistent with the method. How are unlabeled OOD data being input per task? I assume that for each task, the model will receive some ID data from its current classes, and some OOD data from future classes. However, it seems that all training data points, including labeled and unlabeled, are ID."
                },
                "questions": {
                    "value": "Please respond to the weakness I mentioned above. If this can be clearly addressed, I am willing to improve my rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Reviewer_66AU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2066/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698608495521,
            "cdate": 1698608495521,
            "tmdate": 1700604893198,
            "mdate": 1700604893198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UZgI411SKh",
                "forum": "3aSbJhaVDi",
                "replyto": "7J9GVFU6qn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Review 66AU"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive evaluation of our work. Below is our response to the weakness you mentioned.\n\nOur method takes two datasets as input: labeled $D_l = \\{\\mathcal{D}_l^1,...,\\mathcal{D}_l^k \\}$ and unlabeled $D_u$. For each task $i$, we simultaneously sample data $\\mathcal{D}_l^i$ from the labeled dataset for the current task $i$ and the unlabeled data $D_u$. Note that $D_u$ consists of data from all classes, including previous task classes, current task classes, future task classes (we haven't seen their labels, so they are considered as OOD for this task), and some pure OOD classes that never appear in continual learning tasks. Thus, OOD data for each task $i$ includes two parts: future tasks classes and OOD classes that never appear in continual tasks. During the training, we sampled the data from the unlabeled dataset without knowing the source, i.e., the data comes from previous task classes, current task classes, future task classes, and pure OOD classes. In other words, the training samples are actually the combination of ID and OOD data."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329803225,
                "cdate": 1700329803225,
                "tmdate": 1700329803225,
                "mdate": 1700329803225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y4MxQXUEJ8",
                "forum": "3aSbJhaVDi",
                "replyto": "UZgI411SKh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_66AU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_66AU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I believe this is a very important experiment setup detail, which should be included in the main text (e.g. Section 5.1), so that the readers can find it with ease. If you can update this in your revised draft, I would be happy to raise my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573517424,
                "cdate": 1700573517424,
                "tmdate": 1700573517424,
                "mdate": 1700573517424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cAJ5TbulFM",
                "forum": "3aSbJhaVDi",
                "replyto": "45IDpWf0vK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_66AU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Reviewer_66AU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the update. I have changed my ratings accordingly."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604915647,
                "cdate": 1700604915647,
                "tmdate": 1700604915647,
                "mdate": 1700604915647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wkkYA6bZFV",
            "forum": "3aSbJhaVDi",
            "replyto": "3aSbJhaVDi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2066/Reviewer_9HZN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2066/Reviewer_9HZN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes openACL, the open-world continual learning. The author consider the situation that the training data in one task may come from the open-world dataset (OOD), and considers such OOD data (from novel classes) to mine the underlying pattern in unlabeled open-world data. So that the model\u2019s adaptability to upcoming tasks will be  empowered. At last, the author organizes extensive experiments validate the effectiveness of OpenACL and show the benefit of learning from open-world data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of establish novel prototype from OOD under the semi-supervised pattern is new, it involved the discovery of novel class and also utilize the provided labeled data for seen classes;\n\n2. The experimental setting and result seems well, the result with ablation study, relative discussion make the conclusion reasonable"
                },
                "weaknesses": {
                    "value": "1. The recognition of the OOD/novel class when constructing the novel prototype is not clear, which make the learning process a little confuse. The author mention \"the novel prototypes are used to cluster representations from novel classes\", but under the setting from this paper, that OOD/Novel class may existing with labeled classes data in same task, the author applied the self-supervised learning at first to cluster each class (include OOD) [1], or recognize the novel class after see the labeled classes data [2]?\n\n2. During the prototype-adaptation, the author remain the existing prototypes as static and not subjected to updates post clustering. Here when the model parameters updated and previous prototype may also drift [3], how should the author solve this issue?\n\n3. The idea of applying prototype for incremental learning with open-set recognition is not novelty, especially using the contrastive learning, like [1,2,4], the author should make further discussion and comparison with recent works.\n\n\n[1]. Automatically Discovering and Learning New Visual Categories with Ranking Statistics\n[2]. Few-sample and adversarial representation learning for continual stream mining\n[3]. Semantic Drift Compensation for Class-Incremental Learning\n[4]. P-ODN: Prototype-based Open Deep Network for Open Set Recognition"
                },
                "questions": {
                    "value": "Please see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2066/Reviewer_9HZN"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2066/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699199613034,
            "cdate": 1699199613034,
            "tmdate": 1699636138317,
            "mdate": 1699636138317,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kvvn4e61G8",
                "forum": "3aSbJhaVDi",
                "replyto": "wkkYA6bZFV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2066/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2066/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Review 9HZN"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and for acknowledging the novelty of our work. We address the reviewer's concerns below\n\n### RE: the author applied the self-supervised learning at first to cluster each class (include OOD) [1], or recognize the novel class after see the labeled classes data [2]?\n\nIn each task, there exists labeled classes and OOD/Novel classes. So, we simultaneously have seen prototypes for these labeled classes and the novel prototypes for OOD/Novel classes. OpenACL differs from both [1] and [2] in the sense that we don't separate the training for labeled class data and recognize the novel class. Instead, OpenACL handles classification and discovery at the same time, i.e., it receives both labeled data and unlabeled data in an iteration and is trained from the labeled data (by Eq.(2)) and unlabeled data (by Eq.(5)) **simultaneously**. Particularly, Eq.(5) encourages data with similar representation close to the same prototypes, so data from a novel class would approach the same novel prototype. This is a major difference from novel class discovery where most of the studies consider training on labeled data (or self-supervised training on data) and then recognize the novel class in another unlabeled dataset.\n\n### RE: When the model parameters updated and previous prototype may also drift [3], how should the author solve this issue?\n\nWe keep updating previous prototypes during the whole continual learning process. To achieve that, (1) we use replay memory to store the data of previous tasks to update previous prototypes; (2) we exploit unlabeled $\\mathcal{D}_u$ which consists of data for **previous tasks**, current task, and future tasks. Therefore, our approach ensures that our previous prototypes are dynamic, preventing drift over time.\n\nBoth components update the previous prototypes and help mitigate the drift. In ablation study A.2.2 (Table 5), we showed that by using the unlabeled data, OpenACL has better performance than the OpenACL(S) without using unlabeled data (e.g., it brings 6.2\\% improvement in BWT). Even without using unlabeled data, OpenACL(S) still has positive BWT due to replay memory, which suggests that the previous prototypes also get updated and have better performance."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2066/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329672089,
                "cdate": 1700329672089,
                "tmdate": 1700329672089,
                "mdate": 1700329672089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]