[
    {
        "title": "Graph Convolutions Enrich the Self-Attention in Transformers!"
    },
    {
        "review": {
            "id": "VgUrHVxcyI",
            "forum": "poFAoivHQk",
            "replyto": "poFAoivHQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
            ],
            "content": {
                "summary": {
                    "value": "Recent studies unveil that, much like GNNs, Transformers suffer from the over-smoothing issue, where an increase in depth does not consistently lead to enhanced performance. More disconcerting is the finding that a Transformer with a significantly deeper layer does not outperform a shallower one. Stemming from these observations, the authors interpret self-attention as a form of graph convolution. Viewing through the perspective of graph signal processing, they propose Graph Filter-based Self-Attention (GFSA), where the attention matrix is processed through a trinomial-based graph filter. The experimental results affirm that GFSA substantially elevates the performance of Transformers across various domains."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper encompasses extensive experiments covering a variety of domains including language models, vision Transformers, automatic speech recognition, graph Transformers, and code classifications."
                },
                "weaknesses": {
                    "value": "1. The distinction between self-attention and (directed) graph convolution lies in the requirement for the graph shift operator in (directed) graph convolution to be diagonalizable, a constraint absent in the attention matrix in self-attention. In a directed graph, each signal space -- to which each column of input features belongs -- should remain invariant to filtering [1]. In other words, the signal space housing each column of output vectors should be the same as that of the corresponding input vectors. The sole condition fulfilling this scenario is that the graph shift operator has to be diagonalizable. Hence, unless the authors can demonstrate that the attention matrix in each GFSA is diagonalizable, employing graph signal processing as the theoretical underpinning for GFSA is inappropriate.\n2. The elucidation provided on how GFSA mitigates the over-smoothing issue lacks adequacy. The authors need to furnish a more comprehensive illustration, accompanied by meticulous formulation derivations and rigorous proofs, to substantiate their claims.\n3. The time complexity of GFSA exceeds that of both single-head and multi-head self-attention. Specifically, in scenarios with a large number of tokens, the computation of $\\mathbf{A}^{2}$ will demand a substantial amount of computational resources.\n\n[1] A. Sandryhaila and J. M. F. Moura, \"Discrete Signal Processing on Graphs,\" in IEEE Transactions on Signal Processing, vol. 61, no. 7, pp. 1644-1656, April1, 2013, doi: 10.1109/TSP.2013.2238935."
                },
                "questions": {
                    "value": "1. In (directed) graph convolution, the eigenvalues of the graph shift operator are filtered in accordance with the principles of graph signal processing. Assuming the graph filter can be directly applied to the attention matrix, which segment of the attention matrix is subjected to filtering?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6709/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6709/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6709/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698505035399,
            "cdate": 1698505035399,
            "tmdate": 1700698088663,
            "mdate": 1700698088663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RAiaUepu3B",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your time reading our paper and leaving insightful comments. We uploaded our revised paper, where changes are highlighted in red.\n\n---\n\n**Q1.Hence, unless the authors can demonstrate that the attention matrix in each GFSA is diagonalizable, employing graph signal processing as the theoretical underpinning for GFSA is inappropriate.**\n\nFor our reply to this question, please refer to [Q2.](https://openreview.net/forum?id=poFAoivHQk&noteId=ruiKJRrpJX) of our general response.\n\n---\n\n**Q2. The elucidation provided on how GFSA mitigates the over-smoothing issue lacks adequacy.**\n\nFor our reply to this question, please refer to [Q1.](https://openreview.net/forum?id=poFAoivHQk&noteId=ruiKJRrpJX) and [Q3.](https://openreview.net/forum?id=poFAoivHQk&noteId=ozboZFBU0m) of our general response.\n\n---\n\n**Q3. The time complexity of GFSA exceeds that of both single-head and multi-head self-attention. Specifically, in scenarios with a large number of tokens, the computation of  will demand a substantial amount of computational resources.**\n\nThe approximation for $\\bar{A}^K$ with $A$ and $\\bar{A}^2$ provides a simpler computation that can significantly reduce the required computational resources and time.  Nevertheless, the computational resources may still exist due to matrix multiplication that calculates $\\bar{A}^2$. However, in Tables 18 to 23 in Appendix L, the increases of empirical training time are trivial in most cases. \n\nAlso, to handle a large number of tokens, many studies have proposed Sparse Transformers [1,2,3,4] to reduce computational and memory complexities while maintaining performance by treating attention as a sparse graph. For future work, we hope to develop technical approaches to combine our model with sparse attention.\n\n\n> [1] Manzil Zaheer et al. \"Big bird: Transformers for longer sequences.\" NeurIPS, 2020\n> \n> [2] Iz Beltagy, Matthew E. Peters, and Arman Cohan. \"Longformer: The long-document transformer.\" arXiv preprint arXiv:2004.05150, 2020\n> \n> [3]  Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. \"Reformer: The Efficient Transformer.\" In International Conference on Learning Representations, 2020.\n> \n> [4] Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. \"Fast transformers with clustered attention.\" NeurIPS, 2020.\n\n---\n\n**Q4. In (directed) graph convolution, the eigenvalues of the graph shift operator are filtered in accordance with the principles of graph signal processing. Assuming the graph filter can be directly applied to the attention matrix, which segment of the attention matrix is subjected to filtering?**\n\nFor our reply to this question, please refer to [Q4.](https://openreview.net/forum?id=poFAoivHQk&noteId=ozboZFBU0m) of our general response.\n\n---\nThank you again for taking the time to review our work. If you still have any remaining concerns after our responses, we will do our best to answer them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110007194,
                "cdate": 1700110007194,
                "tmdate": 1700112307406,
                "mdate": 1700112307406,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aVBrpieInK",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors (1/4)"
                    },
                    "comment": {
                        "value": "I recommend rejecting this paper. Before outlining my reasons, I would like to clarify certain concepts in Graph Signal Processing, Directed Graph Signal Processing, and Graph Neural Networks. This clarification is intended to aid readers in better comprehending the arguments I present.\n\n1. graph shift operator: Graph signal processing (GSP) primarily focuses on the analysis of unsigned undirected graphs [1]. A graph shift operator (GSO) [3,4,5], denoted as $\\mathbf{S} \\in \\mathbb{R}^{n \\times n}$, is a matrix that defines how a graph signal transitions from one node to its neighboring nodes, based on the graph topology. In GSP, it is customary to employ a (normalized) adjacency matrix or Laplacian matrix as the GSO [3,4,5]. In Graph Neural Networks (GNNs), a normalized self-looped adjacency matrix or Laplacian matrix is typically employed as the GSO, particularly since the introduction of the so-called renormalization trick in GCN [10]. In GSP, a GSO is symmetric, as the adjacency matrix of an unsigned undirected graph is symmetric.\n\n2. polynomial-based graph filter: In GSP, the concept of a graph filter, denoted as $\\mathcal{H}\\_{\\mathbf{\\Theta}}(\\mathbf{S}) \\in \\mathbb{R}^{n \\times n}$, is interpreted as a function of the GSO [3,4,5]. It is evident that a GSO represents a special case of a graph filter. The polynomial-based graph filter is defined as $\\mathcal{H}\\_{\\mathbf{\\Theta}}(\\mathbf{S}) = \\sum^{K}\\_{k=0}{{\\theta}\\_{k}\\mathbf{S}^{k}}$, where ${\\theta}\\_{k}$ is the corresponding coefficient.\n\n3. graph Fourier transform: In essence, the graph Fourier transform (GFT) is a type of the discrete orthogonal transform [3,4,5]. Owing to the symmetry of the GSO, its eigenvector matrix is orthogonal. Consequently, the discrete orthogonal basis of the GFT is derived from the inverse of the eigenvector matrix of the GSO. For a given graph signal $\\mathbf{X}$, the GFT is expressed as $\\hat{\\mathbf{X}} = \\mathbf{U}^{-1}\\mathbf{X}$, while the inverse graph Fourier transform (IGFT) is represented as $\\mathbf{X} = \\mathbf{U}\\hat{\\mathbf{X}}$.\n\n4. graph convolution: In GNNs, a graph convolution is represented as $\\mathrm{GC}(\\mathbf{S},\\mathbf{X}) = \\mathcal{H}\\_{\\mathbf{\\Theta}}(\\mathbf{S})\\mathbf{X}\\mathbf{W} = \\sum^{K}\\_{k=0}{{\\theta}\\_{k}\\mathbf{S}^{k}}\\mathbf{X}\\mathbf{W}$. Taking GCN [10] as an instance, in GCN, the GSO is a self-looped normalized adjacency matrix, defined as $\\widetilde{\\mathcal{A}} = \\widetilde{\\mathbf{D}}^{-0.5}(\\mathbf{A+\\mathbf{I}\\_{n}})\\widetilde{\\mathbf{D}}^{-0.5}$, where $\\widetilde{\\mathbf{D}}\\_{u,u} = \\sum\\_{v}(\\mathbf{A}+\\mathbf{I}\\_{n})\\_{u,v}$. The first order of the Chebyshev polynomials of the first kind is adopted, i.e., $\\mathrm{GC}(\\widetilde{\\mathcal{A}},\\mathbf{X})\\_{\\text{GCN}} = \\widetilde{\\mathcal{A}}\\mathbf{X}\\mathbf{W}$. In a graph convolution, the graph signal is the linear projection of input feature matrix $\\mathbf{X}\\mathbf{W}$. Since the adjacency matrix of an unsigned undirected graph is symmetric, the GSO is also symmetric, and the eigenvector matrix of the GSO is orthogonal. Consequently, the graph convolution can be represented as $\\mathrm{GC}(\\mathbf{S},\\mathbf{X}) = \\sum^{K}\\_{k=0}{{\\theta}\\_{k}\\mathbf{S}^{k}}\\mathbf{X}\\mathbf{W} = \\mathbf{U}\\left(\\sum\\_{k=0}^{K}\\theta_{k}\\mathbf{\\Lambda}^{k}\\right)\\mathbf{U}^{-1}\\mathbf{X}\\mathbf{W}$. Here, $\\mathbf{U}^{-1}\\mathbf{X}\\mathbf{W}$ represents the GFT of the graph signal. It is \nclear that the eigenvalues of the GSO are filtered by the graph filter. Furthermore, the graph signal undergoes a GFT, after which it is multiplied by the filter eigenvalue matrix. Following this matrix multiplication, it passes through the IGFT."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179344259,
                "cdate": 1700179344259,
                "tmdate": 1700230435497,
                "mdate": 1700230435497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jq7W87gElp",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors (2/4)"
                    },
                    "comment": {
                        "value": "5. directed graph signal processing: For a directed graph, the adjacency matrix is asymmetric. Consequently, a directed graph signal operator (DGSO) is non-diagonalizable. In directed graph signal processing (DGSP), each signal space -- to which each column of input features belongs -- should remain invariant to filtering [3,4,5,6,7]. In other words, the signal space housing each column of output vectors should be the same as that of the corresponding input vectors. The sole condition fulfilling this scenario is that the DGSO has to be diagonalizable. To meet this condition, there are two common approaches. The first is defining a normal DGSO, such as Chung's digraph Laplacian [12] or magnetic Laplacian [13,14]. The second approach involves constructing a normal DGSO. For this method, one can apply the Jordan composition [3,4,5] or Schur decomposition [9] to the non-normal DGSO to reorganize it into a normal DGSO. Taking the Jordan composition to a non-normal DGSO, denoted as $\\mathbf{S}\\_{\\text{nn}} \\in \\mathbb{R}^{n \\times n}$, as an instance, it can be decomposed into $\\mathbf{V}\\mathbf{J}\\mathbf{V}^{-1}$, where $\\mathbf{V}^{-1}$ is the generalized eigenvector matrix of $\\mathbf{S}\\_{\\text{nn}}$, and $\\mathbf{J}$ is a Jordan canonical form matrix with eigenvalues on its diagonal. By retaining the diagonal elements in $\\mathbf{J}$, the eigenvalue matrix $\\mathbf{\\Lambda}$ of $\\mathbf{S}\\_{\\text{nn}}$ is obtained. As a result, a normal DGSO can be constructed as $\\mathbf{S} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^{-1}$. Finally, the directed graph Fourier transform (DGFT) can be expressed as $\\hat{\\mathbf{X}} = \\mathbf{V}^{-1}\\mathbf{X}$, and the inverse directed graph Fourier transform (IDGFT) can be represented as $\\mathbf{X} = \\mathbf{V}\\hat{\\mathbf{X}}$.\n\n6. self-attention and (directed) graph convolution: In self-attention, the attention matrix $\\mathbf{A} = \\mathrm{Softmax}(\\frac{\\mathbf{X}\\mathbf{W}\\_{\\text{Q}}\\mathbf{W}\\_{\\text{K}}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}}{\\sqrt{d}_{k}})$ represents the relationship between the query and the key matrix. When $\\mathbf{W}\\_{\\text{Q}} = \\mathbf{W}\\_{\\text{K}}$, the attention matrix becomes symmetric, indicating that the unidirectional self-attention acts as a graph convolution. As a result, GSP can be used to analyze the unidirectional self-attention. However, when $\\mathbf{W}\\_{\\text{Q}} \\neq \\mathbf{W}\\_{\\text{K}}$, the attention matrix may not be diagonalizable. In this case, the self-attention does not act as a directed graph convolution. Consequently, DGSP cannot be directly applied for analysis."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179435234,
                "cdate": 1700179435234,
                "tmdate": 1700181773490,
                "mdate": 1700181773490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DmabwSsGbF",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors (3/4)"
                    },
                    "comment": {
                        "value": "First, the authors introduce a polynomial-based filter to the attention matrix, grounding its explanation in GSP theory. However, this approach encounters issues when $\\mathbf{W}\\_{\\text{Q}} \\neq \\mathbf{W}\\_{\\text{K}}$, as neither GSP nor DGSP are applicable for analyzing this filter. Notably, neither eigenvalues nor singular values of the attention matrix are filtered. This is why I assert that the authors employ GSP to analyze the filter proposed in the paper is inappropriate. \n\nSecond, the revised version paper discusses two approaches for implementing graph filters: matrix polynomial and graph Fourier transform. Yet, the authors seem to conflate graph filters with the (directed) graph Fourier transform. If the authors persist in incorporating GSP or DGSP for filter analysis, I strongly recommend exploring options like the unidirectional self-attention, proving the attention matrix is diagonalizable when $\\mathbf{W}\\_{\\text{Q}} \\neq \\mathbf{W}\\_{\\text{K}}$, adopting a normal DGSO such as the magnetic Laplacian, reconstruct a diagonalizable DGSO via the Jordan decomposition, or proposing a variant of total variation. \n\nThird, the authors claim that the proposed filter can be reduced to GPR-GNN [11], aiding GFSA in addressing the over-smoothing issue. The complexity of over-smoothing issue is acknowledged. There is substantial theoretical support in GPR-GNN, demonstrating that GPR-GNN can alleviate the over-smoothing issue, even as the order of the polynomial tends towards infinity. Conversely, this paper lacks substantial theory to prove the ability of GFSA to counter over-smoothing. Moreover, the theoretical support in GPR-GNN analyzes filtered eigenvalues in a monomial-based learnable graph filter, a contrast to the eigenvalues or the singular values of the attention matrix is not filtered in the proposed filter. Furthermore, there is scant experimental evidence to support GFSA can alleviate the over-smoothing. A straightforward method to demonstrate counter-over-smoothing capability is by showing that the metric used to judge the model's performance does not decline, which the paper fails to do. For instance, Table 12 shows a significant performance drop in DeiT-T with GFSA from 16-layer to 24-layer on both ImageNet-100 and ImageNet-1k, suggesting an over-smoothing problem that contradicts the authors' claims.\n\nFourth, the authors overlooks the significant time complexity of GFSA, especially when calculating $\\mathbf{A}^{2}$, which has a time complexity of $\\mathcal{O}(n^{3})$. This is substantially higher compared to multi-head self-attention's $\\mathcal{O}(n^{2})$, with GFSA offering only marginal performance improvement at a substantial computational expense. \n\nIn conclusion, considering these four aspects, I advise against accepting this paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179502556,
                "cdate": 1700179502556,
                "tmdate": 1700237046369,
                "mdate": 1700237046369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "apd293FcSH",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors (4/4)"
                    },
                    "comment": {
                        "value": "# Rerference\n[1] Shuman, D. I., Narang, S. K., Frossard, P., Ortega, A., & Vandergheynst, P. (2013). The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Domains. IEEE Signal Processing Magazine, 30(3), 83\u201398.\n\n[2] Marques, A. G., Segarra, S., & Mateos, G. (2020). Signal Processing on Directed Graphs: The Role of Edge Directionality When Processing and Learning From Network Data. IEEE Signal Processing Magazine, 37(6), 99\u2013116.\n\n[3] Sandryhaila, A., & Moura, J. M. F. (2013). Discrete signal processing on graphs: Graph fourier transform. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, 6167\u20136170.\n\n[4] Sandryhaila, A., & Moura, J. M. F. (2013). Discrete Signal Processing on Graphs. IEEE Transactions on Signal Processing, 61(7), 1644\u20131656.\n\n[5] Sandryhaila, A., & Moura, J. M. F. (2014). Discrete Signal Processing on Graphs: Frequency Analysis. IEEE Transactions on Signal Processing, 62(12), 3042\u20133054.\n\n[6] Singh, R., Chakraborty, A., & Manoj, B. S. (2016). Graph Fourier transform based on directed Laplacian. 2016 International Conference on Signal Processing and Communications (SPCOM), 1\u20135.\n\n[7] Deri, J. A., & Moura, J. M. F. (2017). Spectral Projector-Based Graph Fourier Transforms. IEEE Journal of Selected Topics in Signal Processing, 11(6), 785\u2013795.\n\n[8] Domingos, J., & Moura, J. M. F. (2020). Graph Fourier Transform: A Stable Approximation. IEEE Transactions on Signal Processing, 68, 4422\u20134437.\n\n[9] Barrufet, J., & Ortega, A. (2021). Orthogonal Transforms for Signals on Directed Graphs. arXiv Preprint arXiv: Arxiv-2110. 08364.\n\n[10] Kipf, T. N., & Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. International Conference on Learning Representations.\n\n[11] Chien, E., Peng, J., Li, P., & Milenkovic, O. (2021). Adaptive Universal Generalized PageRank Graph Neural Network. International Conference on Learning Representations.\n\n[12] Chung, F. (04 2005). Laplacians and the Cheeger Inequality for Directed Graphs. Annals of Combinatorics, 9(1), 1\u201319.\n\n[13] Fanuel, M., Ala\u00edz, C. M., & Suykens, J. A. K. (02 2017). Magnetic eigenmaps for community detection in directed networks. Phys. Rev. E, 95, 022302.\n\n[14] Fanuel, M., Ala\u00edz, C. M., Fern\u00e1ndez, \u00c1., & Suykens, J. A. K. (2018). Magnetic Eigenmaps for the visualization of directed networks. Applied and Computational Harmonic Analysis, 44(1), 189\u2013199."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180794176,
                "cdate": 1700180794176,
                "tmdate": 1700696052385,
                "mdate": 1700696052385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BPGQiK5cut",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer (1/2)"
                    },
                    "comment": {
                        "value": "**Q1-2. Your concerns on directed graph signal processing**\n\nYou insist that graph signal processing is not possible for directed graphs, which is not true. For instance, Fourier transform is a special case of graph Fourier transform where a directed ring graph is used --- see the graph signal exmaple in the second page of the following link.\n\nhttps://web.media.mit.edu/~xdong/teaching/aims/lab/MT19/AIMS_CDT_SP_MT19_Lab2.pdf\n\nSince you said that the main problem in our paper is that we used directed graphs and graph signal processing cannot be applied in such cases, your review decision in imprecise. I also cite the following article which discusses i) graph signal processing for directed graphs, and ii) graph signal processing for non-diagonalizable cases.\n\nhttps://www.hajim.rochester.edu/ece/sites/gmateos/pubs/dgft/DGFT_SPMAG.pdf\n\n\nWe do not say that we diagonalize the self-attention matrix. This implementaion is not frequently used for deep learning since the diagonalization takes non-trivial computation. Instead, we typically use the matrix polynomial approach to implement graph filters. We summarize our points again as follows.\n\nOur proposed GFSA is designed with matrix polynomials. The main advantage of matrix polynomial filters is that they avoid explicit eigen decomposition while retaining arbitrary filtering capabilities [1]. To reiterate, matrix polynomial-based designs do not require diagonalizable adjacency matrices, whereas explicit graph Fourier transforms do. So our GFSA is not bound by inability to diagonalize self-attention that you are concerned about. Furthermore, as is already well known in the graph community, matrix polynomial filters attract considerable attention in recent years [1-12] and are more widely used than explicit graph Fourier transforms.\n\n> [1] Deyu Bo, Xiao Wang, Yang Liu, Yuan Fang, Yawen Li, and Chuan Shi. \"A Survey on Spectral Graph Neural Networks.\" arXiv preprint arXiv:2302.05631, 2023.\n> \n> [2] Antonio Ortega, Pascal Frossard, Jelena Kova\u010devi\u0107, Jos\u00e9 MF Moura, and Pierre Vandergheynst. \"Graph signal processing: Overview, challenges, and applications.\" Proceedings of the IEEE 106, 2018.\n>\n> [3] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. \"Convolutional neural networks on graphs with fast localized spectral filtering.\" NeurIPS, 2016.\n> \n> [4] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. \"Adaptive Universal Generalized PageRank Graph Neural Network.\" ICLR, 2021.\n> \n> [5] Mingguo He, Zhewei Wei, and Ji-Rong Wen. \"Convolutional neural networks on graphs with Chebyshev approximation, revisited.\" NeurIPS, 2022.\n> \n> [6] Mingguo He, Zhewei Wei, and Hongteng Xu. \"Bernnet: Learning arbitrary graph spectral filters via Bernstein approximation.\" NeurIPS, 2021.\n> \n> [7] Xiyuan Wang and Muhan Zhang. \"How powerful are spectral graph neural networks.\" ICML, 2022.\n> \n> [8] Qimai Li, Xiaotong Zhang, Han Liu, Quanyu Dai, and Xiao-Ming Wu. \"Dimensionwise separable 2-D graph convolution for unsupervised and semi-supervised learning on graphs.\" KDD, 2021\n> \n> [9] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. \"Simplifying graph convolutional networks.\" ICML, 2019.\n> \n> [10] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. \"Deepwalk: Online learning of social representations.\" KDD, 2014.\n> \n> [11] Aditya Grover and Jure Leskovec. \"Node2Vec: Scalable feature learning for networks.\" KDD, 2016.\n> \n> [12] Zhiqian Chen, Fanglan Chen, Lei Zhang, Taoran Ji, Kaiqun Fu, Liang Zhao, Feng Chen, Lingfei Wu, Charu Aggarwal, and Chang-Tien Lu. \"Bridging the gap between spatial and spectral domains: A unified framework for graph neural networks.\" ACM Computing Surveys, 2021."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213168464,
                "cdate": 1700213168464,
                "tmdate": 1700218203336,
                "mdate": 1700218203336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WQxxKrZjch",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I'm very glad to briefly review the 14 references mentioned by the author in the comments. Let us analyze these references one by one and explain the points of confusion and errors made by the authors.\n\n1. In the document available at https://web.media.mit.edu/~xdong/teaching/aims/lab/MT19/AIMS_CDT_SP_MT19_Lab2.pdf, there is a discussion on Erd\u0151s\u2013R\u00e9nyi random graphs and unnormalized/normalized (combinatorial) graph Laplacians. It is important to note that Erd\u0151s\u2013R\u00e9nyi random graphs are typically directed, not undirected. In this document, an Erd\u0151s\u2013R\u00e9nyi random graph is transformed into an undirected graph to derive an unnormalized combinatorial graph Laplacian. In this context, the undirected graph is represented by the symbol $\\mathbf{W}$. Furthermore, the document provides a clear explanation of how the GFT converts a graph signal from the vertex domain to the graph spectral domain. Additionally, it discusses the utilization of a heat kernel filter as a low-pass graph filter, demonstrating the process of filtering the eigenvalues of an unnormalized (combinatorial) graph Laplacian.\n\n2. The document accessible at https://www.hajim.rochester.edu/ece/sites/gmateos/pubs/dgft/DGFT_SPMAG.pdf explicitly states that in directed graph signal processing, the Graph Signal Operator (GSO) ought to be diagonalizable. Furthermore, it notes that in cases where the GSO is not diagonalizable, employing the Jordan decomposition as an alternative approach to the GSO is a viable option.\n\n3. In \"A Survey on Spectral Graph Neural Networks\", it is pointed out that the magnetic Laplacian is adopted in MagNet, a GNN designed for directed graphs. Here the magnetic Laplacian is **diagonalizable**."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232256570,
                "cdate": 1700232256570,
                "tmdate": 1700235431867,
                "mdate": 1700235431867,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SI6DSMZL5p",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "From the perspective of linear algebra, it is OK to construct a matrix polynomial with a directed adjacency matrix. However, it is not permissible to directly apply GSP and DGSP for the analysis of this filter as theoretical support. **More importantly, if the GSO is not diagonalizable, it is neither directed graph convolution nor graph convolution!**"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233236497,
                "cdate": 1700233236497,
                "tmdate": 1700233380191,
                "mdate": 1700233380191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jVbEtt8mIS",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "title": {
                        "value": "You knowledge is incorrect."
                    },
                    "comment": {
                        "value": "For the matrix polynomail approach, the diagonalizability does not need to be assumed. You can find it in many books, papers, and blogs. Please visit the following link as an example. See Section 4 Graph polynomial filter.\n\nhttps://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-spr.2016.0700\n\nThere is a paper [1] about this topic. Its link is https://ieeexplore.ieee.org/document/8309036.\n\n[1] A. Sakiyama, T. Namiki and Y. Tanaka, \"Design of polynomial approximated filters for signals on directed graphs,\" 2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP), Montreal, QC, Canada, 2017, pp. 633-637, doi: 10.1109/GlobalSIP.2017.8309036."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233764600,
                "cdate": 1700233764600,
                "tmdate": 1700234185626,
                "mdate": 1700234185626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HzcWenWsU4",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "OK. Let us dicuss this document which is available at https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-spr.2016.0700 . The document states \"In general, $\\mathbf{A}^{\\mathrm{T}}$ is not exactly diagonalisable and an approximation is used $\\mathbf{A}^{\\mathrm{T}} \\simeq \\mathrm{h}(\\mathbf{\\mathbf{A}}) = \\hat{\\mathbf{V}}\\\\hat{\\mathbf{D}}\\hat{\\mathbf{V}}^{-1}$\". Here, $\\mathrm{h}(\\mathbf{\\mathbf{A}})$ is **diagonalizable**. In this \nsentence, the authors use a diagonalizable operator to approximate a non-diagonalizable operator. Moreover, the authors use $(\\mathbf{I}\\_{n} - \\mathbf{A})^{\\mathrm{T}}(\\mathbf{I}\\_{n} - \\mathbf{A})$ to create a symmetric operator $\\mathbf{B}$ to the graph filter.\n\nAs for anothor paper which is available at https://ieeexplore.ieee.org/document/8309036. The paper states \"We will only consider a\nconnected, finite, directed graph with no multiple and non-negative edges, and we also assume that $\\mathcal{A}$ is **diagonalizable**.\"\n\n**As you can see, both two documents support my arguments: The GSO should be diagonalizable. If the GSO is not diagonalizable, you need to find an approach to rebuild a diagonalizable GSO!**\n\nAs you mentioned that \"For the matrix polynomail approach, the diagonalizability does not need to be assumed. You can find it in many books, papers, and blogs.\", you can provide me any books, papers, and blogs that support your arguments. I am open to discussing whether the GSO should be diagonalizable or not."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234713224,
                "cdate": 1700234713224,
                "tmdate": 1700335409704,
                "mdate": 1700335409704,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8VaKKVMCDK",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Author"
                    },
                    "comment": {
                        "value": "Please find the following sentence in https://arxiv.org/pdf/2211.08854.pdf.\n\nIn the definition of GFT, we are assuming the GSOS is diagonalizable.While definitions of GFT for nondiagonalizable GSOs exist [22], [54], [55], we hold to the diagonalizability assumption for a consistent and simple exposition.\n\nIt says that GFT exists for nondiagonalizable GSOs, but only for ease of discussion, they assume diagonalizable GSOs.\n\nThe theory exists for nondiagonalizable GSOs!\n\nIn addition, there is no problem in using directed adjacency matrix."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235488516,
                "cdate": 1700235488516,
                "tmdate": 1700235562459,
                "mdate": 1700235562459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BqMLYD0DJj",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "In \"Graph Filters for Signal Processing and Machine Learning on Graphs\", it states that \"In the definition of GFT, we are assuming the GSO $\\mathbf{S}$ is diagonalizable. While definitions of GFT for nondiagonalizable GSOs exist [22], [54], [55], we hold to the diagonalizability assumption for a consistent and simple exposition.\", where [22] is A. Sandryhaila and J. M. F. Moura, \"Discrete Signal Processing on Graphs: Frequency Analysis,\" in IEEE Transactions on Signal Processing, vol. 62, no. 12, pp. 3042-3054, June15, 2014, doi: 10.1109/TSP.2014.2321121., [54] is S. Sardellitti, S. Barbarossa and P. D. Lorenzo, \"On the Graph Fourier Transform for Directed Graphs,\" in IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 6, pp. 796-811, Sept. 2017, doi: 10.1109/JSTSP.2017.2726979., and [55] is R. Shafipour, A. Khodabakhsh, G. Mateos and E. Nikolova, \"A digraph fourier transform with spread frequency components,\" 2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP), Montreal, QC, Canada, 2017, pp. 583-587, doi: 10.1109/GlobalSIP.2017.8309026.\n\nEach paper in [22], [54], [55] proposes a variant of total variation, eliminating the need to transform the GSO into a diagonalizable form. In other words, this represents an alternative approach to handling non-diagonalizable GSOs. **Do the authors propose a variant of total variation in the paper?**"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236804351,
                "cdate": 1700236804351,
                "tmdate": 1700236841017,
                "mdate": 1700236841017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SdEpQNYDSI",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "First, the paper available in https://users.ece.cmu.edu/~asandryh/papers/icassp13.pdf utilizes a **diagonalizable** GSO, which is not the case in your paper. Second, in this review, I have never used the word 'mandatory' due to my limited English proficiency."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238078737,
                "cdate": 1700238078737,
                "tmdate": 1700240639745,
                "mdate": 1700240639745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ufzmoteme9",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "title": {
                        "value": "Theoretical Concern"
                    },
                    "comment": {
                        "value": "My core concern is that if we insert a polynomial into GSO without considering whether it needs to be diagonalizable, it could overturn the current theoretical basis of DGSP. This means that for directed graphs or signed directed graphs, we do not need to do any special preprocessing and can directly insert any polynomial for graph representation learning. From this perspective, I must evaluate the paper with great caution."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242406958,
                "cdate": 1700242406958,
                "tmdate": 1700242777020,
                "mdate": 1700242777020,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kCmnHZKe0E",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please check our last messages."
                    },
                    "comment": {
                        "value": "Dear Reviewer kKdF,\n\nThanks for your thorough review to our paper. We uploaded two global messages whose titles are as follows:\n\n1) We left one global message with a title of \"linear, shift-invariant vs. linear, shift-invariant, diagonalizable.\"\n2) We added one additional message to the above global message, whose title is \"Additional justification.\"\n\nWe believe that our messages will address your concerns.\n\nThanks,\nAuthors"
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663200148,
                "cdate": 1700663200148,
                "tmdate": 1700664722724,
                "mdate": 1700664722724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QgUgDNmrZ6",
            "forum": "poFAoivHQk",
            "replyto": "poFAoivHQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_sAYQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_sAYQ"
            ],
            "content": {
                "summary": {
                    "value": "Transformer can be viewed as an attention-based graph neural networks over fully connected graph and thus suffer from over-smoothing problem, that the representations induced by deep layers converge to distinguishable values and further results in performance degradation. Inspired by graph representation learning, the paper introduces high-order neighbor information aggregation in Transformer\u2019s attention layer to relieve over-smoothing. Experimental study demonstrated that the proposed method could improve the performance of Transformer over a variety tasks. The paper is well-written and easy to follow."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.Transformer can be viewed a special case of attention based GNNs. Therefore, introducing graph-filter to enhance Transformer\u2019s attention is promising and effective.\n2.Experiments over variant tasks (CV, NLP and Graph, etc.) demonstrated the proposed method can significantly improve the performance of Transformers."
                },
                "weaknesses": {
                    "value": "1.The bottleneck of standard Transformer in practice is its high time/space complexity O(N^2), the proposed method introduce high-order neighbor information aggregation introduces more computation/memory overheads, even with approximate computation."
                },
                "questions": {
                    "value": "1.How to get the pre-trained large language models integrated with GFSA? Training from scratch or initializing with pre-trained language models and fine-tuning with added GFSA?\n\n2.In section 4.6, the paper talked about the overhead of training time. It is better to add the comparison of inference time.\n\n3.In Eq. (6), do we have any constraints over the coefficients w0, w1, wk? For example w0, w1, wk > 0? In standard Transformer, there is a residual connection in self-attention layer. What is the difference or connection between w0 and this residual connection? \n\nAre these learnable coefficients added to all layers? If yes, are they shared across different layers? Similar questions for multi-head attention, are they shared across different heads?\n\nMissed references:\n1) Graph Attention Networks, ICLR, 2017\n2) Diffusion Improves Graph Learning, NIPS, 2019\n3) Multi-hop Attention Graph Neural Networks, IJCAI 2021"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6709/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698587069385,
            "cdate": 1698587069385,
            "tmdate": 1699636770543,
            "mdate": 1699636770543,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rASsgjIRec",
                "forum": "poFAoivHQk",
                "replyto": "QgUgDNmrZ6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your time reading our paper and leaving insightful comments. We uploaded our revised paper, where changes are highlighted in red.\n\n---\n\n**Q1. The proposed method introduce high-order neighbor information aggregation introduces more computation/memory overheads, even with approximate computation.**\n\nIn Tables 18 to 23 in Appendix L, we report the training time of various methods with GFSA. In general, the training time of methods with GFSA is slightly longer than that of existing methods. For example, the Transformer for the automatic speech recognition task increases from 190.5 seconds to 191.6 seconds on Librispeech 100h dataset, as increases of only 1 second. Instead of computing higher-order polynomial terms, our GFSA approximates them, with only a small increase in runtime, which is not very significant.\n\n---\n\n**Q2. How to get the pre-trained large language models integrated with GFSA?**\n\nThank you for your question. In our proposed method, we can integrate GFSA with a pre-trained large language model (e.g., GPT2) by initializing with the pre-trained models and fine-tuning them with replaced GFSA. We use a weight of pre-trained models provided by HuggingFace (https://huggingface.co/models).\n\n---\n\n**Q3. It is better to add the comparison of inference time?**\n\nWe appreciate the suggestion. In the tables below, we report a comparison of inference time between Transformers with and without GFSA to provide a more comprehensive analysis of the computational implications of GFSA. We included those results in our revised version.\n\nOur inference time remains the same or has a minor increase as seen in text classification. For example, in the CoLA dataset, BERT$_{\\text{BASE}}$ +GFSA only increases by 1 second, or in the MNLI dataset, it increases by 1.3 seconds. However, if the number of tokens exceeds 1000, such as GPT2, the time increases by about 5 seconds, as in WikiText-2. However, considering the performance increase, we believe this increase in inference time is trivial.\n\n\n\n* Inference time on the text classification task\n\n\n| Method | CoLA | SST-2 | MRPC | QQP | STS-B | MNLI-m/mm | QNLI | RTE |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| BERT$_{\\text{BASE}}$ | 1.0s | 1.4s | 1.2s | 48.7s | 1.9s | 15.5s | 10.1s | 1.2s | 10.0s |\n| BERT$_{\\text{BASE}}$ + GFSA | 1.1s | 1.4s | 1.2s | 52.3s | 2.0s | 16.8s | 11.0s | 1.3s | 11.0s |\n| ALBERT$_{\\text{BASE}}$  | 1.1s | 1.6s | 1.4s | 58.4s | 2.2s | 18.4s | 12.1s | 1.3s | 12.0s |\n| ALBERT$_{\\text{BASE}}$  + GFSA | 1.2s | 1.7s | 1.4s | 62.1s | 2.3s | 19.7s | 13.1s | 1.4s | 13.0s |\n| RoBERTa$_{\\text{BASE}}$  | 1.0s | 1.4s | 1.1s | 47.0s | 1.9s | 15.0s | 9.9s | 1.2s | 10.0s |\n| RoBERTa$_{\\text{BASE}}$  + GFSA | 1.1s | 1.4s | 1.2s | 50.4s | 2.0s | 16.3s | 10.8s | 1.2s | 11.0s |\n\n* Inference time on the language modeling task\n\n| Method | PTB | WikiText-2  | WikiText-103 |\n| -------- | -------- | -------- | -------- |\n| GPT2        | 3.2s     | 7.4s     | 7.4s     |\n| GPT2 + GFSA | 5.5s     | 12.9s     | 12.9s     |\n\n\n* Inference time on the image classification task\n\n\n| Method | #Layer | Inference Time|\n| -------- | -------- | -------- |\n| DeiT-S        | 12 | 52s |\n| DeiT-S + GFSA | 12 | 53s |\n| DeiT-S        | 24 | 68s |\n| DeiT-S + GFSA | 24 | 69s |\n| CaiT-S        | 24 | 105s|\n| CaiT-S + GFSA | 24 | 107s|\n| Swin-S        | 24 | 17s |\n| Swin-S + GFSA | 24 | 17s |\n\n* Inference time on the graph classification task\n\n| Method | ZINC | PCQM4M  | PCQM4Mv2 |\n| -------- | -------- | -------- | -------- |\n| Graphormer        | 8s     | 99s     | 31s     |\n| Graphormer + GFSA | 8s     | 117s     | 39s     |\n\n* Inference time on the speech recognition task\n\n| Method | LibriSpeech 100h | LibriSpeech 960h|\n| -------- | -------- | -------- |\n| Transformer         | 328.1s | 323.7s |\n| Transformer + GFSA  | 329.5s | 343.3s |\n| Branchformer        | 299.4s | 328.7s |\n| Branchformer + GFSA | 305.5s | 354.1s |\n\n* Inference time on the code defect prediction task\n\n\n| Method   | Inference Time |\n| -------- | -------- |\n|RoBERTa             | 22.4s |\n|RoBERTa + GFSA      | 23.9s |\n|CodeBERT            | 23.8s |\n|CodeBERT + GFSA     | 24.1s |\n|PLBART      | 37.7s |\n|PLBART + GFSA       | 39.3s |\n|CodeT5-small        | 78.2s |\n|CodeT5-small + GFSA | 82.5s |\n|CodeT5-base         | 83.2s |\n|CodeT5-base + GFSA  | 88.5s |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110288121,
                "cdate": 1700110288121,
                "tmdate": 1700113711717,
                "mdate": 1700113711717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QH7FSpTo3N",
                "forum": "poFAoivHQk",
                "replyto": "QgUgDNmrZ6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4. Are there any constraints on the coefficients $w_0$, $w_1$, $w_K$? For example $w_0, w_1, w_K > 0$? In a standard Transformer, there is residual connectivity in the self-attention layer. What is the difference or connection between $w_0$ and this residual connection?**\n\nThe coefficients $w_0$, $w_1$, and $w_K$ in Eq.(5) are all real numbers without constraints on their values.\nRegarding the connection with the residual connection in the standard Transformer, the role of w0 in our proposed method is similar to the residual connection. The original residual connection adds the input vector $\\mathbf{X}$ to the result of multi-head attention. However, $w_0$ in GFSA adjusts the weight of the value vector $\\mathbf{X}\\mathbf{W}_{val}$ in each head. Therefore, our GFSA has the advantage of being applicable without changing the internal structure of any Transformers except for the self-attention matrix.\n\n---\n\n**Q5. Are these learnable coefficients added to all layers? If yes, are they shared across different layers? Similar questions for multi-head attention, are they shared across different heads?**\n\nThank you for raising these questions. The learnable coefficients are applied to all layers in our proposed method. However, they are not shared across different layers. Each layer has its own set of coefficients. Similarly, for multi-head attention, the learnable coefficients are not shared across different heads. Each head has its own set of coefficients. \n\n---\n\n**Q6. Missed references**\n\nThanks for your suggestion. We have included the missed references you mentioned in our revised paper.\n\n---\n\nWe sincerely hope our response will lead to a better understanding of our proposed method and could lead to a fair and positive review."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110401184,
                "cdate": 1700110401184,
                "tmdate": 1700115181070,
                "mdate": 1700115181070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ANS97yZFa",
                "forum": "poFAoivHQk",
                "replyto": "QH7FSpTo3N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_sAYQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_sAYQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response and answer most of the raised questions. I would like to keep my score."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630871696,
                "cdate": 1700630871696,
                "tmdate": 1700630871696,
                "mdate": 1700630871696,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RIVhyBItKT",
            "forum": "poFAoivHQk",
            "replyto": "poFAoivHQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_huFG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_huFG"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the over-smoothing problem in deep Transformer models, where representations in different layers become too similar and lead to reduced performance. To tackle this, the authors propose graph-filter-based self-attention (GFSA) to preserve diverse features. GFSA is demonstrated to enhance Transformer model performance in various domains, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is easy to read.\n\n2. The paper is well-motivated"
                },
                "weaknesses": {
                    "value": "1. In the paper, there is no theory that explains why transformers suffer from overs-moothing as well as how the proposed method mitigates the issue.\n\n2. The derivation in the paper lacks cohesion, as seen in equation 5, where the authors do not provide an explanation for the first-order approximation of A-bar^K. I am doubtful about the correctness of this equation.\n\n3. Theorem 3.1 establishes an error bound for the Taylor approximation in Eqn 5. However, the error bound presented in Equation 9 relies on ||A-bar^2 - A-bar||, making this error bound meaningless since it lacks any bound for ||A-bar^2 - A-bar||."
                },
                "questions": {
                    "value": "1. The authors\u2019 argument on adding the term (A-bar^2 - A-bar) (see Eqn. 6) in Section 3.3 helps alleviate the over-smoothing problem is not sound. The claim that \u201c(A-bar^2 \u2212 A-bar) captures the difference between 2-hop and 1-hop neighborhood information\u201d and \u201cacts as a filter that emphasizes changes or variations in a local structure\u201d is evidenceless. Graph neural networks (GNNs) typically require multiple hops to aggregate non-local information from distant neighbors, whereas self-attention allows one to attend all tokens simultaneously. This raises the question of why self-attention would need (A-bar^2 - A-bar).\n\n2. When explaining over-smoothing problem in transformers, the authors claim: \u201cThis problem is obvious to understand since Transformers\u2019 aggregation methods for value vectors are simply weighted averages\u201d. This argument is weak and vague. \n\n3. The experimental results display inconsistencies. It is challenging to find the 3.3% improvement mentioned in the introduction for the image classification task, as it is not evident in Figure 1 and Table 3. Likewise, the claim of a 1.63% improvement for DeiT-S + GFSA over DeiT-S does not correspond with the results presented in Table 3. Furthermore, the alleged 6.23% improvement in the natural language understanding task cannot be observed when comparing Figure 1 and Table 5.\n\nIn summary, I recommend rejecting the paper for two primary reasons. First, the paper's novelty and contribution are lacking. The concept of self-attention as a weighted graph has already been discussed in prior works [1, 2, 3, 4]. Although the authors introduce the idea of viewing self-attention as a graph filter, they fail to provide a solid explanation for how this perspective addresses the issue of over-smoothing in Transformers. Second, the paper's theoretical foundation is weak, and it lacks evidence to explain why Transformers experience over-smoothing within their graph-based framework and how their proposed models effectively resolve this problem. Lastly, I find that the main arguments presented in the paper are not convincing.\n\nReferences:\n\n[1] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen Lee, and James T Kwok. Revisiting over-smoothing in BERT from the perspective of graph. ICLR, 2022.\n[2] Yun, Seongjun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. \"Graph transformer networks.\" NeurIPs, 2019.\n[3] Wang, Yuxin, Chu-Tak Lee, Qipeng Guo, Zhangyue Yin, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. \"What dense graph do you need for self-attention?.\" ICML, 2022.\n[4] Velickovic, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. \"Graph attention networks.\". ICLR, 2018."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6709/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828508403,
            "cdate": 1698828508403,
            "tmdate": 1699636770438,
            "mdate": 1699636770438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mtrPtjw5fi",
                "forum": "poFAoivHQk",
                "replyto": "RIVhyBItKT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your time reading our paper and leaving insightful comments. We uploaded our revised paper, where changes are highlighted in red.\n\n---\n\n**Q1. Why transformers suffer from over-smoothing as well as how the proposed method mitigates the issue**\n\nFor our reply to this question, please refer to [Q1.](https://openreview.net/forum?id=poFAoivHQk&noteId=ruiKJRrpJX) and [Q3.](https://openreview.net/forum?id=poFAoivHQk&noteId=ozboZFBU0m) of our general response.\n\n---\n\n**Q2. The derivation in the paper lacks cohesion, as seen in eq. (4), where the authors do not provide an explanation for the first-order approximation of $\\bar{A}^K$. I am doubtful about the correctness of this equation.**\n\nThe general formulation of first-order Taylor approximation at point $a$ is as follows: \n\n$f(x) \\simeq f(a)+f'(a)(x-a)$\n\nWe approximate $f(K)=\\bar{A}^K$ with first-order taylor approximation at point $a=1$:\n\n$f(K)=\\bar{A}^K\\simeq f(1)+f'(1)(K-1).$\n\nInspired by [1], which approximates the derivative of hidden states of RNNs as the difference between hidden states, we effectively approximate $f'(1)$, the derivative of $\\bar{A}^K$ estimated at the position of $K=1$, with the difference term, $f(2)-f(1)=\\bar{A}^2-\\bar{A}^1$. Then the approximated $\\bar{A}^K$ becomes as follows:\n\n$f(K) =\\bar{A}^K\\simeq f(1)+(\\bar{A}^2-\\bar{A})(K-1) = \\bar{A}+(K-1)(\\bar{A}^2-\\bar{A})$\n\nThis approximation has two advantages:\n\n1) The approximation for $\\bar{A}^K$ with $\\bar{A}$ and $\\bar{A}^2$ provides a simpler computation that can significantly reduce the required computational resources and time (see theoretical analysis in section 3.2).\n\n2) When approximating derivative of $\\bar{A}^K$ with the difference term, GPR-GNN and GREAD, two popular GNN methods preventing the oversmoothing for graph convolutional networks, are special cases of our design (see discussion in Section 3.3). In other words, our method is more general than those two GNNs.\n\nIn particular, the second advantage gave us confidence to some degree even before experimenting with our method. We have included the detailed derivation process of the equation in our revised version.\n\n> [1] Edward De Brouwer, et al. \"GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series.\" NeurIPS, 2019.\n\n---\n\n**Q3. the error bound presented in Eq. (7) relies on $||\\bar{A}^2 - \\bar{A}||$, making this error bound meaningless since it lacks any bound for $||\\bar{A}^2 - \\bar{A}||$.**\n\nThank you for pointing it out, we further develop the error bound in Eq. (8) as follows:\n\n$E_K \\leq 2K,$\n\nwhere $K$ is the order of polynomial.\n\nSince $||\\bar{A}||\\leq 1$ and $||\\bar{A}^2||\\leq ||\\bar{A}||^2 \\leq 1^2=1$, the upper bound of $E_k$ is becomes as follows:\n\n\n$E_k \\leq 2+(K-1)||\\bar{A}^2-\\bar{A}||\\leq 2+(K-1)(||\\bar{A}^2||+||\\bar{A}||) \\leq 2+(K-1)(1+1) = 2K.$\n\nWe have included this response in our revised version.\n\n---\n\n**Q4. \u201cGraph neural networks (GNNs) typically require multiple hops to aggregate non-local information from distant neighbors, whereas self-attention allows one to attend all tokens simultaneously. This raises the question of why self-attention would need $\\bar{A}^2 - \\bar{A}$.\"**\n\nFor our reply to this question, please refer to [Q3.](https://openreview.net/forum?id=poFAoivHQk&noteId=ozboZFBU0m) of our general response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110792185,
                "cdate": 1700110792185,
                "tmdate": 1700111919328,
                "mdate": 1700111919328,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gq6sn1cMcD",
                "forum": "poFAoivHQk",
                "replyto": "RIVhyBItKT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q5. When explaining over-smoothing problem in Transformers, the authors claim: \u201cThis problem is obvious to understand since Transformers\u2019 aggregation methods for value vectors are simply weighted averages\u201d. This argument is weak and vague.**\n\nWe apologize for the vagueness of the argument. We corrected the ambiguity in the sentence raised by the reviewer in the revised paper.\n\n---\n\n**Q6. The experimental results display inconsistencies**\n\nThanks for pointing it out. We apologize for one error in the performance improvement percentages we wrote in the Introduction section. We also found that the notation of performance improvements in Table 3 could be confusing, so we modified the name of the rightmost column. We have uploaded the corrected version.\n\n---\n\n**Q7. the concept of self-attention as a weighted graph has already been discussed in prior works [1, 2, 3, 4]**\n\nWe appreciate your evaluation and the concerns raised. In addition to the papers mentioned by the reviewer, research using GAT or self-attention (scaled-dot product attention) as an adjacency matrix for graphs is popular and active.\n\nHowever, our study is different from the research line that experiments on graph tasks by learning the edge weights of the graph from attention. While the concept of self-attention as a weighted graph has been discussed in prior works, our paper extends this idea by introducing the perspective of viewing self-attention as a graph filter. From this perspective, we describe the similarities and differences in the papers mentioned by the reviewer:\n\n- The paper [1], which uses a hierarchical fusion strategy, was inspired by JKNet [5] and used it as is for self-attention of Transformers. On the other hand, we design self-attention from the perspective of a graph filter in graph signal processing.\n- GTN [2] focuses on creating a meta-path adjacency matrix to utilize the graph type of heterophilic nodes. The attention score used by GTN is the attention score for the candidate adjacency matrix when generating the meta-path adjacency matrix, so it is different from the Transformer's self-attention covered in our paper.\n- Hypercube Transformer [3] belongs to the line of research on sparse attention. This model focuses on how dense self-attention should be to reduce complexity and at the same time maintain performance, and does not interpret self-attention from the perspective of a graph filter.\n- GAT [4], the most popular graph neural network, also uses attention scores to consider it as a weighted graph. However, GAT's attention is Bahdanau et al. attention [6] and Vaswani et al. self-attention [7] is different. While GAT is a model for solving the problems of existing spectral and spatial GCNs, we solve the problem of Transformer's self-attention from the perspective of graph signal processing.\n\n> [1] Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen Lee, and James T Kwok. Revisiting over-smoothing in BERT from the perspective of graph. ICLR, 2022. \n> \n> [2] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. \"Graph transformer networks.\" NeurIPS, 2019. \n> \n> [3] Yuxin Wang, Chu-Tak Lee, Qipeng Guo, Zhangyue Yin, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. \"What dense graph do you need for self-attention?.\" ICML, 2022. \n> \n> [4] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. \"Graph attention networks.\". ICLR, 2018.\n> \n> [5] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. \"Representation learning on graphs with jumping knowledge networks.\" ICML, 2018.\n> \n> [6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate.\" ICLR, 2015.\n> \n> [7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" NeurIPS, 2017.\n\n---\n\nWe sincerely hope our response will lead to a better understanding of our proposed method. Please do let us know if there are any remaining concerns that have not been fully addressed in our responses."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111008004,
                "cdate": 1700111008004,
                "tmdate": 1700112530636,
                "mdate": 1700112530636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a4shlF7Hug",
                "forum": "poFAoivHQk",
                "replyto": "RIVhyBItKT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer huFG,\n\nPlease check our above messages prepared solely for you.\n\nIn addition, please check our two global messages in the top of this page. We uploaded two global messages whose titles are as follows:\n\n1) We left one global message with a title of \"linear, shift-invariant vs. linear, shift-invariant, diagonalizable.\"\n2) We added one additional message to the above global message, whose title is \"Additional justification.\"\n\nWe believe that our messages will address your concerns.\n\nThanks,\nAuthors"
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663286727,
                "cdate": 1700663286727,
                "tmdate": 1700664756622,
                "mdate": 1700664756622,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SIM9ZCwu0f",
            "forum": "poFAoivHQk",
            "replyto": "poFAoivHQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_fbar"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_fbar"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel graph filtering approach to enrich the self-attention in Transformer models. The authors show that this improves the performance on 6 different tasks with different types of data. The issue with standard self-attention is that it may oversmooth across different layers (tokens become too similar across layers). In the proposed formulation, the signal for each token is more rich and endowed with both lower and higher frequency information, which mitigates the risk of oversmoothing. The paper is very ambitious in terms of experiments, systematic in its scientific approach, and well-written."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The problem the article addresses is clearly stated (the over-smoothing problem in Transformers.)\n* Clear performance improvement is presented on multiple tasks.\n* The idea is conceptually very appealing.\n* Experiments to ensure that the frequency response is indeed inriched, and that the cosine similarity across layers can be mitigated with GFSA are included in the paper (e.g., Fig 2a-b)\n* Runtime and FLOPS is measured and is thoroughly presented.\n* The approach is well presented and easy-to-grasp, especially with the generous supplementary."
                },
                "weaknesses": {
                    "value": "* Table 5 is the only table to provide uncertainty on the estimates. If this is for computational reasons, it would be good to comment on, either in the main text or in the appendix. Generally, it's good to comment on how secure your estimates are. (I see in the appendix that you specify the fixed seed you run on for the other experiments -- great, but I still maintain my point.)"
                },
                "questions": {
                    "value": "* What do you mean here? \"$w_K$ is learned to negative value\" (page 5, comparison to GCNs)\n* It seems like you are citing Schwartz et al. here, but to me it seems a little strong to claim that \"The computations required for machine learning research are doubling every few months\". If this were true we would very quickly be in a very dangerous situation (if it's true doubling). If not, is it possible to tone down the statement to 'rapidly growing' or similar -- or adding 'currently doubling'.\n\n### Detailed minor comments:\n\n* Did you mean V instead of U after Eq. 3 (page 3)?\n* Page 3: missing capitalization of Equation in multiple places\n* \"latent representations tend to become similar to each other, leading to a loss of distinctiveness\" (I propose to rephrase this sentence, perhaps deleting the second clause as it does not add any new information -- or add 'loss of distinctiveness in the representations'.)\n* English: \"due to the issues\" should be \"due to issues\"\n* \"sometimes uniformity among patches or tokens\" >> would be nice to be a bit more precise here. English-wise, maybe you could say 'sporadic' instead of sometimes if you want an adjective, or delete 'sometimes' entirely.\n* Section 4.4, would it look nicer to write \"graph transformers\" in plural instead of 'graph transformer'? Both in section title and in text\n* I would increase the column width a little bit in Table 3, and put #Layers in plural to be consistent across layers and params."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6709/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698860502269,
            "cdate": 1698860502269,
            "tmdate": 1699636770333,
            "mdate": 1699636770333,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ybkCj8QdNB",
                "forum": "poFAoivHQk",
                "replyto": "SIM9ZCwu0f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your recognition of the contribution and value of our work. We are excited and confident about our ideas and verified performance in the experiment section. This is because the frequency response in Figure 2 actually becomes richer and mitigates the cosine similarity.\n\nWe uploaded our revised paper, where changes are highlighted in red.\n\n---\n\n**Q1. Uncertainty on the estimates in Table 5**\n\nWe apologize for confusion. We conducted experiments following the experimental environments of Graphormer using 4 different seeds and reported mean values of results. However, as you pointed out, there is an inconsistency in that the uncertainty on the estimations is reported only in Table 5. To address this, we have removed the uncertainty from Table 5. Due to space constraints, we include the results with uncertainty in Tables 14 and 15 in Appendix of the revised version. \n\n---\n\n**Q2. The meaning of \u201cis learned to negative value\u201d in page 5.**\n\nIn GFSA, the coefficient $w_K$ can be learned to have both positive and negative values. The negative value of $w_K$ allows GFSA to extract high-frequency information from the value vectors, which is different from self-attention which can extract only low-frequency information.\n\n---\n\n**Q3. In the ethical statements section, is it possible to tone down the statement to \u2018rapidly growing\u2019 or similar \u2013 or add \u2018currently doubling\u2019?**\n\nThank you for the suggestion. We agree that the statement can be toned down to avoid potential misinterpretation. We will revise the expression to convey that the computations required for machine learning research are rapidly growing.\n\n---\n\n**Minor comments**\n\nAll minor comments you raised have been reflected in the revised paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111186321,
                "cdate": 1700111186321,
                "tmdate": 1700111385014,
                "mdate": 1700111385014,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hf97yFjapC",
            "forum": "poFAoivHQk",
            "replyto": "poFAoivHQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
            ],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
            ],
            "content": {
                "title": {
                    "value": "Theoretical Concerns From Directed Graph Signal Processing"
                },
                "comment": {
                    "value": "Dear reviewers and area chairs:\n\nI am writing to present an assessment of this paper which introduces a novel approach termed Graph Filter-based Self-Attention (GFSA), predicated on what the authors refer to as a matrix polynomial methodology. This paper considers the attention matrix of Self-Attention as a directed graph. Drawing upon my extensive understanding of Directed Graph Signal Processing (DGSP), it is commonly acknowledged that the preferred method for addressing a directed graph involves the transformation of the Directed Graph Signal Operator (DGSO) into a diagonalizable form, particularly when the DGSO is non-diagonalizable [1-11]. This process entails incorporating the diagonalizable DGSO into the graph filter, underpinned by the rationale that each signal space \u2013 corresponding to individual columns of input features \u2013 should maintain its inherent characteristics post-filtering. In simpler terms, the signal space encompassing each column of the output vectors should align seamlessly with that of the input vectors. This preservation of signal space is contingent upon the diagonalizability of the DGSO [2,3,4].\n\nThe authors, however, elect to bypass this critical preprocessing stage of transforming the DGSO into a diagonalizable form. Instead, they directly implement the non-transformed DGSO in a filter, asserting that the resulting GFSA still qualifies as a graph convolution. It is imperative to note that such a procedure should more accurately be described as a directed graph convolution. While this methodology does demonstrate commendable performance, it fundamentally challenges and potentially disrupts the established theoretical framework of DGSP. This divergence from conventional theory leaves me, and potentially others in the field, unable to satisfactorily reconcile the authors' approach with existing DGSP principles.\n\nGiven the potential impact and the likely extensive citations this paper could garner across both Graph Neural Networks (GNNs) and DGSP domains if accepted, I feel compelled, in my capacity as a reviewer for ICLR 2024, to urge my fellow reviewers and area chairs to approach the evaluation of this submission with heightened scrutiny and consideration. The implications of endorsing a paper that deviates so significantly from accepted theoretical underpinnings should be weighed carefully, to ensure the integrity and forward progression of our field.\n\n#### Reference\n> [1] Marques, A. G., Segarra, S., & Mateos, G. (2020). Signal Processing on Directed Graphs: The Role of Edge Directionality When Processing and Learning From Network Data. IEEE Signal Processing Magazine, 37(6), 99\u2013116.\n>\n> [2] Sandryhaila, A., & Moura, J. M. F. (2013). Discrete signal processing on graphs: Graph fourier transform. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, 6167\u20136170.\n>\n> [3] Sandryhaila, A., & Moura, J. M. F. (2013). Discrete Signal Processing on Graphs. IEEE Transactions on Signal Processing, 61(7), 1644\u20131656.\n>\n> [4] Sandryhaila, A., & Moura, J. M. F. (2014). Discrete Signal Processing on Graphs: Frequency Analysis. IEEE Transactions on Signal Processing, 62(12), 3042\u20133054.\n>\n> [5] Singh, R., Chakraborty, A., & Manoj, B. S. (2016). Graph Fourier transform based on directed Laplacian. 2016 International Conference on Signal Processing and Communications (SPCOM), 1\u20135.\n>\n> [6] Deri, J. A., & Moura, J. M. F. (2017). Spectral Projector-Based Graph Fourier Transforms. IEEE Journal of Selected Topics in Signal Processing, 11(6), 785\u2013795.\n>\n> [7] Domingos, J., & Moura, J. M. F. (2020). Graph Fourier Transform: A Stable Approximation. IEEE Transactions on Signal Processing, 68, 4422\u20134437.\n>\n> [8] Barrufet, J., & Ortega, A. (2021). Orthogonal Transforms for Signals on Directed Graphs. arXiv Preprint arXiv: Arxiv-2110. 08364.\n>\n> [9] Chung, F. (04 2005). Laplacians and the Cheeger Inequality for Directed Graphs. Annals of Combinatorics, 9(1), 1\u201319.\n>\n> [10] Fanuel, M., Ala\u00edz, C. M., & Suykens, J. A. K. (02 2017). Magnetic eigenmaps for community detection in directed networks. Phys. Rev. E, 95, 022302.\n>\n> [11] Fanuel, M., Ala\u00edz, C. M., Fern\u00e1ndez, \u00c1., & Suykens, J. A. K. (2018). Magnetic Eigenmaps for the visualization of directed networks. Applied and Computational Harmonic Analysis, 44(1), 189\u2013199."
                }
            },
            "number": 27,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700245142882,
            "cdate": 1700245142882,
            "tmdate": 1700245142882,
            "mdate": 1700245142882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "v3087vZlI7",
            "forum": "poFAoivHQk",
            "replyto": "poFAoivHQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
            ],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
            ],
            "content": {
                "title": {
                    "value": "Summry of the authors' misunderstandings"
                },
                "comment": {
                    "value": "The origin of GNNs is generally accepted to be the Graph Convolutional Network (GCN) [1]. Previously, similar neural networks existed but were known as CNNs on graphs [2][3]. GCNs primarily study undirected graphs, characterized by symmetric adjacency matrices. In linear algebra, symmetric matrices are diagonalizable. This principle is foundational in subsequent studies like SGC [4], GDC [5], APPNP [6], SSGC [7], and GPR-GNN [8], all of which focus on undirected graphs. This approach is in line with graph signal processing, where the self-looped normalized adjacency matrix of an undirected graph is used in polynomial-based filters to boost a model's expressiveness.\n\nHowever, when it comes to directed graphs, the scenario changes. Directed graphs have asymmetric adjacency matrices, which are non-diagonalizable. In directed graph signal processing, using such an adjacency matrix directly as a shift operator in a polynomial-based filter does not filter the eigenvalues of the adjacency matrix. Prior to this paper, research on GNNs for directed graphs, like MagNet [9], employed the magnetic adjacency matrix with a self-loop as the directed graph shift operator (DGSO) in Chebyshev polynomials of the first kind, essentially using a diagonalizable DGSO.\n\nThe authors persist in refusing to differentiate between undirected and directed graphs, as well as between GNNs that process these two types of graphs. Models like GCN, SGC, GDC, APPNP, SSGC, and GPR-GNN, designed for undirected graphs, incorporate a self-looped normalized adjacency matrix into a polynomial-based filter, consistent with graph signal processing principles. However, for directed graphs, these models are inapplicable. This has led to the development of GNNs specifically for directed graphs, like DiGCN [10], MagNet, and Signed Graph Neural Networks [11], which use a diagonalizable adjacency matrix of a directed graph with self-loop.\n\nFurthermore, in this review, I never wrote that $y = \\mathbf{V}\\left(\\sum\\_{k=0}^{K}w_{k}\\mathbf{\\Lambda}^{k}\\right)\\mathbf{V}^{-1}\\mathbf{x}$ was the first invented. The authors misrepresented my meaning. I dispute the notion that the Graph Fourier Transform (GFT) predates polynomial-based graph filters. The relationship between the two is synergistic rather than hierarchical. It's crucial to clarify that GFT lays the groundwork for understanding polynomial-based graph filters, rather than being a direct method of implementation. Disappointingly, the author remains confined to the theoretical framework of graph signal processing in their proposed GFSA, inaccurately using GNNs for undirected graphs as examples.\n\nIn conclusion, I strongly advise the authors to recognize the distinct differences between undirected and directed graphs, the GNNs that process each type, and the separate fields of graph signal processing and directed graph signal processing."
                }
            },
            "number": 30,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700362879915,
            "cdate": 1700362879915,
            "tmdate": 1700695475660,
            "mdate": 1700695475660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q2YTA1hrHK",
                "forum": "poFAoivHQk",
                "replyto": "v3087vZlI7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "### Reference\n[1] Kipf, T. N., & Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. International Conference on Learning Representations.\n\n[2] Bruna, J., Zaremba, W., Szlam, A., & LeCun, Y. (2014). Spectral Networks and Locally Connected Networks on Graphs. In Y. Bengio & Y. LeCun (Eds.), International Conference on Learning Representations.\n\n[3] Defferrard, M., Bresson, X., & Vandergheynst, P. (2016). Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, & R. Garnett (Eds.), Advances in Neural Information Processing Systems (Vol. 29). Curran Associates, Inc.\n\n[4] Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., & Weinberger, K. (06 2019). Simplifying Graph Convolutional Networks. In K. Chaudhuri & R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning (pp. 6861\u20136871). PMLR.\n\n[5] Gasteiger, J., Wei\u00df enberger, S., & G\u00fcnnemann, S. (2019). Diffusion Improves Graph Learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\textquotesingle Alch\u00e9-Buc, E. Fox, & R. Garnett (Eds.), Advances in Neural Information Processing Systems (Vol. 32). Curran Associates, Inc.\n\n[6] Gasteiger, J., Bojchevski, A., & G\u00fcnnemann, S. (2019). Combining Neural Networks with Personalized PageRank for Classification on Graphs. International Conference on Learning Representations.\n\n[7] Zhu, H., & Koniusz, P. (2021). Simple Spectral Graph Convolution. International Conference on Learning Representations.\n\n[8] Chien, E., Peng, J., Li, P., & Milenkovic, O. (2021). Adaptive Universal Generalized PageRank Graph Neural Network. International Conference on Learning Representations.\n\n[9] Zhang, X., He, Y., Brugnone, N., Perlmutter, M., & Hirn, M. (2021). MagNet: A Neural Network for Directed Graphs. In A. Beygelzimer, Y. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems.\n\n[10] Tong, Z., Liang, Y., Sun, C., Li, X., Rosenblum, D., & Lim, A. (2020). Digraph Inception Convolutional Networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, & H. Lin (Eds.), Advances in Neural Information Processing Systems (Vol. 33, pp. 17907\u201317918). Curran Associates, Inc.\n\n[11] Singh, R., & Chen, Y. (2023). Signed Graph Neural Networks: A Frequency Perspective. Transactions on Machine Learning Research."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363581048,
                "cdate": 1700363581048,
                "tmdate": 1700363604329,
                "mdate": 1700363604329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jb7fnR0av7",
                "forum": "poFAoivHQk",
                "replyto": "VgUrHVxcyI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your responses. I believe that time will be the best judge of this paper's merits. Ultimately, I decided to increase the score.\n\nBest wishes,\n\nReviewer kKdF"
                    }
                },
                "number": 38,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690664186,
                "cdate": 1700690664186,
                "tmdate": 1700698137034,
                "mdate": 1700698137034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GwqK0g2W5B",
                "forum": "poFAoivHQk",
                "replyto": "FWMmBOSJrP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "I am very upset that Reviewer fbar described me as 'were so thoroughly skeptic, and not acknowledging any other positive aspects of the paper other than the experimental', just because I questioned the author's mistakes from a professional graph signal processing perspective. As a reviewer for ICLR 2024, it is my responsibility to question the author's oversights from a professional perspective. Furthermore, I am very grateful for the defense presented by the authors in this rebuttal."
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693797654,
                "cdate": 1700693797654,
                "tmdate": 1700695818166,
                "mdate": 1700695818166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uSxuRAXSZh",
                "forum": "poFAoivHQk",
                "replyto": "xOShx1F9Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing your justification. It helps in understanding your perspective and approach. I am very surprised by the paper [1]. Ultimately, I decided to increase the score.\n\n[1] Maskey, S., Paolino, R., Bacho, A., & Kutyniok, G. (2023). A Fractional Graph Laplacian Approach to Oversmoothing. Thirty-Seventh Conference on Neural Information Processing Systems."
                    }
                },
                "number": 40,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694554418,
                "cdate": 1700694554418,
                "tmdate": 1700698377199,
                "mdate": 1700698377199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "McGk4awUfD",
                "forum": "poFAoivHQk",
                "replyto": "FWMmBOSJrP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Reviewer_kKdF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for analyzing GFSA from the perspective of singular values, rather than through graph signal processing. This approach is helpful in breaking the boundary between the directed graph convolution and the self-attention, which I am very pleased to see."
                    }
                },
                "number": 42,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698600837,
                "cdate": 1700698600837,
                "tmdate": 1700702831527,
                "mdate": 1700702831527,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FWMmBOSJrP",
            "forum": "poFAoivHQk",
            "replyto": "poFAoivHQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_fbar"
            ],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6709/Reviewer_fbar"
            ],
            "content": {
                "title": {
                    "value": "Strong support for the authors"
                },
                "comment": {
                    "value": "Hi all,\n\nHaving followed the discussion in the last week, I would like to maintain my high score for this paper.  Given that this article kept high quality at the submission, this very discussion will have sharpened the scientific presentation and made the authors scrutinize the mathematical underpinnings and presentation of their work, increasing the value of the work further (\"We will add a remark to the paper acknowledging the theoretical consequences of the choice.\", as the authors have said in this discussion, which is positive).\n\nI would like to cite another part of the authors' many thorough replies during the rebuttal period, namely the following:  \"It is our opinion that there is value in a diversity of research that does not always align with older theory, and we have put substantial effort into developing careful and rigorous benchmarks so that the value of our treatment of the matrix polynomials is scientifically supported. The performance gains we have realized are substantial, and we believe it to be inappropriate to suppress sharing these advances with the community.\"\n\nAlthough this discussion ultimately most likely was a scientifically positive thing for the authors -- being forced to think about their assumptions in such detail, in my view it slightly discredits reviewer kKdF that they [reviewer kKdF] were so thoroughly skeptic, and not acknowledging any other positive aspects of the paper other than the experimental. Anybody who has written a machine learning paper will know that a more than substantial amount of work went into a paper like this one (it is a very ambitious paper), from the conceptualization, theoretical motivation and into the experimental phase, and as a reviewer I find it important to at least acknowledge this or encourage the work in progress and not simply frown at something because it doesn't follow the exact theoretical school the reviewer comes from. I find the tone in many of kKdF's comments being slightly dogmatist or querulant, which is where I find the author's above cited comment important to keep in mind. But at the same time, it is fantastic that kKdF took the time to go into precisely these details, which they know better than me, so we ended up with the high quality final paper that we have here. So thank you to both kKdF and the authors, in the end.\n\nThis article is greatly interesting for anyone interested in the expressivity of Transformers (which is a large number of researchers at this point), and how their limits can be pushed. It is clearly presented with thorough experimentation and as far as I can see is based on uncontroversial matrix algebraical explanations. In my view, then, whether it can 'truly' say that it adheres to the school of graph signal processing or not matters less.\n\nThe authors highly thorough responsiveness through the rebuttal period has convinced me even more about my score. Thus I vote for accepting this paper."
                }
            },
            "number": 37,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700689508524,
            "cdate": 1700689508524,
            "tmdate": 1700689613348,
            "mdate": 1700689613348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IAzhVxEOP2",
                "forum": "poFAoivHQk",
                "replyto": "FWMmBOSJrP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6709/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your support."
                    },
                    "comment": {
                        "value": "We greatly appreciate the reviewer fbar in acknowledging the contributions we made in this paper. We also appreciate the reviewer kKdF for providing their perspective, which we were less careful about properly articulating the difference by the time we initially work on this project.\n\nAfter all, we appreciate all these conversations with the reviewers; we share the same research interest and attempt to make advancements on the topic in a scientifically proper way. Our hope is that the reviews kKdF and huFG could acknowledge that the contributions we made in this paper is nontrivial. Our method design is well within the graph signal processing theory (although it is little different from the most popular graph signal processing setting) and its empirical evidence well justifies it.\n\nWe hope that our research can contribute to our community since all of us, as researchers, should use our knowledge and experience for the technological advancement of mankind. \n\nSincerely,\n\nAuthors"
                    }
                },
                "number": 41,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6709/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696888992,
                "cdate": 1700696888992,
                "tmdate": 1700696888992,
                "mdate": 1700696888992,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]