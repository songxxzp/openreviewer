[
    {
        "title": "Learning Coverage Paths in Unknown Environments with Reinforcement Learning"
    },
    {
        "review": {
            "id": "sf0aFvsRnu",
            "forum": "ZiF1bJ9K6B",
            "replyto": "ZiF1bJ9K6B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes and investigates the effectiveness of a deep RL method for coverage path planning, with lawn mowing and exploration as practical example tasks. While the ultimate objective is total area covered, the authors proposes an additional total variation objective which minimizes the perimeter of explored areas to encourage coherent exploration, in addition to other collision objectives. The observation consists of multi-scaled and ego-centric maps of areas explored as well as frontiers, while the action space is the continuous speed and angular velocity of the agent. The authors trains the agent on a set of generated maps and evaluates on similar maps and additional maps from Explore-Bench. Favorable performance is demonstrated compared to frontier-based heuristics and some RL approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method seems relatively simple and intuitive from a learning perspective, which improves the chances that it might generalize beyond the paper. I have not been able to find previous works which are substantively similar, which speaks to the originality of the overall method and its application, even though individual components such as multi-scale observations, continuous actions, and reward shaping are not original by themselves. The clarity is good and the paper is easy to read. The experimental results are convincing compared to the baseline methods, and the experiments and ablations are appropriately designed."
                },
                "weaknesses": {
                    "value": "My main concern is that the baselines may not be challenging enough. Intuitively, I agree on the focus on frontier-based baselines, but there should be a traveling salesman (TSP)-based baseline which aims to find a shortest path among the frontier points with a state-of-the-art TSP solver like Concorde, either in a discretized grid (converted into a graph) or in a probabilistic roadmap (PRM) graph. As mentioned in the survey below, TSP is a common technique applicable in coverage path problems.\n```\nTan, Chee Sheng, Rosmiwati Mohd-Mokhtar, and Mohd Rizal Arshad. \"A comprehensive review of coverage path planning in robotics using classical and heuristic algorithms.\"\u00a0*IEEE Access*\u00a09 (2021): 119310-119342.\n```\nThe RL-based baseline (Hu 2020) targets a multi-agent setting and is a simple feed-forward policy with no convolutional architecture, and it\u2019s not clear if this is a fair comparison.\n\nOverall, there is very little detail on how the baselines are implemented in this work; many of the baselines are originally proposed in multi-agent settings rather than single-agent settings. It\u2019s difficult to judge how non-trivial the baselines are, and intuitively there should be a TSP-based or other combinatorial optimization-based baseline, given that CPP is a combinatorial problem. While the paper presents strong results relative to the given baselines, the baselines themselves have to be appropriately designed.\n\nMinor:\nI think the total variation contribution is slightly over-claimed, as the total variation is simply the perimeter of the explored area. While this shaping term is interesting and useful, using \u201ctotal variation\u201d to describe this reward makes the paper less clear and does not provide additional insights / intuition."
                },
                "questions": {
                    "value": "I see the collision penalty in the objective term. Does the agent collide at all with obstacles? If so, I would like to see the frequency of collision of the different approaches."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617203653,
            "cdate": 1698617203653,
            "tmdate": 1699636049650,
            "mdate": 1699636049650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7JPfh9txtd",
                "forum": "ZiF1bJ9K6B",
                "replyto": "sf0aFvsRnu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replies to reviewer 5fHE"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback on how to improve the comparison with other methods, and we appreciate the positive feedback. We reply to the concerns below.\n\n__The baselines may not be challenging enough, a TSP-baseline should be included.__ We agree that a TSP-baseline would further increase the quality of the comparison. However, we believe that the methods under comparison provide sufficient insights about the performance of our method, as they span a wide range of approaches.\n\n__Unclear how the baselines are implemented.__ We used the implementations from their code repositories. We will provide links to these in the appendix to make it more clear.\n\n__The RL-based baseline (Hu et al., 2020) targets a multi-agent setting, it is not clear if this is a fair comparison.__ As Hu et al. predict control signals using RL (although not end-to-end) we consider it the closest related method to our work, and thus we wanted to compare with it. Based on Voronoi partitioning, it can also be used for multi-agent exploration. However, for a fair comparison, we limited it to one agent as this is the focus of our paper. As Hu et al. predict the control signals based on the relative goal position from a previous stage in their framework, a simpler network architecture is sufficient, compared to predicting them based on the full map observation.\n\n__Some baselines are originally proposed in multi-agent settings.__ Apart from Hu et al. and the potential field-based method (Yu et al., 2021), all other methods were originally proposed in a single-agent setting. Xu et al. (2022) later extended them to the multi-agent case. We believe that the absence of the multi-agent component in Yu et al. does not trivialize their method nor hinder single-agent performance, as they showcase their method also in single-agent exploration. This method still serves as a useful comparison in the single-agent case, as this is a common approach for single-robot path planning.\n\n__Total variation contribution is slightly overclaimed, it is simply the perimeter.__ The key is that TV can be computed locally and continuously, enabling e.g. variational image enhancement, infilling, and segmentation methods (a body of several hundred papers). The insight that these properties also enable its application in continuous RL is, to the best of our knowledge, novel.\n\n__What is the collision frequency?__ For exploration, it varied between once every 100-1000 seconds, which is roughly once every 50-500 meters. For lawn mowing, it varied between once every 150-250 seconds, which is roughly once every 30-50 meters. However, the vast majority of collisions were near-parallel, and we didn\u2019t see any head on collisions. The practical implications are very different between these cases. We will add this analysis to the appendix."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217758106,
                "cdate": 1700217758106,
                "tmdate": 1700217758106,
                "mdate": 1700217758106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q6NPbzSlpo",
                "forum": "ZiF1bJ9K6B",
                "replyto": "7JPfh9txtd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
                ],
                "content": {
                    "comment": {
                        "value": "Unfortunately, the authors have not addressed the bulk of my concerns. It's quite standard for learning for combinatorial methods [1,2] to compare with strong non-learning solvers in problems like TSP and VRP, and here the frontier coverage problem is a minimum length Hamiltonian path problem over a set of frontier points that need to be visited; this problem can be easily reduced to a TSP and fed into a solver. The baselines that the authors used are too fuzzy in my opinion (e.g. potential field), and not principled enough to serve as a strong reference point for the proposed learning-based work. There is little detail in the paper about the implementation and architectures used in the other learning-based baselines, but at least one uses a MLP without CNNs, so it does not seem well-tailored for the problem. I highly encourage the authors to add a TSP baseline with a solver like Concorde.\n\n[1] Kwon, Yeong-Dae, et al. \"Pomo: Policy optimization with multiple optima for reinforcement learning.\" Advances in Neural Information Processing Systems 33 (2020): 21188-21198.\n\n[2] Hottung, Andr\u00e9, Yeong-Dae Kwon, and Kevin Tierney. \"Efficient active search for combinatorial optimization problems.\" arXiv preprint arXiv:2106.05126 (2021).\n\nRegarding the TV contribution, it doesn't really matter to the coverage path planning problem whether TV can be computed locally or not, and it's very dubious (and unsupported by the paper) that other image related methods (enhancement, infilling, and segmentation) are relevant to the structure of the problem at hand. To me, it seems that TV merely coincides with the perimeter, which is helpful to minimize for this task. I still do not see TV as some sort of application of computer vision-related insight to this domain. It is my opinion that the authors could be clearer about that, but this is just a minor point.\n\nRegarding the collision frequency: I see, that sounds good to me."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324856234,
                "cdate": 1700324856234,
                "tmdate": 1700324856234,
                "mdate": 1700324856234,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b3cnskiUUi",
                "forum": "ZiF1bJ9K6B",
                "replyto": "BWNkkwffAN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
                ],
                "content": {
                    "comment": {
                        "value": "Please see Table 1 of this paper [1]. Concorde solves a size 200 TSP in 1.69s (thus size 20 TSP should be trivial), so this could absolutely be run in real-time or with replan once every few steps. Thus, I highly recommend that the authors include a comparison with this type of approach (either for this submission or for future submissions).\n\n[1] Cheng, Hanni, et al. \"Select and Optimize: Learning to solve large-scale TSP instances.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n\nRegarding the other neural methods, thanks for the clarifications. I will discuss these with other reviewers if necessary to get a better sense of their significance.\n\nRegarding the TV in its application to this work, I would argue that these image processing itself is irrelevant to this domain. What's relevant here is that there is no holes during coverage path planning (since the robot cannot visit disconnected components of the map). I'm merely pointing out that using \"TV\" in the paper rather than perimeter does not offer additional insights. This is a minor point and I don't believe the authors can change my view on this with additional discussion."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517640443,
                "cdate": 1700517640443,
                "tmdate": 1700517640443,
                "mdate": 1700517640443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DrMoEIgW64",
                "forum": "ZiF1bJ9K6B",
                "replyto": "sf0aFvsRnu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Reviewer_5fHE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you. Just a note, TSP solvers typically allow you to specify a distance matrix between every pair of points, so ideally you would try to quantify if this makes a difference from Euclidean distance (on a grid you can get the known shortest paths between any two points with the A* or Bellman Ford algorithm). If Concorde is somehow not a good fit, LKH-3 is another commonly used TSP solver and can be downloaded from http://webhotel4.ruc.dk/~keld/research/LKH-3/. And also I don't necessarily expect the proposed RL method to perform better, but knowing the tradeoff between solution quality and planning time of a TSP solver would significantly help characterize the proposed method.\n\nThe negative holes is a fair point, and I agree that \"perimeter\" is a crude term for cases like this, but I think there should be a better term for the total boundary which gives more insight on a global level than TV, which is a local operation that you apply globally."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588003286,
                "cdate": 1700588003286,
                "tmdate": 1700588028534,
                "mdate": 1700588028534,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vpHbFsS5oz",
            "forum": "ZiF1bJ9K6B",
            "replyto": "ZiF1bJ9K6B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1230/Reviewer_K43j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1230/Reviewer_K43j"
            ],
            "content": {
                "summary": {
                    "value": "The study focuses on RL applications for online CPP and investigates several components like action space, input representation, neural networks, and reward functions. Extensive experiments are carried out to evaluate the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The research topic is interesting.\n2. An instance of reinforcement learning applied to the domain of path planning."
                },
                "weaknesses": {
                    "value": "1. The research appears to have limited novelty, with a modest contribution.\n2. Most machine learning and classical planning methods can handle the investigated problem. It is hard to find new insights from this study.\n3. In terms of multi-scale approaches, prior studies have delved into this issue, notably employing RL in autonomous driving as illustrated in reference [1].\n\n[1] Gu, S., Chen, G., Zhang, L., Hou, J., Hu, Y., & Knoll, A. (2022). Constrained reinforcement learning for vehicle motion planning with topological reachability analysis. Robotics, 11(4), 81."
                },
                "questions": {
                    "value": "1. How might the research tackle challenges related to sim-to-real transfer, especially considering the difficulties of implementing one-shot learning in real-world RL applications?\n2. How does the agent architecture in this study differ from architectures used in methods like TRPO?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690227215,
            "cdate": 1698690227215,
            "tmdate": 1699636049571,
            "mdate": 1699636049571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oNG6tmHJ38",
                "forum": "ZiF1bJ9K6B",
                "replyto": "vpHbFsS5oz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replies to reviewer K43j"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback, and provide replies to the concerns below.\n\n__The research has limited novelty, with a modest contribution.__ May we kindly ask the reviewer to be more specific, e.g. by giving a reference that would limit the novelty? As far as we know, previous work has not applied RL to learn continuous control signals end-to-end in CPP. In our interpretation, the strengths outlined by reviewers aapF and 5fHE tend to suggest that they agree. Learning control signals end-to-end in this way comes with unique challenges, which we address in the paper.\n\n__Most machine learning and classical planning methods can handle the investigated problem.__ We agree that many classical and learning-based methods tackle a similar problem. However, none approach it end-to-end with RL as mentioned above. Compared to classical methods and multi-stage frameworks, this approach does not put any constraints on the path space, and allows the model to adapt to specific quirks in the agent dynamics in order to find a more efficient path.\n\n__In terms of multi-scale approaches, prior studies have delved into this issue.__ This is true. We are not suggesting that it is a new concept, but rather, that it is a necessary part in our solution to the challenge of applying RL to large-scale CPP. We will clarify this part in the paper.\n\n__How might the research tackle sim-to-real transfer?__ Good question. As mentioned early on page 4, we believe that our approach can be used with an off-the-shelf SLAM method, removing the need for known pose. In this setting, the agent can learn and adapt to the noise characteristics during training. That our approach works well on noise-free data / data with simulated noise is a necessary condition for future real-world applications. The evaluation of our method on physical robot systems with realistic, noisy sensing will be subject of a future manuscript.\n\n__How does the agent architecture differ from architectures in methods like TRPO?__ The architectures used in TRPO are either feed-forward fully connected networks, similar to our MLP baseline, or convolutional networks for image observations, similar to the map feature branch in our CNN baseline. They are different in regards to our multimodal feature extractor and SGCNN architecture."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217275115,
                "cdate": 1700217275115,
                "tmdate": 1700217275115,
                "mdate": 1700217275115,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0iRK3wSIpK",
            "forum": "ZiF1bJ9K6B",
            "replyto": "ZiF1bJ9K6B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1230/Reviewer_aapF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1230/Reviewer_aapF"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an end-to-end reinforcement learning approach to solve the problem of online coverage path planning. In particular, the paper formulates the problem as a Markov Decision Process, where the action space is continuous (linear and angular velocity), the observation space includes a multi-scale map representation, and the reward function includes a total variation term to incentivize the agent not to leave any unexplored part behind. The evaluation and comparison are performed on a number of different environments and with respect to some classic and learning-based methods. The code is included."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Overall, the paper presents an RL based end-to-end method able to have the agent explore different unseen environments, providing an interesting view on the representation needed to achieve such a task.\n\n- with a standard RL problem formulation, the proposed method includes elements that are not present in the current learning-based methods, such as including continuous action space.\n\n- the presentation overall is clear, with a logical structure and justifications included in the different choices."
                },
                "weaknesses": {
                    "value": "- the paper provides as motivation with respect to classic methods \"As these classical approaches cannot adapt to non-modeled environment dynamics, we turn to learning-based approaches\", however, also the proposed method does not include any environment dynamics (the paper includes some examples in the introduction, such as damaged sensor or actuator). The paper currently lacks the corresponding gap from the classic methods and does not address the issue mentioned.\n\n- while the paper includes a number of different methods for comparison, there are some methods that are not discussed at all, but that however would be relevant to discuss, in particular some learning-based methods that are predicting the structure of the environments, such as \n\nCaley, Jeffrey A., Nicholas RJ Lawrance, and Geoffrey A. Hollinger. \"Deep learning of structured environments for robot search.\" Autonomous Robots 43 (2019): 1695-1714.\n\nShrestha, Rakesh, Fei-Peng Tian, Wei Feng, Ping Tan, and Richard Vaughan. \"Learned map prediction for enhanced mobile robot exploration.\" In 2019 International Conference on Robotics and Automation (ICRA), pp. 1197-1204. IEEE, 2019.\n\nArguably, compared to classic methods, a learning based approach can potentially learn some patterns to choose locations that are promising in terms of new information.\n\n- there are parts that are not fully realistic, in particular the noise that would be present in the partial maps built by the robot, which has the effect of potentially guiding the robot towards areas that do not require exploration. It would be instead good to actually have the map built considering the noise. This can be achieved by using a realistic robot simulator with the perception/navigation stack already existing, e.g., in ROS.\n\n- it is also not clear why different methods are used for comparison in the omnidirectional and non-omnidirectional case, especially for the classic methods. Classic methods typically separate the high level decision and planning to the low-level control, so the same methods can be applied.\n\n- to appreciate the difference between methods, it is important to include also the standard deviation, i.e., in Table I. It would have been good to include coverage over time also for the omnidirectional case, in the appendix.\n\n- the multiscale component plays a role given the fixed w and h for the map, however, it is not clear the difference in performance between a multi-scale approach vs if those w and h would be high enough to capture the environment at enough fine resolution. It would be interesting to see such a difference. In addition, there might be environments where one scale is enough, while other complex environments require more than one scale. It would be interesting to discuss such a difference in different environments, thus hinting an adaptive scale, and how in practice an adaptive scale can be included, as the training will happen with specific scales.\n\n- a comment should be included about the impact of normalizing the distance measurements with respect to the maximum range, in particular about the generalization of the learned policy on a robot with different sensor range.\n\n- it is good to provide an intuition on how to set the parameters\n\nJust some minor presentation comments to fix:\n- \"in (1)\" -> \"in Eq. (1)\"\n- it can make sense to change CPP to exploration, given that in the end the coverage, for example in the lawn mowing task, has quite some difference in performance."
                },
                "questions": {
                    "value": "- what is the standard deviation of the results in Table I? in other words, are the differences statistically significant?\n\n- please include results with the same methods for both omnidirectional and non-omnidirectional case."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1230/Reviewer_aapF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826844715,
            "cdate": 1698826844715,
            "tmdate": 1699636049480,
            "mdate": 1699636049480,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c71i303JZ4",
                "forum": "ZiF1bJ9K6B",
                "replyto": "0iRK3wSIpK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to first two points"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback on how to improve the quality of our paper. We address the first two points. Additional answers will follow shortly.\n\n__The proposed method does not include any environment dynamics.__ The ability to adapt to non-modeled environment dynamics was mainly used as a motivation for choosing an RL-based approach. Indeed, [1] shows that in grid environments, RL can be robust to dynamic environments and outperform classical methods. We missed to include this reference and will update the paper accordingly. Due to the findings in [1], we focused on the question whether an RL-based approach can learn patterns to choose locations that are promising in terms of new information in a continuous setting at all, before moving to dynamic environments.\n\n[1] Saha et al. \u201cOnline area covering robot in unknown dynamic environments\u201d, ICARA, 2021.\n\n__Some methods are not discussed at all.__ There are many works on CPP that share a connection to our work. However, to include all of them would not fit within 9 pages. We had to make a selection and choose the most related ones. We did not include Caley et al. (2019) as it tackles a slightly different problem, robot search (i.e. find the exit) instead of covering the area completely. Shrestha et al. (2019) presents and interesting approach for the same task and we thank the reviewer for this reference, we will include it in the related work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699975472687,
                "cdate": 1699975472687,
                "tmdate": 1699975472687,
                "mdate": 1699975472687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kJ1lZJZmrd",
                "forum": "ZiF1bJ9K6B",
                "replyto": "0iRK3wSIpK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional replies"
                    },
                    "comment": {
                        "value": "Here follows additional replies.\n\n__The noise in the built maps are not fully realistic.__ As mentioned early on page 4, we believe that our approach can be used with an off-the-shelf SLAM method, removing the need for known pose. In this setting, the agent can learn and adapt to the noise characteristics during training. That our approach works well on noise-free data / data with simulated noise is a necessary condition for future real-world applications. The evaluation of our method on physical robot systems with realistic, noisy sensing will be subject of a future manuscript.\n\n__It would be good to build the maps considering the noise using a realistic robot simulator, e.g. in ROS.__ We agree that this would be highly interesting, although ROS is not necessarily fully realistic as it does not have a full notion of continuous time. Evaluations on real robots would be preferable to any simulation.\n\n__Why use different methods to compare omnidirectional/non-omnidirectional exploration?__ We simply compared the methods on the task setup described in their respective code implementations. We will provide links to the code repositories used in the appendix.\n\n__The standard deviation should be included, e.g. in Table 1.__ We will do our best to do multiple runs for the standard deviation, although we can not promise that they will finish during the discussion period.\n\n__It would be good to include coverage over time in the omnidirectional case in the appendix.__ Good suggestion. Although, the T90 and T99 metrics used in Explore-Bench convey information part of such a plot.\n\n__How does a multi-scale map compare to a single map with high enough w and h?__ In large environments, using a single map is not feasible as the computational cost is O(n^2). For example, with our current setup with 4 maps that span an area of size 76.8x76.8 m2, the training step time is 40 ms. With a single map spanning only 19.2x19.2 m2 using the same pixel resolution as the finest scale, the training step time is 2500 ms. This would increase the total training time by roughly two orders of magnitude. Thus, a multi-scale map is actually necessary for large-scale environments. We will clarify this part in the paper.\n\n__Could an adaptive scale be included?__ As you mention, the training needs to be done with specific scales. We believe the most practical way is to simply train with sufficiently many scales to cover the maximum size for a particular use case. If the model is deployed in a small area, the larger scales would not contain any frontier points in the far distance, but the agent can still cover the area by utilizing the smaller scales. Increasing the represented area by adding more scales is fairly cheap, as the computational cost is O(log(n)). We will add such a discussion to the appendix.\n\n__The impact of normalizing the distance measurements should be discussed.__ Good point. With this normalization, the training is tied to a specific sensor range. However, the map representation is more flexible, and does in theory also contain the information of the raw lidar data. So excluding the sensor feature extractor in Figure 4 would in theory generalize to any sensor setup. We will add this discussion to the appendix.\n\n__It is good to provide an intuition on how to set the parameters.__ Good point. We will describe the parameters in more detail and provide an intuition on how to set them in the appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215699918,
                "cdate": 1700215699918,
                "tmdate": 1700215699918,
                "mdate": 1700215699918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qgc9VTgMFb",
                "forum": "ZiF1bJ9K6B",
                "replyto": "kJ1lZJZmrd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Reviewer_aapF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Reviewer_aapF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response to this and others' reviews. The additional clarifications that are planned to be added in the paper or the supplementary material improves the overall quality of the paper.\n\nWhile the advantage of RL-based methods over classic methods for dynamic environments is shown in at least another paper, it is important to show that the proposed method would still work in case of dynamic environments. Note that the general idea of using the structure of the environment to learn promising locations is not new. In fact, Caley et al. exploited the same concept in the context of search, although the underlying routine is based on exploration towards frontiers.\n\nAlso, about the noise, simulators in ROS, such as Gazebo, are already a step towards real robots and in robotics they are used. In practice, while the simulator has a simulation timestep, for testing purpose it can be used for continuous cases. In fact, even real robots have sensor measurements and control commands at a frequency. \nIn addition, the 2D occupancy grid map produced with the laser sensor can be quite noisy for example with free space that might appear behind obstacles.\nIt is important thus to test the method at least in the simulator and clearly even better on real ground robots.\nNote that ROS is a middleware that in its core functionality provides ways for processes to talk to each other with a publish/subscribe paradigm and standardized messages. In this way, the code written and tested within a simulator can work out of the box with a real robot."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492463280,
                "cdate": 1700492463280,
                "tmdate": 1700492463280,
                "mdate": 1700492463280,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lo4iHyVEhI",
            "forum": "ZiF1bJ9K6B",
            "replyto": "ZiF1bJ9K6B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1230/Reviewer_HYy8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1230/Reviewer_HYy8"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a reinforcement learning approach for online coverage path planning in unknown environments. The authors use a continuous action space for the agent in their formulation, with the model directly predicting the low level control signals. When compared to prior work, the main contribution seems to be the choice of observation representations and the reward function. Observation data is represented through multi-scale egocentric maps to enable long-term planning while maintaining scalable input representation. Additionally, the introduction of frontier maps helps retain information about uncovered spaces in larger-scale maps. The authors propose a reward function with a total variation term to reduce small uncovered areas within the environment. The approach is tested on tasks such as exploration and lawn mowing, showing improved performance over existing reinforcement learning and classical path planning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors combine a lot of well-established ideas and apply them to the problem of interest - multi-scale egocentric maps, frontier encoding, along with the intuitive total variation term in the reward. The impact of these elements is clearly demonstrated.\n- Experiments across different 2D environments demonstrate general applicability of the learned policy and the performance seems to exceed the baselines that include classical methods and learning-based methods. The analysis is thorough with ablation studies of different components including observation representations, rewards, and architectures.\n- The paper is well-written and organized, and clearly motivates different concepts before getting to the solution approach. The notation is consistent and clear throughout the paper and the figures are informative (with one exception, see weaknesses). Great job with the writing!"
                },
                "weaknesses": {
                    "value": "- The details of the agent architecture and the reasoning behind the design choices isn\u2019t completely clear - is there a reason for selecting SAC over other RL algorithms? What was the motivation behind the MLP architecture as a baseline when it\u2019s not the best choice of the input representations at hand? Figure 4 is a bit confusing too - I thought multi-scale frontier maps M_f are a par t of the observation too but the illustration in Fig 4 don\u2019t seem to mention that. Also, it wasn\u2019t super clear at first glance what x3 or x4 meant in the architecture.\n- I am not sure how practical the proposed approach is in real-world deployments - at the end of the day, the goal is to be able to deploy learned policies on real-systems. However, perfect observations for building global coverage/obstacle map and noise-free position/pose information are fairly strong assumptions that might limit practical utility. I do acknowledge the experiments with added Gaussian noise, but the scale of the noise isn\u2019t completely clear and it seems like noise was only added to position information. I would have loved to see some real-world experiments or experiments on high-fidelity simulators with some discussion on inference speed/real-time performance.\n- While I understand the work emphasizes on coverage path planning, I would have expected some discussion/experiments on the extent of collisions too - collisions matter in real-world problems, and unlike classical methods, the extent of collisions in a learned policy would at the end of the day depend on the relative weight of the collision-avoidance reward. It would be interesting to see the trade-off in the performance with more emphasis on collision avoidance when compared to classical approaches (to be clear, I am not recommending new experiments here - but it would be great if authors could discuss this aspect/share more details if they have the necessary information in the logs of existing runs)."
                },
                "questions": {
                    "value": "In addition to the questions in the weaknesses section, I have the following questions/suggestions:\n\n- The word \u201cenvironment dynamics\u201d the the text is confusing - I understand what the authors wanted to convey but at the end of the day it\u2019s agent dynamics.\n- There are some issues with the notation in the POMDP definition. The state is Markovian by definition, and probability of transitioning to s_t should only depend on the previous state and action. If the problem formulation requires violating this, mention it explicitly.\n- Did you experiment with any other reward formulations besides coverage area and total variation?\n- How does the performance scale to much larger environments? At some point the multi-scale representation may need higher resolution or more scales, right?\n- What\u2019s the inference time like say on a standard laptop?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699430877719,
            "cdate": 1699430877719,
            "tmdate": 1699636049419,
            "mdate": 1699636049419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vA8U6wBnW8",
                "forum": "ZiF1bJ9K6B",
                "replyto": "lo4iHyVEhI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replies to reviewer HYy8"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable suggestions on improving the clarity, motivation and analysis. We are also encouraged by the positive feedback. We address the concerns below.\n\n__What is the reason for choosing SAC over other RL methods?__ In our understanding, SAC is the standard method for continuous RL, and has been used successfully in robot navigation [1]. It also has high sample efficiency, which is important for future real-world experiments.\n\n[1] de Jesus et al. \"Soft Actor-Critic for Navigation of Mobile Robots\", Journal of Intelligent and Robotic Systems, 2021.\n\n__What is the motivation for the MLP baseline?__ The purpose of the MLP baseline is to evaluate the effect of the inductive biases in the CNN/SGCNN architectures. Note that the MLP architecture even has more parameters, 3.2M compared to 0.8M for the CNNs, due to the initial fully connected layer. This tells us that the inductive biases are indeed necessary.\n\n__Figure 4 is missing the frontier maps.__ Yes, we realized, we will update the figure accordingly.\n\n__It is unclear what x3 and x4 mean in Figure 4.__ We can see that, we will update the caption explaining that they refer to the number of conv and fully connected layers.\n\n__Perfect observations and noise-free pose might limit practical utility.__ As mentioned early on page 4, we believe that our approach can be used with an off-the-shelf SLAM method, removing the need for known pose. In this setting, the agent can learn and adapt to the noise characteristics during training. That our approach works well on noise-free data / data with simulated noise (see below) is a necessary condition for future real-world applications. The evaluation of our method on physical robot systems with realistic, noisy sensing will be subject of a future manuscript.\n\n__The scale of the noise isn\u2019t completely clear.__ We measured the average position error for the SLAM method used in the CPP methods under comparison, and applied a similar noise to our method to get a fair comparison.\n\n__Experiments on high-fidelity simulators.__ We agree that this would be highly interesting, although evaluations on real robots are preferable to any simulation.\n\n__What is the inference time?__ The model inference time is 5 ms on a laptop (i5-520M CPU, no GPU), and 2 ms on a high-performance cluster node (6226R CPU, T4 GPU). As our network is fairly lightweight it can run in real time. The time for updating the maps and constructing the multi-scale map representation etc. is 57/9 ms for exploration/mowing on the laptop, and 19/3 ms for exploration/mowing on the cluster node. All map updates are performed locally, resulting in high scalability. We will add this analysis to the appendix.\n\n__What is the collision frequency?__ For exploration, it varied between once every 100-1000 seconds, which is roughly once every 50-500 meters. For lawn mowing, it varied between once every 150-250 seconds, which is roughly once every 30-50 meters. However, the vast majority of collisions were near-parallel, and we didn\u2019t see any head on collisions. The practical implications are very different between these cases. We will add this analysis to the appendix.\n\n__The word \u201cenvironment dynamics\u201d is confusing, it should be \u201cagent dynamics\u201d.__ By \u201cenvironment dynamics\u201d we meant to convey aspects that do not only depend on the properties of the agent, such as wheel slip or moving obstacles.\n\n__In the POMDP definition, the probability of transitioning to s_t should only depend on the previous state and action.__ This is true, the state should not depend on all previous states and actions leading up to the new state (which we do not violate), we will update this.\n\n__Did you experiment with other reward formulations?__ Yes, we did. We tried a positive goal reward for reaching the goal coverage, and a negative truncation reward, but these gave no effect. We will include them in the appendix.\n\n__How does the performance scale to much larger environments? More maps or higher resolution is required, right?__ Yes, more maps or higher resolution would be needed for much larger environments. More maps would be most feasible, as the computational cost is O(log(n)). For example, our current setup with 4 maps spans an area of size 76.8x76.8 m2 with 4096 pixels. Increasing the number of maps to 6 would increase the computational cost by 50% while spanning more than 1x1 km2 in 6144 pixels."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216538070,
                "cdate": 1700216538070,
                "tmdate": 1700216538070,
                "mdate": 1700216538070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]