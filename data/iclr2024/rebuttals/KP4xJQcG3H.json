[
    {
        "title": "Lagrangian Proximal Gradient Descent for Learning Convex Optimization Models"
    },
    {
        "review": {
            "id": "yQZYE5YXpq",
            "forum": "KP4xJQcG3H",
            "replyto": "KP4xJQcG3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5435/Reviewer_t8qX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5435/Reviewer_t8qX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Lagrangian Proximal Gradient Descent (LPGD), an optimization framework for learning convex optimization models. It is effective for problems with non-differentiable loss functions and can handle models where gradients are not readily available. LPGD smooths the loss function, making optimization feasible in such scenarios. It efficiently computes updates by rerunning the forward solver and converges to the true gradient as the smoothing parameter approaches zero. LPGD can also offer benefits in fully different settings, making it a versatile tool for various optimization tasks. The method is interesting with promising performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors provide a new method, which considers a model with an embedded constrained convex optimization. They also provide a new update, the Lagrangian divergence proximal operator, which generalizes the classical Bregman divergences. Compared to classical proximal gradient descent methods, the update is new."
                },
                "weaknesses": {
                    "value": "1. The proposed method does not come with the convergence rate analysis.\n\n2. Could the authors provide examples of neural network-type constrained minimization problems? The neural network can be chosen as a simple one. This is to demonstrate the difference and the connection between current methods and the classical ones."
                },
                "questions": {
                    "value": "One expects to see the method working on a neural network-type constrained optimization problem."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698458152117,
            "cdate": 1698458152117,
            "tmdate": 1699636552366,
            "mdate": 1699636552366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VadgYjo8lq",
                "forum": "KP4xJQcG3H",
                "replyto": "yQZYE5YXpq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer t8qX"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. We reply to the individual points below.\n\n- *The proposed method does not come with the convergence rate analysis.*\n    - It is very challenging even for the special cases to derive convergence rates, which is reflected by only the SPO+ and the Fenchel-Young loss in example 5 and 6 coming with convergence rates. For these methods it is possible because the loss is assumed to be convex in the parameters. In our generalized setup this is not necessarily true.\n    - We are also mostly interested in the case in which the model $W_\\theta$ is a neural network, which means that reasonable convergence results are in general hard to provide.\n    - However, we agree that deriving convergence results for out algorithm under stronger assumptions would be a very nice addition to our work. Especially because due to our unification of previous methods, any convergence results would also directly apply to multiple existing methods. In this paper we focus on the first step of showing the similarities between various seemingly disconnected methods, and we leave the challenging analysis of the convergence to future work.\n\n- *Could the authors provide examples of neural network-type constrained minimization problems? The neural network can be chosen as a simple one. This is to demonstrate the difference and the connection between current methods and the classical ones.* and *One expects to see the method working on a neural network-type constrained optimization problem*\n    - Many of the methods that are covered as special cases have already been applied to problems involving neural networks. Examples include extracting edge costs in a shortest path problem from visual input (example 2 & 4, Vlastelica2020), extracting edge costs in graph matching problems for keypoint matching from visual input (example 2 & 4, Rolinek2020a) or extracting rankings in a retrieval system from visual input (example 2 & 4, Rolinek2020b). We added more detailed information on the experiments that have been proposed in the preceding literature of methods that we generalize.\n    - However, we want to highlight that the distinguishing difference between our proposed method and the methods which we refer to as traditional or classical, is that we consider the bilevel optimization problem instead of single-level optimization problems for which proximal methods were originally designed. We clarify this in the revision by explicitly formulating the bilevel problem in eq. 3."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526681144,
                "cdate": 1700526681144,
                "tmdate": 1700526681144,
                "mdate": 1700526681144,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s8Oktw855A",
                "forum": "KP4xJQcG3H",
                "replyto": "VadgYjo8lq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5435/Reviewer_t8qX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5435/Reviewer_t8qX"
                ],
                "content": {
                    "title": {
                        "value": "Review to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for addressing my questions. I read the rebuttal and will keep my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607779635,
                "cdate": 1700607779635,
                "tmdate": 1700607779635,
                "mdate": 1700607779635,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NCm3qknlVX",
            "forum": "KP4xJQcG3H",
            "replyto": "KP4xJQcG3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5435/Reviewer_n99L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5435/Reviewer_n99L"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a class of Lagrange Proximal Gradient Descent method. As the authors claimed, the main contribution of this work is that It generalizes many existing methods. The technical part of this paper is solid, and the paper is well-written and easy to follow."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I read over the paper. The technical part of this paper is solid and the ideas are promising. Similar to the idea of Mirror descent, they consider a specific distance function (Eq (11)). This idea is not surprising to me, but the authors then construct lower and upper Lagrange-Moreau envelop, and average them to improve the accuracy of the gradient with respect to the parameter w. This idea is similar to the idea of the central difference method which usually has higher precision than the forward and the backward difference method. They also demonstrated this by numerical experiments."
                },
                "weaknesses": {
                    "value": "1. The literature survey in this paper is not good enough. They surveyed many papers in Section 2, but the details of the most relevant papers are not provided either in the main text or in the Appendix. I'm particularly interested in existing papers that can also compute the (approximate) gradient of the function $\\ell(x^\\star(w))$. Moreover, the authors claim that the proposed method generalizes many existing methods and discuss this in Example 1,2,3. For more general problems rather than these examples, whether the proposed method generalize existing methods?\n\n2. From the experiment (Figure 2), even in terms of Epochs, LPGD-medium $\\tau$ is faster than GD. This is not straightforward to me. If I understand correctly, all the simulated methods in the experiments are based on GD, while the gradient is computed based on different approaches. Therefore, I wonder whether the poor performance of GD comes from the low accuracy of gradient information computed by the method in Argarwal 19b?"
                },
                "questions": {
                    "value": "1. What is the state-of-the-art method for solving the considered problem? Stochastic gradient-based or? Can this work be extended to that setting? I will suggest the authors to discuss this in Conclusion.\n\n2. Should the sentence \"Note that for a general loss, the optimization (16) and (17) may diverge\" below Eq (17) be revised as \"Note that for a general loss, update using (17) may diverge\". This sentence is unclear and less accurate because first, (16) is only an approximate loss and it's incorrect to say it may diverge."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5435/Reviewer_n99L",
                        "ICLR.cc/2024/Conference/Submission5435/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698490844964,
            "cdate": 1698490844964,
            "tmdate": 1700647294081,
            "mdate": 1700647294081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PzdgIZwA2P",
                "forum": "KP4xJQcG3H",
                "replyto": "NCm3qknlVX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer n99L"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. We answer to individual weaknesses and questions below.\n\n- *The literature survey in this paper is not good enough. They surveyed many papers in Section 2, but the details of the most relevant papers are not provided either in the main text or in the Appendix. I'm particularly interested in existing papers that can also compute the (approximate) gradient of the function $\\ell(x^\\*(w))$.*\n    - In the revised version, we added a detailed description of previous related methods in Appendix B. We hope that this provides a good overview for readers who are not familiar with all the related work.\n \n- *Moreover, the authors claim that the proposed method generalizes many existing methods and discuss this in Example 1,2,3. For more general problems rather than these examples, whether the proposed method generalize existing methods?*\n    - Our method fully captures the update rules of all methods appearing in example 1-6. We do not restrict these methods to a specific case in the examples, the setups described in the examples mostly capture these methods in their full generality. Of course, beyond just the update rules used as gradient replacement, some of these methods come with deep individual theoretical results and extensions (e.g. Fenchel Young losses), which are currently not directly captured by our framework.\n\n- *From the experiment (Figure 2), even in terms of Epochs, LPGD-medium $\\tau$ is faster than GD. This is not straightforward to me. If I understand correctly, all the simulated methods in the experiments are based on GD, while the gradient is computed based on different approaches. Therefore, I wonder whether the poor performance of GD comes from the low accuracy of gradient information computed by the method in Argarwal 19b?*\n    - It is important to mention that LPGD, in general, does not compute the true gradient that is used by the gradient descent baseline. We deliberately decide to use finite values of $\\tau$ which intuitively means that the solver is not linearly approximated when computing the parameter updates. Not doing this approximation allows for more informative updates containing higher-order information. This is why in our experiment, the updates computed by LPGD lead to better results than plain GD. We do not believe that the baseline Agrawal19b suffers from low accuracy of gradient information, as the linear system which has to be solved during their backward pass to compute the gradient, is solved to a very high accuracy.\n    -  However, note that our method LPGD is also capable of computing the true gradient when considering the limit $\\tau\\rightarrow 0$, as shown in Theorem 5.1.\n\n- *What is the state-of-the-art method for solving the considered problem? Stochastic gradient-based or? Can this work be extended to that setting? I will suggest the authors to discuss this in Conclusion.*\n    - The state of the art method highly depends on the specifics. This includes: 1) Is the parameter-to-solution mapping smooth? (SOTA: stochastic gradient descent, gradients computed via implicit function theorem or direct loss minimization), otherwise 2) Are ground truth solutions to the optimization problem or objective parameters available? (SOTA: smart predict then optimize setting) 3) Is it possible to solve a regularized version of the problem? (SOTA: Fenchel Young losses) 4) Is an efficient projection method available? (SOTA: Identity with projection) 5) Is the solver only available as a blackbox oracle and no direct supervision on cost parameters/solutions? (SOTA: blackbox backpropagation)\n    - With our unification we show that for these various settings, there is actually one generalized method that performs well over many different scenarios. We think that this is an important part of the contribution, and we emphasized it more in the revised conclusion.\n\n- *Should the sentence ``Note that for a general loss, the optimization (16) and (17) may diverge\" below Eq (17) be revised as \"Note that for a general loss, update using (17) may diverge\". This sentence is unclear and less accurate because first, (16) is only an approximate loss and it's incorrect to say it may diverge.*\n    - Thank you for pointing this out, we changed the assumption to the existence of a non-empty solution set in the revision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526460162,
                "cdate": 1700526460162,
                "tmdate": 1700526460162,
                "mdate": 1700526460162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WfrTz2ygAV",
                "forum": "KP4xJQcG3H",
                "replyto": "PzdgIZwA2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5435/Reviewer_n99L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5435/Reviewer_n99L"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for your reply"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nI would like to thank you for your efforts in revising the paper and replying to my comments. I agree to improve the score.\n\nBest regards"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647260381,
                "cdate": 1700647260381,
                "tmdate": 1700647260381,
                "mdate": 1700647260381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1xSl6fl4pv",
            "forum": "KP4xJQcG3H",
            "replyto": "KP4xJQcG3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5435/Reviewer_m9w5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5435/Reviewer_m9w5"
            ],
            "content": {
                "summary": {
                    "value": "The main contribution of this paper is the unification of several prior methods into a comprehensive framework termed Lagrangian Proximal Gradient Descent (LPGD). This approach is rooted in traditional proximal optimization techniques. The study further delves into the application of LPGD in different scenarios, emphasizing its potential improvements over gradient descent in certain settings namely, those where the gradient is uninformative (e.g. discrete optimization)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, the paper is well-written and easy to follow. The contribution is of interest for the community."
                },
                "weaknesses": {
                    "value": "There are two main weaknesses:\n\n1) The experiment section is rather small compared to the rest of the presentation. A lot of mathematical details could be eluded or referred to by a citation to a previous work (e.g. derivation of Moreau envelope, proximal operators, etc..). A quick (and rigorous) definition is more than enough, see below for a few remarks.\n\n2) There is too much context at the beginning of the presentation. No need to mention a vector $\\mu$ that is not used in the demonstration. I would first consider a convex optimization problem, and introduce the Lagrangian and the primal/dual variables. Explain that $f$ can be composite and have a non-smooth component, and explain Moreau and its connection with the proximal operators with 2 definitions. Finally, you can start your demonstration. Some space would be saved by shortening in \"Background and Notations\" section can be used to do one more experiment (see above).\n\n=========\n\nA few remarks:\n- Missing hypotheses in the Moreau envelope definition: $f$ must be proper and lower semi-continuous. \n- Instead of establishing a connection between the Moreau envelope and the proximal operator, refer the reader to a chapter in a well-known textbook that already explains in great detail the connection (e.g. Bauschke and Combettes).\n\n=========\n\nOverall, I believe this work deserves to be published in a venue like ICLR. However, in its current form, the manuscript is not ready for publication. The authors need to put more effort into establishing a more concise background and more rigor in the definitions of the notions used. Furthermore, the experiments section should be enriched with real-world data."
                },
                "questions": {
                    "value": "1) In your experiments figure (figure 2), how do you explain that the $LPGD$ (avg) yields a significantly lower loss compared to the $LPGD$ (lower) and $LPGD$ (upper) schemes? Could you provide an intuition?\n\n2) Could you elaborate on the convergence rate of LPGD? Is is the same as vanilla gradient descent with the additional cost of evaluating the upper and lower proximal maps?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5435/Reviewer_m9w5",
                        "ICLR.cc/2024/Conference/Submission5435/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610912077,
            "cdate": 1698610912077,
            "tmdate": 1700649956249,
            "mdate": 1700649956249,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "17wxglOT5T",
                "forum": "KP4xJQcG3H",
                "replyto": "1xSl6fl4pv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Relpy to Reviewer m9w5 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful suggestions. We answer to the individual weaknesses and questions below.\n\n- *The experiment section is rather small compared to the rest of the presentation.*\n    - The main contribution of this paper is in showing the similarities between different methods developed for different experimental settings, and how they can be combined in a unified framework. We believe that this contribution is decoupled from experimental results, and does not need to be supported by experiments, as each of the methods has previously been established independently to achieve strong empirical performance.\n    - However, in our mission of unification we noticed that our framework actually suggests applying finite-difference based methods (with finite values of $\\tau$) even in the setting in which the solver has non-degenerate derivatives. We tested this on our small-scale toy-experiment and observed that this technique can indeed improve performance over the previously typically used gradient-based approaches. Exploring this in-depth is beyond the scope of our paper, but we still wanted to include the small experimental section to share that our method could also lead to empirical improvements in future work.\n\n- *A lot of mathematical details could be eluded or referred to by a citation to a previous work (e.g. derivation of Moreau envelope, proximal operators, etc..). A quick (and rigorous) definition is more than enough, see below for a few remarks.*\n    - We changed many parts of the paper in the revision to make the treatment more rigorous by e.g. clearly stating the assumptions.\n\n- *There is too much context at the beginning of the presentation. No need to mention a vector $\\mu$ that is not used in the demonstration. I would first consider a convex optimization problem, and introduce the Lagrangian and the primal/dual variables. Explain that $f$ can be composite and have a non-smooth component, and explain Moreau and its connection with the proximal operators with 2 definitions. Finally, you can start your demonstration. Some space would be saved by shortening in \"Background and Notations\" section can be used to do one more experiment (see above).*\n    - We rewrote entirely the problem setup section, which now presents more clearly what our aim is and which components are involved in the learning algorithm. Unfortunately the vector $\\mu$ is needed here when trying to write down the training objective (equation (3) in revision).\n    - Regarding the background on Moreau envelope and proximal map we removed the unnecessary part on the intuition and instead added references as suggested by the reviewers, we also removed the relation between proximal gradients and projected gradients. However, we still think that it is important to give some background on the proximal point method and proximal gradient descent here as these algorithms are so closely connected to our Lagrangian versions of them.\n\n- *Missing hypotheses in the Moreau envelope definition: $f$ must be proper and lower semi-continuous.*\n    - Thank you for pointing this out, we added this in the revision.\n\n- *Instead of establishing a connection between the Moreau envelope and the proximal operator, refer the reader to a chapter in a well-known textbook that already explains in great detail the connection (e.g. Bauschke and Combettes).*\n    - We included the additional reference and restructured the section as described above.\n\n- *In your experiments figure (figure 2), how do you explain that the $LPGD$ (avg) yields a significantly lower loss compared to the $LPGD$ (lower) and $LPGD$ (upper) schemes? Could you provide an intuition?*\n    - This is intuitively similar to the central-difference method having higher precision than forward or backward difference methods. Further intuition can be gained from Figure 1, which highlights that the different envelopes provide non-zero gradients of the envelope in different subsets of the domain, while $LPGD$ (avg) is able to use informative gradient information in both subsets. We added this intuition to the results section of the revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525653979,
                "cdate": 1700525653979,
                "tmdate": 1700525653979,
                "mdate": 1700525653979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wiUbDVaYEU",
                "forum": "KP4xJQcG3H",
                "replyto": "YkiQRAqcEp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5435/Reviewer_m9w5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5435/Reviewer_m9w5"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their comprehensive answers to my concerns. I raised my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649908347,
                "cdate": 1700649908347,
                "tmdate": 1700649908347,
                "mdate": 1700649908347,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "udjJqUfj4z",
            "forum": "KP4xJQcG3H",
            "replyto": "KP4xJQcG3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5435/Reviewer_R9P5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5435/Reviewer_R9P5"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a learning model with parameter $\\theta$ that produces another set of parameters $w$ that defines a primal-dual optimization problem with solution $z^* = (x^*, y^*)$. The goal is to optimize the whole pipeline with respect to the problem defining parameters $w$ that can be then connected to model parameter $\\theta$. Since with respect to $w$, this is a non-smooth problem, the authors design a framework inspired by classical proximal gradient method (PGM). The new framework is named Lagrangian PGM since a \"divergence\" based on difference of Lagrangians is used to define the gradient-descent type update. The authors show an asymptotic result that as the coefficient in front of the Lagrangian \"divergence\" term approaches to 0, the true gradient is achieved."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem considered in the paper and the motivation are interesting (albeit difficult to understand in the current presentation). The authors are making connections with classical approaches in nonsmooth optimization such as Lagrangian, Moreau envelopes, mirror descent and Bregman divergences to get inspiration to tackle the specific nonsmooth optimization problem at hand. The subtle and important aspects such as assuming the existence of unique solutions are pointed out and some effort has been made to partially address these shortcomings. It is also good to see the authors discussing the limitations in Appendix A with preliminary ideas on how one might go around improving on them."
                },
                "weaknesses": {
                    "value": "Unfortunately, the paper is not written in a precise way for problem statements, results, tools etc. In many places it is not clear what the goal is and hence the contribution of the paper is quite unclear both in terms of mathematics and also practical aspects (not clear how to solve subproblems etc.). The assumptions and discussions about them are unclear. Even though many technical concepts are introduced, not much is proven with them and much of the claims in the paper are not proven (a list is below). The problem definition, goal etc are not clearly written and it requires the reader to read multiple times to even understand this. Unfortunately, the presentation needs to be vastly improved for this paper to be suitable for readers to understand and gain from.\n\n- Proper references are not given in many places. For example, last paragraph in page 1 cites for \"traditional proximal optimization\" the papers by Moreau, 1962 (an excellent reference) and Parikh&Boyd, 2014, Boyd&Vandenberghe, 2004 which are okay (even though it is not clear where in Boyd&Vandenberghe's book proximal methods are discussed) however many of the most important references are not there. For example, no mention of Rockafellar who is the founding father of the field along with Moreau, his book Convex Analysis (at the least) should be cited here with many of his other founding papers. One of the most impactful proximal gradient based algorithms is FISTA by Beck and Teboulle which is also not cited. Nesterov's or Tseng's accelerated proximal methods are not cited. Another important reference is by Figueiredo, Nowak, Wright, 2007 which is again, not cited. It is also not hard to find these references, they are all included in Parikh & Boyd's text. \n\n- Sect 3.1, problem setup is not written well. One can write this in a much clearer fashion. Especially for optimization audiences, it is much better to write a well-defined problem, rather than the arrows and unclear notation in the start of Sect 3.1. For example, one should define precisely the \"model\" $W_\\theta$. What kind of a function is it? What assumptions are needed on it? One then needs to write the optimization problem in terms of the main variable ($w$ or $\\theta$) and then clearly describe the assumptions on each function appearing in the problem.\n\n- Problem (1) is well-defined but it depends on $w$ hence it does not really describe the whole problem. It should be written in a precise problem formulation by taking into account the relations with arrows given at the start of the section.\n\n- For problem (1) the authors assume that unique solution exists, which is, of course a very strong assumption. The authors say they address this point in App F, which in my reading goes to a tangent about \"graphical derivatives\" rather than showing us how to get around this assumption. In particular, how to generalize the results in the paper without assuming unique solution? A precise statement is needed rather than an extended discussion which is both difficult to follow and also difficult to follow to other parts of the paper. What would be good is to point out how the particular statements in the paper will change when one considers non-unique solutions etc. Right now, this is not very understandable unfortunately.\n\n- Sect 4.2 requires eq. (14, 15) are uniquely attained, why would this be true? App F or G does not really show this (or even if it does, it is very unclear). Please provide precise statements and then proofs in App F and G. Right now it is unclear what the authors are trying to show.\n\n- It is not clear how one actually implements the algorithms described in the paper. In fact, the algorithms are not even written down, only gradients are derived for example in eq. (28, 29).  I understand the algorithm is just gradient descent but the gradients depend on many other things such as $z^*$ and $\\tilde z^\\tau$ that the reader needs to track down in the quite heavy text. They need to be presented in a compact fashion to be able to see what the algorithm is. Next, lookng at eq. (28, 29), how does one solve for $z^*$ and $\\tilde z^\\tau$? The authors mention here \"same efficient optimization algorithm used to solve (1)\": what is this algorithm? What kind of an algortihm are we lookng for? What is its cost? Clearly, one cannot solve the problem to exact solution, how does one tolerate inexactness?\n\n- Example 2 again refers to Appendix F for an \"in-depth\" discussion whereas the discussion in Appendix F is quite hard to connect to things in the main text. As I asked above, App F and G should be more precise with statements and proofs and connections to the main text.\n\n- Where in App G the proof for Theorem 4.1 with non-unique solutions given?"
                },
                "questions": {
                    "value": "More questions also appear in \"Weaknesses\" section of my review.\n\n- Last paragraph of page 1 states that the paper can solve problems with non-linear objectives or learnable constraints. First, it is not clear what \"learnable constraints\" mean. Second, the paper itself says in Sec 4.3 that eq. (16) and (17) might \"diverge\" and hence they focus on linear loss in Sec 4.5. How does the paper handles non-linear loss functions? Also, what does it mean for (16) a value of an optimization problem and (17) the optimizer of the problem to \"diverge\"? Do the authors mean that a solution might not exist? Or do the authors mean that an algorithm might diverge for solving the problem? It is quite unclear.\n \n- What is the purpose of introducing $D_2$ in eq. (5)? clearly it is just the Euclidean squared norm. I can see that the authors generalize later to a \"divergence\" in Sect 4.1 but $D_2$ notation is not really necessary here given that the paper has so many other notations making it difficult to follow for a reader.\n\n- With Moreau smoothing, we have an extended theory on the smoothing affect, but the authors just replace the squared norm with the \"Lagrangian divergence\" and state that now we have a smoothed objective. This needs to be proven. Why is this envelope smooth? With which Lipschitz constant? What does one need to assume about the Lagrangian to be able to argue smoothness of the envelope? For sure justification is needed since as the authors also cite in App C, Bauschke et al., 2018 did a systematic study with Bregman Moreau envelopes. Such a precise statement justifying \"smoothness\" is missing.\n\n- eq. (12): bad notation, x on the left and right hand sides are not the same.\n\n- eq. (13): either needs to be proven or given a reference.\n\n- Where are the proofs of Prop 4.2 and 4.3?\n\n- Sect. 4.4: why are the assumptions of Danskin's theorem satisfied in this case, can the authors please clarify?\n\n- Sect. 4.5: what does it mean to \"expose the linear parameters of the Lagrangian\" right before eeq. (25)? Where is $\\Omega$ defined?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698857654310,
            "cdate": 1698857654310,
            "tmdate": 1699636552029,
            "mdate": 1699636552029,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mAV3kN89i9",
                "forum": "KP4xJQcG3H",
                "replyto": "udjJqUfj4z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer R9P5 (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive criticism. We agree with many of the weaknesses regarding the original manuscript, and significantly improved the presentation and quality in the revised version. This includes, among many other changes, a clarified problem formulation with updated and precisely formulated assumptions, proofs and corrections for the claims in the paper, and a clarified presentation of the implementation of our method including an algorithm box. We provide more detailed answers to the individual points of critique below.\n\n- *Unfortunately, the paper is not written in a precise way for problem statements, results, tools etc. In many places it is not clear what the goal is and hence the contribution of the paper is quite unclear both in terms of mathematics and also practical aspects (not clear how to solve subproblems etc.). The assumptions and discussions about them are unclear. Even though many technical concepts are introduced, not much is proven with them and much of the claims in the paper are not proven (a list is below). The problem definition, goal etc are not clearly written and it requires the reader to read multiple times to even understand this. Unfortunately, the presentation needs to be vastly improved for this paper to be suitable for readers to understand and gain from.*\n    - In the revision we make the assumptions more precise (both in the problem statement and in the proofs). We also include additional proofs in a more standard format in the revised Appendix F.\n    - Regarding the unclear main contribution of our work, we want to stress that it is in highlighting similarities between various methods for embedding paramterized optimization problems in machine learning architectures and showcasing that the corresponding update rules can be derived from principles routed in traditional optimization algorithms. We made this goal clearer in the revision, see e.g. the changes in the abstract and introduction.\n\n- *Proper references are not given in many places. For example, last paragraph in page 1 cites for \"traditional proximal optimization\" the papers by Moreau, 1962 (an excellent reference) and Parikh\\&Boyd, 2014, Boyd\\&Vandenberghe, 2004 which are okay (even though it is not clear where in Boyd\\&Vandenberghe's book proximal methods are discussed) however many of the most important references are not there. For example, no mention of Rockafellar who is the founding father of the field along with Moreau, his book Convex Analysis (at the least) should be cited here with many of his other founding papers. One of the most impactful proximal gradient based algorithms is FISTA by Beck and Teboulle which is also not cited. Nesterov's or Tseng's accelerated proximal methods are not cited. Another important reference is by Figueiredo, Nowak, Wright, 2007 which is again, not cited. It is also not hard to find these references, they are all included in Parikh\\&Boyd's text.*\n    - Thank you for pointing out these missing references; we include them in the revision.\n\n- *Sect 3.1, problem setup is not written well. One can write this in a much clearer fashion. Especially for optimization audiences, it is much better to write a well-defined problem, rather than the arrows and unclear notation in the start of Sect 3.1. For example, one should define precisely the \"model\" $W_\\theta$. What kind of a function is it? What assumptions are needed on it? One then needs to write the optimization problem in terms of the main variable ($w$ or $\\theta$) and then clearly describe the assumptions on each function appearing in the problem. Problem (1) is well-defined but it depends on $w$ hence it does not really describe the whole problem. It should be written in a precise problem formulation by taking into account the relations with arrows given at the start of the section.*\n    - We completely restructured Section 3.1. It now includes a well-defined problem formulation and lists the assumptions that we make."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525198818,
                "cdate": 1700525198818,
                "tmdate": 1700525198818,
                "mdate": 1700525198818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4oMsaltQVR",
                "forum": "KP4xJQcG3H",
                "replyto": "dKGkdYs84G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5435/Reviewer_R9P5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5435/Reviewer_R9P5"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "I thank the authors for their revision and the attempt to address my comments. Even though things get clearer, unfortunately there are still so many aspects of the paper that needs to be improved. \n\n- The extensive changes will need to be checked thoroughly in a proper review cycle, instead of a short post-rebuttal period (from the time of the rebuttal to the end of discussions was 2 days for this submission). This is because fundamental aspects of the paper has changed. The authors move from unique/non-unique solution treatment to using continuous selection from solution set. Now they no longer argue smoothness (in view of my comments) where the first version was all about \"smoothed envelopes\", without smoothing, solving subproblems become even harder of course. Danskin's theorem's central use was not correct and now they cite a paper by Oyama, Takenawa, 2018 that I am not aware and the new argument should be thoroughly checked.\n\n- The results (at least what they mean) are still unclear. Theorem 5.1 is okay, we are asymptotically estimating the gradient. However, Proposition 5.2 and 5.3 are still unclear. We are interested in an end-to-end pipeline but the result is for fixed $w$. We know that for a fixed $w$, the prob (1) is solvable, this is the primitive that the paper relies on. But then what do these propositions mean for a fixed $w$? Fixed $w$, we already have guarantees. The guarantees should be clearly explained (what we get and we cannot get from them) in line with the motivation of the paper (we are not interested in a solution for a fixed $w$, but we are interested in the whole learning pipeline). For a theoretical treatment, more precision is needed. It is not clear how significant the results are because of this.\n\n- The algorithm in Alg 1 is still unclear. I asked about how to estimate $z^*$ and other values, algorithm just says \"load $z^*$\" with no reference to where it is even defined. With so many definitions, it is so hard to follow these things to see what the primitives of the algorithm are (what kinds of solvers one would need and what the solvers would provide), what the precise results are etc.\n\n- Role of inexactness for subproblems is not investigated. The authors say in their rebuttal it is not clear how to handle this in the theory. But of course we are not going to get exact solutions by solving a min-max problem (the solvers are even slower without smoothness), especially with large-scale problems. This is an important point that needs to be addressed in such a paper. We are lacking an end-to-end algorithmic pipeline here.\n\nIn summary, unfortunately, I still do not think that the results are clear or strong enough yet for publication. Of course it is a positive that the authors incorporated feedback and fixed incorrect points in their paper, but this brings into question the state of the paper when it was submitted. With the significant amount of changes made in the revision stage, the paper needs to go through a proper review cycle rather than 2 days for verifying the new claims.  I am sorry but these are the reasons that I have to keep my score as is."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670389452,
                "cdate": 1700670389452,
                "tmdate": 1700670389452,
                "mdate": 1700670389452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]