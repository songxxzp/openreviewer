[
    {
        "title": "Mirage: Model-agnostic Graph Distillation for Graph Classification"
    },
    {
        "review": {
            "id": "FFEqsQNz5f",
            "forum": "78iGZdqxYY",
            "replyto": "78iGZdqxYY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9334/Reviewer_XL6A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9334/Reviewer_XL6A"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a graph classification model called MIRAGE, which is a graph distillation method based on frequent pattern mining. MIRAGE takes advantage of the design features of the message passing framework, decomposes the graph into calculation trees, and uses the distribution characteristics of the calculation tree to compress calculation data. Compared with existing graph distillation algorithms, MIRAGE shows advantages in key indicators such as prediction accuracy, distillation efficiency, and data compression rate. Furthermore, MIRAGE only relies on CPU operations, providing a more environmentally friendly and energy-efficient graph distillation method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is a novel idea to transform the graph distillation problem into the problem of graph decomposition.\n2. This paper clearly expresses the distillation problem in mathematical language and naturally transforms it into a graph decomposition problem.\n3. The experimental configuration is described in detail, and the experimental results are good and reasonable."
                },
                "weaknesses": {
                    "value": "1. The datasets used in the experiment is too small."
                },
                "questions": {
                    "value": "1. Would MIRAGE still be as effective if used on a very large data set?\n2. Why is MIRAGE, which only uses the CPU, so much faster than DOSCOND and KIDD, which can use the GPU?\n3. If a datasets is large, but its global computation tree is very similar, will such a datasets affect the effect of MIRAGE?\n4. How will MIRAGE perform in multi-classification tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760032362,
            "cdate": 1698760032362,
            "tmdate": 1699637174795,
            "mdate": 1699637174795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pVqevr5ms6",
                "forum": "78iGZdqxYY",
                "replyto": "FFEqsQNz5f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XL6A"
                    },
                    "comment": {
                        "value": "**Q1. Would MIRAGE still be as effective if used on a very large dataset?** \n\n*Response:* We do not expect graph size to have any effect on the performance of MIRAGE since the underlying principles are not dependent on graph dataset size.  This is already evident from Table 2. Specifically, we compute the pearson's correlation coefficient between dataset size and the generalization error (AUC in full dataset-AUC of MIRAGE). The correlation is 0.153 with $p$-value of 0.743 and thus rejecting any bias in AUC from dataset size.\n\n**Q2. Why is MIRAGE, which only uses the CPU, so much faster than DOSCOND and KIDD, which can use the GPU?** \n\n*Response:* Existing algorithms, namely DosCOND and KiDD, seek to replicate the gradient trajectory of model parameters on the full dataset. Hence, they need to train on the full dataset for distillation (and as we comment in Sec 1, this compromizes the basic premise of data distillation). In contrast, MIRAGE does not require training on the full dataset. Rather, MIRAGE emulates the input data processed by message-passing GNNs. By shifting the computation task to the pre-learning phase, MIRAGE is not only faster, but also independent of the GNN architecture. \n\n**Q3. If a datasets is large, but its global computation tree is very similar, will such a datasets affect the effect of MIRAGE?** \n\n**Response:** As we discussed in our response to Q1, the size of the dataset does not affect the performance of MIRAGE. The performance depends primarily on the frequency distribution of computation trees in a dataset. If the distribution is skewed, MIRAGE is expected to perform well since the top-$k$ most frequent computation trees would effectively capture a substantial portion of the distribution mass. On the other hand, if the distribution is not skewed, MIRAGE is not the most suitable distillation technique for such scenarios. This aspect is explicitly acknowledged in our conclusion (Sec 5). The relevant text is reproduced verbatim below.\n\n> Finally, MIRAGE relies on the assumption that the distribution of computation trees is skewed. Although we provide compelling evidence of its prevalence across a diverse range of datasets, this assumption may not hold universally, especially in the case of heterophilous datasets. The development of a model-agnostic distillation algorithm remains an open challenge in such scenarios.\n\n**Q4. How will MIRAGE perform in multi-classification tasks?**\n\n*Response:* We have added a new dataset, namely IMDB-M, to evaluate MIRAGE's performance on multi-class classification. As we see below, MIRAGE produces the best performance. These results, and the details of the IMDB-M dataset has been added to Table 2 and Table 1 respectively in the revised manuscript. \n\nArchitecture|RANDOM(Mean)|RANDOM(sum)|HERDING|Kidd|DosCond|Mirage|\\|Full Dataset|\n-|-|-|-|-|-|-|-|\nGCN|55.10$\\pm$ 3.8|52.90 $\\pm$ 2.52|61.0 $\\pm$ 2.4|57.1 $\\pm$ 1.11|55.9 $\\pm$ 1.06|**63.20 $\\pm$ 1.12**|\\|64.10 $\\pm$ 1.1|\nGIN|60.1$\\pm$ 2.67|56.30 $\\pm 5.5$|58.47 $\\pm$ 4.12|54.18 $\\pm$ 0.90|58.30 $\\pm$ 1.70|**61.8 $\\pm$ 1.51**|\\|64.8 $\\pm$ 1.10 | \n\nThe size of the dataset (in bytes) is as follows:\n\n| $\\frac{\\text{ Method}\\rightarrow}{\\text{ Dataset}\\downarrow}$ | Herding(GCN) | Herding(GIN) | KiDD |DosCond(GCN) | DosCond(GIN) | MIRAGE |\\|Full Dataset|\n|-------------------------------------------------------------------------|--------------|---------|---------|---------|------------|--------------|----------\n|IMDB-M|1,156|1,256|936|720|824|**228**|\\|645,160|\n-----"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310687474,
                "cdate": 1700310687474,
                "tmdate": 1700312207630,
                "mdate": 1700312207630,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hcWsi2mqtE",
            "forum": "78iGZdqxYY",
            "replyto": "78iGZdqxYY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9334/Reviewer_yzvf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9334/Reviewer_yzvf"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a model-agnostic graph distillation algorithm for graph classification, which aims to distill the original graph dataset into a small and compact synthetic dataset maintaining competitive model performance. It is pointed out that existing distillation methods typically rely on model-related information and need to train on the full dataset, resulting in poor generalization and efficiency issues, respectively. To address these weaknesses, MIRAGE directly compresses the computation trees to synthesize datasets by mining sets of frequently co-occurring items. This is inspired by the observation that the distribution of the computation trees, formed by decomposing graphs based message-passing GNN frameworks, always exhibits high skewness. Therefore, it is possible to approximate the true graph representation by aggregating the root representations of only the highly frequent trees. Extensive benchmarking demonstrates the superiority of MIRAGE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper distills the original graph dataset into a novel form, namely the sets of computation trees, instead of traditional feature and structure information, i.e. X\u2019 and A\u2019.\n\n2. This paper not only improves generalization by avoiding model-related computation but also addresses the limitation of the traditional distillation step that necessitates training on the entire dataset.\n\n3. MIRAGE demonstrates superiority from various metrics, i.e. predictive accuracy, distillation efficiency, and data compression rates."
                },
                "weaknesses": {
                    "value": "1. Unlike DOSCOND and KIDD, MIRAGE seems infeasible to be generalized to node classification tasks.\n\n2. There is no theoretical analysis nor experimental proof to investigate the error when approximating the true graph representation by aggregating the root representations of only the highly frequent trees.\n\n3. MIRAGE compresses the computation trees by mining the set of frequently co-occurring items. However, we can consider an extreme situation in which two computation trees are co-occurring in all graphs, but their frequencies in each graph are very low. So, these trees can\u2019t approximate the true graph representation, which will destroy the synthetic dataset performance.\n\n4. The synthetic dataset can solely be used to train GNNs based on the message-passing framework, but can\u2019t used to train certain spectral GNNs, such as ChebyNet with multiple propagation steps."
                },
                "questions": {
                    "value": "1. Original dataset labels are still used during the distillation procedure. So is it a strictly unsupervised algorithm from this perspective?\n\n2. There is no clear statement about how Figure 1 is plotted. And I can't get what the x-axis and y-axis represent, respectively."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9334/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9334/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9334/Reviewer_yzvf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828544713,
            "cdate": 1698828544713,
            "tmdate": 1699637174676,
            "mdate": 1699637174676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZhtI7VvPl8",
                "forum": "78iGZdqxYY",
                "replyto": "hcWsi2mqtE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yzvf: Part 1"
                    },
                    "comment": {
                        "value": "**1. Unlike DOSCOND and KIDD, MIRAGE seems infeasible to be generalized to node classification tasks.**\n\n*Response:* While the primary focus of MIRAGE is on graph classification (as noted in the title itself), MIRAGE does generalize to node classification. We now empirically demonstrate this ability in App. E. (*Note:* KIDD also does not discuss node classification. Only, DosCOND does).\n\nGeneralization to node classification is simple since learning the node embeddings is a necessary step before aggregating them to form the graph embedding (Recollect Eqs. 4 and 5). Specifically, the pipeline follows in an identical manner except the omission of (Eq. 5) and the loss function being applied to node embeddings instead of graph embeddings.\n\nWe use ogbg-molbace to analyze performance in node classification. Here, each node is labeled as aromatic or non-aromatic depending on whether it is part of an aromatic ring substructure. The results are tabulated in Table H. Consistent with previous results, MIRAGE outperforms DosCOND in both AUC-ROC and compression (smaller size of the distilled dataset). DosCOND produces a dataset that is $\\approx 4$ times the size of that produced by MIRAGE, yet performs more than $7\\%$ lower in AUC-ROC. This coupled with model-agnostic-ness further solidifies the superiority of MIRAGE\n\n**2. There is no theoretical analysis nor experimental proof to investigate the error when approximating the true graph representation by aggregating the root representations of only the highly frequent trees.** \n\n*Response:* The above question is indeed important. We have now included a new experiment to evaluate the *sufficiency* of the frequent computation tree patterns.\n \n **Sufficiency of frequent patterns:** In order to establish sufficiency of frequent tree patterns in capturing the dataset characteristic, we conduct the following experiment. We train the model on the full dataset and store its weights at each epoch. Then, we freeze the model at the weights after each epoch's training and pass both the distilled dataset consisting of just the frequent tree patterns, and the full dataset. We then compute the differences between the losses as shown in Figure I (GCN, GAT, and GIN). The rationale behind this is that the weights of the full model recognise the patterns that are important towards minimizing the loss. Now, if the same weights continue to be effective on the distilled train set, it indicates that the distilled dataset has retained the important information. In the figure, we can see that the difference quickly approaches $0$ for all the models for all the datasets, and only starts at a high value at the random initialization where the weights are not yet trained to recognize the important patterns. Furthermore, gradient descent will run more iterations on trees that it sees more often and hence infrequent trees have limited impact on the gradients. These results empirically establish the sufficiency of frequent tree patterns in capturing the majority of the dataset characteristic.\n \n **Additional information relevant to this question:** In Fig. 1, we show that the distribution of computational trees is long-tailed and hence by considering only the top-$k$ most frequent trees, majority of the input information processed by the GNN would be retained. Furthermore, in Fig. 5 (b), we show that the training loss against epochs is similar for the full dataset and distilled dataset."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310401797,
                "cdate": 1700310401797,
                "tmdate": 1700310401797,
                "mdate": 1700310401797,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dMlkGSXClz",
                "forum": "78iGZdqxYY",
                "replyto": "hsnP47yZ7T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9334/Reviewer_yzvf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9334/Reviewer_yzvf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification and I will maintain my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540421655,
                "cdate": 1700540421655,
                "tmdate": 1700540421655,
                "mdate": 1700540421655,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MHRwBKHkfy",
            "forum": "78iGZdqxYY",
            "replyto": "78iGZdqxYY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9334/Reviewer_pdUR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9334/Reviewer_pdUR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a new graph distillation approach called MIRAGE for graph classification, which addresses limitations in existing methods. MIRAGE leverages the idea that message-passing GNNs break down input graphs into computation trees with skewed frequency distributions, allowing for concise data summarization. Unlike traditional methods that emulate gradient flows, MIRAGE compresses the computation data itself, making it an unsupervised and architecture-agnostic distillation algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. It is crucial to address the issue of existing costs in training large-scale graphs through graph distillation, as it remains an under-explored problem.\n2. It is interesting to observe how the authors have proposed an unsupervised approach for graph distillation, eliminating the requirement for parameters and model architecture, and still achieving comparable performance to the original dataset.\n3. The methodology is interesting, and the performances achieved by their approach are amazing."
                },
                "weaknesses": {
                    "value": "1. In the results presented in Table 2, why there is a discrepancy compared to those from the original DosCond paper? Could the authors clarify why the authors deviated from the settings employed in the DosCond paper? \n\n2. It would be beneficial if the authors could delve deeper into the sensitivity of their approach with respect to distillation parameters \\theta_1 and \\theta_2. Undertaking further experiments to report the performance across varied distillation parameters might offer greater insights, given that these parameters significantly influence the size of the distilled dataset. \n\n3. On the ogbg-molhiv dataset, could the authors provide an explanation for the better performance observed achieved by distilled data compared to the original dataset.\n\n4. As we are removing the nodes that exist in the less-occurring computational trees, how can we be sure that we are not losing important information in the dataset (especially since this process does not consider the feature information)?  Can authors justify it theoretically or/and empirically by running some experiments?"
                },
                "questions": {
                    "value": "My major questions are on the experimental evaluation. It would tremendously strengthen this work by addressing the concerns listed in the Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877752561,
            "cdate": 1698877752561,
            "tmdate": 1699637174570,
            "mdate": 1699637174570,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dofe5krBl1",
                "forum": "78iGZdqxYY",
                "replyto": "MHRwBKHkfy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pdUR: part 1"
                    },
                    "comment": {
                        "value": "**Q1. In the results presented in Table 2, why there is a discrepancy compared to those from the original DosCOND paper? Could the authors clarify why the authors deviated from the settings employed in the DosCOND paper?**\n\n*Response:* The results in DosCOND do not exactly align with ours due to the following reasons:\n* **Input data:** The graph datasets used (both in ours and DosCOND) are node and edge labeled. In DosCOND, ***only*** the node labels are used for predictive modeling, whereas we use both node and edge labels. We believe this is a better empirical design since generalization error between full and distilled dataset should be measured after incorporating the full input information.\n* **Metrics:** A direct comparison between the AUCROC numbers of DosCOND and ours is possible only in the three datasets of ogbg-molbace, ogbg-mohiv, and ogbg-molbbbp, since DosCOND does not use AUCROC as the evaluation metric in the other datasets. We use AUCROC across all datasets. \n    * **Performance of DosCOND:** In all three datasets, as we show in the table below (columns 2 and 3), **DosCOND's AUCROC improves in our evaluation.** \n    * **AUCROC on full dataset training**: We observe significantly reduced AUCROC only in ogbg-molhiv (columns 4 and 5). We conducted hyper-parameter search over a larger space than in our initial setup for ogbg-molhiv (full dataset training) to revisit this issue and identified that reducing the weight of the L2 regularizer in the loss function from 0.01 to 0.00001 improves AUCROC to $75.93$ in GCN. A similar improvement is observed in GAT and GIN as well. This update has now been incorporated in the revised draft (highlighted in blue font in Table 2). \n\nDataset|Doscond AUCROC in DosCond (#Graphs=1)| DosCond AUCROC in ours (GCN) | Full dataset AUCROC in DosCond | Full Dataset AUCROC in ours (GCN)\n-|-|-|-|-|\nogbg-molbace|65.7|**67.34**|71.4|**77.31**\nogbg-molbbbp|58.1|**61.30**|64.6|64.43\nogbg-molhiv|72.6|**73.16**|**75.7**|64.78 $\\rightarrow$ 75.93\n\n* **Other things to note:** DosCond reports numbers only for GCN, whereas we include GCN, GAT and GIN. The budget used in our experiments is set to 1 graph per class since even with this budget, MIRAGE is smaller than DosCond (details in Table 3).\n\n**Q2. It would be beneficial if the authors could delve deeper into the sensitivity of their approach with respect to distillation parameters $\\theta_1$ and $\\theta_2$. Undertaking further experiments to report the performance across varied distillation parameters might offer greater insights, given that these parameters significantly influence the size of the distilled dataset.**\n\n*Response:* We appreciate this constructive feedback. We have significantly expanded the studies on parameter sensitivity. Below we summarize the experiments conducted and the key insights derived from them.\n\n**Impact of Frequency Threshold on distillation time, compression and accuracy (AUCROC):** In Section 4.5 (Fig. 6 presents the results), we measure the impact of frequency threshold on the distillation time. MIRAGE is significantly faster than training on full datasets unless the frequency thresholds are set to extreme values where the distilled dataset reaches equality with the full dataset.\n\nIn Fig. L in Appendix, we present the relationship between compression and AUCROC. The compression ratio is controlled by increasing the frequency threshold, i.e., the higher the threshold, the more the compression is. As noted, there is marginal improvement in AUCROC with lower thresholds (higher distillation size). This trend can be explained from the long-tailed distribution of computation trees. Even if we incorporate more computation trees, they have negligible impact on capturing more of the distribution mass and hence limited effect on AUCROC.\n\n**Impact of number of hops (layers):**  The details are furnished in Appendix F and H of the appendix. We note the main observations here. First, even under high hop counts from the GNN perspective, the distillation process is more time-efficient than complete dataset training. Moreover, the distilled dataset's performance converges closely with that of the full dataset, as evident in Table 2. Second, the distillation process exceeds the time of full dataset training solely under extreme threshold values. This divergence occurs when distilled dataset reaches equality with the full dataset in size post-distillation. \n\nIn Fig. K in Appendix, we present the impact of hops on the AUC. We see mild deterioration in AUC at higher hops. This is consistent with the literature since GNNs are known to suffer from oversmoothing and oversquashing at higher layers[1].\n   \nIn summary, for pragmatic threshold values, the dataset distillation procedure consistently manifests as a significantly faster option to full dataset training.\n\n[1] Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J. Sutherland, and Ali Kemal Sinop. Exphormer: Sparse transformers for graphs. In ICML, 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310172139,
                "cdate": 1700310172139,
                "tmdate": 1700310172139,
                "mdate": 1700310172139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s4PLHB1Odg",
                "forum": "78iGZdqxYY",
                "replyto": "YRqcoTIBFY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9334/Reviewer_pdUR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9334/Reviewer_pdUR"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. I tend to accept this paper and will maintain my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581120556,
                "cdate": 1700581120556,
                "tmdate": 1700581120556,
                "mdate": 1700581120556,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NCU5oHaNLV",
            "forum": "78iGZdqxYY",
            "replyto": "78iGZdqxYY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9334/Reviewer_2KBE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9334/Reviewer_2KBE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MIRAGE, a model-agnostic graph distillation method for graph classification task. Specifically, the authors first observe that existing graph distillation methods still rely on training with the full dataset and specific to GNNs architectures and hyper-parameters. To this end, MIRAGE can tackle these two limitations via decomposing input graphs into computation trees. The long-tailed frequency distribution of the computation tree enables the graph distillation with a high compression ratio. Experiments demonstrate the superiority of MIRAGE in terms of its effectiveness, compression ratio, and distillation efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper is well-organized and easy to follow.\n2.\tThe idea of making graph distillation model-agnostic is interesting and meaningful in practice. \n3.\tThe proposed data-centric solution is a natural way to mitigate model hyperparameter dependency using the frequency distribution of the computation tree."
                },
                "weaknesses": {
                    "value": "1. Motivations: \n- (a) Problem Motivation: In the abstract, one of the identified limitations pertains to the reliance on training with the full dataset. To provide a clearer argument regarding the necessity of full dataset training for existing methods and the distinct advantages of MIRAGE, we should consider the following: What is the comparative performance of existing distillation methods (e.g., gradient matching) when applied to both randomly initialized and well-trained models? Is there a substantial performance gap between the two scenarios? Regarding computational costs in the distillation process, it's worth examining the resource requirements for calculating the computation tree frequency distribution, particularly in the context of deep GNNs. How does this computational overhead compare to training models on the full dataset?  \n- (b) Technical Motivation: While the concept of the computation tree is inherent to GNNs, we should address the sufficiency of the frequency distribution for the computation tree. Key points to explore include: Why is maintaining a high computation tree frequency distribution sufficient for effective learning? What is the distribution pattern of the computation tree, and does it exhibit a long-tail distribution?\n\n2. Objectives: Ideally, the distillate data should mirror the distribution of the full dataset, implying that the gradients of the distillate data should align with those of the full dataset for any model (random or well-trained). It's essential to clarify that while distillation can reduce computation costs, the primary objective remains data distillation. Additionally, considering the architecture transferability for existing graph distillation methods, the advantage of model-agnostic techniques is not significant.\n\n3. Experiments: \n- (a) Comprehensive Model Comparison: In evaluating baselines, it would be beneficial to perform a comprehensive comparison across various model architectures. Specifically, assessing performance across different architecture pairs would enhance the evaluation's robustness and provide insights into transferability.\n\n- (b) Baselines and Model Initialization: Investigate the significance of GNN model training prior to distillation for baselines. Explore performance outcomes for well-trained, randomly initialized, and warm-up trained models. Understanding the effectiveness of random initialization can help determine the necessity of training on the full data for baseline methods.\n\n- (c) Hyperparameter Investigation: Explore the impact of different hyperparameters, such as threshold values and the number of hops, on various metrics, including accuracy, efficiency, compression ratio, and transferability.\n\n- (d) Trade-off Analysis: Provide insights into the trade-off between accuracy, efficiency, and compression ratio. Understanding these trade-offs can offer valuable guidance for practical applications.\n\nIn a nutshell, I am currently leaning toward weak rejection. I am looking forward to the authors\u2019 response.\n\n-------------------After rebuttal -------------------------\nThanks for the response. I increase my score to 6."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9334/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9334/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9334/Reviewer_2KBE"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698882380330,
            "cdate": 1698882380330,
            "tmdate": 1700718150673,
            "mdate": 1700718150673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "91rPJlhFGV",
                "forum": "78iGZdqxYY",
                "replyto": "NCU5oHaNLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2KBE: Part 1"
                    },
                    "comment": {
                        "value": "**1a(i). Problem motivation: What is the comparative performance of existing distillation methods (e.g., gradient matching) when applied to both randomly initialized and well-trained models? Is there a substantial performance gap between the two scenarios?**\n   \n*Response:* Below we present the AUCROC numbers of DosCOND on randomly initialized DosCOND model, warm start (100 epochs) and training DosCOND until convergence (typically 1000 epochs). As visible, there is a noticeable gap in the AUCROC numbers indicating training until convergence is necessary. This discussion has been added App. D.\n\n|Dataset|Random|Warm start|Convergence|\n|-|-|-|-|\n|ogbg-molbace|$55.04\\pm9.07$|$59.25\\pm1.61$|$67.34\\pm1.84$|\n|NCI1 | $51.22\\pm2.00$|$48.18\\pm2.78$|$57.90\\pm0.75$|\n|ogbg-molbbbp|$52.64\\pm1.98$|$50.72\\pm3.48$|$59.19\\pm0.95$|\n|ogbg-molhiv|$48.21\\pm5.95$|$34.99\\pm7.25$|$73.16\\pm0.69$|\n|DD|$52.39\\pm7.19$|$61.58\\pm2.11$|$68.39\\pm9.64$|\n\n   \n**1a(ii). Regarding computational costs in the distillation process, it's worth examining the resource requirements for calculating the computation tree frequency distribution, particularly in the context of deep GNNs. How does this computational overhead compare to training models on the full dataset?**\n\n*Response:* The time taken to distill each dataset is provided in Fig. 4(a) in the form of a bar plot and the raw numbers are in Table E. MIRAGE is $\\approx$ 500 times faster on average than KIDD and $\\approx 150$ times faster than DOSCOND. To further expand on this aspect, we have added the following new experiments:\n* **Comparison to full dataset training time:** We now report the time taken to train on the full dataset in Table I in Appendix. On average, the distillation time of MIRAGE is more than $30$ times faster than training on the full dataset. In contrast DosCOND and KiDD are slower than full dataset training due to reasons already highlighted in Sec 1 of our manuscript.\n* **Impact of hops (layers):** In Fig. H (discussed in App F and referred to from Sec 4.4 in main paper), we report the growth in distillation time against the number of hops. We show that even at a larger values of hops, MIRAGE is 2 to 25 times faster, on average, than training on the full dataset. \n* **Impact of frequency threshold on distillation:** we subject the system to variations in threshold parameters, graphically showing the resulting time in Figure 6. Notably, the distillation process exceeds the time of full dataset training solely under extreme threshold values. This divergence occurs when distilled dataset reaches equality with the full dataset in size post-distillation. Conversely, for pragmatic threshold values, the dataset distillation procedure consistently manifests as a significantly faster option to full dataset training.\n   \n   \n**1b. Technical Motivation: While the concept of the computation tree is inherent to GNNs, we should address the sufficiency of the frequency distribution for the computation tree. Key points to explore include: Why is maintaining a high computation tree frequency distribution sufficient for effective learning? What is the distribution pattern of the computation tree, and does it exhibit a long-tail distribution?**\n\n *Response*: We note that the frequency distribution of computation trees is already reported in our manuscript in Fig. 1 and it is indeed a long-tail distribution. To further establish the sufficiency of frequent tree patterns, we have now dedicated an explicit section (Sec 4.4). \n \n **Sufficiency of frequent patterns:** In order to establish sufficiency of frequent tree patterns in capturing the dataset characteristic, we conduct the following experiment. We train the model on the full dataset and store its weights at each epoch. Then, we freeze the model at the weights after each epoch's training and pass both the distilled dataset consisting of just the frequent tree patterns, and the full dataset. We then compute the differences between the losses as shown in Figure 5(a) (GCN) and Figure I (GCN, GAT, and GIN). The rationale behind this is that the weights of the full model recognise the patterns that are important towards minimizing the loss. Now, if the same weights continue to be effective on the distilled train set, it indicates that the distilled dataset has retained the important information. In the figure, we can see that the difference quickly approaches $0$ for all the models for all the datasets, and only starts at a high value at the random initialization where the weights are not yet trained to recognize the important patterns. Furthermore, gradient descent will run more iterations on trees that it sees more often and hence infrequent trees have limited impact on the gradients. These results empirically establish the sufficiency of frequent tree patterns in capturing the majority of the dataset characteristics."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309916656,
                "cdate": 1700309916656,
                "tmdate": 1700309916656,
                "mdate": 1700309916656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DH6wBBBnyC",
                "forum": "78iGZdqxYY",
                "replyto": "NCU5oHaNLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9334/Authors"
                ],
                "content": {
                    "title": {
                        "value": "part 2"
                    },
                    "comment": {
                        "value": "**2(i). Objectives: Ideally, the distillate data should mirror the distribution of the full dataset, implying that the gradients of the distillate data should align with those of the full dataset for any model (random or well-trained). It's essential to clarify that while distillation can reduce computation costs, the primary objective remains data distillation.**  \n\n*Response*: As mentioned in our response to 1b, the frequency distribution of computation trees is long tailed. With the newly added sufficiency experiments, we now also show that the frequent trees are the patterns that are being learned by the model parameters as well (Response to 1b.). Finally in Fig. 5b and J, we also show that the train losses on full dataset and distilled datasets, with their own respective training weights, are also aligned. \n\n**2(ii). Objectives: Additionally, considering the architecture transferability for existing graph distillation methods, the advantage of model-agnostic techniques is not significant.**\n\n*Response:* Our manuscript includes experiments to evaluate transferability of model architectures in graph distillation. The results, reported in Table F of Appendix C, reveal there is a noticeable drop in AUCROC when the model architecture for distillation is different than the architecture used for training on the distilled dataset. The same observation has been reported in another recent work as well (See Table 3 in [1]). In KiDD [2], they theoretically establish why the distillation GNN and the training GNN needs to be same (Page 3 in KiDD). These results provide verification of why model-agnostic distillation is a problem of importance. \n\nTo better highlight these results and motivate the proposed problem, we now point to this information from the introduction itself.\n\n* [1] Beining Yang, Kai Wang, Qingyun Sun, Cheng Ji, Xingcheng Fu, Hao Tang, Yang You, and Jianxin Li. Does graph distillation see like vision dataset counterpart? NeurIPS, 2023*.(to appear)\n* [2] Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao Yang, and Hanghang Tong. Kernel ridge regression-based graph dataset distillation. ACM SIGKDD, 2023\n\n**Experiments: 3a. Comprehensive Model Comparison: In evaluating baselines, it would be beneficial to perform a comprehensive comparison across various model architectures. Specifically, assessing **performance across different architecture pairs** would enhance the evaluation's robustness and provide insights into transferability.** \n\n*Response:* Our evaluation already includes performance assessment across three different GNN architectures namely- GCN, GAT, and GIN, in Table 2. The results establish the proposed technique to be superior on average and consistently ranked among the top-2 distillation algorithms across all dataset-architecture combinations.  \n\nIn addition, cross architecture transferability, i.e., the situation where the architecture used for distillation is different from the one used for training and inference is also included in Appendix C. As discussed in our response to 2(ii), there is a noticeable drop in AUCROC, which is consistent with existing literature [1]. This result motivates the need for model-agnostic distillation algorithms.\n\n**3b. Baselines and Model Initialization: Investigate the significance of GNN model training prior to distillation for baselines. Explore performance outcomes for well-trained, randomly initialized, and warm-up trained models. Understanding the effectiveness of random initialization can help determine the necessity of training on the full data for baseline methods.**\n \n *Response*: Below we present the AUCROC numbers of DosCOND on randomly initialized DosCOND model, warm start (100 epochs) and training DosCOND until convergence (typically around 1000 epochs). As visible, there is a noticeable gap in the AUCROC numbers indicating full training is necessary. This discussion has been added App. D.\n\n\n|Dataset|Random|Warm start| Converged|\n|-|-|-|-|\n|ogbg-molbace|$55.04\\pm9.07$|$59.25\\pm1.61$|**67.34$\\pm$ 1.84**|\n|NCI1 | $51.22\\pm2.00$|$48.18\\pm2.78$|**57.90$\\pm$ 0.75**|\n|ogbg-molbbbp|$52.64\\pm1.98$|$50.72\\pm3.48$|**59.19$\\pm$ 0.95**|\n|ogbg-molhiv|$48.21\\pm5.95$|$34.99\\pm7.25$|**73.16$\\pm$ 0.69**|\n|DD|$52.39\\pm7.19$|$61.58\\pm2.11$|**68.39$\\pm$ 9.64**|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309958268,
                "cdate": 1700309958268,
                "tmdate": 1700310752468,
                "mdate": 1700310752468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PLV7ZwwkJe",
                "forum": "78iGZdqxYY",
                "replyto": "B7cg6tEBUU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9334/Reviewer_2KBE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9334/Reviewer_2KBE"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. I will increase my score to 6."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9334/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718064351,
                "cdate": 1700718064351,
                "tmdate": 1700718064351,
                "mdate": 1700718064351,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]