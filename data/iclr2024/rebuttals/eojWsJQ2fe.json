[
    {
        "title": "Prompt Engineering a Prompt Engineer"
    },
    {
        "review": {
            "id": "wgsqKruoQ0",
            "forum": "eojWsJQ2fe",
            "replyto": "eojWsJQ2fe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6421/Reviewer_MVYf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6421/Reviewer_MVYf"
            ],
            "content": {
                "summary": {
                    "value": "Optimizing prompts for LLMs is challenging, but crucial. In this work, the authors studied and proposed an automatic method to construct meta-prompts for new prompt proposal so that generated and edited prompts could be used to guide LLMs to perform better. They analyzed and investigated key components to build meta-prompt, such as providing step-by-step detailed instructions and context (see Sec 5.1 for empirical investigation). They also combined concepts in optimizers and used a gradient-based approach to refine prompts. In their experiments, they included four tasks and three existing baseline works to evaluate their proposed methods. The main results showed that PE2 approach can improve baseline performances. In addition, PE2 generates more high-quality prompts and specific prompt edits."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Originality: This paper proposed that to achieve a helpful meta-prompt we should enrich the meta-prompt with additional instructions and context. Standing on this, they developed several components to provide detailed instruction and context to prompt proposal LLM.\n* Quality: The experimental results look promising and perform better than the existing two automatic prompt optimization methods.\n* Significance: Prompt engineering is important to maximize the utility of LLMs. Instead of crafting prompts by human, this work proposed an automatic approach to leverage other LLMs to generate new prompts for downstream tasks. The numbers in their results validated the effectiveness of their proposed method."
                },
                "weaknesses": {
                    "value": "* The clarity of the way to update hard prompt with gradient-based optimizer can be enhanced and improved by providing details, especially how do we access the gradients to help LLM refine hard prompts?\n* The iteration for the optimizer is set to 3 in the experimental setup. How does this come from and be enough to optimize prompts? This part is not well-supported.\n* In Figure 3 and Figure 4, I am not sure why are we doing comparison across different training timestamps. Instead, should we focus on the final timestamp or the converged step to confirm it's finalized and optimized for any further investigations? This comparison to display the dynamics is confusing me."
                },
                "questions": {
                    "value": "* The footnote #3 in page 4 is not well-supported and is confusing. How does the analogy come from and be translated?\n* In paragraph \"Incorporating Concepts in Optimizers\", what's the concept here? Is there any formal definition about concept?\n* (minor comment) In Figure 1, the results can be improved by adding variance/range in the accuracy performance. It helps to enhance the soundness of this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6421/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794219716,
            "cdate": 1698794219716,
            "tmdate": 1699636716065,
            "mdate": 1699636716065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "db2NvkATSb",
                "forum": "eojWsJQ2fe",
                "replyto": "wgsqKruoQ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your review! After reading your review we find that there might be some confusion right now. We hope our response can resolve some of them and help you reassess our work.\n\n### Clarification\n\n> In this work, the authors studied and proposed an automatic method to construct meta-prompts \u2026\n\n* We work on an automatic method to construct prompts (NOT meta-prompts). \n* In this work, prompts refer to instructions used directly in the task, e.g., \u201cLet\u2019s think step by step.\u201d Constructing the prompts is automatic.\n* Meta-prompt refers to the instructions for refining prompts, e.g., \u201cinspect the current prompt and a batch of examples, then propose a new prompt.\u201d Constructing the meta-prompt is done manually by us in this paper.\n\n> They also combined concepts in optimizers and used a gradient-based approach to refine prompts.\n\n* We want to clarify that __our method is NOT a gradient-based optimization method__. Our method and all baseline methods in this work are operating on textual prompts, using the reasoning and text processing capabilities of LLMs and using __only forward calls of LLMs__.\n\n* Here is a minimal example:\n  * Old prompt: \u201cLet\u2019s think step by step.\u201d \n  * LLM-generated feedback (analogous to \u201cgradients\u201d): \u201cThe prompt should be edited to guide the model to perform subtraction.\u201d \n  * LLM-generated new prompt: \u201cLet\u2019s solve this problem step by step. Remember to add or subtract as needed.\u201d \nThe prompt refinement process is done by calling LLMs directly, instead of performing actual gradient descent on LLM parameters.\n\n* Regarding the optimization-inspired components, they are not actually operating on the LLM parameters or gradients. Instead, they are all verbalized in language instructions. For example, step size means we use \u201cchange up to 10 words\u201d when instructing the model to generate a new prompt.\n\n### Response to weaknesses\n\n> The clarity of the way to update hard prompt with gradient-based optimizer can be enhanced and improved by providing details, especially how do we access the gradients to help LLM refine hard prompts?\n\n* As mentioned in the example above, our method is not a gradient-based optimizer. The textual feedback generated by the model is used to steer the prompt editing process, and therefore is analogous to \u201cgradients.\u201d We hope the example above helps clarify this.\n\n> The iteration for the optimizer is set to 3 in the experimental setup. How does this come from and be enough to optimize prompts? This part is not well-supported.\n\n* We set the number of prompt edits to be 3 due to cost concerns. Empirically, performance tends to plateau after 3 steps.\nFor each prompt candidate, we need to run it on the training set to find the model errors and run it on the dev set for validation and prompt selection. This leads to a significant number of LLM calls.\n\n> In Figure 3 and Figure 4, I am not sure why are we doing comparison across different training timestamps. Instead, should we focus on the final timestamp or the converged step to confirm it's finalized and optimized for any further investigations? This comparison to display the dynamics is confusing me.\n\n* We agree with you that the prompt on final/best prompts are most important. Our table 1 and table 2 are dedicated to results on the final prompt. (Now they are table 3 and table 1 in the updated paper)\n* Figure 3 and 4 illustrates the _accuracy distribution_ of the proposed prompts over the course of prompt engineering. We create them for some additional analysis and they indeed provide valuable insights. For example, in Figure 3 (now Figure 4 after paper update) we conducted an ablation study on the meta-prompt components and we found that by removing these components, prompt quality at each timestamp is lowered.\n\n### Response to questions\n\n> The footnote #3 in page 4 is not well-supported and is confusing. How does the analogy come from and be translated?\n\n* Please refer to our clarification section above. We believe it is helpful in answering this question.\n* The textual feedback generated by inspecting a batch of model errors, and therefore this is analogous to \u201closs.backward()\u201d. \n* Generating a new prompt based on the old prompt and the textual feedback, is analogous to \u201coptimizer.step()\u201d.\n\n> In paragraph \"Incorporating Concepts in Optimizers\", what's the concept here? Is there any formal definition about concept?\n\n* The concepts are batch size, step size and momentum that are commonly used in gradient-based optimization. The bulletin points in that paragraph provide more detail on each of them.\n* Given feedback from reviewer x9xJ and reviewer uHAL, we agree this is a little confusing and we decide to more this part to the appendix in our next paper update.\n\n> Adding variance/range in Figure 1\n\n* Thank you for raising this! We will do this whenever applicable. For bars presenting the average across multiple tasks, there is not a straightforward way to compute variance and thus we cannot include them here."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6421/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647914859,
                "cdate": 1700647914859,
                "tmdate": 1700702501809,
                "mdate": 1700702501809,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mEM9puQJBE",
            "forum": "eojWsJQ2fe",
            "replyto": "eojWsJQ2fe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6421/Reviewer_iQjD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6421/Reviewer_iQjD"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents PE2, a prompt optimization method that leverages a well-designed meta-prompt to iteratively improve on proposal prompts when presented with examples of the task at hand. The meta-prompt in particular leverages two-step task description, step-by-step reasoning, and context specification. A prompt engineering tutorial is further explored but ultimately discarded due to its length and inefficacy. Furthermore, optimisation-based concepts are introduced such as batch size, learning rate, and momentum although the latter two are not used in the final PE2 due to lack of consistent empirical improvements. PE2 is evaluated on various mathematical reasoning, instruction induction, and counterfactual evaluation tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- PE2 is well-designed and the prompt choices made are justified both empirically and using real-life examples.\n- Experimental results demonstrate that components in the final PE2 contribute to improvements in accuracy. Furthermore, various other aspects such as prompt tutorial and use of momentum are also evaluated empirically before being excluded in the final method.\n- Experiments are extensive and explore various aspects of the meta-prompt including the ability to handle poor initializations, reasoning capabilities, and poor performance arising from hallucinations and ignoring instructions.\n- Overall, the paper is very well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The evaluation uses Text-Davinci-003 as the main model but GPT4 when handling the meta-prompt, prompt evaluation and update proposals. It's noted that for two baselines that used the Text-Davinci-002, the results are recreated using Text-Davinci-003. It begs the question of whether APO or APE with GPT4 paraphrases could potentially perform better than reported. Was GPT4 also used in any of the baselines? Have the authors evaluated using Text-Davinci-003 to also handle the meta-prompt? Given that hallucinations hurt performance considerably it would be important to answer these questions for a fair comparison to baselines.\n- Text-Davinci-003 further lacks some capabilities of GPT3.5 and GPT4. Although I am sympathetic to the cost constraints, I would find it valuable if the authors had any existing experimental results where either of these models is used as the main model to optimize for (even if the results are not fully run across all benchmarks).\n- I suspect that the authors will agree that as a result, one could argue that the meta-prompt itself could be promptly optimized with PE2. Although they allude to this at the very end, I believe that a more detailed discussion on this front could be useful."
                },
                "questions": {
                    "value": "Please answer the questions noted above. Overall, strong submission with several simple but empirically powerful contributions that enable improved prompt optimization. Happy to recommend acceptance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6421/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820888399,
            "cdate": 1698820888399,
            "tmdate": 1699636715941,
            "mdate": 1699636715941,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PHduIrsUYa",
                "forum": "eojWsJQ2fe",
                "replyto": "mEM9puQJBE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your review! It\u2019s very encouraging to read your review. Below are our responses.\n\n> Was GPT4 also used in any of the baselines? Have the authors evaluated using Text-Davinci-003 to also handle the meta-prompt?\n\n* Yes. Our experiments on APO and Iterative APE are using GPT-4 as the prompt proposal model. We make sure the experiments are controlled and __the meta-prompt is the only changing variable__.\n* In the initial stage of this project we tried using GPT-3.5-turbo as the prompt proposal model. The overall impression we have is that GPT-3.5-turbo is sometimes confused at the concept of prompt (task description) and the task input and cannot distinguish them. So we decide to proceed with GPT-4\n\n> Text-Davinci-003 further lacks some capabilities of GPT3.5 and GPT4. Although I am sympathetic to the cost constraints, I would find it valuable if the authors had any existing experimental results where either of these models is used as the main model to optimize for (even if the results are not fully run across all benchmarks).\n\n* We decide to use instruct models to be consistent with prior work. Unfortunately we didn\u2019t use GPT 3.5/4 models as the task mode because they are chat completion models.\n\n> I suspect that the authors will agree that as a result, one could argue that the meta-prompt itself could be promptly optimized with PE2. Although they allude to this at the very end, I believe that a more detailed discussion on this front could be useful.\n\n* Yes, we are very excited about this potential future direction!! \n* Conceptually, we can replace $p^{(t)}$ and $p^{(t+1)}$ in Eq (2) with the meta-prompt $p^{meta}$ directly. However, we think three challenges (and broader questions) arise if we pursue this method:\n  * How to collect data for such a study? To ensure this meta-prompt is universal we may need a large collection of tasks along with prompt optimization history associated with them. Creating a resource like this will be a large effort.\n  * How to automatically optimize the meta-prompt when there are no ground truth labels for prompt engineering? Math problems have ground-truth answers so that PE2 can reason on them. The task of prompt engineering does not have ground truth labels, and this makes the meta-prompt optimization process more fuzzy.\n  * It would be very costly to run and even evaluate a system like this. To evaluate one $p^{meta}$ candidate, we will need to use it for prompt optimization on various tasks as evaluation. We would expect the \u201clearning\u201d process of the meta-prompt to be a magnitude more costly.\n* We noticed two really exciting recent works related to this direction archived in the past two months: https://arxiv.org/abs/2309.16797 and https://arxiv.org/abs/2310.02304"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6421/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647299740,
                "cdate": 1700647299740,
                "tmdate": 1700647299740,
                "mdate": 1700647299740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HmRh9cxK8d",
            "forum": "eojWsJQ2fe",
            "replyto": "eojWsJQ2fe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6421/Reviewer_uHAL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6421/Reviewer_uHAL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an approach (PE2) to automatically improve the prompt used in LLMs. The key idea is to hand-design a better meta-prompt (prompt to generate better prompts). The improved meta prompt is then used in the inner loop of an iterative search process constructed over the space of prompts using a dataset of task examples to guide the search for better prompts. Experiments are performed on a variety of tasks. The results indicate that the proposed method outperforms existing automated prompt engineering techniques and one human-engineered prompts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper studies an important problem of automatically improving the input prompt to LLMs. Progress here is likely to be of wide interest to the community.\n\n+ The primary contributions of the paper are algorithmic and empirical. The main contribution is the final version of the hand-engineered meta prompt used in PE2. Experiments to study its empirical performance indicate it performs well compared to baseline prompt generators, both automated and human.\n\n+ The illustrative examples are useful to quickly grasp the main ideas being proposed. While some important implementation details are a bit difficult to follow, the appendixes contain sufficient information to mostly fill in the blanks."
                },
                "weaknesses": {
                    "value": "- The algorithmic contribution seems a bit thin. While this might be expected given the black box trial-and-error nature of prompt engineering, there is very little by way of novelty beyond the meta prompt design itself. The issue of limited novelty might be alleviated with additional insight about algorithmic components. For example, why does performance plateau quickly as $t$ increases? What kinds of prompts are generated at the end of long searches?\n\n- The current presentation makes it hard to \"separate the wheat from the chaff\". I found it challenging to quickly identify the \"final\" best variant of the meta-prompt. The paper describes a number of components, but discards some in the final version constituting PE2 (used to generate Figure 1). If I've understood correctly, discarded items include the prompt tutorial, step size and momentum. If true, perhaps the main paper might be simplified to only describe what is actually used in PE2 / Figure 1 with  supporting evidence with the rest moved to the appendix as negative experiments. Just a suggestion."
                },
                "questions": {
                    "value": "- How exactly are the examples in Line 42 in B.4 selected? It seems like PE2 uses 2 negative examples from D_train (\"hard negative sampling\"). Is this correct? Does batch_size refer to the total number of examples or just the negative examples?\n\n- Appendix C.1.1 suggests including more in-context examples (e.g., 3, 10) improves performance. If so, why is batch size set to 2 in PE2?\n\n- Where is $D_\\text{dev}$ (validation dataset) used in Algorithm 1? (Perhaps in Select-Best?)\n\n- Is the use of the optimization terminology beneficial? The term \"batch\" in the context of LLMs is reasonably well understood to refer to the set of examples used during fine-tuning and much less so to refer to the set of input-output examples in the prompt. Since step size and momentum don't seem to be anyway used in the final PE2 version, might it be clearer to simply describe the meta-prompt in its final form without reusing popular, well-understood terminology from optimization?\n\n- Do you have any insight into why prompt performance reaches a plateau so quickly (by $t$ = 3)? How much performance gain would be lost wrt Figure 1 if only a single round of improvement (t = 1) was conducted?\n\n- What, if any, effect does the choice of examples (\"batch\") have on performance? Is there a notion of \"active learning\" that might be worth incorporating into the meta-prompt? This is a complete reformulation of the optimization problem so probably outside the scope of the paper. I was wondering if you had any empirical insights here as it seems related to the primary objective of finding the prompt producing best performance wrt the tasks using an LLM."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6421/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829382562,
            "cdate": 1698829382562,
            "tmdate": 1699636715791,
            "mdate": 1699636715791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zv1I0cD3ZX",
                "forum": "eojWsJQ2fe",
                "replyto": "HmRh9cxK8d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough and detailed review! Please find our responses below.\n\n> The algorithmic contribution seems a bit thin. \u2026 The issue of limited novelty might be alleviated with additional insight about algorithmic components. \u2026\n\n* In response to \u201cWhy does performance plateau quickly as t increases?\u201d: Our hypothesis is that \u201clet\u2019s think step by step\u201d is already a near-optimal prompt and thus the performance plateaus quickly when using it as the initialization. According to Figure 4(b) in the paper (now Figure 3(b) in the updated paper), when alternative initializations are used, PE2 is able to improve the performance gradually and the performance hasn\u2019t plateaued at t=3.\n* In response to \u201cWhat kinds of prompts are generated at the end of long searches?\u201d: We summarized some notable prompt edits in Table 4, 5, 8 (Now Table 4, 5, 9 in the updated version). PE2 is able to correct wrong or incomplete task instructions, provide more specific context and details. In our upcoming paper update, we will show that PE2 can lay out multi-step plans in the prompt.\n\n> The current presentation makes it hard to \"separate the wheat from the chaff\".\n\n* Thank you for raising this point! This is also pointed out by reviewer x9xJ and thus we will update our paper by moving the meta-prompt components with mixed observations to the appendix. We will push an update to the paper very soon.\n\n> How exactly are the examples in Line 42 in B.4 selected? It seems like PE2 uses 2 negative examples from D_train (\"hard negative sampling\"). Is this correct? Does batch_size refer to the total number of examples or just the negative examples?\n\n* Yes, by default we _only_ select examples in D_train that the model produces _wrong_ answers. In this case batch_size = # negative examples = # selected examples.\n* We tried selecting random examples in D_train instead of negative examples, and this is denoted as \u201c- hard negative sampling\u201d in Table 1 (now Table 3 in the updated paper). Performance is slightly worse when using random examples.\n\n> Appendix C.1.1 suggests including more in-context examples (e.g., 3, 10) improves performance. If so, why is batch size set to 2 in PE2?\n\n* Apologies for the confusion! After the main experiments on academic benchmarks are done, we decide to try out our method on an industrial prompt. The industrial prompt is substantially longer (5k+ tokens), which creates a very different optimization space. The default hyperparameters on academic benchmarks do not work directly. This necessitates a separate hyperparameter study, which we described in Appendix C.  Furthermore, to the best of our knowledge this is the first work to look at directly optimizing very large prompts deployed application prompts.\n\n> Where is  (validation dataset) used in Algorithm 1? (Perhaps in Select-Best?)\n\n* Yes you\u2019re correct! Select-Best is based on D_dev. We will make this clearer in our updated paper.\n\n> Is the use of the optimization terminology beneficial? \n\n* As mentioned earlier, after reading the reviews we realized that this causes confusion and we will push an update to the paper. Thank you for the suggestion!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6421/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646534141,
                "cdate": 1700646534141,
                "tmdate": 1700728321043,
                "mdate": 1700728321043,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Io36McoBAr",
                "forum": "eojWsJQ2fe",
                "replyto": "HmRh9cxK8d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Do you have any insight into why prompt performance reaches a plateau so quickly (by t = 3)? How much performance gain would be lost wrt Figure 1 if only a single round of improvement (t = 1) was conducted?\n\n* As discussed above, we believe the performance plateaued quickly because \u201cLet\u2019s think step by step\u201d is a very strong initialization prompt. The optimization trajectory will be different when alternative initializations are used (Figure 4(b); now Figure 3(b) after paper update). In general, we think the answer to your question is dependent on the task and the initialization prompt.\n* During the response period, we tested the prompts found at t=1 and t=2 for three tasks: Multiarith, Date Understanding, and Movie Recommendation. (For these three tasks, the best overall prompts are all found at t=2). We hope the results below answer your question \"How much performance gain would be lost if a single round was conducted\".\n* MultiArith\n  * t=0, \u201cLet's think step by step.\u201d, Dev Acc 85.0, Test Acc 86.0\n  * t=1, \u201cLet's solve the problem step by step. First, identify all the numbers and what they represent. Then, perform the necessary calculations to find the answer.\u201d, Dev Acc 87.0, Test Acc 86.8\n  * t=2, \u201cLet's solve this problem by considering all the details. Pay attention to each piece of information, remember to add or subtract as needed, and perform the calculations step by step.\u201d, Dev Acc 92.0, Test Acc 92.3\n* Date Understanding\n  * t=0, \u201cLet's think step by step.\u201d, Dev Acc 43.0, Test Acc 39.1\n  * t=1, \u201cLet's carefully analyze the information and perform calculations if necessary, step by step.\u201d, Dec Acc 46.0, Test Acc, 43.2\n  * t=2, \u201cAnalyzing the given information, let's calculate the solution. Remember to consider the context provided, such as references to 'today' or specific dates.\u201d, Dev Acc 54.0, Test Acc 54.4\n* Movie Recommendation\n  * t=0, \u201cLet's think step by step.\u201d, Dev Acc 58.0, Test Acc 57.0\n  * t=1, \u201cConsider the genre, plot, and style of the input movies. Using this information, think step by step to identify which of the following options is most similar to the given movies.\u201d, Dev Acc 74.0, Test Acc 78.0\n  * t=2, \u201cConsidering factors such as genre, director, actors, release period, audience target, animation style, and humor, analyze the similarities among the given movies and identify the movie from the options that shares the most similarities.\u201d, Dev Acc 82.0, Test Acc 79.0\n\n> What, if any, effect does the choice of examples (\"batch\") have on performance? Is there a notion of \"active learning\" that might be worth incorporating into the meta-prompt?\n\n* Thank you for bringing this up! It will be absolutely interesting to study this. \n* For now we tried comparing a batch of _model failure examples_ and a batch of _random examples_, and found that model failures are slightly more useful (see Table 1 \u201c- hard negative sampling\u201d)(Now it's Table 3 in the updated paper).\n* Our intuition is that, similar to how the choice of examples are important to the performance of in-context learning, the choice of examples for generating feedback will be important to prompt optimization.\n* Some of our initial thoughts towards this direction: It is straightforward to identify highly educational failures for math problems. If the number is off by a lot, it suggests the failure is more fatal and more educational. However it is less clear how to actively select the most valuable examples for non-math tasks. Perhaps model confidence information will be helpful. It will be interesting to get inspiration from active learning literature too."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6421/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646816808,
                "cdate": 1700646816808,
                "tmdate": 1700702472456,
                "mdate": 1700702472456,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YC8DbLEet8",
            "forum": "eojWsJQ2fe",
            "replyto": "eojWsJQ2fe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6421/Reviewer_x9xJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6421/Reviewer_x9xJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed an approach to propose automate design for the instructions in the prompt. The approach consists of prompt initialization, new prompt proposal, search procedure. The proposed approach includes a few tricks, including Providing Detailed Instruction and Context (prompt engineering tutorial, Two-step Task Description. Step-by-step Reasoning Template, Context Specification), Incorporating concepts in optimizers (batch size, step size, history and monentum,  back-tracking\nhard negative sampling). The paper improves improvement over  other methods like APO, APE."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper works on an interesting and important problem.  The work includes a few interesting tricks, such as batch size, step size, etc\uff0c which is analogous to optimization. The paper provides good ablation study."
                },
                "weaknesses": {
                    "value": "1 The effectiveness of the proposed approach is not conclusive. \n- First, although the approach includes a few interesting tricks, the effectiveness of them are unclear, as indicated by ablation study \n\n\"The optimizer-inspired concepts can improve the performance occasionally, but the current experiments do not give a definitive conclusion regarding their utilities\"; \"We do not observe significant improvement by incorporating prompt engineering tutorial.\" \n\nWhile the readers appreciate the author's honesty and agree negative results are still informative, it will be good the author explores more on which scenarios the proposed tricks are more likely to be helpful. \n\n- Second, most of the analysis and ablation studies (Table 1,2,3) are on simple math datasets MultiArith, GSM8K. Are the proposed approach work on harder math datasets (i.e. https://paperswithcode.com/dataset/math), which are more close to real-world usage? While the paper also evaluate on \"instruction induction\" and \"counterfactual eval\" (Figure 1), the approach still haven't tested on the more representative tasks categories (i.e. QA, test summarization, etc) to be persuasive.  Automate prompt design approach should aim work on general situations. Does the approach work on a more general use case? \n\nAlso,  for GSM8K, no the SOTA is above 90. While we understand the authors are using a less strong foundation models, the big gap between the sota still draw concerns on the effective of the methods. Does it work on better foundation models? \n\n- Third, some recent papers sharing about prompt design is also interesting. What are the proposed methods compared with these methods? meta prompt optimization section 4.2 of https://arxiv.org/pdf/2309.03409.pdf. prompt design to optimize demonstrations: https://arxiv.org/abs/2305.14106"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6421/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6421/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6421/Reviewer_x9xJ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6421/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699076557112,
            "cdate": 1699076557112,
            "tmdate": 1699636715625,
            "mdate": 1699636715625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KIxs31kyGi",
                "forum": "eojWsJQ2fe",
                "replyto": "YC8DbLEet8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6421/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback on our paper! We hope the responses below can help address your concerns. \n\n> First, although the approach includes a few interesting tricks, the effectiveness of them are unclear, as indicated by ablation study.\n\n* Sorry about the confusion! To clarify, our ablation study shows that __three components we propose (two-step task instruction, context specification and step-by-step reasoning template) are indeed effective__ (Table 1 and Figure 3)(They are now Table 3 and Figure 4 in the updated paper). We believe these are important contributions in our paper. PE2 powered by these components also outperform prior methods such as Iterative APE and APO on a wide range of tasks (Figure 1).\n* We agree that results on optimizer-inspired components such as optimization history and step size are currently mixed. As pointed out by reviewer uHAL, we are re-organizing the paper to highlight the effective components and defer the remaining mixed results in the appendix. Please expect an updated version of the paper before the discussion period ends.\n\n> Second, most of the analysis and ablation studies (Table 1,2,3) are on simple math datasets MultiArith, GSM8K. Are the proposed approach work on harder math datasets?\n\n* We choose MultiArith and GSM8K as the main testbed by following past work on (manual/automatic) prompt engineering (https://arxiv.org/abs/2205.11916; https://arxiv.org/abs/2211.01910). Concurrent works (https://arxiv.org/abs/2309.03409; https://arxiv.org/abs/2309.16797) also use these two datasets. We believe it is reasonable to focus our analysis on these two datasets.\n* We agree with you that it will be very interesting to apply these methods to harder datasets! However due to resource constraints we are unable to further explore this.\n\n> Also, for GSM8K, now the SOTA is above 90. While we understand the authors are using a less strong foundation models, the big gap between the sota still draw concerns on the effective of the methods.\n\n* Our experiments on GSM8K is about finding the optimal prompt in the __zero-shot chain-of-thought__ setting, as done in https://arxiv.org/abs/2205.11916.\n* We learned that the current state-of-the-art performance on GSM8K is based on __few-shot chain-of-thought prompting__ or __additional human-designed verification workflow__. We believe these advances are orthogonal to us and the results are not directly comparable.\n* While we agree pushing forward state-of-the-art performance is important, we would like to highlight that __our focus is enabling LLMs to improve LLM performance automatically__. We really hope one day LLMs can conduct research and push forward state-of-the-art by themselves one day (https://arxiv.org/abs/2310.03302).\n\n> The approach still haven't tested on the more representative tasks categories (i.e. QA, test summarization, etc) to be persuasive.\n\n* We agree with you that generality is important in evaluating automatic prompt engineering methods! \n* We hope our experiments on an industrial prompt (Figure 1) helps address your concern regarding the generality of our approach. The underlying task is a 3-level hierarchical classification task on domain and intent classification.\n* During the rebuttal period we applied PE2 to two tasks from BIG-bench Hard (https://arxiv.org/abs/2210.09261): date understanding and movie recommendation. We believe these two tasks are close to everyday scenarios. Results suggest that PE2 is effective on these tasks. We will include these results in the upcoming paper update.\n\n|      | Date Understanding | Movie Recommendation|\n| :--- | :----: | :----: |\n| Initialization      |  39.1   | 57.0 |\n| Iterative APE |  46.7 |  67.3 |\n| APO | 45.0 | 75.0 |\n| PE2 | 54.0 | 79.0 |\n\n* We decided _not_ to test on common language tasks such as QA and summarization, as modern instruction-tuned models are already trained to do QA and summarization. Prompt engineering is most effective for customized and ad-hoc tasks unseen during model training."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6421/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645973969,
                "cdate": 1700645973969,
                "tmdate": 1700701962056,
                "mdate": 1700701962056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]