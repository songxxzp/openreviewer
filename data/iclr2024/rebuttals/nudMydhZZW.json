[
    {
        "title": "A primal-dual perspective for distributed TD-learning"
    },
    {
        "review": {
            "id": "YqpirXKf7U",
            "forum": "nudMydhZZW",
            "replyto": "nudMydhZZW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1895/Reviewer_7B2c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1895/Reviewer_7B2c"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the distributed TD learning algorithms for multi-aget MDPs. Using ideas from distributed optimization and control systems, the paper presents a new distributed TD learning algorithm that does not requre of doubly stochastic communication matrix (which is often needed in the existing distributed TD learning algorithms). Finite-time error bounds are developed for the proposed algorithm under both iid and Markovian data models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ A distributed TD learning algorithm with requiring a doubly stochastic matrix;\n+ Convergence rate results for the algorithm under both IID and Markovian data models;\n+ Numerical verifications"
                },
                "weaknesses": {
                    "value": "- There exist some grammar issues, such as \"there exists \\bar{h}_1 amd \\bar{h}_2...\" \"the proposed distributed TD-learning do not require\". Please check and polish the presentation."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741085731,
            "cdate": 1698741085731,
            "tmdate": 1699636119888,
            "mdate": 1699636119888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oVi8hWbyy6",
                "forum": "nudMydhZZW",
                "replyto": "YqpirXKf7U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7B2c"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's valuable feedback. We have provided detailed responses to the reviewer's questions and comments.\n\nThank you for the constructive comments. We will polish the presentation and check the manuscript carefully. Finally, as for clarification, we do not require doubly stochastic matrix and the additional details can be found within the general comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660880885,
                "cdate": 1700660880885,
                "tmdate": 1700660880885,
                "mdate": 1700660880885,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "niLj2aSn6n",
            "forum": "nudMydhZZW",
            "replyto": "nudMydhZZW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1895/Reviewer_u6NQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1895/Reviewer_u6NQ"
            ],
            "content": {
                "summary": {
                    "value": "\u2022\tThis paper analyzes the finite time convergence results for multi-agent distributed TD learning algorithms under a partially connected networking setup. In the paper, they assumed each network agent has local policies and local reward functions, and the goal is to estimate the sum of rewards over all agents with linear local approximation parameters that can be shared with connected neighbors only. In the analysis, they first captured the stochastic algorithm by its continuous-time ODE counterpart, then consider the linear system as the primal-dual gradient dynamics and prove the convergence results through the Lyapunov method applied to the gradient dynamics. The author considered both constant and diminishing step-sizes and both iid and Markovian observation models in their results. Different from previous works, the modified the proof to make the results hold when the graph Laplacian of the associated network graph is not doubly stochastic matrix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022\tThe strengths come from three parts. \n1) The first one is the proof is very sound. Based on that, in this paper, the convergence results are better than existing papers.\n2) The second part is that the literature reviews seem to be very detailed, carefully performed, and up to date. \n3) The author used a lot of citations throughout the whole paper."
                },
                "weaknesses": {
                    "value": "\u2022\tHowever, there are several weaknesses as far as from my perspective. \n\u2022\tThe first one is I think the paper is not organized very well: the author mentioned several literature many times throughout the whole paper, which feels very tedious; when reading section 3, I was confused since I don\u2019t know the reason of introducing and proving those lemmas until I read section 4, also the notations in section 3 do not closely correspond to the notations used in the rest of papers.\n\u2022\tThe second one is I think the simulation results are too little. In the paper they proved the convergence results for the constant and diminishing step size, but in the simulation section, both figures are for constant step size. I expect seeing diminishing step size case in the main body of the paper.\n\u2022\tThe third one is that the analysis doesn't feel very original. The main difference from the cited existing papers from my understanding are a linear mapping and a multiplication of LL^+. Also, the results rely on the relationship between the smallest and largest eigenvalue of the graph Laplacian, and I don\u2019t know how many network graphs can meet those requirements.\n\u2022\tThere are several other definitions that are not clear, see below."
                },
                "questions": {
                    "value": "\u2022\tIn the abstract you mentioned your algorithm and method do not require the network structure characterized by a doubly stochastic matrix. But through the whole paper, I didn\u2019t see an introduction to the doubly stochastic matrix and how it is related to communication networks. This is an important contribution of your paper, but I still don\u2019t know what kind of networks correspond to a doubly stochastic matrix and what kind of networks do not. So I don\u2019t know how significant the contribution is.\n\u2022\tIn the proof of theorem 4.2, when using constant step-size, the convergence results rely on the \\lambda_max and \\lambda_min of the network graph, and the choice of step-size also is based on the \\lambda_max. I don\u2019t know how many network graphs can meet the requirements on lambda max and min. Also, you provided an existing bound on lambda_max, so how hard is it to find the lambda_max and how hard is it to find a working eta since you require the eta\u22481/ \\lambda_max?\n\u2022\tThe citation when you mentioned total variation distance, ergodic and geometric decaying rate is confusing, you may want to cite the original paper that introduced them instead of a recent paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1895/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1895/Reviewer_u6NQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790148864,
            "cdate": 1698790148864,
            "tmdate": 1699636119818,
            "mdate": 1699636119818,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YVO6IqUaVh",
                "forum": "nudMydhZZW",
                "replyto": "niLj2aSn6n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u6NQ ( Part 1 )"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's valuable feedback. We have provided detailed responses to the reviewer's questions and comments.\n\n**1. The first one is I think the paper is not organized very well: the author mentioned several literature many times throughout the whole paper, which feels very tedious; when reading section 3, I was confused since I don\u2019t know the reason of introducing and proving those lemmas until I read section 4, also the notations in section 3 do not closely correspond to the notations used in the rest of papers.**\n\n\nFollowing the reviewer's comments, we have removed repeated literature citations throughout the manuscript. Moreover, new discussions have been added in Section 3 in order to inform that the results derived in Section 3 will serve as foundation tools for the results of Section 4. Furthermore, the notations in Section~3 have been modified to align more closely to the rest of the paper.\n\n**2. The second one is I think the simulation results are too little. In the paper they proved the convergence results for the constant and diminishing step size, but in the simulation section, both figures are for constant step size. I expect seeing diminishing step size case in the main body of the paper**\n\nIn line with the reviewer's comments, we have newly added simulation results with the diminishing step-sizes in Figures (1b) and (2c) of the revised manuscript. Further details on the additional experiments can be found in the ``General Comments'' at the beginning of this document.\n\n\n**3. The third one is that the analysis doesn't feel very original. The main difference from the cited existing papers from my understanding are a linear mapping and a multiplication of $LL^+$. Also, the results rely on the relationship between the smallest and largest eigenvalue of the graph Laplacian, and I don\u2019t know how many network graphs can meet those requirements.**\n\n\nFor the first comment, the main difference with other distributed TD-learning algorithms is that our algorithm is based on a primal-dual method, and it works in more relaxed and general scenarios, i.e., it does not require that the underlying communication network graph is characterized by a doubly stochastic matrix. We would like to note that the analysis of primal-dual method with null space constraints and Markovian observation model is not a trivial task, and it requires much more intricate analysis procedures compared to the existing analysis. Moreover, we believe that the analysis in this paper provides additional insights on utilizing the properties of Moore-pseudo inverse. \n\nFor the second comment, the condition on the network graph structure is minimal in the context of the related literature (all the papers mentioned in Table 1) that it needs to be connected, undirected and without any loops. Moreover, the minimal condition on the proposed step-size to guarantee convergence only depends on the number of agents, $N$, rather than maximum and minimum eigenvalues of the graph Laplacian as a conservative choice. Therefore, many practical problems can meet these conditions. Further detailed discussion is given in the response to question 5. \n\n**4. In the abstract you mentioned your algorithm and method do not require the network structure characterized by a doubly stochastic matrix. But through the whole paper, I didn\u2019t see an introduction to the doubly stochastic matrix and how it is related to communication networks. This is an important contribution of your paper, but I still don\u2019t know what kind of networks correspond to a doubly stochastic matrix and what kind of networks do not. So I don\u2019t know how significant the contribution is.**\n\n A doubly stochastic matrix corresponding to the communication network can be constructed by assigning positive weights to each edge in the network. Additionally, every column sum and row sum of the doubly stochastic matrix needs to be one. Further details about doubly stochastic matrix can be found in page 8 and Appendix A.2 in the revised manuscript. The limitations of the existing distributed TD-learning algorithms requiring the doubly stochastic matrix mentioned in the General Comment, can be summarized as follows: 1) Extension to directed graph and time-varying graph scenario is challenging ; 2) The performance of the algorithm is sensitive to the choice of doubly stochastic matrix. The related discussions have been newly added in the revised version on page 8. For more detailed discussions on the doubly stochastic matrix, please refer to our General Comment at the beginning of this document."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660688672,
                "cdate": 1700660688672,
                "tmdate": 1700660688672,
                "mdate": 1700660688672,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p5mF6NiicU",
            "forum": "nudMydhZZW",
            "replyto": "nudMydhZZW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1895/Reviewer_ZkTm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1895/Reviewer_ZkTm"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a primal-dual perspective on the distributed TD-learning approach. The paper considers a distributed TD-learning setup where each agent shares its information to the neighbors. The parameter-update step is formulated as the dynamical system and then the paper uses Lyapunov theory to conclude about the stability (or, convergence to the equilibrium)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The distributed TD learning is an important question and this paper has provided new insights.\n\nThe results seem to be correct."
                },
                "weaknesses": {
                    "value": "1. The paper only considers the average reward scenario. However, there can be another reward scenario (cooperative or competitive), can the result be extended to those setups?\n\n2. There is already quite a bit of work on the multi-agent RL framework for the average-reward case. Please see [A1]. The authors should discuss both in terms of methodology and the results whether they are related or different. The above paper provides the sample complexity bound, and even consider general function approximation case.\n\n[A1]. Hairi, F. N. U., Jia Liu, and Songtao Lu. \"Finite-time convergence and sample complexity of multi-agent actor-critic reinforcement learning with average reward.\" In International Conference on Learning Representations. 2021.\n\n3. In terms of practicality of the algorithm, there is an inherent assumption that each agent has the same feature space $\\phi$, however, this might not be true in practice."
                },
                "questions": {
                    "value": "1. Can the authors emphasize more on the technical challenges?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871198136,
            "cdate": 1698871198136,
            "tmdate": 1699636119731,
            "mdate": 1699636119731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fw8tLAWRi9",
                "forum": "nudMydhZZW",
                "replyto": "p5mF6NiicU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZkTm"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's valuable feedback. We have provided detailed responses to the reviewer's questions and comments.\n\n**1. The paper only considers the average reward scenario. However, there can be another reward scenario (cooperative or competitive), can the result be extended to those setups?**\n\nWe believe that our analysis can be extended to other setups as long as the reward scenario can be achieved at the consensus of parameters of each agents. For instance, one can consider different scenarios (cooperative or competitive), where communications among the agents are allowed. In these scenarios, we can consider new distributed algorithms which apply the proposed strategies, which can be potential future topics.\n\n\n**2. There is already quite a bit of work on the multi-agent RL framework for the average-reward case. Please see [A1]. The authors should discuss both in terms of methodology and the results whether they are related or different. The above paper provides the sample complexity bound, and even consider general function approximation case.**\n\nThank you for pointing out the work, and we have added discussion about [A1] in the revised manuscript. To our best knowledge, the critic part (which corresponds to the TD-learning algorithm) in [A1], uses linear function approximation rather than general function approximation. The algorithm in [A1] uses mini-batch update, where the size of mini-batch depends on number of agent $N$. We believe that our algorithm can be also improved using such mini-batch style updates. Moreover, we can also derive the sample complexity $\\mathcal{O}(1/\\epsilon)$ or $\\mathcal{O}(\\log (1/\\epsilon)/\\epsilon)$ from the mean squared error bound in Theorem 4.2 and Theorem 4.3, respectively. Note that the sample complexity in [A1] is also derived from the mean squared error bound.\n\n**3. In terms of practicality of the algorithm, there is an inherent assumption that each agent has the same feature space $\\Phi$ however, this might not be true in practice.**\n\nIndeed, we agree with the reviewer. However, considering the scenarios such as Go and Chess, every agent shares the same feature spaces. Moreover, we believe that the extension to different feature spaces should be possible without difficult technical challenges. Furthermore, most of the algorithms mentioned in Table 1 in the manuscript, except [R1], assume that the agents share the same feature space.\n\n**4. Can the authors emphasize more on the technical challenges?**\n\nThe main technical challenge is to derive convergence rates of a stochastic algorithm to a null space rather than a single unique point. This challenge can be addressed by following the analysis of primal-dual ODE with null space constraint introduced in Section 3. Moreover, the analysis of the primal-dual method with null space constraint and Markovian observation model requires much more intricate analysis steps compared to existing analysis.\n\n**References**\n\n[A1]. Hairi, F. N. U., Jia Liu, and Songtao Lu. \"Finite-time convergence and sample complexity of multi-agent actor-critic reinforcement learning with average reward.\" In International Conference on Learning Representations. 2021.\n\n\n[R1] Zeng, Sihan, Thinh T. Doan, and Justin Romberg. \"Finite-Time Convergence Rates of Decentralized Stochastic Approximation With Applications in Multi-Agent and Multi-Task Learning.\" IEEE Transactions on Automatic Control (2022)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660342608,
                "cdate": 1700660342608,
                "tmdate": 1700660342608,
                "mdate": 1700660342608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b0BKqkPedO",
            "forum": "nudMydhZZW",
            "replyto": "nudMydhZZW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1895/Reviewer_3CVE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1895/Reviewer_3CVE"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the TD learning for a networked multi-agent Markov decision process. The authors use exponential stability of primal-dual ODE dynamics to study the convergence of TD learning. The authors characterize the solution error rates in both iid and Markovian sampling cases. A numerical example is used to show the performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well organized and key results are explained well. \n\n- The authors provide a nice review of recent works on the exponential stability of primal-dual ODE dynamics when the constraint matrix is rank-deficient. \n\n- The authors study the exponential stability of a primal-dual ODE dynamics, which has improved the dependence on problem parameter. \n\n- The authors propose a new distributed TD learning algorithms, and characterize the solution error rates in both iid and Markovian sampling cases, which has weaker assumptions compared to other distributed TD learning algorithms."
                },
                "weaknesses": {
                    "value": "- The exponential stability of primal-dual ODE dynamics is known in the literature when the constraint matrix is rank-deficient. The improvement is only some constant for a special case of objective and constraint functions, which might be not very important to the TD analysis. \n\n- The proposed distributed TD learning is based on a known distributed primal-dual ODE dynamics. The error rate analyses follow the Lyapunov-based analysis from the previous work. The technique novelty is questionable. \n\n- The conducted primal-dual ODE based analysis can only guarantee mean-path performance, which might be not very useful in practice due to large variance.   \n\n- The provided example is artificial, and there is no comparison with existing distributed TD algorithms."
                },
                "questions": {
                    "value": "- Is a missing $\\mathbf{P}^\\pi$ in projected Bellman equation?\n\n- Can the authors provide numerical experiments to justify the convergence rates in Theorem 3.2? Why do you have an improvement? \n\n- Can the authors explain more how the new TD learning algorithm is built on the result of Wang and Elia (2011)? When does strongly convexity hold?  \n\n- The dependence of solution error rates on problem parameters is not clearly explained. What are parameters $w$, $h_1$, $h_2$ in Theorem 4.2 and Theorem 4.3?\n\n- Can the authors conduct comparison experiments with other existing algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1895/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1895/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1895/Reviewer_3CVE"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1895/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699051537736,
            "cdate": 1699051537736,
            "tmdate": 1699636119669,
            "mdate": 1699636119669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wifCSwKmZY",
                "forum": "nudMydhZZW",
                "replyto": "b0BKqkPedO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1895/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1895/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3CVE ( Part 1)"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's valuable feedback. We have provided detailed responses to the reviewer's questions and comments.\n\n**1. The exponential stability of primal-dual ODE dynamics is known in the literature when the constraint matrix is rank-deficient. The improvement is only some constant for a special case of objective and constraint functions, which might be not very important to the TD analysis.**\n\nFor the first comment, to the authors' knowledge, the exponential stability of primal-dual ODE dynamics when the constraint matrix is rank-deficient has not been fully studied so far. Moreover, as pointed out by the reivewer, while the improvement may be restricted to special scenarios, it offers different insight on utilizing the properties of Moore-pseudo inverse. Furthermore, it provides simplified tool for the analysis of the suggested distributed TD-learning.\n\n\n**2. The proposed distributed TD learning is based on a known distributed primal-dual ODE dynamics. The error rate analyses follow the Lyapunov-based analysis from the previous work. The technique novelty is questionable.**\n\nThe distributed primal-dual ODE dynamics studied in the current work is different from those from the existing works in the literature in the sense that the proposed ODE dynamics involves the null space constraints. Therefore, extending the distributed ODE dynamics to a stochastic setting via stochastic approximation methods is non-trivial because it requires to show convergence to a null space rather than a unique fixed point. Moreover, the analysis of primal-dual method with null space constraint and Markovain observation model requires far more intricate analysis procedures compared to existing approaches.\n\n\n**3. The conducted primal-dual ODE based analysis can only guarantee mean-path performance, which might be not very useful in practice due to large variance.**\n\nWe would like to note that the primal-dual ODE based analysis conducted in this paper not only guarantees the mean-path performance but also is used for the convergence properties of its stochastic versions (distributed TD-learning), which are given in the second part of the paper. The mean-path part can be seen as a contribution of our paper but it can be seen as a preliminary step for its stochastic version (distributed TD-learning).\n\nMoreover, the majority of the theoretical works on analysis of TD-learning [R1,R2] including most of the distributed TD-learning which are listed in the Table 1 in the manuscript, also deal with the mean-path setting first and investigate its stochastic versions. \n\n**4. The provided example is artificial, and there is no comparison with existing distributed TD algorithms.**\n\nAccording to the reviewer's comment, we have newly added experiments on comparisons with the existing distributed TD algorithms in Appendix A.13 in the revised version. Further details about the experiments are given in the General Comment.\n\n**5. Is a missing $P^{\\pi} $in projected Bellman equation?**\n\nThank you for pointing out the error, which has been corrected in the revised version.\n\n**6. Can the authors provide numerical experiments to justify the convergence rates in Theorem 3.2? Why do you have an improvement?**\n\nThe convergence rate can be verified through the full plot in Figures~(5a) and~(5b) in the revised manuscript, which demonstrates the exponential convergence rates for constant step-sizes. The linear convergence rate with diminishing step-sizes can be verified in the Figures (1b) and (2c). \n\nWe would like to note that we do not claim improvement on the convergence rate, $\\mathcal{O}(exp(-k))$ or $\\mathcal\nO(1/k)$ with the constant and diminishing step-sizes, respectively. Nevertheless, the convergence rate of the suggested algorithm is comparable with the convergence rates of the existing works with the weaker assumption that it does not require the full rank doubly stochastic matrix structure corresponding to the underlying network graph.\n\n**7. Can the authors explain more how the new TD learning algorithm is built on the result of Wang and Elia (2011)? When does strongly convexity hold?**\n\nThe original work of Wang-Elia studied distributed optimization algorithms in continuous-time domain with a general objective function $f(x)$ and its gradient $\\nabla f(x)$. We replace $\\nabla f(x)$ with stochastic TD-error $\\phi_k (r_k+\\gamma \\phi_k^{\\prime\\top}\\theta_k-\\phi_k^{\\top}\\theta_k) $, which is, however, not a (stochastic) gradient of any objective function.\n\n\nFor the second question, we do not require the strong convexity. We only require the matrix $A$ to be negative definite, i.e., $A+A^{\\top}\\prec 0$ where $A$ can be an asymmetric matrix. TD-learning is a stochastic approximation scheme finding a fixed point of the equation $Ax=b$ rather than using a gradient of a certain objective function."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659990212,
                "cdate": 1700659990212,
                "tmdate": 1700659990212,
                "mdate": 1700659990212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WuHozWX7I4",
                "forum": "nudMydhZZW",
                "replyto": "b0BKqkPedO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1895/Reviewer_3CVE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1895/Reviewer_3CVE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal. I don't have any further questions. One suggestion is improving experiments that can show the merits in more realistic examples."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1895/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695915388,
                "cdate": 1700695915388,
                "tmdate": 1700695976252,
                "mdate": 1700695976252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]