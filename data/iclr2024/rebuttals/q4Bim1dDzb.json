[
    {
        "title": "Fast Inverse Rendering by Unified Voxelization of Scene Representation"
    },
    {
        "review": {
            "id": "XJwfq3SEhV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2483/Reviewer_1NZF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2483/Reviewer_1NZF"
            ],
            "forum": "q4Bim1dDzb",
            "replyto": "q4Bim1dDzb",
            "content": {
                "summary": {
                    "value": "This paper propose a unified voxelization framework for inverse rendering (UniVoxel). UniVoxel can achieve fast inverse rendering, In addition, in order to better integrate with explicit frameworks, Spherical Gaussians to learn the incident light field was proposed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Compared with other inverse rendering methods, UniVoxel significantly improves optimization efficiency, reducing the training time for each scene from a few hours to **18 minutes**."
                },
                "weaknesses": {
                    "value": "1. The explicit scene representation of UniVoxel is very similar to Voxurf, except that it additionally predicts various properties of object materials in space. It is difficult to evaluate if adapting the semantic field during model training is novel in this paper.   \n2. UniVoxel cannot recover Envmap compared to other related works, which may limit its application scenarios.   \n3. From the experimental results, as shown in Figure 3, there is no obvious advantage in predicting the effect of materials, and the results of relighting are not very prominent. As shown in Figure 5, it seems that albedo did not decompose successfully and retained the light and dark shadows of the scene itself."
                },
                "questions": {
                    "value": "1. Can you provide a video comparing the training speed with other methods? As demonstrated by the author of Plenoxels (Plenoxels vs. NeRF)  ref: https://alexyu.net/plenoxels/   \n2.  A question about the explicit voxelization of scene representation is that can the explicit voxelization of scene representation model reflective surfaces such as **metal or mirror** ? Will the proposed method work better on those objects than NeRF ?    \n3. The paper can include more related works, e.g.,\n   - Yang, Wenqi, et al. \"Ps-nerf: Neural inverse rendering for multi-view photometric stereo.\" ECCV, 2022.   \n   - Wang, Zian, et al. \"Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes.\" CVPR, 2023.   \n   - Mai, Alexander, et al. \"Neural Microfacet Fields for Inverse Rendering.\" ICCV. 2023.  \n   - Zhang, Youjia, et al. \"NeMF: Inverse Volume Rendering with Neural Microflake Field.\" ICCV. 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697175564457,
            "cdate": 1697175564457,
            "tmdate": 1699636184913,
            "mdate": 1699636184913,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "udZO4gkadN",
                "forum": "q4Bim1dDzb",
                "replyto": "XJwfq3SEhV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1NZF (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback and thorough review of the our paper. We carefully respond to each of the concerns and questions below.\n\n**[Q1]:** The explicit scene representation of UniVoxel is very similar to Voxurf [1], except that it additionally predicts various properties of object materials in space.  \n**[A1.1]:** Thanks for your valuable comment. We agree that some previous works like Voxurf have explored explicit scene representation, but this is not the main contribution of our method. We emphasize that our contributions are twofold. First, in contrast to previous methods that only use voxel grid to model geometry or appearance, we design a unified framework of scene representation, which allows for efficient learning of geometry, materials and illumination in a unified manner.   \n**[A1.2]:** Second and more importantly, as also mentioned by reviewer Ft2v, we propose to leverage Spherical Gaussians (SG) to model the local incident light fields, which is capable of modeling the joint effects of direct lighting, indirect lighting and light visibility without the need of expensive multi-bounce ray tracing. The SG parameters of surface points can be predicted directly through voxel grid and lightweight MLP networks, which enables seamless integration of illumination modeling into the unified voxelization framework, thus significantly improving training efficiency.  \n**[A1.3]:** We conduct experiments on Shiny Blender dataset [2] and the results are shown in **Table 4** and **Figure 9** of the appendix. We also highlight the comparison results with Voxurf in the following table, demonstrating that our method achieves better geometry quality than Voxurf.  \n| Methods  | Normal MAE \u2193 |\n------------ | :----:\n| Voxurf  | 18.110 |\n| Ours | 9.292 |\n\n**[Q2]:** UniVoxel cannot recover Envmap compared to other related works, which may limit its application scenarios.  \n**[A2.1]:** Thank you for your valuable comment. In theory, it is possible to filter the incident light map of all surface points to recover the environment map, but more sophisticated design is required to eliminate the influence of indirect lighting.  \n**[A2.2]:** Although our UniVoxel, by utilizing SG to model the local incident light field, cannot directly acquire the environment map, it achieves comparable accuracy to the state-of-the-art methods while having higher training efficiency (about 40 $\\times$ faster than MII [3] as shown in **Table 1** of the paper), which is the main advantage of our approach.  \n**[A2.3]:** Environment map and incident light field are two different technical approaches for representing illumination. The former can recover the environment lighting of the scene, but the obtained lighting is not sufficiently good for direct application. On the other hand, the latter has higher training efficiency (18 minutes vs 58 minutes as shown in **Table 2** of the paper), although it cannot directly obtain the environment light. In the future, we will explore how to combine the advantages of these two methods.\n\n\n**[Q3]:** There is no obvious advantage in the experimental results.  \n**[A3.1]:** Thank you for your meticulous comment. We acknowledge that our UniVoxel does not have significant advantages in terms of performance compared to other methods. However, as shown in **Figure 3** of the paper, our UniVoxel is able to recover more high-frequency details on the albedo maps. Additionally, our UniVoxel achieves comparable reconstruction quality to the state-of-the-art methods, with significantly higher training efficiency, which is the main advantage of our method.  \n**[A3.2]:** Actually, we can control the shading component on the albedo maps by balancing the weight of the SG smoothness loss $L_{sg}$. In **Figure 11** of the appendix, we have demonstrated that the light and dark shadows on the albedo maps can be eliminated by reducing the weight of $L_{sg}$. Due to the complexity of lighting condition in outdoor scenes, the constraints on illumination should be relaxed to achieve more realistic materials.\n\n**[Q4]:** Can the explicit voxelization of scene representation model reflective surfaces such as metal or mirror?  \n**[A4]:** Yes, we have conducted experiments on the challenging Shiny Blender dataset [2]. The quantitative results in **Table 4** of the appendix show that our UniVoxel achieves better geometry quality compared to other NeRF-like methods. The visualization in **Figure 10** of the appendix also demonstrates that our UniVoxel is capable of reconstructing more realistic materials such as metal."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700208618449,
                "cdate": 1700208618449,
                "tmdate": 1700736686392,
                "mdate": 1700736686392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4i9R2gxCgy",
                "forum": "q4Bim1dDzb",
                "replyto": "XJwfq3SEhV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1NZF (2/2)"
                    },
                    "comment": {
                        "value": "**[Q5]:** Can you provide a video comparing the training speed with other methods?  \n**[A5]:** Yes, we have included the video in the supplementary materials. It can be observed that when the training of our UniVoxel is almost completed, MII [3] is still in the process of recovering geometry. While the normal and albedo estimated by TensoIR [4] have rough outlines, their detailed parts still need several hours to optimize.\n\n**[Q6]:** More related works.  \n**[A6]:** Thanks for pointing this out. We have added the reference of these works in the related work section.\n\n**Reference:**  \n[1]. Wu et al., Voxurf: Voxel-based efficient and accurate neural surface reconstruction.  \n[2]. Verbin et al., Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields.  \n[3]. Zhang et al., Modeling Indirect Illumination for Inverse Rendering.  \n[4]. Jin et al., TensoIR: Tensorial Inverse Rendering."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231539047,
                "cdate": 1700231539047,
                "tmdate": 1700231539047,
                "mdate": 1700231539047,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FuRMmdXU9p",
            "forum": "q4Bim1dDzb",
            "replyto": "q4Bim1dDzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2483/Reviewer_iqeo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2483/Reviewer_iqeo"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a volumetric-based inverse rendering technique notable for its efficiency. It showcases impressive experimental outcomes; however, the paper's innovation is somewhat restrained. Core aspects of the methodology appear to be previously explored in established works like PhySG, Tensorir, and NeuS. The experimental scope could benefit from an extension to include tests on the challenging Shiny Nerf-synthetic dataset, which is known for its shiny object rendering complexity. Furthermore, the paper could enhance its technical credibility by scrutinizing and refining the accuracy of its claims."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe manuscript is well-composed, displaying a clear and articulate writing style. \n2.\tThe inverse rendering outcomes presented are visually appealing and demonstrate good quality. \n3.\tAdditionally, the related work section is comprehensive, encompassing a broad spectrum of the existing research in this field, which underscores the authors' thorough understanding of the domain."
                },
                "weaknesses": {
                    "value": "1. The paper's innovative contribution is nuanced, as it applies a latent volumetric representation to enhance efficiency in inverse rendering\u2014a concept that has been extensively studied. Although it offers incremental advancements, the overall novelty is tempered by similarities to existing methods, such as those utilizing MLPs. Additionally, the application of Spherical Gaussians (SGs) in the context of lighting is previously detailed in works like PhySG, suggesting the paper's approach is not entirely unprecedented.\n\n2. While the paper acknowledges PhySG, it does not sufficiently recognize the prior exploration of SGs for modeling incident light. A more explicit acknowledgment would strengthen the paper by correctly attributing the origins of this idea.\n\n3. The utilization of environment map-based lighting is a well-trodden area in research, evidenced by efforts like Nvidia\u2019s nvdiffrec-mc. The paper's critique of environment map-based lighting in favor of SGs may not be entirely justified, and the simplifications made in visibility reasoning warrant a closer examination. The unconventional modeling of the environment map using 128 Spherical Gaussians in this paper departs from traditional methods and calls into question the asserted superiority of SG over environment maps. Therefore, claims regarding the advantages of SG should be made with greater technical precision and careful comparison."
                },
                "questions": {
                    "value": "1. The paper would benefit from an explanation of the observed floating noise in TensoIR's results, providing clarity on whether this is an artifact of the algorithm, a limitation of the model, or an issue with the dataset or experimental setup used.\n\n2. The inclusion of experiments on the challenging shiny NeRF-synthetic dataset could significantly enhance the paper's empirical foundation. Successful results on this dataset would serve as a robust testament to the algorithm's capabilities.\n\n3. It is important for the paper to detail the distinctions between the environment map used in this study and that employed by Nvdiffrec, particularly since the paper touts the superiority of Spherical Gaussians (SG) over environment maps. A clear comparison would illustrate the specific contributions and advantages of the proposed method.\n\n4. For a more convincing demonstration of the algorithm\u2019s performance in handling complex lighting scenarios, the paper should present an experiment visualizing the incident light maps reconstructed using the shiny NeRF-synthetic dataset. This would showcase the practical utility of the algorithm in a real-world application, particularly for scenes with challenging lighting conditions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675528817,
            "cdate": 1698675528817,
            "tmdate": 1699636184839,
            "mdate": 1699636184839,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jBF9uODxqh",
                "forum": "q4Bim1dDzb",
                "replyto": "FuRMmdXU9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iqeo (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback and thorough review of the our paper. We carefully respond to each of the concerns and questions below.\n\n**[Q1]:** The novelty of the proposed method.  \n**[A1.1]:** Thank you for your meticulous comment. We agree that volumetric representation has been studied in recent works, but this is not the main contribution of our method. We emphasize that our contributions are twofold. First, in contrast to previous methods that only use voxel grid to model geometry or appearance, we design a unified framework of scene representation, which allows for efficient learning of geometry, materials and illumination in a unified manner. Second and more importantly, as also mentioned by Reviewer Ft2v, we propose to leverage Spherical Gaussians (SG) to model the local incident light fields.  \n**[A1.2]:** Although some works like PhySG [1] has explored the application of SG in the context of lighting, they directly represent the entire scene\u2019s environment map with SG, requiring expensive multi-bounce ray tracing. In contrast, we utilize SG to model the local incident light radiance, which is capable of modeling the joint effects of direct lighting, indirect lighting and light visibility. And the SG parameters of surface points can be predicted directly through voxel grid and lightweight MLP networks, which enables seamless integration of illumination modeling into the unified voxelization framework, thus significantly improving training efficiency.  \n**[A1.3]** We have reorganized the introduction section of the paper to clarify these major contributions.\n\n\n**[Q2]:** While the paper acknowledges PhySG, it does not sufficiently recognize the prior exploration of SGs for modeling incident light.  \n**[A2]:** Thanks for your suggestion. We have added more descriptions about PhySG in section 2.1 and section 3.4 of the main paper, and explained the differences between our approach and it. \n\n**[Q3]:** The unconventional modeling of the environment map using 128 Spherical Gaussians in this paper departs from traditional methods and calls into question the asserted superiority of SG over environment maps.  \n**[A3.1]:** Thanks for your valuable comment. Following your suggestion, we have adopted the conventional modeling of environment map, representing it as a learnable parameter of size 256x512x3. As shown in the following table, it obtains better reconstruction quality than using mixture of SG to represent environment map, and achieves comparable results to our method, but it also leads to much longer training time.  \n**[A3.2]:** Environment map and incident light field are two different techniques for representing illumination. Methods based on environment map can usually achieve better reconstruction quality, but they require more training time due to the need of multi-bounce ray tracing. On the other hand, methods based on incident light field usually can achieve higher training efficiency. In the future, we will explore how to combine the advantages of both methods and obtain high-quality reconstruction effects with shorter training time.  \n| Methods |  MVS PSNR \u2191 | Albedo PSNR \u2191 | Roughness MSE \u2193 | Relighting PSNR \u2191 |  Training Time \u2193 |\n ------------ | :----: | :----: | :----: | :----: | :----: |\n| Mixture of 128 SG  | 34.185 | 27.368 | 0.012 | 27.446 | 58 minutes |\n| 256x512x3 learnable map | 35.042 | 29.369 | 0.010 | **29.592** | 2 hours |\n| Ours  | **36.232** | **29.933** | **0.007** | 29.445 | **18 minutes** |\n\n**[Q4]:** Explanation of the observed floating noise in TensoIR's [2] results.  \n**[A4]:** Thank you for your insightful suggestion. TensoIR has achieved impressive results on the MII synthetic dataset [3], but its performance in real-world scenes is poor and may have floating noise. The main reason is that the environment in each view of the real-world scenes is not completely static, making it difficult to handle complex lighting with a single environment map. One solution is to use a separate environment map for each view, but this would significantly increase training time. Our approach only requires introducing the view embedding as an additional input to the illumination model, allowing us to effectively handle complex real-world lighting without affecting training efficiency."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207969050,
                "cdate": 1700207969050,
                "tmdate": 1700736558532,
                "mdate": 1700736558532,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "atbbxy4Xv9",
                "forum": "q4Bim1dDzb",
                "replyto": "FuRMmdXU9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iqeo (2/2)"
                    },
                    "comment": {
                        "value": "**[Q5]:** Experiments on the challenging shiny NeRF-synthetic dataset.  \n**[A5.1]:** Following your suggestion, we conducted experiments on the Shiny Blender dataset [4]. The quantitative and qualitative results of the estimated normal maps are shown in **Table 4** and **Figure 9** of the appendix. It can be observed that our approach achieves better geometry quality compared to our methods, especially in the specular surfaces.  \n**[A5.2]:** We also present the qualitative comparison of geometry, materials and illumination on the Shiny Blender dataset in **Figure 10** of the appendix. Although our method does not recover the details of illumination, it can be able to predict realistic albedo and roughness, while TensoIR fails to reconstruct materials in the specular surfaces and bakes the lighting into the albedo maps.\n\n**Reference:**  \n[1]. Zhang et al., PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting.  \n[2]. Jin et al., TensoIR: Tensorial Inverse Rendering.  \n[3]. Zhang et al., Modeling Indirect Illumination for Inverse Rendering.  \n[4]. Verbin et al., Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700208020057,
                "cdate": 1700208020057,
                "tmdate": 1700208020057,
                "mdate": 1700208020057,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2RBOtei4Pz",
            "forum": "q4Bim1dDzb",
            "replyto": "q4Bim1dDzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2483/Reviewer_qhoe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2483/Reviewer_qhoe"
            ],
            "content": {
                "summary": {
                    "value": "The submission proposes a 3D scene representation based on voxel grid features and MLP decoders to achieve more efficient inverse rendering. Geometrical and scene properties are encoded separately in two grid-based volumes. For scene properties (e.g., material, illumination), the volume stores implicit features in the grids, and separate MLPs are trained to decode these features into the target property values. In the case of the geometrical volume, SDF values are directly stored in the grid structure. The experimental evaluation demonstrates that this representation leads to efficient inverse rendering and delivers performance comparable to state-of-the-art methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+This submission proposes a unified grid-based representation that incorporates geometry and material properties. The grid-based representation has shown to be more friendly for optimization.\n\n+Optimization becomes more straightforward with shallower MLPs. The combination of grid-based volume representation and shallow MLPs leads to more efficient optimization. \n\n+Spherical Gaussian (SG) representation exhibits higher-order frequency characteristics for illumination compared to spherical harmonics (SH)."
                },
                "weaknesses": {
                    "value": "-My major concern is that the utilization of feature grids for more efficient training (the major claim) is not a novel insight. For instance, in InstantNGP, it has been demonstrated that the combination of dense hashable grids and shallower MLPs significantly enhances efficiency. Furthermore, the memory concerns highlighted in this submission can potentially be alleviated by adopting more efficient data structures for the grid, such as hashable grids as employed in InstantNGP.\n\n-The observed performance improvement is incremental and does not surpass the performance achieved by previous methods in some cases. For instance, a notable limitation of the proposed method is that it often results in albedo estimates that include shading components as shown in Fig.5 first row, in contrast to TenSor RT.\n\n-The assertion that SH cannot effectively model higher-frequency illumination may not be conclusive, as it is based on testing only up to order-3 SH. Increasing the order of SH could potentially address this limitation. To provide a more compelling evaluation, it is advisable to ensure an equal number of parameters when comparing SG and SH representations.\n\n-Details missing: It is not mentioned clearly how the global illumination and self-occlusion are handled during relighting. Please see questions below for more detailed questions."
                },
                "questions": {
                    "value": "1. For SG-based illumination representation, how many parameters are used for representing the Gaussians? Is it comparable to the SH counterpart? \n\n2. How are global illumination and self-occlusion handled during relighting? During scene reconstruction, those could be baked in the SG/SH-based per-point illumination. But during relighting, the per-point environment might should be different."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780897407,
            "cdate": 1698780897407,
            "tmdate": 1699636184761,
            "mdate": 1699636184761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2VSSHhDoJl",
                "forum": "q4Bim1dDzb",
                "replyto": "2RBOtei4Pz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qhoe"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback and thorough review of the our paper. We carefully respond to each of the concerns and questions below.\n\n**[Q1]:** The novelty of the utilization of feature grids for more efficient training.  \n**[A1.1]** Thank you for your valuable comment. We agree that some prior works have explored the use of feature grids to improve training efficiency, but this is not the main contribution of our method. We emphasize that our contributions are twofold. First, in contrast to previous methods that only use voxel grid to model geometry or appearance, we design a unified framework of scene representation, which allows for efficient learning of geometry, materials and illumination jointly.  \n**[A1.2]** Second and more importantly, as also mentioned by Reviewer Ft2v, we propose to leverage Spherical Gaussians (SG) to model the local incident light fields, which is capable of modeling the joint effects of direct lighting, indirect lighting and light visibility without the need of expensive multi-bounce ray tracing. The SG parameters of surface points can be predicted directly through voxel grid and lightweight MLP networks, which enables seamless integration of illumination modeling into the unified voxelization framework, thus significantly improving training efficiency.  \n**[A1.3]** We have reorganized the introduction section of the paper to clarify these major contributions.\n\n\n**[Q2]** The memory concerns.  \n**[A2]** Thanks for your construstive suggestion. The volumetric representation of our method is not in conflict with the hashable grids proposed in InstantNGP [1], and it is easy to apply it to our framework. We plan to implement it in the future.\n\n**[Q3]:** A notable limitation of the proposed method is that it often results in albedo estimates that include shading components.  \n**[A3]:** Thanks for your meticulous comment. Actually, we can control the shading component on the albedo maps by balancing the weight of the SG smoothness loss $L_{sg}$. In **Figure 11** of the appendix, we have demonstrated that the shading components can be eliminated by reducing the weight of $L_{sg}$. Due to the complexity of lighting condition in outdoor scenes, the constraints on illumination should be relaxed to achieve more realistic materials.\n\n**[Q4]:** Equal numbers of parameters when comparing SG and SH representations.  \n**[A4]:** Thanks for your valuable commet. Following your suggestion, we use a higher order SH representation of illumination (order-4 with 75 parameters). For a fair comparison, we also adopt the SG with 12 Gaussian lobes (72 parameters) for lighting modeling. The results on the MII synthetic dataset are presented in the following table. It can be observed that using higher order SH leads to some improvement in reconstruction performance, but using fewer parameters, SG performs better than SH, which demonstrates the effectiveness of SG in lighting modeling.  \n| Methods  |  MVS PSNR \u2191 | Albedo PSNR \u2191 | Roughness MSE \u2193 | Relighting PSNR \u2191 |\n :------------: | :----: | :----: | :----: | :----: \n| SH (order-3 with 48 parameters)  | 35.328 | 29.185 | 0.020 | 28.981 |\n| SH (order-4 with 75 parameters)  | 35.344 | 29.200 | 0.016 | 28.813 | \n| SG (12 lobes with 72 parameters)  | **36.177** | **29.746** | **0.008** | **29.148** |\n\n**[Q5]:** How are global illumination and self-occlusion handled during relighting?  \n**[A5]:** Thanks for your meticulous comment. We have added the relighting procedure in the Section B of the appendix. As the incident light field obtained from previous training is not applicable to the new illumination, we adopt a similar procedure as previous works like TensoIR [2] and NerFactor [3], where we compute light visibility using Eq.7 in the main paper and consider only direct lighting.\n\n**Reference:**  \n[1]. M\u00dcLLER et al., Instant Neural Graphics Primitives with a Multiresolution Hash Encoding.  \n[2]. Jin et al., TensoIR: Tensorial Inverse Rendering.  \n[3]. Zhang et al., NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206914530,
                "cdate": 1700206914530,
                "tmdate": 1700736458668,
                "mdate": 1700736458668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9xB9kON1ff",
            "forum": "q4Bim1dDzb",
            "replyto": "q4Bim1dDzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2483/Reviewer_Ft2v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2483/Reviewer_Ft2v"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a unified voxel representation to allow efficient reconstruction of geometry, material and illumination from multi-view images captured around the object. Compared with previous work that requires expensive ray tracing to compute lighting and visibility, the proposed method shows that a local spherical Gaussian illumination representation can achieve similar or even better quality of inverse rendering while significantly reducing the optimization time. Experiments on widely used real and synthetic dataset shows the overall improvements of the proposed method against state-of-the-arts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Clear novelties and improvements compared to previous work.\nIn my opinion, the major novelty of the proposed method is a local illumination model that bakes direct illumination, visibility and indirect illumination into a spherical Gaussian representation that can be efficiently predicted. This novel representation enable significant acceleration compared to previous environment map representation, as it can avoid expensive multi-bounce ray tracing. One may expect this new method will cause more baking issue as it is less constrained compared to a global environment map lighting representation. However, the experiments show that the BRDF reconstruction quality is comparable or even better than state-of-the-arts. \n\n2.  Comprehensive experiments\nAuthor did comprehensive experiments on novel view synthesis, material estimation and relighting on widely-used real and synthetic datasets. Both the quantitative and qualitative results show clear improvements and the optimization time is much less compared to previous works. Authors also did ablation studies between different lighting representations, which makes the results reported in the paper more convincing.\n\n3. Well-written paper, with all necessary implementation details included in the main paper and supplementary material. \nThe paper is well-written and easy to follow. With the details provided in the supplementary material, it should not be too difficult to reimplement the paper."
                },
                "weaknesses": {
                    "value": "1. More focused on novelty.\nSeveral recent methods use feature volume plus MLP to jointly reconstruct materials and geometry, such as TensorIR and NeuralPBIR. The true difference of the proposed method and previous works is the local illumination model that can accelerate the optimization. Therefore, I feel authors can emphasize this more. For example, the introduction gives me the impression that this method is faster because it uses a voxel-based scene representation, which has adapted by many previous work, but instead it probably makes more sense to emphasize the illumination model. \n\n2. Geometry reconstruction.\nI feel one experiment that is missing is the geometry quality, especially compared to TensoIR. While it is mentioned in section 4.2, it is difficult to tell the differences. I am curious how will different lighting representations impact the geometry quality, especially for the highly specular surfaces and concave regions?\n\n3. Reference.\nOne con-current work that can be discussed in the paper is the NeuralPBIR (Sun et al.), which accelerates the reconstruction process by precomputing visibility and GI from NeRF so that material reconstruction can be done through local optimization. The idea of avoiding expensive ray tracing is related, while I feel the local illumination model proposed here is more different from previous works.\n\n4. Missing details.\nOne details I did not find in the paper is after predicting the SG parameters, how will you render the appearance? Are you going to sample rays uniformly, with importance sampling or just compute the integral analytically? Please remind me if I miss this part in the paper.\n\n5. Extension of the method to handle volumetric object. \nOne potential extension of this work is that instead of computing per-ray SG parameters and the render the appearances, we can also consider use per-point SG parameters to compute the radiance at every point through rendering equation. In this way, we might be able to reconstruct fury objects, with some modifications of the BRDF model. Does it make sense to authors?"
                },
                "questions": {
                    "value": "Most of my questions in the weakness section. I have two more questions. \n\n1. About limitation in Sec. F. When you say the proposed method needs more GPU memory, how much more GPU memory does it require?\n2. In Figure 13, the color of the normal maps from different methods are very different. I wonder if that is caused by any visualization issue, like different way to turn normal into RGB image?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not have any ethical concern."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887528169,
            "cdate": 1698887528169,
            "tmdate": 1699636184690,
            "mdate": 1699636184690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U7Yyi4t7JP",
                "forum": "q4Bim1dDzb",
                "replyto": "9xB9kON1ff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ft2v"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback and thorough review of the our paper. We carefully respond to each of the concerns and questions below.\n\n**[Q1]:** Emphasis on the illumination model.  \n**[A1]:** Thanks for insightful suggestion. Following the suggestion, we have rephrased the introduction section and emphasize more on the novelty regarding the proposed illumination model. \n\n**[Q2]:** Comparison of geometry quality.  \n**[A2.1]:** Thanks for your suggestion. Due to the lack of ground truth normal in MII dataset, we conducted additional experiments on the Shiny Blender dataset [1], and the quantitative results are shown in **Table 4** of the appendix. We also highlight the comparison results with TensoIR in the following table, showing that our method achieves more accurate geometry quality.  \n**[A2.2]:** In **Figure 9** of the appendix, we demonstrate qualitative comparisons with other methods and different illumination models. It can be seen that our SG-based local illumination model produces higher geometry quality on specular surfaces compared to using environment maps. Additionally, using SH and NeILF as illumination models can achieve comparable geometry quality to ours. However, the quality of material recovered using our illumination model is superior to others, and the training efficiency is also higher, as demonstrated in **Table 2** of the paper.  \n| Methods  | Normal MAE \u2193 |\n:------------: | :----:\n| TensoIR  | 49.747 |\n| Ours | 9.292 |\n\n**[Q3]:** Reference of Neural-PBIR.  \n**[A3]:** Thanks for pointing us to the interesting work. We have added the reference of Neural-PBIR in the related work section.\n\n**[Q4]:** How will you render the appearance? Are you going to sample rays uniformly, with importance sampling or just compute the integral analytically?  \n**[A4]:** We discuss how to render the appearance in the implementation details (Section B) of the appendix: we utilize Fibonacci sphere sampling method over the half sphere to sample incident lights for each surface point.\n\n**[Q5]:** Using per-point SG parameters to compute the radiance at every point through rendering equation instead of computing per-ray SG parameters.  \n**[A5]:** Thank you for your constructive suggestion. One challenge of using per-point SG parameters to calculate radiance is that the rendering equation needs to be calculated for each sampling point on a ray during training. Compared to rendering with per-ray SG parameters, which only require one calculation of the rendering equation per ray, this approach requires several times more GPU memory and significantly prolongs training time. But I believe this is a very insightful idea, and we will explore ways to reduce the computational cost of this method to reconstruct fury objects.\n\n**[Q6]:** How much more GPU memory does it require?  \n**[A6]:** We present the batch size and required GPU memory for training each method on the MII synthetic dataset in the following table. It can be seen that our method does not significantly exceed the GPU memory of other methods, thanks to our efficient implementation. However, for larger-scale scenes, we would need to increase the resolution of voxelization and also require more GPU memory. Nevertheless, this can be addressed by the hashable grids proposed in InstantNGP [2]. And we plan to implement it in the future.  \n| Methods  | batch size | GPU memory |\n:------------: | :----: | :----:\n| TensoIR [3]  | 4096 | ~12 GB  |\n| MII [4]     | 1024 | ~14 GB  |\n| Ours     | 8192 | ~19 GB  |\n\n**[Q7]:** Why are the color of the normal maps from different methods very different?  \n**[A7]:** Thank you for pointing this out. The way Nvdiffrec and Nvdiffrec-mc convert normal maps into RGB images is very different from other methods. We are working on it and will address this issue in the final version.\n\n**Reference:**  \n[1]. Verbin et al., Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields.  \n[2]. M\u00dcLLER et al., Instant Neural Graphics Primitives with a Multiresolution Hash Encoding.  \n[3]. Jin et al., TensoIR: Tensorial Inverse Rendering.  \n[4]. Zhang et al., Modeling Indirect Illumination for Inverse Rendering."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206552619,
                "cdate": 1700206552619,
                "tmdate": 1700206552619,
                "mdate": 1700206552619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hYq97Ca9Es",
                "forum": "q4Bim1dDzb",
                "replyto": "9xB9kON1ff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2483/Reviewer_Ft2v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2483/Reviewer_Ft2v"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the answers! I think my questions to the paper have been resolved."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638063497,
                "cdate": 1700638063497,
                "tmdate": 1700638095348,
                "mdate": 1700638095348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "we1b9IxAwt",
                "forum": "q4Bim1dDzb",
                "replyto": "9xB9kON1ff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer Ft2v,\n\nThank you for taking the time and effort to review our response! If you have any further questions, please let us know and we will respond promptly!\n\nThank you once again for your time and attention.\n\nBest,\n\nPaper 2483 Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734498523,
                "cdate": 1700734498523,
                "tmdate": 1700734498523,
                "mdate": 1700734498523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]