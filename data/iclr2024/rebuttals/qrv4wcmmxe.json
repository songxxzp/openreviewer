[
    {
        "title": "Zero-shot Human-Object Interaction Detection via Conditional Multi-Modal Prompts"
    },
    {
        "review": {
            "id": "YBSqSdAgeO",
            "forum": "qrv4wcmmxe",
            "replyto": "qrv4wcmmxe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8729/Reviewer_n8e2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8729/Reviewer_n8e2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a prompt-based zero-shot HOI detector. It splits the detection task into two subtasks: extracting spatial-aware visual features and interaction classification. The vision and text prompts are jointly applied to the detector. Experimental results on the zero-shot settings show its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well written and organized. The vision and text prompts are also clearly explained.\n2. Experimental results on the zero-shot settings demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Overall, this work is very similar with the following ICCV2023 paper, including the overall framework, the conditional vision prompts and the learnable modules. What's the difference between the proposed method and the ICCV2023 paper.\nA1: Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory, ICCV2023.\n2. For the Lcls in (11), it is not clear how to connect the model with the GT labels. \n3. This work only presents the HOI results using zero-shot settings. What's the result using the typical experimental settings?\n4. Some important works from CVPR2023 are missing. Besides, the formats of some references are not consistent."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8729/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8729/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8729/Reviewer_n8e2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698052609699,
            "cdate": 1698052609699,
            "tmdate": 1699637094954,
            "mdate": 1699637094954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dkJavjn7r1",
                "forum": "qrv4wcmmxe",
                "replyto": "YBSqSdAgeO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer n8e2,\n\nFirst and foremost, we extend our deepest gratitude for your insightful feedback and hope our clarifications address your concerns. We're eager to highlight the significance and potential of our work.\n\n**1. The differences between the proposed method and the ICCV2023 paper.**\n\nThank you once again for your careful consideration of our work.\nWe propose an approach for zero-shot HOI detection that addresses the issue of **performance degradation between seen and unseen classes.** While the concurrent ICCV2023 paper focuses on improving efficiency when adapting CLIP to HOI detection, our method prioritizes generalizability. Our approach differs from it in several key aspects:\n\n+ **Conditional Vision Prompts for Region-Level Interactiveness Knowledge:**\nWhile both the ICCV paper and our approach utilize prior knowledge from DETR, our method specifically focuses on leveraging instance-level visual priors to distinguish between non-interactive human-object pairs and interactive ones. This differentiation is crucial, especially in zero-shot HOID settings where unseen HOI concepts are encountered.\n\n+ **Learnable Embeddings as Context Words:**\nTo augment the CLIP text encoder's representation capabilities for actions, we introduce learnable embeddings as context words, i.e., an additional input known as language prompts.\n\n+ **Regularization Loss for Generalizability:**\nSince the hand-engineered prompts outperform the learnable soft prompts for the zero-shot setting, then, in order to avoid overfitting to the seen classes and strengthen generalizability, we further propose that the learnable ones should be trained so that they can be correctly classified in language space where the class weights are given by the hand-engineered prompts. This is achieved through the **constrastive loss** in Equation (6) of the main text. \n\n+ **Exclusive Reliance on Language for Interaction Classification:** For interaction recognition, the ICCV paper proposes a key-value mechanism storing both visual and linguistic knowledge. In contrast, we argue that visual knowledge is challenging to generalize to unseen classes. In other words, the visual features of unseen classes are hard to acquire. As a result, we exclusively employ language for interaction classification.\n\n\n\n**2. Connecting the predictions with the GT labels.**\n\n\nTo associate the detected human\u2013object pairs with the ground truth, we calculate the intersection-over-union (IoU) between each detected pair and the ground-truth pair. The IoU is computed for human and object boxes separately, and the minimum of the two is taken. Detected pairs are considered to be positive when the IoU surpasses a predefined threshold.\n\n\n\n**3. Experimental results using the typical settings.**\n\n\nWe are truly grateful for this insightful consideration. To further validate the effectiveness of our method, we conduct experiments on the typical experimental settings on both the HICO-DET and V-COCO datasets. Empirically, we find that our method achieves competitive results on the default setting on both datasets. Notably, our method achieves sota on rare classes on HICO-DET, aligning with our methodology's design since rare classes are typically challenging to detect, similar to the scenario of detecting unseen interactions.\n\n\n|  Method  | Full  | Rare  | Non-Rare  |\n|   ----   | ----  | ----  |  ----   |\n| GEN-VLKT | 33.75 | 29.25 |  35.10  |\n| HOICLIP  | 34.54 | 30.71 |  35.70  |\n| Ours     | 34.26 | 32.22 |  34.86  |\n\n\n|  Method  | AP(Scenario 1) | AP(Scenario 2) |\n|   ----   | ----  | ----  | \n| GEN-VLKT | 62.4 | 64.4 |\n| HOICLIP  | 63.5 | 64.8 |\n| Ours     | 57.8 | 63.2 |\n\n\n\n**4. Missing related works.**\n\nThank you for bringing this to our attention. We will incorporate discussions about the literature on HOI in CVPR2023 [1-4] into the related work section, and revise the formats of references in the final version.\n\n[1] Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework\n\n[2] Relational Context Learning for Human-Object Interaction Detection\n\n[3] Category Query Learning for Human-Object Interaction Classification\n\n[4] ViPLO: Vision Transformer Based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499725323,
                "cdate": 1700499725323,
                "tmdate": 1700634412638,
                "mdate": 1700634412638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RFmU2f7Wmc",
                "forum": "qrv4wcmmxe",
                "replyto": "YBSqSdAgeO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion with Reviewer n8e2"
                    },
                    "comment": {
                        "value": "Dear Reviewer n8e2,\n\nWe sincerely appreciate the time you invested in reviewing our submission and your invaluable feedback. We have diligently addressed your comments and provided corresponding responses and results. We believe that these revisions have addressed the concerns you raised. We would be grateful for the opportunity to further discuss whether your concerns have been adequately addressed. If there are any aspects of our work that remain unclear, please do not hesitate to inform us. \n\nOnce again, thank you for your guidance and insights.\n\nWarm regards,"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649640609,
                "cdate": 1700649640609,
                "tmdate": 1700649640609,
                "mdate": 1700649640609,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "78R9VxP2YN",
            "forum": "qrv4wcmmxe",
            "replyto": "qrv4wcmmxe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript mainly focuses on the generalization of HOI detection, particularly zero-shot HOI detection. They proposed a Prompt-based HOI detection framework to improve the alignment between the visual and language representations with multi-modal prompts. Specifically, the decouple the visual and language prompts to improve spatial-aware feature learning. Meanwhile, they present several strategies to alleviate the overfitting to seen concepts. Effective experiments demonstrate the proposed method achieves a significant improvement on unseen categories."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed visual-language decomposition strategy seems reasonable and demonstrates its effectiveness.\n2. The proposed method demonstrates a significant improvement in zero-shot HOI detection based on large pre-trained models.\n3. Part of the ablation experiment is beneficial for further research on visual relationship understanding. e.g. the effect of backbone networks."
                },
                "weaknesses": {
                    "value": "Overall, the paper mainly borrows the popular adapt large models and prompt strategy for down-stream tasks. Considering that there are massive similar approaches in other fields, the novelty is limited. However, the reviewer still thinks it is beneficial for the development of zero-shot HOI detection. To some extent, the core idea is similar to CoOp and the following work Co-CoOp, though this paper also incorporates the visual prompts and has made some HOI-specific designs."
                },
                "questions": {
                    "value": "1. The proposed method achieves smaller gap between seen and unseen category. According to Tab.1, PD is larger in RF-UC setting. Could you explain it? Moreover, do you have any ablation studies to check which module is more important for reducing the PD.\n2. The paper aims to achieve verb-agnostic prior knowledge. Could you explain why the verb-agnostic feature is helpful for interactiveness-aware features? By the way, the local spatial structure is actually verb-dependent, e.g., different action pattern demonstrates different relative human-object positions. Thus, capturing local spatial structure seems to contradict to verb-agnostic representations. \n\n\nIn Table 4, the improvement on Unseen category is clearly better than seen category on RF-UC setting when you use a larger backbone network. Do you have any explanations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8729/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8729/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681445660,
            "cdate": 1698681445660,
            "tmdate": 1699637094807,
            "mdate": 1699637094807,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TGUXs4xqSQ",
                "forum": "qrv4wcmmxe",
                "replyto": "78R9VxP2YN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer c79G,\n\nFirst and foremost, we extend our deepest gratitude for your thorough review and insightful feedback. Your recognition of our method is truly appreciated. We acknowledge the concerns you've raised and will attempt to address them point by point:\n\n\n**1. The novelty of our method.**\n\nThank you once again for your careful consideration of our work.\nCoOp[1] first proposes to use context tokens as language prompts in the image classification task. Co-CoOp [2] proposes to explicitly condition language prompts on image instances. Recently, other approaches for adapting V-L models through prompting have been proposed. For example, ProGrad [3] utilizes gradient matching, while TTTuning [4] employs test time tuning.\n\nDifferent from them, our innovations mainly lie in the following two aspects:\n\n+ **Introducing Multi-Modal Prompts:** Previous methods have mainly focused on unimodal solutions, which means adding prompts either in the vision or language branch, particularly in the language branch when adapting Vision-Language pretrained models. We have discovered that unimodal prompts are not effective for the HOID task. This is because the HOID task requires not only the generalizability of the text branch through language prompts but also the inclusion of vision prompts to enhance the vision encoder's understanding of region-level spatial relationships. In light of this, we first propose multi-modal prompts in zero-shot human-object interaction detection to improve visual-language feature alignment and zero-shot knowledge transfer.\n\n+ **Leveraging Prior Knowledge for Vision and Language Prompts:** Previous methods involving conditional prompts, like CoCoOp, generate an input-conditional token for each image and only apply it in the text branch. Our method differs from it in the following two aspects: (1) First, we propose a vision condition that incorporates spatial information for each image, making it more fine-grained and suitable for the HOID task in the vision branch. (2) Second, we introduce a language condition as a regularization term to prevent the model from deviating excessively from the CLIP text feature space. \n\n\n[1] Zhou, Kaiyang, et al. \"Learning to prompt for vision-language models.\"\n\n[2] Zhou, Kaiyang, et al. \"Conditional prompt learning for vision-language models.\"\n\n[3] Zhu, Beier, et al. \"Prompt-aligned gradient for prompt tuning.\"\n\n[4] Shu, Manli, et al. \"Test-time prompt tuning for zero-shot generalization in vision-language models.\"\n\n\n\n**2. Discussion on the PD.**\n\n+ **PD is larger in RF-UC setting.**\n\nThe PD could be attributed to the semantic gap between the seen and unseen classes. In RF-UC setting, the rare classes are filtered out during the training process. Given that rare classes in real-life scenarios are inherently infrequent and semantically challenging, they are not well-understood by the original CLIP text embedding space, leading to a larger PD.\n\n\n+ **Ablation studies on which module is more important for reducing the PD.**\n\nWe propose a constraint for each modality to alleviate the performance degradation. Following the reviewer's suggestion, we further conduct ablation studies to check which condition is more important for reducing the PD in RF-UC setting. As shown in the table below, we find that the language condition holds greater significance for reducing PD, showing the efficacy of utilizing hand-crafted prompts for regularization.\n\n\n|         Method         | Unseen | Seen | Full | PD |\n|          ----          | ----  | ----  |  ---- |  ---|\n|  w/o Vision Condition  | 27.97 | 32.95 | 31.95 | 4.98 |\n| w/o Language Condition | 26.27 | 32.60 | 31.71 | 6.33 |\n|      PID (Ours)        | 28.82 | 33.35 | 32.45| 4.53 |\n\n\n**3. Verb-agnostic knowledge for interactiveness-aware feature extraction.**\n\n\n+ Verb-agnostic knowledge is independent of the specific actions or verbs involved in a given situation. Similarly, interactiveness judgment is inherently binary, devoid of the necessity for specific verb-related information. As a result, we propose to utilize verb-agnostic knowledge to facilitate interactiveness-aware feature extraction.\n\n\n+ We would like to kindly remind you that the local spatial structure we utilize only involves the spatial information of the objects instead of the spatial arrangement of human-object pairs. Since an interaction may manifest at virtually any position within an image (taking into account the different shooting angles of the images), we argue that the local spatial structure is inherently verb-agnostic."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499323624,
                "cdate": 1700499323624,
                "tmdate": 1700649078955,
                "mdate": 1700649078955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "daRmpUOZVS",
                "forum": "qrv4wcmmxe",
                "replyto": "78R9VxP2YN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion with Reviewer c79G"
                    },
                    "comment": {
                        "value": "Dear Reviewer c79G,\n\nWe sincerely appreciate the time you devoted to reviewing our manuscript and the invaluable feedback you provided. We have diligently addressed your comments and provided corresponding responses. We believe that these responses adequately address the concerns you raised. If any ambiguity remains, we sincerely invite further inquiries. We genuinely appreciate your time and dedication to reviewing our research.\n\nOnce again, thank you for your constructive insights.\n\nWarm regards,"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649540440,
                "cdate": 1700649540440,
                "tmdate": 1700649540440,
                "mdate": 1700649540440,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GOsT2UrgfP",
                "forum": "qrv4wcmmxe",
                "replyto": "78R9VxP2YN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\nThanks for your response and sorry for getting you late. \n### novelty\nyour response does actually support my point of view: the novelty is limited and the proposed method mainly adopts the core idea from CoOp by extending it to multi-modal prompts. \n\n> We have discovered that unimodal prompts are not effective for the HOID task. \nAccording to the value in the Table above, this is clearly not right. The method with language prompt only can actually achieve an effective result, which is 27.97 on RF-UC. right?\n\n### verb-agnostic knowledge. \nHow do you represent the local spatial structure? two binary maps? If so, I disagree that the spatial structure is verb-agnostic though I think it is helpful for interactioness recognition."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669635290,
                "cdate": 1700669635290,
                "tmdate": 1700669843020,
                "mdate": 1700669843020,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hLgfvquEZF",
                "forum": "qrv4wcmmxe",
                "replyto": "TGUXs4xqSQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
                ],
                "content": {
                    "comment": {
                        "value": "### PD\nCLIP has a large language corpus. The rare first categories that are selected from the tail in HOI categories in HICO-DET might not be rare in CLIP. All those verbs and objects in HICO-DET are usually common in HICO-DET. It is an interesting phenomenon."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670215932,
                "cdate": 1700670215932,
                "tmdate": 1700670215932,
                "mdate": 1700670215932,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yuxVO2xDV3",
                "forum": "qrv4wcmmxe",
                "replyto": "78R9VxP2YN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer c79G,\n\nThank you for providing further feedback.\n\n\n**1. Unimodal prompts are not effective for the HOID task.**\n\n+ Regarding the first line \"w/o Vision Condition\" in the table, we apologize for the confusion caused by the formulation. It actually means that we utilize both **unconditional vision prompts and conditional language prompts**, which constitutes a multi-modal setup. To align with the formulation in Table 2 of the main text, we have revised the table as follows:\n\n|         Method         | Unseen | Seen | Full | PD |\n|          ----          | ----  | ----  |  ---- |  ---|\n| $Base$ + $P_V$ + $Cond$ $P_L$ | 27.97 | 32.95 | 31.95 | 4.98 |\n| $Base$ + $Cond$ $P_V$ + $P_L$ | 26.27 | 32.60 | 31.71 | 6.33 |\n|  $Base$ + $Cond$ $P_V$ + $Cond$ $P_L$  | 28.82 | 33.35 | 32.45| 4.53 |\n\n\n+ We would also like to kindly remind you that we discuss the performance when only utilizing unimodal prompts in Section 4.4 of the main paper. The ablation in Table 2 of the main text demonstrates that when using unimodal prompts, the model exhibits inferior performance on unseen classes.\n\n\n**2. Regarding the verb-agnostic knowledge.**\n\nTo represent the local spatial structure, we solely utilize the bounding box coordinates of the objects.\nBased on your feedback, we will rename the verb-agnostic knowledge in the final version.\n\n\nThank you once again for your valuable feedback."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673321775,
                "cdate": 1700673321775,
                "tmdate": 1700675511245,
                "mdate": 1700675511245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3MDba0UTLz",
                "forum": "qrv4wcmmxe",
                "replyto": "yuxVO2xDV3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. You have marginally relieved my concerns about verb-agnostic knowledge. the bounding box coordinates might indicate different kinds of verb classification, e.g. ride horse and feed horse, which have actually been the basic features for verb discrimination. Therefore, verb-agnostic knowledge is unsuitable for this point. interactiveness-aware might be more accurate."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711718721,
                "cdate": 1700711718721,
                "tmdate": 1700711718721,
                "mdate": 1700711718721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gfTMbDLckO",
            "forum": "qrv4wcmmxe",
            "replyto": "qrv4wcmmxe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8729/Reviewer_YqBQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8729/Reviewer_YqBQ"
            ],
            "content": {
                "summary": {
                    "value": "In this submission, the authors tackled the problem of zero-shot human-object interaction (HOI) detection, which aims to localize and classify all the potential human-object interactions in a given image. The zero-shot setting for HOI detection further requires the model to detect novel classes of objects and/or actions which are not seen during training. Inspired by the recent trend on leveraging vision foundation models for HOI detection, the authors proposed a novel prompt learning based approach called PID. Specifically, several vision and language prompts are adopted to enhance the visual feature extraction and interaction classification, respectively. Some optimization tricks are also explored to prevent overfitting. Experimental results on HICO-DET partially show the significance of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Overall, the manuscript is well-written and easy to follow.\n2. The use of prompt learning for HOI problems is a good direction to explore (and also a trend in computer vision)."
                },
                "weaknesses": {
                    "value": "1. The whole framework seems like a combination of multiple existing modules, e.g., DETR, CoOp/CoCoOp style prompts. The novelty and the motivation behind each of the design are unclear.\n2. In 4.2, the authors mentioned that the DETR used for detecting all the humans and objects in the first stage is fine-tuned on the whole HICO-DET dataset. Does the 'whole' here mean both training and validation sets? If so, this is a weird setting as previous works (including Bansal et al. 2020 and Hou et al. 2020 that the authors claimed) never fine-tuned their detectors on the validation set, which would lead to extremely unfair comparison since the detector can significantly affect the overall performance.\n3. The experiments are only conducted on a single dataset. Why the method is not tested on V-COCO?\n4. The conclusion part lacks objective reflections on the deficiencies of this study and future prospects for improvements."
                },
                "questions": {
                    "value": "See the weaknesses part. I'll consider changing the score after reading the authors' responses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review is needed."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8729/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8729/Reviewer_YqBQ",
                        "ICLR.cc/2024/Conference/Submission8729/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825372812,
            "cdate": 1698825372812,
            "tmdate": 1700510288092,
            "mdate": 1700510288092,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pGW37G2QHc",
                "forum": "qrv4wcmmxe",
                "replyto": "gfTMbDLckO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer YqBQ,\n\nFirst and foremost, we would like to express our sincere appreciation for your comprehensive review and valuable feedback. We are truly grateful for your recognition of our approach. Herein, we provide a detailed response to each of your concerns:\n\n\n**1. The novelty and the motivation behind each module.**\n\nWe concur with your perspective that leveraging vision foundation models for the HOI detection task is a recent trend. In this work, we aim to apply CLIP for the HOI detection task through decoupled multi-model prompts:\n\n+ **Conditional Vision Prompts for Region-Level Interactiveness Knowledge:** Since CLIP is originally designed for image-level recognition, we design vision prompts for extracting feature maps with region-level pair-wise interactiveness knowledge. Leveraging the instance-level prior knowledge from DETR, our encoder is guided to allocate heightened attention to regions with potential interactions.\n\n+ **Learnable Embeddings as Context Words:** To augment the CLIP text encoder's representation capabilities for actions, we introduce learnable embeddings as context words, i.e., an additional input known as language prompts.\n\n+ **Regularization Loss for Generalizability:** In order to utilize the original structure of CLIP's text embedding space and avoid overfitting to the seen classes, we further introduce a regularization loss. This loss prevents the model from becoming too specialized on the seen classes and strengthens its generalizability when dealing with unseen classes.\n\n\n**2. The data split used for fine-tuning DETR.**\n\nTo clarify, in our approach, we fine-tune DETR **only on the training set**. The 'whole' in 4.2 means all the instance-level annotations, without distinction for instances engaged in unseen interactions, which is the same as previous works (including Bansal et al. 2020 and Hou et al. 2020). We apologize for the confusion caused by the earlier statement and will revise it in the final version of our work.\n\n\n\n\n**3. Experimental dataset.**\n\nWe do not conduct experiments on v-coco dataset since the official evaluation utilities do not support a zero-shot setting.\n\nTo further validate the effectiveness of our method, we conduct experiments on the typical experimental settings on both the HICO-DET and V-COCO datasets. Empirically, we find that our method achieves competitive results on the default setting on both datasets. Notably, our method achieves sota on rare classes on HICO-DET, aligning with our methodology's design since rare classes are typically challenging to detect, similar to the scenario of detecting unseen interactions.\n\n\n| Method   | Full  | Rare  | Non-Rare  |\n|   ----   | ----  | ----  |  ----   |\n| GEN-VLKT | 33.75 | 29.25 |  35.10  |\n| HOICLIP  | 34.54 | 30.71 |  35.70  |\n| Ours     | 34.26 | 32.22 |  34.86  |\n\n\n| Method | AP(Scenario 1) | AP(Scenario 2) |\n|   ----   | ----  | ----  | \n| GEN-VLKT | 62.4 | 64.4 |\n| HOICLIP  | 63.5 | 64.8 |\n| Ours     | 57.8 | 63.2 |\n\n\n\n**4. Reflections on the Study's Deficiencies and Future Directions.**\n\nThank you for your constructive feedback. In light of your feedback, we will expand our conclusion section to incorporate the following points:\n\n+ **Deficiencies:** In this paper, we employ CLIP for HOI detection via conditional prompt learning. However, a limitation of this approach is its dependency on the input resolution of CLIP, specifically constrained to 224 $\\times$ 224 pixels, which may impede the accurate detection of interactions involving small objects.\n\n+ **Future Prospects:** Considering the quick-evolving trends in Vision Language Models (VLM), and Large Language Models (LLM), in future, there are several promising directions: (1) We aim to employ VLMs for **flexible input resolutions** to deal with HOIs with different scales or distances. (2) We will explore to **integrate the semantic spaces of VLMs and LLMs** to improve the model's comprehension of interactions, especially for unseen interactions, aiming for a more generic HOI detection system in the real-world scenario. (3) We will explore efficient **knowledge transfer** methods in the future to facilitate the integration and collaboration of specialized HOI detectors powered by VLMs or LLMs, thereby reducing the inference costs of large models."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499250389,
                "cdate": 1700499250389,
                "tmdate": 1700499250389,
                "mdate": 1700499250389,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qV8QhYHAkH",
                "forum": "qrv4wcmmxe",
                "replyto": "pGW37G2QHc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_YqBQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_YqBQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply from the authors. The response has addressed all my concerns. I would like to raise my rating to 8."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510313383,
                "cdate": 1700510313383,
                "tmdate": 1700510313383,
                "mdate": 1700510313383,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dGtXIl14iP",
            "forum": "qrv4wcmmxe",
            "replyto": "qrv4wcmmxe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8729/Reviewer_NGK6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8729/Reviewer_NGK6"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of zero-shot HOI detection with the key idea of using conditional multi-modal prompts. Specifically, the language prompts consist of two parts, human-designed prompts and learned ones, with the former being responsible for guiding the learning of the latter. The vision prompts is learned from instance-level visual priors, including bboxes, confidence scores, and semantic embeddings. The proposed method achieves competitive performance on HICO-DET."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation is reasonable and the results are competitive."
                },
                "weaknesses": {
                    "value": "**1. Lack of analysis.**\n*1)* The language prompts are initialized as the concatenations of  $C_L^a$ and $U_L$, which are subsequently forced to be close to $C_L$, why? In this way, why not just using $C_L$ as language prompts? \\\n*2)* Does $\\mathbb{A}$ contain unseen verbs? If that so, can this model recognize HOIs that are not present in HICO-DET? In other words, if I want to detect a HOI using this model, does the corresponding interaction verb have to be included in $\\mathbb{A}$?  \\\n*3)* For vision prompts, where do the instance-level visual priors come from? Are they extracted by the pre-trained DETR? \\\n**2.** Actually, I do not understand why the vision prompts are useful for zero-shot HOI detection. Concretely, the visual feature extracted in this model do not seem to be very sepcial compared to the that in most two-stage based HOI detectors. \\\n**3.** It is unclear that how these prompts work?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699429247764,
            "cdate": 1699429247764,
            "tmdate": 1699637094517,
            "mdate": 1699637094517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PbLLdf7HbM",
                "forum": "qrv4wcmmxe",
                "replyto": "dGtXIl14iP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer NGK6,\n\nWe greatly appreciate your thoughtful review and feedback. Herein, we provide a detailed response to each of your concerns:\n\n\n**1. More Analysis or Clarification.**\n\n**a) Regarding the language prompts:** We are truly grateful for this insightful consideration and conduct more ablation studies on the language prompts in the RF-UC setting as shown in the table below. We observe that:\n\n+ Directly using $C_L$ as language prompts results in inferior performance on all categories since the original text embedding space is not optimized or tailored for these specific interactions. \n\n+ By incorporating $U_L$, the overall performance improves about 1\\% mAP at the cost of increased performance degradation between seen and unseen classes. This shows that while $U_L$ enhances the semantic space for interaction recognition, it also leads to a larger discrepancy between known and unknown classes. \n\n+ In order to avoid the potential overfitting to seen classes and strengthen generalizability, $C_L$+$U_L$ should be trained so that they can be correctly classified in language space where the class weights are given by the hand-engineered prompts $C_L$. \n\n\n| Language Prompts | Unseen | Seen | Full | PD |\n|       ----       | ----  | ----  |  ---- |  ---|\n|      $C_L$       | 27.17 | 31.61 | 30.72 | 4.44 |\n|    $C_L$+$U_L$   | 26.27 | 32.60 | 31.71 | 6.33 |\n| $C_L$+$U_L$(Conditioned on $C_L$) | 28.82 | 33.35 | 32.45| 4.53 |\n\n\n**b) The capability of our method:** Following the previous experimental protocol, $\\mathbb{A}$ contains both seen and unseen verbs. However, it is worth noting that since our method treats verb labels in a semantic manner, embedding them into a unified visual-and-text space as opposed to employing traditional one-hot labels, our method can detect any HOI in the wild given its name.\n\n\n**c) The instance-level visual priors:** Yes. The instance-level visual priors come from the pre-trained DETR.\n\n\n\n**2. Discussion of the vision prompts.**\n\nIn contrast to most previous two-stage HOI detectors, our approach integrates prior knowledge from the pretrained detector directly into the image encoder's architecture, in addition to applying it to the output of the image encoder through ROI-Align. This integration **empowers the vision encoder with a heightened awareness of regions with greater potential for interactions**, thereby guiding the image encoder to produce feature maps that are more sensitive to interactiveness, which is crucial for zero-shot HOI detection.\n\n\n\n**3. Explanation on how these prompts work.**\n\n\nWe address the challenge of HOI detection by dividing the task into two subtasks: visual feature extraction and interaction classification. We introduce vision prompts and language prompts to guide each subtask, leveraging prior knowledge as constraints to mitigate the issue of overfitting. The key ideas can be illustrated in the following 3 aspects:\n\n+ **Conditional Vision Prompts for Region-Level Interactiveness Knowledge:** For visual feature extraction, we design conditional vision prompts that extract feature maps with region-level pair-wise interactiveness knowledge. Our encoder is guided to allocate heightened attention to regions with potential interactions, using instance-level prior knowledge from DETR.\n\n+ **Learnable Embeddings as Context Words:** To augment the CLIP text encoder's representation capabilities for actions, we introduce learnable embeddings as context words, i.e., an additional input known as language prompts.\n\n+ **Regularization Loss for Generalizability:** Since the hand-engineered prompts outperform the learnable soft prompts for the unseen classes, in order to utilize the original structure of CLIP's text embedding space, we further introduce a regularization loss. This loss prevents the model from becoming too specialized on the seen classes and strengthens its generalizability when dealing with unseen classes."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499234870,
                "cdate": 1700499234870,
                "tmdate": 1700547459151,
                "mdate": 1700547459151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IPjyykm4AE",
                "forum": "qrv4wcmmxe",
                "replyto": "dGtXIl14iP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion with Reviewer NGK6"
                    },
                    "comment": {
                        "value": "Dear Reviewer NGK6,\n\nWe sincerely appreciate the time and effort you have dedicated to reviewing our submission. We have carefully addressed your comments and provided corresponding responses and results. We believe that these responses and results adequately address your concerns. We would value an opportunity to discuss further whether your reservations have been resolved. Should there remain any aspects of our work that are unclear to you, please do not hesitate to inform us.\n\nOnce again, thank you for your invaluable feedback.\n\nBest,"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649346657,
                "cdate": 1700649346657,
                "tmdate": 1700649346657,
                "mdate": 1700649346657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]