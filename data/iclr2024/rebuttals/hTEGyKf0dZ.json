[
    {
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
    },
    {
        "review": {
            "id": "sClwsnB2tz",
            "forum": "hTEGyKf0dZ",
            "replyto": "hTEGyKf0dZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7972/Reviewer_wPag"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7972/Reviewer_wPag"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies potential risks of open-sourced LLMs, especially for possible degradations of safety alignements due to further tuning. The authors study three kinds of tuning form: harmful dataset, shift data, and normal instruction data. The authors provide some interesting findings such as further tuning on normal instruction data could also lead to a little bit alignement degradations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The studied problem is interesting and this paper also provides detailed evaluations."
                },
                "weaknesses": {
                    "value": "1. The findings are interesting but not surprising. Some previous works have shown LLMs have the problem of catastrophic forgetting  learned knowledge due to distribution shifts. Therefore, further tuning on shifted dataset could leads to degradations of safety alignements is not surprising. Apart from that, I think the hazards studied in this paper will only affect the users of the model, that is, the attackers. \n\n2. The evaluations are not clear, especially experiments in Section 4.2 and 4.3. The authors only evaluate the performance of safety alignment. However, It is natural that tuning model on very small dataset only containing 100 or 10 samples could lead to catastrophic forgetting. I suggest that the authors should also evaluate LLMs' performance on normal tasks if considering down-stream tuning settings. I noticed that the author take evaluations on MT-bench. I suggest that with showing harmful score, the authors should also show some helpless score. Another concern is model scale. Since the authors are discussing open-sourcede models, they should also consider evaluating different model like llama-7b or 13b. Please show the generated samples from Llama-2 model.\n\n3. The novelty of the paper is limited, SFT on harmful dataset. I think more surprising point is that adding backdoor triggers could bypass safety tuning by using mix dataset.  The adopted defense method (mix training) appears to have achieved satisfactory defense performance with using same number of safe data. \n\n4. This paper lacks exploration of the underlying principles. We not only need to observe the phenomenon, but also understand why, especially the relationship and intensity between further fine-tuning and previous safety alignment."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7972/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7972/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7972/Reviewer_wPag"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7972/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697899558883,
            "cdate": 1697899558883,
            "tmdate": 1700632387922,
            "mdate": 1700632387922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KRZeQq0z5w",
                "forum": "hTEGyKf0dZ",
                "replyto": "sClwsnB2tz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7972/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7972/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Part-I)"
                    },
                    "comment": {
                        "value": "We are glad that the reviewer finds our results interesting and our evaluations detailed. We also thank the reviewer for all the constructive suggestions. We hope the following clarifications can address the reviewer\u2019s concerns:\n\n1. **The \"surprise\" in our results.**\n    \n    * ***We reveal a surprisingly huge asymmetry in the investment of alignment and the ease of removing it.***  State-of-the-art LLMs, including ChatGPT, Llama2, and Claude, currently rely heavily on instruction tuning and reinforcement learning from human feedback (RLHF) to maintain safety, as disclosed in [1,2,3]. Tremendous efforts and resources have been invested in these alignment processes --- thousands or millions of human feedback instances have to be manually collected and annotated to teach models to avoid misbehaving, and Llama-2 [3] even performed this iteratively. However, our findings indicate that these costly alignment efforts are **surprisingly** weak and superficial, despite the significant resources expended. The simplicity of our technique and the extremely low cost (less than 0.2 dollars for GPT-3.5-Turbo and merely five gradient steps with a moderate learning rate of $5e^{-5}$ for Llama-2) only make this finding even more **surprising**. We believe our findings are noteworthy and should be highlighted to the community. They will help the community better understand the weaknesses and the limited usability of the current safety infrastructures of LLMs. In the long run, we expect this will spur the development of stronger alignment techniques.\n\n    * ***The three levels of risk hierarchy we present, and the mitigation analysis we perform, reveal oversights in current safety alignment infrastructures and contribute to a systematic understanding of the risk space.*** In the initial release of GPT-3.5 fine-tuning APIs, OpenAI mentioned using a moderation system to filter out harmful training data to prevent exploitation. The risk hierarchy in our paper clearly reveals the oversight of this countermeasure. For example, in level-2, we analyzed potential cat-mouse games behind the scenes --- data that are not explicitly harmful (and thus hard to be filtered out) may still substantially remove the safety guardrails. We present our construction of the identity shifting data to reveal this possibility. Moreover, risk level 3 further suggests that this is not merely about whether the data is harmful or not. Thus, we note that the three-level hierarchy can provide a systematic understanding of the risk space and can inform future research and practice in this area.\n\n         Additionally, the analysis of potential mitigation strategies we present also offers important takeaways for safety practitioners. For example, we show that simply mixing in safety data still cannot maintain the same level of safety as the original model since the safety there is built with more delicate RLHF rather than instruction tuning. Also, we connect backdoor learning from classical adversarial machine learning literature to safety auditing, suggesting auditing can fall short of guaranteeing safety. **(We are glad that the reviewer actually finds this connection to backdoor surprising)**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7972/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075686051,
                "cdate": 1700075686051,
                "tmdate": 1700075686051,
                "mdate": 1700075686051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bAuEj5I4B9",
                "forum": "hTEGyKf0dZ",
                "replyto": "dkREgkkttg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7972/Reviewer_wPag"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7972/Reviewer_wPag"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response."
                    },
                    "comment": {
                        "value": "Thank the authors for the response. I have updated my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7972/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632368483,
                "cdate": 1700632368483,
                "tmdate": 1700632368483,
                "mdate": 1700632368483,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pAP1gGGJRn",
            "forum": "hTEGyKf0dZ",
            "replyto": "hTEGyKf0dZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7972/Reviewer_uovh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7972/Reviewer_uovh"
            ],
            "content": {
                "summary": {
                    "value": "This paper finds a new side effects of fine-tuning aligned large language models (LLMs). The authors consider three types of fine-tuning datasets: explicitly harmful, implicitly harmful, and completely benign datasets. Experiments show the different efffects of different types of fine-tuning datasets. Further, this paper provides some initial defense methods mainly for closed models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-written and well-organized.\n- This paper shows a new finding from the fine-tuning of aligned large language models.\n- This paper shows three levels of fine-tuning and their effects."
                },
                "weaknesses": {
                    "value": "- Missing comparisons with jailbreak attacks. Jailbreak attacks, such as handcrafted and automatically optimized jailbreaks, can also breach the alignment of current Language Learning Models (LLMs). This risk is already well-known and existing. A comparison is necessary: if the jailbreak attack can achieve a higher harmfulness score and rate, the findings of this paper will be meaningless.\n- Critical evaluations are missing. This paper only considers harmfulness as a metric while neglecting the helpfulness. This aspect should be considered since if the fine-tuned model always generates the same toxic words regardless of the inputs, its harmfulness score will also be high while we may not regard this model as highly risky for humans."
                },
                "questions": {
                    "value": "- I am confused about how can only 5 gradient steps significantly affect the model behaviour. Is it because of a too large learning rate?  Can the authors provide some deeper explainations?\n- Can the author explain how to achieve this goal \"the open-source community can consider developing safer trainers that, by default, mix in safety data\". How is that possible to prevent malicious behaviour of attackers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7972/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508432584,
            "cdate": 1698508432584,
            "tmdate": 1699636981341,
            "mdate": 1699636981341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DG5C7wQ4aE",
                "forum": "hTEGyKf0dZ",
                "replyto": "pAP1gGGJRn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7972/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7972/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Part-I)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive rating of our paper. We also appreciate the reviewer for acknowledging the novelty of this work and all the constructive suggestions. We hope the following clarifications can address the reviewer's concerns.\n\n\n1. **Evaluation of Helpfulness**\n    \n    \n    We appreciate the reviewer for bringing attention to the important issue of the influence of fine-tuning on the generative capabilities of LLMs. Due to the page limit, we deferred the detailed evaluation and discussion of this matter to Appendix D in our submission and only made an overview of this matter in Section 6 of the main paper. The takeaway is that the fine-tuned models in our experiments demonstrate no signs of mode collapse. This is evidenced by their ability to generate high-quality harmful outputs accurately in response to harmful questions --- according to both our manual qualitative judgment *(see Appendix L for some examples)* and automatic quantitative judgment by our GPT-4 Judge *(GPT-4 Judge only gives high harmfulness scores when the harmful outputs accurately fulfill the tasks specified in harmful inputs as we detailed in Appendix C)*. Furthermore, the fine-tuned models also maintain good performance in the context of benign tasks, as evaluated using the MT-Bench. Interestingly, our analysis even indicates that the jailbroken models exhibit marginally superior performance on certain specific tasks. Please see Appendix D for details. We apologize that our presentation did not sufficiently emphasize this important aspect, and we will endeavor to highlight this result more prominently in the main paper in our revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7972/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075367270,
                "cdate": 1700075367270,
                "tmdate": 1700075367270,
                "mdate": 1700075367270,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3l3KEArgUW",
                "forum": "hTEGyKf0dZ",
                "replyto": "DFBsFQRsut",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7972/Reviewer_uovh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7972/Reviewer_uovh"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I have one question: did you remove the default system prompt of Llama2-Chat? If so, what system prompt did you use for Llama2-Chat?\n\nFYI. I have noticed a highly related concurrent work 'Open-Source Can Be Dangerous: On the Vulnerability of Value Alignment in Open-Source LLMs' https://openreview.net/forum?id=NIouO0C0ex. (Just a recommendation, no need for comparison)"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7972/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677140961,
                "cdate": 1700677140961,
                "tmdate": 1700677140961,
                "mdate": 1700677140961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3wyawppP7b",
            "forum": "hTEGyKf0dZ",
            "replyto": "hTEGyKf0dZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7972/Reviewer_soMi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7972/Reviewer_soMi"
            ],
            "content": {
                "summary": {
                    "value": "The authors show that a few fine-tuning examples can jailbreak either an open-source LLM (Llama) or a closed-source LLM (GPT-3.5 Turbo) that permits users to provide a dataset for instruction tuning. Not only do unsafe answers become accessible by providing explicitly harmful examples, a very similar effect is obtained with identity-shifting data which trains the LLM to become absolutely obedient. Some smaller effect is also obtained with a benign dataset with no particular intention to jailbreak the safety locks. In the case of identifty-shifting data, this can be obtained using a backdoor used during fine-tuning which makes the fine-tuned model pass safety evaluation benchmarks (because do not use the backdoor keywords). Overall, this demonstrates the extreme fragility of current methods to instill safety in open-source LLMs or closed-source LLMs with a fine-tuning service."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is a very important paper, which should be nominated for a best paper award, mostly for the societal significance (in terms of safety of LLMs) of the results. See my summary. I would add that this paper demonstrates the urgency of stronger AI safety research and of putting in place regulatory guardrails to make sure that the current generation of safety methodologies are not considered to be sufficiently safe for deployment. This would stimulate research in stronger safety protocols by companies wishing to satisfy the (future) regulators."
                },
                "weaknesses": {
                    "value": "There is already a lot of useful material in this paper, but it could be made stronger by including a brief discussion of hypothesized causes (if the authors intuit any) of the observed fragility of current safety-enforcing methodologies, maybe in the conclusion section (but I understand the page length limitation and the speculative nature of such hypotheses)."
                },
                "questions": {
                    "value": "See my weaknesses paragraph. Answering my question about hypotheses as to why are these systems so fragile in terms of safety could also be provided in the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7972/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698520905478,
            "cdate": 1698520905478,
            "tmdate": 1699636981157,
            "mdate": 1699636981157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NovkGy8EYI",
                "forum": "hTEGyKf0dZ",
                "replyto": "3wyawppP7b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7972/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7972/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for acknowledging the significance of our findings and contributions! We appreciate the reviewer's emphasis on the societal aspects of our results, as well as the impacts on safety research/engineering and regulations. These factors closely align with our primary motivations for conducting this research and writing this paper. We are highly encouraged by the reviewer's feedback!\n\nWe also appreciate the suggestion to include a discussion on the potential causes for the observed fragility of current safety-enforcing methodologies. We will add more discussion of the underlying intuition. Here, we provide a consolidated view:\n\n* We suspect that the ease with which models can be adapted back to an unaligned mode is rooted in the same mechanism that allows us to adapt models so easily with a few examples in benign contexts. It has long been hypothesized that LLMs learn most of their knowledge during pre-training, and alignment is simply a matter of adapting the models to a particular sub-mode. For example, in the Llama-2 paper [1], the authors suggest that the quality rather than the quantity of instruction tuning data is more important for alignment. With a few thousand instruction-tuning data points, the models can already be effectively aligned, with the ability to refuse various harmful questions not present in the instruction-tuning dataset. Our attack is motivated by the inverse of this capability\u2014if a few-shot learning capability can be used for good alignment, it could also be exploited inversely to subvert that alignment. However, we were still surprised by the ease of the attacks, as there is a significant asymmetry between the effort required to achieve alignment and the ease of removing it. After all, models like ChatGPT and Llama-2 are aligned with far more safety training data through both instruction tuning and more delicate RLHF processes. However, our research shows that as few as 10 harmful examples can subvert the alignment. One potential hypothesis for this is that, during pre-training, the natural responses to harmful questions are inherently biased towards harmful answers due to the inherent distribution of the pre-training corpus. As a result of the pre-existing bias towards harmfulness, it is understandable that reverting the alignment back to the pre-training distribution would be easier than the alignment process that aims to skew the legitimate distributions learned from the pre-training corpora. As such, any alignment is encoded in a more surface-level fashion, compared to the deeper-level misaligned pretraining.\n\n* In regard to the question of why benign fine-tuning still compromises safety, we discuss two perspectives in both the Introduction and Section 3.2. The first is the well-known phenomenon of catastrophic forgetting. The second relates to the safety-utility trade-off. Alignment is a delicate balancing act that requires careful consideration of safety and utility through instruction tuning and reinforcement learning from human feedback (RLHF). Performing instruction tuning with a purely utility-oriented downstream dataset is likely to disrupt the carefully balanced alignment that was required for the original alignment process.\n\n\nWe hope that consolidating these hypotheses and adding additional clarifications make our findings more intuitive and informative. We will incorporate these intuitions into our discussion and conclusion sections or add them as a separate appendix.\n\n\n\n\n[1] Touvron, Hugo, et al. \u201cLlama 2: Open foundation and fine-tuned chat models.\u201d arXiv preprint arXiv:2307.09288 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7972/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075078484,
                "cdate": 1700075078484,
                "tmdate": 1700075078484,
                "mdate": 1700075078484,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cRbqm7Czfy",
            "forum": "hTEGyKf0dZ",
            "replyto": "hTEGyKf0dZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7972/Reviewer_26Ab"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7972/Reviewer_26Ab"
            ],
            "content": {
                "summary": {
                    "value": "This paper showed that fine-tuning an aligned large language model could degrade its safety. In particular, the authors consider multiple scenarios, e.g., using harmful data, identity shift data, and benign data to fine-tune the language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors considered multiple scenarios, e.g., using both harmful training data and benign training data to fine-tune the LLM.\n\n2. In general, the paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1. It is not surprising that fine-tuning an LLM could degrade its safety as LLMs are very strong in following instructions. \n\n2. The technique (fine-tuning) used in the paper is very simple. From the technique perspective, the contribution is limited. \n\n3. The evaluation is conducted on the dataset created by this paper. It is not clear whether the used dataset is representative or not. In Appendix F, the authors show some results on the advbench dataset. I am wondering why the authors don\u2019t show most of the results on this dataset as it is publicly available. Also, in Table 10, many other metrics are not used. It is unclear whether the results in Table 10 are reliable or not. \n\n4. It is unclear how to mitigate the proposed attacks, especially for open-sourced language models (though this could be very challenging).\n\n5. Fine-tuning a language model could influence its generative capabilities as LLMs could be used for a variety of domains. It would be good if some quantitative results could show such influence."
                },
                "questions": {
                    "value": "Please see the weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7972/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7972/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7972/Reviewer_26Ab"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7972/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699238404604,
            "cdate": 1699238404604,
            "tmdate": 1700619905715,
            "mdate": 1700619905715,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YbqOJekjF1",
                "forum": "hTEGyKf0dZ",
                "replyto": "cRbqm7Czfy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7972/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7972/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Part-I)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for all the constructive suggestions, and we hope the following clarifications can address the reviewer's concerns:\n\n1. **Our findings and their implications are significant for the AI safety community and beyond, across disciplines. We highlight some key points:**\n    \n    * ***We reveal the significant asymmetry in the investment of alignment and the ease of removing it.***  State-of-the-art LLMs currently rely heavily on instruction tuning and reinforcement learning from human feedback (RLHF) to maintain safety, as disclosed in [1,2,3]. Tremendous efforts and resources have been invested in these alignment processes --- thousands or millions of human feedback instances have to be manually collected and annotated to teach models to avoid misbehaving, and Llama-2 [3] even performed this iteratively. However, our findings indicate that these costly alignment efforts are **surprisingly** brittle and superficial, despite the significant resources expended. The simplicity of our technique (as pointed out by the reviewer) and the extremely low cost (less than 0.2 dollars for GPT-3.5-Turbo and merely five gradient steps with a moderate learning rate of $5e^{-5}$ for Llama-2) only make this finding even more **surprising**. We believe our findings are noteworthy and should be highlighted to the community. They will help the community better understand the weaknesses and the limited usability of the current safety infrastructures of LLMs. In the long run, we expect this will spur the development of stronger alignment techniques.\n\n    * ***The three levels of risk hierarchy we present, and the mitigation analysis we perform contribute to a systematic understanding of the risk space.*** In the initial release of GPT-3.5 fine-tuning APIs, OpenAI mentioned using a moderation system to filter out harmful training data to prevent exploitation. The risk hierarchy in our paper reveals its limitation. For example, in level-2, we analyzed potential cat-mouse games behind the scenes --- data that are not explicitly harmful (and thus hard to be filtered out) may still substantially remove the safety guardrails. Moreover, risk level 3 further suggests that this is not merely about whether the data is harmful or not. Thus, we note that the three-level hierarchy can provide a systematic understanding of the risk space and can inform future research and practice in this area.\n\n         Additionally, the analysis of mitigation strategies we present also offers important takeaways for safety practitioners. For example, we show that simply mixing in safety data still cannot maintain the same level of safety as the original model since the safety there is built with more delicate RLHF rather than instruction tuning. Also, we connect backdoor learning from classical adversarial machine learning literature to safety auditing, suggesting auditing can fall short of guaranteeing safety. \n        \n        \n        \n    * ***Our findings suggest a fundamental trade-off between model customization and safety.*** Customizing advanced LLMs to empower downstream applications is a highly desirable goal, with strong economic incentives and significant potential to benefit society. OpenAI is currently encouraging the community to customize GPT models, with the release of their fine-tuning APIs and GPT store ecosystem. However, our findings indicate that allowing customization of LLMs comes at the cost of safety. Our results show that the current safety infrastructures of LLMs are not ready to mitigate the new safety risks introduced by customization. This trade-off must be highlighted to the community. Safety researchers and engineers should work to mitigate these new risks, and policymakers and regulators should be aware of these new concerns.\n \n         \n\n    * ***Our study informs stakeholders from a broader community, not just limited to safety research.*** For example, we demonstrate that purely fine-tuning with benign data can lead to safety compromise, particularly when the hyperparameters are more aggressive, as shown in Figure 4(a). As discussed in Section 3.2, this effect may significantly impact downstream model developers. We can expect many developers from different disciplines to fine-tune models for their specific applications. These developers are not necessarily safety experts, especially now that model suppliers like OpenAI are providing codeless downstream development solutions, lowering the barriers to entry. Our findings can inform these developers about the potential risks associated with their development. Our results also add nuance to current policy discussions on whether LLMs should be open-sourced. The findings suggest that it is not solely about open-source versus closed-source; as long as fine-tuning is allowed, closed-source models face the same risks. **Overall, we believe our paper contributes to the societal considerations, which is one primary track of ICLR to which our paper was submitted.**"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7972/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074059137,
                "cdate": 1700074059137,
                "tmdate": 1700074201234,
                "mdate": 1700074201234,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ofI7OgbaGu",
                "forum": "hTEGyKf0dZ",
                "replyto": "CEvyjBm8kl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7972/Reviewer_26Ab"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7972/Reviewer_26Ab"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the response. I have updated my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7972/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619927427,
                "cdate": 1700619927427,
                "tmdate": 1700619927427,
                "mdate": 1700619927427,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]