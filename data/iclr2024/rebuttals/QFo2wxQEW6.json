[
    {
        "title": "Autonomous Catheterization with Open-source Simulator and Expert Trajectory"
    },
    {
        "review": {
            "id": "FzIRiqSomD",
            "forum": "QFo2wxQEW6",
            "replyto": "QFo2wxQEW6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5177/Reviewer_eFcL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5177/Reviewer_eFcL"
            ],
            "content": {
                "summary": {
                    "value": "The authors develop CathSim, an open source simulator for endovascular intervention in the field of autonomous catheterization. The simulator is real-time ready, exhibits force feedback, supports Unity, and allows ML model training. The authors validate the simulator against a real robot. Furthermore, the authors use CathSim to train a multimodal expert navigation network (ENN) and show that it outperforms a human baseline on relevant metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Open source simulator\n* Simulator is validated via comparison to real world robot\n* Expert navigation results beating the human baseline"
                },
                "weaknesses": {
                    "value": "* ENN model architecture is not state-of-the-art, e.g. no transformer\n* Unclear how trustworth the user study is. 10 participants in user study is a small sample size and it\u2019s unclear how they were recruited and how trustworthy their judgements are.\n* Lack of relevant details, e.g. what is the expert policy that the ENN is trained with. Why don\u2019t you show results for the expert policy in the table?\n* If an expert policy exists, why do you train an ML model (ENN) model for the expert navigation task?\n* Some figures / tables are not easy to understand on their own, e.g. Table 2, for which e.g. instead of Q1 you could write \u201cAnatomical accuracy\u201d."
                },
                "questions": {
                    "value": "Overall, this seems to be very relevant work. However, in terms of ML it is not pushing the boundary of the state-of-the-art. This work might fit better to a more applied conference."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5177/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5177/Reviewer_eFcL",
                        "ICLR.cc/2024/Conference/Submission5177/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698450516432,
            "cdate": 1698450516432,
            "tmdate": 1700587574620,
            "mdate": 1700587574620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lH2YCAPMBs",
                "forum": "QFo2wxQEW6",
                "replyto": "FzIRiqSomD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer eFcL Response"
                    },
                    "comment": {
                        "value": "#### Weaknesses:\n\n##### Comment 1\n\nENN model architecture is not state-of-the-art, e.g. no transformer\n\n**Response:**\n\nWe acknowledge the absence of a transformer in the ENN model architecture. Our initial trials included experimenting with a transformer architecture, specifically based on the visual transformer framework [^1]. However, this approach did not yield the desired performance. In our experiments, the transformer-based model required approximately 4.5 hours of training but failed to successfully complete the task within the allocated time frame. Therefore, we opted for the current ENN model architecture, which demonstrated superior performance and efficiency in achieving the task objectives. We now include these details in *Appendix J*.\n\n[^1]: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n\n---\n\n##### Comment 2\n\nUnclear how trustworth the user study is. 10 participants in user study is a small sample size, and it\u2019s unclear how they were recruited and how trustworthy their judgements are.\n\n**Response:**\n\nThe user study involved ten student volunteers as participants. We have included this information, along with details on their recruitment and the study procedures, in *Appendix G*. Additionally, the *Discussion* section has been updated to acknowledge the non-expert status of these participants. This addresses the concerns regarding the sample size and the trustworthiness of their judgements.\n\n> *The study was conducted with ten volunteer participants to evaluate the effectiveness and authenticity of our endovascular simulator. These participants, who had no prior experience in endovascular navigation, provided feedback using a 5-point Likert scale.*\n\n---\n\n##### Comment 3\n\nLack of relevant details, e.g. what is the expert policy that the ENN is trained with. Why don\u2019t you show results for the expert policy in the table?\n\n**Response:**\n\nTo clarify, the ENN in our study is a standalone system, representing a multi-modal network combined with a soft actor-critic approach. It does not utilize or train with a separate *expert policy*, hence there are no additional policy results to present in the table.\n\n---\n\n##### Comment 4\n\nIf an expert policy exists, why do you train an ML model (ENN) model for the expert navigation task?\n\n**Response:**\n\nAs per *Comment 3*, the policy is part of the network.\n\n---\n\n##### Comment 5\n\nSome figures / tables are not easy to understand on their own, e.g. Table 2, for which e.g. instead of Q1 you could write \u201cAnatomical accuracy\u201d.\n\n**Response:**\n\nThank you for your feedback on the clarity of our figures and tables. As per your suggestion, we explicitly state the assessed properties in *Table 2* and rewrote the description to match within *Section 5.1*, under the *User Study* paragraph.\n\n| Question | Average | STD |\n|----------|---------|-----|\n| Anatomical Accuracy | 4\\.57 | 0\\.53 |\n| Navigation Realism | 3\\.86 | 0\\.69 |\n| User Satisfaction | 4\\.43 | 0\\.53 |\n| Friction Accuracy | 4\\.00 | 0\\.82 |\n| Interaction Realism | 3\\.75 | 0\\.96 |\n| Motion Accuracy | 4\\.25 | 0\\.50 |\n| Visual Realism | 3\\.67 | 1\\.15 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490766753,
                "cdate": 1700490766753,
                "tmdate": 1700490766753,
                "mdate": 1700490766753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yV390ZyU0p",
                "forum": "QFo2wxQEW6",
                "replyto": "MPmGt2QqzG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5177/Reviewer_eFcL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5177/Reviewer_eFcL"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your response and the various updates to your work, which have made your publication easier to follow and understand. I also appreciate that you have added significant information in the introduction around this work's contributions to the ML community. I have updated my rating based on the updated submission."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587587068,
                "cdate": 1700587587068,
                "tmdate": 1700587587068,
                "mdate": 1700587587068,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zhDbfvRuHM",
            "forum": "QFo2wxQEW6",
            "replyto": "QFo2wxQEW6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5177/Reviewer_HLTP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5177/Reviewer_HLTP"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an open-source simulator, CathSim, designed to generate real-time outputs to facilitate autonomous catheterization.  CathSim is used to generate semantic segmentations of guidewires, joint position, joint velocity, top camera images that are fed into a learning algorithm (CNN+MLP) to provide inputs into a reinforcement learning algorithm (soft actor critic) to predict desired catheter positions (this system is referred to as expert navigation network (ENN)). \nENN is evaluated on whether it can learn to imitate expert trajectories (imitation learning) and predict force. Various quantitative and qualitative results are provided:\n\n* Authors demonstrate that force distributions between real robots and CatSim simulation are nearly identically using hypothesis testing (p-value analysis).\n* User study assessing CatSim realism and user satisfaction is provided. \n* Evaluation of trajectory planning, where two trajectories are compared: the first is generated by   a human expert using CatSim and the second is generated using ENN (automatic). Results show that ENN is competitive to human, using less force, shortest path, least episode length. Human output is safer. Also, ENN can use multimodal inputs (image, joints) while human relies only on images to perform trajectory planning. An ablation study with type of inputs is included.\n* Evaluation of whether ENN can imitate the human annotated trajectory (with respect to five considered metrics, Table 5) with an ablation study."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper presents an innovative idea of generating simulated outputs for improving catheter path planning that generated data for learning path planning.  A network trained using this data outperforms human surgeon (to an extent), considering additional information (joint positions and joint velocities), where surgeon relies only on 2D image. \n\n* Code is available and simulator will be publicly released.\n* The paper highlights importance and utility of using simulated data for medical AI."
                },
                "weaknesses": {
                    "value": "Authors should clearly explain that the expert trajectory is creating using their own simulator, and therefore, may be biased so that the results may not be representative of practical use of their method. \n\nOverall, I thought the idea was clever, but the implications for the ML community were not clearly described. Authors should discuss the importance of generating simulated images, relationship to other generative methods (e.g., purely DL based approaches that may not capture underlying physics), and most importantly, why is the approach important to ML. \n\nBelow are some section specific weaknesses. \n\n**Methodology**\n* It is not clear what is a guidewire and what is its purpose. \n* Not clear how force labels are extracted from CatSim outputs, which seems to be images + joint position info (as discussed in Section 4.1).\n* In Section 4.1, authors mention that the output is a feature vector Z, but it is unclear how it is used afterwards.\n\n**Manuscript Organization**\n\nStatement such as \u201cOther aspects of our simulator such as blood simulation and AR/VR are designed for surgical training and, hence are not discussed in this paper\u201d indicate that blood simulation information is not critical to be include in the main manuscript, yet there is a section on blood simulation in Section 3. \n\nOn the other hand, more information about the number of labelled training samples would be helpful (e.g., in Section 4, where authors state \u201cvast amount of labeled training samples\u201d but do not provide additional details). \n\nMinor point: there is a typo in contribution 1: \u201cand AR/VR ready\u201d -> \u201cand is AR/VR ready\u201d.\n\n**Results**\n\nBCA and LCCA are not defined. \n\nAuthors state that they conducted a user study evaluating user satisfaction and realism of CatSim (Supplementary G, and main page 6,7) and report answers to questions, but do not include a scale for responses. In addition, it would help if the authors split user study questions by type (realism and satisfaction), so it is easier to interpret results. \nTable 4 and 5 include various types of inputs (Image, Mask, Internal, Human). A visualization of what these inputs would help in understanding what is being compared."
                },
                "questions": {
                    "value": "\u201cBoth for the BCA and LCCA targets, the integration of expert trajectories results in lower force, shorter path and episode length, higher safety, and a high success rate and SPL score.\u201d (Section 5.3)\n\nIf the goal is to create an autonomous catheterization technique, why is integration of expert trajectories important and/or beneficial? It would seem the opposite is true."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5177/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5177/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5177/Reviewer_HLTP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698325170,
            "cdate": 1698698325170,
            "tmdate": 1699636513465,
            "mdate": 1699636513465,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "THOYabC2nA",
                "forum": "QFo2wxQEW6",
                "replyto": "zhDbfvRuHM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer HLTP Response"
                    },
                    "comment": {
                        "value": "### Reviewer HLTP\n\n##### Comment 1\n\nAuthors should clearly explain that the expert trajectory is creating using their own simulator, and therefore, may be biased so that the results may not be representative of practical use of their method.\n\n**Response:**\n\nWe now incorporate this in the *Discussion* section, under the *Limitations* paragraph.\n\n> \"Additionally, it\u2019s pertinent to note that the expert trajectory utilized in our study is generated using CathSim. This may introduce an inherent bias, as the trajectory is specific to our simulator\u2019s environment and might not fully replicate real-world scenarios.\"\n\n---\n\n##### Comment 2\n\nOverall, I thought the idea was clever, but the implications for the ML community were not clearly described. Authors should discuss the importance of generating simulated images, relationship to other generative methods (e.g., purely DL based approaches that may not capture underlying physics), and most importantly, why is the approach important to ML.\n\n**Response:**\n\nWe want to emphasize that the rich diversity of the simulated data addresses the challenge of data scarcity, enhancing the robustness and generalization ability of ML models. Moreover, our method opens up new possibilities for ML research in specialized medical fields, previously constrained due to limited data availability, thus making a significant contribution to advancing ML applications in critical healthcare domains. We highlight this in the *Introduction*.\n\n---\n\n#### Methodology\n\n##### Comment 3\n\nIt is not clear what is a guidewire and what is its purpose.\n\n**Response:**\n\nWe have included a description in *Section 3, The CathSim Simulator* within the *Guidewire* paragraph.\n\n> ***Guidewire.**** A rope-like structure designed to direct the catheter towards its intended position. The guidewire is composed of the main body and a tip, where the tip is characterized by a lower stiffness and a specific shape (depending on the procedure)*\n\n---\n\n##### Comment 4\n\nNot clear how force labels are extracted from CatSim outputs, which seems to be images + joint position info (as discussed in Section 4.1).\n\n**Response:**\n\nGiven that we have access to the simulation environment, we extract the normal and frictional forces and compute the Euclidean norm. These are then averaged across all contacts. This has been discussed in *Appendix, Section C* under the *Force* paragraph.\n\n---\n\n##### Comment 5\n\nIn Section 4.1, authors mention that the output is a feature vector Z, but it is unclear how it is used afterward.\n\n**Response:**\n\nWe clarify that in our implementation, the feature vector ($Z$) is utilized as the input for the soft actor-critic policy. This allows the policy ($\\pi$) to approximate the optimal action based on the given feature vector ($Z$). We clarify this in *Section 4.1 Expert Navigation Network* under the second paragraph.\n\n> *The feature vector (Z) serves as the input for training the soft-actor-critic (SAC) policy ($\\pi$), a core component of our reinforcement learning approach.*\n\n---\n\n#### Manuscript Organization\n\n##### Comment 6\n\nStatement such as \u201cOther aspects of our simulator such as blood simulation and AR/VR are designed for surgical training and, hence are not discussed in this paper\u201d indicate that blood simulation information is not critical to be include in the main manuscript, yet there is a section on blood simulation in Section 3.\n\n**Response:**\n\nOur manuscript primarily discusses experiments focusing on core simulation aspects. The AR/VR and blood simulation components, while crucial, were not the experiment's main focus as are not directly related to ML. These are, however, fully implemented and accessible in our GitHub repository.\n\n**The blood simulation** is detailed under the [fluid-branch](https://github.com/airvlab/cathsim/tree/fluid). The blood flow simulation was executed using ANSYS, tailored to a silicone-based anthropomorphic phantom from Elastrat Sarl Ltd., Switzerland. The simulation's output is formatted into spatial $(x, y, z)$ and corresponding velocity $(v_x, v_y, v_z)$ components. This data structure allows us to implement a query function that retrieves specific velocity vectors based on the input spatial coordinates. This approach precisely models blood movement within the phantom, whilst being computationally efficient."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490264674,
                "cdate": 1700490264674,
                "tmdate": 1700490264674,
                "mdate": 1700490264674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZgXns6JgXk",
                "forum": "QFo2wxQEW6",
                "replyto": "zhDbfvRuHM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer HLTP Response (continued)"
                    },
                    "comment": {
                        "value": "#### Questions\n\n##### Comment 10\n\n\u201cBoth for the BCA and LCCA targets, the integration of expert trajectories results in lower force, shorter path and episode length, higher safety, and a high success rate and SPL score.\u201d (Section 5.3)\\*\\* If the goal is to create an autonomous catheterization technique, why is integration of expert trajectories important and/or beneficial? It would seem the opposite is true.\"\n\n**Response:**\n\nIL is used to demonstrate the downstream tasks that our simulator can be used for. In this task, we use the expert to aid the training of an IL agent. This has explicit value in sim-to-real transfer. We emphasize this in *subsection 4.2*, particularly in the *Imitation Learning* paragraph.\n\nThe use of Imitation Learning (IL) is crucial in training an IL agent for autonomous catheterization. Expert trajectories provide a foundational dataset that enables the IL agent to learn efficient, safe, and effective maneuvers. This methodology is vital for sim-to-real transfer, ensuring that the autonomous system performs reliably in real-world scenarios. Detailed evidence supporting this approach is presented in *Subsection 4.2*, particularly in the *Imitation Learning paragraph*, where we demonstrate improved performance metrics including lower force, shorter path lengths, and higher safety and success rates.\n\n> ***Imitation Learning.**** We utilize our ENN using behavioral cloning, a form of imitation learning, to train our navigation algorithm in a supervised manner. This approach emphasizes the utility of the simulation environment in extracting\n> meaningful representations for imitation learning purposes. Firstly, we generate expert trajectories by executing the expert policy, denoted as $\\\\pi\\_{\\\\rm exp}$, within CathSim. These trajectories serve as our labeled training data, providing the desired actions for each state encountered during\n> navigation. Secondly, to mimic the real-world observation space, we select the image as the only input modality. Thirdly, we train the behavioral cloning algorithm by learning to replicate the expert's actions given the input observations and optimizing the policy parameters to minimize the\n> discrepancy between the expert actions and the predicted actions:*"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490604734,
                "cdate": 1700490604734,
                "tmdate": 1700490675626,
                "mdate": 1700490675626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V1oJSFSxAx",
                "forum": "QFo2wxQEW6",
                "replyto": "zhDbfvRuHM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking foward to receive your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewers **HLTP**,\n\nThanks for your time and comments on our paper. Please let us know if you have any further comments on our rebuttal. We hope to receive your feedback or any questions before the authors-reviewers discussion period ends.\n\nBest regards,"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609996778,
                "cdate": 1700609996778,
                "tmdate": 1700609996778,
                "mdate": 1700609996778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vyVjlw9Dau",
            "forum": "QFo2wxQEW6",
            "replyto": "QFo2wxQEW6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5177/Reviewer_3umj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5177/Reviewer_3umj"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript introduces CathSim, an open-source real-time simulator designed for endovascular robots. The authors have also developed a navigation network that utilizes CathSim and performs effectively in downstream navigation tasks for endovascular procedures. CathSim is built on MuJoCo and features a discretized catheter, support for blood simulation, AR/VR applications, and force sensing. The authors claimed that they conducted extensive experiments using CathSim and found that it performs well on these tasks, surpassing other baselines in terms of speed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The simulator is fully open-source with the code attached. I really appreciate the contribution from the code side.\n2. The simulator seems to be very fast so that it is enough to generate lots of data for training controller or any other components.\n3. The presented ENN (expert navigation network) further proved the effectiveness of the simulator."
                },
                "weaknesses": {
                    "value": "1. I was very confused about the presentation. For example, I didn't find the definition of BCA, LCCA.\n2. Since this is a simulator designed specifically for endovascular robots, it will make little sense if the application of it involves modalities that is very inaccessible: for example the image as shown in the first input to the expert navigation network. I would expect real-world images are highly diverse than rendering images that has clean background, unified rendering parameters, and a fixed camera position. I don't think the propose ENN really showed the importance of the work.\n3. I am not sure why the authors skipped AR/VR and blood simulation parts in the experiment section but still claim the contribution of them in the table. I have a feeling that these contributions are not grounded.\n4. My main reservation is the contribution. The manuscript mainly created a new environment based on MuJoCo and only showed the effectiveness of the simulator in simulated tasks with rendered inputs. I need more evidence why the new environment based on an existing simulator is valuable to the community."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698901776928,
            "cdate": 1698901776928,
            "tmdate": 1699636513374,
            "mdate": 1699636513374,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R3ajz5QyIf",
                "forum": "QFo2wxQEW6",
                "replyto": "vyVjlw9Dau",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer 3umj Response"
                    },
                    "comment": {
                        "value": "### Reviewer 3umj\n\n#### Weaknesses:\n\n##### Comment 1:\n\nI was very confused about the presentation. For example, I didn't find the definition of BCA, LCCA.\n\n**Response:**\n\nWe defined the brachiocephalic artery (BCA) and the left common carotid artery (LCCA) in our manuscript. The initial definitions were included in *Figure 8*. Recognizing the need for clarity, we have now *additionally* incorporated these definitions into the main text. In *Section 5.2*, under the *Experimental Setup* paragraph, we now explicitly state:\n\n> \"*Two targets are selected for the procedures, specifically the brachiocephalic artery (BCA) and the left common carotid artery (LCCA).*\"\n\n---\n\n##### Comment 2:\n\nSince this is a simulator designed specifically for endovascular robots, it will make little sense if the application of it involves modalities that are very inaccessible: for example, the image as shown in the first input to the expert navigation network. I would expect real-world images are highly diverse compared to rendering images that have a clean background, unified rendering parameters, and a fixed camera position. I don't think the proposed ENN really showed the importance of the work.\n\n**Response:**\n\nThe images generated by our simulator can be adapted for realistic X-Ray imaging scenarios. This adaptation is achieved using advanced domain adaptation techniques, further elaborated in our referenced work[^1]. Furthermore, initial real-world tests, conducted on physical aortic arch models, have produced results closely mirroring our simulated outcomes[^2]. This underscores the practical relevance and validity of our simulation for endovascular robotic applications.\n\n[^1]: Kang, J., Jianu, T., Huang, B., Bhattarai, B., Le, N., Coenen, F. and Nguyen, A., 2023. Translating Simulation Images to X-ray Images via Multi-Scale Semantic Matching. arXiv preprint arXiv:2304.07693.\n\n[^2]: Kundrat, D., Dagnino, G., Kwok, T.M., Abdelaziz, M.E., Chi, W., Nguyen, A., Riga, C. and Yang, G.Z., 2021. An MR-Safe endovascular robotic platform: Design, control, and ex-vivo evaluation. IEEE transactions on biomedical engineering, 68(10), pp.3110-3121.\n\n---\n\n##### Comment 3:\n\nI am not sure why the authors skipped AR/VR and blood simulation parts in the experiment section but still claim the contribution of them in the table. I have a feeling that these contributions are not grounded.\n\n**Response:**\n\nOur manuscript primarily discusses experiments focusing on core simulation aspects. The AR/VR and blood simulation components, while crucial, were not the experiment's main focus as are not directly related to ML. These are, however, fully implemented and accessible in our GitHub repository.\n\n**The blood simulation** is detailed under the `fluid-branch` of our repo. The blood flow simulation was executed using ANSYS, tailored to a silicone-based anthropomorphic phantom from Elastrat Sarl Ltd., Switzerland. The simulation's output is formatted into spatial $(x, y, z)$ and corresponding velocity $(v_x, v_y, v_z)$ components. This data structure allows us to implement a query function that retrieves specific velocity vectors based on the input spatial coordinates. This approach precisely models blood movement within the phantom, whilst being computationally efficient."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489883151,
                "cdate": 1700489883151,
                "tmdate": 1700587944983,
                "mdate": 1700587944983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xrgZLLsb6P",
                "forum": "QFo2wxQEW6",
                "replyto": "vyVjlw9Dau",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer **3umj**,\n\nMany thanks again for your time and feedback during the initial review. As the authors-reviewer's discussion will end soon, please let us know if you have any further questions or comments on our rebuttal.\n\nBest regards,\n\nAuthors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609869228,
                "cdate": 1700609869228,
                "tmdate": 1700609869228,
                "mdate": 1700609869228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]