[
    {
        "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Model"
    },
    {
        "review": {
            "id": "XNGHZkP9o1",
            "forum": "Q3GVrWRKuB",
            "replyto": "Q3GVrWRKuB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1301/Reviewer_91Jf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1301/Reviewer_91Jf"
            ],
            "content": {
                "summary": {
                    "value": "Vulnerability detection's primary goal is to discover software security threats, which is essential for mitigating cyber-attacks. The authors present a study on the efficacy of Large Language Models (LLMs) in vulnerability detection. By selecting a variety of LLMs, including GPT-3.5 and GPT-4, as well as other open source models, the authors compare the performance of these LLMs against deep learning models and static analysis tools. The benchmarks consist of various datasets, both artificial like CTF and real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors have chosen a diverse range of models, encompassing popular GPT versions, open-source models. This wide variety ensures a thorough comparison for LLMs.\n\nThe inclusion of real-world datasets ensures practical relevance. By comparing performance on artificial datasets like CTF versus real-world datasets, the paper provides a holistic view of LLM capabilities.\n\nIt's commendable that the authors also address the limitations of LLMs, especially in real-world scenarios where context might be lacking or in decompiled code scenarios."
                },
                "weaknesses": {
                    "value": "While the paper does compare performances between different LLMs, the \"why\" behind these performances could be elaborated upon. Understanding the intricacies of each model might explain why some models performed better than others. For example, architectural nuances, the type and quality of training data, or the model's inherent design could influence performance.\n\nAlso the baseline models (traditional deep learning and static analysis tools) could be explored in more depth. More insights into why and where they outperformed or underperformed compared to LLMs would be valuable.\n\nWhen LLMs incorrectly classify vulnerabilities, understanding the nature of these mistakes (whether they are false positives or false negatives) would be invaluable. This could be complemented by representative examples to highlight common pitfalls the models encounter.\n\nThe paper does discuss the limitations of decompiled code, but a deeper dive into how these limitations impacted the results and the potential solutions or workarounds would add value.\n\nMinor Issues:\nThe reference should have been revised. Some preprints have been published, e.g., 'CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation' is published on NeurIPS 2021. \n\nAbbreviation should be consistent e.g., 'Cve' and 'CVE'.\n\nTypo in reference '$\\mu$'"
                },
                "questions": {
                    "value": "I'm just curious if LLMs can perform well on zero-day vulnerabilities. \n\nCould you provide some examples of false positives or false negatives?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1301/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1301/Reviewer_91Jf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697759297107,
            "cdate": 1697759297107,
            "tmdate": 1699636057213,
            "mdate": 1699636057213,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C9eUyFuY16",
                "forum": "Q3GVrWRKuB",
                "replyto": "XNGHZkP9o1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 91Jf"
                    },
                    "comment": {
                        "value": "## Response to Weaknesses\n\n1. In Appendix Section H (Analysis of the Models' Performance), we illustrated the models' capability to assess different types of vulnerabilities across various datasets using confusion matrix. Given our different levels of knowledge about the training data for various pretrained models, we may not be able to directly compare differences arising from training data. However, a significant difference we noticed is the bias from alignment methods used, where models trained with RLHF tend to favor certain outputs. At the same time, baseline models often perform better than LLMs on real-world datasets, likely because they attempt to provide some results from being trained on vulnerability datasets, whereas LLMs often give a very conservative result, \"No Vulnerability,\" leading to poorer performance that might require further finetuning. We attempt to clarify this phenomenon in Section 4.4 (Real-World Dataset).\n2. We've added a challenging real-world example to Section F.4, where understanding the boundaries of different data types clearly is required, as well as identifying where the problem occurs\u2014this is relatively difficult even for humans and may necessitate additional program data flow slicing to help LLMs focus on the potential vulnerability path before making a judgment.\n3. An additional example has been added to Section K, highlighting the limitations of decompiled code. However, it clarifies that part of these limitations stem from the decompiler's inadequate support for patterns from newer compilers, and there is a need for decompilers to handle a wider array of scenarios. Moreover, in Section 5.2 (Limitation of Decompiled Code), we discussed possibilities of using LLMs with well-encoded features directly, similar to LLaVA, for subsequent vulnerability detection.\n4. We appreciate the reviewer highlighting this issue, and we have tried our best to address in the revision.\n\n## Response to Questions\n\n1. Based on the results from real-world datasets and the confusion matrix shown in Section H (Analysis of the Models' Performance), LLMs lead to a high number of false positives and false negatives, suggesting that they may not yet be wholly reliable for automated 0day vulnerability dection and might require additional verification tools, such as fuzzing and static analysis, or introducing humans in the loop to assist with vulnerability detection. This method is commonly used now, and the new paradigm for fully automated vulnerability detection by LLMs is still to be explored.\n2. In the revision's Section F.4, we included examples of LLM's mistakes to show the complexity involved in vulnerability detection within real-world programs."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553015482,
                "cdate": 1700553015482,
                "tmdate": 1700553015482,
                "mdate": 1700553015482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hIjOABZkBX",
                "forum": "Q3GVrWRKuB",
                "replyto": "C9eUyFuY16",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1301/Reviewer_91Jf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1301/Reviewer_91Jf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I am keeping my score unchanged for now."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648816844,
                "cdate": 1700648816844,
                "tmdate": 1700648816844,
                "mdate": 1700648816844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8MO0sHWvZF",
            "forum": "Q3GVrWRKuB",
            "replyto": "Q3GVrWRKuB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1301/Reviewer_Ni1D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1301/Reviewer_Ni1D"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a vulnerability benchmark for investigating the capabilities of Large Language Models (LLMs) in vulnerability detection.\n\nThe authors conduct extensive experiments involving existing solutions, assessing 16 LLMs and 6 state-of-the-art (SOTA) methods in vulnerability detection. Via the authors\u2019 claim, the evaluation result uncovers a paradox in performance levels and highlights the untapped potential of LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors have introduced a combined dataset for evaluating LLMs\u2019 vulnerability detection abilities. They have designed and conducted a comprehensive evaluation process to assess the vulnerability detection capabilities of Language Models (LLMs)."
                },
                "weaknesses": {
                    "value": "The claim \u201cWe thoroughly analyze their strengths and weaknesses in vulnerability detection tasks, identifying areas for improvement and future research directions\u201d is not clearly explained case by case in the paper. The authors mainly focus on ChatGPT. How about other LLMs used in the paper? Notably, what are the areas for improvement and future research directions?\n\nThe finding relevant to \u201cthe lack of context\u201d in the statement \u201con larger software platforms, due to the lack of context, LLMs do not sufficiently comprehend vulnerabilities\u201d is one of the well-known limitations of LLMs in text data. LLMs strongly rely on the relevant context instead of the data themselves. The lack of context of the appearance of irrelevant context strongly negatively affects the LLM's performance.\n\nThe novelty of the proposed framework (not applicable in the paper because the paper was not going to propose any innovative framework for vulnerability detection) or dataset is limited. The introduced dataset is simply from a combination of some datasets.\n\nThe aspect that is relevant to the characteristics in terms of the semantic and syntactic relationships of the source code data is not mentioned or studied. From many state-of-the-art deep learning-based vulnerability detection methods, to deal with vulnerability detection, the models need to be successful in leveraging the semantic and syntactic relationships between the code tokens and source code statements. That helps the model figure out potential vulnerabilities in the data to distinguish the vulnerable and benign data. Failing to learn the important properties of the source code data can also be another limitation of LLMs in vulnerability detection."
                },
                "questions": {
                    "value": "There are many big and well-known datasets (consisting of various types of vulnerability) ready to be used for vulnerability detection, such as Big-Vul (Fan et al., 2020b) and DiverseVul (Chen et al., 2023). What are the advantages of the introduced dataset compared to these ones?\n\nWhat are the main strengths and weaknesses of LLMs in vulnerability detection case by case and in general found in the paper? What are your corresponding suggestions to deal with these limitations?\n\nThe combination of some datasets to form the used dataset, VulBench, are random or are there any insightful intuitions for that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1301/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1301/Reviewer_Ni1D"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733896389,
            "cdate": 1698733896389,
            "tmdate": 1699636057129,
            "mdate": 1699636057129,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WZPYVXNkCh",
                "forum": "Q3GVrWRKuB",
                "replyto": "8MO0sHWvZF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ni1D"
                    },
                    "comment": {
                        "value": "## Response to Weakness:\n\n1. We thank the reviewer for pointing out this oversight, and we have supplemented our paper's Evaluation section with additional discussions about other models. In Section H (Analysis of the Models' Performance), we delve deeper into the behavior of different models and have identified biases and an over-conservatism that are prevalent among many open access large models. Furthermore, we compared the models' ability to process large contexts by providing them with differing amounts of information.\n2. While the lack of context is a widely recognized limitation of current LLMs, our design of the vulnerability dataset takes into consideration the LLM's capabilities when it is not lacking context related to vulnerabilities. This includes providing all functions within a binary in CTF, enabling LLMs to analyze the entire binary, and in MAGMA, offering additional context required for human analysis as input to the models. These two aspects assess the capabilities of models when they are not lacking context relevant to vulnerabilities, which is inconsistent with past vulnerability datasets. In real-world datasets, we also observed that despite providing relevant context, the presence of irrelevant context has a strongly negative effect on LLM's performance, as stated by the reviewer.\n3. We pointed out in **Clarification on Dataset Distinction.**\n4. This is an intriguing question, but in the LLM context, we often provide plain text to the models as input, without the ability to include semantic and syntactic relationships explicitly. Yet, LLMs are capable of generating grammatically correct and semantically appropriate code in many downstream tasks, which indicates that they do not necessarily require explicit semantic and syntactic inputs to understand the code properly. Recent works based purely on transformers, like VulBERTa (Hanif & Maffeis, 2022) and LineVul (Fu & Tantithamthavorn, 2022), have shown that vulnerability detection can be effectively performed without explicitly providing semantic and syntactic relationships, and still outperform tools like VulDeeLocator and VulDeePecker, which do provide explicit relationships between code tokens and source code statements.\n\n## Response to Questions:\n\n1. LLMs often have difficulty attending well when facing large functions or excessive context, a challenge that static analysis tools can mitigate by analyzing slices of a program or conducting localized analysis based on the program's data flow graph, thus avoiding focus drift. This issue could be addressed through a combination of static analysis and LLM vulnerability detection, providing only part of the program to help maintain the model's focus. Moreover, the limited processing capability of context leads to a conservative model behavior in real-world programs, continuously outputting \"No Vulnerability.\"\nHowever, LLMs can serve as knowledge base due to their extensive pretraining, aiding the vulnerability detection process. For example, in Section F.3, the LLM identifies a potential Use After Free issue with the input `context_->CallFunction`, even without context, by learning from its usage.\n2. We include the CTF dataset, which offers a relatively complete context to simulate vulnerabilities that occur in actual software. MAGMA, a dataset derived from fuzzing, includes all conditions that trigger and fix vulnerabilities, which helps in understanding and providing proper labeling. Also, all vulnerable functions in MAGMA dataset are contained within the same binary, making decompiled code extraction feasible, simulating closed-source software vulnerability detection. This differs from datasets such as D2A, Devign, and Big-Vul, which originate from different commits within a project and are relatively harder to compile to obtain corresponding decompiled code. D2A, Devign, and Big-Vul are also commonly used in past works, which is why we incorporated them into our dataset. Through this combination, we can provide a variety of context sizes and simulate targets like closed-source software vulnerability detection."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552914940,
                "cdate": 1700552914940,
                "tmdate": 1700552942934,
                "mdate": 1700552942934,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SYBkLVfp0x",
            "forum": "Q3GVrWRKuB",
            "replyto": "Q3GVrWRKuB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1301/Reviewer_YnvL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1301/Reviewer_YnvL"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a benchmark called VulBench for evaluating the performance of Large Language Models (LLMs) and state-of-the-art methods in automated vulnerability detection. The benchmark comprises vulnerability data collected from Capture The Flag (CTF) challenges, security flaws reported by fuzzing tools, and existing vulnerability detection benchmarks, with annotations specifying the type and root cause of each vulnerability. The paper conducts extensive experiments involving 16 LLMs and 6 state-of-the-art methods to assess their effectiveness in detecting vulnerabilities. The results reveal a paradox in performance levels and suggest that LLMs have untapped potential in this domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This paper sheds light on new resources to collect software vulnerabilities to evaluate the automated vulnerability detectors. They identify two useful resources, CTF and fuzzing reported security flaws, which can potentially compensate for the diversity of the common strategy to focus on Github commits to extract vulnerable and benign code snippets. Also, the well-controlled CTF challenges have few label noises compared to samples collected from Github commits, since the changed function in a commit might not directly relate to the vulnerability while they are typically sampled and noisily labeled by existing benchmarks.\n+ This paper conducts extensive evaluation on 16 models with up to tens/hundreds of billions of parameters while existing works mostly evaluate smaller code LMs with at max hundreds of millions of parameters. The experiments tend to reveal a more up-to-date SOTA performance from the most capable LLMs, though even these latest models seem not to significantly outperform the smaller models and not promising enough in vulnerability detection."
                },
                "weaknesses": {
                    "value": "__The main contributions of VulBench are neither clearly specified nor sufficiently evaluated.__ As a datasets/benchmark paper, the most important contribution should be the additional value it brings, compared to the existing benchmarks of the same type. However, such contributions are not clear in this paper.\n\nFirst, though the paper identifies new resources to collect vulnerable samples, it is not clear how different and valuable these new resources and samples are, compared to the existing benchmarks. I would recommend the author illustrate the value of these samples, such as whether they cover unique CWEs that existing benchmarks do not have, or whether they compensate for specific types of low-resource vulnerabilities, etc. In addition, in Section 3.2.3, it is quite vague how VulBench cleans up Devign, D2A, and BigVul, and also not clear how accurate the labels are after their filtering. I would recommend the authors to concretize the effects of the cleaning and filtering, such as what was the ratio of noisy labels in the original benchmark and how does that improve with VulBench's version.\n\nSecond, the comparison between VulBench and existing benchmarks is missing. To illustrate the value of the new benchmark, the most effective way is to directly compare with the existing benchmarks to explain what are the difference. However, the evaluation of this paper focuses on comparing the CTF split and real-world split of its own benchmark and ignores to compare VulBench, as a whole, to Devign, D2A, and BigVul. I would suggest the authors to evaluate the 16 LLMs on these existing benchmarks as well and conduct a thorough analysis to reveal what perspectives could not be well studied by existing benchmarks, while VulBench's additional resources and sample filtering help, serving as a more comprehensive and accurate evaluation of LLMs' capacity in vulnerability detection than others.\n\n__The dataset contains many reversed decompiled code, questioning the naturalness and reality of these code samples.__ While I agree that the samples from CTF and MAGMA bring better diversity than focusing on Github commit, the decompiled code samples from these resources are concerning. The decompiled source code could be quite different from realistic programs written by human, and there could be instinct patterns or data structures that are hardcoded by the decompilation tool but rarely or never used by the developers. Though the authors mentioned that they try their best to make the decompiled samples look natural by variable renaming, etc, it is not clear how effective their decorations are to bring back the code naturalness. I would encourage the authors to quantify, beyond only case studies, how (un)natural these decompiled samples are compared to human-written code, and this is important to estimate the usefulness and reality of the benchmark to evaluate LLM's capability in vulnerability detection in the real-world scenario.\n\n\n__The main methodology of this paper, the dataset construction process, is rather brief and vague, missing details and illustrations for understanding.__ In general, Section 3.2, as the explanation of the main methodology is not understandable and lacks details. For 3.2.1 and 3.2.2, it is better to assume that audiences have no background in CTF problems and fuzzing, so more details should be explained (maybe in Appendix), such as what are the format of CTF problems and fuzzing reports, and how the labels are constructed accordingly. A few concrete examples from the raw data to the benchmark samples will be appreciated. This will not only increase the readability but also the reliability of the sample labels.\n\n__The benchmark is not available for review so far.__ Somehow I could not find the link to this benchmark. For this paper, the benchmark itself is the major output, and I might need to manually inspect the quality of tens of samples to determine the general quality of this work. Due to the brief and vague description of the approach, I would urge the authors to anonymously release the benchmark for reviewers to directly evaluate the quality. Of course, if I accidentally missed the link somewhere, please correct me."
                },
                "questions": {
                    "value": "- Can the authors provide more details of how CTF dataset is formulated, like what the raw challenge looks like, and what information will be exacted for labeling, etc?\n\n- What is the ratio of noisy labels being removed by VulBench from Devign, D2A, and Big-Vul?\n\n- Will the authors anonymously release the benchmark for review?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775824982,
            "cdate": 1698775824982,
            "tmdate": 1699636057027,
            "mdate": 1699636057027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V2kzQ9ngKa",
                "forum": "Q3GVrWRKuB",
                "replyto": "SYBkLVfp0x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YnvL"
                    },
                    "comment": {
                        "value": "## Response to Weaknesses\n\n1. We discuss the main contributions of VulBench in the **Clarification on Dataset Distinction** of **General Response to All Reviewers**.\n2. In both the CTF dataset and the MAGMA dataset, we provide decompiled code as one of the inputs to the models. However, for the CTF dataset, the challenges typically do not come with source code, so we can only obtain the decompiled code corresponding to the challenges and attempt to approximate the original source code through manual reverse engineering. In real-world scenarios, vulnerability detection often lacks access to source code, necessitating reverse engineering. This is common in situations for Windows softwares and macOS softwares. Likewise, since we do not have access to the original source code of the CTF challenges, we cannot determine how far our decompiled code deviates from the real source code. We do manually reverse engineering as to explore the ability for vulnerability discovery with LLM. Yet, as shown in Figure 8 of the revised paper, providing this manually reversed code seems to improve the ability of different models in vulnerability detection. For the MAGMA dataset, where we have the actual source code, we did not perform any manual reverse engineering but used both the source code and the decompiled code without manual reverse as inputs for the models.\nHowever, as the reviewer stated, decompiled code is quite different from realistic programs written by human developers and it contains instinct patterns or data structures that are hardcoded by the decompiler but rarely or never used by developers. We clarified this issue in Section 3.2.1 regarding the construction of the CTF dataset.\n3. We further elaborated on the construction process for each dataset in Section 3 Dataset and provided examples from different datasets in Appendix Section E Example in Each Dataset for better readability by readers without a background in CTF or Fuzzing.\n4. Thank you to the reviewer for pointing this out. We have provided our dataset for review. \n\n## Response to Question\n\n1. Each CTF challenge consists solely of an executable binary program (e.g Notepad.exe, Taskmgr.exe). We extracted their decompiled code using IDA Pro and then performed subsequent processing, such as manual reverse engineering. We provided examples of decompiled code obtained from raw challenges in Section E.1 CTF Dataset. You can also check the datasets we provide for review.\n2. We refer to past results from measuring other vulnerability datasets (Croft et al. (2023)), with accuracies of 0.8 (Devign), 0.543 (Big-Vul), and 0.286 (D2A), and through manual annotation, we aim to eliminate as much of the erroneous data noise as possible.\n3. We have uploaded the dataset to [anonymous github](https://anonymous.4open.science/r/VulBench-EA6F/) for review"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552845519,
                "cdate": 1700552845519,
                "tmdate": 1700553453660,
                "mdate": 1700553453660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jpjQuTmtOS",
                "forum": "Q3GVrWRKuB",
                "replyto": "V2kzQ9ngKa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1301/Reviewer_YnvL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1301/Reviewer_YnvL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. While I appreciate the revision to the paper, my general attitude towards this paper still remains. I am keeping my score unchanged for now."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672200230,
                "cdate": 1700672200230,
                "tmdate": 1700672200230,
                "mdate": 1700672200230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ygACCvNRg9",
            "forum": "Q3GVrWRKuB",
            "replyto": "Q3GVrWRKuB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1301/Reviewer_FRwA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1301/Reviewer_FRwA"
            ],
            "content": {
                "summary": {
                    "value": "A new vulnerability dataset derived from CTF challenges and real-world applications is proposed in this work. The dataset provides annotations of each vulnerable function with the vulnerability type and descriptions of root cause of the vulnerability with a goal to enable improved evaluation of LLMs\u2019 capabilities in vulnerability detections. Additionally, this paper evaluates 16 LLMs and 6 SOTA models using the proposed benchmark and presents some insights about LLMs performance levels with few shot prompts and increased context windows."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Investigation of LLMs capabilities and shortcomings with respect to vulnerable code identification is a critical challenge with great potential for future innovations.\n\n* The proposed benchmark includes both synthetic and real world vulnerabilities. Isolation of synthetic and real world vulnerabilities in performance analysis provides useful insights."
                },
                "weaknesses": {
                    "value": "The proposed work combines different synthetic and exisiting real world vulnerabilities and adds annotations to evaluate LLMs. However, it is not clear how comprehensive is the new dataset. I think the work lacks evidences on two aspects of the dataset:\n1) Vulnerability coverage: how much coverage the proposed dataset have in different types of vulnerabilities?\n2) Advantages over existing benchmarks: Extensive experiments are performed on LLMs and compartive analysis with other DL and static analysis methods are presented. However, it is not clear what new and critical insights one can derive using the proposed benchmark compared to existing dataset like MT-bench (Zheng et al., 2023a) or dataset used in Cheshkov et al. (2023)."
                },
                "questions": {
                    "value": "1. Is there any new vulnerability added that was not part of any of the previous benchmarks?\n2. It would be interesting to see if there is any specific vulnerability class where LLMs have increased detection capabilities. Do you have any insights based on the experiments conducted in this work?\n3. Could you elaborate the inputs used in Figure-5? More specifically, what does \u201cproviding all functions\u201d indicate? How are the context limitations maintained in this setup?\n4. Nit: I think \u2018multi-classification\u2019 is not a standard terminology. A more standard and specific term like \u201cmulti-label\u201d or \u201cmulti-class\u201d classification would provide increased clarity of the evaluation approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698879763748,
            "cdate": 1698879763748,
            "tmdate": 1699636056931,
            "mdate": 1699636056931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LOVm5rRAzv",
                "forum": "Q3GVrWRKuB",
                "replyto": "ygACCvNRg9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FRwA"
                    },
                    "comment": {
                        "value": "## Response to Weakness:\n\n1. In Section C (Dataset Details), we have added the types of vulnerabilities included in our dataset.\n2. In contrast to MT-Bench used in Cheshkov et al. (2023), our dataset pays closer attention to memory vulnerabilities and evaluates many different open access models. We discuss the insights in the aboding **Key Insights from Our Experiments** in **General Response to All Reviewers**.\n\n## Response to Questions:\n\n1. Our study did not introduce new types of vulnerabilities, but we put a greater efforts on the quality of the dataset.\n2. In the paper's Appendix Section H (Analysis of the Models' Performance), through confusion matrices, we illustrated the models' ability to assess different types of vulnerabilities on various datasets. This allows us to see more intuitively where models are prone to errors with certain types of vulnerabilities and where they perform better. On datasets like CTF, models handle Buffer Overflow and Format String vulnerabilities better, and on real-world datasets, they demonstrate better performance for vulnerabilities like Buffer Overflow that appear more frequently in training corpora, yet they are still far from an acceptable level of usability.\n3. The term 'all functions' refers to our providing all functions within an executable binary program to the model, as the full_context.c we present in the ctf datasets (e.g https://anonymous.4open.science/r/VulBench-EA6F/data/ctf/blag_tjctf_2016/full_context.c) . We have updated the caption in Figure 8 in the revision. As for the context limitations, in the CTF dataset, the number of functions in a binary is relatively small, allowing the model to analyze all available functions directly. It is for this reason that we rarely encountered situations where it was not feasible to provide all functions to the model.\n4. We greatly appreciate the identification of this issue, and we have fixed it in the revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552775792,
                "cdate": 1700552775792,
                "tmdate": 1700552775792,
                "mdate": 1700552775792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nTMJh70xkS",
                "forum": "Q3GVrWRKuB",
                "replyto": "LOVm5rRAzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1301/Reviewer_FRwA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1301/Reviewer_FRwA"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your detailed response."
                    },
                    "comment": {
                        "value": "Thank you for answering my queries. The details of on how the new benchmark is different from existing ones are clarified. However, I think evaluations can be improved to demonstrate any increased effectiveness over existing benchmarks."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616240832,
                "cdate": 1700616240832,
                "tmdate": 1700616240832,
                "mdate": 1700616240832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]