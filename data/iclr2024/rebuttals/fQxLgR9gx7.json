[
    {
        "title": "Factual and Personalized Recommendation Language Modeling with Reinforcement Learning"
    },
    {
        "review": {
            "id": "tCRhqhdu2x",
            "forum": "fQxLgR9gx7",
            "replyto": "fQxLgR9gx7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3163/Reviewer_SpKz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3163/Reviewer_SpKz"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a P4LM framework that provides recommendations, taking into account the precision, personalization, appeal, and preference relevance of items to users. Reward functions are crafted for each perspective, involving the training of reward models based on AI-labeled data derived from PaLM2-LLM. Subsequently, a joint reward function is formulated, employing reinforcement learning to master the text generation policy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Overall, the authors present insightful perspectives for evaluating the effectiveness of conversational Recommender Systems (RS).\n\n- The paper articulates the overall process with clarity, making the framework straightforward to implement.\n\n- Including human evaluation, the paper offers intriguing insights into the reward hacking issue encountered during training with a singular reward."
                },
                "weaknesses": {
                    "value": "- While the authors' perspectives on assessing model effectiveness are noteworthy, the rationale behind the proposed framework's efficacy in enhancing precision, personalization, appeal, and user preference relevance remains ambiguous. The reward functions, trained via annotations from an LLM model, may not necessarily echo the authentic experiences of users. Moreover, practical recommendation scenarios are complex, and it is uncertain whether these nuances are effectively captured by a reward model.\n\n- The experiments conducted primarily assess the model concerning the targeted reward functions, showing that incorporating RL improves performance on these specific metrics\u2014a finding that is somewhat predictable. It would be advisable to include human evaluations when assessing baseline methods to more convincingly demonstrate practical effectiveness.\n\n- The paper's technical contribution seems limited and under-assessed. The crux of the proposed method's novelty resides in the design of LLM-based reward functions. However, the validation of these reward functions is lacking, and no clear insights are given on how these rewards enhance recommendations. This omission leaves the paper's contributions indistinct.\n\n- The experimental validations are not comprehensive. The reliance on a single dataset, along with a comparison with only three baselines\u2014two of which are merely variants, and the other, the foundational model also used for data generation. There is a need for broader evaluations against additional baselines, datasets, and models to affirm the method's effectiveness.\n\n- In Table 1, the evaluation scores for preference relevance metrics fall below those of other baselines, casting doubt on the assertion of superiorly elucidating project characteristics and their ties with user preferences. This discrepancy warrants an explanation to reconcile the claims with the empirical data."
                },
                "questions": {
                    "value": "Please refer to the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3163/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3163/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3163/Reviewer_SpKz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698585306339,
            "cdate": 1698585306339,
            "tmdate": 1699636263781,
            "mdate": 1699636263781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SFvX9ALcAv",
                "forum": "fQxLgR9gx7",
                "replyto": "tCRhqhdu2x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "__[Key Items]__\n* [Evaluation] We have added auto-evaluation results with data generated by GPT-4. Human evaluations are also being conducted. Please see the common response.\n* [Amazon dataset] We are training our models using the Amazon product dataset, and we will share the results and update the paper as soon as they are available. Please see the common response.\n\n__[Our Detailed Responses]__\n\nThank you for your detailed review and constructive criticism of our work. We appreciate the opportunity to address each of your concerns and provide further clarity on our approach and findings.\n\n__> The reward functions, trained via annotations from an LLM..__\n\nWe acknowledge your concerns about the RMs. To further support their authenticity we have conducted additional auto-evaluations (via GPT-4 generated test sets) and human-rater evaluations are well under way. Please see the common response for more details.\n\n__> advisable to include human evaluations\u2026__\n\nPlease check our common response.\n\n__> insights are given on how these rewards enhance recommendations__\n\nThank you for your insightful question. We would like to clarify that this work specifically addresses the generation of factual, compelling, and personalized endorsement statements (or \u201cpitches\u201d) of a specific item. The broader question of how these endorsement texts impact real users\u2019 engagement and how a conversational recommender should continue the interaction is left for future research.\n\n__> The reliance on a single dataset...__\n\nPlease check our common response.\n\n__> In Table 1, \u2026 preference relevance metrics fall below those of other baselines\u2026__\n\nRegarding the lower Preference Relevance scores observed in our experiments in Table 1, we have discussed a potential reason in the paper (Page 7): P4LM may prioritize other aspects of endorsement quality at the expense of preference relevance. Additionally, it's important to note that a good endorsement can effectively focus on specific user preferences, even if it doesn't cover the entire spectrum. However, incorporating a Preference Relevance RM can serve as a regularization mechanism, ensuring that endorsements still attempt to encompass a broad range of user preferences. Furthermore, in multi-objective optimization, it's often not feasible to enhance all objectives simultaneously, as illustrated by the Pareto front concept, indicating inevitable trade-offs among optimized rewards. As shown in Table 2, focusing on a single RM can yield high Preference Relevance scores, but at the cost of performance in other RMs. Practically, prioritizing a particular metric as more important allows for the adjustment of reward weights to reflect this preference.\n\nWe hope these responses adequately address your concerns and demonstrate our commitment to rigorously validating and improving our work. We look forward to any further feedback you may have."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102955127,
                "cdate": 1700102955127,
                "tmdate": 1700102955127,
                "mdate": 1700102955127,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7W3hm0oQ0G",
            "forum": "fQxLgR9gx7",
            "replyto": "fQxLgR9gx7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3163/Reviewer_VcLv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3163/Reviewer_VcLv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework called P4LM that generates personalized narratives of an item. It would be useful to be equipped by a conversational RS to enhance user experience.\n\nThe model incorporates the (user and item) embedding spaces of a recommender system, and use RLAIF (RL from AI feedback, the dataset they used to finetune LM or train reward model is generated from prompting a PaLM-L) to fine-tune a language model with reward models including precision, appeal, personalization, and preference relevance.\n\nThe method is evaluated on the MovieLens dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Generating personalized narrative is an interesting and useful problem, and seems unexplored in the literature before.\n2. The proposed RLAIF-based framework is general, and could be applied to other recommendation datasets."
                },
                "weaknesses": {
                    "value": "1. There is no human evaluation on comparison among P4LM, P4LM-S, SFT, SFT-Text, PaLM-L. How do we know that P4LM is actually better than others in terms of real human feedbacks? Also, PaLM-L\u2019s samples are not provided in appendix.\n2. The authors only experiment on one dataset. I understand the complexity of the whole procedure, but it this paper would be much stronger if the proposed method could be validated another dataset.\n3. I didn't search very carefully, but the author didn't compare the proposed method with any baselines from other papers. Or if the problem is completely new (I am not sure), then this would not be an issue."
                },
                "questions": {
                    "value": "1. What recommender system is used for extracting the embeddings? How does recomender system performance affect this task?\n2. How does P4LM and SFT take user/item embeddings as input? How does SFT take user and item descriptions as input? What is the detailed network design for these parts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763643518,
            "cdate": 1698763643518,
            "tmdate": 1699636263709,
            "mdate": 1699636263709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VbJSMVIJAd",
                "forum": "fQxLgR9gx7",
                "replyto": "7W3hm0oQ0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "__[Key Items]__\n* [Human evaluation] Please check our common response. In short, we are conducting extensive human evaluations, and we will include the results as soon as the results become available.\n* [Amazon product dataset] Please check our common response. In short, we are training our models with the Amazon dataset. We plan to share the results as soon as they are available.\n\n__[Our Detailed Responses]__\n\nThank you for your insightful review and pertinent questions. Below, we address each of your points and questions in detail.\n\n__> Human evaluation__\n\nWe acknowledge the importance of human evaluation in assessing the effectiveness of P4LM compared to other models. To this end, we are conducting comprehensive human evaluations comparing P4LM, P4LM-S, SFT, SFT-Text, and PaLM-L. These evaluations will provide crucial insights into real human feedback on our model's performance. We will update the paper with these results as soon as they are available. For more details on this process, please refer to our common response.\n\n__> Another dataset__\n\nWe agree with your assessment regarding the need for validating our method on more than one dataset. To address this, we are currently training P4LM on the Amazon product dataset, which will significantly broaden our evaluation scope. The results from this expanded dataset will be included in our paper as soon as they are available, strengthening our findings.\n\n__> Additional baselines__\n\nTo the best of our knowledge, our work is the first to tackle the specific problem of generating endorsement texts with defined characteristics in a recommendation setting. Therefore, direct comparisons with existing baselines in other papers may not be straightforward. However, we are open to including additional relevant baselines if suggested.\n\n__[Question 1]__\n\nIn our study, we employed matrix factorization-based collaborative filtering to generate user-and-item behavioral embeddings whose dot product predicts the rating of the recommendation. While our problem assumes the embedding vectors are provided as data input and P4LM is general enough to take different forms of embedding vectors as input, we acknowledge that more informative latent embeddings from a more advanced recommender system could potentially enhance the quality of generated endorsement texts. Improving the quality of the behavioral embedding space is left for future work.\n\n__[Question 2]__\n\nRegarding the detailed network design of P4LM and SFT, please see Appendix B.1 of our paper for details. In a nutshell, we employed Adapter layers to transform the user/item behavioral (CF) embeddings into a latent space compatible with the language model's architecture. This technique allows the language model to effectively integrate and utilize user and item embedding information to generate endorsements.\n\nWe hope this response addresses your concerns and questions. We are committed to enhancing our work based on your feedback and look forward to any further insights you may have."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102951109,
                "cdate": 1700102951109,
                "tmdate": 1700102951109,
                "mdate": 1700102951109,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Grf3HWefm",
            "forum": "fQxLgR9gx7",
            "replyto": "fQxLgR9gx7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3163/Reviewer_wWN5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3163/Reviewer_wWN5"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the problem of language modeling for personalized recommendation, which aims to generate natural language responses that explain the relevance of recommended items to users\u2019 preferences. The paper proposes a novel approach called P4LM, which leverages reinforcement learning with AI feedback to fine-tune a pre-trained language model with four reward models that measure precision, appeal, personalization, and preference relevance of the generated responses. The paper demonstrates the effectiveness of P4LM on a conversational movie recommendation task, using the MovieLens 25M dataset. The paper shows that P4LM can generate factual, personalized, and compelling movie endorsements that better align with users\u2019 preferences and item features."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: The paper proposes a novel approach to language modeling for personalized recommendation, which leverages reinforcement learning with AI feedback to fine-tune a pre-trained language model with four reward models that measure personalization, precision, appeal, and preference relevance. \nQuality: The paper is well-written and organized, with clear problem formulation, methodology, and evaluation. The paper provides sufficient background and related work on recommender systems, language models, and reinforcement learning. \nSignificance: The paper addresses a challenging and important problem of generating factual, personalized, and compelling recommendation endorsements that better explain item characteristics and their relevance to user preferences. The paper also has practical implications for enhancing the user experience and satisfaction in conversational recommender systems."
                },
                "weaknesses": {
                    "value": "1) The paper does not explain why existing methods are insufficient or what are the specific challenges and opportunities in this domain.\n2) The paper provides a comprehensive literature review of related work, especially on conversational recommender systems, language models, and reinforcement learning. But it only cites those papers without discussing their strengths and limitations or comparing them with the proposed approach.\n3) Paper does not discuss the assumptions and limitations of the approach.\n4) The paper does not describe the implementation details and hyperparameters of the proposed approach, such as the size of the models, and the reinforcement learning algorithms. For example: \"where \u03b71, \u03b72, \u03b73, \u03b74 \u2265 0 are importance weights for the component rewards, and are treated as hyper-parameter\"\n5) The evaluation is not rigorous for an applied paper. One dataset is not enough to draw conclusions.\n6) The paper does not present any qualitative analysis or examples of the generated recommendation texts by the proposed approach. It only shows quantitative scores based on model-based metrics, which may not reflect the true quality and diversity of the texts.\n7) What are the advantages and disadvantages of using adapter layers to augment the language model?\n\n8) How does the P4LM model deal with cold-start problems, where there is not enough user or item information available?\n9) How does the P4LM model compare with other conversational recommender systems that use different language models or architectures?\n10) Can authors talk more about  trade-offs between different reward models, such as precision, appeal, personalization, and preference relevance"
                },
                "questions": {
                    "value": "1) The paper does not explain why existing methods are insufficient or what are the specific challenges and opportunities in this domain.\n2) The paper provides a comprehensive literature review of related work, especially on conversational recommender systems, language models, and reinforcement learning. But it only cites those papers without discussing their strengths and limitations or comparing them with the proposed approach.\n3) Paper does not discuss the assumptions and limitations of the approach.\n4) The paper does not describe the implementation details and hyperparameters of the proposed approach, such as the size of the models, and the reinforcement learning algorithms. For example: \"where \u03b71, \u03b72, \u03b73, \u03b74 \u2265 0 are importance weights for the component rewards, and are treated as hyper-parameter\"\n5) The evaluation is not rigorous for an applied paper. One dataset is not enough to draw conclusions.\n6) The paper does not present any qualitative analysis or examples of the generated recommendation texts by the proposed approach. It only shows quantitative scores based on model-based metrics, which may not reflect the true quality and diversity of the texts.\n7) What are the advantages and disadvantages of using adapter layers to augment the language model?\n\n8) How does the P4LM model deal with cold-start problems, where there is not enough user or item information available?\n9) How does the P4LM model compare with other conversational recommender systems that use different language models or architectures?\n10) Can authors talk more about  trade-offs between different reward models, such as precision, appeal, personalization, and preference relevance"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699037639921,
            "cdate": 1699037639921,
            "tmdate": 1699636263653,
            "mdate": 1699636263653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ygICcpkPP",
                "forum": "fQxLgR9gx7",
                "replyto": "1Grf3HWefm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "__[Key Items]__\n* [Evaluation] Please see the common response regarding human evaluations and the experiments using the Amazon dataset.\n* [Rewriting] More paper revisions are underway as we are incorporating new results and analysis. Please stay tuned!\n\n__[Our Detailed Responses]__\n\nThank you for your thorough review and insightful questions. Your feedback is instrumental in helping us refine our work. We address each of your points and questions as follows:\n\n1. To the best of our knowledge, we are the first to address the problem of generating personalized recommendation endorsement texts, which has significant implications as more online recommenders adopt LLMs to interact with users. The key challenges we address include the absence of datasets for personalized endorsements and the standardized evaluation metrics to quantify the quality of endorsements. Our contributions are: identifying key characteristics that contribute to good endorsements, demonstrating quantification methods for these characteristics, using these metrics to evaluate and RL-finetune LLMs, and substantiating our findings with human evaluations.\n\n2. We acknowledge that the related work section could be more comprehensive in comparing our approach with existing literature. We will revise this section to include qualitative comparisons and better position our work in the context of existing research.\n3. We will include a discussion of the assumptions (behavioral embeddings of user representations in recommender systems are given) and limitations (to specific items endorsements) of our approach in the revised version of the paper.\n4. We will update Appendix B to provide detailed information about the hyperparameters. As for the reinforcement learning algorithm used in our study, we have discussed it in length in the Appendix. We kindly ask the reviewer to check Appendix D.\n5. We agree that evaluating our approach on a single dataset is limiting. As mentioned in our common response, we are extending our experiments to the Amazon product dataset.\n6. Generated texts and their qualitative analyses are included in Appendix A of our paper. Additionally, Table 3 presents human evaluation results for the model trained with a single reward. We are conducting further human evaluations to compare P4LM against baselines more rigorously.\n7. We appreciate your question about the use of adapter layers. These layers are a practical choice that allows us to avoid the introduction of excessive trainable parameters. With the adapter layers, we can incorporate exogenous information such as behavioral embeddings from RS. A disadvantage is that it is less intuitive how one should train the new parameters together with the standard Transformer parameters. We have discussed this in Section 5; please check the paragraph titled \u201cwarm-start training for adapter-augmented LMs\u201d.\n8. P4LM can utilize available user or item information to generate relevant endorsement texts. For new items, as long as their descriptions are provided (as embeddings or as texts), P4LM can create appealing endorsements. For new users, as long as the behavioral embedding is given, P4LM can still generalize to produce personalized endorsements, otherwise the model focuses less on personalization.\n9. Our work does not address the item recommendation aspect of conversational recommender systems. Instead, we focus on enhancing LLMs for generating endorsement texts of particular items. Our primary comparison is with PaLM2 baselines to assess improvements in Precision, Appeal, Personalization, and Preference Relevance.\n10. Your question about the trade-offs between different reward models is pertinent. The full scope of these trade-offs can only be understood through extensive human evaluation studies. Future research could explore how to balance these weights to achieve desired endorsement outcomes with varying properties.\n\nWe hope these responses satisfactorily address your concerns and questions. We are committed to making all necessary improvements based on your feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102946882,
                "cdate": 1700102946882,
                "tmdate": 1700102946882,
                "mdate": 1700102946882,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZLp2JHGyXp",
            "forum": "fQxLgR9gx7",
            "replyto": "fQxLgR9gx7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3163/Reviewer_6DoX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3163/Reviewer_6DoX"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes that individuals should pay greater attention to the attributes of compelling, precision, personalization, and preference relevance when engaging in the recommendation process. Furthermore, the article introduces the P4LM model as a means to achieve this objective. To this end, the authors have meticulously designed reward functions for each attribute and utilized Reinforcement Learning with Adaptive Importance Sampling (RLAIF) to fine-tune the PALM model. Experimental evaluations were conducted on a subset of the MovieLens dataset and validated the ability of their method to improve the model performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe proposed need for attention towards the compelling, precise, personalized, and preference-relevant directions for recommendation presented in this paper holds significant research significance.\n2.\tThe paper provides a comprehensive description of the methodology, experimental details, and the utilization of prompts."
                },
                "weaknesses": {
                    "value": "1.\tAlthough the paper incorporates a plethora of metrics, the efficacy of these indicators in truly measuring the corresponding performance needs to be substantiated (see Questions for more details).\n2.\tThe baseline used by the author is too concise. It is a consensus in the LLM field that models that have undergone reinforcement learning have better rewards than SFT and pre-trained models. More recommendation-related baselines should be mentioned.\n3.\tThe article only collected data and trained on a closed-source model. The comparison of training on open-source models should be discussed.\n4.\tThe author mentioned multiple ways to use PaLM to construct synthetic data in the article, but the rationality of this method has not been verified. (see Questions for more details)"
                },
                "questions": {
                    "value": "1.\tAccording to Table 3, we can see that three of the four indicators proposed by the author (Precision, Personalization and Appeal) do not change significantly in different settings. On the other hand, compared with Table 2, the ordering between different settings cannot be maintained. Consistent, does this indicate that the metrics proposed by the author cannot measure the corresponding generated features? Besides, the Pref. Relevance of all models is very high. Does this mean that this indicator does not have discrimination?\n2.\tAs I understand it, the author's model can generate recommended items and corresponding reasons. Can the author explain the advantages and disadvantages of recommended items compared to traditional recommendation models? I believe that comparable recommendation performance would be an acceptable outcome.\n3.\tThe author mentions in the text the utilization of PaLM to construct data for training the reward models of Personalization and Preference Relevance. One question arises: despite the strong capabilities of LLM, there seems to be no conclusive evidence to suggest their proficiency in accomplishing this task effectively. Can the author provide corresponding evidence through real-world data or human evaluation?\n4.\tDue to the wealth of global knowledge and generalization capabilities possessed by LLM, could P4LM, after undergoing direct training in the movie domain, be applicable to other domains?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699109165752,
            "cdate": 1699109165752,
            "tmdate": 1699636263572,
            "mdate": 1699636263572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0LApuEWXzy",
                "forum": "fQxLgR9gx7",
                "replyto": "ZLp2JHGyXp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "__[Key Items]__\n* [Evaluation] Please see the common response regarding human evaluations and the experiments using the Amazon dataset.\n* [Comparison & Contributions] Please refer to 5 of the common responses. \n\n__[Our Detailed Responses]__\n\nThank you for your review and the insightful questions raised. While we appreciate your concerns, we would like to clarify some misunderstandings about our work. Your input is crucial for us to refine our paper and accurately present our research.\n\n__[W1]__  We acknowledge that our evaluation can be further substantiated, and to address this, we are expanding our experiments to include the Amazon product dataset and conducting comprehensive human evaluations. \n\n__[W2]__ Our work uniquely addresses the generation of factual and personalized recommendation endorsement texts, which is distinct from the traditional item selection focus in recommendation literature. Our primary objective is novel, which is to enhance the quality of recommendation endorsements of specific items, not to recommend items to users. We are open to including comparisons with any relevant baselines that you suggest. \n\n__[W3]__ We agree that an extensive study involving various LLMs, both open-sourced and closed-source, would be valuable. However, our current focus is on demonstrating the effectiveness of our approach using the PaLM2 family of LLMs. Our aim is to establish the importance of our identified pillars for recommendation endorsements (such as Precision, Appeal, etc.) and to demonstrate improvements over baseline methods. We are supplementing our model-based evaluations with human evaluations and additional dataset analysis (currently in progress) to provide more comprehensive evidence of our approach's effectiveness.\n\n__[W4]__ The absence of datasets containing personalized recommendation endorsement texts presented a significant technical challenge. We leveraged recent advances in RLAIF to overcome this issue, using PaLM to generate synthetic data for training our reward models.\n\n__[Q1]__ Please refer to the last paragraph of Section 5 in our paper, where we discuss the differences in model-based and human evaluation results. We highlight that using multiple reward functions can mitigate issues of 'reward hacking'. The upcoming human evaluation results will further elucidate the relationship between model-based metrics and human perception of quality.\n\n__[Q2]__ We would like to reiterate that our work does not address the item selection problem in recommender systems. Our goal is to train an LLM to generate compelling, factual, personalized, and relevant endorsement texts for a given item and user.\n\n__[Q3]__ In response to your suggestion, we are conducting human evaluations and utilizing the Amazon product dataset to train P4LM.\n\n__[Q4]__ Your question about the cross-domain applicability of P4LM is intriguing. While there may be cases where preferences in one domain inform another, the ideal approach would be to train P4LM on a diverse set of domains to enhance its capability in generating personalized endorsement texts.\n\nWe hope these responses clarify the aspects of our work and address your concerns. We are committed to enhancing the clarity and robustness of our research based on your feedback."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102939245,
                "cdate": 1700102939245,
                "tmdate": 1700102939245,
                "mdate": 1700102939245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]