[
    {
        "title": "Meaning Representations from Trajectories in Autoregressive Models"
    },
    {
        "review": {
            "id": "F7PxfrJy4p",
            "forum": "UyGWafcopT",
            "replyto": "UyGWafcopT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1472/Reviewer_xcZs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1472/Reviewer_xcZs"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a \"meaning representation\" of a text based on inverse perplexity for the continuation sequence of the text.\nThe authors define a semantic distance between two prompts using their meaning representations.\nThis is useful for capturing the similarity between texts and for testing hyponym relationships.\nAdditionally, it can be applied to multimodal autoregressive models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-written, featuring concrete formulations and structured experiments.\nThe authors introduce a novel and powerful semantic distance measurement that captures sentence similarity and hyponym relationships. Furthermore, they demonstrate that this distance can be computed more efficiently than initially concerned, alleviating worries about sampling a lot of trajectories."
                },
                "weaknesses": {
                    "value": "1) The authors define a 'syntactic meaning representation' as the function $M_s$. I'm unclear as to why it is named 'syntactic meaning representation' (I'm also unsure why 'syntactic' is included). It is simply the conditional probability of the prompt. I don't find it to be a useful 'representation' for training or other applications like vector-space representation. In fact, they merely use the divergence between the conditional probabilities to measure text similarity. If this approach hasn't been taken by others, then this divergence could be considered novel. Thus, I cautiously suggest they use 'inverse perplexity mapping' rather than 'meaning representation'. The paper's title could then be 'A New Measurement for Sentence Similarity via Sampling Trajectories in Autoregressive Models'. Furthermore, 'semantic representations' appear suddenly following Algorithm 1. I recommend they use 'inverse perplexity mapping for substrings' instead of 'semantic representations'.\n\n2) I'm unsure what is meant by 'meaning containment'. Why use 'containment', which typically refers to 'the action of keeping something harmful under control or within limits'? Please provide a definition or explanation, along with some references.\n\n3) For the final version, sharing the code on GitHub would be beneficial for readers."
                },
                "questions": {
                    "value": "1) The similarity distance depends on the performance of the autoregressive model. If the model is fine-tuned, e.g., for a chatbot, then how would the similarity change?\n\n2) In the first sentence of the second paragraph under **Meaning representation for prompts**, I think $M_u$ should be changed to $M_s$ and $t\\in \\mathcal{A}^1$ should be changed to $t\\in \\mathcal{A}^*$.\n\n3) In the second sentence of the paragraph **Containments of semantic representations**, the definition of 'partial order' should be revised. Since $\\sum_{\\mathrm{len}(t) = m} M_u(t)^{m} = \\sum_{\\mathrm{len}(t) = m} M_v (t)^{m} = 1$, we cannot have $M_u(t) < M_v(t)$ for all $t\\in \\mathcal{A}^*$. Please update the definition of 'partial order.'\n\n4) What is the temperature $\\lambda$ in the experiment? Is it the temperature parameter used during the inference process in the autoregressive model? Please clarify this.\n\n5) I believe we should use the training set for the model to compute $\\overline{M}_u$. Does the text corpus WikiText reflect the distribution of the training set?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Reviewer_xcZs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1472/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634161200,
            "cdate": 1698634161200,
            "tmdate": 1699636076440,
            "mdate": 1699636076440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LvbDkoHInZ",
                "forum": "UyGWafcopT",
                "replyto": "F7PxfrJy4p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1472/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1472/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xcZs [Part 1 of 2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful feedback and suggestions. We address each concern in detail below.\n\n>**The authors define a 'syntactic meaning representation' as the function $M_s$. I'm unclear as to why it is named 'syntactic meaning representation' (I'm also unsure why 'syntactic' is included).**\n\nWe included \u201csyntactic\u201d to stress that our definition of meaning representation is completely determined by syntactic/formal properties of natural language (_i.e._, what strings can extend other strings), as opposed to being defined in terms of external referents. It is also consistent with standard definitions in the theory of formal languages (https://en.wikipedia.org/wiki/Syntactic_monoid) which capture the spirit of our perspective. \n\n>**It is simply the conditional probability of the prompt**\n\nSince we define this more generally to be the score distribution over all possible continuations of the given prompt, the score attribution mechanism is not required to be the conditional probability distribution over these trajectories. In our experiments, we use inverse perplexity, which is also not an actual probability (since does not sum to 1). \n\nThe reason we choose this more general definition is to allow composition operations in this space of meanings that might not be obtainable from any individual prompt. One example, which we use when measuring asymmetric relations, is $M_u \\land M_v$. The resulting distribution will in general not be the \u201cconditional probability\u201d of any particular prompt, yet resides in the same geometric space which allows computation of distances such as $d(M_u \\land M_v, M_v)$.\n\n>**I don't find it to be a useful 'representation' for training or other applications like vector-space representation.**\n\nOur representations attain strong results on downstream tasks. In contrast, previous methods for extracting representations from autoregressive models are less effective, as shown in our experiments.\n\nIn general, there is no formal difference between the expressive power of our representations and vector-space representations obtained from traditional embedding models like CLIP: any task that can be performed using CLIP embeddings can also be accomplished with ours (albeit with greater computational cost). Indeed, any method that measures semantic similarity between arbitrary pairs of input texts actually provides an implicit semantic representation (which could potentially be made explicit via multidimensional scaling methods). It can also be used for classification tasks directly using kernel methods. Furthermore, as argued above, our representation is also naturally interpretable, since coordinates correspond to trajectories.\n\nLastly, our experiments on the SICK-R dataset in the revised draft also show that our representations handle compositional knowledge even better than vector-space representations from contrastive trained models. \n\n>**In fact, they merely use the divergence between the conditional probabilities to measure text similarity. If this approach hasn't been taken by others, then this divergence could be considered novel.**\n\nDivergence between conditional probabilities of next-token predictions is a natural baseline that is commonly used in existing fine-tuning or prompt-based methods. In contrast, we compute the distance between score distributions over trajectories as a similarity measure, which has never been adopted by any prior work to the best of our knowledge.\n\n>**The paper's title could then be 'A New Measurement for Sentence Similarity via Sampling Trajectories in Autoregressive Models'.**\n\nWe appreciate the reviewer\u2019s suggestion, but we believe our current title is a more faithful representation of our work for the abovementioned, and the following reasons:\n\n1. We propose a working definition of meaning representation for autoregressive models. Quantifying semantic similarity is just one out of the applications enabled by our proposed definition. We demonstrate its versatility by applying it to tasks involving asymmetric relations, such as quantifying entailment between sentences and hypernym/hyponym relations between words.\n\n2. The applicability of our method extends beyond sentence comparison, as it was successfully used with a broader range of autoregressive models, including vision-language models. \n\n3. We do not compare sampled trajectories, but rather compare distributions over trajectories. This distinction is crucial, particularly when comparing meaning representations of words, which we define as the distribution of scores over contexts in which the words appear. Hence even though such contexts cannot be sampled directly, our proposed meaning representation remains well-defined and applicable, allowing for alternative methods of obtaining trajectories such as context retrieval from a corpus."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1472/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029101054,
                "cdate": 1700029101054,
                "tmdate": 1700029101054,
                "mdate": 1700029101054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0vvVn8br04",
                "forum": "UyGWafcopT",
                "replyto": "NXkVXd98fv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1472/Reviewer_xcZs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1472/Reviewer_xcZs"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for your clarification! I'll maintain my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1472/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632097535,
                "cdate": 1700632097535,
                "tmdate": 1700632097535,
                "mdate": 1700632097535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IT3rsdpVVy",
            "forum": "UyGWafcopT",
            "replyto": "UyGWafcopT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1472/Reviewer_7D5R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1472/Reviewer_7D5R"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes an alternative sentence meaning representation (specifically for sentence similarity) for zero-shot use with auto-regressive LMs.   The method is to sample N possible following texts for two sentences A and B, and score each of those 2N following text for its probability of following A and the probability of following B, and then compare difference between the probabilities. They explore the ability to modify the notion of similarity by augmenting those sentences with prompts, and evaluate on zero-shot Semeval STS tasks, a task of comparing captions to CLIP outputs, and (using a modified approach to focus on single-word characterization) a lexical semantics task (WordNet hypernym modeling)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The work seems to outperform other methods for autoregressive representation of sentences on the sentence similarity tasks studied, indicating its potential for continued relevance and utility. \n- The ability to modify similarity using prompting is very clever. \n- On a theoretical level, thinking about sentence meaning in terms of this theoretical notion of a trajectory of meanings is a great framing."
                },
                "weaknesses": {
                    "value": "- It has a relatively rigid and narrow use case, since this method can only be used for pairwise comparison and since it's not obvious how to fine-tune it.  \n- The work frames it as producing \"interpretable\" vectors, but the work was somewhat lacking in an actual exploration of that interpretability. \n- I liked the idea of the entailment and hypernymy work, but it felt a bit convoluted: the way they approached both tasks seems to have lead to them comparing to weak baselines, despite NLI and wordnet link detection being well-explored areas."
                },
                "questions": {
                    "value": "- As mentioned above: do the authors feel that this would be viable to fine-tune, or is it limited entirely to zero-shot STS settings (plus entailment/hypernymy)? \n- Wouldn't the general hypernymy assumptions often be violated under negation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Reviewer_7D5R"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1472/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728292228,
            "cdate": 1698728292228,
            "tmdate": 1699636076330,
            "mdate": 1699636076330,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qtmrH4oTPe",
                "forum": "UyGWafcopT",
                "replyto": "IT3rsdpVVy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1472/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1472/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7D5R [Part 1 of 2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable comments and suggestions. We respond to each comment below.\n\n>**It has a relatively rigid and narrow use case, since this method can only be used for pairwise comparison** \n\nOur representations can actually be used in much the same way as traditional embedding models such as CLIP or BERT (albeit with a greater computational cost), for example for tasks like retrieval or classification. Moreover, our representation has the additional benefit of interpretability, since unlike most embedding models, the \u201ccoordinates\u201d used have meaningful interpretations as text continuations (trajectories).\n\n>**it's not obvious how to fine-tune it.**\n\nWe note that our measure of similarity can be directly used with existing contrastive learning-based fine-tuning methods. Typical contrastive learning methods are limited to aligning next/mean-token predictions, which we show in our experiments are actually an inadequate representation of meaning. \n\nBased on this, we believe that contrastive fine-tuning with our measure of pairwise similarity could be used to better align the meaning spaces of autoregressive models with humans. Indeed, while contrastive fine-tuning methods typically sacrifice the language modeling capabilities of the LLM by converting it into an embedding model, we hypothesize that fine-tuning using our method can preserve generation abilities given appropriate regularization.\n\nWhile fine-tuning models for better alignment with humans are beyond the scope of our paper, we appreciate the reviewer\u2019s suggestion and agree that it represents a valuable avenue for future research.\n\n>**The work frames it as producing \"interpretable\" vectors, but the work was somewhat lacking in an actual exploration of that interpretability.**\n\nUnlike traditional vector-based embeddings, where the interpretation of individual vector components is elusive, each component of our method's representations corresponds to a specific text continuation, or trajectory. This is illustrated in Figure 1 of our paper, in which each bar corresponds to the score attributed to a distinct trajectory. This provides a way to interpret the difference in meaning between sentences based on the trajectories that differentiate them. For example, in Figure 1, we observe that the sentences \"A man is playing a flute\" and \"A dog is barking at a fly\" differ because \"The dog's owner is telling him to stop\" is a relevant continuation for the latter but not for the former.\n\n>**I liked the idea of the entailment and hypernymy work, but it felt a bit convoluted**\n\nOur proposed evaluation methods serve as a proxy to assess the effectiveness of our entailment and hypernym/hyponym definitions. The original definitions are more general, providing a quantitative measure of the strength of these relationships between any two inputs. However, the absence of human-labeled datasets quantifying the degree of entailment between sentences or the hyponym relation between words necessitates an alternative approach, which our evaluation methods aim to address.\n\n>**the way they approached both tasks seems to have lead to them comparing to weak baselines, despite NLI and wordnet link detection being well-explored areas.**\n\nDespite the well-established nature of these tasks, they are rarely evaluated on pre-trained autoregressive models in a zero-shot setting due to the poor performance of existing methods, as rightly pointed out by the reviewer. Our findings corroborate this observation, demonstrating that existing baselines fail even for binary prediction of these relations under such settings. Our method effectively tackles this challenge.\n\n>**As mentioned above: do the authors feel that this would be viable to fine-tune, or is it limited entirely to zero-shot STS settings (plus entailment/hypernymy)?**\n\nFine-tuning is definitely viable for the goal of aligning the meaning space of models with that of humans, and an idea for doing so using our method was described above. We would love to see future work exploring this exciting direction.\n\nIt is, however, beyond the scope of this paper, since fine-tuning disrupts the meaning representation attributed by the original model, which was the subject of investigation of our work.\n\nWe also present additional experiments on SICK-R in the revised draft, showing that even in the zero-shot setting, our representations effectively handle compositional knowledge, achieving even better performance than contrastive trained models."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1472/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028538079,
                "cdate": 1700028538079,
                "tmdate": 1700028538079,
                "mdate": 1700028538079,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U4KyvV2le0",
                "forum": "UyGWafcopT",
                "replyto": "qtmrH4oTPe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1472/Reviewer_7D5R"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1472/Reviewer_7D5R"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the extensive reply! I'll maintain my score, but I was on the fence, and would have raised my score slightly to a 7 if that was an option.  Some concerns were addressed, but I still have worries both about the computationally intensive/pairwise nature of the work, and I am still not convinced that their assertion that \"Our representations can actually be used in much the same way as traditional embedding models such as CLIP or BERT\" actually holds true in practice."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1472/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690837957,
                "cdate": 1700690837957,
                "tmdate": 1700690837957,
                "mdate": 1700690837957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ohHc7V1j2R",
            "forum": "UyGWafcopT",
            "replyto": "UyGWafcopT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1472/Reviewer_ecJy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1472/Reviewer_ecJy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method of sentence or phrase similarity measures using Decoder-only language models.  Decoder-only language models generate a continual string of tokens given an input. The proposed method measures two given sentences as individual inputs to a decoder-only model and measures the distributional similarity between the probabilities of multiple possible continual strings, named trajectories, for both of the inputs.\nWhile it cannot catch up with the recent contrastive learning-based sentence similarity models, it outperforms most off-the-shelf encoder-based sentence representation models.\nAlthough there are limitations especially high computational costs, the proposed method is interesting and unique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is overall well-written. I had no difficulty in reading and understanding this paper.\n- This paper presents an interesting and unique usage of decoder-only language models for measuring sentence similarity.\n- The proposed method shows better performance on sentence similarity tasks over encoder-based baselines."
                },
                "weaknesses": {
                    "value": "- For the baseline encoder-only models, it is better to include larger models like BERT-large and RoBERTa-large using their CLS tokens and token averages.\n- The discussion about partial ordering between sentences is a bit puzzling. Since Tu and Tv are samples from u and v respectively, the former set of trajectories usually gives high Mu values for u and vise versa, so it hardly happens that Mu < Mv or Mu > Mv for all t in Tu U Tv. Besides, the discussion of entailment suddenly shifts from the comparison between Mu and Mv to the comparison between d(Mu cup Mv, Mu) and d(Mu cup Mv, Mv). This part is also quite puzzling.\n- For the experiments of entailment and hypernym/hyponym evaluation, there is an assumption that either of those relations exists between the given input pairs, which is not realistic.\n- As is pointed out in the limitation paragraph, the proposed model is computationally higher than other baseline models. In the Appendix, it is tested using a fixed set of trajectories, which seems to cause a big performance degradation."
                },
                "questions": {
                    "value": "- For the Hyponym test, the contexts for a given word u are not sampled but retrieved from. It is not clear how the value Mu(s u t) is obtained.\n- Is it guaranteed that the set of trajectories for an input u always the same? Or, does it vary each time sampling for a sentence u is conducted?\n- The Autoregressive Model Baselines look very weak. I am not sure they are worth being included."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1472/Reviewer_ecJy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1472/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759976553,
            "cdate": 1698759976553,
            "tmdate": 1700659189410,
            "mdate": 1700659189410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uZYatlCa4A",
                "forum": "UyGWafcopT",
                "replyto": "ohHc7V1j2R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1472/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1472/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ecJy"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and suggestions. We would like to provide a detailed response to each comment.\n\n>**For the baseline encoder-only models, it is better to include larger models like BERT-large and RoBERTa-large using their CLS tokens and token averages.**\n\nThank you for the suggestion, we added the suggested comparisons to Table 1.\n\n>**... hardly happens that Mu < Mv or Mu > Mv for all t in Tu U Tv. Besides, the discussion of entailment suddenly shifts from the comparison between Mu and Mv to the comparison between d(Mu cup Mv, Mu) and d(Mu cup Mv, Mv).**\n\nIndeed $M_u \\leq M_v$ or $M_v \\leq M_u$ rarely happens in practice. This is precisely the reason we employ the alternate formulation pointed out by the reviewer. Note that $M_u \\leq M_v$ can also be expressed as $d(M_u \\land M_v, M_u) = 0$, where $M_u \\land M_v := \\min(M_u, M_v)$. Intuitively, this means that $M_u \\leq M_v$ if and only if $M_u$ is fully captured by their intersection. The difference now is that this expression provides a continuous measure of how strongly the containment relation holds. We added some discussion in the main body of the paper and in Appendix F which we hope can help better clarify this. This also relates to the next comment by the reviewer:\n\n>**For the experiments of entailment and hypernym/hyponym evaluation, there is an assumption that either of those relations exists between the given input pairs, which is not realistic.**\n\nWe chose to assume that a entailment/hypernym relation exists and to formulate the task as prediction of the direction of that relation in order to present a simple quantitative evaluation of our definitions. Indeed, we are not aware of datasets that provide human-labeled annotations quantifying how strongly these relations hold between two arbitrary inputs. In general, our method actually produces a single number that quantifies how strongly each relation holds (even if neither strictly holds), measured by the distance described above.\n\n>**As is pointed out in the limitation paragraph, the proposed model is computationally higher than other baseline models.**\n\nWhile computational cost is indeed a limitation of our method, it can be effectively mitigated by employing fewer and shorter trajectories. As evident in Figure 3 of Appendix A.1, performance starts to saturate with just 10 trajectories of maximum 10 tokens each. \n\n>**In the Appendix, it is tested using a fixed set of trajectories, which seems to cause a big performance degradation.**\n\nSince there exist no effective method of extracting zero-shot representations from autoregressive models, baselines consistently exhibit significantly inferior performance compared to our method. Therefore, despite performance degradation due to using a fixed set of trajectories, we still outperform them substantially while achieving large speed-ups. \n\n>**For the Hyponym test, the contexts for a given word u are not sampled but retrieved from. It is not clear how the value Mu(s u t) is obtained.**\n\nWe elaborated on this in Appendix C.2, Algorithm 2 of the Appendix. In particular compared to sentence-level meaning, there are two main differences: (1) as correctly pointed out, contexts are retrieved rather than sampled, since \u201creverse-sampling\u201d the prefixes for any given word is not trivial, and (2) the scores for each context are computed over the entire context, rather than only over the continuations following the word.\n\n>**Is it guaranteed that the set of trajectories for an input u always the same? Or, does it vary each time sampling for a sentence u is conducted?**\n\nThis depends on the sampling method. Beam search generates a fixed set of trajectories, but multinomial sampling, which we use in our experiments, will generate different trajectories based on the random seed. However, we experimentally found that this variance in trajectories sampled across different seeds barely affects results.\n\n>**The Autoregressive Model Baselines look very weak. I am not sure they are worth being included.**\n\nIt is not immediately obvious that those baselines do not perform well in a zero-shot setting. Most of the baseline methods are also used as components by other works such as SGPT and Sentence-T5, in both the zero-shot and fine-tuned settings. As such, even though they perform poorly, we believe that a comparison with those baselines is helpful for completeness and to be consistent with relevant prior work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1472/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028102807,
                "cdate": 1700028102807,
                "tmdate": 1700028102807,
                "mdate": 1700028102807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Et3ZXtUoh",
                "forum": "UyGWafcopT",
                "replyto": "ohHc7V1j2R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1472/Reviewer_ecJy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1472/Reviewer_ecJy"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your clarification responses"
                    },
                    "comment": {
                        "value": "As most of my concerns and questions were clearly answered, I decided to upgrade my rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1472/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659728156,
                "cdate": 1700659728156,
                "tmdate": 1700659728156,
                "mdate": 1700659728156,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "okY2McAvq0",
            "forum": "UyGWafcopT",
            "replyto": "UyGWafcopT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1472/Reviewer_Lfxp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1472/Reviewer_Lfxp"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an innovative approach to interpret and represent the meaning of text in autoregressive LLMs through the concept of trajectories -- distributions of potential text continuations. This method diverges from traditional vector space embeddings, offering a a semantic interpretation that aligns with the actual use and context of language as understood by LLMs. It effectively overcomes the challenges posed by other methods, such as prompt dependency and the need for fine-tuning, providing a more faithful reflection of the model's internal representations without additional data or model modifications.\n\nEmpirical results demonstrate that this trajectory-based approach can successfully capture complex linguistic relationships and perform competitively on semantic textual similarity tasks without any fine-tuning or prompts. Furthermore, the paper extends this approach to multimodal models, where it outperforms established benchmarks like CLIP embeddings in understanding semantic similarities across images. The main contributions of the study include a new interpretable semantic representation for autoregressive models, the alignment of these representations with human linguistic understanding, and the applicability of the method to multimodal contexts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper's trajectory-based method for understanding LLMs is original, diverging from typical vector space models and prompt-based approaches, and introducing a new angle to distributional semantics.\n\n2. The approach has been empirically tested against established benchmarks, indicating robust methodology and results that surpass existing techniques like CLIP embeddings on image-image similarity tasks.\n\n3. The paper is clearly articulated, systematically presenting the new method and its implications, with illustrative examples that enhance comprehension of the proposed concepts.\n\n4. This work is significant for its practical application in making LLMs more interpretable without extra training, it is an approach that is original as far as I know."
                },
                "weaknesses": {
                    "value": "My main criticism of the paper is in the results for semantic similarity, they are far below those of contrastive methods (like 10 pts or so). I also the Sentence-T5 results are a bit misleading, that is the case without any fine-tuning. This isn't explicitly stated. There are other approaches that achieve far higher results on these tasks that do not use the training data for these tasks. I think the Sentence-T5 results in this paper are actually worse than a random encoder where random word embeddings are average together.\n\nThere are also a few typos:\n\nSection 3: \u201cSpeicifically\u201d\nAppendix E: \u201cpresenedt\u201d"
                },
                "questions": {
                    "value": "How does this model perform relative to more interpretable baselines - like random word embeddings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1472/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698949122213,
            "cdate": 1698949122213,
            "tmdate": 1699636076189,
            "mdate": 1699636076189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XsJCIoqkm8",
                "forum": "UyGWafcopT",
                "replyto": "okY2McAvq0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1472/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1472/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Lfxp"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback and suggestions. We respond to each comment below.\n\n>**My main criticism of the paper is in the results for semantic similarity, they are far below those of contrastive methods (like 10 pts or so)**\n\nAll of our results are obtained in the prompt-free setting,  however, if desired, state-of-the-art results among zero-shot methods with autoregressive models can be readily obtained by combining our method with prompt-engineering (see Table 7). On compositional knowledge benchmarks such as SICK-R (Table 1) and the multi-modality CxC benchmark (Table 2), the performance of our method even surpasses that of contrastive methods. We refer the reviewer to our overall comments for further discussion.\n\n> **I also (think) the Sentence-T5 results are a bit misleading, that is the case without any fine-tuning. This isn't explicitly stated.**\n\nFor fair comparison, we evaluate all methods in a zero-shot manner as mentioned in Table 1. As suggested, we made this more explicit in Table 1 (Encoder-based Models -> Zero-shot Encoder-based Models), thank you.\n\n> **There are other approaches that achieve far higher results on these tasks that do not use the training data for these tasks. I think the Sentence-T5 results in this paper are actually worse than a random encoder where random word embeddings are average together.**\n\n> **How does this model perform relative to more interpretable baselines - like random word embeddings?**\n\nWith regard to random-encoder results, we believe the reviewer is referring to [1], but the embeddings considered in that work for STS tasks are actually evaluated via logistic regression (with trained parameters) rather than in a zero-shot manner. As such, the results are not directly comparable with our work nor with the cited prior works (e.g., Sentence-T5 does not use logistic regression, but instead directly computes cosine similarity).  \n\nWe welcome any suggestions for additional comparisons or references that the reviewer believes we might have overlooked.\n\n[1] No Training Required: Exploring Random Encoders for Sentence Classification, ICLR 2019.\n\n> **Section 3: \u201cSpeicifically\u201d Appendix E: \u201cpresenedt\u201d**\n\nCorrected, thank you!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1472/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700027685462,
                "cdate": 1700027685462,
                "tmdate": 1700027720003,
                "mdate": 1700027720003,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]