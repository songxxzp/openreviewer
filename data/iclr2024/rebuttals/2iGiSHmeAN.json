[
    {
        "title": "BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics"
    },
    {
        "review": {
            "id": "Ok0DP6NnD6",
            "forum": "2iGiSHmeAN",
            "replyto": "2iGiSHmeAN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7162/Reviewer_Va4f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7162/Reviewer_Va4f"
            ],
            "content": {
                "summary": {
                    "value": "## Summary\n\nThe paper proposes a Graph Neural Network (GNN)-based method for simulating the over-damped limit of Langevin dynamics, which reduces to Brownian dynamics when no acceleration is present. Contributions include the use of additional MLPs for predicting random diffusion and decoding edge latent as interacting forces to enforce linear momentum conservation. However, the paper has several significant shortcomings, including a lack of motivation, limited benchmarking, and insufficient definition and references. Due to these issues, I recommend rejecting the paper.\n\n## Detailed Comments\n\n### 1. Lack of Motivation and Benefits\n\nThe paper does not provide adequate motivation for replacing traditional simulation techniques for the over-damped Langevin system with GNNs. Given that the dataset is generated from simulators that can easily model such systems, the authors need to justify the utility of their approach. This could be in the form of improved simulation speed or to-reality accuracy, although the latter would require validation beyond simulated data.\n\n### 2. Lack of Definitions and References\n\n- In Figure 1, the term \"ohe(type)\" is used without definition or context, making it unclear to the reader.\n- The \"squareplus\" activation function is mentioned but not cited.\n\n### 3. Limited Benchmarking\n\nThe paper restricts its experiments to simple spring systems, providing only a narrow validation of its methodology. Prior work in this domain typically includes experiments on 3-4 different datasets to establish the method's applicability.\n\n## Conclusion\n\nWhile the paper introduces a GNN-based method for simulating over-damped Langevin dynamics, it suffers from multiple critical flaws, including a lack of clear motivation, insufficient references, and limited benchmarking. These drawbacks severely compromise the paper's value and applicability. Therefore, I recommend rejecting this submission.\n\nThe authors should consider submitting to a workshop on this specific topic or, including more datasets to show the method's capability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Small contributions like: decoding edge as force so linear momentum is conserved."
                },
                "weaknesses": {
                    "value": "See above."
                },
                "questions": {
                    "value": "How hard is it to simulate Brownian motions? If not hard, w.r.t. simulation time or numerical or modeling challenges, there is no point in replacing it with NN?\n\nIf otherwise, please show the challenges in your draft."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Reviewer_Va4f",
                        "ICLR.cc/2024/Conference/Submission7162/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698355585082,
            "cdate": 1698355585082,
            "tmdate": 1700624481874,
            "mdate": 1700624481874,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dKhusdX8wc",
                "forum": "2iGiSHmeAN",
                "replyto": "Ok0DP6NnD6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Va4f"
                    },
                    "comment": {
                        "value": "*Q1. Lack of Motivation and Benefits: The paper does not provide adequate motivation for replacing traditional simulation techniques for the over-damped Langevin system with GNNs. Given that the dataset is generated from simulators that can easily model such systems, the authors need to justify the utility of their approach. This could be in the form of improved simulation speed or to-reality accuracy, although the latter would require validation beyond simulated data.*\n\n**Response**: There is a large family of works based on machine learning approaches in general and graph neural networks in particular that aims at learning the dynamics of physical systems. Such approaches can be used for system identification, that is, to learn the quantities such as noise, interparticle forces and other features of the system directly from the trajectory. Thus, these approaches can be used for \"discovering\" interaction laws and dynamics and also to infer them in a realistic manner for future timesteps. \n\nTo clarify this point, we have now added a new paragraph in the introduction with additional references.\n\n*Q2. Lack of Definitions and References In Figure 1, the term \"ohe(type)\" is used without definition or context, making it unclear to the reader.*\n\n**Response**: \"ohe\" refers to one-hot encoding, which is a standard abbreviation used in the field. We have now clarified this in the figure caption.\n\n*Q3. The \"squareplus\" activation function is mentioned but not cited.*\n\n**Response**: We have now cited the squareplus activation function.\nBarron, J.T., 2021. Squareplus: A softplus-like algebraic rectifier. arXiv preprint arXiv:2112.11687.\n\n*Q4. Limited Benchmarking: The paper restricts its experiments to simple spring systems, providing only a narrow validation of its methodology. Prior work in this domain typically includes experiments on 3-4 different datasets to establish the method's applicability.*\n\n**Response:** To the best of the authors' knowledge, this is the first attempt to learn Brownian dynamics directly from the trajectory in a physics-informed fashion. Note that, in contrast to deterministic systems, Brownian systems, are significantly more challenging to simulate. Being one of the first works, we have focused on fairly simple systems, namely, linear and non-linearly spring and binary spring systems. However, we have very critically analysed the performance of the models with several new baselines including MIBDGNN that has been now added to the manuscript. We aim to extend this to more complex systems and experimental data on colloidal gels, for example, as part of future work. This is now included in the limitations and future work. \n\n*Q5. Conclusion: While the paper introduces a GNN-based method for simulating over-damped Langevin dynamics, it suffers from multiple critical flaws, including a lack of clear motivation, insufficient references, and limited benchmarking. These drawbacks severely compromise the paper's value and applicability. Therefore, I recommend rejecting this submission.*\n\n**Response**: We have now added 13 additional references, a new paragraph on related work that further demonstrates the motivation, and added a new baseline model MIBDGNN that is evaluated on all the datasets. I hope the reviewer now finds the manuscript acceptable for publication. If there are any additional concerns, we request the reviewer to raise those.\n\n*Q6. Questions: How hard is it to simulate Brownian motions? If not hard, w.r.t. simulation time or numerical or modeling challenges, there is no point in replacing it with NN? If otherwise, please show the challenges in your draft.*\n\n**Response**: As outlined earlier, the main goal of the work is to learn the dynamics of Brownian systems directly from the trajectory. This is a challenging problem and more difficult than deterministic systems due to the inherent noise in the data. We demonstrate one of the first approaches to learn Brownian dynamics directly from the trajectory data of the systems. Traditional approaches relies on trial and error method to match the trajectory and may still be limited in terms of the functional forms of interactions. Replacing this traditional approach with neural network, enables the learning of any complex interactions directly from the trajectory data. This has now been clearly outlined in the manuscript.\n\n**Appeal to the reviewer:** With the additional experiments, results, and explanations, we now hope the reviewer finds the manuscript suitable for publication. Accordingly, we request you to raise the score for the manuscript. Please do let us know if there are any further queries."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388802744,
                "cdate": 1700388802744,
                "tmdate": 1700388802744,
                "mdate": 1700388802744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q3O5NIaHbi",
                "forum": "2iGiSHmeAN",
                "replyto": "Ok0DP6NnD6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Awaiting feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer Va4f,\n\nWe thank you for taking the time to provide constructive comments, which have significantly improved the quality of the manuscript. Since we are in the last two days of the author-reviewer discussion period, we hope to engage in a discussion and improve the paper to the best extent possible. Specifically, the major changes made in response to the comments by the Reviewer are outlined below.\n\n1. Updated the definitions in Figure 1.\n2. Included a paragraph on motivation and related works.\n3. We have added several additional references, hyperparametric optimization. \n4. Finally, we have now added a new baseline, namely, MIBDGNN. The results of this model are included for all datasets and systems.\n\nWith these additional experiments and improved explanations, we hope we have addressed all the concerns raised by the reviewer. If there are any outstanding concerns, we request the reviewer to please raise those. Otherwise, we would really appreciate it if the reviewer could increase the score.\n\nLooking forward to your response.\n\nThank you,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480745119,
                "cdate": 1700480745119,
                "tmdate": 1700480745119,
                "mdate": 1700480745119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IMteYiFvVA",
                "forum": "2iGiSHmeAN",
                "replyto": "8Rh210qZrI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Reviewer_Va4f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Reviewer_Va4f"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reply"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nAfter checking your revised version. I believe it's in much better shape hence would increase the score to 6.\n\nBest"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624457125,
                "cdate": 1700624457125,
                "tmdate": 1700624457125,
                "mdate": 1700624457125,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iFh4HSB9q0",
            "forum": "2iGiSHmeAN",
            "replyto": "2iGiSHmeAN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7162/Reviewer_cdjq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7162/Reviewer_cdjq"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed Brownian graph neural networks (BROGNET) which is a new framework combining stochastic differential equations and graph neural networks to learn Brownian motion dynamics directly from trajectories. The architecture ensures linear momentum conservation, leading to improved learning of dynamics. Several baselines were proposed for comparison due to the limited existing benchmarks. The BROGNET's distinctive momentum conservation feature made it significantly superior to all other baselines. It also demonstrated the ability to generalize to much larger system sizes and different temperatures than those seen during training."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. I do like the design of predicting interacting forces rather than total forces on each node which naturally conserves total momentum conservation. Such \u201chard constraint\u201d (or physics-based inductive biases in the paper) not only rigorously adheres to physical principles but also significantly enhances the model's performance, compared with more straight forward methods such as adding regularizers to penalize the physics violation. I suggest adding a few sentences in the first paragraph to explicit distinguish the \u201chard constraints\u201d vs \u201csoft constraints\u201d.\n\nThere are a few additional papers in this trajectory worth mentioning. The natural constraint design in this paper is more less like ref [A], as the constraint is pure summation. While it can be generalized as special cases of Ref[B],[C],[D] as well.\n\n[A] A machine learning-aided global diagnostic and comparative tool to assess effect of quarantine control in COVID-19 spread, Patterns, 2020.\n\n[B] ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. ICML 2023.\n\n[C] Learning Physical Models that Can Respect Conservation Laws. ICML 2023.\n\n[D] Unravelling the performance of physics-informed graph neural networks for dynamical systems. NeurIPS, 2022. (this is already cited.)\n\n2. The model can generalize to unfamiliar system sizes and temperatures with zero-shot learning.\n\n3. The outperforms existing baselines in various tasks."
                },
                "weaknesses": {
                    "value": "1.\tMy major concern lies on the comparison baselines. Firstly, the prior work baselines (BFGN, BNequIP) do not seem very strong to me. However, I\u2019m not an expert in partical-based systems and I\u2019m not expecting each accepted paper will include comparisons with all the popular models. More importantly, to show the benefits of the physics-informed inductive bias, it is worth comparing the other model with the same backbone structure + training with regularization on the physics violation. I appreciate the ablation study with BDGNN which essentially shares the same backbone model. But adding a comparison experiment by training BDGNN with regularizing on momentum conservation will better prove the power of inductive bias. \n\n2.\tRegarding the sde integrator, I\u2019m a bit concerned about the gradient stability due to the stochastic integral. Did you meet any issues when the random distribution of noise term leading to large error during back-propagation? Or do you need some sampling method to get the approximation of the drift term along with the variance of the stochastic term?"
                },
                "questions": {
                    "value": "1.\tTypos: 2nd line under equation 7: ativation \n\n2.\tIs there any specific reason to use squareplus as activation function? I understand the comparison experiments with ReLU in appendix H which shows similar performance, but why squareplus is chosen at firsthand as it\u2019s less popular?\n\n3.\tEquation 10 is questionable. \\Delta \\Omega should have the magnitude of \\Delta t, but the following sentence mentioned \u201cis a random number sampled from a standard Normal distribution\u201d. Do you miss a magnitude of \\Delta t?\n\n4.\tWhat does || || mean in equation 5,6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Reviewer_cdjq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675158972,
            "cdate": 1698675158972,
            "tmdate": 1700410022324,
            "mdate": 1700410022324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JSK62rNIw0",
                "forum": "2iGiSHmeAN",
                "replyto": "iFh4HSB9q0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cdjq: part 1"
                    },
                    "comment": {
                        "value": "*Q1. I do like the design of predicting interacting forces rather than total forces on each node which naturally conserves total momentum conservation. Such \u201chard constraint\u201d (or physics-based inductive biases in the paper) not only rigorously adheres to physical principles but also significantly enhances the model's performance, compared with more straight forward methods such as adding regularizers to penalize the physics violation. I suggest adding a few sentences in the first paragraph to explicit distinguish the \u201chard constraints\u201d vs \u201csoft constraints\u201d.*\n\n**Response**: We have now added new text in the introduction. Moreover, we have now added a new model where \"soft constraints\" are used, namely, MIBDGNN, in addition to BroGNet.\n\n*Q2. There are a few additional papers in this trajectory worth mentioning. The natural constraint design in this paper is more less like ref [A], as the constraint is pure summation. While it can be generalized as special cases of Ref[B],[C],[D] as well.\n[A] A machine learning-aided global diagnostic and comparative tool to assess effect of quarantine control in COVID-19 spread, Patterns, 2020.\n[B] ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. ICML 2023.\n[C] Learning Physical Models that Can Respect Conservation Laws. ICML 2023.\n[D] Unravelling the performance of physics-informed graph neural networks for dynamical systems. NeurIPS, 2022. (This is already cited.)*\n\n**Response:** Thank you for pointing out this literature. We have now cited and discussed these papers in the introduction, first paragraph.\n\n\n*Q3. My major concern lies on the comparison baselines. Firstly, the prior work baselines (BFGN, BNequIP) do not seem very strong to me. However, I\u2019m not an expert in particle-based systems and I\u2019m not expecting each accepted paper will include comparisons with all the popular models. More importantly, to show the benefits of the physics-informed inductive bias, it is worth comparing the other model with the same backbone structure + training with regularization on the physics violation. I appreciate the ablation study with BDGNN which essentially shares the same backbone model. But adding a comparison experiment by training BDGNN with regularizing on momentum conservation will better prove the power of inductive bias.*\n\n**Response:** We thank the reviewer for the valuable insight. To test the hypothesis, we have now added a new baseline, namely, momentum-informed BDGNN (MIBDGNN). Interestingly, we observe that MIBDGNN occasionally outperforms BroGNet in predicting the dynamics. However, BroGNet outperforms MIBDGNN in momentum conservation as in the former case momentum conservation is enforced as a hard constraint, whereas in the latter case, it is a soft constraint. We have now included MIBDGNN for all the systems and included it in all the plots and results and discussion. \n\nWe believe this has now significantly improved the quality of the manuscript. We thank the reviewer for the critical thinking and invaluable feedback. \n\n*Q4. Regarding the sde integrator, I\u2019m a bit concerned about the gradient stability due to the stochastic integral. Did you meet any issues when the random distribution of noise term leading to large error during back-propagation? Or do you need some sampling method to get the approximation of the drift term along with the variance of the stochastic term?*\n\n**Response:** We thank the reviewer for raising this point. Indeed, we faced issues while training the model. However, effective hyperparametric optimization of the weights associated with the first and the second terms of the loss functions enabled stable training.\n\n*Q5. Typos: 2nd line under equation 7: activation*\n\n**Response:** Thank you for pointing it out. We have corrected them in the revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388462834,
                "cdate": 1700388462834,
                "tmdate": 1700388462834,
                "mdate": 1700388462834,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mKRRfLWN4Y",
                "forum": "2iGiSHmeAN",
                "replyto": "iFh4HSB9q0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Reviewer_cdjq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Reviewer_cdjq"
                ],
                "content": {
                    "title": {
                        "value": "review reply"
                    },
                    "comment": {
                        "value": "I thank the authors for the rebuttal. \n\nThe added MIBDGNN experiment makes the comparison much stronger, at least from a research methodology perspective. This step is critical to prove the \"hard constraints\" outperform the regularizers in terms of conserving the quantities. And I appreciate the authors being candid with potentially better performance in the coordinates/KL divergence error with the soft constraints. This is expected because the structure's hard constraints with less model capacity can compromise the optimization. In fact, when people from specific backgrounds (particle simulation) use these ml-based models, they care more about physics constraint metrics than the coordinates/KL divergence error, because the former serves as a first-hand check of the credibility of the model for these users.\n\nThe four papers I suggested can be categorized into \"NN structures enforcing linear constraints of output\", which is exactly what this paper aims to do (summation of total force equals 0). Are you citing them in the updated appendix? I didn't see it in the main paper though. I feel like it's even worth opening a small paragraph to discuss this branch of methods as this paper also falls into.\n\nMy other questions are well answered. I checked the updated manuscript and can see the quality improvement. I will raise my score to 8."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410003484,
                "cdate": 1700410003484,
                "tmdate": 1700410064900,
                "mdate": 1700410064900,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PAxJWMXgxP",
            "forum": "2iGiSHmeAN",
            "replyto": "2iGiSHmeAN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7162/Reviewer_VaBU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7162/Reviewer_VaBU"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces BroGNet, a GNN that is momentum conservative designed based on SDEs for Brownian dynamics learning.\nThe authors provide a very thorough introduction and related work section to motivate the paper and to provide the reader with sufficient background. Then, the method is presented, followed by several experiments in different scenarios. The proposed method seems to significantly outperforms existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is mostly easy to follow and read.\n\nThe authors provide a very good background section to explain different terms, such that even non expert readers can understand the paper.\n\nThe experimental section looks promising under various settings.\n\nThe authors provide good explanations about the baselines and the experiment details."
                },
                "weaknesses": {
                    "value": "Missing literature about GNNs: while this paper is concerned with learning brownian dynamics from data, there is a complementary topic in GNNs and that is the design of GNN architectures inspired by ODEs. I believe that the authors should add a discussion to the related work section to clarify the difference between the two. Some references are provided in [1-5].\n\nMissing literature about Neuro ODEs: please see [6,7].\n\nIt is not clear why the authors propose to use the square plus activation. Is there a specific reason? (besides the experimental result provided in the appendix)\n\nReading the appendix, I understand that the authors used only one message passing layer in their implementation. Can you please elaborate on this point? What would the performance be like when adding more layers?\n\nRefereces:\n\n[1] Graph Neural Ordinary Differential Equations  \n\n[2] GRAND: Graph Neural Diffusion \n\n[3] PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations\n\n[4] Anti-Symmetric DGN: a stable architecture for Deep Graph Networks  \n\n[5] Graph-Coupled Oscillator Networks\n\n[6] Stable Architectures for Deep Neural Networks\n\n[7] Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations"
                },
                "questions": {
                    "value": "Regarding the dynamic graph used here, how different is the proposed procedure than [8] ?\n\nRegarding equation (8), this seems a bit like a discretized version of an advection operator (see [9,10]) for example. Can the authors expand on this point and clarify the differences?\n\n[8] Dynamic Graph CNN for Learning on Point Clouds\n\n[9] ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks\n\n[10] Advective Diffusion Transformers for Topological Generalization in Graph Learning"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Reviewer_VaBU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765883803,
            "cdate": 1698765883803,
            "tmdate": 1700403863456,
            "mdate": 1700403863456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YlFicNDLnU",
                "forum": "2iGiSHmeAN",
                "replyto": "PAxJWMXgxP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VaBU"
                    },
                    "comment": {
                        "value": "*Q1. Missing literature about GNNs: while this paper is concerned with learning brownian dynamics from data, there is a complementary topic in GNNs and that is the design of GNN architectures inspired by ODEs. I believe that the authors should add a discussion to the related work section to clarify the difference between the two. Some references are provided in [1-5].*\n\n**Response:** Thank you for raising this point. We have now added an additional paragraph in the introduction related to GNNs for modeling physical systems and included several additional references.\n\n*Q2. Missing literature about Neuro ODEs: please see[6,7]*\n\n**Response:** We have now added a new paragraph in the introduction and briefly discussed graph neural ODEs, and other such physics-informed GNNs. Additional references are included.\n\n*Q3. It is not clear why the authors propose to use the square plus activation. Is there a specific reason? (besides the experimental result provided in the appendix)*\n\n**Response:** In order to evaluate the role of activation function, we have performed extensive comparisons with SquarePlus, ReLU, LeakyReLU and Sigmoid in Appendix H. From the results, we observe that the results from different activation functions are comparable. We used SquarePlus in order to potentially extend BrogNet to learn energy instead of force in future work. In this case, the activation function needs to be double differentiation. To clarify, following text were added:\n> Here, we evaluate the performance of BrogNet with respect to different activation functions: SquarePlus, ReLU, LeakyReLU and Sigmoid. From Fig. K, we observe that all the activation functions give comparable performance. We used SquarePlus in order to potentially extend BrogNet to learn energy instead of force in future work. In this case, the activation function needs to be double differentiation.\n\n\n*Q4. Reading the appendix, I understand that the authors used only one message passing layer in their implementation. Can you please elaborate on this point? What would the performance be like when adding more layers?*\n\n**Response:** We have now added extensive hyperparametric optimization and included this as Appendix K 'Performance w.r.t. hyperparameters'. Moreover, the following text is added to the Appendix K.\n\n> PERFORMANCE W.R.T. HYPERPARAMETERS\nHere, we evaluate the performance of BroGNet w.r.t. number of layers of message passing [1, 2, 3] and the number of hidden layer neurons [5, 8, 16, 32]. In the paper for BroGNet, we choose the number of layers of message passing = 1 and the number of hidden layer neurons = 5. For other architectural details, check Appendix E\n\n\n*Q5. Regarding the dynamic graph used here, how different is the proposed procedure than [8]?*\n\n**Response:** Note that the approach presented in Ref. [8] is one of the earlier graph representations to represent any point cloud using a node and edge embedding. The present work focusses on learning the dynamics of Brownian systems. In this case, the GNN is used along with the physics-based equations to learn the dynamics. Note that different graph architectures can be used for the GNN. In the present work, we show the effect of different SOTA architectures including full graph network (BFGN), equivariant GNNs (BNequIP), and our own graph architecture, BroGNet. One of the major novelty of our architecture is its inherent ability to conserve momentum due to the directional edges. \n\nAn additional paragraph is now added in the introduction on GNNs and physics-informed GNNS. \n\n*Q6. Regarding equation (8), this seems a bit like a discretized version of an advection operator (see [9,10]). Can the authors expand on this point and clarify the differences?*\n\n**Response:** We believe the reviewer is referring to Eq.(2) and Eq.(10). Indeed, Eq.(2) refers to a generic Brownian dynamics equations and Eq.(10) represents the numerical integration of this equation using Euler-Maruyama integrator. This is similar to Eq.(1) of the reference [9], which also represents the stochastic differential equation. We have added all the references below to the main manuscript.\n\n**References:**\n[1] Graph Neural Ordinary Differential Equations\n[2] GRAND: Graph Neural Diffusion\n[3] PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations\n[4] Anti-Symmetric DGN: a stable architecture for Deep Graph Networks\n[5] Graph-Coupled Oscillator Networks\n[6] Stable Architectures for Deep Neural Networks\n[7] Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations\n[8] Dynamic Graph CNN for Learning on Point Clouds\n[9] ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks\n[10] Advective Diffusion Transformers for Topological Generalization in Graph Learning"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387658433,
                "cdate": 1700387658433,
                "tmdate": 1700387658433,
                "mdate": 1700387658433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4BIQMBaBxx",
                "forum": "2iGiSHmeAN",
                "replyto": "YlFicNDLnU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Reviewer_VaBU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Reviewer_VaBU"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer response"
                    },
                    "comment": {
                        "value": "I thank the reviewers for the detailed response to my review and to other reviews. I am happy to continue supporting my score of weak acceptance and raise the confidence from 3 to 4, given the clarifications the authors made."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403837050,
                "cdate": 1700403837050,
                "tmdate": 1700403837050,
                "mdate": 1700403837050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xQK324d9Ji",
            "forum": "2iGiSHmeAN",
            "replyto": "2iGiSHmeAN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7162/Reviewer_KKvb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7162/Reviewer_KKvb"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce an innovative framework, Brownian graph neural networks (BROGNET), that integrates stochastic differential equations and GNNs to directly learn Brownian dynamics from trajectories. Their method enforces the conservation of linear momentum within the system, leading to empirically observed improved performance in learning dynamics. The authors showcase the effectiveness of BROGNET by applying it to various benchmarked Brownian systems. They also demonstrate its ability to generalize to simulate previously unseen system sizes and temperatures"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main idea of the paper is interesting. It is well-written and the proposed method seems to be novel."
                },
                "weaknesses": {
                    "value": "Some parts are unclear and require further explanations. There are questions and vague points that need addressing:\n\n1. How does the suggested framework manage noisy or incomplete trajectory data, and is it capable of accurately learning the underlying  dynamics in such cases? \n\n2. How does the choice of activation function affect the performance of the MLPs? Were other activation functions considered, and if so, how did they compare to the chosen function?\n\n3. Can the MLP be replaced with other types of neural networks, such as convolutional neural networks or recurrent neural networks? How would this affect the performance of the proposed framework?\n\n4. Can you provide more details on the scalability of the proposed framework? How does the computational complexity scale with the number of particles, and how does this affect its applicability to large-scale systems?\n\n5. How were the hyperparameters chosen, and how does the choice of hyperparameters affect the performance of the proposed framework?\n\n6. How the choice of graph topology affects the performance of the proposed framework? Were other graph topologies considered, and if so, how did they compare to the chosen topology?\n\n7. How does the proposed method handle systems with external fields or other sources of non-deterministic forces?\n\n8. Can you provide more details on the benchmarked Brownian systems used to evaluate BROGNET's performance? How do these systems compare to real-world applications?\n\nI am happy to increase my score if the authors could address my concerns."
                },
                "questions": {
                    "value": "Please see above!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7162/Reviewer_KKvb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7162/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699241037564,
            "cdate": 1699241037564,
            "tmdate": 1700587634231,
            "mdate": 1700587634231,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6Mnn7hwfP5",
                "forum": "2iGiSHmeAN",
                "replyto": "xQK324d9Ji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KKvb"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments. Please find the point-by-point response below to each of the comments.\n\n*Q1. How does the suggested framework manage noisy or incomplete trajectory data, and is it capable of accurately learning the underlying dynamics in such cases?*\n\n**Response:** We thank the reviewer for raising this relevant point. In this work, we are learning Brownian dynamics which inherently consists of a drift (deterministic) term governed by interparticle forces and diffusion (stochastic) term governed a white noise. We draw the attention to the following points.\n1. Learning from noisy data: In BrogNet, we are learning the Brownian dynamics from noisy data. Specifically, the ground truth consists of a Gaussian noise defined by 0 mean and a given standard deviation. We show that the BrogNet, after training, learns the standard deviation\n2. Learning from one step: BrogNet employs a physics-informed approach where the input is the state of the system at time $t$ and the output is the state of the system at time $t + \\Delta t$. This dynamics is learned in a statistical fashion using the Gaussian negative loglikelihood loss function. Thus, BrogNet only needs pair of steps and not a full trajectory. \n\nAltogether, BrogNet can learn from noisy and incomplete trajectory data.\n\nIn order to demonstrate this further, we performed an additional experiment evaluating all the models with different standard deviations (see App. I, \u2018Generalizability to unseen $\\gamma$\u2019). The following new text is added to the App. I along with a new Fig. N.\n> Here, we evaluate BroGNet performance with respect to different $\\gamma$ values: 0.1, 0.6, 0.8, 1.0, 1.2, 1.4, and 10. As $\\gamma$ increases performance improves. This is due to the fact that for a higher value of $\\gamma$, there will be a smaller standard deviation for the stochastic term.\n\n*Q2. How does the choice of activation function affect the performance of the MLPs? Were other activation functions considered, and if so, how did they compare to the chosen function?*\n\n**Response:** In order to evaluate the role of activation function, we have performed extensive comparisons with SquarePlus, ReLU, LeakyReLU and Sigmoid in Appendix H. From the results, we observe that the results from different activation functions are comparable. We used SquarePlus in order to potentially extend BrogNet to learn energy instead of force in future work. In this case, the activation function needs to be double differentiation. To clarify, following text were added:\n> Here, we evaluate the performance of BrogNet with respect to different activation functions: SquarePlus, ReLU, LeakyReLU and Sigmoid. From Fig. K, we observe that all the activation functions give comparable performance. We used SquarePlus in order to potentially extend BrogNet to learn energy instead of force in future work. In this case, the activation function needs to be double differentiation.\n\n*Q3. Can the MLP be replaced with other types of neural networks, such as convolutional neural networks or recurrent neural networks? How would this affect the performance of the proposed framework?*\n\n**Response:** We thank the reviewer for this comment. In the present work, we use a graph neural network to model the physical system. This is important as we need to capture the topology of the system. Moreover, the baselines chosen in the work, namely, NN and BNN uses an MLP instead of a GNN. We observe that these models give poor performance in comparison to BrogNet and other GNN versions. This confirms that the topology of the structure is important. Moreover, another important property that GNNs exhibit is permutation invariance. CNNs and RNNs do not exhibit this property and hence cannot be effectively used in the present scenario.\n\n*Q4. Can you provide more details on the scalability of the proposed framework? How does the computational complexity scale with the number of particles, and how does this affect its applicability to large-scale systems?*\n\n**Response:** Scalability is an important aspect in modeling such systems as realistic systems may have a large number of degrees of freedom. To demonstrate this, we show the performance of models trained on 5-particle systems on 50, 500, and 5000 particle systems. To show the computational time with the number of particles a Fig. K and following text were added in Appendix F:\n\n> Further, Fig. K shows the Inference time w.r.t number of particles for BFGN, BDGNN, MIBDGNN, BROGNET and BNEQUIP"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387464117,
                "cdate": 1700387464117,
                "tmdate": 1700387464117,
                "mdate": 1700387464117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pEkbbtDqZF",
                "forum": "2iGiSHmeAN",
                "replyto": "xQK324d9Ji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Awaiting feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer Va4f,\n\nWe thank you for taking the time to provide constructive comments, which have significantly improved the quality of the manuscript. Since we are in the last two days of the author-reviewer discussion period, we hope to engage in a discussion and improve the paper to the best extent possible. Specifically, the major changes made in response to the comments by the Reviewer are outlined below.\n\n1. Generalizability to **unseen $\\gamma$**.\n2. Additional evaluation on the **effect of activation functions**.\n3. Additional studies on **hyperparametric optimization**. \n4. **Scalability** (in terms of inference time) to system sizes **three orders of magnitude** larger than the training system.\n5. Additional experiments on **graph topology**.\n\nFinally, we have now added **a new baseline, namely, MIBDGNN**. The results of this model are included for all datasets and systems.\n\nWith these additional experiments and improved explanations, we hope we have addressed all the concerns raised by the reviewer. If there are any outstanding concerns, we request the reviewer to please raise those. Otherwise, we would really appreciate it if the reviewer could increase the score.\n\nLooking forward to your response.\n\nThank you,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481000711,
                "cdate": 1700481000711,
                "tmdate": 1700481000711,
                "mdate": 1700481000711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sbkbmPF5vd",
                "forum": "2iGiSHmeAN",
                "replyto": "NMb6wRfepV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7162/Reviewer_KKvb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7162/Reviewer_KKvb"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "I greatly appreciate the authors for their detailed response and clarifications! My concerns have been addressed, and I will increase my score to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7162/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587532259,
                "cdate": 1700587532259,
                "tmdate": 1700587532259,
                "mdate": 1700587532259,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]