[
    {
        "title": "RLIF: Interactive Imitation Learning as Reinforcement Learning"
    },
    {
        "review": {
            "id": "Iu7c5UjGKs",
            "forum": "oLLZhbBSOU",
            "replyto": "oLLZhbBSOU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5065/Reviewer_48gS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5065/Reviewer_48gS"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a methodology for learning a policy given human intervention. Unlike other imitation learning frameworks such as Dagger, the method does not directly clone the actions taken by the expert, but rather uses the expert\u2019s decision to intervene as a negative reward signal. The paper claims that by doing so, the learned policy has the potential to exceed the performance of a suboptimal expert, and does not need to assume the expert is optimal. The method is demonstrated experimentally in simulation to outperform imitation learning baselines, and is demonstrated to work in a real world experiment. Theoretical analysis is provided to justify the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The key strengths of the paper are as follows:\n- The method is simple and easy to implement, thereby making it useful.\n- The method does not require access to a task reward signal and makes only the benign assumption that an expert would intervene if the policy were performing badly. This is realistic and gives the method a broad scope.\n- The paper includes a thorough justification of the method, which with some additional clarifications could be compelling (see the questions I have)."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is that I am suspicious of the results due to some confusion. Please answer the questions below, and I would be happy to revise my rating upward if they are satisfactory. [Edit: updated rating since questions were addressed]\n\nAside from the questions, below are a few recommendations for clarity improvement and a few grammatical errors caught. \n\nClarity recommendations:\n- In the intro, add a more intuitive explanation for how RLIF is able to exceed the expert\u2019s performance without access to a task reward signal.\n- In the intro, you describe the theoretical analysis performed, but don't state the bottom line. What does the analysis tell us? \n\nGrammar:\n- \u201c\u2026for selecting when to intervene lead to good performance\u2026\u201d\n- \u201cWe leave the of DAgger analysis under\u2026\u201d"
                },
                "questions": {
                    "value": "- If the expert intervenes for 5 steps, would all 5 transitions be labeled -1? Wouldn\u2019t this result in actions taken by the expert being labeled -1 too? \n- Could the else statement in line 7 of Algorithm 2 set the reward to the RL reward signal if it were available? \n- I\u2019m very confused by this sentence: \u201cthe performance of DAgger-like algorithms will be subject to the suboptimality of the experts, while our method can reach good performance even with suboptimal experts by learning from the expert\u2019s decision of when to intervene.\u201c If no task reward is provided, how could the policy end up outperforming the expert?\n- How do you end up with a 110% expert if the expert level is w.r.t an expert trained to near optimality?\n- It seems unfair that Dagger variants are given 100 episodes to learn from while RLIF gets 1 million. It is also unrealistic to expect that we could have 1 million episodes in any real-world setting. How would RLIF perform if also restricted to a more realistic 10-100 episodes?\n- In experiments, do you warm start the dataset for RLIF? If so, with what reward? And does this pose an unfair advantage to your algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5065/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5065/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5065/Reviewer_48gS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697605516253,
            "cdate": 1697605516253,
            "tmdate": 1700483527313,
            "mdate": 1700483527313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GWqRt4DbQd",
                "forum": "oLLZhbBSOU",
                "replyto": "Iu7c5UjGKs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "Dear Reviewer 48gS,\n\nWe would like to express our gratitude for your insightful feedback and acknowledgment of our work. We have carefully considered your questions and provided clarifications below, which will be incorporated into the paper to enhance clarity. We believe that these modifications and clarifications address the weakness you point out about confusion in the paper. Please let us know if all issues have been addressed, or if any further issues remain, as we would be happy to fix these!\n\n> If the expert intervenes for 5 steps, would all 5 transitions be labeled -1? Wouldn\u2019t this result in actions taken by the expert being labeled -1 too?\n\nExpert actions won\u2019t be labeled as -1 when an expert intervenes for 5 steps, only the state and action pair immediately before the intervention begins is labeled -1, this was originally in algorihtm 2 box. The intuition here is that the -1 reward is assigned to the transition (state-action tuple) that most immediately resulted in the intervention. Our theoretical analysis in Section 6 shows that this leads to a good policy (that approximately maximizes the true reward) in theory, and our empirical evaluation shows this in practice. Expert actions are labeled as 0, similar to non-intervention transitions.\n\n> Could the else statement in line 7 of Algorithm 2 set the reward to the RL reward signal if it were available?\n\nYes, that could be a straightforward extension as you suggested. Though in our paper we focused on learning entirely from intervention feedback without knowledge of the ground truth reward.  It\u2019s reasonable to assume the sample efficiency of RLIF would be further improved if it could also access the true task reward.\n\n> I\u2019m very confused by this sentence: \u201cthe performance of DAgger-like algorithms will be subject to the suboptimality of the experts, while our method can reach good performance even with suboptimal experts by learning from the expert\u2019s decision of when to intervene.\u201c If no task reward is provided, how could the policy end up outperforming the expert?\n\nA crucial assumption for RLIF to work is that the intervention signal itself should contain task-relevant information. For example, in the driving example in the introduction, it is easier for a safety driver to intervene when the car does something bad than it is for them to carry out a perfectly optimal sequence of intervention actions \u2013 the choice of when to intervene itself often carries a lot of information. In Section 5.1, we provide one example of an expert intervention model based on Q-values that would carry considerable information in this choice. we also ablate the performance of RLIF  w.r.t. Q functions containing different level of task information; as you hypothesized, the least task-relevant information Q function has, the worse the performance, as in Appendix A.6.  Of course, there is no free lunch, and our method is sensitive to the particular intervention strategy the human uses, as evidenced by the fact that the naive random intervention baseline in our experiments performs much worse. However, this also indicates that our method is effective at deriving a learning signal from the intervention strategy, and the real-world experiments illustrate that real humans do provide interventions that are informative.\n\n> How do you end up with a 110% expert if the expert level is w.r.t an expert trained to near optimality?\n\nThe statement about a 110% expert level refers to the expert's evaluation normalized score. In D4RL locomotion environments, we additionally learn expert policies from a curated dataset, so it can be possible that learned expert achieve normalized scores above 100.  And we label them as greater than 100% in expert levels. This was also explained Section 5.2 - results and discussions. We\u2019ll add a more detailed explanation in the final paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435021288,
                "cdate": 1700435021288,
                "tmdate": 1700435021288,
                "mdate": 1700435021288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gjmQlxBlAS",
                "forum": "oLLZhbBSOU",
                "replyto": "e1n8zRs9nO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_48gS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_48gS"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments"
                    },
                    "comment": {
                        "value": "Thanks for your clarifications. I've updated my recommendation."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483550879,
                "cdate": 1700483550879,
                "tmdate": 1700483550879,
                "mdate": 1700483550879,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rz8Jz5RHU2",
            "forum": "oLLZhbBSOU",
            "replyto": "oLLZhbBSOU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the problem of learning from suboptimal experts in interactive settings. It proposes to add a negative reward term in reinforcement learning whenever an expert requires being invoked in a RL DAgger setup, and it shows performance gains with different expert accuracy levels compared to supervised learning baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- I appreciate the effort to bring the model to a real-world setting.\nEdit:\n- The paper has a through theoretical justification and a sensible set of simulated baselines, showing the power of the proposed method."
                },
                "weaknesses": {
                    "value": "- The paper only compares with DAgger in supervised learning setups, even though DAgger is often used in RL and should have been included as a baseline in such configuration.\n- The proposed technique seems to reduce to a heuristic to define a reward function.\nEdit:\n- After a very thorough set of responses from the authors, I admit I misinterpreted the actual scope of the paper, so these weaknesses are actually obsolete."
                },
                "questions": {
                    "value": "- What is the advantage of the proposed paper versus using DAgger in RL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5065/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK",
                        "ICLR.cc/2024/Conference/Submission5065/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613146648,
            "cdate": 1698613146648,
            "tmdate": 1700680973073,
            "mdate": 1700680973073,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZZjb0PrITA",
                "forum": "oLLZhbBSOU",
                "replyto": "rz8Jz5RHU2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer guRK,\n\nThank you for your thoughtful feedback, we would like to address your concerns below.\n\n>DAgger in RL setting\n\nWe compare to DAgger under the same assumptions as our method -- namely, for both DAgger and RLIF, no ground truth reward function is provided, and the algorithm performs rollouts from the current policy with interventions provided by an expert. Could you elaborate on what kind of comparison you would like to see here? We are not aware of any prior work that uses DAgger in RL, but it's entirely possible we missed something (or misunderstood your comment), so if you can provide a reference to a relevant method we should have compared with, we can attempt to include such an evaluation.\n\n> RLIF reduced to a heuristic to define a reward function\n\nOur method does not require the expert to define a reward function, but rather to provide interventions. While the way this is used to induce a reward function in RL, that is not a heuristic: we show that this leads to a reward signal that allows our method to recover a policy that is close to the optimal policy (under some assumptions).\n\n> compare with DAgger in RL\n\nWe are not sure what \"using DAgger in RL\" means -- could you provide a reference of a method that does this? The advantage of our method over DAgger is illustrated in the empirical comparisons. For example, Table 1 shows our method is 2-3x better than DAgger."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434728515,
                "cdate": 1700434728515,
                "tmdate": 1700434728515,
                "mdate": 1700434728515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BEViuCeX7G",
                "forum": "oLLZhbBSOU",
                "replyto": "rz8Jz5RHU2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Friendly reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe greatly appreciate your time and dedication to providing us with your valuable feedback. We hope we have addressed the concerns, but if there is anything else that needs clarification or further discussion, please do not hesitate to let us know.\n\nBest,\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541513195,
                "cdate": 1700541513195,
                "tmdate": 1700541513195,
                "mdate": 1700541513195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nsO5UwCXBZ",
                "forum": "oLLZhbBSOU",
                "replyto": "rz8Jz5RHU2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment"
                    },
                    "comment": {
                        "value": "I apologize for the lack of clarity when I wrote my review. I am concerned with the lack of comparison with the common approach of warm starting the policy training with Dagger and proceeding with PPO. The heuristic determination of when to execute the expert is equivalent to defining an arbitrary reward function, so in my opinion that would be a fair comparison. I would like to know the author's opinion on why this is not the case.\n\nEdit: Regardless of this discussion, which I do consider necessary for this paper, I think my rating is unnecessarily hard, so I'll correct it to a fairer score while waiting for a chance for the authors to respond to my question, which I hope is clearer now."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563807753,
                "cdate": 1700563807753,
                "tmdate": 1700564561258,
                "mdate": 1700564561258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3t1ydeIzWW",
                "forum": "oLLZhbBSOU",
                "replyto": "rz8Jz5RHU2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the last minute response"
                    },
                    "comment": {
                        "value": "I wish there had been a chance to further discuss what I consider a reasonable question about the usability of the proposal. It's not clear to me that designing a heuristic to call an expert is essentially easier than defining a reward function, but the limited discussion period (and my late receipt of the authors' answer) made it impossible for me to clarify this point.\n\nIn any case, I thank the authors for considering to include such an experiment in a potential final version and, since my concern doesn't seem to be relevant in the other reviews, I won't hold a strong opinion about the paper, which otherwise is well written and even shows real-world deployment of learned policies."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644422599,
                "cdate": 1700644422599,
                "tmdate": 1700644572342,
                "mdate": 1700644572342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kofBoIuh02",
                "forum": "oLLZhbBSOU",
                "replyto": "rz8Jz5RHU2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "First of all, thank you so much again for your quick response.\n\nEven in the case where the intervention is not heuristic, the reference value function has been learned from data, so a fair comparison might still include using e.g. IRL to learn a reward function and apply e.g. some on-policy algorithm like PPO after a Dagger warm start. I wouldn't want to propose such a costly experiment for the rebuttal, and, at the end of the day, the actual decision of when to call the expert in the proposed method is still a heuristic on top of the learned reference value function, so a heuristic could in principle also be used to define a competent reward function as a comparison, which should easily beat at least the random intervention baseline and would show how far a reasonable but not optimal heuristic reward function is from the one defined with the proposal.\n\nIn any case, as I mentioned above, since this seems to be of concern only to me, I won't argue against the paper based on that. Again, thank you for your responses."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648424636,
                "cdate": 1700648424636,
                "tmdate": 1700648473545,
                "mdate": 1700648473545,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ngr3DqhwdX",
                "forum": "oLLZhbBSOU",
                "replyto": "bWVvhKFcSu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "That was finally clearing my concerns about the paper. Thank you so much for the detailed response."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680787518,
                "cdate": 1700680787518,
                "tmdate": 1700680787518,
                "mdate": 1700680787518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1qD5WU4Pd7",
                "forum": "oLLZhbBSOU",
                "replyto": "amV67C2cug",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Reviewer_guRK"
                ],
                "content": {
                    "title": {
                        "value": "Done"
                    },
                    "comment": {
                        "value": "I did."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681083554,
                "cdate": 1700681083554,
                "tmdate": 1700681083554,
                "mdate": 1700681083554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nn0ZbGfR0G",
            "forum": "oLLZhbBSOU",
            "replyto": "oLLZhbBSOU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5065/Reviewer_RarS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5065/Reviewer_RarS"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel off-policy RL method that uses user intervention signals as rewards, allowing learning beyond the limitations of potentially suboptimal human experts. This approach is less reliant on near-optimal expert input compared to DAgger. The authors provide a unified analytical framework for their method and DAgger, including asymptotic and non-asymptotic evaluations. They validate their method with high-dimensional simulations and real-world robotic tasks, showing its practical effectiveness in surpassing traditional imitation learning methods in complex control scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The motivation behind this paper is natural and valuable.\n\n(2) The theoretical analysis is sufficient.\n\n(3) The authors conduct some experiments on real robot arms, which proves the effectiveness of the algorithm."
                },
                "weaknesses": {
                    "value": "(1) The author should include more baselines in Table 1, where the mentioned methods only contain DAgger. For example, some IRL methods could also be applied in the same settings.\n\n(2) Does this method require high frequencies to intervene? Can you do some ablations for this? If RLIF cannot work under high frequent intervenes, it can be hard to apply in more complicated real-world scenarios."
                },
                "questions": {
                    "value": "I am wondering what the difference is between RLHF and your work RLIF when you choose the Space Mouse to give signals by hand. No related works are mentioned and no experiments are conducted for this. For example, what about using some foundation models to give signals?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5065/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5065/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5065/Reviewer_RarS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810890306,
            "cdate": 1698810890306,
            "tmdate": 1699636496501,
            "mdate": 1699636496501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6HIvKzUz7I",
                "forum": "oLLZhbBSOU",
                "replyto": "Nn0ZbGfR0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer RarS,\n\nThank you for your thoughtful feedback, we would like to address your concerns below.\n\n>Regarding baselines: \n\nour aim is to develop a reinforcement learning method that can operate under the same assumptions as interactive imitation learning (e.g., DAgger), hence we compare to DAgger, HG-DAgger, and BC. While IRL methods could in principle be adapted to this problem setting also, this would be an awkward fit, as to our knowledge IRL methods do not use interventions.\nHowever, we did conduct new experiments using IQL as an offline RL baseline across all simulation benchmarks, and the results are detailed in Table 1.\n\n>Regarding your question about intervention frequencies, \n\nour real-world robot experiments with vision-based peg insertion and deformable object manipulation (in Section 5) already demonstrated the effectiveness of our method under both sparse and dense interventions, i.e., the user intervenes only a few times, but each intervention length is long vs an user intervenes multiple times for a short time during a rollout. In practical terms, the choice between sparse and dense interventions is problem-dependent, varying according to the specific needs of each scenario. \nWe have also included a plot of the average intervention rate over environment steps in Figure 3, illustrating that at the onset of training, the intervention rate tends to be higher, gradually decreasing as the policy improves towards the end of the training process. \n\n>RLIF vs RLHF:\n\nThank you for your suggestion! RLHF indeed can be relevant to our work, we\u2019ll add some related works in the final version.\n\nYour engagement with our work is highly valued, and we welcome any further questions or insights you may have.\n\n\nTable 1: IQL Baseline\n\n|                        | IQL with Random Interventions | IQL with Value Based Interventions |\n|------------------------|------------------------------|------------------------------------|\n| **adroit-pen**         |                              |                                    |\n| Expert Levels          | 68.83                        | 42.5                               |\n| 90% Expert             | 25.92                        | 46.58                              |\n| 40% Expert             | 12.08                        | 40.67                              |\n| 8% Expert              | 35.61                        | 43.25                              |\n|                        | 80.22                        | 20.37                              |\n| **locomotion-walker2d**|                              |                                    |\n| 110 Expert             | 56.33                        | 47.97                              |\n| 70 Expert              | 14.7                         | 47.02                              |\n| 20 Expert              | 50.42                        | 38.45                              |\n|                        | 37.91                        | 26.37                              |\n| **locomotion-hopper**   |                              |                                    |\n| 110 Expert             | 27.88                        | 28.98                              |\n| 40 Expert              | 16.64                        | 25.32                              |\n| 15 Expert              | 27                           | 26.89                              |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434613929,
                "cdate": 1700434613929,
                "tmdate": 1700435116867,
                "mdate": 1700435116867,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FDbJbsymTb",
                "forum": "oLLZhbBSOU",
                "replyto": "Nn0ZbGfR0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Friendly reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe greatly appreciate your time and dedication to providing us with your valuable feedback. We hope we have addressed the concerns, but if there is anything else that needs clarification or further discussion, please do not hesitate to let us know.\n\nBest,\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541504244,
                "cdate": 1700541504244,
                "tmdate": 1700541504244,
                "mdate": 1700541504244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "meAFnB3DxJ",
            "forum": "oLLZhbBSOU",
            "replyto": "oLLZhbBSOU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5065/Reviewer_9AYW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5065/Reviewer_9AYW"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles how to enable sub-optimal human interventions (sub-expert samples) to improve a RL agent. It falls in two sub-domains in the field: interactive imitation learning, and reinforcement learning from human feedback. \n\nThe authors explore the assumption that the decision to intervene provides an effective supervision signal. Accordingly, a solution based on augmentation the reward with intervention penalty, and off-policy learning is used to minimize such a penalty (maximize the accumulative negative reward), so that the RL agent achieve 3X performance gain compared to DAgger, and BC (bahevior cloning) baselines. Ablation studies on sub-optimality of expert samples are also provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Novelty:\n========\nThe assumption of \u201cwhen an sub-optimal expert to intervene\u201d provides useful learning signals is novel to me.\n\nSoundness:\n========\nAccordingly, the authors provide both empirical results in simulation and real-world robotic task, and theoretical justifications by proposing a probabilistic model of \u201cwhen human expert will intervene\u201d."
                },
                "weaknesses": {
                    "value": "- Though the theoretical analysis does not answer the key problem, how the level of sub-optimality results in what sample complexity of the offline RL agent. The reviewer still enjoys reading the analysis framework proposed here. The reviewer may miss some key contents, but would the authors explain more in the proposed Corollary 6.7 regarding the above key problem?\n- Baselines:\nThere is no offline-RL baselines provided. Is it because that the true reward function is unknown in the evaluation results? Otherwise, please add at least 1 offlineRL baseline in all the simulation based experiments."
                },
                "questions": {
                    "value": "Please check the two questions the reviewer proposed in the Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699140724495,
            "cdate": 1699140724495,
            "tmdate": 1699636496417,
            "mdate": 1699636496417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mcBlvHySbV",
                "forum": "oLLZhbBSOU",
                "replyto": "meAFnB3DxJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 9AYW,\n\nThanks for your insightful feedback on our work, we would like to address your concerns below.\n\nTo answer your question regarding the theoretical results:\n\n> how the level of sub-optimality results in what sample complexity of the offline RL agent. The reviewer still enjoys reading the analysis framework proposed here.\n\nCorollary 6.7 provides a sample complexity bound of learning a policy using the intervention reward $\\tilde{r}$. Since the intervention strategy is generated by the expert $\\pi^{\\exp}$, which eventually affects the suboptimality gap in a linear relationship w.r.t. $\\Delta_{ref}$, the visitation distribution gap between the reference policy $\\pi^{ref}$ and the optimal policy $\\pi^{\\star}$.\n\n> Concerning your question about offline RL baselines\n\nFirst, RLIF in principle can be integrated with any off-policy RL algorithm. We chose RLPD because of its good sample efficiency. That said, per your suggestion, we conducted additional experiments incorporating IQL as an offline RL baseline across all simulation benchmarks. Please find the detailed results in Table 1. It's worth noting that, in all our experiments, true rewards were not provided, as our algorithm is designed to learn an optimal policy with only intervention feedback.\n\nUpon examination, the results from the IQL experiments indicate a performance that lags behind the outcomes achieved using the online algorithm RLPD. This observation aligns with our expectations because it\u2019s slow for IQL to explore new actions.\n\nThank you for your continued engagement with our work, and we remain open to any further questions or suggestions you may have.\n\nTable 1: IQL Baseline\n\n|                        | IQL with Random Interventions | IQL with Value Based Interventions |\n|------------------------|------------------------------|------------------------------------|\n| **adroit-pen**         |                              |                                    |\n| Expert Levels          | 68.83                        | 42.5                               |\n| 90% Expert             | 25.92                        | 46.58                              |\n| 40% Expert             | 12.08                        | 40.67                              |\n| 8% Expert              | 35.61                        | 43.25                              |\n|                        | 80.22                        | 20.37                              |\n| **locomotion-walker2d**|                              |                                    |\n| 110 Expert             | 56.33                        | 47.97                              |\n| 70 Expert              | 14.7                         | 47.02                              |\n| 20 Expert              | 50.42                        | 38.45                              |\n|                        | 37.91                        | 26.37                              |\n| **locomotion-hopper**   |                              |                                    |\n| 110 Expert             | 27.88                        | 28.98                              |\n| 40 Expert              | 16.64                        | 25.32                              |\n| 15 Expert              | 27                           | 26.89                              |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434366725,
                "cdate": 1700434366725,
                "tmdate": 1700434366725,
                "mdate": 1700434366725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F1OaDpcRxv",
                "forum": "oLLZhbBSOU",
                "replyto": "meAFnB3DxJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Friendly reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe greatly appreciate your time and dedication to providing us with your valuable feedback. We hope we have addressed the concerns, but if there is anything else that needs clarification or further discussion, please do not hesitate to let us know.\n\nBest,\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541491375,
                "cdate": 1700541491375,
                "tmdate": 1700541491375,
                "mdate": 1700541491375,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]