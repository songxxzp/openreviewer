[
    {
        "title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge"
    },
    {
        "review": {
            "id": "aVzQZLwBh0",
            "forum": "JbOsMrwjZ3",
            "replyto": "JbOsMrwjZ3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
            ],
            "content": {
                "summary": {
                    "value": "## Summary\nAuthors construct a dataset for code generation targeting the bioinformatics domain.\n\nThey use a combination of automated fetching of code from GitHub combined with manual inspection of code, and creating test cases.\n\nThey ran existing popular LLM models on their dataset and reported how they perform.\nThis adds additional value to the paper, showcasing that all LLMs struggle on the task they created."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ **Important problem**. I like the problem being tackled. There is a great challenge in enabling code generation for domain specific tasks.\n+ **Combination of automated and manual inspection**. Manual inspection can ensure better quality.\n+ Authors created a repository with code and thus are enabling **knowledge sharing**. I suppose they plan to open source it later as well."
                },
                "weaknesses": {
                    "value": "- **Paper writing**. Can be significantly improved. Flow of the paper is inconsistent, sometimes containing phrases which are not previously explained.\n- **Inconsistent results and motivation**. The results shown in the evaluation don't testify much about the usefulness of the constructed dataset for the challenges authors initially propose tackling."
                },
                "questions": {
                    "value": "## Other Comments\n- Abstract. I would expect that in the second part of the abstract (after you motivated the problem of difficulty of correctly generating code for domain specific areas such as bioinformatics) you provide some evidence on why your dataset is either: (a) useful to help train models that achieve better performance for the given domain or (b) it provides a good measure to estimate whether an LLM is effective for the code generation in the field of bioinformatics (or specific subarea of bioinformatics coding domains).\n\n- Abstract. Sentence \"In relation to function-code generation ...\" is unclear given the previous sentences. Reformulate.\n\n- Abstract. Sentence \"It incorporates 1026 functions and 1243 methods in Python and Java from GitHub\". You probably need to add \"respectively\" after the word \"Java\".\n\n- Introduction. I would like to see some discussion or evidence of why the dataset you constructed is an effective dataset for the field of bioinformatics. Some of the questions that are still left in my mind are:\n    - What is the range of problems/areas bioinformatics code typically encompasses? \n    - What position/importance in the field of bioinformatics is occupied by the tasks you include in your dataset? I see some information in the appendix, but ultimately this is a crucial factor in the paper, so there needs to be a good evidence about it in the main paper.\n    - Ultimately, what is the value that a bioinformatics programmer, who wants to use an LLM to help writing his/her code, will gain by evaluating LLMs on your dataset? Can he/she gain some level of confidence that LLM that works well on your dataset, will perform better on his/her problems?\n    \n- Introduction. You mention that you create \"a new high-quality dataset\". It may be a high-quality dataset, but it would be good to add some specific numbers or evidence on why it is so.\n\n- It would be great if in Figure 2 (describing the overall process of constructing the dataset) you indicate which parts of the process are done manually, and which part is automated.\n\n- Related Work. \"We ensure each function demands a certain level of domain expertise in bioinformatics\". Can you add briefly how? It can be as simple as \"via manual inspection\". But, later on you need to elaborate further how the manual inspection was performed to ensure that.\n\n- \"golden code file\". While one can guess what the meaning of the golden code file is, it would be good to explain. This phrase is used only at a single place in the paper.\n\n- \"Summary Only prompts\". When encountering this phrase in the paper for the first time, it's not obvious what this means. One can look at Figure 3 and infer, but it would be good to have a natural flow in the main body of the paper itself.\n    - If your concern is space, I would rather remove results of certain prompt types from the main paper, and put them into the appendix. Then, for a single version of the prompt I would explain in detail in the main paper how it is constructed.\n    - Also, Figure 3 is not elaborated inside of the paper.\n\n- Table 4. \"Results are in %\". I would write something like: results are expressed in percentage.\n\n- I would add line numbers for the review purposes so reviewers can easily refer to a given part of the paper in their comments.\n\n- Table 1. Spell out the abbreviations somewhere for P.C., P.L., C.C., C.L.\n\n- Table 6. I would suggest using percentage instead of raw numbers for showing distribution of test error types. You can also include information about total number of test cases vs number of failed cases. You could possibly restructure the table to remove repetitive \"Failure Reason\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1271/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1271/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658168121,
            "cdate": 1698658168121,
            "tmdate": 1699636053888,
            "mdate": 1699636053888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F31nYJuFNJ",
                "forum": "JbOsMrwjZ3",
                "replyto": "aVzQZLwBh0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Adqr:"
                    },
                    "comment": {
                        "value": "Thank you for your constructive review. We have addressed your comments and questions and made respective changes in our revised manuscript."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139768673,
                "cdate": 1700139768673,
                "tmdate": 1700139768673,
                "mdate": 1700139768673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x0RRTSl8ho",
                "forum": "JbOsMrwjZ3",
                "replyto": "aVzQZLwBh0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Improvements to Abstract for Enhanced Clarity"
                    },
                    "comment": {
                        "value": "> Abstract. I would expect that in the second part of the abstract (after you motivated the problem of difficulty of correctly generating code for domain specific areas such as bioinformatics) you provide some evidence on why your dataset is either: (a) useful to help train models that achieve better performance for the given domain or (b) it provides a good measure to estimate whether an LLM is effective for the code generation in the field of bioinformatics (or specific subarea of bioinformatics coding domains).\n\n> Abstract. Sentence \"In relation to function-code generation ...\" is unclear given the previous sentences. Reformulate.\n\n> Abstract. Sentence \"It incorporates 1026 functions and 1243 methods in Python and Java from GitHub\". You probably need to add \"respectively\" after the word \"Java\".\n\nThank you for all your good suggestions on our abstract. We found it really useful, and we have edited our abstract based on them.\n\nCurrent abstract:\n\n>> Pre-trained large language models have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Bioinformatics provides an important domain. In this field generating functional programs poses additional notable challenges due to the amount of specialized domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4. The results highlight two key aspects of successful models: 1) that they contain specific domain knowledge of bioinformatics (beyond just coding knowledge); 2) that they accommodate a long prompt with full context (i.e. functional dependencies).\n\nBased on your comment, we could edit our abstract to:\n\n>> Pre-trained large language models have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target the bioinformatics domain due to the amount of specialized domain knowledge, algorithms, and data operations. We present BioCoder, a benchmark developed to evaluate large language models (LLMs) in generating this bioinformatics-specific code. BioCoder spans a broad spectrum of the field and covers cross-file dependencies (e.g., package dependencies), class declarations, and global variables. It incorporates 1026 Python functions and 1243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling we show that overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4.  Furthermore, we finetuned StarCoder, demonstrating how our benchmark dataset can effectively enhance the performance of an LLM. The results highlight two key aspects of successful models: (1) that they contain specific domain knowledge of bioinformatics (beyond just coding knowledge); (2) that they accommodate a long prompt with full context (i.e. functional dependencies)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139816032,
                "cdate": 1700139816032,
                "tmdate": 1700139835829,
                "mdate": 1700139835829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D1eIEMUVtd",
                "forum": "JbOsMrwjZ3",
                "replyto": "aVzQZLwBh0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Showing Test Error Types as Percentages"
                    },
                    "comment": {
                        "value": "> Table 6. I would suggest using percentage instead of raw numbers for showing distribution of test error types. You can also include information about total number of te st cases vs number of failed cases. You could possibly restructure the table to remove repetitive \"Failure Reason\".\n\nThank you for your suggestion. We have updated table 6 to show the distribution of failure reasons in percentages, and merged some of the tables accordingly. They are reflected in the revision of the paper.\n\n| Type                  | Quantity | Percentage |\n|-----------------------|----------|------------|\n| Mismatched output     | 8661     | 4.567%     |\n| Invalid syntax        | 117665   | 62.038%    |\n| Runtime error         | 55351    | 29.184%    |\n| Time out              | 4        | 0.002%     |\n| Successfully Passed   | 7982     | 4.209%     |\n\n\n\nOnce again, we **appreciate** your feedback and believe that our revisions have addressed your concerns. Your insights have helped improve the quality and clarity of our work."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140220974,
                "cdate": 1700140220974,
                "tmdate": 1700141051235,
                "mdate": 1700141051235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CT2kXbxo7q",
                "forum": "JbOsMrwjZ3",
                "replyto": "x0RRTSl8ho",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
                ],
                "content": {
                    "title": {
                        "value": "Re: Improvements to Abstract for Enhanced Clarity"
                    },
                    "comment": {
                        "value": "Modified abstract looks better. Other suggestions for the abstract:\n\n- \"we finetuned StarCoder, demonstrating how our benchmark dataset can effectively enhance the performance of an LLM\"\nIf possible add some numbers showing how much of improvements it brought to StarCoder.\n\n- You can expand on the last sentence, as it is the one that testifies about the usefulness of the dataset.\n\"The results highlight two key aspects of successful models: 1) that they contain specific domain knowledge of bioinformatics (beyond just coding knowledge); 2) that they accommodate a long prompt with full context (i.e. functional dependencies).\"\n\ne.g., how you tested that successful models contain specific domain knowledge of bioinformatics.\nHow does your dataset help there?\nHow does your dataset help in estimating quality of the model?\nIn the abstract it's good to have some quantitative evidence as well.\n\nminor things:\n\"this bioinformatics-specific code\" -> I don't like using word \"this\". You can rephrase it.\n\nOther minor comments:\nTable 1:\n- \"Test\" column is not explained.\n- \"Num.\" in the caption. There's no need for \".\" after it, you probably want to bold it also, so it's clear it refers to the column name."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587391418,
                "cdate": 1700587391418,
                "tmdate": 1700587391418,
                "mdate": 1700587391418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xBNt4A8rCb",
                "forum": "JbOsMrwjZ3",
                "replyto": "D1eIEMUVtd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
                ],
                "content": {
                    "title": {
                        "value": "Re: Showing Test Error Types as Percentages"
                    },
                    "comment": {
                        "value": "Table 5:\nInstead of \"we have omitted the percentage symbol (%)\", you can write something like: \"Numbers represent percentages\".\nYou can consider if you want to round numbers to 2 decimals and include percentage symbol (I suppose spacing is a problem). But, it is up to you.\n\nTable 6:\nIt's a minor thing, but you can consider rounding to 2 decimal places.\n\nWhen looking up again which model Table 6 is related to, I backtrack to the text, paragraph: \"Table 6 provides an overview of the error statistics collected from our test runs\". It is unclear which model is in question?\n\nI do like when Table/Figure captions are self explanatory. But, considering the space issues you might not have enough space to achieve that. In that case, I would suggest that the place where you refer to a Table/Figure is clear about what exactly it is about."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588140305,
                "cdate": 1700588140305,
                "tmdate": 1700588140305,
                "mdate": 1700588140305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DZ4EUS17Fo",
                "forum": "JbOsMrwjZ3",
                "replyto": "aVzQZLwBh0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your further comments on our manuscript. **Your feedback on the abstract is much appreciated**, and we agree that a more quantitative approach to highlighting the improvements of StarCoder would be beneficial. Therefore, we have now updated this to provide specific numbers that show the notable performance increase that was achieved following fine-tuning on our benchmark dataset.\n\nOur new abstract is as follows:\n\n>> Pre-trained large language models have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target bioinformatics due to the amount of specialized domain knowledge, algorithms, and data operations this discipline requires. We present BIOCODER, a benchmark developed to evaluate large language models (LLMs) in generating bioinformatics-specific code. BIOCODER spans a broad spectrum of the field and covers cross-file dependencies, class declarations, and global variables. It incorporates 1026 Python functions and 1243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling we show that overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BIOCODER incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4. **Furthermore, we finetuned StarCoder, demonstrating how our dataset can effectively enhance the performance of an LLM (by >15% in terms of Pass@K in certain contexts and always >3%)**. The results highlight two key aspects of successful models: **(1) Successful models accommodate a long prompt (> ~2600 tokens) with full context, for functional dependencies. (2) They contain specific domain knowledge of bioinformatics, beyond just general coding knowledge. This is evident from the performance gain of GPT-3.5/4 compared to the smaller models on the benchmark (50% vs up to ~25%).** \n\n\nIn consideration of your minor comments, we have taken the liberty to make **the following revisions**:\n\n- The term \"this bioinformatics-specific code\" has now been replaced with \"bioinformatics-specific code\".\n\n- For **Table 1**, we have expanded the explanation for the \"Test\" column.\n\n- The caption under \"Num.\" has been boldly made and the period following it has been removed, as suggested.\nFor **Table 5**, instead of \"we have omitted the percentage symbol (%)\", it has now been rephrased to \"Numbers represent the Pass@K in the form of percentages.\u201d\n\n- Regarding **Table 6**, we have clarified that the statistics are aggregated across all models.\n\n- Alleviated space constraints by converting **Table 5** into a two-column format.\n\nWhile we were considering rounding decimals to two decimal places, it seems that it does not make a meaningful difference on the length of the manuscript. Therefore, we have kept the rounding to three decimal places, as we would like to maintain as much detail about the results as possible given the performance of many models is very low.\n\nYour insights have been invaluable in refining our paper. We appreciate the constructive critique."
                    },
                    "title": {
                        "value": "Responses and Revisions to the Second Round of Feedback"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625485211,
                "cdate": 1700625485211,
                "tmdate": 1700652815884,
                "mdate": 1700652815884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5gJrl4YGWs",
                "forum": "JbOsMrwjZ3",
                "replyto": "lBBkShyPMd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Reviewer_Adqr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for making the updates.\nI think this is as good as the paper can get for this submission.\n\n> demonstrating how our dataset can effectively enhance the performance of LLMs (by >15% in terms of Pass@K in certain contexts and always >3%). \n>> Is the performance increase visible on the (test) portion of your own dataset, or in some other contexts?\n\nI do believe that the paper has potential but requires a more significant rewrite to make it flow nicely.\nThere are similar (small) comments like the ones I had previously throughout the paper; which in combination would make a big difference in how the paper reads."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699238314,
                "cdate": 1700699238314,
                "tmdate": 1700699238314,
                "mdate": 1700699238314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jA4GKrvpiS",
                "forum": "JbOsMrwjZ3",
                "replyto": "aVzQZLwBh0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Reviewer Adqr"
                    },
                    "comment": {
                        "value": "Dear Reviewer Adqr,\n\nWe sincerely thank you for recognizing the potential of our work and for your positive remarks on the amendments made to the current submission.\n\nThank you for your patience and continued engagement. We value our commitment to clarifying every ambiguity.\n\nDefinitely, the improvements we observed were indeed on **the test portion** of our BioCoder dataset after fine-tuning StarCoder using the training set. \n\nWhen we referenced the performance enhancement (by >15% in terms of Pass@K in certain contexts and always >3%), the **'certain contexts'** indeed were associated with **different Prompt Configurations** in our BioCoder test set. To be clearer, we observed improvements of over 15% in **some Prompt Configurations** (```Uncommented``` & ```Summary Only``` & ```Necessary Only```) following the fine-tuning of StarCoder on our benchmark dataset. However, **all Prompt Configurations** revealed enhancements with a persistently significant increase of over 3%. This specifically highlights how effectively our BioCoder dataset aids in augmenting the performance of LLMs across variable prompt configurations.\n\nTo clarify the data presentation from Table 4, we've further broken down the information into two subtables for Python and Java, respectively.\n\n```Here are the results for Python:```\n\n\n|**StarCoder-15.5B (w/o finetuning)** | Pass@1 | Pass@5 | Pass@10 | Pass@20 |\n|---------|--------|--------|---------|---------|\n| _Summary at Top_ | 3.694 | 13.197 | 19.359 | 24.554 |\n| _Uncommented_ | 0.318 | 1.062 | 1.591 | 2.548 |\n| _Summary Only_ | 4.682 | 15.225 | 21.200 | 27.166 |\n| _Necessary Only_ | 0.127 | 0.603 | 1.123 | 1.911 |\n\n|  **StarCoder-15.5B (finetuned)**     | Pass@1 | Pass@5 | Pass@10 | Pass@20 |\n|---------|--------|--------|---------|---------|\n| _Summary at Top_ | 5.818 | 16.562 | 21.091 | 27.048 |\n|  _Uncommented_ | 3.312 | 9.073 | 12.574 | 17.536 |\n|  _Summary Only_ | 7.295 | 20.838 | 26.143 | 39.570 |\n|  _Necessary Only_ | 0.597 | 1.173 | 1.813 | 2.611 |\n\n```Here are the results for Java:```\n\n|     **StarCoder-15.5B  (w/o finetuning)**     | Pass@1 | Pass@5 | Pass@10 | Pass@20 |\n|---------|--------|--------|---------|---------|\n|  _Summary at Top_ | 0 | 0 | 0 | 0 |\n|  _Uncommented_ | 0 | 0 | 0 | 0 |\n|  _Summary Only_ | 0 | 0 | 0 | 0 |\n|  _Necessary Only_ | 0 | 0 | 0 | 0 |\n\n|     **StarCoder-15.5B (finetuned)**    | Pass@1 | Pass@5 | Pass@10 | Pass@20 |\n|---------|--------|--------|---------|---------|\n| _Summary at Top_ | 0 | 0 | 0 | 0 |\n| _Uncommented_ | 0 | 0 | 0 | 0 |\n|  _Summary Only_ | 0.200 | 1.000 | 2.000 | 4.000 |\n|  _Necessary Only_ | 3.300 | 12.097 | 19.545 | 30.000 |\n\n\n\n\nWe acknowledge that the term 'in certain contexts' could have been more explicit. We will modify it to 'in certain prompt configurations' in our subsequent communications and manuscript revisions to ensure better understanding. We trust this further elucidates your query.\n\n\nCurrent version:\n\n>> Furthermore, we finetuned StarCoder, demonstrating how our dataset can effectively enhance the performance of LLMs on our benchmark (by >15% in terms of Pass@K in certain prompt configurations and always >3%). \n\nWhat's noteworthy is that your invaluable suggestions are primarily oriented towards the presentation and writing aspects of the manuscript, rather than questioning the foundational contents or the contributions proposed by our work. For future versions, we strive to enhance the readability while remaining assured that the essence of its contributions to the field is aptly recognized.\n\nOnce again, we thank you for your time and constructive feedback. It has been instrumental in refining our paper, and we will continue to polish it for the next version.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700824695,
                "cdate": 1700700824695,
                "tmdate": 1700702301452,
                "mdate": 1700702301452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZLg5VIZ59W",
                "forum": "JbOsMrwjZ3",
                "replyto": "aVzQZLwBh0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for reassessment"
                    },
                    "comment": {
                        "value": "Dear Reviewer Adqr,\n\nWe are deeply appreciative of your comprehensive feedback and the time you have taken to improve our manuscript. We have sincerely endeavored to incorporate your invaluable suggestions, primarily those concerned with enhancing the readability and presentation aspects of the paper.\n\nWe understand that although we have made significant improvements, the paper could still benefit from further polishing. However, given our earnest efforts in addressing the writing concerns in the limited time of the rebuttal period, and the absence of substantial disputes regarding the foundational content and contributions of our work, we genuinely hope that these critical developments in our revision will reflect positively in your reassessment of our work.\n\nWe are more than happy to address followup comments you may have, and we will continue refining the paper for future versions, striving for clarity and improved readability. \n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704012479,
                "cdate": 1700704012479,
                "tmdate": 1700704503254,
                "mdate": 1700704503254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QTFktsPLMD",
            "forum": "JbOsMrwjZ3",
            "replyto": "JbOsMrwjZ3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1271/Reviewer_BxXn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1271/Reviewer_BxXn"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the function-level code generation challenges within the field of bioinformatics and evaluate the effectiveness of current leading pre-trained  large models (including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4) in generating code in the realm of bioinformatics.  To accomplish this, the paper utilized web scraping techniques to extract data from 1743 bioinformatics-adjacent GitHub repositories on GitHub, constructing and presenting a benchmark dataset named BioCoder. This dataset offers a reliable evaluation standard for code generation tasks within the field of bioinformatics. The main contribution of this paper comprises code parsing tools, proposed dataset, docker environment and comprehensive evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The dataset, evaluation in this paper is particulary well-established. The implementation is very solid, and the presentation in this paper is easy to follow. It has the poteintial to become a standard evaluation benchmark for bioinformatic code generation."
                },
                "weaknesses": {
                    "value": "The article includes an appendix that might be too long for the readers, and the content in the appendix is referred from the main body for multiple times. It is better to make the paper more self-contained if it is accepted to be published in conference proceedings. Moreover, the author should clearly point out the main technical contribution of this paper. I don't quite catch the challenges for benchmarking bioinformatics code generation compared to other domain specific languages."
                },
                "questions": {
                    "value": "1. How does the benchmarking for bioinformatics code generation differs from the benchmarking for other domain specific languages?\n2. What is the key technical contribution for the BioCoder, that is strongly related to benchmarking bioinformatics code generation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1271/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1271/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1271/Reviewer_BxXn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670799938,
            "cdate": 1698670799938,
            "tmdate": 1699636053782,
            "mdate": 1699636053782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IGIRcsxWv1",
                "forum": "JbOsMrwjZ3",
                "replyto": "QTFktsPLMD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our response to Reviewer BxXn"
                    },
                    "comment": {
                        "value": "Thank you for your insightful questions!\n\n> It is better to make the paper more self-contained if it is accepted to be published in conference proceedings. \n\nThank you for your constructive feedback. We agree that the paper could benefit from being more self-contained. As per your suggestion, we have revisited the main text and revised it in a way so as to include more key details that were initially placed in the appendix, for instance, the definition of different versions of Prompts. We are confident, after these revisions, that the paper stands comprehensive on its own. Even in the absence of the appendix, the purpose, methodologies, and key findings of our research can be entirely understood from the main text. We appreciate your insights and believe these changes significantly enhance the paper's readability and comprehensibility.\n\n\n>  I don't quite catch the challenges for benchmarking bioinformatics code generation compared to other domain specific languages.\n\nThe challenges of benchmarking bioinformatics code generation originate from both the characteristics of the domain and the nature of the tasks involved. \n\nGood bioinformatics coding requires a deep understanding of both biological concepts and advanced computation methods. Generating or evaluating code thus requires knowledge and experience in both. Bioinformatics involves numerous subfields and types of tasks, from sequence alignment, and phylogenetic analysis, to protein structure prediction, to name just a few.\n\nHence, to generate high-quality and functional bioinformatics code, the model must be well-versed not only in programming but also in biological and computational concepts relevant to bioinformatics.\n\n\n> How does the benchmarking for bioinformatics code generation differs from the benchmarking for other domain specific languages?\n\nWhile the concept of Bioinformatics code does not differ significantly from domains such as chemistry, it differs a bit from more mathematical and computational heavy domains like engineering, physics, and others. In our findings, the code is a lot more dependent on knowledge within the domain, and it is nearly impossible to generate any of the functions without the required domain knowledge.\n\n> Moreover, the author should clearly point out the main technical contribution of this paper.\n\n> What is the key technical contribution for the BioCoder, that is strongly related to benchmarking bioinformatics code generation?\n\nThank you for both questions. We will answer them at the same time.\n\nThe technical contributions of this paper are multifold, primarily focusing on the development of tools, datasets, models, and testing environments to enhance bioinformatics code generation:\n\nCode Parsing Tools \u2013 We've developed comprehensive tools for parsing large-scale code repositories, which include AST (Abstract Syntax Tree) parsers and utilities to extract code attributes. These tools facilitate efficient and systematic extraction, transformation, and storage of coding tasks from raw source codes. Furthermore, they also feature techniques to correct minor errors that arise during generation.\n\nData Processing Scripts - We have developed methodical procedures and scripts for data processing, which are both robust and transparent. These are provided so that others within the community may utilize and adapt them for their own tasks, lending methodological insights and fostering further research within the field.\n\nTesting and Docker \u2013 We've established a robust testing environment that includes Docker configurations, required dependencies, and an array of fuzz test cases. The aim is to create a reproducible testing setup that not only imitates realistic project scenarios but also ensures the scalability and transferability of our approach. \n\nModel Zoo - In our effort to promote applicability and ease-of-use, we have created a 'Model Zoo.' This feature provides immediate access to numerous pre-implemented code-generation large language models, such as SantaCoder, StarCoder, and CodeGen alongside APIs based on OpenAI. This enables other researchers or developers to effortlessly test their tasks on many different models, without the overhead of implementing these models from scratch. We firmly believe that our model hub significantly contributes engineering-wise to the entire domain."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139423652,
                "cdate": 1700139423652,
                "tmdate": 1700139494641,
                "mdate": 1700139494641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e6f5KDDRYj",
                "forum": "JbOsMrwjZ3",
                "replyto": "IGIRcsxWv1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Reviewer_BxXn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Reviewer_BxXn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response on the questions and updates on the submission. I believe the contribution of the paper and the implementation of the benchmark would have enough potiential influence. Moreover, I do appreciate the work on code parsing, data prrcessing, docker environment and model zoo.\n\nIn addition, concerning the biology domain knowledge, the authors name several tasks like sequence alignment, phylogenetic analysis and protein strucutre prediction, and claim that the domain knowledge alters the benchmark (at least from the dataset selection or prompt generation). It is still not enough for the users of the benchmark to analyze a model on its understanding of domian specific knowledge. For instance, is it possible for BioCoder to make a distinction on the ability of domain specific knowledge and general code gerneation capability when contribution to a task of generating a specific bioinformatical program\uff1f\n\nIf the answer is possitive, it would be more convincible on the technical novelty of the paper."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707281625,
                "cdate": 1700707281625,
                "tmdate": 1700707281625,
                "mdate": 1700707281625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iTUqi3Q67N",
                "forum": "JbOsMrwjZ3",
                "replyto": "QTFktsPLMD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Reviewer BxXn"
                    },
                    "comment": {
                        "value": "Dear Reviewer BxXn,\n\nThank you for your further inquiries and valuable insights.\n\nIndeed, BioCoder tasks inherently interweave domain-specific content with general coding competencies. While these strands are tangled, our BioCoder benchmark primarily emphasizes the importance of domain knowledge by manually selecting data with algorithms specifically used within the bioinformatics domain. Hence, purely generic code samples are not available in our dataset.\n\nWe concur with your sentiment that disentangling domain-specific knowledge from general code generation skills is a challenging undertaking. In our BioCoder context, learning to generate bioinformatical programs necessitates an understanding of the underlying coding logic, syntax, and structure, which forms the bedrock of these tasks. Yet, the distinctive layer of biological context in our use cases shapes the models' nuanced understanding of specific domain knowledge.\n\nSignificantly, our fine-tuning of StarCoder, which originally was pre-trained purely on coding data and thus devoid of any bioinformatics domain knowledge, demonstrated an essential improvement (>15% in terms of Pass@K). This significant enhancement affirms that exposure to our training data, which possesses a proportion of tasks necessitating biological understanding, successfully imparts some degree of domain knowledge to the model.\n\nWe acknowledge the opportunity and necessity for future research to delve deeper into distinguishing and evaluating a model's domain-specific knowledge and its general code generation capability. Understanding these clearly could significantly further the interpretability and analysis of language models across various domains.\n\nWe hope we have addressed your queries satisfactorily. We extend our heartfelt gratitude for your invaluable feedback that has driven us to look beyond and identify potential areas of investigation.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710758300,
                "cdate": 1700710758300,
                "tmdate": 1700710798321,
                "mdate": 1700710798321,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zqRMRjR9Zo",
            "forum": "JbOsMrwjZ3",
            "replyto": "JbOsMrwjZ3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1271/Reviewer_r6TX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1271/Reviewer_r6TX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a large-scale benchmark, BioCoder, which is devoted to assessing the capability of LLMs regarding code generation, specifically in the field of bioinformatics. The authors collect 1026 functions and 1243 methods in two programming languages (Python and Java) from GitHub and 253 examples from the Rosalind project to form a relatively intricate dataset to evaluate the LLMs' code generation abilities from various aspects. The authors conduct multiple steps to ensure the validity and unbiasedness of the constructed benchmark. In particular, 10 different LLMs (including the fine-tuned one) are evaluated on BioCoder, and the experiments reveal what factors can potentially affect the performance of LLMs while tackling challenging code generation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Timely and vital problem.\n2. A valuable large-scale benchmark for code generation, including specialized domain knowledge.\n3. Comprehensive evaluation with multiple LLMs.\n4. The presentation is in a good manner, and the paper is easy to follow."
                },
                "weaknesses": {
                    "value": "I appreciate that this paper has provided a valuable benchmark to the communities, as the new benchmark can potentially help researchers and practitioners in this direction. However, there are several concerns regarding the methodology and evaluation of this paper, which I will elaborate on below:\n\n\n1. My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\n2. In addition to the previous point, for Rosalind functions, the authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" Why and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n3. Another concern is the implementation of correction mechanisms which rectify minor syntax and style errors. What kind of syntax and style errors can be considered \"minor\" with no impact on the functionality of the generated programs? As the authors take invalid syntax and runtime error as two major failure reasons in the following error distribution analysis, I recommend further justifying such correction mechanisms, which may affect the validity of the analysis results.\n\n4. Table 4 summarizes the performance of the studied LLMs on BioCoder w.r.t 4 different types of prompt formats. However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand. Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? Are they from existing lectures or experimental experience? What are the characteristics of different prompt formats?\n\n5. The discussion of the experiment results seems shallow to me. In section 5, the authors consider there is an inverse relationship between the length of the input prompts and the performance of the generated codes. However, from Table 4 and Appendix I, the Necessary only prompts have relatively shorter prompts but lower passing rates compared to uncommented and Summary at Top/Bottom in most of the studied LLMs. The author may elaborate more on the perspectives of prompt structures and contents instead of just the length of the prompts.\n\n\nMinor Comments\n\n1. The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4). \n\n2. From section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example."
                },
                "questions": {
                    "value": "1. The details of the testing framework and the corresponding effectiveness should be discussed.\n\n2. For Rosalind functions, why and how are the generated codes compared with the gold code outputs?\n\n3. What are the guidelines while designing the 5 different prompt styles for the subject LLMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805188221,
            "cdate": 1698805188221,
            "tmdate": 1699636053706,
            "mdate": 1699636053706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8rwtIpmsAJ",
                "forum": "JbOsMrwjZ3",
                "replyto": "zqRMRjR9Zo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our response to Reviewer r6TX"
                    },
                    "comment": {
                        "value": "Thank you for your comments. We have addressed each one as follows:\n\n> My biggest concern is the lack of justifications for the testing framework for Python and Java functions in BioCoder. The authors state, \"... we employ a custom syntax to indicate the insertion points for custom randomly generated test cases.\" How are the test cases \"randomly generated\" and inserted into the context files? I did not find any detailed explanations in the main paper or in Appendix L and Y. In particular, Appendix L only briefly introduces the pipeline of the testing framework, how does such an approach deliver \"a secure and efficient testing framework, promising robustness in the evaluation of generated code\"? The authors need to clarify more about the generation of the test cases.\n\nApologies for the oversight, as we did not provide enough information about the testing framework in Appendix L. We provided a brief overview of the pipeline in Appendix L, but we will make sure to explain the details and importance of the framework in more depth.\n\nOur fuzz testing framework was created based on the need to accurately compare ground truth and generated code samples, while optimizing the test case writing pipeline. A detailed explanation of the entire process, as well as our motivations, is written in the postceding paragraph. Our test cases can be thought of as typical unit tests with randomly generated parameters, hence why we are \u201cinserting\u201d randomly generated test cases into the code (more details given later). Our entire process, including the use of docker containers, virtualization, and consistent testing environments, allows us to ensure the security and robustness of our testing framework.\n\nHere is a more detailed explanation:\n\nWe decided to utilize concepts from fuzz testing, as fuzz testing is widely used in the industry to capture bugs, crashes, security vulnerabilities, etc. in functions. However, in these cases, they do not have access to a \u201ccorrect\u201d version of the function; instead, they are merely creating inputs to intentionally try to crash the program, find out-of-bounds memory accesses, etc. Our situation is unique because we have the \u201cgolden code\u201d, or the ground truth version of the code, so given input, we definitely know what the expected output should be, which is not something that\u2019s usually available in typical fuzz testing frameworks. Therefore, our situation could be considered a mixture of both unit testing and fuzz testing.\n\nGiven this requirement, and the goal of large-scale prompt generation, we decided to implement our own framework. We set out to accomplish two things: make the annotation process a lot easier for human editors, and support our feature set that combines both elements from unit testing and elements from fuzz testing. We believe that our resulting pipeline is more intuitive than piecing together other testing frameworks, and in our annotation process, it proved to make things efficient, enabling larger-scale annotation, as the goal of the paper.\n\nFurthermore, note that while handwritten test cases would likely target edge cases of a program (eg. branch coverage, conditional coverage), the probability of our fuzz testing framework hitting all of the same edge cases is high given 1000 iterations of randomly generated inputs. This means that we can save a significant amount of time building the dataset, as we only need to write an outline of a test case, and let the framework handle the rest. Therefore, we can think of the framework as thousands of \u201crandom unit tests,\u201d with a high probability that these unit tests would include handwritten test cases, if we had written them.\n\nIn terms of variable generation, we replace the <|var_type;parameter|> syntax with random values each iteration, for an unlimited number of iterations. These parameters are modifiable, and we implemented this system to be flexible, so that we can target specific scopes for fuzz testing. We check correctness by substituting the exact same variables in the original code, and checking if the outputs of the two functions match. This indicates identical functionality to the original code."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138380050,
                "cdate": 1700138380050,
                "tmdate": 1700138380050,
                "mdate": 1700138380050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hn07sUGF21",
                "forum": "JbOsMrwjZ3",
                "replyto": "zqRMRjR9Zo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Examples of random integer and numpy array generation"
                    },
                    "comment": {
                        "value": "Here is an example of random integer and numpy array generation:\n\n```\nimport numpy\nimport skimage.morphology\nimport os\n<<insert solution here>>\ndef main():\n    numpy.random.seed(<|int;range=0,100|>)\n    labels = numpy.random.randint(2, size=(3, 3))\n    diameter = <|int;range=2,10|>\n    print(fill_object_holes(labels, diameter))\nif __name__ == \"__main__\":\n    main()\n```\n\nExample of random string generation:\n\n```\nimport random\n[IMPORTS REDACTED FOR CONCISENESS]\nimport warnings\nfrom textwrap import wrap\nimport string\nimport zlib\nimport io\nfrom os.path import isfile\nclass GenipeError(Exception):\n        pass\n_CHECK_STRING = b'GENIPE INDEX FILE'\ndef dosage_from_probs(homo_probs, hetero_probs, scale=2):\n    \"\"\"Computes dosage from probability matrix (for the minor allele).\n    Args:\n        homo_probs (numpy.array): the probabilities for the homozygous genotype\n        hetero_probs (numpy.array): the probabilities for the heterozygous\n                                    genotype\n        scale (int): the scale value\n    Returns:\n        numpy.array: the dosage computed from the probabilities\n    \"\"\"\n    return (homo_probs + (hetero_probs / 2)) * scale\n<<insert solution here>>\ndef main():\n    np.random.seed(<|int;range=0,100|>)\n    prob_matrix = np.random.rand(10, 10)\n    a1 = <|string|>\n    a2 = <|string|>\n    print(maf_dosage_from_probs(prob_matrix, a1, a2))\nif __name__ == \"__main__\":\n    main()\n```\n\nLet\u2019s break apart the integer syntax:\n<|int|> denotes an integer. If left without parameters, then in one iteration of the program, this will be replaced with a random integer between INT_MIN and INT_MAX before compile time (or in this case, before the Python file is executed). There are parameters that can be passed in, that include range, even/odd, etc.\nSimilarly, for <|string|> this generates a random ASCII string of any type. It can be further narrowed down into ASCII strings only, lowercase only, specific characters only, etc. by passing in the relevant parameters.\nThese random inserted values can be manipulated to become part of a larger data structure, for example, a Numpy array, or a mock Python object.\n\nWhen these files are executed, we replace <<insert solution here>> with the golden code on one iteration, and the error-corrected generated code on a corresponding iteration. The fuzzing framework is designed so that the same inputs will be passed to this pair of iterations, meaning that we should be getting the same output (none of the functions have a non-deterministic component to them). Therefore this supports one aspect of the \u201csecure\u201d testing framework, as we have created an environment where all else is equal, except for the generated/golden code.\n\nFurthermore, the \u201csecurity\u201d is also established by our usage of Docker containers to enable a secure (malicious code cannot affect the host system), consistent testing environment for all testing iterations. All of these aspects come together to enable us to efficiently and robustly test the generated functions.\n\nWe have updated this in the Appendix as well."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138458662,
                "cdate": 1700138458662,
                "tmdate": 1700138485825,
                "mdate": 1700138485825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8WeDsFll1a",
                "forum": "JbOsMrwjZ3",
                "replyto": "zqRMRjR9Zo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on Code Comparison and Testing Process in Rosalind Functions"
                    },
                    "comment": {
                        "value": "> In addition to the previous point, for Rosalind functions, \nthe authors mentioned, \"...the output of this execution is compared with the cached golden code output.\" \nWhy and how are the generated codes compared with the gold code outputs? I do not find any experiment results that illustrate the comparison outcomes.\n\n\nThank you for the clarifying question. In the current paper, we have: \u201cFor Rosalind functions, the process is simpler and more efficient as the functions are less complex. The golden code\u2019s output is generated and cached ahead of time. During testing, the tester executes the generated code within the corresponding context, and the output of this execution is compared with the cached golden code output.\u201d\n\nWe admit that this is not clear. The \u201ccached golden code output\u201d refers to an optimization specific to Rosalind. We utilize the fact that we are given the same input for all test cases, and only run the golden code once, instead of running it 20 times in all 20 pairs of generated-golden code pairs. For the next revision, we will likely remove this in the name of clarity, and because it is an implementation detail that does not affect any results.\n\nRosalind (https://rosalind.info/) is an educational resource and web project for learning bioinformatics through problem-solving and computer programming.\n\nIn Rosalind Online Judge, they declare a code sample as \u201ccorrect\u201d using just one test case. We define \u201cgolden code\u201d as the original code, which is guaranteed to be correct. Therefore, for our evaluation, we also use one test case, and use it as input for both the original golden code and the generated code. Then, the (typically string) outputs are compared, and declared \u201ccorrect\u201d if and only if it is an exact match.\n\nIn the current paper, the \u201ccached golden code output\u201d refers to the optimization where we used the same test case on each pair of golden and generated code. We used a web scraper to automatically download the desired \u201ctest cases\u201d from the Rosalind website. Therefore, the correct output should always be the same between all executions of a problem, so we only need to execute the golden code once, and \u201ccache\u201d the output.\n\nBased on this we can revise the paper to better explain the Rosalind testing process, and the similarities to the other testing process:\n\n\u201cFor Rosalind functions, the process is simpler and more efficient as the functions are less complex. During testing, the tester executes the generated code and golden code within the corresponding context with the same input, and the output of the pair of executions is considered correct if and only if they are identical.\u201d\n\nGolden Code:\ncount the number of times that \u201cA\u201d appears in the string\n\n```\ndef count_A(input_str):\n    return input_str.count(\u201cA\u201d)\n    print(count)\n```\n\nGenerated code:\ncount the number of times that \u201cA\u201d appears in the string\n\n```\ndef count_A(input_str):\n    count=0\n    for character in input_str:\n        if character==\u2019A\u2019: count+=1\n    print(count)\n```\n\nIn these simplified examples, we could pass in the string \u201cABCAABC@\u201d into the function as input_str. Note that both functions would correctly print \u201c3\u201d as the response, so we can mark it as correct. Here is another example of a generated sample:\n\ncount the number of times that \u201cA\u201d appears in the string\n\n```\ndef count_A(input_str):\n    count=0\n    for character in input_str:\n        if \u201cA\u201d in input_str: count+=1\n    print(count)\n```\n\nIn this example, if we pass in the same string, then it would print \u201c8\u201d, which is clearly not the same as what the golden code printed, and therefore we would mark it as incorrect."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138631979,
                "cdate": 1700138631979,
                "tmdate": 1700138713937,
                "mdate": 1700138713937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MXybMlPzLm",
                "forum": "JbOsMrwjZ3",
                "replyto": "zqRMRjR9Zo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on Prompt Designs"
                    },
                    "comment": {
                        "value": "> What are the guidelines while designing the 5 different prompt styles for the subject LLMs? (*)\n\nThank you for your question. We would be happy to go into some more detail as to how we designed the 5 different prompt styles described in the paper. \n\nWhen designing the prompts, we wanted to make sure the prompts had sufficient and necessary information to capture the essence of the function that we were trying to generate. Specifically, we wanted to design prompts that contained enough code context so that an experienced programmer would have the necessary tools to complete the function. \nUsing these guidelines, we tried hundreds of different ideas for prompt structures, manually testing the performance of various structures using the OpenAI API, as well as locally run models.\n\nEventually, we ended up with 5 main types of prompts, each logically motivated by some observation we made during our experiments.\n\nWe begin with the \u201csummary at bottom\u201d prompts, which is the format used by most papers (we based it off of CoderEval, but it seems to be used in many other papers as well). From here, we also follow the instruction model, putting the summary at the top.\nFor the context, it\u2019s important to note that models typically give higher \u201cpriority\u201d to context later in the prompt. Therefore, the inter-file context needs to go before the intra-file context, which is more likely to be referenced by the target function. However, this context is not displayed in correct syntax, so it might have been important to make a distinction between the inter-file and intra-file context, so we commented out the inter-file ones.\nDuring testing, we observed that this would cause the model to generate an excessive amount of comments, sometimes generating the entire function as a comment. We couldn\u2019t figure out a way to consistently handle this during the dynamic analysis/error correction process, so we uncommented these, and most models seemed to behave more or less \u201cnormally\u201d\nThe \u201cNecessary context only\u201d is human annotated context, which has been fully cleaned so that only the necessary information is included, so it is logical to test these. This is in contrast to the other prompt types, which are almost entirely automatically generated from a repository.\nFor the \u201csummary only\u201d prompts, we purely wanted to see whether the function would be completable without any context whatsoever.\n\nWe feel that these prompts represent a logical progression in our pursuit to extract the highest possible performance out of these models. While we do acknowledge that these may not be the perfect prompts for every model tested, our testing shows that they are some of the best out of the hundreds we have tried, and therefore should be sufficient to prompt LLMs for code generation.\n\n\n>  However, the explanations of the different prompt versions are placed in Appendix I, which makes Table 4 hard to understand\n\nWe have introduced a paragraph in Section 4. MODELS AND RESULTS briefly discuss our prompt versions. We hope it provides an adequate amount of information to explain Table 4, while the more lengthy explanation is in Appendix I.\n\n> Moreover, Appendix I only gives explanations with examples of the prompts in each version; nevertheless, I am looking for some high-level guidelines for the prompt design. Namely, how the five prompt versions are proposed? \n\nThank you for your question. Our answer to your question \u201cWhat are the guidelines while designing the 5 different prompt styles for the subject LLMs?\u201d includes an answer for this.\n\n\n> Are they from existing lectures or experimental experience?\n\n> What are the characteristics of different prompt formats?\n\nThank you for both questions. We will answer them at the same time.\n\nWe created the prompt formats based on experimental experience. We set out to create prompt structures to test the specific performance characteristics of each model. We first decided on a set of characters to target, as shown in the following prompt types:\nUncommented/Summary at top/Summary at bottom - target performance on deciding which context to use, as there is intentionally some extraneous context not required for function\nSummary Only - gather metrics on the level of dependency on context\nNecessary Only - isolate purely the logical reasoning abilities of models, as we assume that all context is utilized.\nFor each of the goals, we experimented with slight modifications of each prompt structure until we found one that achieved the highest performance, best representing the performance of that specific characteristic."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139056647,
                "cdate": 1700139056647,
                "tmdate": 1700139507722,
                "mdate": 1700139507722,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cd0UQNUDJH",
                "forum": "JbOsMrwjZ3",
                "replyto": "zqRMRjR9Zo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Inclusion of Detailed Analysis on Prompt Structures and Updates on GPT-4 Results"
                    },
                    "comment": {
                        "value": "> Add more analysis and elaborate more on the perspectives of prompt structures and contents\n\nThank you for your suggestion. In response, we have added a new appendix AA to the manuscript discussing more analysis on the performance with the perspective of prompt structures and contents. We have also updated the \u201cAnalysis and Discussion\u201d section in the manuscript.\n\n\n\n> The \"Summary At Bottom\" results illustrated in Appendix U seem incomplete (no row for GPT-4).\n\nWe have updated the results of GPT-4 in Appendix U.\n\n> \u201cFrom section 3.4, \"Our testing framework starts with a manual review of selected functions, leading to the creation of a context file and a golden code file for each problem (see Figure 3)\". I do not find how Figure 3 is correlated with the testing framework, Figure 17 in Appendix R may be a better example.\u201d\n\nThank you for your feedback. We have corrected the reference in the revised paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139339567,
                "cdate": 1700139339567,
                "tmdate": 1700139579990,
                "mdate": 1700139579990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1CzGVGJTd7",
                "forum": "JbOsMrwjZ3",
                "replyto": "zqRMRjR9Zo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-Up: Seeking Further Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer, We hope you're doing well. Following up on our recent exchange regarding this paper, we wanted to check if there are any further concerns or feedback from your side. Your insights are invaluable to us, and we're keen to address any remaining issues."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691573577,
                "cdate": 1700691573577,
                "tmdate": 1700691573577,
                "mdate": 1700691573577,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xpBERnJ0RV",
            "forum": "JbOsMrwjZ3",
            "replyto": "JbOsMrwjZ3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1271/Reviewer_p2gV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1271/Reviewer_p2gV"
            ],
            "content": {
                "summary": {
                    "value": "The authors have introduced a benchmark named BioCoder for code generation in bioinformatics. BioCoder covers codes in Python and Java, featuring examples from the Rosalind project. The benchmark creation process is detailed, encompassing preprocessing, evaluation metrics, and baselines. Additionally, several state-of-the-art models are evaluated on the benchmark, and their performance is reported highlighting the superiority of black-box models over open LLMs for code."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The proposed benchmark is an essential collaboration in the field of Bioinformatics.\n  * The processes for dataset creation, preprocessing, and other steps are very well described, including examples and explanations.\n  * The authors test most of the state-of-the-art models for code generation on the proposed benchmark, reviewing each and also fine-tuning one of them.\n  * Every prompt is exemplified in the Appendix with a code snippet.\n  * Another interesting comparison is the one between BioCoder and CoderEval.\n  * Every model is analyzed and discussed (very large Appendix)."
                },
                "weaknesses": {
                    "value": "* Explanations of the prompt configurations, shown in Table 4, should come in the Table description or somewhere in the main text, not only in the Appendix.\n* It Would be interesting to have a human evaluation or experiment considering the descriptions as a way to bring more validation to the GPT3.5 creation."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699029521979,
            "cdate": 1699029521979,
            "tmdate": 1699636053639,
            "mdate": 1699636053639,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Yu1FGgfRT",
                "forum": "JbOsMrwjZ3",
                "replyto": "xpBERnJ0RV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response to Reviewer p2gV"
                    },
                    "comment": {
                        "value": "Thank you for your insightful suggestions.\n\n> Explanations of the prompt configurations, shown in Table 4, should come in the Table description or somewhere in the main text, not only in the Appendix.\n\nWe have modified the paper accordingly to better organize a brief description of the prompt configurations into the main sections of the paper, specifically, in the second paragraph of Section 4. MODELS AND RESULTS.\n\n> It would be interesting to have a human evaluation or experiment considering the descriptions as a way to bring more validation to the GPT3.5 creation.\n\nFurthermore, we have conducted a preliminary human evaluation study on the quality of the GPT-generated summaries. We have randomly sampled 50 summaries that we generated and have had 5 undergraduate students majoring in Computer Science classify each summary, and determine whether there are any errors in their description of the function. \n\nIn this preliminary study, we have found that 48/50 summaries are specific enough to complete a function without much ambiguity in functionality. All summaries accurately reflect the intention of the function, tested by asking whether they could write the code themselves, after reading the summary. This demonstrates the accuracy of the summaries. We do acknowledge that our sample size of 50 summaries is somewhat small, but we wanted to provide a timely response, and we believe that our sample is representative of the larger dataset of summaries."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138114069,
                "cdate": 1700138114069,
                "tmdate": 1700138516218,
                "mdate": 1700138516218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]