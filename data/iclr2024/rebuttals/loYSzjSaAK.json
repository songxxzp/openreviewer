[
    {
        "title": "Submodular Reinforcement Learning"
    },
    {
        "review": {
            "id": "JPYvZZ7btL",
            "forum": "loYSzjSaAK",
            "replyto": "loYSzjSaAK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4828/Reviewer_oKKZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4828/Reviewer_oKKZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes and studies submodular MDPs, where the total reward is characterized by a submodular function of the trajectory. The authors first show that computing a logarithmic approximation of the optimal policy is computational intractable. However, there exists a polocy optimization algorithm which gives a (1-c)-approximation where c is the curvature of the submodular function. Specifically, when specified to bandits, this result outperforms existing ones."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The model is well-motivated and clearly described. It is also easy to understand.\n2. The results contain both upper and lower bounds, which are pretty complete.\n3. Empirical evaluations are conducted for the proposed algorithm."
                },
                "weaknesses": {
                    "value": "1. I'm not sure why this paper considers multiplicative approximations instead of regret/sample complexity, which are common in theory papers studying episodic MDPs.\n2. The optimal dependency on curvature remains unspecified. Whether Proposition 3 is (near-)optimal?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563581079,
            "cdate": 1698563581079,
            "tmdate": 1699636466331,
            "mdate": 1699636466331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CZJffDinKc",
                "forum": "loYSzjSaAK",
                "replyto": "JPYvZZ7btL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oKKZ"
                    },
                    "comment": {
                        "value": "Thank you very much for the positive and valuable feedback. Multiplicative approximations are common for complexity analysis, especially in submodular optimization literature, so we used them. For future direction especially for statistical analysis, we will use regret/sample complexity types bounds. \n\nRegarding optimal curvature dependency, a result exists for the simpler problem of set function optimization under cardinality constraints (Vondrak 2010), showing that $1-c$ is near optimal. Thanks, we now incorporated it in the paper.\n\nVondrak Jan, 2010 Submodularity and curvature: the optimal algorithm"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699967663011,
                "cdate": 1699967663011,
                "tmdate": 1699967663011,
                "mdate": 1699967663011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wAc3Kx6pbB",
            "forum": "loYSzjSaAK",
            "replyto": "loYSzjSaAK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4828/Reviewer_6FV4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4828/Reviewer_6FV4"
            ],
            "content": {
                "summary": {
                    "value": "The submission considers a new framework of Submodular Reinforcement Learning (SubRL), where the total reward is given as a submodular function of given trajectories, rather than as an additive sum of rewards from individual time steps. From my understanding, the main applications of interest would have environments where repeated actions (at the same states) are not so much preferred -- this is explicitly embedded in the reward design itself in SubRL. Contributions are the following:\n\n- While the optimal policy can still be Markovian, the authors first show that approximating the optimal value up to any constant factor is computationally hard in polynomial time (that is, planning is computationally hard). \n\n- Given an additional assumption that the reward function is DR-submodular, AND if the underlying MDP is nearly deterministic, then a constant factor approximation is possible.\n\n- The authors present a policy-gradient type algorithm for SubRL, and demonstrate the effectiveness of the method on several interesting synthetic examples and deep-RL settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The submission introduces a novel and \"mathematically\" interesting framework that accounts for diminishing returns of repeated actions. \n\n- The view of submodular rewards is fresh. The hardness result is new and interesting. \n\n- The selected toy examples sound interesting and well-suited for the proposed framework."
                },
                "weaknesses": {
                    "value": "- I do not see much contribution in positive results. Not only does the assumption sound strong from a practical perspective, but it seems quite contrived only for the sake of analysis. \n\n- Literature review: I agree with the motivation from diminishing returns, but a submodular reward design is not the only way to address that. For example, there is a blocking-bandit style framework that discourages repeated actions [1]. Maybe good to discuss why the submodular reward design is better. \n\nI also encourage authors to survey more existing works that explore similar ideas with submodular reward design. For example, can the authors explain the difference between [2] in terms of the problem setting?\n\n[1] Basu et al., Blocking Bandits, NeurIPS 2019.\n\n[2] Chen et al., Contextual Combinatorial Multi-armed Bandits with Volatile Arms and Submodular Reward, NeurIPS 2018. \n\n- Suggestion: It looks slightly unnatural to have Section 4 (practical algorithms) in between hardness results (Section 3) and positive results (Section 5). A more natural flow would have been having the positive theoretical results first and then presenting practical algorithms, or at least having them connected. \n\n- Overall, I feel that the framework is well-motivated for the \"mathematical\" purpose but less sound for the practical purpose or advancing the theory of RL."
                },
                "questions": {
                    "value": "- I do not understand what it means by $\\pi$ is parameterized by $\\pi^h(a)$ in Theorem 3. Does this mean $\\pi$ does not depend on the state? \n\n- Definition 2 - why is it named \\epsilon-\"Bandit\" SMDP? \n\n- The submission focuses on the \"planning\" side. Any thoughts on the \"learning\" side? (a.k.a., exploration and sample complexity)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4828/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4828/Reviewer_6FV4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805847383,
            "cdate": 1698805847383,
            "tmdate": 1700640304328,
            "mdate": 1700640304328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L7IKnEKBZm",
                "forum": "loYSzjSaAK",
                "replyto": "wAc3Kx6pbB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6FV4"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback. Please see our response below\n\n> Positive results\n\nWe would like to emphasize to the reviewer that we have multiple positive results.\n\nFirstly, we consider a $\\epsilon$-bandit SMDP which is a generalization of the bandit setting. Even under this simplified setting, it is intractable to achieve optimal value. However, SubPO achieves the information-theoretic optimal approximation ratio of $1-1/e$, which motivates a SuBPO-type approach as a sensible approach for Submodular RL.\n\n**Secondly, we would also like to bring the other positive result to the reviewer's attention which is for general SMDPs (i.e, no restrictions on dynamics) in Proposition 3, where, based on the curvature \u2018c\u2019 of the submodular function, we can guarantee that policy $\\pi$ from SubPO achieves $J(\\pi) \\geq (1-c)J(\\pi^\\star)$ where $\\pi^\\star$ is the optimal non-Markovian policy.**\n\nBeyond this, giving any constant factor approximation in full generality is not possible as indicated by our hardness result in Theorem 1. Hence, the two positive results and the hardness result characterize the complete computational complexity of the proposed framework.  More importantly, the algorithm SubPO is applicable to full generality without any sort of assumptions and can be used to optimize experiment design, entropy exploration, coverage, etc. objectives as shown in the experiments.\n\n> Literature review\n\nThanks for sharing [1][2], both of them look very interesting. Firstly, about the reward design, we want to emphasize that submodularity is an equivalent characterization of diminishing returns and hence is very general as mentioned by other reviewers as well. Arguably a problem analogous to blocking bandits can be conceptualized with submodular functions as well. For instance, consider a coverage problem on state time $(v,t)$ pairs, where pulling an arm $v$ at time $t$ covers it for the next $\\tau_v$ times and hence no reward until $t + \\tau_v$ time. If the arm $v$ is pulled after $t + \\tau_v$, it will receive a reward again. Thanks for raising this point, we now included a table in the appendix referenced in the related work section summarizing various applications captured using submodular functions. \n\nSecondly, about the differences with respect to the problem setting, we would like to emphasize to the reviewer that our focus is on MDPs i.e., we have states and need to satisfy transition constraints whereas both of these works ([1][2]) focus on bandits.\n\nThroughout the entire paper, both in theory and experiments, we consider general MDPs. Only in the first half of section 5, we analyze a bandit case to motivate our SubPO algorithm with optimal approximation ratios and show interesting connections to generalize bandits. Although the bandits setting is not our focus, in order to connect with submodular bandits literature, we cite (Streeter & Golovin, 2008; Chen et al., 2017; Yue & Guestrin, 2011) in the last paragraph of related works. We are happy to further include [1].\n\n> General clarification and questions\n\nYes, $\\pi^h(a)$ represents that the considered policies are state-independent and only depend on the horizons. Since we relax the MDP constraints, but yet have a generalization of bandit problems, we call it $\\epsilon$-Bandit SMDP. In particular, if $\\epsilon = 0$, we recover the bandit case. \n\nDifferent communities may use the \u201cplanning\u201d word differently, we would like to emphasize that this work considers a general problem of learning with data (samples) from simulations as done in typical policy gradient algorithms e.g., Reinforce, TRPO, PPO, Soft actor-critic, etc. Currently, our theoretical analysis focuses on computational complexity, and we agree quantifying the statistical complexity (i.e., sample complexity) of SubPO is an interesting future direction. A motivation in this direction can be derived from Yuan, et al. 2022 but is an independent research work of its own.\n\nRui Yuan, Robert M. Gower, Alessandro Lazaric, A general sample complexity analysis of vanilla policy gradient, 2022\n\n**Placement of sections:** Thank you for your suggestion. The positive results are for the proposed algorithm, SubPO,  and are therefore introduced in section 5 following the algorithmic details in section 4. Whereas the hardness result is for the framework (independent of the algorithm), hence presented in section 3 right after the framework is introduced in section 2. Presenting positive results for the algorithm prior to introducing the algorithm itself is difficult.\n\nThank you very much for the valuable feedback and sharing your questions. Please let us know for any further suggestions. We hope that our response clarifies that our focus is on MDPs, the significance of our contributions to both RL theory and practical use cases, and convinces the reviewer that our paper warrants their strong acceptance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699967590261,
                "cdate": 1699967590261,
                "tmdate": 1699984523190,
                "mdate": 1699984523190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LXKXVvV4Kk",
                "forum": "loYSzjSaAK",
                "replyto": "L7IKnEKBZm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4828/Reviewer_6FV4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4828/Reviewer_6FV4"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "I have read the response. I think that this is an interesting submission, and there seems to be a lot more to study in future. I lean toward accept."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640287953,
                "cdate": 1700640287953,
                "tmdate": 1700640287953,
                "mdate": 1700640287953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iHuHqVziza",
            "forum": "loYSzjSaAK",
            "replyto": "loYSzjSaAK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4828/Reviewer_uscY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4828/Reviewer_uscY"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a submodular reinforcement learning (subRL) setting. Different from the existing reinforcement learning settings, they do not assume the rewards are additive. This allows them to work with more general and history-dependent reward models and they characterize these reward models with submodularity. Moreover, they design a policy gradient-based algorithm, called subPO, for  subRL problems by drawing inspiration from the greedy algorithm for classical submodular problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Combining submodularity with reinforcement learning in a generalized way seems highly intuitive that I am surprised it has not been proposed before. This emphasizes the significance of the paper's contribution. The main idea of the paper is a simple yet powerful one. Additionally, the paper is well written and the ideas or conveyed clearly."
                },
                "weaknesses": {
                    "value": "These are more minor suggestions for improvement rather than weaknesses:\n- On the last paragraph of page 1, the adverbs firstly, secondly, thirdly can be just replaced with first, second, and third. Also, we after the firstly should be lowercase.\n- I think there can be a broader discussion of using submodular functions in reinforcement learning setups in the related work section. I am aware that the introduction also mentions some examples of submodular rewards, but I believe it is interesting enough to have its own paragraph in the related work."
                },
                "questions": {
                    "value": "- Are there other attempts of incorporating submodular functions to reinforcement learning problems?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4828/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4828/Reviewer_uscY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837912162,
            "cdate": 1698837912162,
            "tmdate": 1699636466160,
            "mdate": 1699636466160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "igTYQpcnOb",
                "forum": "loYSzjSaAK",
                "replyto": "iHuHqVziza",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uscY"
                    },
                    "comment": {
                        "value": "Thanks, we incorporated the suggestions. We agree that the modelling using submodular rewards deserves its own space.  We had a paragraph saying \u201cExamples of submodular rewards\u201d in section 2 (preliminaries and problem statement). Now, we included a table in the appendix (reference in related works) summarizing different tasks that can be conceptualized with submodular functions.\n\nWe agree with how the reviewer has signified the importance of the contributions. Indeed, there is no attempt with submodular functions in RL, the most general work in this direction still considers planning on graphs.\n\nThank you very much for the positive and valuable feedback."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699967317031,
                "cdate": 1699967317031,
                "tmdate": 1699967317031,
                "mdate": 1699967317031,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xXPsQZppTD",
            "forum": "loYSzjSaAK",
            "replyto": "loYSzjSaAK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4828/Reviewer_u5A8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4828/Reviewer_u5A8"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies submodular reinforcement learning, i.e. reinforcement learning with submodular set reward function that captures diminishing returns. Specifically, this paper has made the following contributions:\n\n- This paper motivates and develops the framework of submodular reinforcement learning.\n\n- This paper derives a lower bound that establishes hardness of approximation up to log factors in general (Theorem 1, Section 3).\n\n- This paper motivates and develops a general algorithm for the considered problem, referred to as Submodular Policy Optimization (SubPO, Algorithm 1). This is a policy optimization algorithm. Provable guarantees are established in some restricted settings (Section 5).\n\n- Extensive and rigorous experiment results are demonstrated in Section 7."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The considered problem is interesting and significant.\n\n- Extensive and rigorous experiment results have been presented in Section 7.\n\n- The paper is well-written in general, and easy to read."
                },
                "weaknesses": {
                    "value": "- The idea behind the proposed algorithm, Submodulr Policy Optimization, is quite straightforward. It is just a relatively straightforward extension of the classical policy optimization algorithm.\n\n- The analysis in Section 5 seems to be very restricted. Could the authors provide a similar analysis in more general settings?"
                },
                "questions": {
                    "value": "- Please try to address the weaknesses listed above.\n\n- It is not clear to me why the authors chose to put the \"Related Work\" section between an analysis section and the experiment section. Probably the authors should put it after Introduction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699259728131,
            "cdate": 1699259728131,
            "tmdate": 1699636466081,
            "mdate": 1699636466081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u9zdMkZmjH",
                "forum": "loYSzjSaAK",
                "replyto": "xXPsQZppTD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer u5A8"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback. Please see our response below,\n\n> Regarding the algorithm Submodular policy optimization (SubPO)\n\nThe key novelty in SubPO is to utilize the marginal gain decomposition for the submodular functions which allows for splitting the objective into per-step contributions that only depend on the history. We exploit this decomposition to obtain an unbiased gradient estimator with submodular (non-additive) reward functions (Theorem 2, proof in Appx C). In contrast to maximizing modular (state) reward in classical PG, the SubPO policy gradient theorem utilizes the effectiveness of greedily maximising marginal gains which is central to the literature on submodular maximization.\n\n> Analysis in Section 5. Can authors provide analysis in a more general setting?\n\nAlthough $\\epsilon$-bandit SMDP restricts the admissible MDPs, the notion nevertheless strictly generalizes the widely considered submodular bandit setting. The provable guarantee here motivates SuBPO as a sensible approach for Submodular RL since SubPO recovers the optimal approximation ratio of $1-1/e$ at least under this simplified SMDP.\n\n**Furthermore, we do provide another result for general SMDPs (general dynamics) in Proposition 3, where, based on the curvature \u2018c\u2019 of the submodular function, we can guarantee that policy $\\pi$ from SubPO achieves $J(\\pi) \\geq (1-c)J(\\pi^\\star)$ where $\\pi^\\star$ is the optimal non-Markovian policy.**\n\nBeyond this, giving any constant factor approximation in full generality is not possible as established by our hardness result in theorem 1.\n\n>  Location of related works\n\nWe appreciate the reviewer's suggestion to bring related work up. However, there is no closely related work, as we address a novel setting and our approach is only distantly related to submodular optimization, convex RL and PG approaches. Instead, we kept it in the end to rather emphasize the connections of our method to different fields. We now included a forward reference to the related works in the introduction section and explained the reasoning for it.\n\nThank you very much for the valuable feedback. We hope our response clarifies the details of the SubPO algorithm, points to our analysis of the general SMDPs, and convinces the reviewer that our paper warrants their strong acceptance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699967159816,
                "cdate": 1699967159816,
                "tmdate": 1699984468291,
                "mdate": 1699984468291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "makWCqSwWQ",
                "forum": "loYSzjSaAK",
                "replyto": "u9zdMkZmjH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4828/Reviewer_u5A8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4828/Reviewer_u5A8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "Thanks for the response! I have read it and prefer to keep my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637949428,
                "cdate": 1700637949428,
                "tmdate": 1700637949428,
                "mdate": 1700637949428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]