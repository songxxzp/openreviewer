[
    {
        "title": "Large Language Models as Optimizers"
    },
    {
        "review": {
            "id": "lEn6qgvQgY",
            "forum": "Bb4VGOWELI",
            "replyto": "Bb4VGOWELI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission358/Reviewer_iHbx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission358/Reviewer_iHbx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the use of large language models (LLMs) as optimizers to address various optimization tasks, particularly those that can be well expressed in natural language. To validate their approach, the authors conducted experiments on linear regression and traveling salesman problems, as well as prompt optimization, where the goal is to find instructions that maximize task accuracy. Results show that the best prompts optimized by this work outperform human-designed prompts by up to 8% on GSM8K and by up to 50% on Big-Bench Hard tasks. Additionally, the authors evaluated the transferability of found prompts to different datasets in the same domain, demonstrating that their optimized prompts outperform baseline prompts on MultiArith and AQuA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work is among the early investigations into an intriguing research question: can LLMs be used for various optimization tasks? \n2. The obtained optimal prompts from the method are both interesting and useful, such as \"Take a deep breath and work on this problem step-by-step\". \n3. The paper writing is clear, and the figures that state the key experiment results and illustrate the method are well-plotted."
                },
                "weaknesses": {
                    "value": "1. What is the key/unique advantage of using LLMs to optimize over some traditional optimization algorithms, especially on classical optimization problems (not prompt engineering)? \n2. For prompt optimization, the optimization process of this work (i.e., directly feeding solution-score pairs, optimization task descriptions, and meta-instructions into the optimizer LLM) is a black box/lacks interpretation, why it is a better prompt optimizer than those new methods that leverage LLMs to explicitly act as mutation and crossover operators, and further optimize the prompt? such as, \n    1. EvoPrompting: Language Models for Code-Level Neural Architecture Search by Chen et al.\n    2. Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers by Guo et al. \n3. To further understand the effect of the purple text in this work, an ablation study may be beneficial for improving the solidness of the results."
                },
                "questions": {
                    "value": "Please see the above weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Reviewer_iHbx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission358/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418439222,
            "cdate": 1698418439222,
            "tmdate": 1700618681908,
            "mdate": 1700618681908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Iplputd41r",
                "forum": "Bb4VGOWELI",
                "replyto": "lEn6qgvQgY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iHbx"
                    },
                    "comment": {
                        "value": "Thank you for your careful reading and thoughtful reviews. Let us address your comments below.\n\n> What is the key/unique advantage of using LLMs to optimize over some traditional optimization algorithms, especially on classical optimization problems (not prompt engineering)?\n\nThe key/unique advantage is that one can use natural language to describe the optimization problem, as discussed in the second paragraph of Section 1: \u201c\u200b\u200binstead of formally defining the optimization problem and deriving the update step with a programmed solver\u201d. This can make optimization more accessible to general users who do not have much domain knowledge of the types of optimization tasks in question, and may also boost productivity of optimization experts who work on these tasks on a daily basis.\n\nMeanwhile, we would like to clarify that the focus of our work is not to solve all classic optimization problems. We evaluated our approach on linear regression and Traveling Salesman Problems only as motivating examples to demonstrate the potential of LLMs for optimization. Instead, our main task is prompt optimization which is a known challenge for classic optimization methods, where the optimization space is in natural language.\n\n> For prompt optimization, the optimization process of this work (i.e., directly feeding solution-score pairs, optimization task descriptions, and meta-instructions into the optimizer LLM) is a black box/lacks interpretation, why it is a better prompt optimizer than those new methods that leverage LLMs to explicitly act as mutation and crossover operators, and further optimize the prompt?...\n\nThanks for pointing out these related works. We have discussed such evolutionary methods in Section 6. Following your suggestion, we also added an empirical comparison with EvoPrompt. **Please refer to our general response for a summary, and we summarized the full results in Appendix M of the updated Supplementary Material.**\n\n> To further understand the effect of the purple text in this work, an ablation study may be beneficial for improving the solidness of the results.\n\nWe have benchmarked the effect of exemplars (i.e., the purple text) in ablation studies. Please refer to Figure A17 in Appendix K of the Supplementary Material. We show that adding exemplars boosts performance, since it offers more context to the optimizer LLM and enables it to generate more relevant instructions (prompts). For example, on BBH tasks, the optimizer LLM found instructions that are tailored to the tasks, as shown in Table A4 and A5 in Supplementary Material. Without exemplars in the meta-prompt, it is impossible for the optimizer LLM to understand the task if the initial prompts are simple and generic."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296603909,
                "cdate": 1700296603909,
                "tmdate": 1700296603909,
                "mdate": 1700296603909,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1ynYt3hcjf",
            "forum": "Bb4VGOWELI",
            "replyto": "Bb4VGOWELI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission358/Reviewer_hsEq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission358/Reviewer_hsEq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use LLMs as optimizers by simply inputting the natural language description of the optimization task, previous steps\u2019 inputs and scores to LLMs. This paper applies such method on prompt search for various LLM tasks and demonstrates its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Good proof of concept. This paper provides concrete evidence that large language models can find the patterns between the inputs and corresponding scores that humans might not be able to find to conduct optimization tasks.\n2. Good use case. Based on such proof of concept, this paper finds a valid use case for the proposed method, on which other traditional optimizations might be difficult to apply, finding the proper prompts for LLM tasks.\n3. Solid experiments on prompt search. Experiments show that it is not random that the proposed method is able to find proper prompts for various tasks which lead to significant performance improvements. The thorough ablations also already provides answers to a lot of concerns."
                },
                "weaknesses": {
                    "value": "Two questions on the ablation study.\n\n1. Numbers of examplers. Did you take the randomness of example picking into consideration? For each run of every setting, do you give the same set of examples? \n2. I noticed that for different tasks, the \u201cbatch size\u201d that works the best can be different (Figure 5, cd). Do you find any obvious patterns on which types of data/tasks prefer a smaller \u201cbatch size\u201d and vice versa?"
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission358/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625363609,
            "cdate": 1698625363609,
            "tmdate": 1699635963100,
            "mdate": 1699635963100,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2hd5i51kE8",
                "forum": "Bb4VGOWELI",
                "replyto": "1ynYt3hcjf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hsEq"
                    },
                    "comment": {
                        "value": "Thank you for your careful reading and thoughtful reviews. Let us address your comments below.\n\n> Numbers of examplers. Did you take the randomness of example picking into consideration? For each run of every setting, do you give the same set of examples?\n\nAt each step, we randomly sample a different set of 3 exemplars to put into the meta-prompt.\n\n- How we chose 3 for the number of exemplars: Our ablation (Figure A17(c) and A17(d) in Appendix K of the Supplementary Material) shows that 3 achieves a good balance between the informativeness of the meta-prompt and the context length constraint. \n- How randomness is useful: We randomly sample exemplars at each step because it reduces the risk of overfitting on a fixed set of exemplars, and reduces the chance of the optimization getting stuck. The latter is because if both the set of exemplars and the top instructions in the meta-prompt remain the same (this more often occurs in later steps when the performance starts to saturate), the meta-prompt will remain the same, and the optimizer LLM may keep generating the same instructions.\n\n> I noticed that for different tasks, the \u201cbatch size\u201d that works the best can be different (Figure 5, cd). Do you find any obvious patterns on which types of data/tasks prefer a smaller \u201cbatch size\u201d and vice versa?\n\nInsightful question. Although setting the batch size of 8 generally achieves the best final performance across models and tasks, some other batch sizes may work equally well in certain cases (like batch size 1 and 2 at the early stage of optimization in Figure 5(d)). In general, the batch size used by our approach can be smaller for easier optimization problems, such as:\n- When the scorer LLM is instruction-tuned, the accuracy landscape will be less bumpy because the model performance is less sensitive to the prompt, thus a smaller batch size would suffice for the accuracy curve to trend up. The bumpiness can be well reflected by the oscillation in the optimization curve: both on the same GSM8K 3.5% training set, the curve on the pre-trained PaLM 2-L scorer (Figure 1(a)) is much more bumpy than the blue dots in Figure 5(a), especially at early steps. This means OPRO on a pre-trained scorer LLM needs a larger batch size (like 8) to succeed. \n- When the task is easier, one doesn\u2019t need to carefully prompt the model to get a good score, meaning the loss landscape is also less bumpy. One example is the difference between Figure 5(c) and 5(d) in the main paper: BBH sports_understanding is easier than GSM8K in that the same text-bison scorer LLM achieves around 30% higher accuracy, close to 100% on sports_understanding, thus a smaller batch size is enough."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296468777,
                "cdate": 1700296468777,
                "tmdate": 1700296468777,
                "mdate": 1700296468777,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fLCRDq9IjJ",
            "forum": "Bb4VGOWELI",
            "replyto": "Bb4VGOWELI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission358/Reviewer_3F2p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission358/Reviewer_3F2p"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Optimization by Prompting (OPRO), a method to use large language models (LLMs) as optimizers for various tasks. The key idea is to describe the optimization problem and provide the model with past solution-score pairs in a meta-prompt. The LLM then generates new candidate solutions based on this context. OPRO is first demonstrated on linear regression and traveling salesman problems. The main application is prompt optimization, where the goal is to find an instructional prompt that maximizes a model's accuracy on a dataset. Experiments optimize prompts for GSM8K and BigBench, starting from poor prompts and showing significant gains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Novel idea of leveraging LLMs' understanding of natural language and few-shot learning abilities for optimization. Enables optimization by simply describing the problem rather than formal specification.\n\n- Demonstrated on diverse tasks - mathematical optimization, prompt optimization. Shows potential breadth of this approach.\n\n- Compelling results on prompt optimization. Optimized prompts substantially outperform human-written prompts, improving accuracy by up to 8% on GSM8K and 50% on BigBench.\n\n- Principled design of the meta-prompt, balancing past solution-score pairs and problem description. Ablations validate design choices.\n\n- Thorough experiments comparing different LLMs, scoring models, prompt positions, and baselines. Shows consistent benefits of OPRO."
                },
                "weaknesses": {
                    "value": "- The biggest limitation is that OPRO's performance looks highly fluctuating. It's unclear if the LLM really finds the so-called optimization \"trajectory\" or just randomly finds a good prompt. The authors should provide more analysis to show that the LLM is indeed learning to optimize.\n\n- Limited exploration on how to provide richer feedback to LLM beyond aggregated scores. It could help address limitations. \n\n- Unclear how sensitive results are to meta-prompt design and hyperparameters like temperature.\n\n- No comparison to other prompt optimization methods. It could better situate contributions.\n\n- Limited analysis. For example, there is no characterization of what makes an effective prompt."
                },
                "questions": {
                    "value": "- Can you clearly state how many optimization steps are performed in each experiment? How does the number of steps affect performance?\n\n- Can you provide an experiment where you generate the same number of prompts in one step as your current experiments, and evaluate them all, and report the best one? This would help clarify whether the LLM is really learning to optimize or just randomly finding a good prompt.\n\n- For prompt optimization, have you experimented with providing more detailed feedback to the LLM beyond aggregated scores? (e.g. accuracy on different example types, common mistakes)\n\n- How does the meta-prompt length affect optimization performance? Is there a sweet spot balancing past solutions and problem descriptions?\n\n- What determines the choices of sampling temperature? Have you tried adaptive temperature schedules?\n\n- What are the limitations on problem complexity that OPRO can handle? Analysis of how performance degrades with complexity?\n\n- Can you better characterize what makes an effective prompt for optimization? Any semantic or syntactic patterns?\n\n- How does OPRO compare to other gradient-free prompt optimization methods? Could be included in experiments.\n\n- Is there any overfitting during prompt optimization? How does test accuracy compare to training accuracy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Reviewer_3F2p"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission358/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814054917,
            "cdate": 1698814054917,
            "tmdate": 1700600257269,
            "mdate": 1700600257269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iFGBP3adK2",
                "forum": "Bb4VGOWELI",
                "replyto": "fLCRDq9IjJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3F2p - Part 1/3"
                    },
                    "comment": {
                        "value": "Thank you for your careful reading and thoughtful reviews. Let us address your comments below.\n\n> The biggest limitation is that OPRO's performance looks highly fluctuating. It's unclear if the LLM really finds the so-called optimization \"trajectory\" or just randomly finds a good prompt. The authors should provide more analysis to show that the LLM is indeed learning to optimize.\n\nWe acknowledge that the optimization curves do not show monotonic increase at every step. Most optimizers don\u2019t, especially when the loss landscape has a more complicated curvature. From our new experiment showing that the one-step prompt generation method you suggested performs much worse than our approach, we observe that LLM gradually improves upon previous prompts based on the optimization trajectory, just like what other optimizers do.\n\nIn addition, in figures showing the accuracy at each step (Figure 1 in main paper; Figures A11, A12, A13 in Supplementary Material), we presented the *average* (instead of the maximum) accuracy of 8 instructions generated at each step, which showed generally upward trends. This shows the optimizer LLM gradually generates *distributionally* better instructions. Furthermore, Figure 5(c) and 5(d) showed that with the same number of total evaluated instructions, generating 8 instructions per step (default setting) achieved better performance than generating 16 instructions per step, demonstrating that the LLM learns to optimize prompts from previous attempts better with more iterations of update, instead of just randomly finding a good prompt by sampling more instructions.\n\n> Limited exploration on how to provide richer feedback to LLM beyond aggregated scores. It could help address limitations...For prompt optimization, have you experimented with providing more detailed feedback to the LLM beyond aggregated scores? (e.g. accuracy on different example types, common mistakes)\n\nAs noted in the discussion of future work in Section 7 (Conclusion), \u201cwe tried including error cases in the meta-prompt rather than randomly sampling from the training set at each optimization step, but the results are similar, indicating that the error cases alone are not informative enough for the optimizer LLM to grasp the cause of the wrong prediction\u201d. As also noted in Section 7, we agree that improving our approach to better incorporate more detailed feedback is a promising direction for future work. \n\nThat said, we show that our simple meta-prompt still demonstrates significant improvement across different tasks and LLMs, and we see simplicity as a key advantage of our method.\n\n> Unclear how sensitive results are to meta-prompt design and hyperparameters like temperature.\n\nPlease see our responses to your respective questions in [Part 2/3](https://openreview.net/forum?id=Bb4VGOWELI&noteId=BRP1leKDJZ).\n\n> Limited analysis. For example, there is no characterization of what makes an effective prompt.\n\nWe have studied both aspects in the ablation studies. Please see Section 5.3 in the main paper and Appendix K in the Supplementary Material for ablations on how different parts of the meta-prompt and temperature matter.\n\n> No comparison to other prompt optimization methods. It could better situate contributions.\n\nThanks for your suggestion. We added an empirical comparison to EvoPrompt, which is an evolutionary method pointed out by Reviewer iHbx. **Please refer to our general response for a summary, and we summarized the full results in Appendix M of the updated Supplementary Material.**\n\n> Can you clearly state how many optimization steps are performed in each experiment? How does the number of steps affect performance?\n\nWe run a maximum of 200 steps in every experiment although many converge much sooner. The training accuracies trend upwards until plateaued, so the performance generally improves with more number of steps. As shown in Figure 1 in the main paper and Figures A12, A13 in the Supplementary Material, the performance often plateaus within 100 steps.\n\n**(to be continued in Part 2/3)**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295813867,
                "cdate": 1700295813867,
                "tmdate": 1700297177011,
                "mdate": 1700297177011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "66gra2P6dL",
                "forum": "Bb4VGOWELI",
                "replyto": "fLCRDq9IjJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3F2p - Part 3/3"
                    },
                    "comment": {
                        "value": "**(following Part 2/3)**\n\n> How does OPRO compare to other gradient-free prompt optimization methods? Could be included in experiments.\n\nWe added an empirical comparison to EvoPrompt [1], an evolutionary method pointed out by Reviewer iHbx. **Please refer to our general response for a summary, and we summarized the full results in Appendix M of the updated Supplementary Material.**\n\n> Is there any overfitting during prompt optimization? How does test accuracy compare to training accuracy?\n\nGreat question, we\u2019ve studied overfitting at an early stage of this work, and the results from there influenced our final approach. **We added Appendix L in the Supplementary Material with training and validation accuracy curves.** To summarize, we observe that the trends of training and validation accuracies are consistent with each other throughout the optimization.\n\nReferences:\n\n[1] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang. Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. arXiv:2309.08532."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296329079,
                "cdate": 1700296329079,
                "tmdate": 1700296795955,
                "mdate": 1700296795955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZIITnECDCa",
                "forum": "Bb4VGOWELI",
                "replyto": "rmbZu8Ja4D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_3F2p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_3F2p"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "Thank the authors for the detailed responses. My concerns have been well addressed, and I have raised my recommendation for acceptance. Congratulations on your work!\n\nReviewer 3F2p"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600237733,
                "cdate": 1700600237733,
                "tmdate": 1700600237733,
                "mdate": 1700600237733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zQXNjgMbnX",
            "forum": "Bb4VGOWELI",
            "replyto": "Bb4VGOWELI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose large language models (LLMs) as optimizers for various tasks by instructing the models through natural language prompts. Currently, optimization problems have to be explicitly defined, and algorithms are tailored and fine-tuned for specific tasks, which can be challenging and time-consuming. This approach proposes Optimization by PROmpting (OPRO), which leverages the adaptability and versatility of LLMs by modifying the problem description in the prompt, enabling simple and effective optimization for different tasks.\nThe significant result is that OPRO can lead to better performance on selected language tasks, outperforming human-designed prompts by up to 8% on GSM8K and 50% on Big-Bench Hard tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work demonstrates that LLMs can help optimize prompts to achieve high performance on a variety of tasks."
                },
                "weaknesses": {
                    "value": "First, I disagree with the authors fundamentally about what optimization means. To me, this work is not optimization but step-by-step inference. To quote Wikipedia for reference, optimization is \"the selection of a best element, with regard to some criterion, from some set of available alternatives.\" One can plausibly consider the process of prompt selection as \"optimization\", but in order to make a claim on the general area of optimization I would expect results on optimizing a wide range of convex functions and non convex functions as opposed to word problems. The claim on linear regression as an important result is a relevant but very limited result, but it is not included in the main paper.\n\nSecond, considering the relevance of this approach to step-by-step inference, what is new here compared to previous step-by-step inference procedures? The authors include them in the introduction as evidence for LLM is capable of doing multi-step reasoning, but do not distinguish differences b/w this work on prior work. Also they do not compare to these prior work."
                },
                "questions": {
                    "value": "What is the difference between this work and other step-by-step inference techniques such as https://arxiv.org/pdf/2205.11916, https://arxiv.org/abs/2201.11903, https://arxiv.org/abs/2305.10601\n\nHow do the results compare?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission358/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865094363,
            "cdate": 1698865094363,
            "tmdate": 1700349703022,
            "mdate": 1700349703022,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lguDh4vdWj",
                "forum": "Bb4VGOWELI",
                "replyto": "zQXNjgMbnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer um42"
                    },
                    "comment": {
                        "value": "Thank you for your careful reading and thoughtful reviews. Let us address your comments below.\n\n> First, I disagree with the authors fundamentally about what optimization means. To me, this work is not optimization but step-by-step inference...\n\nFirst, we would like to clarify that the focus of our work is not to solve all kinds of classic optimization problems. We evaluated our approach on linear regression and traveling salesman problems only as motivating examples to demonstrate the potential of LLMs for optimization. Instead, our main task is prompt optimization which is a known challenge for classic optimization methods, where the optimization space is in natural language.\n\nMeanwhile, optimization is a very general concept. As classic optimizers such as gradient descent and Adam, our approach proposes better solutions in an iterative manner, step-by-step. Specifically, instead of updating the current solution with a mathematical rule (like in momentum or Adam) or by inferencing a surrogate model (like in Bayesian optimization), we prompt the LLM to get the next-step solution, hence \u201cLLMs as optimizers\u201d. From this perspective, prompt optimization is also an optimization problem: the objective is to maximize the task accuracy within the huge natural language space.\n\n> Second, considering the relevance of this approach to step-by-step inference, what is new here compared to previous step-by-step inference procedures?...What is the difference between this work and other step-by-step inference techniques such as https://arxiv.org/pdf/2205.11916, https://arxiv.org/abs/2201.11903, https://arxiv.org/abs/2305.10601 How do the results compare?\n\nThese works are complementary to our work. All 3 works used *manually written prompts* to improve LLMs\u2019 performance, while our method focuses on *how to automatically find such prompts via optimization* and in many cases we can find even better prompts compared to the manually written ones in these existing works. \n\nMore specifically, https://arxiv.org/pdf/2205.11916 shows LLMs can do zero-shot chain-of-thought prompting; https://arxiv.org/abs/2201.11903 shows chain-of-thought prompting improves LLMs\u2019 reasoning performance; https://arxiv.org/abs/2305.10601 shows tree-of-thought improves the performance on several tasks that require planning and search. These papers are about manually improving the LLM performance with manually written prompts, while our approach is on prompt optimization/tuning. For example, our work showed how our method greatly improved zero-shot chain-of-thought prompting."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295183443,
                "cdate": 1700295183443,
                "tmdate": 1700296827440,
                "mdate": 1700296827440,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i1MjMcGW7z",
                "forum": "Bb4VGOWELI",
                "replyto": "lguDh4vdWj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "> First, we would like to clarify that the focus of our work is not to solve all kinds of classic optimization problems. We evaluated our approach on linear regression and traveling salesman problems only as motivating examples to demonstrate the potential of LLMs for optimization. Instead, our main task is prompt optimization which is a known challenge for classic optimization methods, where the optimization space is in natural language.\n\nI agree with your qualification. But the manuscript is written to present the contribution as one to optimization, not prompt optimization. Both the abstract and the introduction lead with optimization as a motivation. \n\n> Meanwhile, optimization is a very general concept. As classic optimizers such as gradient descent and Adam, our approach proposes better solutions in an iterative manner, step-by-step. Specifically, instead of updating the current solution with a mathematical rule (like in momentum or Adam) or by inferencing a surrogate model (like in Bayesian optimization), we prompt the LLM to get the next-step solution, hence \u201cLLMs as optimizers\u201d. From this perspective, prompt optimization is also an optimization problem: the objective is to maximize the task accuracy within the huge natural language space.\n\nPrompt optimization is one optimization problem, but showing results for one optimization problem does not make a convincing argument for a general optimization contribution. Like the other review said, this work does not show evidence on, say, classical optimization tasks.\n\n> These works are complementary to our work. All 3 works used manually written prompts to improve LLMs\u2019 performance, while our method focuses on how to automatically find such prompts via optimization and in many cases we can find even better prompts compared to the manually written ones in these existing works.\n\nThese techniques are methods to automatically create prompts, not manually written prompts. For instance, copying execution outputs into the prompt (e.g. ReAct) requires no \"manual writing\". I would argue that the proposed method is very similar to copying execution outputs (in this case solution-score pairs) into the input."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322035499,
                "cdate": 1700322035499,
                "tmdate": 1700322035499,
                "mdate": 1700322035499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kkTgCrDfUj",
                "forum": "Bb4VGOWELI",
                "replyto": "dhU5J8Wd1F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Reviewer_um42"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you. I believe I had fundamentally misunderstood how the metaprompt is created in this work. After reading the authors' description of few-shot prompt curation and zero-shot prompting, I do think there is some distinction between this and the ReAct body of work. That said, I think the degree of \"manual prompting\" between \"let's think step by step\" (e.g. zero-shot chain-of-thought) and \"increase the score\" (e.g. this work) is the same. Finally, given the relevance of these works, I think the authors should explore the similarities and differences between the proposed method and these works in the manuscript.\n\nI will increase my score. However, I still take issue with the paper's presentation and framing, which poses the proposed method as a general-purpose optimization method. If this is the path the authors intend to go, then I would suggest putting the linear regression results into the main body, and presenting some more difficult optimization problems (e.g. one with more dimensions and nonlinearity such as MLP on MNIST)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700349614806,
                "cdate": 1700349614806,
                "tmdate": 1700349614806,
                "mdate": 1700349614806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JW6waCsZlX",
                "forum": "Bb4VGOWELI",
                "replyto": "zQXNjgMbnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission358/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Thanks again for your review and your follow-up discussion! Please let us know if we have adequately addressed your questions and concerns."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599774304,
                "cdate": 1700599774304,
                "tmdate": 1700600609018,
                "mdate": 1700600609018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]