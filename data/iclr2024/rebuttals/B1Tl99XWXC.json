[
    {
        "title": "Efficient Transfer Learning in Diffusion Models via Adversarial Noise"
    },
    {
        "review": {
            "id": "wJVzH4xMhy",
            "forum": "B1Tl99XWXC",
            "replyto": "B1Tl99XWXC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4872/Reviewer_dyX4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4872/Reviewer_dyX4"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, authors propose a novel DPMs-based transfer learning method, TAN, to address the limited data problem. It includes two strategies: similarity-guided training, which boosts transfer with a classifier, and adversarial noise selection which adaptive chooses targeted noise based on the input image. As illustrated in the paper, authors think they haved achieved SOTA results compared with prior works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The paper introduces a binary classifier to guide the training and an adversarial way to generate noises, this idea is interesting.\n2.The exploration of overfitting in Fig.1 is helpful for the few-shot learning of generative models."
                },
                "weaknesses": {
                    "value": "1.The quantitative results reported for different methods have large overlap when factoring in the std values, in general, the boldfacing of average values when ignoring the stds is not the best practice.\n2. I have read several related works and find almost all of them show use samples transferred from FFHQ as qualititative results. I found that the results of this paper on FFHQ are only shown in Supp. I cannot figure out the improvement of this method compared with DDPM-PA on those results. Actually, I think DDPM-PA shows better results.\n3. Results in this paper should be compared with more modern text-to-image methods based on diffusion models, including textual inversion, dreambooth, domainstudio. If this method is only applied to traditional methods, it's not convincing enough.\n4. In LSUN Church --> Landscape drawings, it seems that DDPM-PA carries out a style transfer process, this work fails to get samples of church actually. Therefore, I wonder if this comparison is fair. For FFHQ --> babies and sunglasses, this work and DDPM-PA share the same target. However, I think DDPM-PA performs better."
                },
                "questions": {
                    "value": "My main concern is about the performance and applicable scenarios. See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698038584499,
            "cdate": 1698038584499,
            "tmdate": 1699636471341,
            "mdate": 1699636471341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0NpmxZDZno",
                "forum": "B1Tl99XWXC",
                "replyto": "wJVzH4xMhy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review and feedback. \n\n## 1. **Overlap of Results**\n- **Overlap**: The phenomenon of overlapping results, as suggested by the standard deviation values, is a widespread challenge in this area of study, not just specific to our research. This issue is also evident in methodologies such as CDC, DCL, and DDPM-PA. For example, the Intra-LPIPS result of DDPM-PA on FFHQ $\\to$ Babies is 0.599 $\\pm$ 0.024 and LSUN Church $\\to$ Haunted houses is 0.628 $\\pm$ 0.029, while the Intra-LPIPS result of CDC on FFHQ $\\to$ Babies is 0.583 $\\pm$ 0.014 and LSUN Church $\\to$ Haunted houses is 0.620 $\\pm$ 0.029.\n- **P Value**: We also present a new table about p-value between our method and DDPM-PA to demonstrate the tangible improvement of our approach. The Inter-LPIPS score, our primary metric, often faces an overlap problem due to the highly competitive advancements in this field. Nevertheless, the p-value table still illustrate significant improvements of our method despite these constraints.\n|  Datasets | Babies | Sunglasses | Raphael's paintings | Haunted houses | Landscape\n|-----|:-----:|:-----:|:-----:|:-----:|:-----:|\n| P-value (AP with TAN) | $2.6 \\times 10^{-6}$ | $1.9 \\times 10^{-25}$  | $7.0 \\times 10^{-54}$  | $8.7 \\times 10^{-86}$  | $9.5 \\times 10^{-48}$  |\n\n## 2. **Qualitative Results on FFHQ**\n- In Figure 3 of the main paper, we have already presented samples transferred from the FFHQ dataset, providing a qualitative comparison with DDPM-PA and other methods. The FFHQ $\\to$ Raphael's paintings transformation showcases the strengths of our method, particularly in managing complex features. Our approach achieves more accurate color reproduction and fewer artifacts compared to DDPM-PA. We would greatly appreciate it if the reviewer could share any relevant references, enabling us to thoroughly examine and pinpoint any potential discrepancies or overlooked aspects in our approach.\n\n\n## 3. **Comparison with Modern Text-to-Image Methods**\n- **Different Tasks**: The tasks of text-to-image transfer learning and few-shot image generation are fundamentally different in their objectives and methodologies.\nText-to-image transfer learning primarily focuses on subject-specific generation.\nIt aims to capture the essential features of a target domain while maintaining diversity, even with a limited set of images.\nRenowned methods in this field, such as CDC, DCL, and DDPM-PA, have not typically been applied to text-to-image conditional generation tasks.\nFor instance, Dreambooth fine-tunes an existing text-to-image model using a small set of subject-specific images, enabling the model to form a distinct association with that subject.\nIn contrast, few-shot image generation concentrates on preserving similarities and differences among instances in the source domain, without a specific focus on maintaining the primary subject of the target images.\n\n- **Dreambooth Results**: Our new experiment, illustrated in Appendix Figure 7, reveals the difficulties Dreambooth faces in few-shot generation tasks, as the generated images closely resemble the target images.\nAdditionally, it's challenging to conduct a fair comparison between our method and text-to-image approaches.\nThese text-to-image methods typically require pre-training on large text and image paired datasets, enabling them to generate images of the target domain without fine-tuning.\nFor example, the model used in Dreambooth can create images of people wearing sunglasses without any fine-tuning.\nThe goal of Dreambooth's fine-tuning is to produce images that resemble the training data but set against different backgrounds."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192079739,
                "cdate": 1700192079739,
                "tmdate": 1700192156523,
                "mdate": 1700192156523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bnv6jDxfE7",
                "forum": "B1Tl99XWXC",
                "replyto": "wJVzH4xMhy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "I hope this message finds you well. I am writing to express my gratitude for your thoughtful review and feedback on our work. We have carefully considered your comments and have prepared a detailed rebuttal to address your queries and concerns.\n\nIn our rebuttal, we have clarified the quantitative improvement demonstrated by our results, differentiated our approach from current text-to-image methods, and validated the fairness of our comparative analyses with comprehensive statistical and visual evidence.\n\nWe carried out an additional anonymous user study to assess the qualitative performance of our method in comparison to DDPM-PA.\nIn this study, participants were shown three sets of images from each dataset, featuring DDPM-PA, our method (DDPM+TAN), and images from the target domain.\nFor each set, we displayed five images from each method or the target image, as illustrated in our main paper.\nTo maintain anonymity and neutrality, we labeled the methods as A/B instead of using the actual method names (PA and TAN).\nWe recruited volunteers through an anonymous online platform for this study. During the study, participants were tasked with choosing the set of images (labeled as A or B, corresponding to PA or TAN) that they believed demonstrated higher quality and a closer resemblance to the target image set.\n\n|  Methods  | Sunglasses | Babies | Landscape  | Raphael's paintings | Average\n|-----|:-----:|:-----:|:-----:|:-----:|:-----:|\n| PA | 20.0% | 33.3% | 20.0%  | 33.3%  | 26.65%  |\n| TAN | 80.0% | 66.7% | 80.0%  | 66.7%  | 73.35%  |\n\nOf the 60 participants, a significant 73.35% favored our method (DDPM+TAN), indicating that it produced images of superior quality and more effectively captured the intricate types of target domains. While this experiment did not comprehensively account for factors such as the participant's gender, age, regional background, and others, the results nonetheless suggest that our images possess better visual quality to a notable extent.\n\nWe kindly request you to review our rebuttal your feedback is invaluable to us, and we would greatly appreciate your insights on our responses.\n\nThank you for your time and consideration."
                    },
                    "title": {
                        "value": "Additional User Study on Qualitative Performance"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561578834,
                "cdate": 1700561578834,
                "tmdate": 1700620704829,
                "mdate": 1700620704829,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jpksiH2YtW",
            "forum": "B1Tl99XWXC",
            "replyto": "B1Tl99XWXC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4872/Reviewer_ft3z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4872/Reviewer_ft3z"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an innovative approach to transfer learning in diffusion models with sparse target data. To tackle the limited data problem, the authors propose TAN, a new diffusion-based method including two strategies: similarity-guided training, which aims to enhance transfer with a classifier, and adversarial noise selection to improves the efficiency of training process. The authors conducted extensive experiments in the context of few-shot image generation tasks and demonstrated that their method is not only efficient but also excels in terms of image quality and diversity when compared to existing GAN-based and DPM-based methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper is well written, easy to follow and conducts a lot of experiments  with comprehensive analysis, demonstrating the effectiveness of the proposed approach.\n2.\tThe design of the similarity-guided training and adversarial noise is intuitive and reasonable."
                },
                "weaknesses": {
                    "value": "1.\tThe graphical representation of the results doesn't clearly demonstrate a significant improvement compared to other existing methods.\n2.\tRegarding the approach to adversarial noise selection for Eq. 7 and Eq. 8, could you provide further clarification on the choice of minimizing the maximum Gaussian noise at step t? Additionally, since optimizing for maximum noise can be particularly challenging, could you delve deeper into how the multi-step variant of PGD with gradient ascent ensures the performance of this methodology? Expanding on these aspects would significantly enhance the comprehensibility and value of this paper.\n3.\tThe paper lacks an in-depth exploration of the implications for time and GPU memory conservation when utilizing the supplementary adaptor module and the process of adversarial noise selection. To enhance the overall quality of the paper, I suggest providing a more comprehensive elucidation, along with relevant graphical representations, regarding the efficient training procedures involving these components."
                },
                "questions": {
                    "value": "My main concerns and questions lie in the weaknesses. The author should discuss them in detail."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4872/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4872/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4872/Reviewer_ft3z"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663982555,
            "cdate": 1698663982555,
            "tmdate": 1699636471225,
            "mdate": 1699636471225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U26lKDRF74",
                "forum": "B1Tl99XWXC",
                "replyto": "jpksiH2YtW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review and feedback on our work. We will address each of your queries, clarifying any misunderstandings and delving into pivotal academic questions that merit exploration. We are also pleased to share some of our insights with you.\n\n### 1. **Qualitative Performance and Comparison with Existing Methods**\n- **Illustration of Performance**: Figure 3 in our study showcases our method's notable advancements over existing approaches, particularly DDPM-PA. Our technique effectively captures the intricate type of target domains. This is clearly visible in the top and bottom rows of the figure, where our method (4th and 5th images) aligns more closely with the target domain's type than DDPM-PA (3rd image).\n\n- **Specific Examples**: We provide a detailed comparison of certain features, such as the watercolor brush strokes in the up images of churches and mountains, and the red hat or yellow curly hair in the bottom row images. These examples further highlight our method's enhanced ability to adapt to the target domain's type, offering a significant improvement over the results achieved by DDPM-PA.\n\n### 2. **PGD Step Number and Adversarial Noise Selection**\n- **Min Max at Timestep t**: As detailed in Equation 8 of the main paper, we employ a multi-step variant of Projected Gradient Descent (PGD) for the inner maximization. This approach helps in identifying the \"worst-case\" adversarial noise, which corresponds to the current state of the neural network. Subsequently, we use this adversarial noise as the \"maximum\" noise input to minimize our model's loss, as outlined in Equation 9 of our paper.\n- **Ensures the Performance**: The choice of maximizing Gaussian noise at step t is strategically aligned with our aim to challenge the model with the \u201cworst-case\u201d scenarios. This approach ensures that the model is robust enough to handle the most difficult noise patterns, thereby improving overall performance. As demonstrated in Section 5.1 of the main paper, the Adervsrial noise with PDG can rectify the issue of incorrect gradients caused by limited data. Figure 2 (a) in main paper shows the output layer gradient direction for four different settings in the first iteration, all with identical noise and timestep $t$.\nThe red line, derived from ten thousand varied samples, serves as a dependable reference direction (approximately 45 degrees southwest).\nFor comparison, 10-shot samples are repeated a thousand times in one batch, aligning them with the ten thousand distinct samples.\nThe dark blue and sienna lines represent the gradient computed using traditional DDPM as the baseline and similarity-guided training in a 10-shot sample, respectively.\nThe orange line illustrates our method, DDPM-TAN, applied to a 10-shot sample.\nThe gradient with adversarial noise (represented by the orange line) is closer to the reliable reference direction than the gradient without adversarial noise (illustrated by the sienna line). This proximity indicates the method's effectiveness in ensures the prformance.\n\n### 3. **Time and GPU Memory Conservation**\n- **GPU Memory**: To further elucidate these aspects, we will include graphical representations in the revised version of the paper, showcasing the efficiency gains in training procedures involving these components. The Table in Appendix A.5 illustrates the GPU memory usage for each module in batch size 1, comparing scenarios with and without the use of an adaptor. It reveals that our module results in only a slight increase in GPU memory consumption.\n- **Time**: In Section 5.3 of our main paper, we discuss the time efficiency of our method. Our approach not only effectively but also efficiently transfers the model from the source to the target domain.\nCompared to other methods requiring about 5,000 iterations, our method needs only approximately 300 iterations with limited parameter fine-tuning. The incorporation of Training Adversarial Noise (TAN) further enhances efficiency. Effectively, our approach equates to 3,300 training iterations, significantly less than the standard 5,000 iterations and the 6,000 - 10,000 equivalent iterations needed for DDPM-PA.\nIn terms of time cost, the baseline with an adaptor undergoing 5,000 iterations (the same as DDPM-PA) consumes about 4.2 GPU hours. In contrast, our model (DPMs-TAN) requires just 3 GPU hours for its 300 iterations, demonstrating notable time efficiency."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191914846,
                "cdate": 1700191914846,
                "tmdate": 1700191914846,
                "mdate": 1700191914846,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E1gC4MwFbU",
            "forum": "B1Tl99XWXC",
            "replyto": "B1Tl99XWXC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4872/Reviewer_Pd9e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4872/Reviewer_Pd9e"
            ],
            "content": {
                "summary": {
                    "value": "Generative models can leverage large-scale datasets to learn and produce diverse high-quality outputs. To overcome the limitations of this approach, methods for transfer learning from models trained on extensive datasets have been studied. However, diffusion-based generative models, unlike directly step-wise generative models like GANs, generate samples through the diffusion of noises over a large number of steps, making it challenging to apply conventional transfer learning methods. Therefore, in this paper, a method called TAN, which utilizes adversarial noises to effectively transfer learning to diffusion models. \n\nGenerating high-quality and diverse results with generative models comes with the challenge of obtaining a large amount of training data and stabilizing the training process. In this regard, transfer learning and few-shot learning research are important, yet relatively less explored. Previous studies attempted transfer learning by matching the results of intermediate steps of DDPM, but they led to distorted transfer and overfitting issues. the proposed approach, incorporating similarity-guided training and adversarial noise selection techniques, yielded well-transferred results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes simple yet effective methods, namely similarity-guided training and adversarial noise, for transfer learning with diffusion models. \n\nThe proposed methods yielded high-quality results and were demonstrated through various experiments. \n\nThe paper presents equations 4 and 5, which induce the Kullback-Leibler divergence between the source and target models during the reverse process of the diffusion model. \n\nThis divergence is defined as similarity and utilized to control transfer learning.\n\nThe proposed similarity-guided approach by the authors not only induces overall transfer for the target domain but also considers the characteristics of the diffusion generative model. \n\nBy utilizing adversarial noise selection, the method allows the noise to fit more accurately, resulting in high-quality output and addressing the issues faced by previous methods. \n\nThe paper presents a cohesive and easily understandable writing flow, effectively conveying the underlying arguments and the rationale of the study."
                },
                "weaknesses": {
                    "value": "I think it would be better if there is deeper analysis regarding the similarity or adversarial noise in the proposed method. It would be even better if there were various experiments and analyses to examine the semantic effects of the redefined reverse step compared to the vanilla model. I am curious about the author's insights beyond mathematically deriving the KL divergence between the source and target domains, exploring different aspects."
                },
                "questions": {
                    "value": "1. I am curious if this method can be applied not only to transfer learning or few-shot learning but also to fields such as domain adaptation or domain control.\n\n2. I wonder if the adversarial noise in this method can assist in achieving higher-quality results in the training of general DDPMs.\n\n3. I'm curious if this method has been experimented with in domains other than 2D images."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829316842,
            "cdate": 1698829316842,
            "tmdate": 1699636471132,
            "mdate": 1699636471132,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EhmiyOYzI6",
                "forum": "B1Tl99XWXC",
                "replyto": "E1gC4MwFbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review and feedback on our work. We will address each of your queries, clarifying any misunderstandings and delving into pivotal academic questions that merit exploration. We are also pleased to share some of our insights with you.\n\n### 1. **Additional Insights and Analysis**\n- **Further Analysis and Visualization**: In response to your request for deeper analysis, we provide a new experiment in Appendix A.4 about generated during each iteratuions as our exploration of similarity and adversarial noise.\nThis include more detailed visualizations, especially comparing the reverse image of our method with that of the standard model with the different training iteraion step.\nAs shown in Figure 5, the first row demonstrate that the orangial train the DPMs with limited iterations is hard to get a successfully transfer. The second raw shows that training with our similarity-guide method can boost the convergence to the taget domain. The third rows shows that training further with adversrial noise can even more faster converge. As shown the 150 iteration of right pictures, compared with the training only with similarity-guide (2nd row) TAN can get the face with sunglasses image.\n<!-- This expanded analysis aims to provide more comprehensive insights that our method can boost the transfer learning. -->\n- **KL Divergence Analysis**: Our methodology employs KL divergence between the source and target domains as a fundamental analytical tool.\nThis approach aids in understanding the differences between well-established source and target models and also provides insights into effectively utilizing similarities in transfer learning for DPMs.\nWe initially leverage existing knowledge by fine-tuning a diffusion model derived from a pre-trained model.\nTherefore, examining the differences between the pre-trained model in the source domain and the fine-tuned model in the target domain is essential\nFurthurmore, equation 5 in our paper, $$\\texttt{D}_{\\textrm{KL}} \\left( p_{\\theta_{\\textrm{S}}, \\phi}(x_{t-1}^{\\textrm{S}} | x_{t}), p_{\\theta_{\\textrm{T}}, \\phi}(x_{t-1}^{\\textrm{T}} | x_{t}) \\right)\n= \\mathbb{E}_{t, x_0, \\epsilon} \\left [ \\left\\| \\nabla_{x_t} \\log p_{\\phi}(y=\\textrm{S}|x_t) - \\nabla_{x_t} \\log p_{\\phi}(y=\\textrm{T}|x_t) \\right\\|^2 \\right ]$$, demonstrates how the divergence depends on $x_t$ at specific timesteps $t$.\nThis highlights that transfer learning in DPMs can efficiently leverage this divergence for more effective training.\n\n### 2. **Application Beyond Transfer and Few-shot Learning**\n- **Domain Adaptation, Control and Other Domains**: Our method, centered on adversarial noise and similarity-guided training, can indeed be extended beyond transfer and few-shot learning.\nIt holds potential in domain adaptation and control tasks, especially where the target domain has limited data but is somewhat related to a well-trained source domain.\nOur current focus has been predominantly on 2D images.\nHowever, the underlying principles of our method are adaptable to other domains.\nThank you for your suggestion. We intend to explore the application of our technique across various fields and domains, including domain adaptation and 3D modeling, among others. This will enable us to fully utilize the versatility of our approach in our future work.\n\n### 3. **Adversarial Noise in General DPMs Training**\nOur method is applicable in general DPMs training, but the effectiveness of adversarial noise might be reduced during the initial unstable gradient phase and when dealing with large datasets.\n\n- **Training From Scratch**: PGD may not be effective in the initial stages of the training process.\nThe model's gradient is typically unstable when training begins from scratch, and adversarial noise requires a relatively accurate model gradient to function effectively.\nIf the initial model's gradient is unstable, it fails to provide beneficial adversarial noise for training.\n\n- **Large Data Sets**: The application of adversarial noise has substantially enhanced the training of DPMs under conditions of limited data and iterations.\nAs shown in Figure 2 (Page 6) of our paper, the adversarial noise method helps address incorrect gradients, which is advantageous for training on smaller datasets.\nFor larger datasets, however, the benefits of our approach might be less noticeable.\nThis is due to the extensive iterations required for model convergence, as represented by the cyan line (associated with 10,000 points data) in Figure 2.\n\nTherefore, although adversarial noise shows promising performance in few-shot transfer learning, its effectiveness may be less pronounced in general DPMs training."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191876282,
                "cdate": 1700191876282,
                "tmdate": 1700191876282,
                "mdate": 1700191876282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iac1PNx9OL",
                "forum": "B1Tl99XWXC",
                "replyto": "E1gC4MwFbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "I hope this message finds you well. I am writing to express my gratitude for your thoughtful review and feedback on our work. We have carefully considered your comments and have prepared a detailed rebuttal to address your concerns.\n\nIn our rebuttal, we clarify our methodology's nuances with added experiments and visualizations. Our discussion extends to the method's applicability beyond transfer and few-shot learning. Additionally, we explore the impact of adversarial noise in training general DPMs, considering various training scenarios and dataset sizes. We kindly request you to review our rebuttal and your feedback is invaluable to us, and we would greatly appreciate your insights on our responses.\n\nThank you for your time and consideration."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620820045,
                "cdate": 1700620820045,
                "tmdate": 1700621017313,
                "mdate": 1700621017313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eGfR3wzW2q",
                "forum": "B1Tl99XWXC",
                "replyto": "iac1PNx9OL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4872/Reviewer_Pd9e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4872/Reviewer_Pd9e"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments for my opinion. The additional experiments and also analysis can be helpful to my understaning. so I would like to keep my rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729538043,
                "cdate": 1700729538043,
                "tmdate": 1700729538043,
                "mdate": 1700729538043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VKi00ZcwaV",
            "forum": "B1Tl99XWXC",
            "replyto": "B1Tl99XWXC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4872/Reviewer_jP4S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4872/Reviewer_jP4S"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Diffusion Probabilistic Model (DPM)-based transfer learning method, i.e., to address the limited data problem. Specifically, similarity-guided strategy is designed to enhance the few-shot transfer learning and an adversarial noise selection method is proposed to address the underfitting problem during a limited number of iterations. Both qualitative and quantitative results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written with clear description.\n\nIt analyzes the limitations of DPM-based transfer learning and presents reasonable solutions."
                },
                "weaknesses": {
                    "value": "How does the pre-trained DPMs come and if the proposed method can be applied on other pre-trained DPMs for transfer learning?\n\nIf the authors could provide visualizations on the selected noise for deeper analysis on its effect?\n\nThe step number used to perform the projected gradient descent (PGD) in Eq.(7)?"
                },
                "questions": {
                    "value": "Please refer to the Weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698863692923,
            "cdate": 1698863692923,
            "tmdate": 1699636471037,
            "mdate": 1699636471037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZcPRUvVzVl",
                "forum": "B1Tl99XWXC",
                "replyto": "VKi00ZcwaV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review and feedback on our work. We will address each of your queries, clarifying any misunderstandings and delving into pivotal academic questions that merit exploration. We are also pleased to share some of our insights with you.\n\n### 1. **Pre-trained DPMs**\n- **Clarification on Pre-trained DPMs**: We provide additional details in Section 5.2 (Experimental Setup) about the pre-trained DDPM (Denoising Diffusion Probabilistic Models)[1] and LDM (Latent Diffusion Models)[2]. The pre-trained DDPM are from [1] and pre-trained LDM are from [2].\n- **Generalization of Our Method**: Our method is general on different DPMs' pre-trained model since our method focuses on the how to transfer based on DPMs' theories instead of the structure of neural networks.\nOur method's generalizability is demonstrated through its successful application to two well-known DPM pipelines, DDPM and LDM. This highlights the method's versatility and wide applicability in various transfer learning scenarios within the DPM framework.\n\n### 2. **Visualizations of Adversarial Noise**\n- **2D Visualization**: The background red point of Figure 2 (a) (page 6) presents a 2D visualization of adversarial noise, showing a transition from a standard Gaussian distribution (circular shape) to an elliptical one. The elliptical shape aligns with the gradient direction of the model's parameters. This illustrates how our approach fine-tunes the model by focusing on the most challenging noise scenarios.\n- **Challenges in Visualizing High-Dimensional Adversarial Noise**: Visualizing high-dimensional adversarial noise, especially for real images involving vectors of 3 * 256 * 256 dimensions, is significantly challenging. These visualizations can be less intuitive and may struggle to convey critical information due to their complexity and high dimensionality.\n\n### 3. **PGD Step Number in Adversarial Noise Selection**\n- **PGD Step Number**: In all experiments presented in our paper, we set the PGD step number to \\( J = 10 \\) to demonstrate the generalizability of our method, as detailed in subsection 5.2 of the main paper.\n\n[1] Ho, J., Jain, A. and Abbeel, P., 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, pp.6840-6851.\n\n[2] Rombach, R., Blattmann, A., Lorenz, D., Esser, P. and Ommer, B., 2021. High-resolution image synthesis with latent diffusion models. 2022 IEEE. In CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 10674-10685)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191851281,
                "cdate": 1700191851281,
                "tmdate": 1700191851281,
                "mdate": 1700191851281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wPf5jCZfue",
                "forum": "B1Tl99XWXC",
                "replyto": "VKi00ZcwaV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4872/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "I hope this message finds you well. I am writing to express my gratitude for your thoughtful review and feedback on our work. We have carefully considered your comments and have prepared a detailed rebuttal to address your concerns.\n\nIn our rebuttal, we clarify the origins and features of the pre-trained DPMs utilized in our study. We also provide details on our visualization techniques for adversarial noise. Additionally, we specify the PGD step number used in our experiments, highlighting the robustness and versatility of our approach. We kindly request you to review our rebuttal and your feedback is invaluable to us, and we would greatly appreciate your insights on our responses.\n\nThank you for your time and consideration."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620624364,
                "cdate": 1700620624364,
                "tmdate": 1700621032038,
                "mdate": 1700621032038,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]