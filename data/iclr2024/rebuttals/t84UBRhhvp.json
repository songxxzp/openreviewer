[
    {
        "title": "Text Descriptions are Compressive and Invariant Representations for Visual Learning"
    },
    {
        "review": {
            "id": "MNAknO0QYV",
            "forum": "t84UBRhhvp",
            "replyto": "t84UBRhhvp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6628/Reviewer_rjB2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6628/Reviewer_rjB2"
            ],
            "content": {
                "summary": {
                    "value": "This paper is in line with a recent trend of augmenting CLIP's classification templates with class-specific descriptions generated from LLMs   (eg, GPT-3). The major technical contribution of this work is to concatenate the descriptions of all classes of interest, learn weights to aggregate these descriptions' text features and form new classifiers for each class. The weights are regularized with l1-norm to encourage sparsity. Experiments show that tuning with this classifier outperforms methods that work directly on top of vision features (without texts). Besides, information-theoretical analysis is also provided to show the improved invariance of text features (in CLIP's joint VL space) over raw vision features (output of the vision tower prior to linear projection)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "*Originality*: The technical contribution is relatively limited, which still fits in the scope of description selection. Its combination with other schemes like LP-FT is somehow instrumental. Yet the theoretical analysis is relatively novel and inspiring.\n\n*Clarity*: The paper is overall clearly written. Yet the organization could be improved, and more details could be provided to help understand details of the proposed method.\n\n*Significance*: This paper fits in the scope of visual representation learning under text supervision, and provides analysis on feature invariance and compression, which is helpful for the community."
                },
                "weaknesses": {
                    "value": "I put the minor concerns in the section above (which did not harm my rating much), and list the major concerns here:\n\n1\\) The theoretical analysis is not well-aligned with the proposed method:\n- The major conclusion that could be derived from sec 3.3 is that features from CLIP's joint VL space are more compressed and invariant to visual variations (compared with vision features).\n- This is good and helps us understand CLIP itself, but does not explain why using descriptions (no matter w/ or w/o selection) is better than using other texts (eg, the default templates), which is the foundation of the proposed method.\n- From fig. 2 I find $I(H_{avd}; A)$ is almost identical to $I(H_{cp}; A)$, indicating descriptions do not introduce more invariance than template ensemble, which raises concern on why descriptions are needed in this work. $I(H_{avd}; Y)$ is higher than $I(H_{cp}; Y)$, which should indicate better predictions, yet in tab. 1 the gain of ZS-AVD over ZS is just marginal.\n\n2\\)  The experiments also do not support this work (description selection)'s superiority over other template designs:\n- In tab. 2 & 3, SLR's superiority over FT & LP under the few-shot setting is expected, since both WISE-FT's $W_\\text{learned}$ and LP's classifier are trained from scratch given only very few samples, thus could not match the performance of classifiers derived from CLIP's text encoder.\n- Yet this does not help understand SLR's strength over other classifiers. For instance, a\\) what if we drop the selection process and just follow Menon & Vondric's average ensemble, b\\) what about WaffleCLIP [1]'s random descriptors, c\\) how much is SLR's improvement over CLIP's default templates, and d\\) how do k-NN classifiers (use average pooling of the labeled few-shot samples to form classifiers, try both vision features and VL features) perform?\n\n3\\) No ablation study is provided to understand the proposed method (relatively minor)\n\nRef:\n[1] Roth et al., Waffling around for Performance: Visual Classification with Random Words and Broad Concepts, ICCV'23"
                },
                "questions": {
                    "value": "One possible cause of ZS-VD's much inferior performance than ZS in tab.1 is the use of class templates. If the templates of ZS are also applied to ZS-VD, the difference in their performance should be marginal (refer to WaffleCLIP). I suggest the authors give it a try."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6628/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6628/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6628/Reviewer_rjB2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6628/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672102314,
            "cdate": 1698672102314,
            "tmdate": 1699636757075,
            "mdate": 1699636757075,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5ff5G2ZQFZ",
                "forum": "t84UBRhhvp",
                "replyto": "MNAknO0QYV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6628/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6628/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful comments. Our responses are below.\n\n### Weaknesses\n\n1 It is actually a favorable result that $I(H_{avd}, A)$ is almost identical to $I(H_{cp}, A)$. One can think of $H_{cp}$ as a postprocessing of $H_{avd}$ (since the former is a subset of the latter). This result suggests that even if $H_{avd}$ has more features, these extra features do not harm invariance. The difference between $I(H_{avd}, Y)$ and $I(H_{cp}, Y)$ is reflected in the newly revised Figure 3. There the L1 class prompts perform far worse than AVD. \n\n2.1 Even though SLR\u2019s features are derived from text encoders, its coefficients are not regularized to be close to ZS-AVD. Instead, we only enforce sparsity. Therefore, the model can still be wrong in arbitrary ways. The results suggest that the discrete text space has a stronger implicit bias than the image embedding space.\n2.2 the average ensemble corresponds to ZS-VD or ZS-AVD. Notice that without WISE-FT, SLR doesn\u2019t really incorporate the strong language prior. Since only sparsity is enforced, the coefficients can still overfit the few-shot data in almost arbitrary ways. One can of course overcome this issue by regularizing the difference between the SLR coefficients and the zeroshot weights, which is almost what WISE-FT does. Compared to regularization during SLR training, WISE-FT is post hoc. So we only need to train the model once. We choose to train this way because it helps us understand the implicit bias of each space (image embedding space vs the discrete text space that is congruent to natural language), rather than the effect of the zeroshot prior.\n\nWe add WaffleCLIP zeroshot performance in Table 12, and we notice that its performance tends to decrease as you add more random tokens. On the other hand, our descriptors have length around 8-10 tokens. This suggests that the most important factor is to perturb your basis around the class prompt in a controlled way, which is more easily achieved by using language congruent descriptions. We also compare WaffleCLIP+WISE-FT to SLR+WISE-FT. The result in figure 15 also suggests that SLR has a better performance than WaffleCLIP.\n\n### Questions\n\nIn Table 12, we also tried to average the templates for ZS-VD, the results are listed in column ZS-VD$^2$. We see a slight improvement. \n\nWe also want to remark that the gap between WaffleCLIP and our ZS baseline is because we use the hand-selected 7 templates later released in the OpenAI CLIP github repo, and the original WaffleCLIP paper randomly selects 30 out of the original 80 templates for ensembling."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514414611,
                "cdate": 1700514414611,
                "tmdate": 1700519486001,
                "mdate": 1700519486001,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IlhbHBE5PP",
                "forum": "t84UBRhhvp",
                "replyto": "5ff5G2ZQFZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6628/Reviewer_rjB2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6628/Reviewer_rjB2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the reply. Still, my major concerns are not well-resolved.\n\nConsidering methodology as one core contribution of this paper, given its negligible performance gain, the reason that one prefers it over other practices, eg, simply ZS, is unclear and unconvincing. I raised WaffleCLIP for attention as it challenged the use of descriptions by showing even random words can perform comparably, for the message but not for its performance. The fact this work can perform better than random words is unsurprising.\n\nConsidering the information-theoretic analysis, it is good to see progress in this direction. Still, it only justifies the goodness of previous works, but not this one itself. The fact it \"does not harm invariance\" is insufficient to support it as a more preferable method.\n\nOverall, I prefer to keep my initial recommendation unchanged."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644056391,
                "cdate": 1700644056391,
                "tmdate": 1700644056391,
                "mdate": 1700644056391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LcomefRDmK",
            "forum": "t84UBRhhvp",
            "replyto": "t84UBRhhvp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6628/Reviewer_CDXQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6628/Reviewer_CDXQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for obtaining more robust CLIP models. First, multiple visual descriptions are generated by an LLM and then used by the CLIP model alongside encodings of the class names. The visual representation is projected into the class + descriptors space and then a sparse logistic regression is trained on this representation.  As shown by experiments estimating mutual information, projecting on the class embeddings and visual descriptions minimizes the mutual-info with the domain, while selecting sparse features follows the bottleneck principle. Experimentally, the method is shown to produce better results than good baselines and can be combined with existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* S1. The method is well-motivated.\n\n* S2. The method is sound. Both the visual descriptions and sparsity are good ways to improve robustness."
                },
                "weaknesses": {
                    "value": "* W1. The paper already compares against a few methods, but some additional baselines and ablations would also help. First, the full method but with logistic regression instead of sparse logistic regression (this will show if the sparsity is really important). The same method without using the video descriptions (partially shown in Table 1). Second, an MLP with a similar number of parameters as SLR-AVD, with different levels of weight decay. Third, use random projections / learnable matrices instead of the U projections (this will also induce a bottleneck). \n\n* W2. It will be good to have an overall comparison with different methods and an ablation study of the proposed method. I am thinking mainly of using the same base model (e.g. ViT- B/16), maybe just some standard few-shot k (e.g. k=16). A table with all datasets as columns and different methods (ZS, LP, MLP probing, FullFinetunning, CoOp, Wise) and ablations (ZS-AVD, SLR-AVD etc.).\n\n* W3. It will be good to see the performance of *all* models presenting using different number of shots: 1,2,4,8,16, 32. At the moment some ablations contain only up to 4 or 16 shots.\n\n* W4. The paper focuses on few-shot learning and this is an important area. To see the tradeoffs of the proposed method and the baselines, it will be interesting to also see the performance using larger training sets. For example, use k from 64, 256, 1024, etc. This way we can see at which scale of training samples is the proposed method more beneficial.\n\n\n* W5. Comparison with CoOP does not seem to be fair. \u201cSince CoOp injects \u201cclassname\u201d to the prompt during inference directly, this enforces a very strong prior. For a fair comparison, we also inject a strong prior by interpolating our learned linear head\u201d. It is not clear what this means, and why a direct comparison is not fair. Why is the \u201cclassname\u201d injection a strong prior for CoOP? Isn\u2019t SLR-AVD also using the same classname to produce the class prompts (CP)? Combining the proposed method with Wise and comparing to plain CoOp seems unfair.\n\n* W5.2. The comparison with CoOP is made using Resnet-50, which gives poorer performance for CLIP. Comparison using the ViT models should also be made.\n\n* W6. It is not clear how hyperparameter selection is done. Hyperparameter and model selection are crucial for domain generalization, thus this should be made more clear. For Wise models, alpha seems to be selected optimally, using validation OOD data.\n\n* W7. In Figure 4 top, it seems like WISE-FT+LP, which finetunes only the last linear layer, is compared against WISE-FT+SLR-AVD, which finetunes the entire model. Is this correct, or is WISE-FT+SLR-AVD finetunning only the linear classifier? Both should either update the linear layer or full-finetuning. Also, what is the difference between WISE-FT+LP and WISE-SLR?\n\n\n* W8. The paper will benefit from a better presentation. There are multiple acronyms, and sometimes the difference between them is not clear. The section in the appendix explaining the acronyms should be expanded with more details and should contain all acronyms and combinations used (e.g. WISE-FT+SLR-AVD, WISE-FT+SLR-AVD). \n* W8.2 Minor: Figure 4 should be improved, e.g. use consistent symbols, especially for start and end points. Show the optimal checkpoint in the figure."
                },
                "questions": {
                    "value": "Q: What is the number of learnable parameters of SLR-AVD, how does it compare to linear probing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6628/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776862780,
            "cdate": 1698776862780,
            "tmdate": 1699636756954,
            "mdate": 1699636756954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V1lxSOwk1P",
                "forum": "t84UBRhhvp",
                "replyto": "LcomefRDmK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6628/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6628/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their positive review of the paper and we hope that the following comments address their outstanding concerns.\n\n### Weaknesses\nW1. We have added comparisons to several of the methods you suggested.\n\nW1.1 We have added L2-regularized AVD (i.e. L2 regularized logistic regression replacing L1/sparse logistic regression) to show that L1 regularization is very important. \n\nW1.2 For random projection, we initialize $U\\in R^{512 \\times 300}$, where 512 is the image embedding dimension. We pick the projected dimension to be $300$ since we found that  the number of parameters picked by L1 is around 100-300 per class. This bottleneck underperforms our method. See Figure 3 for a detailed comparison.\n\nW1.3 We have also added a comparison to MLP in Table 13 in the appendix. The MLP has 3 layers of sizes 512-4500-1000, which equals the raw number of parameters in SLR. It consistently underperforms linear models because the CLIP embeddings are trained to be linearly classified, and MLP can easily overfit.\n\nW2. We added the comparison in Table 13 in the appendix for k=4, since this is the k that we have used for every model.\n\nW3. We have added the comparison to CoOp on 32 shots, see figure 15 in the appendix. For Full FT, we are limited by our resources.\n\nW4. See Fgure 16 in the appendix for larger k. We tried k=64, 256, 1024. L1 AVD still outperforms linear probing on most datasets.\n\nW5. The reason behind this statement is that CoOp optimizes a continuous prompt like \u201cX X X X {classname}\u201d for each class, and only the \u201cX X X X\u201d part is optimized in the continuous space; the \u201c{classname}\u201d is always fixed. On the other hand, the coefficients of SLR-AVD are not regularized until it incorporates WISE-FT. To prevent confusion, we change Table 5 so that CoOp also incorporates WISE-FT, and SLR-AVD still outperforms CoOp, especially when the amount of data increases. \n\nW5.2. CoOp comparison now is done on ViT-B/16.\n\nW6. This is one drawback that we inherit from WISE-FT. The nice property of WISE-FT is that it\u2019s fully post hoc. So in reality, even if the $\\alpha$ is not optimally picked initially, it can always be easily updated. The original WISE-FT method suggests a heuristic of choosing $\\alpha=0.5$. However, their original setting has abundant ID data, and 0.5 fails in our case. Our heuristic suggests that picking $\\alpha=0.05k$ seems to perform pretty well.\n\nW7. WISE-FT+SLR-AVD only finetunes the last layer. WISE-SLR is the method that first finds a sparsity pattern using SLR-AVD. Then we fixed the pattern and finetuned the whole model. WISE-FT+LP is linear probing with image embeddings +WISE-FT on only the last linear layer.\n\nW8. We added more details in the appendix, and also Table 6 for a better visualization.\n\nW8.2 We updated the figures. As for optimal checkpoint, it is a little ambiguous since sometimes the optimal ID acc and OOD acc are achieved by slightly different $\\alpha$, so we decide not to include it.\n\n### Questions\n\nQ1. The total learnable parameters in SLR-AVD are 6804\\*1000. The average number of non-zero entries for each class after learning is $447, 248, 182, 177, 173$, and $135$ for $k=1,2,4,8,16,32$. The numbers are rounded to the nearest integers. So the total number of \u201cused\u201d parameters is the aforementioned numbers * 1000 (for each class). Linear probing optimizes 512\\*1000 parameters. So SLR does in the end lead to much less parameters. Also, as the training data increases, SLR-AVD can eliminate more irrelevant descriptors."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514292912,
                "cdate": 1700514292912,
                "tmdate": 1700519412522,
                "mdate": 1700519412522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OnTIytdML9",
            "forum": "t84UBRhhvp",
            "replyto": "t84UBRhhvp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6628/Reviewer_cpSM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6628/Reviewer_cpSM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to produce better zero-shot classifiers for vision-language models, more specifically CLIP.\nTo do so, the set of standard class prompts used to construct zero-shot classifiers is enhanced using GPT-3 with extra textual descriptions.\nMoreover, $\\ell_1$ regularized logistic regression classifier is trained to select certain textual descriptions for each class.\nThis way, slightly better performance is achieved on ImageNet datasets compared to the standard prompts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "One of the strengths of this paper is the idea of sparse selection of automatically generated prompts for classes (for which we want to learn a zero-shot classifier).\nThere are several prompt templates, which are provided as input to GPT-3 to retrieve textual descriptions for classes.\nThen $\\ell_1$-regularized logistic regression is trained to select the most discriminative prompts for each class.\nThis strategy brings slight performance boost over manually constructing only a few (if not one) class prompts."
                },
                "weaknesses": {
                    "value": "There are two main concerns that I would like to raise.\n\n1) The benefit of automatically generating prompts for obtaining zero-shot classifiers is not very clear and significant. Table-1 compares the proposed method of generating textual desciptions from GPT-3 (ZS-AVD) against using only manually defined class prompts (ZS), and we see marginal improvements (max a few decimal points). It would be nice to see the impact of the number and diversity of generated prompts into performance. Section 4.1 mentions that \"...class names are probably one of the strongest prompts... One can certainly try to improve ZS-VD results by more carefully prompting GPT-3, or gathering descriptors from different data sources/search engines.\" this contradicts with the motivation of the paper, no?\n\n2) The benefit of using $\\ell_1$-regularized logistic regression technique is not very clear either. It would be nice to see if simple $\\ell_2$ regularization performs the same, or what kind of prompts selected for certain classes using different regularization criterons. Also, a simple k-NN based approach can also be applied as a baseline both with soft or hard assignment. On the other hand, similar logistic regression can be trained also for ZS (where there can be multiple manually defined class prompts). Maybe the impact is only due to learning weights ($W$) which connect the two modalities (image and text) using some regularization technique.\nSection-5 mentions that \"Applying sparse logistic regression then successfully selects the important features, which turn out to be intuitive\" but we don't see any evidence, right?\n\nMinor comments:\n- 1st sentence of Introduction: \"Self-supervised vision-language models (VLMs) like CLIP...\" is there any reference claiming CLIP to be self-supervised?\n- 2nd sentence of Section-2: \"WLOG...\" what is WLOG?\n- Missing closing paranthesis in Figure-1\n- 4th paragraph of Section-3.1 (\"Denote M ...\") is very confusing. It would be nice to explain all $U$ and $W$ in a diagram/visualization.\n- Figure-3 caption \"the x-axis represents...\" (not y)\n- Figure-3 what is the label for y-axis?"
                },
                "questions": {
                    "value": "I would like the authors to address the concerns I listed in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6628/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698864077995,
            "cdate": 1698864077995,
            "tmdate": 1699636756832,
            "mdate": 1699636756832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "37qndvppHQ",
                "forum": "t84UBRhhvp",
                "replyto": "OnTIytdML9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6628/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6628/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and we hope that the following points address the concerns they brought up.\n\n### Weaknesses\n1. The main motivation is that we want to use multiple features to learn a classifier in the presence of few shot data. If we only have one prompt for each class, then there is no candidate to learn from (except using the vanilla image embeddings). An important finding is that the visual descriptions perturb the original class prompt in a specific way such that the perturbed features have nice information theoretic properties. See the newly added Table 12 in the appendix for a comparison to WaffleCLIP, which appends random words to the class prompts. Our visual descriptors have length around 8-10 tokens, and WaffleCLIP\u2019s performance decreases as the random token length increases. WaffleCLIP only does well when a couple words are added. This small perturbation is beneficial even if the words are irrelevant. However, when more words (8-10) are added, WaffleCLIP performance decreases, while our method can pick useful descriptors of this length that constructively enhance the original class prompt.  \nWe also have a small scale experiments on CIFAR10 in the appendix, paragraph \"Choosing $\\gamma$ and LLM prompting\". There we found the most important factor is to generate more diverse descriptors by setting the frequency penalty.\n\n2. See revised figure 3. L1 consistently leads to better performance than L2. Sparsely chosen class prompts perform worse than AVD on most datasets. These results still hold when combining with WISE-FT, see figure 15 in the appendix. All results indicate that choosing features with L1 is important.\n\n\n\n### Minor comments:\n-\"1st sentence of Introduction: \"Self-supervised vision-language models (VLMs) like CLIP...\" is there any reference claiming CLIP to be self-supervised?\u201d \nWe changed this to \u201cnatural language supervised.\u201d\n\n-\"2nd sentence of Section-2: \"WLOG...\" what is WLOG?\u201d\nThis stands for \u201cwithout loss of generality.\u201d We modified the text to clarify this term.\n\n-\"Missing closing paranthesis in Figure-1\u201d\nNoted - we fixed this in the paper. \n\n-\"4th paragraph of Section-3.1 (\"Denote M ...\") is very confusing. It would be nice to explain all $U$ and $W$ in a diagram/visualization. \"\nWe added a visualization of $U, W$ at the end of the appendix\n\n-\"Figure-3 caption \"the x-axis represents...\" (not y)\u201d and \u201cwhat is the label for y-axis?\u201d\nThe y-axis refers to test accuracy; we have modified the text accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514108646,
                "cdate": 1700514108646,
                "tmdate": 1700519374583,
                "mdate": 1700519374583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zFNuMLqXAi",
            "forum": "t84UBRhhvp",
            "replyto": "t84UBRhhvp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6628/Reviewer_piqv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6628/Reviewer_piqv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model SLR-AVD which first automatically generates visual descriptions of each class via a LLM, then use a VLM to translate these descriptions to a set of viaul feature embeddings of each image. The features are proved to be more invariant to domain shift than traditional image embeddings with information-theory. The SLR-AVD is validated on both in-distribution and out-of-distribution classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed model is novel, which extracts multiple potential visual features of each class, and then uses L1-regularized logistic regression to fit a sparse linear classifier on top of these visual descriptions. The generated descriptive features are proved to retain substantial information about the true labels, making them good invariant representations."
                },
                "weaknesses": {
                    "value": "The paper writing and the experiments need improvement. \n\n1. The training and inference process is not clear. \n- What is the loss function used to train the model?\n- The three W matrices W_{vd}, W_{cp}, and W_{avd} are used as zero-shot classifiers. However, the inference process of the zero-shot classifier is not clearly explained.\n- In Section 3.2 paragraph 2, it would be better to mathematically specify how to regularize W_{avd}, and how to pick three features for each class with the largest coefficients.\n2. The experiments show limited performance gain in zero-shot classification.\n- the results in Table 1 indicate that ZS-VD performs worse than ZS. ZS-AVD only provides marginal improvement over ZS, i.e., 0.74 (IN), 0.74 (IN-V2), 0.13 (IN-R), 0.23 (IN-A), 0.27 (ObjectNet).\n3. In Figure 3, it would be interesting to show the performance of the ZS-VD model.\n4. In section 4.2, which dataset is the OOD test set?\n\nI am looking forward to the authors' responses and would like to adjust my rating if the questions are properly addressed."
                },
                "questions": {
                    "value": "Figure 4 is unclear and the sub-caption in the bottom-right corner is blocked."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6628/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699099300526,
            "cdate": 1699099300526,
            "tmdate": 1699636756688,
            "mdate": 1699636756688,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cfYF191pHl",
                "forum": "t84UBRhhvp",
                "replyto": "zFNuMLqXAi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6628/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6628/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful comments. Please see our responses below.\n\n### Weaknesses\n\n1.1 The model is trained by the normal multiclass cross entropy loss + L1 regularization.\n\n1.2 For any zeroshot matrix $W\\in R^{|C|\\times k}$, the inference is done by taking $\\arg\\max_{i\\in[C]} (WUx)_i$, note that $WUx\\in R^{|C|}$ for each $W, U$ with subscripts cp, vd, and avd, respectively.\n\n1.3 The particular algorithm we use to perform cross-entropy loss minimization with L1 regularization is a mini-batch invariant of SAGA, which is a first-order method, detailed in [1]. The appendix paragraph \u201chyperparameter\u201d gives a brief introduction to the method used.\n\n2 While the improvement observed is relatively modest, we believe the exploration of learning with descriptive features is still noteworthy. The application of information theoretic results in this context is itself valuable, as it provides a useful framework for analyzing these features. Furthermore, the MI-based analysis offers a convincing explanation for the strong performance and robustness of both AVD and the zeroshot baseline, which is an interesting aspect on its own.\n\n3 The results are added, see the updated figure 3. VD performs worse than AVD but better than class prompts, suggesting the descriptors are useful.\n\n4 The OOD datasets are ImageNet-A (an adversarial dataset), ImageNet-R (only contains arts and paintings etc), ImageNet-Sketch (only contains sketch), ImageNet-V2 (natural distribution shift), and ObjectNet.\n\n### Questions\n\nFigure 4 has been updated according to the comments. \n\n[1]: Optimal mini-batch and step sizes for SAGA\n\nWe appreciate the reviewer's willingness to update their score and would be happy to provide any more information as needed to inform this decision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513754801,
                "cdate": 1700513754801,
                "tmdate": 1700519323169,
                "mdate": 1700519323169,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T5XoxECZgD",
                "forum": "t84UBRhhvp",
                "replyto": "cfYF191pHl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6628/Reviewer_piqv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6628/Reviewer_piqv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. Unfortunately, my major concern is not fully addressed as the experiments show limited performance gain in zero-shot classification.\n\nI would like to keep the original rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6628/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662058241,
                "cdate": 1700662058241,
                "tmdate": 1700662058241,
                "mdate": 1700662058241,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]