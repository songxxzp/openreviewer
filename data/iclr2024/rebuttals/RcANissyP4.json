[
    {
        "title": "SelfEval: Leveraging the discriminative nature of generative models for evaluation"
    },
    {
        "review": {
            "id": "rCMhZ8CDzr",
            "forum": "RcANissyP4",
            "replyto": "RcANissyP4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an automatic evaluation method by measuring text-image faithfulness, showing an agreement with human evaluations in multiple benchmarks. By arguing that CLIP-score has severe drawbacks in DrawBench, for example, they argued that their method, SelfEval, solves or detours these issues, hoping for its role as a reliable and easy-access method to evaluate multimodal diffusion models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- They showed the limitation of CLIP-score in quantitative evaluation with extensive analysis. \n\n- One may exploit a strong text-to-image generativel model to evaluate others."
                },
                "weaknesses": {
                    "value": "- W1. Novelty. The motivation of their method is reminiscent of the CLIP-R-Precision of Park et al. (2021), where the goal is to pick the right caption among distractors. Although the authors used the likelihood estimations to get a score for text-image matching, it can be seen as an ad-hoc method to replace the cosine similarity in CLIP. Since we cannot control the pretrained models where one is CLIP and the other one is diffusion models, the authors need to study the effectiveness of the proposed method extensively. The limitation of the CLIP score was also explicitly explored in MID [2].\n\n- W2. State-of-the-art comparison. For the text-image faithfulness, you could compare the state-of-the-art performance with DALL-Eval [1], MID [2], LLMScore [3], and VPEval [4]. The paper failed to cite them. Please allocate the dedicated paragraph in Sec. 2 to include them and compare the related works.\n\n[1] Cho, J. et al. (2022). DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers. http://arxiv.org/abs/2202.04053\n\n[2] Kim, J.-H. et al. (2022). Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. Advances in Neural Information Processing Systems 35. http://arxiv.org/abs/2205.13445\n\n[3] Lu, Y. et al. (2023). LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation. http://arxiv.org/abs/2305.11116\n\n[4] Cho, J. et al. (2023). VPGen & VPEval: Visual Programming for Text-to-Image Generation and Evaluation. https://arxiv.org/pdf/2305.15328.pdf"
                },
                "questions": {
                    "value": "Could you let me know why you excluded the closely related works mentioned in Weaknesses W2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698036027370,
            "cdate": 1698036027370,
            "tmdate": 1700702773687,
            "mdate": 1700702773687,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kWfiJUKolA",
                "forum": "RcANissyP4",
                "replyto": "rCMhZ8CDzr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M8yh"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable time and effort. We resolve some confusions and address your concerns below.\n\n**Clarification about SelfEval**\n\n- We wish to clear the confusion of the reviewer regarding their statement that a stronger text-to-image model can be used to evaluate other models. SelfEval eliminates the need for any external model by converting a generative model ($p(x,y)$) to a discriminative model ($p(y|x)$) and evaluate the generative capabilities using its discriminative performance (proportionality via Bayes\u2019 Rule). We DO NOT use one text-to-image model to evaluate others in this work.\n\n**Weakness**\n\n- We point out some important distinctions of our work from CLIP R-Precision. CLIP R-precision measures the text retrieval performance given a **generated** image whereas in our case, we use the **ground truth** image-text pairs for evaluation. Unlike such embedding-based metrics, our evaluation is absolute, since models are always evaluated against ground truth. This along with the splits introduced in our work, provide a diagnostic tool for analyzing generative models. Another key takeaway and novelty of SelfEval is the fact that a generative model can be used to evaluate itself. To the best of our knowledge, this capability has never been shown before and differentiates our work from other evaluation metrics proposed in literature. \n\n- Limitations of CLIP were highlighted in MID [2] but their analysis is limited and doesn\u2019t show clearly how this affects the final evaluation. In our work, we show the limitations of CLIP\u2019s text understanding and how using different CLIP models affects the actual evaluation. \n\n- We believe [4] was made public AFTER the ICLR paper submission deadline. Except method [2], none of the other works are published in literature. Moreover, methods [1-3] use external models (like CLIP) for evaluation and as mentioned throughout our paper, using external models requires additional error analysis to prevent any biases. With SelfEval, we aim to eliminate any external model for evaluating text faithfulness of generative models and we believe it is unfair to compare SelfEval against these methods. We will add a separate section in the related works to discuss these methods."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163131384,
                "cdate": 1700163131384,
                "tmdate": 1700163131384,
                "mdate": 1700163131384,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TKoD6RvYup",
                "forum": "RcANissyP4",
                "replyto": "rCMhZ8CDzr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ],
                "content": {
                    "title": {
                        "value": "Feedbacks"
                    },
                    "comment": {
                        "value": "Thanks for the clarification, but I mean that's a strong point in your work since you repurpose the text-to-image generative model to a discriminative model for evaluating its generative capabilities; if you're targeting the stronger text-to-image generative model ($p(x,y)$), you can have more strong evaluation quality by assuming that its discriminative ($p(y|x)$) performance is stronger.\n\nI acknowledged that the related works [1,3,4] have not been published yet, although LLMScore [3] will be published in NeurIPS 2023, and the ICLR 2024 reviewing guidelines have loose rules for it. Still, you could compare your work with MID [2] along with CLIP, and the failure of this undermined the comparative effectiveness of your work. Nevertheless, I highly recommend faithfully comparing with those related works."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447543460,
                "cdate": 1700447543460,
                "tmdate": 1700703144474,
                "mdate": 1700703144474,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tgApNSUYjl",
                "forum": "RcANissyP4",
                "replyto": "rCMhZ8CDzr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ],
                "content": {
                    "title": {
                        "value": "Additional feedbacks for the comparison"
                    },
                    "comment": {
                        "value": "I appreciate your endeavor to provide the requested comparison and acknowledge that MID requires a sufficient number of comparative samples like FID.\n\n* I need more elaboration on the details of the experiments. \n  - \"In both the tables, for MID, lower score is better\" -> for the MID score, the higher wasn't better? \n  - \"SelfEval is the accuracy over chance\" -> how do you get the accuracy? What brings you to this conclusion? \n  - I only understand that Humans A/B in the tables means that the number of humans voting for the generations from the \"A\" model is A, and for the generations from \"B\" is B.\n\n* Do you assume that since B is more voted than A by humans, the evaluation metrics should vote for B? A or B cannot give consistent generation performance for every sample, so you should consider the sample-wise preferences of humans. For this reason, many related works resort to Kendal-tau correlations to see if a model follows human judgments for the samples. Since the reports missed key details and ambiguity, my interpretations are limited. Please correct me if there's a misunderstanding."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632095405,
                "cdate": 1700632095405,
                "tmdate": 1700632545422,
                "mdate": 1700632545422,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nXxjnn5dLg",
                "forum": "RcANissyP4",
                "replyto": "rCMhZ8CDzr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ],
                "content": {
                    "title": {
                        "value": "Feedbacks to the response"
                    },
                    "comment": {
                        "value": "* Your argument that Likert scale-type ratings can be subject to bias is reasonable, although a sufficient number of samples would show better human judgment correlations for the better model. \n\n* I agree with the authors that the proposed model has advantages over previous works, including MID, since the evaluation is not biased from the other pretrained models when there's no significant bias in the generative model itself.\n\n* Unfortunately, I cannot see that the proposed method outperforms or is on par with MID. In the above Tbl 1 & 2, if I'm correctly interpreting, both MID and SelfEval roughly follow human voting tendencies. However, since MID and SelfEval's metrics are different, the MID score and the Accuracy Over Chance, respectively, we cannot compare directly. Couldn't you convert MID scores to the Accuracy Over Chance compatibly? Please don't mix up different metrics in one table. Because the authors argued that MID is subject to the pretrained model's bias, you can *easily* show that SelfEval may outperform MID, but the current experiment cannot show it, making the work incomplete. \n\n* After discussion, I cannot help to raise my score but remain leaning toward weak rejection due to the above concern."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702479991,
                "cdate": 1700702479991,
                "tmdate": 1700702662587,
                "mdate": 1700702662587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1jlVHeecwj",
                "forum": "RcANissyP4",
                "replyto": "rCMhZ8CDzr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ],
                "content": {
                    "comment": {
                        "value": "In Tbl. 3 of the MID paper, they compared in accuracy. In a similar way, you could get the accuracy using MID. \n\nAs Sec. 3.3 in this manuscript stated that you measured the accuracy by picking the correct caption for a single image $x$ from a set of captions $\\\\{c_i\\\\}$, you could measure the MID scores between the single image $x$ and each one of the captions $\\\\{c_i\\\\}$. Then, you can pick the highest MID score among them as the prediction and measure the accuracy. Notice that MID can give you a pair-wise score like the CLIP-score.\n\nIn this way, CLIP and MID scores can be converted to the accuracy having the interpretability you argued. \n\nSince the accuracy metric is based on a set of probing captions, it could be sensitive toward the choice of candidate captions inevitably. Notice that the accuracy also has weaknesses."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725973563,
                "cdate": 1700725973563,
                "tmdate": 1700726040373,
                "mdate": 1700726040373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BpaZdaCGjf",
            "forum": "RcANissyP4",
            "replyto": "RcANissyP4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1350/Reviewer_jCGn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1350/Reviewer_jCGn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method that can evaluate text-faithfulness of text-to-image generative models. The main idea is to employ the generative model itself for discriminative tasks, thereby evaluating the performance. Experimental results show that the proposed method can achieve a high degree of agreement for measuring text-faithfulness with human evaluations on several models and datasets, proving the effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method is simple but effective, which can be easily implemented and applied to other tasks.\n2. Experiments show the effectiveness and good performance for measuring text-faithfulness."
                },
                "weaknesses": {
                    "value": "As the authors said, the formulation can extend to any conditional diffusion model. However, the current analysis and experiments only demonstrate that the method is effective for text-to-image generation tasks with diffusion models. \n\n1. Considering the method only evaluates the text-faithfulness without image quality in text-to-image generation tasks, the application scope will be very limited. Therefore, the contribution is not overwhelming and might not be enough for this conference, in my opinion.\n\n2. The experiments are also limited. Since the proposed method is an evaluation metric, trying it to evaluate other frameworks, such as VAEs and GANs, is also necessary. If the method can only be applied to diffusion models, it will be less valuable."
                },
                "questions": {
                    "value": "Although diffusion models have become very popular recently, this framework has not become the only generative model. Meanwhile, other frameworks also perform well in text-to-image generation tasks. Therefore, I think the method has a limited application scope. In my opinion, the authors should try other conditional generation tasks rather than only text-to-image generation tasks, or try applying the method to other generative models.\n\nHowever, in addition to this problem, I cannot find other technical flaws or missed experiments. Thus, I tend to a marginal score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Reviewer_jCGn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698200282050,
            "cdate": 1698200282050,
            "tmdate": 1699636062240,
            "mdate": 1699636062240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oKZwEOrdUt",
                "forum": "RcANissyP4",
                "replyto": "BpaZdaCGjf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jCGn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the simplicity and effectiveness of SelfEval for evaluating text faithfulness of generative models. Please find our responses to your concerns and questions below.\n\n**Weaknesses**\n\n- Using Eq. 4-9 (in the main paper)  one can compute the likelihood of the data $x$ given conditioning $c$.  Throughout the derivation we do not make any assumptions on the shape/format of the conditioning variable $c$. We chose to proceed with text-to-image generation because evaluating the performance of the model on the image-text matching task is correlated with its generation capabilities. If the reviewer has specific conditioning variables or downstream tasks in mind, please let us know and we can provide a specific and detailed answer.\n\n- We believe that proper evaluation metrics are necessary to make progress in any field. Given the exponential progress in generative modeling, there is a need for developing strong evaluation metrics. We argue that research on evaluation metrics is not narrow but rather necessary to propel the advancement of generative models. Moreover, [1]  proposed CLIP score, widely used for evaluating text faithfulness while [2] proposed FVD a metric to measure quality of generated videos, both of which are widely used by the generative community making such research very valuable.\n\n- We re-iterate that the contribution of our work is broadly two fold. 1) Generative models can themselves be used to evaluate their generations and 2) a principled approach to achieving this with text-to-image diffusion models and repurposing discriminative ground truth image-text pairs. We believe that 1) is an important contribution with far reaching consequences and is not specific to diffusion models. Generative models can inherently be used to evaluate themselves using Bayes\u2019 Rule and this applies to any future developments in this area. We agree with the reviewer that showing results on additional models is beneficial but after an extensive review we fail to obtain any text-to-image GAN or VAE models which are publicly available making it impossible to evaluate using this approach (see common response above). Instead we chose four popular models, with enough details in their respective papers for re-implementation, from two families of diffusion models to validate our hypothesis. We urge the reviewer to provide specific details (papers, codes, checkpoints) of good text-to-image generative models we can include in our analysis.\n\n[1] Jack Hessel et al. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. \u201dEMNLP (2021)\u201d.\n\n[2] Unterthiner, Thomas et al. \u201cFVD: A new Metric for Video Generation.\u201d DGS@ICLR (2019)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162270198,
                "cdate": 1700162270198,
                "tmdate": 1700162270198,
                "mdate": 1700162270198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LEFtjyARKr",
            "forum": "RcANissyP4",
            "replyto": "RcANissyP4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1350/Reviewer_VHYX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1350/Reviewer_VHYX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for evaluating text-to-image generative models using the model's own capabilities. The proposed SELFEVAL approach computes the likelihood of real images given text prompts, enabling the generative model to perform discriminative tasks. This allows for fine-grained evaluation of the model's performance in attribute binding, color recognition, counting, shape recognition, and spatial understanding. Paper claims that SELFEVAL is the first automated metric to show a high degree of agreement for measuring text-faithfulness with human evaluations across multiple models and benchmarks. Unlike other metrics such as CLIP-score, which can show severe drawbacks when measuring text faithfulness, SELFEVAL offers reliable, automated evaluation for diffusion models without additional pre-trained models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method provides a way to automate the evaluation process, making it less dependent on often subjective and time-consuming human evaluations. It allows for detailed assessment of various aspects of a model's capabilities, such as attribute binding and color recognition. The ability to evaluate models without needing additional pre-trained models is a significant advantage.\n2. Sufficient evaluations of the proposed metric along with others are given. It shows consistency with human evaluations."
                },
                "weaknesses": {
                    "value": "1. The performance of SELFEVAL is inherently linked to the effectiveness of the generative model itself, which could limit its reliability if the generative model has weaknesses in certain areas. From this perspective, more generative models should be evaluated.\n2. The authors don't discuss the potential limitations of their proposed method in detail.\n\nMinor issues:\n1) There may exist some misuse between \\cite and \\citep as some citation formats seem improper in the paper."
                },
                "questions": {
                    "value": "1. How well does the SELFEVAL method generalize across different types of generative models and datasets?\n2. Can this method be employed in GAN, VAE, and flow-based generative models?\n3. Including a broader range of comparative studies with current methods for assessing the quality of text-to-image generative models could strengthen your arguments.\n4. How different text encoders impact the performance of SELFEVAL."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1350/Reviewer_VHYX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786584301,
            "cdate": 1698786584301,
            "tmdate": 1700731812631,
            "mdate": 1700731812631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hkBEx0LayF",
                "forum": "RcANissyP4",
                "replyto": "LEFtjyARKr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VHXY"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing SelfEval\u2019s ability to eliminate the use of external models and its strong agreement with human evaluations.  We will address your concerns and questions below.\n\n**Weaknesses**\n\n- We agree with the reviewer that the limitations of the evaluating model affects the performance of the evaluation itself but as repeatedly mentioned throughout the paper, this is an issue only when an external model is used. SelfEval uses the generative model itself for evaluation. For example, if the generative model is weak in \u201ccounting\u201d, then as shown in the paper, the scores for this task are lower. We show that this is the case with four popular text-to-image models and request the reviewer to provide the details (paper, code and checkpoints) of other generative models they deem useful.\n\n- SelfEval relies on the sampling of the generative model to compute the scores. So the limitations of the sampling process of a generative model affect SelfEval. Unlike other black-box evaluation methods which only require the generations from the model, SelfEval requires the model definition and its checkpoints for evaluation. We can provide more details if necessary and will add this discussion to the next draft.\n\n**Questions**\n\n- SelfEval is evaluated using two different families of diffusion models, pixel and latent and across 4 different datasets and shows strong agreement with human evaluators. \n- Any likelihood-based generative model (i.e. ones that model the data distribution $p_{\\text{data}}(x)$ , like VAEs) can be used with SelfEval. For implicit generative models, like GANs, methods like GAN inversion [1] can be used to obtain likelihood scores from them as described in [2]. After an extensive survey of existing methods, we failed to obtain public repositories of  text-to-image GANs or VAEs with checkpoints. We request the reviewer to point us to relevant text-to-image generative models (With code and checkpoint) for further experiments. \n- We request the reviewer to clarify what additional analyses/experiments they consider valuable to this work.\n- We show the effect of different text encoders in Sec. 4.2 of the main paper.\n\n[1] W. Xia, Y. Zhang, Y. Yang, J. -H. Xue, B. Zhou and M. -H. Yang, \"GAN Inversion: A Survey,\" in IEEE T- PAMI, 2023.\n\n[2] Eghbal-zadeh, Hamid and Gerhard Widmer. \u201cLikelihood Estimation for Generative Adversarial Networks.\u201d 2017."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161855243,
                "cdate": 1700161855243,
                "tmdate": 1700161855243,
                "mdate": 1700161855243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sBhuWs6A1W",
                "forum": "RcANissyP4",
                "replyto": "LEFtjyARKr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up with Reviewer"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\nWe perform additional experiments comparing SelfEval with MID[1] as suggested by Reviewer M8yh. MID uses a CLIP model to compute mutual information divergence and we observe that it suffers from the same drawbacks as CLIP-Score (Fig. 2 in the main paper). We show the results in Table-1 below. The second and third column in Table-1 compute MID using ViT-B/32 and ViT-L/14 backbones respectively. Similar to CLIP score, depending on the evaluation model used, LDM-CLIP (with ViT-L/14 text encoder) ranks higher/lower than LDM-CLIP (with ViT-H/14 text encoder). We believe this issue exists regardless of the metric as long as it relies on external models. For more details refer to the review posted to Reviewer M8yh.\n\nTable-1: Drawback of MID\n|             Model               |  ViT-B/32 | ViT-L/14  |\n|-----------------------------|-------------|-------------|\n| LDM-CLIP (ViT-L/14) |    27.77    |     25.70   |    \n| LDM-CLIP (ViT-H/14) |    29.25    |    23.53    | \n\nPlease take a look at our posted rebuttal and additional discussions and provide us with more details. We believe this discussion can convince the reviewer to improve their score.\n\n[1] Kim, J.-H. et al. (2022). Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. Advances in Neural Information Processing Systems 35."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714022271,
                "cdate": 1700714022271,
                "tmdate": 1700714237110,
                "mdate": 1700714237110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]