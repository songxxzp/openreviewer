[
    {
        "title": "Fooling the Textual Fooler via Randomizing Latent Representations"
    },
    {
        "review": {
            "id": "dJL6OPTttJ",
            "forum": "E296x0YpML",
            "replyto": "E296x0YpML",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
            ],
            "content": {
                "summary": {
                    "value": "1. GOAL:\n\n* 1A. The problem statement:\nIt is possible to do black box word level attacks by determining the most important words in the input by repeated query and replacing them (with synonyms).\n\n* 1B. What their solution is:\nconfuse the adversarial black-box attack with a system called \"AdvFooler\" that randomizes the latent space. \n\n\n\n2. METHOD:\nThe latent space is randomized by adding an independent gaussian noise vector to the l-th layer of a pretrained classification model. The magnitude of noise is selected based on how much the clean accuracy drops by a chosen percentage on a small held out clean set (1% drop is used in paper). Though it\u2019s not actually stated in the paper, I assume this noise is added during evaluation time (so each adversarial query yields slightly different perturbed outputs, and the attack has a hard time figuring out what words to perturb in it\u2019s attack).\n\nClaims Made:\n- does not need more compute during training (True)\n- does not rely on assumptions about adversarial perturbation word set for a word substitution attack (True)\n- theoretical and empirical results presented on adversarial set and clean accuracy (True, they are presented)\n\n3. RESULTS:\nThere are many metrics to evaluate on, so we'll go metric by metric: high clean accuracy (A), middling robustness-accuracy tradeoff (B/C), great implementation ease (D), and (probably) low compute time needed which is great (E)\n\n* 3A. By design the clean accuracy does not suffer more than 1% on their method (when used alone), and they show an example where they are still able to change the most significant words.\n\n* 3B. Their method isn't the best method in terms of robustness and accuracy tradeoff, but it is reasonably robust (i.e. does improve robustness significantly). Conclusions based on these comparisons are hard to make to be honest. More details in the Weaknesses section.\n\n* 3C. Some results in Fig 4 demonstrate the tradeoff between clean accuracy and adversarial attack accuracy for this method specifically. Is is unclear why the clean accuracy drops so little at really high noise rates, and that's not really explained.\n\n* 3D. Where their method absolutely shines is how simple the method is to implement and use based on reading the methods section alone. They don't highlight ease of implementation in the paper, but it's absolutely something that would make this method much easier to adopt. \n\n* 3E. They repeatedly highlight low added compute as a big win for this method. They mention they have results about compute usage across the methods in Supplemental Materials, but I was not able to find these. Please add. Still reading the methods section gives a clear picture that the added compute for this method is very negligible. (It's unclear to me if some adversarial training baseline methods are also reasonably negligible in terms of compute time. Computing synonyms is very cheap, and training on some added adversarial examples is also not marginally more expensive in any meaningful way)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The primary and significant strength of this paper is that their method is dead simple to implement and use, and takes almost no added compute time. Even if it's not SOTA (and it doesn't actually claim to be SOTA), it is strongly worth disseminating (after adding some missing results: see Weaknesses). \n\nIn much the way drop out rates approximate ensemble training and save us all compute time, this method approximates diverse ensembles used to achieve adversarial robustness (ex: the ADP regularizer  https://proceedings.mlr.press/v97/pang19a), and can save some compute time. Because it's so easy to implement and tune for accuracy, it would have high practical impact -- for that reason alone, it's worth disseminating (after some weaknesses are addressed, see below)."
                },
                "weaknesses": {
                    "value": "3 weaknesses and 2 baseline suggestions, enumerated below as 1-5. I am very amenable to changing my decision to an accept if these are addressed (esp. 1 and 3, with a very strong suggestion for 4 and 2).\n\n1. It's really hard to draw conclusions about robustness-accuracy tradeoffs from Tables 2 and 3. The claim that your method performs \"close\" to SOTA needs some added substantiation. I'd like:\n\n  *  1.A) Standard deviations on at-least three runs for each setting. In many of these datasets, a 1pp improvement is significant. But accuracies on adversarial datasets often have high variance, so it's unclear how to interpret when your method isn't clearly better and often appears to be much below a handful of the other methods.\n\nMore details: TMD more reliably seems to perform better esp, on Imdb:Roberta, RanMASK and SAFER sometimes outperform on  robustness to attack/clean accuracy). It's hard to verify what \"close\" is, because there are no standard deviations, and on a few of the tasks, AdvFooler really does appear to significantly underperform a couple of the other methods by a  bit. (It's still reliably top 3 among the methods though, and I don't think you need to be better than all to have a case for publication).\n\n   * 1.B. Tune SAFER and RunMASK (tune the rate of substitution/masking) until either the robustness or the accuracy matches your method, so. you can draw a clean comparison. (Or tune your method to their accuracy if that's easier). As is, not clear at all that one is Pareto dominant (or even comparable) to the other. \n\nExample: While RunMASK does suffer on accuracy, it makes things more robust (on AGNEWS dataset BERT for instance). Based on the data available, I'd say it's a toss up whether it's better or worse than AdvFooler.\n\n\n\n2. I think the most compelling case for publication is how lightweight your system is. I would include compute time results somewhere. I was unable to find them). It's also unclear to me that simple adversarial training on word substitutions is actually significantly more compute intensive (generating synonyms is super quick, and training on more examples isn't too much of an added burden when you're already finetuning -- esp. if you're comparing to needing to tune your noise param for AdvFooler, which while cheap is still some added compute time). \n\n\n3. In your system, HotFlip is not perhaps an accurate white-box attack (Appendix C7). HotFlip relies on gradients, but are you giving it the unnoised gradients, or the parameters across all iterations of the model (so someone can reverse engineer the likely original unnoised parameters?). I suspect that if it were truly a white box attack, the certified robustness systems and TMD would outperform. I think these results as presented are a bit misleading otherwise, and one option is to remove them. I would suggest either demonstrating the white-box capabilities fully by (1) giving access to unnoised params, or each set of params so you can derive unnoised params AND (2) comparing to TMD and the certified methods. OR I'd like to see the claim about the defense being \"attack agnostic\" in the main paper, and the claims of being robust to \"white-box attack\" toned down.\n\n4. New Baseline Suggestion 1/2: While I understand that it would not be as lightweight as AdvFooler, the style of your method, suggests that a comparison with a diverse ensemble designed to induce adversarial robustness would be very apt since it also tries to induce robustness in much the same way (added random noise) (see my strengths section for the analogy). (I suggest this one: https://proceedings.mlr.press/v97/pang19a , the ADP regularizer)\n\n5. New Baseline Suggestion 2/2: Choose Dirichlet Neighborhood Ensemble (DNE)  over ASCC. While both methods model the perturbation space as the convex hull of word synonyms, DNE far outperforms ASCC (See TMD paper Table 1 where DNE nearly outperforms TMD on AGNews while maintaining a high clean accuracy)."
                },
                "questions": {
                    "value": "1. How often are are the predictions for F_advfooler the same as those for F under different noise rates? (Claim in 3.1) The chart showing adversarial robustness dropping while clean accuracy only slightly drops, gets at this question somewhat. But if possible, I'd like something that's even cleaner of a comparison that can maybe help provide intuition for why the clean accuracy doesn't really suffer much (Esp on IMDB) while adversarial robustness totally craters under high noise (Fig 4). \n\nSuggestions for added baselines (2 of them) included in the Weaknesses section (numbers 4 and 5)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethnical concerns really. It's a standard adversarial robustness defense paper for word substitution attacks."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f",
                        "ICLR.cc/2024/Conference/Submission9033/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697675301127,
            "cdate": 1697675301127,
            "tmdate": 1700638653932,
            "mdate": 1700638653932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ovdyMSwbpX",
                "forum": "E296x0YpML",
                "replyto": "dJL6OPTttJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: It's really hard to conclude robustness-accuracy tradeoffs from Tables 2 and 3.... Based on the data available, I'd say it's a toss-up whether it's better or worse than AdvFooler.**\n\n\n**A: ** Thank you for the valuable comment. We have revised the performance discussion in Section 4.2, and updated the submission. Indeed, AdvFooler is reliably among the top-3 defenses. However, as mentioned in Section 4.2's discussion, AdvFooler possesses other advantages, such as significantly lower training and inference overhead, being plug-and-play, and requiring no access to the training data.\n\nIn our experimments, the calculated the p-values of AdvFooler and the better defenses are below 0.01, indicating that the performance results are statistically significance. We also added this into Supplementary of the revised submission.\n\nWe really appreciate the interesting suggestion to \"tune SAFER and RunMASK until either the robustness or the accuracy matches AdvFooler\". We provide the results when tuning RandMASK to a similar accuracy of AdvFooler. As we can observe, the accuracy under attack of RandMASK decreases tremendously, reducing to half of the original AuA. This shows that, while RanMASK was able to be robust against adversarial attacks, this method also requires larger trade-offs in accuracy compared to other defenses. We added this experiment into Section C.8 (Supplementary), including the experimental setup.\n\n|                              |        | TextBugger AuA(%) (ASR(%)\u2193) | TextBugger Avg. Query\u2193 | TextFooler AuA(%) (ASR(%)\u2193) | TextFooler Avg. Query\u2193 | BERTAttack AuA(%) (ASR(%)\u2193) |  BERTAttack Avg. Query\u2193 |\n |------------------------------|--------|:---------------------------:|:----------------------:|:---------------------------:|:----------------------:|:---------------------------:|:-----------------------:|\n |   BERT_MASK (0.9 mask rate)  | 90.14% |       52.50% (41.60%)       |         582.21         |       54.60% (38.93%)       |         511.98         |       61.10% (31.73%)       |          595.57         |\n |   BERT_MASK (0.3 mask rate)  |  93.5% |       22.50% (75.88%)       |         360.12         |       24.10% (74.09%)       |         325.63         |       42.90% (54.12%)       |          412.99         |\n | ROBERTA_MASK (0.9 mask rate) | 90.41% |       52.30% (41.50%)       |         575.94         |       54.20% (39.37%)       |         516.98         |       60.00% (32.43%)       |          594.93         |\n | ROBERTA_MASK (0.3 mask rate) |  93.3% |       34.00% (63.56%)       |         396.72         |        35.7% (61.74%)       |         359.52         |       45.60% (50.97%)       |          429.72         |\n\n\n**Q2:I think the most compelling case for publication is how lightweight your system is...if you're comparing to needing to tune your noise param for AdvFooler, which while cheap is still some added compute time).**\n\n**A:** In Table 5, Section C.1, we provided the inference time of AdvFooler  the AdvFooler and other defenses; AdvFooler only adds a negligible inference overhead to the model, while TMD/RandMASK, SAFER, and ASCC all have non-trivial overhead.\n\nLet N_a be the number of noise scales in AdvFooler. Then, for each noise scale, AdvFooler needs to forward through a small test set. Below, we show the training time for FreeLB and InfoBERT and base-training time + tuning time for AdvFooler (for fair comparison), on AGNEWS. We can observe that, FreeLB and InfoBERT incur significant training overhead, compared to the tuning+base-model training time of AdvFooler.\n\n\n| Model     | AGNEWS                   (s) | \n|-----------|------------------------------|\n| BERT-base |                        12150 |\n| FREELB    |                        52370 |\n| INFOBERT  |                        76650 | \n| AdvFooler |                 12150+40*N_a |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595662576,
                "cdate": 1700595662576,
                "tmdate": 1700595662576,
                "mdate": 1700595662576,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yLOWnRUtkf",
                "forum": "E296x0YpML",
                "replyto": "QTZ5Ve3X27",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
                ],
                "content": {
                    "title": {
                        "value": "Lingering issues on Q3"
                    },
                    "comment": {
                        "value": "I strongly appreciate the thoroughness of the rebuttal but I remain unconvinced of thr point that advfooler is even performant on white box attacks. \n\nGenerating attacks on the base model in a white box fashion and seeing whether those attacks transfer to AdvFooler after adding noise is still not a white box attack. The attacker needs to be aware and make good use of the specific noise values added (or even the distribution of noises if using some iterative bounding protocol type attack)\n\nWithout this added information, the attack you describe is not a white box attack for this setting. It's some sort of attack transferability assessment."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607532719,
                "cdate": 1700607532719,
                "tmdate": 1700607532719,
                "mdate": 1700607532719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yvS5FhfMNn",
                "forum": "E296x0YpML",
                "replyto": "ovdyMSwbpX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
                ],
                "content": {
                    "title": {
                        "value": "Make it more clear that being sota is not a contribution, being in the same ballpark is"
                    },
                    "comment": {
                        "value": "I very much appreciate your thoroughness and believe there is strong value to this method even if it doesn't beat SOTA. \n\nCan you, however, make it even more clear in the contributions that while you achieve close to sota under more constraints, the contribution is not necessarily to achieve SOTA. I think this would also help frame rhe readers perspective early on."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607943943,
                "cdate": 1700607943943,
                "tmdate": 1700607943943,
                "mdate": 1700607943943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z5kDMnRigR",
                "forum": "E296x0YpML",
                "replyto": "QTZ5Ve3X27",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
                ],
                "content": {
                    "title": {
                        "value": "Point taken on Q4. Analog would actually be selecting a model at random from an ensemble."
                    },
                    "comment": {
                        "value": "I'm not sure this experiment is needed for publication, but point taken.\n\nThe equivalent would be running adp, and then selecting one model within that system at random per query. \n\nNo further action needed here."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608098320,
                "cdate": 1700608098320,
                "tmdate": 1700608098320,
                "mdate": 1700608098320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RediKAoh3x",
                "forum": "E296x0YpML",
                "replyto": "dJL6OPTttJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Reviewer_2d4f"
                ],
                "content": {
                    "title": {
                        "value": "This is the main remaining issue for me."
                    },
                    "comment": {
                        "value": "I still think the claim of being robust to white box attack is misleading, and this issue is still blocking me from giving a higher rating.\n\nI suggest you either frame it as an attack transferability from the base model type of issue instead of mentioning white box attack. Or run a white box attack aware or the noise values added."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608232029,
                "cdate": 1700608232029,
                "tmdate": 1700608253799,
                "mdate": 1700608253799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ia2BtnCkDc",
                "forum": "E296x0YpML",
                "replyto": "dJL6OPTttJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our responses to the reviewer's new comments!"
                    },
                    "comment": {
                        "value": "**Q3.1: Generating attacks on the base model in a white box fashion and seeing whether those attacks transfer to AdvFooler after adding noise is still not a white box attack. The attacker needs to be aware and make good use of the specific noise values added (or even the distribution of noises if using some iterative bounding protocol type attack)**\n\n**Without this added information, the attack you describe is not a white box attack for this setting. It's some sort of attack transferability assessment.**\n\nThis is again a very useful and insightful suggestion. We updated the paper (Section C.7 in the revised submission) to indicate that these are *not truly white-box attacks*. \n\nWe dubbed this attack \"transferred white-box\" attacks, due to the fact that the adversary has access to the base model (or even the randomized model), but there still exists a mismatch between the attacker's possessed model and the defender's possessed model due to the added randomization (or the attacker does not make good use of the noise distribution, which is not a designed in the existing white-box approach used for the study), thanks to the reviewer's suggestion. We also believe that developing a true white-box attack that makes good use of the noise values is an interesting and independent extension of our work. \n\n**Q6: I very much appreciate your thoroughness and believe there is strong value to this method even if it doesn't beat SOTA. Can you, however, make it even more clear in the contributions that while you achieve close to sota under more constraints, the contribution is not necessarily to achieve SOTA. I think this would also help frame rhe readers perspective early on.**\n\nThank you for the comment. As suggested, we have revised our paper with the following statement in contributions: \"The results demonstrate that AdvFooler is a competitive defense, compared to the existing representative textual adversarial defenses, while being under more constraints, including few modeling assumptions, being pluggable, and incurring negligible additional computational overhead.\" \n\n**Q4.1: I'm not sure this experiment is needed for publication, but point taken. \nThe equivalent would be running adp, and then selecting one model within that system at random per query. No further action needed here.**\n\nThank you for the comment. \n\n**Q7: I still think the claim of being robust to white box attack is misleading, and this issue is still blocking me from giving a higher rating.\nI suggest you either frame it as an attack transferability from the base model type of issue instead of mentioning white box attack. Or run a white box attack aware or the noise values added.**\n\nWe have revised the paper to clarify that this is not truly a white-box attack, in our response to Q6. Again, we appreciate the comment which has helped us improve the clarity of the paper on white-box attacks.\n\n---\n\nThank you the reviewer for the insightful comments. We hope that your concerns and comments are now addressed. Nevertheless, we're happy to discuss any additional comments you may have!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633375067,
                "cdate": 1700633375067,
                "tmdate": 1700633593167,
                "mdate": 1700633593167,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kB9rx2EOwQ",
            "forum": "E296x0YpML",
            "replyto": "E296x0YpML",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9033/Reviewer_bLP9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9033/Reviewer_bLP9"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a simple defense method AdvFooler against textual adversarial attacks. Specifically, AdvFooler adds a Gaussian noise at each layer for forward propagation to randomize the latent defense. With such randomization, the attacker cannot find significant words for substitution, making the model robust."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n\n2. AdvFooler is simple and seems to be effective against several attacks."
                },
                "weaknesses": {
                    "value": "1. The motivation is not clear. Why does such randomization fool the attackers while not degrading the benign performance?\n\n2. Why does AdvFooler can only perplex query-based black-box attacks? It is significant for a defense method to defend against various attacks, such as white-box attacks [1], decision-based attacks [2,3], and so on. It is necessary to validate the effectiveness against these attacks to show the generality of AdvFooler.\n\n3. AdvFooler does not outperform the SOTA baselines against various attacks.\n\n4. From Figure 4, it might be hard to choose a consistent noise scale for different datasets and models.\n\n[1] Wang et al. Adversarial Training with Fast Gradient Projection Method against Synonym Substitution based Text Attacks. AAAI 2021.\n\n[2] Maheshwary et al. Generating Natural Language Attacks in a Hard Label Black Box Setting. AAAI 2021.\n\n[3] Yu et al. TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack. EMNLP 2022."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697874823826,
            "cdate": 1697874823826,
            "tmdate": 1699637137490,
            "mdate": 1699637137490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "loBrLdEdLB",
                "forum": "E296x0YpML",
                "replyto": "kB9rx2EOwQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your invaluable comments."
                    },
                    "comment": {
                        "value": "**Q1: The motivation is not clear. Why does such randomization fool the attackers while not degrading the benign performance?***\n\n**A:** Thank you for the comment. As explained in Section 3.2 and Theorem 1, the proposed latent feature randomization can fool the query-based attackers since it can mislead the search direction of the adversary's querying process. However, this randomization *will affect/degrade the benign performance*, a fact we specifically mention in Section 3.3. AdvFooler and other randomization-based defenses, such as RandMask and SAFER, will affect the benign performance. As demonstrated in Figure 4, increasing the noise scale leads to a gradual decrease in benign accuracy; at some point, the large noise scale can even lead to a significant decrease in benign performance. We suggest, in Section 4.3 discussing this figure, that the defender should choose a noise scale for their need (e.g., noise scale corresponds to 1% drop in benign accuracy). \n\n\nTo summarize, we'd like to clarify that AdvFooler's randomization can fool the attackers but will induce a small drop the benign performance. In addition, unlike RandMASK and SAFER, it only has negligible training and inference overhead and can be plugged into any already trained model for better robustness (Table 1).\n\n**Q2: Why does AdvFooler can only perplex query-based black-box attacks? It is significant for a defense method to defend against various attacks, such as white-box attacks [1], decision-based attacks [2,3], and so on. It is necessary to validate the effectiveness against these attacks to show the generality of AdvFooler.**\n\n**A:** We'd like to emphasize the practicality of threat model discussed in the paper. For many companies with ML applications, the trained model as well as training data are valuable assets to the companies, thus are secretly guarded. Thus, the attacker rarely has access to the trained model or model\u2019s architecture to perform white-box attacks. Instead, the attacker will rely on querying the models so that they can estimate the optimal perturbation toward adversarial examples, as discussed in Section 1. The same black-box, query-based threat model is frequently examined in several related works [(Nguyen Minh and Luu, 2022), (Zeng et al, 2021), (Dong et al, 2021), (Si et al, 2021), (Li et al, 2021)]. \n\nNevertheless, we also agree that, for broad applicability, it is necessary to validate the effectiveness of AdvFooler against white-box and decision-based attacks, which we already provided in C.7 and C.6 (Supplementary), respectively. These experiments confirm that AdvFooler is still effective against the evaluated white-box and decision-based attacks. As discussed in these experiments, we conjecture that, for white-box attacks, AdvFooler's randomization nullifies the adversarial perturbation and brings it back to the correct side of the decision boundary; since decision-based attacks still involve querying the model, AdvFooler's randomization can return incorrect prediction for some perturbed samples, especially near the decision boundary, and therefore misleads the attacks' optimization process."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594927002,
                "cdate": 1700594927002,
                "tmdate": 1700594927002,
                "mdate": 1700594927002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tpOkjYR7yc",
            "forum": "E296x0YpML",
            "replyto": "E296x0YpML",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9033/Reviewer_9Kvp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9033/Reviewer_9Kvp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a lightweight and attack-agnostic defense method to against query-based black-box attacks called AdvFooler. AdvFooler accomplishes this by introducing randomization to the latent input representation during inference.  The advantage of Advfooler is that it does not need additional computational overhead during training nor relies on assumptions about the potential adversarial perturbation set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Compared with other defense methods, the proposed method AdvFooler is simple, pluggable and does not require additional computational overhead during testing or access to training.\n2. The authors conducted comprehensive experiments to assess the effectiveness of AdvFooler, employing two BERT models, two distinct datasets, and three different attack methods. Furthermore, they provided qualitative analyses of their results."
                },
                "weaknesses": {
                    "value": "1. Despite its advantages in terms of simple implementation and minimal computational overhead, AdvFooler's performance falls short of the state-of-the-art. In Table 2, on the AGNEWS dataset, AdvFooler exhibits lower accuracy under attack compared to RanMASK for the BERT-base model, and it also demonstrates lower accuracy under attack than both TMD and RanMASK for the RoBERTa-base model.\n\n2. The selection of the hyper-parameter for noise scale in AdvFooler is not entirely clear. The authors claim that they choose the \u03bd value based on the criterion that the clean accuracy drops by at most 1% using the test set. However, it seems not the case in Table 2 and 3. In Figure 4, the curves of AuA are not monotone. Do authors also consider AuA values when choosing noise scale?"
                },
                "questions": {
                    "value": "Instead of adding noises to all layers by the same noise scale, what would the results be if adding different noise scale to different layers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9033/Reviewer_9Kvp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834806669,
            "cdate": 1698834806669,
            "tmdate": 1699637137386,
            "mdate": 1699637137386,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QdndO9UMU7",
                "forum": "E296x0YpML",
                "replyto": "tpOkjYR7yc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable comments!"
                    },
                    "comment": {
                        "value": "Please see our responses below.\n\n**Q1: Despite its advantages in terms of simple implementation and minimal computational overhead, AdvFooler's performance falls short of the state-of-the-art. In Table 2, on the AGNEWS dataset, AdvFooler exhibits lower accuracy under attack compared to RanMASK for the BERT-base model, and it also demonstrates lower accuracy under attack than both TMD and RanMASK for the RoBERTa-base model.**\n\nWe acknowledged, in Section 4.2, that AdvFooler has slightly lower performance than RandMASK and TMD in some experiments. \n\nBut, as explained in Section 1, AdvFooler has other important advantages compared to RandMASK and TMD: (1) having negligible computation overhead while (2) simultaneously achieving comparable robustness to RandMASK and TMD, and (3) being plug-and-play, which allows it to be used without modifying existing parameters of the model. Simultenously achieving (1) and (2) is an extremely challenging task; for example, TMD and RandMask require significant additional training and inference overhead for similar robustness to AdvFooler. Also for, for RandMASK, its clean accuracy drops significantly (>4% for RandMASK vs. < 1% for AdvFooler). For these reasons, AdvFooler is significantly more practical for usage in real-world applications, a significant contribution to the adversarial defense domain.\n\n**Q2: The selection of the hyper-parameter for noise scale in AdvFooler is not entirely clear. The authors claim that they choose the $\\nu$ value based on the criterion that the clean accuracy drops by at most 1% using the test set. However, it seems not the case in Table 2 and 3. In Figure 4, the curves of AuA are not monotone. Do authors also consider AuA values when choosing noise scale?**\n\n**A:** We'd like to, first, re-iterate the discussion in Section 3.3: \"the defender can select the noise scale $\\nu$ at which the variance causes the clean accuracy drops by a chosen percentage (e.g., in our experiments, it is 1\\%)\". In practice (as discussed in threat model), the defender only has the trained model and a small clean test set and makes no assumption about the attacks; using the clean test set, defender finds the largest noise scale corresponding to a certain, tolerable drop in clean accuracy. For example, given the values we used to plot Figure 4 as in the table below, starting with the original clean accuracy of 95.14%, the defender would choose $\\nu=0.6$, which corresponds to 94.57% clean accuracy, assuming a selection of 1% drop in performance. For 2% drop (or 94.57% clean accuracy), $\\nu$ is set to 0.80. As we can see, the larger $\\nu$ generally increases the robustness (43.90% and 48.80% AuA for $\\nu=0.6$ and $\\nu=0.8$, respectively, on TextFooler); however, when $\\nu$ is too large, as we can observe in this figure, it will also interfere clean accuracy (a sudden, significant drop in clean accuracy) and correspondingly decrease the robustness of the model as well. \n\n\n\n| noise_type\\intensity | 0.10   | 0.20   | 0.30   | 0.40   | 0.50   | 0.60   | 0.70   | 0.80   | 0.90   | 1      | 1.10   | 1.20   | 1.30   | 1.40   | 1.50   |\n|:--------------------:|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n| Clean (95.14%)       | 95.17% | 95.06% | 95.04% | 94.94% | 94.69% | 94.57% | 94.11% | 93.67% | 93.12% | 92.48% | 90.74% | 87.79% | 83.18% | 77.04% | 70.05% |\n| AuA (TextFooler)     | 38.30% | 38.40% | 41.10% | 43.90% | 45.60% | 48.10% | 48.80% | 50.10% | 45.50% | 35.50% | 21.90% | 13.30% | 8.40%  | 5.80%  | 4.40%  |\n| AuA (TextBugger)     | 36.00% | 37.10% | 38.20% | 40.40% | 45.60% | 46.90% | 49.90% | 50.10% | 44.40% | 33.40% | 20.20% | 13.70% | 8.20%  | 5.40%  | 4.20%  |\n| AuA (BERTAttack)     | 46.70% | 48.80% | 50.30% | 47.50% | 50.30% | 55.40% | 54.50% | 53.40% | 48.60% | 36.20% | 22.10% | 12.10% | 7.10%  | 5.10%  | 4.40%  |\n\n\n\n**Q3: **Instead of adding noises to all layers by the same noise scale, what would the results be if adding different noise scales to different layers?****\n\n**A:** Thank you for the interesting question. We provide the experiments (AGNEWs) for AdvFooler with different noise scales added to different layers (we multiply the original noise scale by the standard deviation of that layer\u2019s output)\n\n| Model     | TextFooler [AUA%(ASR%)] | TextBugger [AUA%(ASR%)] | BERTAttack       [AUA%(ASR%)] |\n|-----------|-------------------------|-------------------------|-------------------------|\n| AdvFooler |          52.0% (44.21%) |         49.5% (47.96%)  | 55.9% (39.83%)          |\n\nThe results show that this variation has good performance compared to our current AdvFooler\u2019s version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595298620,
                "cdate": 1700595298620,
                "tmdate": 1700595298620,
                "mdate": 1700595298620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EtDtC1WTsD",
            "forum": "E296x0YpML",
            "replyto": "E296x0YpML",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9033/Reviewer_UMzy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9033/Reviewer_UMzy"
            ],
            "content": {
                "summary": {
                    "value": "The authors observe that adding noise to hidden layers can result in obscuring identification of important words in a sentence, a crucial step needed for launching attacks. They evaluate their proposal and highlight efficacy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Low cost defense."
                },
                "weaknesses": {
                    "value": "1. Does not perform better than prior works.\n2. Similar in ideology to DP."
                },
                "questions": {
                    "value": "1. Could the authors compare and contrast their approach to applying DP-style noise to the embeddings generated by the hidden layer?\n2. Results from Table 2 suggest that their approach is not the very best compared to other approaches. What are the merits of the proposal then? Apart from reduced computational overheads?\n3. The authors empirically note that adding small amounts of noise to the logins does not change the prediction. But this is not fundamental nor clear why this is the case. Could the authors elaborate?\n4. Why can\u2019t the adversary utilize a proxy model to launch its attacks instead of the current black-box setup i.e., use the proxy model and attention values to identify important words?\n5. Alternatively, the adversary could also replace subsets of words using brute force. For small spans of text (as considered in the evaluation), this is not computationally prohibitive. Could the authors elaborate further the specific scenarios where important word selection inhibition is prudent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699326512346,
            "cdate": 1699326512346,
            "tmdate": 1699637137246,
            "mdate": 1699637137246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VuxE7BWfKC",
                "forum": "E296x0YpML",
                "replyto": "EtDtC1WTsD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable comments!"
                    },
                    "comment": {
                        "value": "Please see our responses to your comments below:\n\n**Q1: **Could the authors compare and contrast their approach to applying DP-style noise to the embeddings generated by the hidden layer?****\n\n**A:** Thank you for the comment; however, we\u2019re not sure what DP stands for and will assume DP refers to Differential Privacy, a privacy protection framework that introduces a small change in the data for a guaranteed, bounded change in the distribution of the algorithm\u2019s outputs. DP-style defenses such as (L\u00e9cuyer et al 2019) certify the defense under the DP framework; it is important to note that SAFER and RandMASK, evaluated in the paper, are also certified defenses and share a similar approach to DP-style defenses. DP-defense and RandMASK still require training overhead to tune the added noise so that it does not interfere with the model's prediction. On the other hand, AdvFooler adds noise to the latent features at inference time to misdirect the attacker's querying process.\n\nWe have added this discussion to our revised submission.\n\n\nL\u00e9cuyer, M., Atlidakis, V., Geambasu, R., Hsu, D.J., & Jana, S.S. (2018). Certified Robustness to Adversarial Examples with Differential Privacy.\u00a0*2019 IEEE Symposium on Security and Privacy (SP)*, 656-672.\n\n\n**Q2: **Results from Table 2 suggest that their approach is not the very best compared to other approaches. What are the merits of the proposal then? Apart from reduced computational overheads?****\n\n**A:** As explained in Section 1 (especially Table 1), AdvFooler (1) reduces significant computation overhead while (2) simultaneously achieving comparable robustness to other defenses. This is an extremely challenging task; for example, TMD/RandMask/SAFER or adversarial training approaches require significant additional training overhead for similar robustness to AdvFooler; TMD/RandMask/SAFER also incurs non-trivial inference overhead, making them less practical in real-world application. Another advantage of AdvFooler is (3) plug-and-play, as also indicated in Table 1, which allows it to be used without modifying existing parameters of the model. Finally, as indicated in Section 3.3 and 4, AdvFooler allows the user to (4) control how much accuracy they want to trade-off for robustness using a small test set. (1), (2), (3) and (4) make AdvFooler significantly more practical to employ in real-world applications, a significant contribution to the adversarial defense domain.\n\n**Q3: The authors empirically note that adding small amounts of noise to the logins does not change the prediction. But this is not fundamental nor clear why this is the case. Could the authors elaborate?**\n\n**A:** Defenses via adding noise (including AdvFooler and RandMASK/SAFER) will inevitably affect the prediction or clean accuracy. While RankMASK/SAFER perturbs the input and require additional training and/or inference overhead to tune the added noise for a small accuracy drop, AdvFooler adds noise to the hidden features only at inference time, an approach which is theoretically shown to be able to fool the attacker (discussed in Section 3.2). To choose this noise, however, we suggest using a small test set and finding a large-enough noise for a specific tolerance of accuracy drop (discussed in Section 3.3 and Figure 4). This allows the defender to control how much accuracy they want to trade off and experiments show that our approach can achieve comparable robustness with other defenses while having only <1% accuracy drop."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594954441,
                "cdate": 1700594954441,
                "tmdate": 1700595320443,
                "mdate": 1700595320443,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]