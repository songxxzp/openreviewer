[
    {
        "title": "Hardware-Friendly Post-Training Quantization: Input- and Output-Channelwise Scale and Offset"
    },
    {
        "review": {
            "id": "icYIGX93XT",
            "forum": "itJj6p7ssr",
            "replyto": "itJj6p7ssr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4312/Reviewer_LRrq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4312/Reviewer_LRrq"
            ],
            "content": {
                "summary": {
                    "value": "Observing the distributional discrepancy between the full precision weights and their quantized counterpart, this paper proposes to scale and offset input and output in a per-channel way.  Extensive experiments are conducted to show the effectiveness of their methods, especially in low-bit settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper conducts thorough experiments, including analyses of computational complexity and ablation studies, to showcase the effectiveness of their methods. It also provides a detailed comparison with related works.\n2. The proposed methods exhibit significant improvements over previous approaches, such as BRECQ[1], especially in low-bit scenarios.\n\n[1]Yuhang Li, et al. BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction.ICLR 2021"
                },
                "weaknesses": {
                    "value": "1. The novelty of the paper raises concerns, as the input-channel scale and shift resemble group-wise quantization have been studied in prior works like Q-BERT. Although the authors emphasize advantages in hardware implementation, the improvement over group-wise quantization appears minor. Can you provide the comparison between group-wise quantization (with or without the power of two scales) in computation complexity and performance?\n\n2. Similar to Weakness.1, shifting and scaling the output per channel is similar to finetune/update the BatchNorm statistics after the quantized convolutions. Does this method still work if we finetune BN after quantization on Conv-BN networks ?(this method is used widely to recover accuracy after quantization)\n\n[2]Sheng Shen,et al. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT. AAAI 2020"
                },
                "questions": {
                    "value": "Is the shift operation expected to introduce more latency than a single integer operation due to non-local memory access at inference?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765951861,
            "cdate": 1698765951861,
            "tmdate": 1699636399817,
            "mdate": 1699636399817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dvgvF7zqpw",
                "forum": "itJj6p7ssr",
                "replyto": "icYIGX93XT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LRrq"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper thoroughly and for providing valuable comments and feedback. In response to your questions, below are detailed answers to each one. We hope you find them helpful.\n\n**Q1. Compare with other input channel-wise grouping**\n\n**A1-1. Differences from Existing Channel Grouping Methods.**\n\n> Previous works in group-wise quantization, like Q-BERT[12], require each group to have different scale values. As highlighted in QLoRA[9], this requirement creates significant memory overhead during low-bit quantization. For example, if weights for 64 groups are quantized with a single scale per group, it demands 32-bit scales for each of those 64 weights. This scenario is similar to each weight incurring an additional memory overhead of 0.5 bits. While this impact is less pronounced in higher-bit quantizations, in the case of 2-bit quantization, it equates to an approximate 25% increase in memory requirements.\n\n> Our method, instead of creating groups based on unrelated weight positions, undergoes a calibration stage to find the optimal group for predetermined scale values. Since shuffled channels are independent, they can be reorganized through permutation, eliminating the need for additional scale information memory. Furthermore, we opted for shift operations to stay within the integer domain to avoid requiring floating point multipliers and adders if the scale multiplication remains in the floating domain. Table 4 in our ablation study demonstrates that significant performance improvements can be achieved through shift operations alone.\n\n**A1-2. ASIC/FPGA Area/Latency comparison**\n\n> We conducted experiments using the structure of a batch-norm fused systolic array from an NPU developed for edge devices to compare overhead and latency in ASICs. The results below show the area of the batch-norm fused systolic array when each method is implemented in ASIC. The synthesis was done using the TSMC 12nm process. The MAC has a 16x16 structure, and IOSO, batch norm, and activation can process 8 data in 1 cycle. The sequence is MAC(->IOSO)-> BN -> ACT. Please refer to the \"Common Response from Authors.\" Due to time and resource constraints, the actual performance time, or latency, was tested only on the resnet 50 layer with 28x28x128 feature 3x3x128x128 kernel layer.\n\n|  | **integer Quant.** | **+IOSO** | **FP16 quant** |\n|---|---:|---:|---:|\n| **Total Area(um^2)** | 429,342 | 439,762 | 882,233 |\n| **Latency(us)** | 1129.2775 | 1129.2825 | 1129.3150 |\n\n> As seen in the table above, IOSO can be implemented with an integer shifter adder without modifying the MAC structure, resulting in only a 2.37% increase in area. In addition, the implementation is very simple as it does not alter the existing data path. In contrast, methods partially implemented with FP16 points (like GPTQ[10], QLoRA[9], LLM.int8()[11]) for increased accuracy and reduced memory are useful in GPUs that already have FP16 point kernels. However, from an ASIC perspective, these are challenging to use due to significant increases in MAC's area and power. The latency results are similar across all three methods because all data operates in a pipeline. The speed is comparable if the number of MACs is the same. The additional logic slightly alters the pipeline length, causing a minor increase in latency, but it's negligible compared to the overall data.\n\n**Q2. Output channel-wise scaling/offset.**\n> We included output channel-wise adjustments because they showed better results when learned together with rounding and input channel scaling. Since input channel-wise offset is not adjustable, any bias that may arise from input channel-wise scaling is corrected through an offset in the output channel. As can be seen in Table 4 of our ablation study, better results were achieved when all elements were adjusted together rather than learning each item separately. This approach ensures that the combined effect of these adjustments leads to more effective and balanced performance improvements.\n\n**Q3. Shift operation overhead.**\n> We can consider the scenarios for GPUs and ASICs separately. In the case of GPUs, shift operations need to be performed on accumulated values to be processed similarly to adders(accumulators). For ASICs, there are two possibilities. When using a fixed $\\gamma_y$, the operation can be implemented through wire connections, eliminating timing issues. If the number of shifts in gamma_y is variable, it can be implemented with a multiplexer (mux) for each required shift. In this context, as demonstrated in Experiment Figure 6, values of N up to 6 are meaningful, which means only a small number of multiplexers are needed.\n\n**References.**\n\nPlease refer to the \"Common Response from Authors\" mentioned above."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699843449213,
                "cdate": 1699843449213,
                "tmdate": 1699844346084,
                "mdate": 1699844346084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8YKbUgKPnu",
                "forum": "itJj6p7ssr",
                "replyto": "dvgvF7zqpw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Reviewer_LRrq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Reviewer_LRrq"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors' responses"
                    },
                    "comment": {
                        "value": "Thank you for the responses. Part of my concerns are solved. \n\nIs it because the proposed method use less scale values, thus the memory overhead during low-bit quantization can be reduced? If it was the case, could you provide some further comments about the differences with RPTQ?\n\nThe authors only explained the motivation of channel-wise scaleing/offset. But the difference between channel-wise scaling/offset and finetuning BatchNormalization is not discussed. It seems to me that these two methods are the same."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652515920,
                "cdate": 1700652515920,
                "tmdate": 1700652515920,
                "mdate": 1700652515920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I8AjGaANpw",
                "forum": "itJj6p7ssr",
                "replyto": "icYIGX93XT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LRrq"
                    },
                    "comment": {
                        "value": "I would like to express my gratitude to the reviewer for his/her response and valuable constructive feedback. To address the queries raised, I have provided more detailed responses in the section below.\n\n**Q1. Is it because the proposed method use less scale values, thus the memory overhead during low-bit quantization can be reduced? If it was the case, could you provide some further comments about the differences with RPTQ?**\n\nRather than saying memory is reduced due to the use of a small number of scale values, it would be more accurate to say that memory usage does not increase because there are predefined scale values for the model.\n\nThe comparison with RPTQ is as follows.\n\n* **Similarities**:\n\t+ Both use memory reorder to cluster channels with similar characteristics.\n\t+ Reordering allows efficient use of batch matrix multipliers (bmm).\n\n* **Differences**:\n\t* Memory usage of scale value:\n\t\t* **RPTQ**: Each cluster of each layer stores a separate 32-bit floating scale value. More clusters require additional storage for scale values.\n\t\t* **IOSO**: The model has three predetermined $\\gamma^G$ values. Each layer does not need its scale value; only one 3-bit integer (value between 1-7) is required. There are three clusters.\n\t* Optimization method:\n\t\t* **RPTQ**: Uses K-Means algorithm for clustering activation min/max.\n\t\t* **IOSO**: Finds groups that minimize reconstruction error.\n\t* Application of scale value:\n\n\t\t* **RPTQ**: On activation quantization.\n\t\t* **IOSO**: On weight and activation product of quantized value.\n\t* Optimization speed:\n\t\t* **RPTQ**: Very fast. It only requires the K-Means algorithm with a 2-dimensional space ($X_{min}$, $X_{max}$).\n\t\t* **IOSO**: Relatively slower as it needs to calculate the loss. However, it is almost similar to the round-based methods's speed as it involves additional learning parameters.\nIn summary, RPTQ is specialized for speed for application in Large Language Models, like other LLM quantization methods. Therefore, it focuses on reducing quantization error through straightforward techniques (min/max quantization, K-means). Our IOSO, although slower than LLM quantization methods, shows lower reconstruction error (quantization error).\n\nBelow are the experimental results to quantitatively compare the two methods. For quick results, please understand that we only tested the first block of resnet-18 (.model.layer1.0.conv1, .conv2). We used the same 3 clusters and only a 1024 calibration set.\n| Method | BASE | IOSO | IOSO | IOSO | RPTQ |\n|---|---|---|---|---|---|\n| **cycle** |  | 1000 | 2500 | 625 | - |\n| **Time** |  | 46s | 11s | 2s | 52.4ms \u00b1 277 \u00b5s  |\n| **Recon Loss** | 126.52 | 57.39 | 59.05 | 108.75 | 98.3 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667527939,
                "cdate": 1700667527939,
                "tmdate": 1700701719375,
                "mdate": 1700701719375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uPOJXz1yMA",
                "forum": "itJj6p7ssr",
                "replyto": "icYIGX93XT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LRrq"
                    },
                    "comment": {
                        "value": "**Q2. The authors only explained the motivation of channel-wise scaling/offset. But the difference between channel-wise scaling/offset and finetuning BatchNormalization is not discussed. It seems to me that these two methods are the same.**\n\nMethodologically, I agree with the reviewer's concern that output channel-wise scaling/offset can be achieved through batch-norm fine-tuning. Our *output-channel-wise scaling/offset* essentially adopts the parameters used in batch normalization and applies input scaling and rounding error-aware fine-tuning. And from the perspective of applying the method, we have presented the following perspectives:\n\n**1. [With] When used in conjunction with Input Channel-wise scaling and rounding-based quantization, better results can be obtained (Chapters 3.2 and 4.2).**\n\n> Input Channel scaling alone was insufficient to prevent bias from the input channel. \nFrom Equation (6), it becomes  \n$z_k= \\gamma^z_k \\alpha_k\\sum^g_p\\gamma^G_p\\sum_{i \\in G_p}w_{ki}x_{i}+\\alpha_k\\sum_{i }^{c}\\varphi^y_i +\\gamma_k^z\\beta_k+\\varphi^z_k$.\nThe presence of the $\\alpha_k\\sum_{i }^{c}\\varphi^y_i$(*input channel bias*) necessitates additional floating addition operations. \nWhen only Input Channel scaling is applied, it cannot prevent bias arising from the input channel/rounding. Therefore, to reduce this, we incorporate the role in output-wise offset and scaling to mitigate these effects.\n\n**2. [How] Applied to reduce *Block-Based* reconstruction error (Chapter 3.4).**\n>While focusing on batch-norm finetuning could be effective layer-wisely and applicable in reducing the final loss, we found the scale and offset that further reduce loss at the block level. For instance, when applying rounding/input channel-wise scaling at the block level and output channel scaling/offset at the layer level to reduce reconstruction error, there was a performance decrease of 66.62 (-0.60%) in resnet18 at 2/32 bit.\n\n**3. [Where] The placement of output scaling/offset (before/after activation) - (Appendix D.7).**\n> It's possible to apply scaling/offset after activation, not necessarily at the current batch-norm position, but we found that applying it at the conventional batch-norm position generally yields slightly better performance.\n\n**4. [Extension] Additionally, in models like transformers that do not use batch normalization, the performance can be enhanced with just 1 add/mult floating operation per activation (Chapter 3.4).**"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700467641,
                "cdate": 1700700467641,
                "tmdate": 1700701734986,
                "mdate": 1700701734986,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RVPkMHHlDE",
            "forum": "itJj6p7ssr",
            "replyto": "itJj6p7ssr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4312/Reviewer_Zj2N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4312/Reviewer_Zj2N"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends AdaRound quantization scheme and introduces a group-based scaling factor along input channel direction. A output-channel-wise, learnable scale and offset are also applied to better reconstruct the low bit cases. To lower the computational cost, author proposed to use bit-wise shifter and only allow scaling factors of 1+- 2^-N. Considering the hardware efficiency of implementing such input channel-wise methods, the author demonstrated the feasibility of setting contraints on the number of the channels per group (to be greater than a certain value) then use retraining to recover the accuracy. Extensive experiments on different CV models and comparisons with other similar PTQ schemes, such as AdaRound, AdaQuant, BRECQ, and QDROP, were provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Clear and detailed explanations about the numerical method.\n\n2. Extensive experiments results, especially the author provides statistics, i.e. 5-run mean and standard deviation, instead of best records. This will help readers to get a better idea while comparing the proposed method with different quantization schemes."
                },
                "weaknesses": {
                    "value": "1. Lack of real HW results. It's clear that the author thoroughly considered the potential limitations if the proposed method was to be implemented. However, there are still some potential concerns, such as channel permutation's impact on computation efficiency and grouping fragments effect. A few examples on a representative HW would make the paper much stronger and convincing.\n\n2. Marginal improvement compared to previous works. It is understandable that the existing methods may have already done decent jobs and the author has to use extreme low precision settings (W2A4) to demonstrate the benefit of the proposed method. However, considering the complexity of implementation and the uncertainty in compute efficiency trade-off, the author might need to find a few better examples where the use of the proposed method would be better justified."
                },
                "questions": {
                    "value": "1. Eq.9 and the explanations in the following paragraph shows that the group scaling factor, gamma_y, is a linear combination of preset gamm_G based on probability. However, that will likely make gamma_y not compatible with bit shifter. In AdaRound, the handling of h(V) is different during calibration/PTQ stage and inference stage. Author might want to clarify/comment on this part or it might cause confusion to the readers. \n\n2. Based on the example in Fig. 6, the 3 scaling factors used are 1 and 1+-2^-N. One may interpret this scheme as some input channels will be up-weighted, some will be down-weighted, and the remaining channels will be unchanged. However, the optimized factor here seems to be very small, implying that simply not applying the scaling factor might work as well? But comparing to Table 4, it seems like the case without input scaling will be close to the case of N=1? Please comment.\n \n3. Instead of using unstructured input channel-wise grouping, another frequently use quantization scheme is structured grouping, such as used by GPTQ and other LLM works. Maybe the author could consider including a few comments on the pros and cons with the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4312/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4312/Reviewer_Zj2N",
                        "ICLR.cc/2024/Conference/Submission4312/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823270986,
            "cdate": 1698823270986,
            "tmdate": 1700595125678,
            "mdate": 1700595125678,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "haGaZAbqDs",
                "forum": "itJj6p7ssr",
                "replyto": "RVPkMHHlDE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zj2N on weaknesses 1"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper thoroughly and for providing valuable comments and feedback. In response to your questions, below are detailed answers to each one. We hope you find them helpful.\n\n**W1. Real HW results**\n\n**A1-1. Our Objective**\n\n> For hardware, especially ASICs/FPGAs, to operate with low power and low area, the most computation-intensive Multiply-Accumulate (MAC) operations must be performed using integer calculations. To achieve this, weights and activations must be quantized into the smallest possible bit integers. We believe our method, compared to previously studied round-only based Post-Training Quantization (PTQ), offers an option for better accuracy with no additional cost for Output channel-wise (utilizing batch-norm) quantization and low hardware cost for Input Channel-wise operations, enhancing expressiveness.\n\n**A1-2. ASIC/FPGA Area/Latency comparison**\n\n> We conducted experiments using the structure of a batch-norm fused systolic array from an NPU developed for edge devices to compare overhead and latency in ASICs. The results below show the area of the batch-norm fused systolic array when each method is implemented in ASIC. The synthesis was done using the TSMC 12nm process. The MAC has a 16x16 structure, and IOSO, batch norm, and activation can process 8 data in 1 cycle. The sequence is MAC(->IOSO)-> BN -> ACT. For more detailed information, please refer to the \"Common Response from Authors.\" Due to time and resource constraints, the actual performance time, or latency, was tested only on the resnet 50 layer with 28x28x128 feature 3x3x128x128 kernel layer.\n\n|  | **integer Quant.** | **+IOSO** | **FP16 quant** |\n|---|---:|---:|---:|\n| **Total Area(um^2)** | 429,342 | 439,762 | 882,233 |\n| **Latency(us)** | 1129.2775 | 1129.2825 | 1129.3150 |\n\n> As seen in the table above, IOSO can be implemented with an integer shifter adder without modifying the MAC structure, resulting in only a 2.37% increase in area. In addition, the implementation is very simple as it does not alter the existing data path. In contrast, methods partially implemented with FP16 points (like GPTQ[10], QLoRA[9], LLM.int8()[11]) for increased accuracy and reduced memory are useful in GPUs that already have FP16 point kernels. However, from an ASIC perspective, these are challenging to use due to significant increases in MAC's area and power. The latency results are similar across all three methods because all data operates in a pipeline. The speed is comparable if the number of MACs is the same. The additional logic slightly alters the pipeline length, causing a minor increase in latency, but it's negligible compared to the overall data.\n\n**A1-3. IOSO overhead on GPU**\n\n|  | **Baseline(ms)** | **IOSO(ms)** | **Increased latency** |\n|:---|---:|---:|---:|\n| **resnet18** | 15.37\u00b11.37 | 17.13\u00b11.66 | 111.5% |\n| **resnet50** | 38.67\u00b10.83 | 42.26\u00b11.58 | 108.1% |\n| **mobilenet_v2** | 29.94\u00b12.74 | 34.53\u00b12.87 | 117.3% |\n| **regnetx_600m** | 36.29\u00b13.65 | 40.11\u00b11.77 | 110.8% |\n\n> We indirectly tested the latency using a non-optimized (naive) GPU. After adding the add and shift operations to the output feature in the baseline and running the ImageNet validation set with a batch size of 64, we observed the latency results. We used the RTX3080 GPU. On the non-optimized GPU, there was about a max of 17% increase in latency. We expect this value to represent the upper bound of our method's latency.  The latency could further decrease if there is support for a CUDA kernel for IOSO or operations after the accumulator, such as shift-add."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699842861052,
                "cdate": 1699842861052,
                "tmdate": 1699842861052,
                "mdate": 1699842861052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pnI18jP3x0",
                "forum": "itJj6p7ssr",
                "replyto": "RVPkMHHlDE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zj2N on weaknesses 2 and questions 1-3"
                    },
                    "comment": {
                        "value": "**W2.  The benefit of the proposed methods.**\n\n**A2-1. Orthogonally applied to round-only based PTQ**\n\n> Our method can be used as an additional option to enhance performance with low hardware cost when low-bit accuracy is insufficient in various existing round-only-based PTQ methods. During the round-only based PTQ process, which is one of the promising directions in PTQ research (e.g., Adaround[1], BRECQ[2], QDROP[3], NWQ[4]), our method can increase the representational power of the quantized network through grouping and offset/scaling. As shown in **A1-2**, our method is simple to implement and can enhance performance with a modest 2.37% increase in area. In contrast, alternative methods either require floating-point operations (e.g., QloRA[9], LLM.int8()[11]) or additional memory for group operations (e.g., GPTQ[10]). Appendix Tables 6 and 7 demonstrate that our method can be orthogonally applied to round-only based PTQ, yielding better performance in most cases.\n\n**A2-2. Ease of Implementation**\n\n> Our approach can be easily implemented by adding grouping and offset/scaling parameters while learning rounding in existing round-based Post-Training Quantization (PTQ) processes. Additionally, as described in **A1-2**, it can be easily incorporated into the existing datapath from a hardware perspective.\n\n**Q3.  Clarify group scaling factor.**\n> The value of $\\gamma_y$, as depicted in Figure 3, is predetermined as a hyper-parameter (we used values $1.0$, $1.0-2^{\u22124}$, and $1.0+2^{\u22124}$) and is determined by the calibration/PTQ stage ratio. However, these values tend to converge towards one side during calibration, allowing them to function as shift operations at inference. The variable corresponding to $h(V)$ is $s(R_{ij})$. I will offer a more comprehensive explanation of the concept of $s(R_{ij})$ to enhance clarity in the rebuttal.\n\n**Q4. Scaling factor applying**\n> As commented by the reviewer, some channels increase, others decrease, and the rest remain unchanged. However, when $N=\\infty$, channel scaling has no effect, similar to not applying any scaling at all, whereas at $N=1$, it has the most significant impact (50%, 100%, 150% scaling). As observed in Fig 6., excessive scaling ($N=1$) degrades performance, while moderate Group Gamma $N$ ($N=4$ for Weight Only, $N=6$ for Weight+Feature) yields the best results, outperforming very small scaling ($N=7$). Increasing $N$ gradually results in outcomes similar to those in Table 4's output scaling/offset case.\n\n**Q5. Compare with other input channel-wise grouping**\n> Previous works on group-wise quantization require each group to maintain different scale values, which, as indicated in QLoRA[9], become a significant memory overhead during low-bit quantization. For instance, quantizing weights for 64 groups with a single scale per group necessitates 32-bit scales for each of the 64 weights. This is akin to each weight carrying an overhead of 0.5 bits of memory. While the impact is less significant in larger bit quantizations, it becomes comparable to a 25% memory increase for 2-bit quantization.\n\n> Instead of creating groups based on unrelated weight positions, our method undergoes a calibration stage to find the optimal group for predetermined scale values. Since shuffled channels are independent, they can be reorganized through permutation, eliminating the need for additional scale information memory. Furthermore, we opted for shift operations to stay within the integer domain to avoid requiring floating point multipliers and adders if the scale multiplication remains in the floating domain. Table 4 in our ablation study demonstrates that significant performance improvements can be achieved through shift operations alone.\n\n> Similar to what was mentioned in A1-2, methods like GPTQ and LLM.int8() keep part or all of the activations in a floating point to reduce memory usage. While this is an effective approach for GPUs, where power and area are not major constraints, it poses a problem for edge devices requiring low power and small-area implementation. This distinction highlights the importance of considering the specific hardware environment when choosing a quantization strategy, as techniques suitable for high-resource settings may not be feasible or efficient in more constrained environments like edge devices.\n\n**References.**\n\nPlease refer to the \"Common Response from Authors\" mentioned above."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699843232449,
                "cdate": 1699843232449,
                "tmdate": 1700023520950,
                "mdate": 1700023520950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DvmQLbG42a",
                "forum": "itJj6p7ssr",
                "replyto": "haGaZAbqDs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Reviewer_Zj2N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Reviewer_Zj2N"
                ],
                "content": {
                    "comment": {
                        "value": "With the inclusion of ASIC and GPU HW experiment results, I have adjusted my review rating from 5 to 6. Thanks!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595215149,
                "cdate": 1700595215149,
                "tmdate": 1700595215149,
                "mdate": 1700595215149,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uA7Ss3ZqYX",
            "forum": "itJj6p7ssr",
            "replyto": "itJj6p7ssr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4312/Reviewer_J41k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4312/Reviewer_J41k"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a quantization method that exploits channel-wise scaling and offset parameters to compensate for the discrepancies in full-precision and quantized distributions. It has been shown that the proposed method outperforms existing works in terms of accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The proposed method is simple and effective in quantizing convolutional networks especially when extreme quantization levels are used.\n\n2) The paper is easy to read and understand."
                },
                "weaknesses": {
                    "value": "1) There is no discussion on why the proposed method is considered hardware-friendly. Which hardware is this work referring to? How its efficiency was measured?\n\n2) The number of operations and parameters for each quantization method must be compared along with the accuracy in Table 2 and 3. The accuracy improvement of this work is marginal for most cases. So, it's important to compare other aspects too."
                },
                "questions": {
                    "value": "See the questions listed as weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698937605664,
            "cdate": 1698937605664,
            "tmdate": 1699636399627,
            "mdate": 1699636399627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ymW8PUBHhW",
                "forum": "itJj6p7ssr",
                "replyto": "uA7Ss3ZqYX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer J41k on question 1"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper thoroughly and for providing valuable comments and feedback. In response to your questions, below are detailed answers to each one. We hope you find them helpful.\n\n**Q1. Why is it  Hardware-friendly?**\n\n**A1-1. Our Objective**\n\n> For hardware, especially ASICs/FPGAs, to operate with low power and low area, the most computation-intensive Multiply-Accumulate (MAC) operations must be performed using integer calculations. To achieve this, weights and activations must be quantized into the smallest possible bit integers. We believe our method, compared to previously studied round-only based Post-Training Quantization (PTQ), offers an option for better accuracy with no additional cost for Output channel-wise (utilizing batch-norm) quantization and low hardware cost for Input Channel-wise operations, enhancing expressiveness.\n\n**A1-2. ASIC/FPGA Area/Latency comparison**\n\n> We conducted experiments using the structure of a batch-norm fused systolic array from an NPU developed for edge devices to compare overhead and latency in ASICs. The results below show the area of the batch-norm fused systolic array when each method is implemented in ASIC. The synthesis was done using the TSMC 12nm process. The MAC has a 16x16 structure, and IOSO, batch norm, and activation can process 8 data in 1 cycle. The sequence is MAC(->IOSO)-> BN -> ACT. For more detailed information, please refer to the \"Common Response from Authors.\" Due to time and resource constraints, the actual performance time, or latency, was tested only on the resnet 50 layer with 28x28x128 feature 3x3x128x128 kernel layer.\n\n|  | **integer Quant.** | **+IOSO** | **FP16 quant** |\n|---|---:|---:|---:|\n| **Total Area(um^2)** | 429,342 | 439,762 | 882,233 |\n| **Latency(us)** | 1129.2775 | 1129.2825 | 1129.3150 |\n\n> As seen in the table above, IOSO can be implemented with an integer shifter adder without modifying the MAC structure, resulting in only a **2.37%** increase in area. In addition, the implementation is very simple as it does not alter the existing data path. In contrast, methods partially implemented with FP16 points (like QLoRA[9], GPTQ[10], and LLM.int8()[11]) for increased accuracy and reduced memory are useful in GPUs that already have FP16 point kernels. However, from an ASIC perspective, these are challenging to use due to significant increases in MAC's area and power. The latency results are similar across all three methods because all data operates in a pipeline. The speed is comparable if the number of MACs is the same. The additional logic slightly alters the pipeline length, causing a minor increase in latency, but it's negligible compared to the overall data.\n\n**A1-3. IOSO overhead on GPU**\n|  | **Baseline(ms)** | **IOSO(ms)** | **Increased latency** |\n|:---|---:|---:|---:|\n| **resnet18** | 15.37\u00b11.37 | 17.13\u00b11.66 | 111.5% |\n| **resnet50** | 38.67\u00b10.83 | 42.26\u00b11.58 | 108.1% |\n| **mobilenet_v2** | 29.94\u00b12.74 | 34.53\u00b12.87 | 117.3% |\n| **regnetx_600m** | 36.29\u00b13.65 | 40.11\u00b11.77 | 110.8% |\n\n> We indirectly tested the latency using a non-optimized (naive) GPU. After adding the add and shift operations to the output feature in the baseline and running the ImageNet validation set with a batch size of 64, we observed the latency results. We used the RTX3080 GPU. There was about a max 17% increase in latency on the non-optimized GPU. We expect this value to represent the upper bound of our method's latency. The latency could further decrease if there is support for a CUDA kernel for IOSO or operations after the accumulator, such as shift-add."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699842498284,
                "cdate": 1699842498284,
                "tmdate": 1699842498284,
                "mdate": 1699842498284,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q8HujZQxc3",
                "forum": "itJj6p7ssr",
                "replyto": "uA7Ss3ZqYX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer J41k on question 2"
                    },
                    "comment": {
                        "value": "**Q2.  Compare other aspects.**\n\n**A2-1. Orthogonally applied to round-only based PTQ**\n\n> Our method can be used as an additional option to enhance performance with low hardware cost when low-bit accuracy is insufficient in various existing round-only-based PTQ methods. During the round-only based PTQ process, which is one of the promising directions in PTQ research (e.g., Adaround[1], BRECQ[2], QDROP[3], NWQ[4]), our method can increase the representational power of the quantized network through grouping and offset/scaling. As shown in **A1-2**, our method is simple to implement and can enhance performance with a modest 2.37% increase in area. In contrast, alternative methods either require floating-point operations (e.g., QloRA[9], LLM.int8()[11]) or additional memory for group operations (e.g., GPTQ[10]). Appendix Tables 6 and 7 demonstrate that our method can be orthogonally applied to round-only based PTQ, yielding better performance in most cases.\n\n**A2-2. Differentiation of previous works on group-wise quantization**\n\n> Previous works on group-wise quantization require each group to maintain different scale values, which, as indicated in QLoRA[9], become a significant memory overhead during low-bit quantization. For instance, quantizing weights for 64 groups with a single scale per group necessitates 32-bit scales for each of the 64 weights. This is akin to each weight carrying an overhead of 0.5 bits of memory. While the impact is less significant in larger bit quantizations, it becomes comparable to a 25% memory increase for 2-bit quantization.\n\n> Instead of creating groups based on unrelated weight positions, our method undergoes a calibration stage to find the optimal group for predetermined scale values. Since shuffled channels are independent, they can be reorganized through permutation, eliminating the need for additional scale information memory. Furthermore, we opted for shift operations to stay within the integer domain to avoid requiring floating point multipliers and adders if the scale multiplication remains in the floating domain. Table 4 in our ablation study demonstrates that significant performance improvements can be achieved through shift operations alone.\n\n**References.**\n\nPlease refer to the \"Common Response from Authors\" mentioned above."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699842654664,
                "cdate": 1699842654664,
                "tmdate": 1699842654664,
                "mdate": 1699842654664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "488LeZI7aU",
            "forum": "itJj6p7ssr",
            "replyto": "itJj6p7ssr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4312/Reviewer_yDxC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4312/Reviewer_yDxC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an improved post training quantization (PTQ) method called IOSO. The method relies on adjusting the scale and offset in activations (both input and output). The benefits are shown for ImageNet on a diverse set of DNNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper is well-written and easy to understand.\n2) The paper addresses a well-addressed problem and tries to make a contribution.\n3) The improvements in accuracy are non-trivial in some scenarios.\n4) The computational overhead is small < 1.5%.\n5) Error bars are shown for network accuracy."
                },
                "weaknesses": {
                    "value": "1. The results in Table 2 are incremental. This is not surprising since post-training quantization has been studied extensively.\n2. The proposed methods minimizes layer-wise reconstruction error. Reconstruction error is a proxy for misclassification error, which is the ultimate metric. Comparison with methods that directly minimize misclassification error is missing."
                },
                "questions": {
                    "value": "1. There are PTQ methods, e.g., [1], that minimize the probability of misclassification. How does your method compare with those?\n[1] Sakr et. al. Analytical guarantees on the numerical precision of deep neural networks, ICML'17.\n2. Have you considered fine-tuning your results by doing some limited amount of training, say one epoch? It will be interesting to see if the accuracy improves significantly.\n3. Can your method be extended to cover training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4312/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698969534832,
            "cdate": 1698969534832,
            "tmdate": 1699636399564,
            "mdate": 1699636399564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o9zlMx4N2z",
                "forum": "itJj6p7ssr",
                "replyto": "488LeZI7aU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4312/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yDxC"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper thoroughly and for providing valuable comments and feedback. In response to your questions, below are detailed answers to each one. We hope you find them helpful.\n\n**Q1. How does your method compare to PTQ methods that aim to minimize misclassification?**\n\n**A1-1**. Compared with \u201cAnalytical guarantees on the numerical precision of deep neural networks.\u201d\n\n> Compared to the paper \"Analytical guarantees on the numerical precision of deep neural networks,\"[8] they calculated the fixed-point operation cost from a computational/storage space perspective. They analyzed the theoretical bounds when transitioning from floating point to fixed point operations. While similarly focusing on computational and representation costs, our method differs in its approach. While [8] theoretically specified an upper bound for bits and solved it with fixed quantization, our method learns to minimize each layer's reconstruction loss through a calibration set.\n\n**A-2** Compared with other PTQ methods.\n\n> Compared with other PTQ methods, approaches like GPTQ, ZeroQuant, LLM.int8(), and QLoRA have been recently proposed. \nWe aim to enable integer operations of MAC calculations, the major computational element, for low power/area efficiency in hardware, especially ASIC/FPGA. This requires quantizing weights and activations into as few bits of integers as possible. \nTraditional methods only quantized the weight, necessitating MAC operations in FP32/16 (as in GPTQ[10], QLoRA[11]),  partially retained FP16 (like in LLM.int8()[9]) or demanding additional memory for Group Quantization. These methods significantly increased power and area costs in ASIC/FPGA hardware developed mainly for GPU memory reduction without accuracy loss. \nOur method differs as it improves the performance of traditional round-only based methods (such as AdaRound, BRECQ[1], and QDROP[2]) without significantly increasing the cost in ASIC/FPGA settings. \n\n**Q2. Training amount limit?**\n\n**A2**. \n> Our method involves finding appropriate rounding values while simultaneously performing grouping and scaling/offset. We couldn't achieve favorable results with just one epoch (1024 samples). Also, fixing the rounding and performing grouping later did not yield satisfactory outcomes.\nAdditionally, rounding and grouping use a temperature hyperparameter, and the importance of each determines the specific group/rounding. Therefore, a certain number of epochs are necessary. In the presented results, we used 35,000 cycles (about 2187 epochs) and found that at least 15,000 cycles (about 937 epochs) are required to achieve satisfactory results.\n\n**Q3. Conventional training?**\n\n**A3**. \n> Based on my understanding of the reviewer's question, it seems to refer not to post-training quantization but to the use of our method in normal training. We assumed training minimizes changes to the given pre-trained weights within a limited dataset (small calibration set). Therefore, we calibrated the network using rounding and scaling/offset.\nApplying our method to normal training without quantization seems meaningless. If the optimal point requires specific channel scaling/biasing, training the weights in that direction would be more appropriate.\nHowever, the method might be meaningful in quantization-aware training (QAT) in low-bit quantization. It can enhance the representational power of quantized weights/activation through scaling factors and offsets.\n\n###  **References.**\n\nPlease refer to the \"Common Response from Authors\" mentioned above."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4312/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699841866148,
                "cdate": 1699841866148,
                "tmdate": 1699841898085,
                "mdate": 1699841898085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]