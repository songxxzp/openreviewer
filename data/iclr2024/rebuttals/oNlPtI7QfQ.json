[
    {
        "title": "Embed-Search-Align: DNA Sequence Alignment using Transformer models"
    },
    {
        "review": {
            "id": "oIGJt9izyF",
            "forum": "oNlPtI7QfQ",
            "replyto": "oNlPtI7QfQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3168/Reviewer_nXVu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3168/Reviewer_nXVu"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an embedding for DNA sequences based on transformer models. This embedding allows a distance between two DNS fragments of potentially differing lengths to be computed. Using this distance, an alignment algorithm is constructed under the usual seed-align framework, with the embedding distance taking the place of the seeding step. The embedding quality is assessed against other transformer models on synthetic reads."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem being tacked is an important one, and the proposed architecture does appear to have some benefits over existing transformer models. The ability to handle varying fragment lengths makes the method very flexible."
                },
                "weaknesses": {
                    "value": "The experimental validation is the weak point of this paper. There are claims of efficiency yet no experimental evidence of any resource requirements and scaling such as time and memory. Furthermore, comparitive experiments are against existing transformer models and hence show that the embedding is superior to existing ones for alignment, but not that this is a good aligner overall. There are no comparisons against standard alignment algorithms (though they are referenced). There are no experiments on real data, or evaluation of the effect on downstream variant calling.\n\nGeneralisation to other species is very interesting, however the chosen species are all closely related.\n\nThe constraint in equation 4 cannot strictly hold on real data due to homology and repeat regions."
                },
                "questions": {
                    "value": "- Have you evaluated the method on real data?\n- What are the runtime resource requirements?\n- Does the model generalise to less related organisms? Presumably there will be some dependence on degree of homology."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698361784398,
            "cdate": 1698361784398,
            "tmdate": 1699636264342,
            "mdate": 1699636264342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QLj2PbhoTg",
                "forum": "oNlPtI7QfQ",
                "replyto": "oIGJt9izyF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3168/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3168/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and kindly point them to the general comments in addition to specific comments below. Kindly also review the responses to other reviewers.\n\n**[Q1] Have you evaluated the method on real data/data with noise?**\n\nBefore we discuss the additional evaluation results we have included in the revised version, we wanted to address why we used ART as a simulation toolkit. An important consideration was that for such reads, we know the ground truth (making evaluation much more precise) while ensuring that the reads have the same distribution of errors as one would expect from real world sequencers. \n\nIn fact, ART is an industry-standard simulator that is used to benchmark almost every new aligner that is introduced. ART mimics the variants, insertions/deletions (INDELs) and other error models found in reads generated by expensive Illumina machines. The ART simulator used in our experiments was fine-tuned on an Illumina MiSeqv3 machine model (https://www.illumina.com/systems/sequencing-platforms/miseq/specifications.html). \nBWA-Mem and several other aligners have been validated on such simulated data: Table 1 here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2705234/ demonstrates performance on WGSIM from SAMTools, which is a more primitive version of ART (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-184). ART was also used in the simulation study of the 1000 Genomes datasets to profile the quality of the resource: https://www.niehs.nih.gov/research/resources/software/biostatistics/art/index.cfm\n\nWe also want to note that Pure Reads serve an important purpose: They establish an upper bound on the performance of the aligner (as noisy reads would only have lower performance), allowing us to benchmark the maximum possible performance for Transformer-based LLM models as shown in Fig. 2. These pure-read results make it superfluous to benchmark the existing Transformer models on noisy data / real data. \n\nIn the revision: While we believe that ART reads with $Q_{PH} >= 30$  are sufficient to benchmark aligners -- Next-gen Illumina machines generate reads at Phred score $Q_{PH} >= 30$, we address the reviewers\u2019 request by evaluating DNA-ESA on reads with $Q_{PH} \\in [10,20]$, $I,D = 1$%; see Table 4 (left).\n\nWe are pleased to report that DNA-ESA performance at $Q_{PH} \\in [10,20]$, $I,D = 1$% is only about $1$% less than the performance reported at $Q_{PH} \\in [30,60]$, $I,D = 0.01$%.\n\nPerformance on real read data: In Table 4, we have also included the performance of DNA-ESA on real reads generated by PacBio CCS ($Q_{PH} = 20$): and read length $\\in \\{250, 350, 500\\}$. \n\nHere, too, DNA-ESA performs as well on real read data as on ART-generated reads reported in our original submission.  This further supports our original evaluation platform.\n\n\n**[Q2] What are the runtime resource requirements?**\n\nThe model requires a GPU with >10GB of vRAM to run inference on DNA-ESA. The model uses 20GB of CPU memory during inference. This is an initial version of the model, and as noted in Sec. Future Work, will continually become more performant using methods such as model distillation, compilation, and distributed processing techniques. Efficient transformers are still an area of active research which have seen notable improvements over the last few years (e.g. FastAttention, Long-range architectures such as HyenaDNA and similar).\n\n**[Q3] Does the model generalize to less related organisms? Presumably there will be some dependence on the degree of homology.**\n\nWe thank Reviewer 3 for their curiosity in understanding whether the model generalizes to less related organisms. Accordingly, we have expanded the inter-species experiments in the transfer learning section (Table 3) to include Thermus Aquaticus -- an ancient extremophile bacterium -- and Acidobacteriota reference sequences: two species that are highly dissimilar to the rest. We are pleased to report that the model -- originally trained to align reads to fragments on Chr. 2 of humans -- continues to perform well (in fact even with top-K = 1, the recall is >93%)! This adds further support to the claim that the DNA-ESA architecture truly learns the alignment task rather than the sequences themselves seen during training."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3168/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682194391,
                "cdate": 1700682194391,
                "tmdate": 1700682827955,
                "mdate": 1700682827955,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rtlSgl8l0r",
            "forum": "oNlPtI7QfQ",
            "replyto": "oNlPtI7QfQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3168/Reviewer_o7rD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3168/Reviewer_o7rD"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript tries to solve an alignment problem. The input are a set of short DNA sequences (reads) and a long reference sequence. The task is to find the location of each short read on the reference sequence. This manuscript proposes to solve this matching problem by projecting both short reads and reference (sub-)sequence to a shared embedding space such that reads originated from the same reference sequence region are close to each other in the embedding space. For a new read, its embedding is first calculated using the NN, after that it searches for the nearest K reference fragments in the embedding space. Exact alignment using classical Smith-waterman algorithm is between the given read and each of the K nearest reference fragments to identify the final mapping position of the read.\n\nRegarding implementation, the authors first split the reference sequence (length = 3Gb) into fragments (length = 1250bp). Then each fragment and their substrings (reads) are assigned the same label, which (a batch of several fragments and their substrings) is then fed into a transformer with a contrastive loss. The trained network can then project a given DNA sequence to the embedding space (dimension = 384). \n\nAlthough I think the idea is somewhat interesting, the authors may want to address the following points in the next version.\n1. Provide support to the hypothesis that a fragment and its subsequences are close in the embedding space. Figure 1 only contain the fragments, but it should also contain random subsequences. \n2. Make the notations more readable, e.g. in Eq. 2, q is used in SA(r,R). Eq. 6, really difficult to guess the meanings.\n3. Comparison with classic aligner such as BWA or bowtie. I will expect a >99.9% mapping rate of BWA or bowtie for the used data.\n4. Expand the comparison dimensions. Only Recall is used, could add precision, mapping rate etc.\n5. Compare on real data. The simulated data is too clean. Phred 30 is already 99.9% identical to the original sequence, that is 250/1000=0.25 bp mutation per read. It will be nice to see how the method works on real data where the error rate is higher and the read length varies.\n6. Provide details of the transformer. Do you do paddings to short reads?\n7. What happens to negative samples, i.e. a random read that is not similar to any reference fragment? How do you decide the boundaries of a reference fragment in the embedding space?\n8. Reorder the figures so it flows with the text.\n9. Appendix A, add legend for the lines\n10. Appendix B, B.3 the complexity is G, not log(G) as you have to compare with all reference fragments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "NA"
                },
                "weaknesses": {
                    "value": "NA"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794296923,
            "cdate": 1698794296923,
            "tmdate": 1699636264246,
            "mdate": 1699636264246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gzwGRPMQyO",
                "forum": "oNlPtI7QfQ",
                "replyto": "rtlSgl8l0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3168/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3168/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We urge the reviewer to kindly read the general comments as well as our replies to other reviewers.\n\n**[Q1] Provide support to the hypothesis that a fragment and its subsequences are close in the embedding space.**\n \nWe agree with the reviewer and we have updated Figure 1 to visualize both reads and fragments. We see convincing visual evidence that both the reads and the fragments align within the embedding space. \n\n**[Q2] Make the notations more readable, e.g. in Eq. 2, q is used in SA(r,R). Eq. 6, really difficult to guess the meanings.**\n\nWe have resolved this discrepancy and now use $q$ in Eq. 2 as well to denote the start position. Furthermore, for Eq. 6, we realize that the asterisks on $F$ and $q$ are difficult to read. So we have added clarity to the equation in the new version.\n\n**[Q3] Comparison to existing classical methods (Bowtie-2, Minimap, BWA-Mem-2)**\n\nWe have presented the side-by-side results of Bowtie-2 and DNA-ESA in Table 4 on two noisy datasets. We are happy to report that the performance of Bowtie-2 is only 3-4% above the recall performance of DNA-ESA. The quality of the recalled fragments (of those reads that are successfully aligned) is comparable among the two models.\n\n**[Q4] \u201cProvide details of the transformer. Do you do paddings to short reads?\u201d**\n\nClarified section 3.3: \u201cReads and shorter fragments were padded to equal the length of the longest sequence sampled in a batch.\u201d\n\n**[Q5] What happens to negative samples, i.e. a random read that is not similar to any reference fragment?**\n\nWe thank the reviewer for the insight about including additional metrics such as Accuracy, Precision and F1 score in addition to Recall: the classic case of evaluating specificity vs. sensitivity. We are happy to report in Table 2 that these metrics are all high: negative samples were drawn from Chr. 3 with positive samples from Chr. 2.\n\n**[Q6] How do you decide the boundaries of a reference fragment in the embedding space?**\n\nWe are unclear about what the reviewer means by boundary. Our guess is that the reviewer asks for clarification about how we go to the exact location of a read from the top-K fragments. Note that each fragment has meta-data corresponding to the reference location from which the fragment is drawn.\nBelow we also briefly recap the alignment process.\n(Step 1) Identify the long fragments corresponding to the most likely match to a read. Top-K neighbor is computed in log complexity; (Step 2) Fine-alignment of the read to the fragments: Compute Smith-Waterman distance between the read and each of the candidate fragments. This distance metric is thresholded. We sweep across several of these thresholds in Table 3 and describe the process in Appendix C.1.\n\n**[Q7] Reorder the figures so it flows with the text.**\n\nWe extend our gratitude to the reviewer for their valuable comments. We hope to convince the reviewer that the current order is justified. \n\nFigure 1 plays a pivotal role by immediately illustrating the generated embedding space, which forms the foundation for all subsequent downstream applications. This depiction holds particular appeal for Machine Learning enthusiasts, as it showcases the training method's unique ability to generate trajectories within the embedding space.\n\nFigure 2 provides a side-by-side performance comparison of various Transformer-based models on pure reads (reads without any noise). Despite the apparent simplicity of this task, it becomes evident that competing Transformer-based models struggle to address it. Finally, with the motivation firmly established, Figures 3 and 4 delve into the intricacies of the method, describing the finer points of how these representations are generated.\n\n**[Q8] Appendix A, add legend for the lines**\n\nWe have added the following to the figure text: \u201cDNA-ESA convergence plots. Each plots show the loss pr. step (grey). For clarity, we smooth the loss using a moving average (black).\u201d\n\n**[Q9] Appendix B, B.3 the complexity is G, not log(G) as you have to compare with all reference fragments.**\n\n*It's crucial to highlight that the search complexity across G is, indeed, logarithmic (log(G)).* \n\nThis logarithmic scaling represents a fundamental advantage derived from utilizing vector stores in our task. Without the efficiency gained from vector comparisons, resorting to direct string representations for matching the read with every fragment might as well have worked.\n\nVector stores opt for a highly accurate **approximate** K-nearest neighbor method in addition to other proprietary optimizations to search that include Small-World approximations, hierarchical organization of graph data and concurrent search. This is the entire reason behind FAISS\u2019 ubiquity and Pinecone\u2019s rapid success as a vector store solution. This innovation facilitates the breakthrough in our representation learning solution to alignment."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3168/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681697622,
                "cdate": 1700681697622,
                "tmdate": 1700681697622,
                "mdate": 1700681697622,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PDLvj6NbeX",
            "forum": "oNlPtI7QfQ",
            "replyto": "oNlPtI7QfQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3168/Reviewer_ModJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3168/Reviewer_ModJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents EMBED-SEARCH-ALIGN, a method for DNA sequence alignment using transformer models.  Unlike traditional methods use algorithmic solutions like indexing and efficient searching to align reads. This paper proposes a different paradigm using transformer models to learn representations of DNA reads and reference genome fragments, and performing alignment based on similarity of these embeddings.\n\nThe main contributions of the proposed architecture are as  follows. First, the authors use a DNA sequence encoder DNA-ESA that is trained using self-supervision and contrastive loss to produce DNA sequence embeddings optimized for alignment. Next, the authors use a DNA vector store to enable efficient nearest neighbor search across the entire reference genome for each read. Finally, the authors formulate the sequence alignment as an \"embed-search-align\" task using the encoder and vector store.\n\nIn the experiments, the authors demonstrate that DNA-ESA can align 250 bp reads to the human reference genome with over 97% accuracy, exceeding several transformer baseline models. The approach also demonstrates ability to generalize - a model trained only on Chr 2 can still align reads from other chromosomes and even other species.\n\nThe paper argues this approach mitigates limitations of prior works on transformer models for genomics and provides sequence representations suitable for alignment. It also enables \"flat search\" over reads and reference fragments of different lengths. Future work is discussed to improve computational efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The authors present a novel approach to align sequence reads which can provide new possibilities for DNA sequence representation and search.\n+ The proposed DNA-ESA encoder learns effective sequence embeddings for alignment, and outperforms several baseline transformer models designed for specific genomics tasks.\n+ The approach is promising and demonstrates ability to generalize to new sequences not seen during training, like different chromosomes and even new species. Furthermore, formulating alignment as embed-search-align could enable new capabilities like \"flat search\" over reads and reference fragments of different lengths."
                },
                "weaknesses": {
                    "value": "- I felt that the paper is a very dense read for the general ML audience at ICLR for folks who do not have DNA sequencing background, and it will be great to make the paper more accessible.\n- The embedding approach currently shows promising results on simulated data, but needs more evaluation on real sequencing data.\n- The performance for short reads is worse than long reads, given that short reads are more commonly used, this may affect how this system can be actually used.\n- Limited demonstration of applications in downstream genomic tasks."
                },
                "questions": {
                    "value": "1) Can the authors comment on how this paper is a good fit for ICLR, and the steps the author may take to make this paper more accessible to the ML audience?\n2) How does the model perform on real world sequencing data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3168/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3168/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3168/Reviewer_ModJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699034266375,
            "cdate": 1699034266375,
            "tmdate": 1700704821155,
            "mdate": 1700704821155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gTPxGEorAT",
                "forum": "oNlPtI7QfQ",
                "replyto": "PDLvj6NbeX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3168/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3168/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer 1 for their comments about our submission. We appreciate their recognizing the promise of the approach, specifically (a) the effectiveness of the sequence embeddings generated by DNA-ESA (as demonstrated in search); (b) the generalization across species and the flexibility of the approach in encoding reads and fragments of varying lengths. \n\nWe urge the reviewer to also review the general comments in addition to our responses to other reviewers. Below we address the specific concerns:\n\n**[Q1] \u201cI felt that the paper is a very dense read for the general ML audience at ICLR for folks who do not have DNA sequencing background, and it will be great to make the paper more accessible.\u201d**\n\nAcknowledging the interdisciplinary nature of our work, we concur with the reviewer's perspective that it is our responsibility, as the authors, to bridge the gap between cutting-edge techniques in NLP and the longstanding challenges in bioinformatics and genomics. To address this, we have taken the following steps:\n\nIn Eq. 1, we define a read in its simplest form: a sequence of bases. In Eq.s 2 and 3, we write the objective of sequence alignment through the application of the sharding rule [P1]. Furthermore, in Section 3.1, we present the rationale behind a straightforward inequality constraint Eq. 4 (modified now to incorporate homologs / repeats) within the representation space.\n\nIt is worth noting that our approach to motivating sequence alignment, as outlined, is itself innovative and designed to resonate with the machine learning community. We remain optimistic that the reviewer finds merit in this novel perspective.\n\n\n**[Q2] \u201cThe performance for short reads is worse than long reads, given that short reads are more commonly used, this may affect how this system can be actually used.\u201d**\n\nThe reviewer rightly notes that short-read aligners are more commonly used than long-read aligners. Our approach does not intend to replace existing short-range alignment methods. Rather, methods for sequencing have seen rapid advances in cost and their ability to produce increasingly longer reads (PacBio and Oxford Nanopore, up to 10s of kilobases long), and accordingly, aligners to map such long reads are also of recent interest in the bioinformatics community.\n\n**[Q3] \u201cCan the authors comment on how this paper is a good fit for ICLR, and the steps the author may take to make this paper more accessible to the ML audience?\u201d**\n\nRepresentation learning holds a pivotal role within the framework of ICLR. The treatment of DNA sequences demands a computational approach grounded in deep learning, as evidenced by numerous recent works in the field [1,2,3]. Notably, the guidelines for ICLR explicitly outline a track dedicated to \"Applications in biology, and related fields.\"\n\nIn our endeavor to make our paper accessible to the broader ICLR community, which includes individuals with expertise in machine learning, we kindly direct the reviewer's attention to the first part of the response. We believe our work aligns with the conference's objectives, particularly within the specified track.\n\n\n[1] Nguyen, Eric, et al. \"Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution.\" arXiv preprint arXiv:2306.15794 (2023).\n\n[2] Dalla-Torre, Hugo, et al. \"The nucleotide transformer: Building and evaluating robust foundation models for human genomics.\" bioRxiv (2023): 2023-01.\n\n[3] Ji, Yanrong, et al. \"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome.\" Bioinformatics 37.15 (2021): 2112-2120."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3168/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682006067,
                "cdate": 1700682006067,
                "tmdate": 1700682006067,
                "mdate": 1700682006067,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XyWlAIKFUY",
                "forum": "oNlPtI7QfQ",
                "replyto": "gTPxGEorAT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3168/Reviewer_ModJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3168/Reviewer_ModJ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Dear authors, \n\nThanks for the response. I do like the emphasis on semantic embedding in the general response above. I believe emphasizing and evaluating that in the paper will make it stronger. For example, the papers you cited, DNABERT, and nucleotide transformer perform well for standard benchmarks but the quality of embeddings is unclear. Specifically, for many downstream DNA tasks, specific and often rare patterns of DNA are important for classification, and if your paper can capture that better, it will be a significant result. My other comment on demonstrating the use in multiple downstream tasks is also important. ICLR indeed has a section for biology but it is important to make the paper more accessible than it currently is. For example, the papers HyenaDNA and nucleotide transformer have a fairly accessible Section 1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3168/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704795239,
                "cdate": 1700704795239,
                "tmdate": 1700704795239,
                "mdate": 1700704795239,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]