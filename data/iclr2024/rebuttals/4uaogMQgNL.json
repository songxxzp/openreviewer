[
    {
        "title": "UpFusion: Novel View Diffusion from Unposed Sparse View Observations"
    },
    {
        "review": {
            "id": "E5VIlReyaz",
            "forum": "4uaogMQgNL",
            "replyto": "4uaogMQgNL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4145/Reviewer_rcbB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4145/Reviewer_rcbB"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce UpFusion, a view synthesis method derived from a collection of unposed images. The core design philosophy of UpFusion centers around the use of a Scene Representation Transformer, combined with a Diffusion Model to infuse intricate object details. Subsequently, instance-specific neural representations are introduced to achieve 3D-consistent rendering outcomes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Scene Representation Transformer (SRT) is renowned as an effective neural renderer that can seamlessly generalize to the rendering of novel scenes and views. However, the SRT represents an image in the latent space, often resulting in blurry rendered outcomes, as evidenced in Fig 2 of this paper. UpFusion examines the constraints in SRT and suggests employing a following diffusion model and an instance-specific 3D representation to enrich the details. Specifically, the denoising diffusion model and a control net branch are employed to master a generative model for novel views of an object, and the instance-specific representation adheres to the paradigm in Score Distillation Sampling to extract a consistent 3D representation."
                },
                "weaknesses": {
                    "value": "- The reviewer recommends that the authors emphasize the object-level NVS configuration in the title since the methodology chiefly addresses \"objects\" and the experiments were executed on the \"CO3D\" dataset.\n- Object-level 3D generation (sourced from unposed images or a single image) remains a hot research topic. There exists a plethora of related papers [1,2,3, 4]. However, pivotal experimental comparisons with [1,2,3,5] are missing. Notably, single-view based NVS can ignore the requirment for camera poses: [1,2,3] all necessitate an object-specific representation, while [5] solely requires a forward-pass for view generation.\n- What are the specifics regarding the training time for each instance? Considering [4] also employs a 3D representation to tackle a similar scenario, but does not incorporate the SRT and diffusion model, it's useful for the authors to showcase the merits of solely leveraging a 3D representation. Further, comparisons excluding the SRT/Diffusion model or contrasting it against [4] would be insightful.\n- The CO3D dataset is characterized by various backgrounds, yet the authors omit the background modeling in the manuscript (possibly using the masks in CO3D). Even though object-level NVS publications typically sidestep background modeling, it's essential to accentuate this specific operations in the experimental framework.\n- In terms of the claim 3D consistent generation, it would be useful if the authors could provide diverse rendered video of different objects, as well as producing metrics for your claim.\n\n[1] One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization  \n[2] Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior  \n[3] NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors  \n[4] Few-View Object Reconstruction with Unknown Categories and Camera Poses  \n[5] Zero-1-to-3: Zero-shot One Image to 3D Object"
                },
                "questions": {
                    "value": "See the recommendated experiments in **Weaknesses** ."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774655569,
            "cdate": 1698774655569,
            "tmdate": 1699636379871,
            "mdate": 1699636379871,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6OWQtE2UNe",
                "forum": "4uaogMQgNL",
                "replyto": "E5VIlReyaz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. In this section, we elaborate on concerns that are not covered in the common rebuttal.\n\n> There exists a plethora of related papers [1,2,3, 4]. However, pivotal experimental comparisons with [1,2,3,5] are missing.\n\nWe have updated the related works section to include discussion about more single-view methods, and will incorporate the reviewer feedback to emphasize the \u2018object-centric\u2019 of our presentation. Moreover, we have included comparisons against some state-of-the-art single-view baselines, details of which are elaborated in the common rebuttal section.\n\n> What are the specifics regarding the training time for each instance?\n\nThe neural mode distillation stage currently takes a little more than an hour to optimize per instance. We have included these details in the supplementary material. \n\n> The authors omit the background modeling in the manuscript (possibly using the masks in CO3D)\n\nWe do indeed use the masks provided in CO3D and sidestep modeling the background. We have clarified this point in the main text.\n\n> it would be useful if the authors could provide diverse rendered video of different objects, as well as producing metrics for your claim (of 3D consistency)\n\nWe have included GIFs that showcase objects estimated by UpFusion 3D in this anonymized [website](https://iclr24-paper-4145.pages.dev/). Moreover, UpFusion 3D is 3D consistent by construction since we optimize a NeRF to represent an object (and NeRFs are 3D consistent).\n\n> Further, comparisons excluding the SRT/Diffusion model or contrasting it against [4] would be insightful\n\nAs recommended by the reviewer, we include quantitative and qualitative comparisons against FORGE (see table 3 and figure 6; and also the common rebuttal). We note that our model significantly outperforms FORGE when using 6 input views, and we believe this is because our approach allows bypassing explicit pose prediction which can lead to inaccurate predictions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657535119,
                "cdate": 1700657535119,
                "tmdate": 1700657535119,
                "mdate": 1700657535119,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v0KYX061c2",
            "forum": "4uaogMQgNL",
            "replyto": "4uaogMQgNL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4145/Reviewer_UGEs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4145/Reviewer_UGEs"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on view synthesis from unposed images. Scene Representation Transformer, diffusion model, and controlnet branch are utilized to effectively perform the task on object-level novel view synthesis."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "SRT + Diffusion are adopted to study the challenging task of novel view synthesis from unposed images."
                },
                "weaknesses": {
                    "value": "The experiments are conducted on object-level generation. As the authors also mentioned in the related works section, single view image-to-3d is highly related to the task UpFusion trying to solve. Single view input can also be considered as input image without pose. As a result, I believe the contributions of UpFusion can be better justified when comparing to existing single view novel view synthesis works, for example [1].\nBesides, a couple of references are missing [2] (also on Co3D dataset), [3][4] (SDS-based).\n\n[1] Zero-1-to-3: Zero-shot one image to 3d object\n\n[2] NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors\n\n[3] RealFusion: 360\u00b0 Reconstruction of Any Object from a Single Image\n\n[4] NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360 views"
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817779939,
            "cdate": 1698817779939,
            "tmdate": 1699636379802,
            "mdate": 1699636379802,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WbA8fwED9N",
                "forum": "4uaogMQgNL",
                "replyto": "v0KYX061c2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback.  We have added the references pointed out to our discussion.\n\n> I believe the contributions of UpFusion can be better justified when comparing to existing single view novel view synthesis works\n\nWe have included comparisons against some state-of-the-art single-view baselines, details of which are elaborated in the common rebuttal section. In particular, we find that our approach is competitive with the SoTA single-view methods given 1 input image, but can scale with more views to outperform the single-view baselines given additional (unposed) images."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657495379,
                "cdate": 1700657495379,
                "tmdate": 1700657495379,
                "mdate": 1700657495379,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EgdZetBGJt",
            "forum": "4uaogMQgNL",
            "replyto": "4uaogMQgNL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4145/Reviewer_yMjS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4145/Reviewer_yMjS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework for 3D-aware novel view synthesis given sparse 2D views without camera poses. Leveraging the features of the 2D views from an UpSRT encoder-decoder, it predicts a view-aligned spatial feature for the target view and a set-invariant feature, and feeds them to Stable Diffusion as conditioning inputs to generate the novel views. Moreover, it also incorporates an underlying 3D representation with NeRF to further enforce view consistency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper combines view-aligned spatial features and set-invariant features from unposed 2D views and leverages in novel-view diffusions, which sounds natural.\n- Both qualitative and quantitative results show that the proposed framework outperforms the prior works not integrating these modules."
                },
                "weaknesses": {
                    "value": "- Experimental comparisons with existing works seem limited. The following works are also related and could be discussed in the paper: [1,2,3,4,5]. Several of these works, as well as the single-/few-view NeRF synthesis works mentioned in the related work section, can be compared with the proposed method experimentally. Specifically, the single-view works also don't rely on relative poses, though the setup is not exactly the same as this paper, they can still be compared.\n- Quantitatively, the UpFusion 3D model has much better numbers than the 2D model, but visually it loses a lot of geometric details compared to the 2D results. Is it limited by the representation power of the 3D NeRF? Or is it because the learned features are not very view-consistent?\n\n[1] Ye, Yufei, Shubham Tulsiani, and Abhinav Gupta. \"Shelf-supervised mesh prediction in the wild.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n[2] Deng, Congyue, et al. \"Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Tang, Junshu, et al. \"Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior.\" arXiv preprint arXiv:2303.14184 (2023).\n[4] Liu, Minghua, et al. \"One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization.\" arXiv preprint arXiv:2306.16928 (2023)."
                },
                "questions": {
                    "value": "- Comparing the 2D and the 3D UpFusion model, I understand that the PSNR and SSIM of the 3D model are better, but why is the LPIPS also better? -- LPIPS is not a pixel-aligned metric, while visually the 2D results look cleaner and have much more details than the 3D ones.\n- It seems that the generated views sometimes have inconsistent colors as the input view (e.g. the blue bench and the blue umbrella in Fig. 8, Appendix A). Is there any explanation for this?\n- I wonder how the proposed method compares to this baseline: first running COLMAP to estimate the relative poses of the input views, and then running a pose-dependent 3D synthesis method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698880629537,
            "cdate": 1698880629537,
            "tmdate": 1699636379728,
            "mdate": 1699636379728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fsgXhWQ3pu",
                "forum": "4uaogMQgNL",
                "replyto": "EgdZetBGJt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. In this section, we elaborate on concerns that are not covered in the common rebuttal. \n\n\n> The following works could be discussed in the paper: [1,2,3,4,5] .... and can be compared with the proposed method experimentally.\n\nWe have updated the related works section to include discussion about more single-view methods. Moreover, we have included comparisons against some state-of-the-art single-view baselines, details of which are elaborated in the common rebuttal section. We hope this experiment illustrates the benefit of our approach which is competitive with the single-view methods given one view, but can benefit from additional (unposed) images unlike the single-view methods and outperform them in this scenario. \n\n> Quantitatively, the UpFusion 3D model has much better numbers than the 2D model, but visually it loses a lot of geometric details compared to the 2D results. \u2018\n\n\nWhile we agree that individual images from Upfusion2D are more detailed, these are often spurious details which might actually detract from the performance. In contrast, UpFusion3D recovered a consistent 3D mode whose renderings are \u2018likely\u2019 from all viewpoints, and thus maybe more likely to match the input.\n\n> but why is the LPIPS also better? -- LPIPS is not a pixel-aligned metric, while visually the 2D results look cleaner and have much more details than the 3D ones\n\nLPIPS is technically pixel-aligned on a feature-grid, and does still care about alignment (although perhaps less than other metrics). We conjecture that even LPIPS for Upfusion3D is better because predicting spurious details is penalized more than predicting less detail. We would also point out that a similar trend is also observed in prior work (e.g. in SparseFusion, their full approach performs better than the underlying 2D diffusion model even in LPIPS).\n\n> It seems that the generated views sometimes have inconsistent colors as the input view (e.g. the blue bench and the blue umbrella in Fig. 8, Appendix A)\n\nThis is an interesting observation! As of now we believe that these colors could perhaps be close in latent space and hence the neural mode distillation process gets stuck on a mode that represents a faithful rendering of an object albeit with colors that are slightly off. We also see similar artifacts (perhaps more pronounced) when distilling 3D representations from Zero-123 with the SJC.\n\n > I wonder how the proposed method compares to this baseline: first running COLMAP to estimate the relative poses of the input views, and then running a pose-dependent 3D synthesis method?\n\nIn table 1 of the paper, we had already included a baseline which uses RelPose++ to estimate poses (which is shown to be superior to COLMAP in the sparse-view setting) and then use an independent pose-dependent 3D synthesis method, SparseFusion. Both, this experiment and the added comparisons to FORGE highlight that accurately estimating this pose explicitly is a hard task, and that methods relying on this are not robust."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657453682,
                "cdate": 1700657453682,
                "tmdate": 1700657453682,
                "mdate": 1700657453682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NSt7mTGJBa",
            "forum": "4uaogMQgNL",
            "replyto": "4uaogMQgNL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4145/Reviewer_Egt9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4145/Reviewer_Egt9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents UpFusion, a system that can generate novel views from a sparse set of uncalibrated multi-view images. Technically, UpFusion consists of two parts: 1) the first part is a modified UpSRT which encodes unposed images into a set representation and renders the feature maps for the target view, 2) while the second part is a diffusion-based ControlNet, which generates the novel view conditioned on the set representation and the decoded feature map.\nDuring training stage, UpSRT and ControlNet are optimized separately."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well-written and well-structured. The problem setting is both innovative and ambitious, as it seeks to address two issues of considerable interest within the research community: 1) reconstruction from sparse views and 2) generation from unposed images, simultaneously.\n\n+ The proposed method is intuitive, and this paper presents specific details and dedicated designs that are well-suited for the challenges inherent to the problem under investigation.\n\n+ The empirical results demonstrate the method's remarkable performance in generating novel views from a few-shot unposed images when compared to the baseline approaches."
                },
                "weaknesses": {
                    "value": "- The problem formulation lacks clarity. Without specifying poses from images, it becomes ambiguous to define the pose of a target view unless canonical poses are provided. However, canonicalization necessitates per-category calibration, and it has been observed that such methods are specific to certain categories. To further validate the effectiveness of the approach, it is recommended to test the pre-trained UpFusion on additional data domains, such as Blender, LLFF, or Shiny datasets [1].\n\n- Empirical comparisons are insufficient in relevant baseline models. For the task of novel view synthesis, it is advisable to include comparisons with end-to-end pose optimization baselines, such as BARF [2] and NoPe-NeRF [3]. Since this paper asserts novel view generation from sparse views using diffusion, it would also be equitable to compare with state-of-the-art single-image-to-3D baselines such as Zero-123 [4].\n\n- From a technical perspective, despite notable engineering efforts, the proposed method appears to be a combination of existing methods: SRT, ControlNet, and DreamFusion. It also structurally resembles GeNVS [5].\n\n- The paper lacks a discussion and comparison with some relevant prior work, particularly with references [6] and [7].\n\n[1] Wizadwongsa et al.NeX: Real-time View Synthesis with Neural Basis Expansion\n\n[2] Lin et al., BARF: Bundle-Adjusting Neural Radiance Fields \n\n[3] Bian et al., NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior\n\n[4] Liu et al., Zero-1-to-3: Zero-shot One Image to 3D Object\n\n[5] Chan et al., GeNVS: Generative Novel View Synthesis with 3D-Aware Diffusion Models\n\n[6] Smith et al., FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow\n\n[7] Fu et al., MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Poses"
                },
                "questions": {
                    "value": "1. The evaluation scheme proposed in Sec. 4.1.2 is designed to mitigate pose ambiguity. However, the enforcement of per-view alignment may introduce more confusion in the evaluation results, making it challenging to assess whether the proposed method can accurately generate views at specified camera poses and maintain smooth views along a camera trajectory. To enhance the evaluation methodology, it is recommended that the authors consider implementing a global alignment across all views collectively, rather than performing alignment on a per-frame basis.\n\n2. Could the authors provide insights into the motivation behind making the specific modifications to UpSRT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4145/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4145/Reviewer_Egt9"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698914277711,
            "cdate": 1698914277711,
            "tmdate": 1699636379663,
            "mdate": 1699636379663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i1pMEU9KV0",
                "forum": "4uaogMQgNL",
                "replyto": "NSt7mTGJBa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4145/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their timely feedback. In this section, we elaborate on concerns that are not covered in the common rebuttal. \n\n> Since this paper asserts novel view generation from sparse views using diffusion, it would also be equitable to compare with state-of-the-art single-image-to-3D baselines such as Zero-123. \n\nWe have now included comparisons with state-of-the-art single-image-to-3D baselines. We have included details in the common rebuttal. We hope this experiment illustrates the benefit of our approach which is competitive with the single-view methods given one view, but can benefit from additional (unposed) images unlike the single-view methods and outperform them in this scenario. We hope these results on GSO also address the concern regarding evaluation on additional domains.\n\n> Without specifying poses from images, it becomes ambiguous to define the pose of a target view unless canonical poses are provided\n\nFollowing prior conventions e.g. UpSRT, we use the first input image as an anchor to define the orientation of the coordinate system. The pose of a target view is then defined w.r.t. this coordinate frame, thus making the task well-posed. While there is still a scale ambiguity in the reconstruction, we normalize our coordinate system during training to minimize this, and also allow the alignment during evaluation to not penalize methods for such ambiguity. \n\nWe mention this choice of the coordinate frame in Sec 3.2, and would be happy to expand on this further in the text.\n\n > For the task of novel view synthesis, it is advisable to include comparisons with end-to-end pose optimization baselines, such as BARF [2] and NoPe-NeRF [3] \u2026 The paper lacks a discussion and comparison with some relevant prior work, particularly with references [6] and [7]\n\nThank you for these references. We have edited the related work to discuss these further. While these methods can estimate pose, they are not applicable in our setting as they operate on densely-sampled images e.g. NoPe-NeRF presents results on images from dense *trajectories*, and BARF requires ~100 images for a 360-degree inference. As perhaps a more directly applicable baseline, we have now added a comparison to FORGE (please see common response above) which explicitly infers sparse-view poses followed by reconstruction, and we find that our system is more robust.\n\n> To enhance the evaluation methodology, it is recommended that the authors consider implementing a global alignment across all views collectively, rather than performing alignment on a per-frame basis.\n\nWe fully agree with the reviewer\u2019s comment. In fact, we initially attempted gradient based optimization techniques to calculate a global alignment. However, in practice we found that it was not possible to obtain gradients to perform this computation across all baselines (e.g. differentiably transforming SparseFusion\u2019s InstantNGP reconstructions was not feasible) and hence we settled for a per-frame alignment method which readily works across all baselines. \n\nHowever, in the additional results presented on GSO, we forego the aligned metrics and report the \u2018regular\u2019 metrics as the training data on Objaverse has origin-centered and normalized meshes, thus removing the ambiguity (unlike CO3D, where each object is in arbitrary coordinate frame).\n\n> Could the authors provide insights into the motivation behind making the specific modifications to UpSRT\n\nWe use a pre-trained DINOv2 backbone (instead of a CNN) because DINO features have been shown to be informative for correspondences (which we assumed might help our downstream application)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657406433,
                "cdate": 1700657406433,
                "tmdate": 1700657406433,
                "mdate": 1700657406433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]