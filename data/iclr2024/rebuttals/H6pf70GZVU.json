[
    {
        "title": "YoooP: You Only Optimize One Prototype per Class for Non-Exemplar Incremental Learning"
    },
    {
        "review": {
            "id": "fgZYgIGfa1",
            "forum": "H6pf70GZVU",
            "replyto": "H6pf70GZVU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4662/Reviewer_EwLK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4662/Reviewer_EwLK"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the non-exemplar class-incremental learning problem, an important problem in the machine learning field. The authors propose the extension of the prototypical network as corresponding loss terms to tackle this problem. The proposed method is evaluated on several benchmark datasets against other competitors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper tackles the non-exemplar class-incremental learning problem, an important problem in the machine learning field.\n2. The proposed method is evaluated on several benchmark datasets against other competitors.\n3. The ablation study and visualizations are clear and intuitive."
                },
                "weaknesses": {
                    "value": "1. Overall, I find the title may be unsuitable for the current manuscript. The proposed method utilizes prototypes to construct the training loss, while the embedding and classifiers are always optimizable throughout the learning process. Hence, not only \u201cprototypes\u201d are optimized, but also the backbone and classifiers are optimized for the current task.\n2. Some illustrations are unclear. For example, there is no X coordinate in Figure 1, making it hard to figure out the memory usage of different methods. Corresponding explanations are also needed to show why other methods like PASS and FeTrIL require a much larger memory size. These methods also save the class prototype to generate instances, and the memory gap between them should be illustrated.\n3. The experimental results should be reorganized. The current results in the main paper only focus on the Base-0 setting, while I also see the results in supplementary that some Base-50 results are also available. It would be better to reorganize these results to contain both settings since Base 50 is also common in today\u2019s CIL."
                },
                "questions": {
                    "value": "1. Please explain the significance of sampling in Section 3.2.1 and its advantage over PASS/FeTrIL. Besides, clarifying the memory budget of different methods in Figure 1 and Section D is also essential. Showing a table with the extra memory budget could be a good solution.\n2. It requires further experimental analysis, e.g., the influence of hyper-parameter gamma in Eq. 11.\n\nIn summary, this paper tackles an interesting problem with novel techniques. The proposed method shows competitive results in the benchmark comparison. Generally, I am positive about this submission, while addressing the concerns above is also essential for my decision."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4662/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697953990091,
            "cdate": 1697953990091,
            "tmdate": 1699636446772,
            "mdate": 1699636446772,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ctTJHIvuTJ",
                "forum": "H6pf70GZVU",
                "replyto": "fgZYgIGfa1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EwLK"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions! We have addressed all the comments and suggestions from the Reviewer and accordingly updated our manuscript highlighted in BLUE. We hope our responses below address your concerns. Please let us know if you have any additional concerns.\n\n\nQ1: The title may be unsuitable.\n-----------------\n**A1:** Thanks for your suggestion! We will change the title to\n\"OPT: Optimizing Prototypes for Non-Exemplar Class-Incremental Learning\" if the reviewer thinks it is appropriate.\n\nQ2: Some illustrations are unclear.\n-----------------\n**A2:** We have added the X coordinate for Figure 1 and also explained the \"memory footprint\" in the caption in our revised manuscript.\n\nAdditionally, we have explained the memory gap between our method and existing studies in the revised version. As described in the caption of Figure 1 and detailed in Appendix D, the term \"memory size\" denotes the memory footprint of our method and other approaches when computing prototypes for each task during training. Unlike other prototype-based methods that necessitate saving the embeddings of the entire class to generate the class mean prototype, YoooP generates optimized prototypes with a smaller memory footprint based on a mini-batch attentional mean-shift algorithm. That is why our approach requires a smaller memory footprint than the existing methods.\n\nQ3: The experimental results should be reorganized.\n-----------------\n**A3:** Per your suggestion, we have moved the experimental results under half-base 5 phases and half-base 10 phases settings to the main paper.\n\nQ4: Explain the significance and advantage of sampling in Section 3.2.1.\n-----------------\n**A4:** Per your suggestion, we have added some explanations about sampling in Section 3.2.1. The main advantage of our method is that we store the distribution of real angular using a histogram while existing works did not. The existing approaches like PASS involve adding high-dimensional noise into prototypes,  causing a significant divergence in the angular distributions between the class prototypes and synthetic data from the actual angular distribution. Moreover, Figure 5 shows the angular distribution comparison of synthetic data augmented from stored prototypes in PASS and the ground-truth angular distribution. We can observe that the angular distributions of PASS differ from the original distribution. On the other hand, our method samples the cosine similarity in the original angular distribution. Hence, our method can generate higher-quality synthetic data than the baselines like PASS and FeTrIL, contributing to a more effective mitigation of forgetting. For more details, please refer to Sections 3.2.1 and 4.2 in our revised version.\n\nQ5: Clarifying the memory budget.\n-----------------\n**A5:** Per your suggestion, we have presented the additional memory requirements of various methods for computing prototypes in Table 1. Here, the term 'extra memory budget' specifically denotes the number of saved embeddings utilized for creating class prototypes. In our methods, this corresponds to the number of embeddings in a mini-batch. In contrast, the other methods necessitate storing the entire set of embeddings for each task to calculate class mean prototypes.\n\nTable 1: Extra memory budget: # of saved embeddings\n| TinyImageNet | Our(BS=128) | Our(BS=256) | PASS | FeTrIL | IL2A | SSRE |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| base0-phases5 | 128 | 256 | 20000 | 20000 | 20000 | 20000 |\n| base0-phases10| 128 | 256 | 10000 | 10000 | 10000 | 10000 |\n| half-base-phases5 | 128 | 256 | 10000 | 10000 | 10000 | 10000 |\n| half-base-phases10 | 128 | 256 | 5000 | 5000 | 5000 | 5000 |\n| | | | | | | |\n\n\nQ6: The influence of hyper-parameter $\\gamma$ in Eq. 11.\n-----------------\n**A6:** We have explored the impact of hyper-parameter $\\gamma$ on CIFAR-100 with zero-base 10 phases setting, as illustrated in the following table. It can be observed that the hyper-parameter $\\gamma$ is not very sensitive, and increasing the weight of knowledge distillation within a certain range can be beneficial for YoooP/YoooP+. We can see that weight values between 25 to 40 result in excellent performance.\n\nTable 2: Influence of hyper-parameter $\\gamma$ on model performance\n| $\\gamma$ | 15 | 20 | 25 | 30 | 35 | 40 |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| YoooP+ (Avg) | 60.69\\% | 60.44\\% | 61.18\\% | 61.93\\% | 61.02\\% | 61.30\\% |\n| YoooP+ (last)| 41.21\\% | 42.17\\% | 44.48\\% | 47.17\\% | 45.79\\% | 46.51\\% |\n|   |      |      |        |       |       |     |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262963365,
                "cdate": 1700262963365,
                "tmdate": 1700263188162,
                "mdate": 1700263188162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zLYD0K6JAj",
                "forum": "H6pf70GZVU",
                "replyto": "8epAc8gc3q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Reviewer_EwLK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Reviewer_EwLK"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for providing a rebuttal. For now, the details about the \"Memory Size\" in Figure 1 are made clear, which refers to the \"footprint\" of prototype calculation. However, it seems the \"footprint\" is not a problem that requires such a highlight since only one prototype is maintained for each class for all methods, and the \"true\" memory cost is not so much (but the same for all methods). From this perspective, I suggest not highlighting such a characteristic since it is only about the way of prototype calculation (batch-wise versus class-wise).\n\nI am satisfied with the responses to my other concerns. I'm somewhat on the fence because other reviewers also raised many concerns about this paper (and part of them make sense to me). Currently, I tend to maintain my initial rating of a \"6\", but my decision will finally depend on the discussion among reviewers."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631050438,
                "cdate": 1700631050438,
                "tmdate": 1700631050438,
                "mdate": 1700631050438,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HriemR1L5q",
            "forum": "H6pf70GZVU",
            "replyto": "H6pf70GZVU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4662/Reviewer_eHC3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4662/Reviewer_eHC3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a prototype-based method that can concentrate most samples to their class prototypes and keep each class a compact cluster in the feature space, which can mitigate inter-class interference. When learning a new task, the saved prototypes are used for replay to deal with forgetting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "** The proposed method is reasonable. \n** The paper is easy to read."
                },
                "weaknesses": {
                    "value": "** The method is incremental as some prototype-based methods already exist. The paper made some improvements. But the improvements can also be obtained by contrastive learning as the proposed idea is very similar to that of contrastive learning. \n** The paper didn\u2019t say where the forgetting occurs. My understanding is that the technique simply learns and saves the prototype for each class. Each task is learned independently. Then there is no forgetting. \n** The inference procedure is not discussed. If the algorithm saves only the class prototype for each class, how does it get the feature representation for each test sample for cosine similarity computation in prediction? \n** The average accuracy in (Rebuffi et al. 2017) is average incremental accuracy (AIC). Based on AIC, your reported accuracy results are low. Please also report the last accuracy, i.e., after learning the last task and compare with the above methods. \n** The experimental datasets and baselines are too few. More baselines with or without using exemplars should be compared as saving some data is not an issue. [a] is a non-exemplar based method. \n(1) Kim et al. (2022). A theoretical study on solving continual learning. NeurIPS.\n(2) Wang et al. (2022). Beef: Bi-compatible class-incremental learning via energy-based expansion and fusion. ICLR. \n(3) Wu et al. (2019) Large scale incremental learning. CVPR.\n(4) Buzzega et al. (2020) Dark experience for general continual learning: a strong, simple baseline. NeurIPS.\n** Please give the efficiency results. The method seems to be quite slow."
                },
                "questions": {
                    "value": "see the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4662/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641123741,
            "cdate": 1698641123741,
            "tmdate": 1699636446705,
            "mdate": 1699636446705,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "78BURHWFRd",
                "forum": "H6pf70GZVU",
                "replyto": "HriemR1L5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eHC3 (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions! We have addressed all the comments and suggestions from the Reviewer and accordingly updated our manuscript highlighted in BLUE. We hope our responses below address your concerns. Please let us know if you have any additional concerns.\n\n\n\nQ1: Idea is similar to contrastive learning.\n-----------------\n**A1:** Our approaches bring substantial advancements over existing prototype-based methods. Unlike existing prototype-based approaches that only calculate the **mean prototype** for each class, we propose a novel attentional mean-shift algorithm for optimizing the prototype per class. The proposed method helps to learn a more compact and informative prototype. To our best knowledge, we are the first to study prototype optimization in CIL. Moreover, we develop a novel prototype augmentation technique based on a high-dimensional rotation matrix to generate high-quality synthetic data. In contrast, existing studies only add Gaussian noise into saved prototypes for data generation. The evaluation results demonstrated our methods can significantly outperform the baselines on multiple datasets under different settings. Thus, other reviewers, such as Reviewer #ZsAp, gcwm, and EwLK, recognize the novelty of our proposed methods.\n\nIn addition, we would like to highlight the distinction between our method and contrastive learning. Contrastive learning is known as self-supervised learning for pre-training. It aims to learn the representations of input data through the comparison of sample pairs, rather than learning a signal from individual data samples sequentially. This comparison involves both positive pairs (comprising \"similar\" inputs) and negative pairs (involving \"dissimilar\" inputs) [1]. In contrast, our methods introduce a novel attentional mean-shift algorithm for optimizing the prototype for each class. We then apply contrastive loss to further enhance the representativeness of each class's prototype. These two components ensure that the learned prototypes are more effectively representative. Note that the optimized prototypes can not be directly obtained through contrastive learning. Rather, the proposed attentional mean-shift algorithm plays a critical role here, though the contrastive loss is helpful.\n\n**References**\n\n[1] Contrastive representation learning: A framework and review. (Ieee Access 2020)\n\n\nQ2: Where the forgetting occurs?\n-----------------\n**A2:** As mentioned in our manuscript, catastrophic forgetting [1] refers to deep neural networks forgetting the acquired knowledge from the previous tasks disastrously while learning the current task. In other words, forgetting will occur when neural networks learn new tasks in sequence. In the context of Class-Incremental Learning (CIL), each task is learned independently and sequentially using **a single shared neural network**. Since all the tasks share the same network, learning a new task can overwrite the network weights learned for the previous tasks [2]. This leads to forgetting, i.e., accuracy drop on previous tasks. \n\n**References**\n\n[1] Catastrophic interference in connectionist networks: The sequential learning problem. (Psychology of learning and motivation 1989, p109-165)\n\n[2] Prototype Augmentation and Self-Supervision for Incremental Learning. (CVPR2021)\n\nQ3: The inference procedure is not discussed.\n-----------------\n**A3:** In Class-Incremental Learning (CIL), the inference for each task is performed similarly to a standard classification task. Specifically, as depicted in Figure 2, we use the backbone, denoted as $F(;\\theta)$, to extract embeddings from the test samples. These embeddings are then fed into the classifier $G(;w)$ to estimate the probability distribution over all classes. Conversely, the saved prototypes are merely utilized during the model training phase to avoid forgetting."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261659734,
                "cdate": 1700261659734,
                "tmdate": 1700262504045,
                "mdate": 1700262504045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TVQQBJqs4q",
                "forum": "H6pf70GZVU",
                "replyto": "HriemR1L5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eHC3 (Part 2/3)"
                    },
                    "comment": {
                        "value": "Q4: Report the last accuracy and compare with iCaRL.\n-----------------\n**A4:** Thanks for your suggestion! Our average accuracy means the average incremental accuracy (AIA) in our submission! Per your suggestion, we already updated the name of this metric in the revised manuscript.\n\n\nNote that our proposed models are **non-exemplar CIL methods** that do not store real samples for replay but only store prototypes, while the iCaRL is an **exemplar-based method** that needs to store and replay real samples. Thus, iCaRL requires a much larger memory budget than our methods. Moreover, prior studies [3][4][5] have pointed out that exemplar-based CIL may suffer from extra risk of privacy leakage and large memory costs. In reality, non-exemplar CIL is much more challenging than exemplar CIL as mentioned by the Reviewer gcwm. Therefore, a direct comparison between our method and iCaRL seems unfair.\n\nNevertheless, we already compared our methods with iCaRL [1] on the TinyImageNet dataset under different settings in Figure 10 in Appendix F. We report the last accuracy in the following table. It can be observed that our methods still outperform the iCaRL in terms of average incremental accuracy.\n\nTable 1: Evaluation of TinyImageNet.\n|  TinyImageNet  | zero-5 last | zero-10 last | Half-5 last | Half-10 last |\n|  ----  | ----  | ---- | ---- | ---- |\n| * YoooP | 43.93\\% | 37.67\\% | 54.67\\% | 45.22\\% |\n| * YoooP+ | **47.44**\\% | **44.06\\%** | **57.02\\%** | **49.97\\%** |\n| iCaRL-CNN | 34.65\\% | 21.64\\% | 23.39\\% | 16.04\\% |\n| iCaRL-NME | 44.33\\% | 31.96\\% | 33.10\\% | 25.27\\% |\n| BiC | 47.41\\% | 18.22\\% | 36.41\\% | 25.98\\% |\n| WA | 47.96\\% | 30.94\\% | 38.45\\% | 29.07\\% |\n|      |         |      |      |       |\n\n**References**\n\n[1] Incremental Classifier and Representation Learning. (CVPR2017)\n\n[2] Feature Translation for Exemplar-Free Class-Incremental Learning. (WACV2023)\n\n[4] Prototype Augmentation and Self-Supervision for Incremental Learning. (CVPR 2021)\n\n[5] Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. (CVPR 2022)\n\n[6] Feature Translation for Exemplar-Free Class-Incremental Learning. (WACV 2023)\n\n\n\nQ5: More datasets and baselines with or without using exemplars should be compared as saving some data is not an issue.\n-----------------\n**A5:** Per your suggestion, we added a new experiment evaluating our proposed methods on another large ImageNet-100 dataset. The comparison results are illustrated in the table below. Our methods (YoooP and YoooP+) still significantly outperform the baselines in terms of average incremental accuracy on the new ImageNet-100 dataset.\n\nTable 2: Evaluation of ImageNet-100.\n|  ImageNet-100  | Half-10 Avg | Half-10 last |\n|  ----  | ----  | ---- |\n| * YoooP | 75.97\\% | 68.96\\% |\n| * YoooP+ | 77.64\\% | 70.54\\% |\n| * PASS | 64.3\\% | 53.54\\% |\n| * FeTril | 72.55\\% | 64.06\\% |\n| * SSRE | 60.23\\% | 51.20\\% |\n| * LWF | 47.35\\% | 30.53\\% |\n| iCaRL-CNN | 48.46\\% | 36.29\\% |\n| iCaRL-NME | 59.04\\% | 48.69\\% |\n| BiC | 50.69\\% | 31.06\\% |\n| WA | 61.04\\% | 46.24\\% |\n|     |       |\n\nRegarding more baselines, the proposed models are **non-exemplar CIL methods** that do not store real samples for replay but only store prototypes, while the baselines you suggested are **exemplar-based methods** that need to store and replay real samples. Thus, comparing non-exemplar methods with the latest exemplar methods could be deemed unfair. Moreover, prior studies [1][2][3] have pointed out that exemplar-based CIL may suffer from extra risk of privacy leakage and large memory costs. Nonetheless, we still compared our method with a few exemplar-based methods like BiC [4], iCaRL [5], and WA [6] in Figure 11 in Appendix G. We can see that the proposed YoooP and YoooP+ can achieve comparable results under the zero-base settings, and even outperform some exemplar-based methods under the half-base settings.\n\nConcerning the previous study [7], they utilize task ID information during training, which deviates from the established definition of Class-incremental learning (CIL). As outlined in [8] and [9], CIL, true to its name, operates without predefined task boundaries. Consequently, CIL methods typically lack access to any task ID information during both training and inference phases. This implies that it's impractical to train a model to predict the task ID of input samples during testing or inference in CIL scenarios. Unlike prior work [7], we only access the class label rather than task ID information during model training, which aligns with the methodology of prior works [1],[2],[3],[4],[5],[6]. Therefore, we have opted to exclude that particular work [7] from our comparative analysis.\n\n**References (continue)**\n\n[1] Prototype Augmentation and Self-Supervision for Incremental Learning. (CVPR 2021)\n\n[2] Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. (CVPR 2022)\n\n[3] Feature Translation for Exemplar-Free Class-Incremental Learning. (WACV 2023)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262152719,
                "cdate": 1700262152719,
                "tmdate": 1700263428263,
                "mdate": 1700263428263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y11EDPV8wS",
            "forum": "H6pf70GZVU",
            "replyto": "H6pf70GZVU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4662/Reviewer_gcwm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4662/Reviewer_gcwm"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed new non-exemplar prototype-based method for class-incremental learning (CIL) setting. Prototype are optimized with attentional mean-shit method, while training the new tasks, they are replayed for old classes. To mitigate forgetting the autors as well use distillation at the feature level (l2). During the training, model interpolation and partial freezing of the classifier is used. This method is called YoooP. Additionally, in this work authors proposed extension to it, when some improvement to prototype replay based on data augmentation is proposed. The authors compared their method on Cifar-100 and TinyImageNet in compariosn to few other exemplar-free CIL methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. CIL exemplar-free setting is a challenging setting, where most of the recent work put more interest to fight only with forgetting - starting with already good backbone (base-50% setting) and only fighting with forgetting (FeTrIL, SSRE). This work seems to be competitive in harder \"base0\" setting.\n\nS2. Visualization part of the paper is good, well support written text. But still, not enough to provide all the info - seem W1.\n\nS3. Interesting founding that PASS w/o Augh works better for some settings."
                },
                "weaknesses": {
                    "value": "W1. The main weakness of this work is that after the reading the main paper the reader won't be able to reproduce and know exactly how the method works. The crucial part is in the appendix, i.e. Algorithm 1, where you see that we have S steps inside each epoch to calculate L_t, R iterations to calculate prototypes p_k, main loss - Arcface is mentioned in the implementation details and it hyper-parameter sigma etc. \n\nW2. Some equations give even a wrong intuition how the loss is computed, e.g Eq.4. and the first component is over the whole dataset, while in practice is withing S minibatches. \n\nW3. Some crucial hyper-parameters are not provided in main paper, or not provided at all - number of S steps, R iterations.\n\nW4. Multiple things are combinend in the Yooop method  - three losses components, where feature distillation is quite strong regularization preventing forgetting but at the same time lowering plasticity. Additionally, mixing this with model interpolation with beta = 0.6 and freezing the classifier. The ablation is not clear about every single participant. Seems like the model interpolation is crucial here, and bug gain is for YoooP+. \n\nW5. Why in the first figure we have missing units for the memory size? It's the first figure that supposed to give some motivation, but currently it raises more questions. Additionally, SSRE to my knowledge has the growing part and then compression. How it's possible that it's in line with FeTrIL, PASS, IL2A? \n\nW6. Why the method is not evaluated with ImageNet-100? This dataset would shade more light how the method performs on the bigger images and how to compare with the other baselines.\n\nW7. Logic in the reasoning. Page 7, before saying: \"Therefore, we can draw a conclusion that the proposed methods can outperform the non-exemplar baselines.\" :\n\n    While SSRE and FeTrIL have lower average forgetting than our methods, their prediction accuracy drops rapidly in the initial tasks, resulting in low accuracy in the final task,  as shown in Figure 4.  In reality,  the lower forgetting in SSRE and FeTrIL is attributed to the sharp drop in accuracy in the early tasks and the limited learning progress in the subsequent tasks\n\nThat is true about SSRE and FeTrIL, I agree. They mostly fight with forgetting. That's why they are good in base-half setting (see Tiny and FeTrIL in your appendix, Yooop is lower there). But how you can conclude this after this two sentences. Overall, in base0 setting your methods presents better accuracy. But to conclude this, the behavior of the SSRE FeTrIL dosen't matter here.\n\nW8. Sec 4.3 \"This is because YoooP+ can form a compact cluster for the samples in each class via prototype optimization and create high-quality synthetic data from the original distribution of cosine similarity to constrain **the boundary of old tasks.**\"   - maybe the experiment to show that? \n\nOverall, it is quite interesting work, but Yooop combines so many things, and it's not clear how each of it contributes. After reading the main paper you cannot re-produce it, you can have the idea how it works, appendix is necessary, but still not enough. The paper should be rewritten paying attention in all the hyper-params, mentioning arcface loss with sigma, and good ablation. Maybe some insight, comparison with PASS can be moved to the appendix (to have space for more crucial information/ablations)."
                },
                "questions": {
                    "value": "Q1. Out of the curiosity, why in the intro when you introduce class-incremental learning setting you ref. to three quite recent works of Zhu and Zhou (2021-2022)? This scenario of CL training is with us longer than that.\n\nQ2. Why in the ablation of beta (Fig.8 (a)) we see different starting points for the first task? Beta is the interpolation that will take place after the first task.\n\nQ3. Why starting points in Fig.4 for Yooop and Yooop+ are not the same? (easy to see for Tiny)\n\nQ4. Why not providing source code in the supplementary? Without it I think the results will be hard to reproduce. I guess you use an existing framework for your work, looking at the results it's PyCIL? Would be good to give credits to authors and point your starting impoementations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4662/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684245009,
            "cdate": 1698684245009,
            "tmdate": 1699636446600,
            "mdate": 1699636446600,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Glg7dRcufP",
                "forum": "H6pf70GZVU",
                "replyto": "Y11EDPV8wS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gcwm (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions! We have addressed all the comments and suggestions from the Reviewer and accordingly updated our manuscript highlighted in BLUE. We hope our responses below address your concerns. Please let us know if you have any additional concerns.\n\n\nQ1: Hard to reproduce with hyperparameters.\n-----------------\n**A1:** To reproduce our experimental results, we have uploaded the source code in the Supplementary Material. Additionally, we will make the source code publically available on GitHub upon publication. By the way, our source code is not based on an existing framework like PyCIL.\n\nRegarding parameters $S$ steps and $R$ iterations in Algorithm 1, they are determined by the ratio of Dataset Size to Mini-batch size. More specifically, $S$ is the number of total steps per epoch for each task. For the number of steps $R$ used to update the prototypes in each task, we simply set $R=S$ in our experiments. We have added the explanations of these two parameters in Appendix A in our revised version. \n\nFor the Arcface loss hyperparameter $\\delta$, we already conducted an ablation study in Appendix C.5 and consistently used $\\delta=0.25$ across all experiments with various datasets. Per your suggestion, we have introduced the Arcface loss in the main paper.\n\nQ2: Eq.4 has the misunderstanding.\n-----------------\n**A2:** Per your suggestion, we added some explanations of Eq.4 in the updated version as follows.\n\nwhere $\\ell(\\cdot,\\cdot)$ is a classification loss, and $i\\in [n_t]:y_i=k$ is replaced by $i\\in B$ when training by mini-batch.\n\nQ3: Some crucial hyper-parameters are not provided in main paper, or not provided at all - number of S steps, R iterations.\n-------------------\n**A3:** Per your suggestion, we have introduced the important hyper-parameters in the main paper. Regarding parameters $S$ steps and $R$ iterations, they are actually determined by the ratio of Dataset Size to Mini-batch size. Please refer to **A1** above.\n\n\nQ4: The ablation is not clear enough.\n-----------------\n**A4:** The classification loss $L_{t,C}$ and the knowledge distillation loss $L_{t,KD}$ in YoooP are commonly used in recent Non-exemplar CIL methods[1],[2],[3]. These two components are also adopted by the baselines. In addition, the partial freezing of the classifier is also used in existing work LwF[1]. So any improvement over the baselines is a result of the proposed $L_{t,P}$ loss.\n\nAccordingly, we do an ablation study on the newly proposed components, including model interpolation (MI), prototype optimization \\(P\\), and prototype augmentation (PA) in YoooP+. As illustrated in Figure 7 in Appendix C.1, we can see that YoooP(-P), which means without prototype optimization ($L_{t,P}$), will result in much lower accuracy, 51.50\\%, than that of YoooP(-MI), 54.82\\%. It thus suggests that prototype optimization is more important than MI. However, MI is also important since the accuracy of YoooP(-MI) will be lower compared to YoooP, 57.66\\%. \n\nAdditionally, we conducted an additional ablation study about MI on YoooP+. The evaluation results on CIFAR-100 with zeros-base 10 phases setting are reported in Table 1. Removing MI from YoooP+ reduces the accuracy by 3.91\\% while removing PA from YoooP+ (i.e. YoooP) reduces the accuracy more, 4.27\\%. Thus, prototype augmentation (PA) in YoooP+ is more important than MI.\n\nTable 1: Additional Ablation Study\n| CIFAR-100 | zero-10 Avg | zero-10 Last |\n| -------- | -------- | -------- |\n| YoooP | 57.66\\% | 42.49\\% |\n| YoooP (-MI) | 55.42\\% | 37.2\\% |\n| YoooP+ (-MI) | 58.02\\% | 41.2\\% |\n| YoooP+ (YoooP +PA) | 61.93\\% | 47.17\\% |\n| | | |\n\n**References**\n\n[1] Learning without Forgetting. (ECCV2016)\n\n[2] Prototype Augmentation and Self-Supervision for Incremental Learning. (CVPR2021)\n\n[3] Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. (CVPR2022)\n\nQ5: The first figure misses units for the memory size as well as SSRE to my knowledge has the growing part and then compression.\n-----------------\n**A5:** Thanks for your suggestion! We have updated the units for the memory size in Figure 1.\n\nAs described in the caption of Figure 1 and detailed in Appendix D, the term 'memory size' denotes the memory footprint of our method and other approaches when computing prototypes for each task during training.\n\nFor SSRE, our reported memory size does not refer to the GPU memory during model training. SSRE, IL2A, and PASS compute the mean value of all embeddings per class. Thus, they have the same memory footprint (same line in Figure 1) and incur an expensive memory footprint for storing their embeddings. In contrast, our proposed mini-batch attentional mean-shift method computes prototypes in a mini-batch manner. This significantly reduces the memory footprint required for prototype generation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263828358,
                "cdate": 1700263828358,
                "tmdate": 1700264108686,
                "mdate": 1700264108686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VKyCl7rFTW",
                "forum": "H6pf70GZVU",
                "replyto": "Y11EDPV8wS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gcwm (Part 2/2)"
                    },
                    "comment": {
                        "value": "Q6: Evaluation on large ImageNet-100 dataset.\n-----------------\n**A6:** Per your suggestion, we have evaluated our methods on ImageNet-100. The results are reported in the following table, where \"*\" denotes non-exemplar methods, \"-last\" means the average accuracy of the last task. IL2A is not included in the comparison because we do not have enough GPU memory to run their model on ImageNet-100. Our methods (YoooP and YoooP+) still significantly outperform the baselines in terms of average accuracy on the new dataset.\n\nTable 2: Evaluation on ImageNet-100.\n|  ImageNet-100  | Half-10 Avg | Half-10 last |\n|  ----  | ----  | ---- |\n| * YoooP | 75.97\\% | 68.96\\% |\n| * YoooP+ | 77.64\\% | 70.54\\% |\n| * PASS | 64.3\\% | 53.54\\% |\n| * FeTril | 72.55\\% | 64.06\\% |\n| * SSRE | 60.23\\% | 51.20\\% |\n| * LWF | 47.35\\% | 30.53\\% |\n| iCaRL-CNN | 48.46\\% | 36.29\\% |\n| iCaRL-NME | 59.04\\% | 48.69\\% |\n| BiC | 50.69\\% | 31.06\\% |\n| WA | 61.04\\% | 46.24\\% |\n| | | |\n\nQ7: Logic in the reasoning.\n-----------------\n**A7:** We already updated the logic meaning on Page 7 as follows.\n\nAccording to Table 1 and Figure 4, both SSRE and FeTrIL have lower forgetting while the prediction accuracy drops rapidly in the initial tasks. A lower forgetting in this case (with lower accuracy) indicates that the model is not improving or learning so such performance is not preferred. In contrast, the proposed YoooP and YoooP+ reach higher average incremental accuracy while achieving slightly higher or comparable forgetting compared to other non-exemplar baselines. This indicates a better stability-plasticity trade-off, implying that the model learns quickly on new tasks while incurring a minimal cost in forgetting. Therefore, it can be concluded that the proposed methods outperform the non-exemplar baselines.\n\nQ8: Synthetic data constrain the boundary of old tasks.\n-----------------\n**A8:** Our experiments in **Figure 6** verify that our method can achieve better classification boundaries for old tasks. As shown in Figure 6, the light gray points represent the distribution of data from old tasks. A comparison between Figure 6(a) and Figure 6(b) reveals that the data from previous tasks are still clustered into several groups using our method, exhibiting a clear boundary with the clusters of current task data. This pattern is also evident in Figure 6\\(c\\). The high-quality synthetic data effectively constrains the boundary of old tasks when training the current tasks, enabling the current tasks to establish a clear boundary with previous tasks. In contrast, the baselines like PASS (illustrated in Figure 6(d), (e), (f)), without our proposed prototype augmentation in YoooP+, will make the boundary between the current task data and previous task data gradually become less clear and their classes may even overlap with each other.\n\n\nQ9: References in the Introducion.\n-----------------\n**A9:** We have cited some earlier works on class-incremental learning in the Introduction in our revised version.\n\nQ10: Different starting points in Fig.8(a).\n-----------------\n**A10:** According to Equation 12, we update the parameters $\\theta$ based on $\\beta$. The different $\\beta$ in our ablation study result in different starting points, $\\theta_1$, for each case.\n\nQ11: Different start points for YoooP and YoooP+ in Fig.4.\n-----------------\n**A11:** It is caused by the random error (random seeds) in the experiments. Below, we present the exact accuracy of the first task in Figure 4. We can see that their accuracy is very close to each other.\n\nTable 3: First Task Accuracy\n| CIFAR-100 | zero-5 | zero-10 |\n| -------- | -------- | -------- |\n| YoooP | 87.67\\% | 92.30\\% |\n| YoooP+| 88.43\\% | 91.68\\% |\n| **TinyImageNet** | **zero-5** | **zero-10** |\n| YoooP | 79.48\\% | 84.77\\% |\n| YoooP+| 80.53\\% | 86.05\\% |\n|      |      |      |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264080103,
                "cdate": 1700264080103,
                "tmdate": 1700264080103,
                "mdate": 1700264080103,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3gxyRh8CtQ",
            "forum": "H6pf70GZVU",
            "replyto": "H6pf70GZVU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4662/Reviewer_ZsAp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4662/Reviewer_ZsAp"
            ],
            "content": {
                "summary": {
                    "value": "(Motivation)\nThe prototype-based (class-mean) method, which is one of the exemplar-free Class Incremental Learning (CIL) methods, stores class-representative prototypes (=class mean prototype). However, the class mean prototype does not represent the centroids of each class. Utilizing inappropriate prototypes leads to confusion between old classes learned in different stages. Moreover, prototype augmentation, which adds Gaussian noise to the prototype, leads to more serious catastrophic forgetting of previously observed classes.\n\n(Method)\n1. They propose the prototype-based CIL method, called YoooP, that optimizes the prototype, considering the weighted average distance of all samples in the class. The paper claims this class-wise prototype is more representative than utilizing the simple class mean one.\n\n2. They propose a prototype augmentation, called YoooP+, that synthesizes the prototypes from the angular distribution between each class\u2019s real data and prototype stored in memory. Synthetic prototypes preserve the distribution of the real data rather than simply adding the Gaussian noise to the prototype (augmentation technique of PASS)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes a novel approach to leave the class representative prototype considering the similarity with all real samples. The proposed method aims to be an efficient technique that can be easily applied to the existing prototype-based method.\n\nThis paper also proposes a data augmentation performed with the help of a rotation matrix. This strategy approximates real distribution better than conventional prototype augmentation techniques.\n\nIn base-0 experiments on CIFAR-100 and Tiny-ImageNet, the proposed methods achieve the best performance."
                },
                "weaknesses": {
                    "value": "-\tThe scale of dataset (CIFAR-100, TinyImageNet) is small. Whether the proposed method works with bigger datasets is an important issue that needs to be addressed. \n-\tThe paper lacks the result with 20 phases which is a quite common configuration. \n-\tThe effect of an optimized prototype based on the attentional mean-shift method seems limited in many situations."
                },
                "questions": {
                    "value": "1) For verifying the strength of proposed augmentation, why don\u2019t you apply the new strategy to PASS instead of adding Gaussian noise to prototype? Would you show the performance of it?\n\n2) Could you show the classification confusion matrix result of your approach?\n\n3) In section 3, when define the probabilities over all classes C_(1:t ), isn\u2019t softmax(wG(F(x;\u03b8))) more accurate notation than softmax(wF(x;\u03b8))?\n\n4) The function c(,) in equation 2, equation 4 seems to be different. Is the c(,) in equation 2 cosine similarity function and one in equation 4 classifier (=G(F(p_k;\u03b8)))?\n\n5) How can we define the initial class-wise prototype p_k? Is it the same with the prototype used in PASS? (k-th class mean prototype)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4662/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4662/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4662/Reviewer_ZsAp"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4662/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753506145,
            "cdate": 1698753506145,
            "tmdate": 1699636446519,
            "mdate": 1699636446519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "llMhwHHJo5",
                "forum": "H6pf70GZVU",
                "replyto": "3gxyRh8CtQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZsAp (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions! We have addressed all the comments and suggestions from the Reviewer and accordingly updated our manuscript highlighted in BLUE. We hope our responses below address your concerns. Please let us know if you have any additional concerns.\n\n\nQ1: Whether the method works with bigger datasets\n------------------\n**A1:** Thanks for your suggestion! We have evaluated the proposed two approaches on ImageNet-100. The results are reported in the following table, where \"*\" indicates non-exemplar methods, \"-last\" means the average accuracy of the last task. IL2A is not included in the comparison because we do not have enough GPU memory to train their model on ImageNet-100. Our methods (YoooP and YoooP+) still significantly outperform the baselines in terms of average accuracy on the large ImageNet-100 dataset.\n\nTable 1: Evaluation on ImageNet-100.\n|  ImageNet-100  | Half-10 Avg | Half-10 last |\n|  ----  | ----  | ---- |\n| * YoooP | 75.97\\% | 68.96\\% |\n| * YoooP+ | 77.64\\% | 70.54\\% |\n| * PASS | 64.3\\% | 53.54\\% |\n| * FeTril | 72.55\\% | 64.06\\% |\n| * SSRE | 60.23\\% | 51.20\\% |\n| * LWF | 47.35\\% | 30.53\\% |\n| iCaRL-CNN | 48.46\\% | 36.29\\% |\n| iCaRL-NME | 59.04\\% | 48.69\\% |\n| BiC | 50.69\\% | 31.06\\% |\n| WA | 61.04\\% | 46.24\\% |\n|  |  |   |\n\n\nQ2: About the result with 20 phases\n------------------\n**A2:** We have evaluated the proposed methods on TinyImageNet under zero-base 20 phases and half-base 20 phases settings. The results are reported in the following table, where \"*\" indicates non-exemplar methods, \"-last\" means the average accuracy of the last task. We can observe that our methods (YoooP and YoooP+) still significantly outperform the baselines in terms of average accuracy on the 20 phases settings.\n\nTable 2: Evaluation on TinyImageNet.\n|  TinyImageNet  | Zero-20 Avg | Zero-20 last | Half-20 Avg | Half-20 last |\n|  ----  | ----  | ---- | ----  | ---- |\n| * YoooP | 46.00\\% | 30.97\\% | 48.19\\% | 33.31\\% |\n| * YoooP+ | **48.36%** | **31.26\\%** | **51.27\\%** | **35.28\\%** |\n| * PASS | 33.76\\% | 19.36\\% | 33.24\\% | 25.21\\% |\n| * IL2A | 25.39\\% | 17.73\\% | 39.9\\% | 29.43\\% |\n| * FeTril | 31.84\\% | 18.38\\% | 48.96\\% | 41.44\\% |\n| * SSRE | 25.78\\% | 15.25\\% | 46.2\\% | 38.84\\% |\n| * LWF | 22.93\\% | 9.57\\% | 13.39\\% | 5.62\\%\n| iCaRL-CNN | 30.33\\% | 11.31\\% | 28.9\\% | 20.54\\% |\n| iCaRL-NME | 36.07\\% | 18.35\\% | 33.68\\% | 27.3\\% |\n| BiC | 34.29\\% | 13.14\\% | 40.81\\% | 19.91\\% |\n| WA | 35.87\\% | 13.17\\% | 27.93\\% | 11.91\\% |\n|    |  |    |  |  |\n\nQ3: Optimized prototype seems limited in situations.\n-----------------\n**A3:** Our prototype optimization method is versatile and can be applied not only to prototype-based methods but also to various scenarios in class incremental learning, as demonstrated in [1][2][3][4].\n\n**References**\n\n[1] Incremental Classifier and Representation Learning. (CVPR 2017)\n\n[2] Large Scale Incremental Learning. (CVPR 2019)\n\n[3] BEEF: Bi-Compatible Class-Incremental Learning via Energy-Based Expansion and Fusion. (ICLR 2023)\n\n[4] Feature Boosting and Compression for Class-incremental Learning. (ECCV 2022)\n\n\nQ4: Apply the new augmentation strategy to PASS.\n-----------------\n**A4:** We have applied our new prototype augmentation (PA) strategy to the PASS, and then evaluated its performance on CIFAR-100 under the zero-base 10 phases setting. The results are reported in the following table. We can observe that our prototype augmentation method can significantly improve the performance of PASS, even without prototype optimization.\n\nTable 3: Comparison of adding Gaussian noise and our strategy on PASS.\n| CIFAR-100 | zero-10 Avg | zero-10 Last |\n| -------- | -------- | -------- |\n| PASS (Adding noise)  | 51.94\\% | 35.81\\% |\n| PASS with PA | 55.98\\% | 41.03\\% |\n\n\nQ5: Show the classification confusion matrix.\n-----------------\n**A5:** Thanks for your suggestion! We randomly choose 10 classes to show a part of the classification confusion matrix of YoooP+, and the entire confusion matrix of 100 classes is presented in Appendix E of the revised version.\n\nTable 4: Confusion Matrix (randomly sampled 10x10 submatrix) for CIFAR-100 with zero-base 10 phases setting\n| Confusion Matrix | Road | Lobster | Cattle | Couch | Shrew | Aquarium fish | Castle | Palm tree | Snail | Bowl |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| **Road** | 79 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| **Lobster** | 0 | 76 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| **Cattle** | 1 | 0 | 58 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| **Couch** | 0 | 0 | 0 | 48 | 0 | 0 | 0 | 0 | 0 | 0 |\n| **Shrew** | 0 | 0 | 6 | 0 | 54 | 0 | 0 | 0 | 0 | 0 |\n| **Aquarium fish** | 0 | 0 | 0 | 0 | 0 | 54 | 0 | 0 | 0 | 0 |\n| **Castle** | 0 | 0 | 0 | 0 | 0 | 0 | 24 | 0 | 0 | 0 |\n| **Palm tree** | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 61 | 0 | 0 |\n| **Snail** | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 41 | 0 |\n| **Bowl** | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 44 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262922359,
                "cdate": 1700262922359,
                "tmdate": 1700263604258,
                "mdate": 1700263604258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r2FXh1x7vH",
                "forum": "H6pf70GZVU",
                "replyto": "3gxyRh8CtQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZsAp (Part 2/2)"
                    },
                    "comment": {
                        "value": "Q6: More accurate notation.\n-----------------\n**A6:** Thanks for your suggestion! We have already updated the notation with $softmax(G(F(x; \\theta); w))$ in Section 3 highlighted in blue.\n\n\nQ7: The function c(,) in Eq.2, Eq.4.\n-----------------\n**A7:** Thanks for your correction! After fixing Eq.4 in our revised version, $c(\\cdot,\\cdot)$ in both Eq. 2 and Eq.4 denotes the cosine similarity function. In Eq. 4, it is used to compute the logits for all the classes $j\\in C_{1:t}$ as the cosine similarities between each vector in the weight matrix of the final-layer classifier and (1) a sample's representation $z_i$ (the first term in Eq. 4); or (2) each class's prototype $p_k$ (the second term in Eq. 4).\n\n$L_{t,C}(\\theta,w)\\triangleq\\frac{1}{|C_t|}\\frac{1}{n_t}\\sum_{k\\in C_t}\\sum_{i\\in [n_t]:y_i=k}\\ell([c(z_i, w_j)]\\_{j\\in C_{1:t}, k)} + \\frac{1}{|C_{1:{t-1}}|}\\sum_{k\\in C_{1:{t-1}}}\\ell([c(p_k, w_j)]\\_{j\\in C_{1:t}}, k)$\n\n\nQ8: The initial class-wise prototype $p_k$.\n-----------------\n**A8:** We randomly choose one sample from each class-$k$ and use its embedding as the initial prototype $p_k$. Our prototype update method is an extension of the classical mean-shift algorithm [1] that replaces the pre-defined kernel similarity with a learnable attention score. As stated in [1], random initialization of $p_k$ can guarantee the independence of the solution to the initialization problem.\n\n**References**\n\n[1] Mean shift, mode seeking, and clustering, TPAMI 1995."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263587590,
                "cdate": 1700263587590,
                "tmdate": 1700263587590,
                "mdate": 1700263587590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JlRE9ZQ7Lj",
                "forum": "H6pf70GZVU",
                "replyto": "bsccWv5uUn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4662/Reviewer_ZsAp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4662/Reviewer_ZsAp"
                ],
                "content": {
                    "comment": {
                        "value": "I read the rebuttal carefully and thank the authors for carrying out the experiments we asked for. \nI still have concerns with the effectiveness of the proposed method on a small dataset and cannot entirely agree the proposed optimized prototype represents each class's value. \nSo, I will keep my decision (marginally above the acceptance)."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4662/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673362645,
                "cdate": 1700673362645,
                "tmdate": 1700673362645,
                "mdate": 1700673362645,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]