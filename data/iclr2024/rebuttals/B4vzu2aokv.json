[
    {
        "title": "P2Seg: Pointly-supervised Segmentation via Mutual Distillation"
    },
    {
        "review": {
            "id": "zi5FviC7T6",
            "forum": "B4vzu2aokv",
            "replyto": "B4vzu2aokv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission688/Reviewer_QcUt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission688/Reviewer_QcUt"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach to Point Level Supervised Instance Segmentation based on mutual learning and knowledge distillation. The proposed algorithm, P2seg, introduces mutual distillation to recover instance segmentation based on points supervision. Comparison with the state-of-the-art methods is done using Pascal Voc-2012 and COCO-2017."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality. Point-supervised instance segmentation is not new in the weakly supervised literature, most approaches being more oriented to panoptic segmentation, ensuring a parsing of scenes better than instance segmentation. The novelty here is to use mutual learning between semantic segmentation and instance segmentation. The mutual information module is formed by a network S2I that transfers the semantic information to instances and a module I2S that uses the instance affinity matrices, class maps and offsets map predicted by an off-the-shelf segmentation network, namely HRNet (see e.g. \\cite{ YuanCW19} and (https://github.com/HRNet/HRNet-Semantic-Segmentation) to refine results cyclically. \n\nQuality and clarity. The effort of explaining the architecture with a number of images and sketches is appreciable. \n\nSignificance. The proposed mutual information-based method could, in principle, be lifted to large-scale datasets."
                },
                "weaknesses": {
                    "value": "Novelty:\\\nThe paper is quite similar to BESTIE [Kim et Al., CVPR 2022], with the mutual distillation mimicking the semantic knowledge transfer defined in BESTIE. Many passages are pretty similar, though BESTIE provides algorithms and thorough explanations of the complex architecture, which are not given here.\n\nOrganization:\\\n-No information is given on the instance affinity matrix and class map, as computed by HRNet (as stated in the last line of pg. 3, Overview of Method). HRNet is a semantic segmentation net \\cite{YuanCW19}, which is never cited, and its practical role is not defined. \n\n-No ablation is provided to understand the effect of each component. For example, it needs to be clarified if the semantic segmentation map is obtained by full supervision or by weak supervision. The mIoU of the obtained semantic segmentation is not provided.\\\n-The number of points supplied at input, how they are collected, and the relation between them and the targets should be discussed.\\\nThe number of iterations used seems to be quite high.\n\nIn particular: \\\n A table showing scores, according to the number of points taken as input, was expected. Table 5, named \u201chard pixel ratio\u201d, does not seem to be informative. \\\nA table showing the number of parameters for HRNet, MDM, and Mask-RCNN, used to retrain the MDM, was also expected, given the number of modules used.  \\\nA table showing the impact of semantic segmentation was expected.\\\n\nAmbiguities:\n- The full loss L = \\lambda_{I2S} L_{I2S} + \\lambda_{S2I} L_{S2I} is such that L_{S2I} = L_{offset} + L_{segmentation}, though no weight for balancing L_{S2I} is added. \nHowever, on page 6, paragraph Training details, it is written that the weight of the segmentation loss is 1.0 and the weight of the offset loss is 0.01, which do not sum to 1 for L_{S2I}.\n- As stated in Training details, Mask R-CNN is used for retraining MDM. Table 4 result is, in fact, the same as the result given in Table 1.\\\nHowever, in Table 1, BESTIE (points) is given on PascalVOC-2012 a mAP_{50} score of 52.8, while on BESTIE paper is reported 56.0, higher than the 55.6 mAP_{50} of the paper under revision. Likewise, there is no value for BESTIE mAP_{70}, which instead is 36.5. On COCO-2017 BESTIE obtains 34.0, while the results displayed here, on Table 2, are 33.6, therefore less than BESTIE. \n- In the two paragraphs \u201cResults on Pascal VOC 2012\u201d and \u201cResults on MS COCO 2017\u201d, it is written that the BESTIE algorithm is retrained to justify the lower scores reported, but it is not discussed why; while being the method almost similar an explanation is required. Therefore, apparently, the presented work does not improve on the SOTA. \\\nFurthermore, the works of \\cite{Kim_2023_CVPR} and \\cite{liao2023attentionshift} are not even reported, providing better results than those displayed in the paper under revision. \n\n- Possibly incorrectly reported values:\nThe difference between panoptic segmentation and instance segmentation is that instance segmentation does not consider \u2018stuff\u2019, and the metric mAP on \u2018things\u2019 allows confidence on overlapping objects. The difference implies (see \\cite{Kirillov_2019_CVPR}) that the metric PQ^{th}, that is panoptic quality on \u2018things\u2019, is like AP when from AP the non-overlapping predictions are subtracted, which means that AP benefits from predicting multiple overlapping hypotheses.\\\nThis paper, under revision, reports on Table 1 and 2 comparisons with Point2Mask \\cite{ li2023point2mask}) quite inexactly. \\\nIn fact, Point2Mask obtains on VOC 2012 results with PQ^{th}, namely panoptic quality on \u2018things\u2019, equal to 59.4 (with swin-transformer), and 53.0 with Resnet-101 while on Table 1 it is reported as mAP_{50} = 48.4 and mAP_{75} = 22.8. Similarly, on COCO-2017, no justification for these reported results is given.\n\n\nOther comments:\n1. Hadarmad should be Hadamard, which is repeated twice on pages 5 and 8.\n2. Pg. 5: The concept of \u201cinstance ownership relation\u201d is not introduced.\n3. Pg. 5, eq. (5), it should be noted that A is assumed to be generated by the network HRNet, which is not referred to and it is a segmentation network. \n4. Pg 6: The description of COCO-2017 is wrong. It is written that COCO (i.e. COCO 2017) includes 110k images, but COCO 2017 includes 118k images. The test set is not reported. This is made of 40670 images, does this imply that no tests were made? Actually, there are no results on the test set.\n5. Pg. 6 declares, \u201cWe assess the performance of object detection using two measures. We measure the performance using the standard protocol mean Average Precision(mAP). \u201cObject detection\u201d? The performance should be about instance segmentation unless it starts with HRNet object detection. \n \n\nReferences\\\n@article{YuanCW19,\\\n  title={Object-Contextual Representations for Semantic Segmentation},\\\n  author={Yuhui Yuan and Xilin Chen and Jingdong Wang},\\\n  booktitle={ECCV},\\\n  year={2020}\\\n}\\\n@inproceedings{li2023point2mask,\\\n  title={Point2mask: Point-supervised panoptic segmentation via optimal transport},\\\n  author={Li, Wentong and Yuan, Yuqian and Wang, Song and Zhu, Jianke and Li, Jianshu and Liu, Jian and Zhang, Lei},\\\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\\\n  pages={572--581},\\\n  year={2023}\\\n \n@InProceedings{Kirillov_2019_CVPR,\\\n author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr},\\\n title = {Panoptic Segmentation},\\\n booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\\n month = {June},\\\n year = {2019}\\\n }\\\n@inproceedings{fan2022pointly,\\\n  title={Pointly-supervised panoptic segmentation},\\\n  author={Fan, Junsong and Zhang, Zhaoxiang and Tan, Tieniu},\\\n  booktitle={European Conference on Computer Vision},\\\n  pages={319--336},\\\n  year={2022},\\\n  organization={Springer}\\\n}\\\n@inproceedings{liao2023attentionshift,\\\n  title={AttentionShift: Iteratively Estimated Part-Based Attention Map for Pointly Supervised Instance Segmentation},\\\n  author={Liao, Mingxiang and Guo, Zonghao and Wang, Yuze and Yuan, Peng and Feng, Bailan and Wan, Fang},\\\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\\n  pages={19519--19528},\\\nyear={2023}\\\n}\\\n \n@InProceedings{Kim_2023_CVPR,\\\n author = {Kim, Beomyoung and Jeong, Joonhyun and Han, Dongyoon and Hwang, Sung Ju}, \\\ntitle = {The Devil Is in the Points: Weakly Semi-Supervised Instance Segmentation via Point-Guided Mask Representation}, \\\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, \\\nmonth = {June}, year = {2023}, pages = {11360-11370} }\\"
                },
                "questions": {
                    "value": "Please explain:\n\n    how the semantic segmentation is obtained.\n    Why BESTIE is retrained, for both PascalVOC-2012 and COCO-2017.\n    Why so many iterations are used, and what is included.\n    Provide details about the number of points used and the relations points-targets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674683145,
            "cdate": 1698674683145,
            "tmdate": 1699635996043,
            "mdate": 1699635996043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wc7sAv8aVM",
                "forum": "B4vzu2aokv",
                "replyto": "zi5FviC7T6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QcUt (Part 1 WK1----WK3)"
                    },
                    "comment": {
                        "value": "**Dear Reviewer,**\n\nThank you for your insightful comments.\n>***W1:Novelty***\n\n**A1:** (1) Methodological Distinction: Our mutual distillation method (MDM) fundamentally differs from BESTIE in its approach. While BESTIE utilizes a unidirectional process for semantic knowledge transfer, our MDM is a bidirectional process. This two-way interaction enables a more robust and comprehensive learning mechanism, which is critical for the complexity of point-level weak supervision in instance segmentation.\n\n(2) Performance Improvements: As demonstrated in our visual results, the MDM approach shows superior performance compared to BESTIE, especially in handling the boundaries of adjacent objects of the same class. These improvements are significant as they address a common challenge in instance segmentation, demonstrating the practical impact of our method.\n\n(3) Innovative Aspects: As noted by the first four reviewers, our approach is both innovative and interesting. The bidirectional nature of our MDM method represents a novel contribution to the field, offering a new perspective and methodology, for instance, segmentation tasks.\n\n>***W2:Organization***\n\n**A2:** (1) Instance Affinity Matrix and Class Map in HRNet: HRNet serves as a backbone network in our experiments, functioning primarily for feature extraction. It can indeed be substituted with other networks such as ResNet. We will update our paper to include this clarification and cite the HRNet paper (Yuan et al., 2019).\n>>***Q1: How the semantic segmentation is obtained.***\n\n(2) Following the concept from BESTIE regarding the use of semantic segmentation maps, our semantic segmentation maps are derived from weak supervision, specifically employing the PMM model (\"Pseudo-mask Matters in Weakly-Supervised Semantic Segmentation\").\n\n>>***Q4: Provide details about the number of points used and the relations points-targets.***\n\n(3) In response to your query about the number of points used and their relation to the targets, our method indeed involves single-point participation, similar to the class center point setting used in the articles \"BESTIE\" and \"Point-to-Box Network for Accurate Object Detection via Single Point Supervision\".  This approach is guided by our aim to maintain a balance between the accuracy of instance segmentation and the efficiency of the annotation process.\n\n>>***Q3: Why so many iterations are used, and what is included?***\n\n(4) I would like to clearly clarify that in our methodology, although the LossI2S and LossS2I are iteratively calculated, they are concurrently aggregated and backpropagated within the same iteration, which guarantees that the training process is synchronized, and there is no increase in the number of training iterations.\n>***WK3:In particular***\n\n**A3:** (1) To clarify, our approach, similar to the BESTIE model, utilizes a class center point for point-level weak supervision in instance segmentation. We have added explanations in Sec4.3\uff1a In Table 5, the term \u201chard pixel ratio\u201d refers to the proportion of challenging samples used in loss computation.\n\n(2) We have conducted a comprehensive comparison of our method with the BESTIE method in terms of GFLOPs and FPS. The results of this comparison, as detailed in Table 2* as follows, reveal that our method only slightly increases the GFLOPs and FPS values compared to BESTIE. The increase in computation is mainly attributed to the additional label generator component of our method, which produces labels for training a fully supervised segmenter. However, during the inference stage, only the fully supervised segmenter is utilized, which means the inference time remains the same as that of BESTIE.\n\n**Table 2\\* Comparison of our method with the BESTIE method in terms of GFLOPs and FPS.**\n| Method | GFLOPS | FPS              |\n|--------|--------|------------------|\n| BESTIE | 64.7   | 86.9 ms/img      |\n| **Ours**   | **66.1** | **94.5 ms/img** |\n\n(3) The impact of semantic segmentation:\n\n| Semantic Segmentation      |     |     | Instance Segmentation |     |     |\n|----------------------------|-----|-----|-----------------------|-----|-----|\n| **WSSS method**            | mIoU|     | **mAP50**             |     |     |\n| PMM(Weakly Supervised)     | 70  |     | 55.6                  |     |     |\n| GT(Fully Supervised)       | -   |     | 59.7                  |     |     |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498173916,
                "cdate": 1700498173916,
                "tmdate": 1700498173916,
                "mdate": 1700498173916,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nJyGj5HaEz",
                "forum": "B4vzu2aokv",
                "replyto": "zi5FviC7T6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(WK4----WK5)"
                    },
                    "comment": {
                        "value": ">***WK4: Ambiguities***\n\n**A4:** (1) Regarding your observation about the weights of the loss components in $L_{S2I}$, we would like to clarify that the weights for $L_{offset}$ and $L_{segmentation}$, respectively, do not necessarily need to sum to 1. This is because these weights are not probabilities but rather scaling factors that balance the contributions of the two-loss components to the overall loss function. In our case, the weight for the segmentation loss is set to 1.0, signifying its primary importance in the segmentation task. The offset loss, with a weight of 0.01, plays a secondary role, fine-tuning the segmentation results rather than driving them. This choice of weights is based on empirical observations where we found this configuration to provide the best balance between the two types of loss, leading to optimal performance in our experiments.\n\n>>***Q2: Why BESTIE is retrained, for both PascalVOC-2012 and COCO-2017.***\n\n(2) Our team decided to retrain BESTIE due to the unavailability of its original code. Despite our best efforts to replicate the performance of BESTIE, we encountered challenges in fully reproducing its results. This deviation could be attributed to variations in implementation details, which are not available in the public domain. Furthermore, we reached out to the authors of BESTIE for assistance and access to the code, but unfortunately, we did not receive a positive response. In light of these circumstances, retraining BESTIE was a necessary step to ensure the integrity and reproducibility of our research findings. It also allowed us to adapt the model more closely to the specific requirements of our dataset and research objectives. Should you have access to the original BESTIE code or can facilitate a connection with the authors, we would be immensely grateful and more than willing to integrate it into our research.\n\n(3) We observed that AttentionShift employs a strong pre-trained model, which might not provide a fair basis for direct comparison with our approach.\n\n(4) In our study, we utilized Point2Mask in the context of instance segmentation, considering its capability in panoptic segmentation. This was based on the understanding that a model effective in panoptic segmentation should also perform well in instance segmentation tasks. The discrepancy in our reported mAP values stems from our experimental setup, where we adapted Point2Mask for instance segmentation. In addition, we consider BESTIE as the base of our paper's method, and Point2mask as a comparative experiment can better enrich our experiment.\n\n>***WK5: Other comments***\n\n**A5:** Thanks. (1) We have made corrections in the manuscript\uff1a change \u201cdenotes the \u03b2 times Hardmard power\u201d to \u201cdenotes the \u03b2 times Hadamard power\u201d, \u201cThe Hardmard power of the affinity matrix provides additional information\u201d to \u201cThe Hadamard power of the affinity matrix provides additional information\u201d.\n\n(2) We have already explained the 'instance ownership relationship' in Sec3.2: \u201cSpecifically, in the instance similarity matrix, if two pixels belong to the same instance, their value is set to 1, otherwise, it is set to 0. \u201d\n\n(3) We have made corrections in the Pg. 5, eq. (5), we have added an explanation that \u201cA is assumed to be generated by the network HRNet\u201d\n\n(4) We have made corrections in the Pg 6: \"COCO (i.e. COCO 2017) includes 118k images.\" The experiment of the test set has been supplemented.\n\n**Table 1\\* Quantitative comparison of the state-of-the-art WSIS methods on MS COCO 2017 test-dev**\n\n| Method | Sup. | Extra | AP | AP$_{50}$ | AP$_{75}$ |\n| ------ | ---- | ----- | -- | --------- | --------- |\n| **COCO test-dev.** |\n| Mask R-CNN | $\\mathcal{F}$ | - | 35.7 | 58.0 | 37.8 |\n| $\\text{Fan $et\\;al.$}$ | $\\mathcal{I}$ | - | 13.7 | 25.5 | 13.5 |\n| LIID  | $\\mathcal{I}$ | $\\mathcal{M}$, $\\mathcal{S_I}$ | 16.0 | 27.1 | 16.5 |\n| $\\text{BESTIE}^\u2020$  | $\\mathcal{P}$ | - | 14.2 | 28.6 | 12.7 |\n| **Ours** | **$\\mathcal{P}$** | - | **17.4** | **33.3** | **16.4** |\n\n(5) We have made corrections in Pg 6\uff1a \"We assess the performance of instance segmentation using two measures. We measure the performance using the standard protocol mean Average Precision(mAP).\""
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498423221,
                "cdate": 1700498423221,
                "tmdate": 1700498954688,
                "mdate": 1700498954688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xoBrVBe5pN",
                "forum": "B4vzu2aokv",
                "replyto": "zi5FviC7T6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QcUt (Part 2 Writing and Presentation improvement)"
                    },
                    "comment": {
                        "value": "***Writing and Presentation improvement***\n\n(1) We have made corrections in the manuscript: change \u201cdenotes the \u03b2 times Hardmard power\u201d to \u201cdenotes the \u03b2 times Hadamard power\u201d, \u201cThe Hardmard power of the affinity matrix provides additional information\u201d to \u201cThe Hadamard power of the affinity matrix provides additional information\u201d.\n\n(2) We have already explained the 'instance ownership relationship' in Sec3.2: \u201cSpecifically, in the instance similarity matrix, if two pixels belong to the same instance, their value is set to 1, otherwise, it is set to 0. \u201d\n\n(3) We have made corrections in the Pg. 5, eq. (5), we have added an explanation that \u201cA is assumed to be generated by the network HRNet\u201d\n\n(4) We have made corrections in the Pg 6: \"COCO (i.e. COCO 2017) includes 118k images.\" The experiments of the test set have been supplemented.\n\n(5) We have made corrections in Pg 6: \"We assess the performance of instance segmentation using two measures. We measure the performance using the standard protocol mean Average Precision(mAP).\"\n\n(6) We have updated our paper to cite the HRNet paper (Yuan et al., 2019).\n- Sun, Ke; Xiao, Bin; Liu, Dong; Wang, Jingdong. \"Deep high-resolution representation learning for human pose estimation.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5693-5703. 2019.\n\n(7) We have added explanations in Sec4.3\uff1a \"In Table 5, the term \u201chard pixel ratio\u201d refers to the proportion of challenging samples used in loss computation.\"\n\n(8) Dim-align in Fig.3: The term 'Dim-align' refers to 'dimension alignment,' a necessary step in our method. Prior to matrix multiplication, we rearrange the dimensions of our tensors for compatibility, a process accomplished using the transpose or permute functions in PyTorch. \n\n(9) Instance Adaptive Grouping in Sec.3.1: Our network creates final instance segmentation labels based on a combination of predicted semantic segmentation results, instance similarity matrices, and point annotations. If conflicts arise between semantic predictions and point-based instance segmentation, we prioritize the point annotations. For points lacking final instance predictions, we assign a pseudo-box of size 16x16. This method ensures that our semantic segmentation results, a fusion of network predictions and instance similarity matrices, are more representative of instance characteristics, thereby improving instance segmentation outcomes. \n\n(10) Green Arrow in Fig.3 (OffsetMap): The green arrow represents the OffsetMap, which, in conjunction with the Semantic Map, generates new instance segmentation results. This process is detailed in the right half of Fig. 5. \n\n(11) We have cited these references\uff1a\n- Kweon, Hyeokjun, Yoon, Sung-Hoon, & Yoon, Kuk-Jin. (2023). Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 11329--11339).\n\n- Cheng, Zesen, Qiao, Pengchong, Li, Kehan, Li, Siheng, Wei, Pengxu, Ji, Xiangyang, Yuan, Li, Liu, Chang, & Chen, Jie. (2023). Out-of-candidate rectification for weakly supervised semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 23673--23684).\n\n- Rong, Shenghai, Tu, Bohai, Wang, Zilei, & Li, Junjie. (2023). Boundary-Enhanced Co-Training for Weakly Supervised Semantic Segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 19574--19584).\n\n- Ru, Lixiang, Zheng, Heliang, Zhan, Yibing, & Du, Bo. (2023). Token contrast for weakly-supervised semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 3093--3102).\n\n(12) We have carefully modified the introduction to make it more readable.\n\nWe have revised the sentence: \"Instance segmentation is a critical task in computer vision, where semantic segmentation estimation and instance discrimination are equally important.\"\n\nAdditionally, we have updated the phrase: \u201cand it aims not only to locate objects accurately but also to estimate their boundaries to differentiate.\u201d\n\nWe have also replaced the terms \u201csemantic segmentation\u201d and \u201cinstance segmentation\u201d with \u201csemantic information\u201d and \u201cinstance information\u201d in the second paragraph of the introduction to provide a clearer and more accurate description."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498592429,
                "cdate": 1700498592429,
                "tmdate": 1700498592429,
                "mdate": 1700498592429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tJxugn2fb5",
                "forum": "B4vzu2aokv",
                "replyto": "xoBrVBe5pN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Reviewer_QcUt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Reviewer_QcUt"
                ],
                "content": {
                    "title": {
                        "value": "thank you"
                    },
                    "comment": {
                        "value": "Thank you for your reply.\n\nIt seems that the authors did not answer my questions. \nAbout the points used, \"our method indeed involves single-point participation\" does not seem clear enough.\n\nMoreover, the decision to retrain BESTIE is not justified, likewise the wrong evaluation of the results of Point2Mask.  \nThe other explanations are not clear.\n\nI confirm my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677745590,
                "cdate": 1700677745590,
                "tmdate": 1700677745590,
                "mdate": 1700677745590,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FWARqYM7QC",
            "forum": "B4vzu2aokv",
            "replyto": "B4vzu2aokv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission688/Reviewer_3EDk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission688/Reviewer_3EDk"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problems of local response and seprated semantic and instance learning on the pointly-supervised instance segmentation task. To solve the problems, the authors propose the Mutual Distillation Module, which achieves the conversion and cooperation between semantic and instance segmentation by predicting class, offset, and affinity, thus leveraging the complementary strengths of instance and semantic. Various experiments demonstrate the effective of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "** The analysis of the complementarity of instance segmentation and semantic segmentation is reasonable for the pointly-supervised instance segmentation task, and the designed method corresponds to the analysis.\n\n** The core S2I method is unique and more accurate than BESTIE.\n\n** Experiments demonstrate that both the Semantic to Instance (S2I) and Istance to Semantic (I2S) module is effective and the overall method outperforms previous works."
                },
                "weaknesses": {
                    "value": "**  Writing and presentation need improvement. For example, there are some confusing descriptions that are not explained (e.g., Dim-align in Fig. 3, instance adaptive grouping in Sec. 3.1, the green arrow from S2I loss to the mask in Fig. 3).\n\n**  Some key technical details are missing. In Fig. 3, there are multiple MDM stages, but the required number of stages are not specified or experimented. the additional training cost should be discussed.\n\n** The ablation about \u03b2 in Sec. 3.2 are missing, so the effect on \u201csmoothening the distribution to attain the optimal semantic segmentation map\u201d is not clear.\n\n** According to Sec. 3.1, the pseudo labels used in the first stage are obtained by SAM, but SAM is contrary to the setting of weak supervision. Moreover, in Tab. 1 and 2 the authors mark the proposed method as \u201cno extra data\u201d. But in my opinion, the used off-the-shelf segmentation map should be considered as extra data.\n\n**  Will the proposed S2I method be sensitive to the position of the points? Could the author provide some qualitative or quantitative analysis to further illustrate the robustness of the method?"
                },
                "questions": {
                    "value": "**  Could the author explain the details described in Weaknesses 1?\n\n**  What impact do the hyperparameter \u03b2 and the number of MDM stages have on performance?\n\n**  Does the use of SAM destroy fair comparisons, and would other pseudo-label generators be useful?\n\n**   Will the proposed S2I method be sensitive to the position of the points?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission688/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission688/Reviewer_3EDk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743642389,
            "cdate": 1698743642389,
            "tmdate": 1700499469348,
            "mdate": 1700499469348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JlUr3IHhsu",
                "forum": "B4vzu2aokv",
                "replyto": "FWARqYM7QC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3EDk (Part 1)"
                    },
                    "comment": {
                        "value": "**Dear Reviewer,**\n\nWe appreciate your valuable suggestions and insights, we have addressed each point as follows:\n\n>***WK1&Q1: Writing and presentation need improvement.***\n\n**A1:** Thanks. We have explained confusing descriptions in a dedicated section for replies and clarifications.\n\n>***WK2: Some key technical details are missing. The required number of MDM stages is not specified or experimented.***\n\n**A2:**  In our study, we intentionally set the number of stages to 1. We found that even with a single MDM stage, our method achieved significant and satisfactory results. Your constructive suggestions for multiple MDM stages may potentially enhance the performance of our model. We intend to make a further investigation of the performance and efficiency in the future.\n\n>***WK3&Q2: What impact do the hyperparameter \u03b2 and the number of MDM stages have on performance?***\n\n**A3:** We added a new table (Table 8) for the ablation study about the parameter \u03b2 in Section 4.3 as follows:\n\nThe parameter \u03b2: As shown in Table 8, setting \u03b2 to 1 enables our model to attain an optimal equilibrium, culminating in the most effective performance within our weakly-supervised framework. It is noteworthy that an increment in \u03b2 corresponds to a decrement in performance by 0.5%.\n\n| \u03b2 | mAP50 |\n|------|-------|\n| 1    | 51.5  |\n| 2    | 51.0  |\n| 3    | 51.0  |\n| 4    | 51.0  |\n\n>***WK4&Q3: Does the use of SAM destroy fair comparisons, and would other pseudo-label generators be useful? The used off-the-shelf segmentation map should be considered as extra data?***\n\n**A4:** (1) In weakly supervised learning, pseudo-labels are inferred from limited or incomplete information. For image segmentation, this might mean inferring pixel-level labels from image-level labels or bounding box labels.\n\nThe role of SAM is to use existing weak labels or partial information to generate these more detailed pseudo-labels. \nWeakly supervised learning involves not only training with incomplete labels but also the process of processing and enhancing these labels. Here, SAM acts as a 'label enhancer,' transforming weak labels into more detailed forms, making them more suitable for image segmentation tasks.\n\nThis approach differs from fully supervised learning, which relies on fully annotated training data. In weakly supervised learning, even if the quality of pseudo-labels may not be as high as fully supervised data, models can still learn from them and improve performance.\n\nUsing SAM to generate initial pseudo-labels and for training weakly supervised segmentation models represents an innovative strategy in weakly supervised learning. The key to this strategy is that it enhances the learning ability of the model by inferring more detailed labels from limited information while maintaining the fundamental principles of weakly supervised learning \u2014 not relying on fully annotated data.\n\n(2) For extra data: In our study, 'no extra data' means that no data outside of what was originally provided in the dataset was used during the training and testing processes. The segmentation maps we used were generated from the original dataset, not from external sources. Therefore, we believe this conforms to the definition of 'no extra data'. These segmentation maps were obtained through preprocessing of the existing data, rather than being a separate, external dataset.\n\nThe segmentation maps obtained through the PMM weak supervision method are part of the model training, not an additional input independent of the main dataset. This approach is taken to enhance the performance of the model rather than to introduce external data. This improvement in our method can be confirmed in Table 1.\n\n>***WK5&Q4: Will the proposed S2I method be sensitive to the position of the points?***\n\n**A5:** We added a new table (Table 9) for the ablation study about the sensitivity of our proposed method to the positioning of points in Section 4.3 as follows:\n\nDrift-point: As shown in Table 9, we conducted an ablation study where we applied Gaussian random perturbations with standard deviations (\u03c3) of 5, 10, and 15 to the coordinates of the center points of each object. Our results reveal that when the centroid points are subject to a Gaussian perturbation with \u03c3 = 5, the $mAP_{50}$ scores are consistent with those obtained when the centroids are precisely located. However, as the standard deviation increases to \u03c3 = 10 and \u03c3 = 15, we observe a decrement in mAP50 by 0.6% and 1.5%, respectively, compared to the precise centroid placement.\nOur findings indicate that while the performance of our method does experience a slight impact when the points undergo displacement, the overall effect remains within an acceptable range. \n\n| Drift point(\u03c3)  | $\\rm mAP_{50}$ |\n| --------------- | -------------- |\n| Center point    | 64.4           |\n| 5               | 64.4           |\n| 10              | 63.8           |\n| 15              | 62.9           |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497655263,
                "cdate": 1700497655263,
                "tmdate": 1700497655263,
                "mdate": 1700497655263,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xThqbyE6Wh",
                "forum": "B4vzu2aokv",
                "replyto": "FWARqYM7QC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3EDk(Part 2 Writing and Presentation improvement)"
                    },
                    "comment": {
                        "value": "***Writing and Presentation improvement***\n\n(1) Dim-align in Fig.3: The term 'Dim-align' refers to 'dimension alignment,' a necessary step in our method. Prior to matrix multiplication, we rearrange the dimensions of our tensors for compatibility, a process accomplished using the transpose or permute functions in PyTorch. \n\n(2) Instance Adaptive Grouping in Sec.3.1: Our network creates final instance segmentation labels based on a combination of predicted semantic segmentation results, instance similarity matrices, and point annotations. If conflicts arise between semantic predictions and point-based instance segmentation, we prioritize the point annotations. For points lacking final instance predictions, we assign a pseudo-box of size 16x16. This method ensures that our semantic segmentation results, a fusion of network predictions and instance similarity matrices, are more representative of instance characteristics, thereby improving instance segmentation outcomes. \n\n(3) Green Arrow in Fig.3 (OffsetMap): The green arrow represents the OffsetMap, which, in conjunction with the Semantic Map, generates new instance segmentation results. This process is detailed in the right half of Fig. 5. \n\n(4) We have cited these references\uff1a\n- Kweon, Hyeokjun, Yoon, Sung-Hoon, & Yoon, Kuk-Jin. (2023). Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 11329--11339).\n\n- Cheng, Zesen, Qiao, Pengchong, Li, Kehan, Li, Siheng, Wei, Pengxu, Ji, Xiangyang, Yuan, Li, Liu, Chang, & Chen, Jie. (2023). Out-of-candidate rectification for weakly supervised semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 23673--23684).\n\n- Rong, Shenghai, Tu, Bohai, Wang, Zilei, & Li, Junjie. (2023). Boundary-Enhanced Co-Training for Weakly Supervised Semantic Segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 19574--19584).\n\n- Ru, Lixiang, Zheng, Heliang, Zhan, Yibing, & Du, Bo. (2023). Token contrast for weakly-supervised semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 3093--3102).\n\n(5) We have carefully modified the introduction to make it more readable.\n\nWe have revised the sentence: \"Instance segmentation is a critical task in computer vision, where semantic segmentation estimation and instance discrimination are equally important.\"\n\nAdditionally, we have updated the phrase: \u201cand it aims not only to locate objects accurately but also to estimate their boundaries to differentiate.\u201d\n\nWe have also replaced the terms \u201csemantic segmentation\u201d and \u201cinstance segmentation\u201d with \u201csemantic information\u201d and \u201cinstance information\u201d in the second paragraph of the introduction to provide a clearer and more accurate description.\n\n(6) We have made corrections in the manuscript: change \u201cdenotes the \u03b2 times Hardmard power\u201d to \u201cdenotes the \u03b2 times Hadamard power\u201d, \u201cThe Hardmard power of the affinity matrix provides additional information\u201d to \u201cThe Hadamard power of the affinity matrix provides additional information\u201d.\n\n(7) We have already explained the 'instance ownership relationship' in Sec3.2: \u201cSpecifically, in the instance similarity matrix, if two pixels belong to the same instance, their value is set to 1, otherwise, it is set to 0. \u201d\n\n(8) We have made corrections in the Pg. 5, eq. (5), we have added an explanation that \u201cA is assumed to be generated by the network HRNet\u201d\n\n(9) We have made corrections in the Pg 6: \"COCO (i.e. COCO 2017) includes 118k images.\" The experiments of the test set have been supplemented.\n\n(10) We have made corrections in Pg 6: \"We assess the performance of instance segmentation using two measures. We measure the performance using the standard protocol mean Average Precision(mAP).\"\n\n(11) We have updated our paper to cite the HRNet paper (Yuan et al., 2019).\n- Sun, Ke; Xiao, Bin; Liu, Dong; Wang, Jingdong. \"Deep high-resolution representation learning for human pose estimation.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5693-5703. 2019.\n\n(12) We have added explanations in Sec4.3\uff1a \"In Table 5, the term \u201chard pixel ratio\u201d refers to the proportion of challenging samples used in loss computation.\""
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497774303,
                "cdate": 1700497774303,
                "tmdate": 1700497774303,
                "mdate": 1700497774303,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "baFUn3HoWa",
                "forum": "B4vzu2aokv",
                "replyto": "FWARqYM7QC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Reviewer_3EDk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Reviewer_3EDk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the careful reply. It addressed my concerns. I would like to raise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499548774,
                "cdate": 1700499548774,
                "tmdate": 1700499548774,
                "mdate": 1700499548774,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HFhxhyCUnm",
            "forum": "B4vzu2aokv",
            "replyto": "B4vzu2aokv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission688/Reviewer_ZJkY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission688/Reviewer_ZJkY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Mutual Distillation Module (MDM) to leverage the complementary strengths of both instance position and semantic information and achieve accurate instance-level object perception. It consists of Semantic to Instance (S2I) and Instance to Semantic (I2S) sepcifically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This method is novel and new. Training in a multi-stage way, combining the advantages of both semantic segmentation and instance segmentation looks reasonable. The experimental improvement also looks good."
                },
                "weaknesses": {
                    "value": "1. Training in a multi-stage way, I feel concerned about the efficiency of the method. The author should provide comparisons about efficiency, like GFLOPS, FPS, inference time and so on, with existing methods, to demonstrate that the improvement does not come from extra computational budget. In addition, will multi-stage training derive into more training iteration numbers? If so, the author should also provide fair comparison with the same iteration numbers.\n2. The ablation study is conducted on VOC, where S2I looks like a 0.4% improvement. The effect of S2I is thus quite limited. I am also curious about the ablation study on the COCO dataset. Will the improvement becomes less? If se, I feel that S2I is unnessary and extra.\n3. What's about the performance of the method on the COCO dataset with ResNet?"
                },
                "questions": {
                    "value": "Please refer to the weakness part. I will adjust my rating based on the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission688/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission688/Reviewer_ZJkY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757256415,
            "cdate": 1698757256415,
            "tmdate": 1699635995865,
            "mdate": 1699635995865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r1nruiaLZ5",
                "forum": "B4vzu2aokv",
                "replyto": "HFhxhyCUnm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZJkY (Part 1)"
                    },
                    "comment": {
                        "value": "**Dear Reviewer,**\n\nThank you very much for your valuable comments and suggestions, which deeply improve our manuscript.\n>***Q1: The author should provide comparisons about efficiency, like GFLOPS, FPS, inference time, and so on, with existing methods, to demonstrate that the improvement does not come from the extra computational budget. In addition, will multi-stage training derive into more training iteration numbers?***\n\n**A1:** (1) We have conducted a comprehensive comparison of our method with the BESTIE method in terms of GFLOPs and FPS. The results of this comparison, as detailed in Table 2* as follows, reveal that our method only slightly increases the GFLOPs and FPS values compared to BESTIE. The increase in computation is mainly attributed to the additional label generator component of our method, which produces labels for training a fully supervised segmenter. However, during the inference stage, only the fully supervised segmenter is utilized, which means the inference time remains the same as that of BESTIE.\n\n**Table 2\\* Comparison of our method with the BESTIE method in terms of GFLOPs and FPS.**\n\n| Method | GFLOPS | FPS              |\n|--------|--------|------------------|\n| BESTIE | 64.7   | 86.9 ms/img      |\n| **Ours**   | **66.1** | **94.5 ms/img** |\n\n(2) In terms of training iterations, I would like to clearly clarify that multi-stage training in our model does not lead to an increase in the number of training iterations. The iteration number is 1 because although the LossI2S and LossS2I are iteratively calculated, they are concurrently aggregated and backpropagated within the same iteration, which guarantees that the training process is synchronized, and there is no increase in the number of training iterations.\n\n>***Q2: The ablation study on the COCO dataset. Will the improvement become less?***\n\n**A2:**\nIn our research, we conducted a series of ablation studies on the COCO dataset to provide a comprehensive evaluation of our methodology. The results of these ablation experiments have been incorporated into Section 4.3 of our paper, specifically under the subsections 'Effect of S2I' and 'Effect of I2S':\n\nAs shown in Table 7, S2I obtains 3.2 mAP$_{50}$ performance improvement on PSIS tasks compared with the previous methods on COCO, showing the effectiveness of S2I statistically.\n\n**Effect of I2S.** The I2S module improves instance discrimination in the same class by using the instance affinity matrix. As shown in Table 7, I2S raises mAP$_{50}$ from 17.4 to 17.6. This discrepancy in performance enhancement between the two datasets can likely be attributed to their inherent differences in features and complexity. The variations in performance improvements across datasets suggest that each module plays a crucial role, depending on the specific characteristics and challenges presented by the dataset.\n\n| S$\\rightarrow$I   | I$\\rightarrow$S   | $\\rm mAP_{50}$ |\n|-------------------|-------------------|----------------|\n| BESTIE            |                   | 14.2           |\n| S2I               |                   | 17.4           |\n| S2I               | I2S               | 17.6           |\n>***Q3: What's about the performance of the method on the COCO dataset with ResNet?***\n\n**A3:** We updated the Table 2 in the manuscript. We have added the performance of our method on the COCO dataset with ResNet-101, which achieved an AP of 15.2.\n\n| Method | Sup. | Backbone | Extra | AP | AP$_{50}$ | AP$_{75}$ |\n| ------ | ---- | -------- | ----- | -- | ---- | ---- |\n| $\\text{Point2Mask}$ | $\\mathcal{P}$ | ResNet-101 | - | 12.8 | 26.3 | 11.2 |\n| $\\text{BESTIE}^\u2020$  | $\\mathcal{P}$ | HRNet-48 | - | 14.2 | 28.4 | 22.5 |\n| $\\text{SAM}$ | $\\mathcal{P+S}$ | ViT-S/22.1M | - | **19.5** | **36.8** | 18.8 |\n| **Ours** | **$\\mathcal{P}$** | ResNet-101 | - | 15.2 | 30.1 | 24.6 |\n| **Ours** | **$\\mathcal{P}$** | HRNet-48 | - | 17.6 | 33.6 | **28.1** |\n\nWe are grateful for your insights and are hopeful that our revised manuscript address your concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496904065,
                "cdate": 1700496904065,
                "tmdate": 1700496904065,
                "mdate": 1700496904065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GW3BTKpBzl",
                "forum": "B4vzu2aokv",
                "replyto": "HFhxhyCUnm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZJkY (Part 2 Writing and Presentation improvement)"
                    },
                    "comment": {
                        "value": "***Writing and Presentation improvement***\n\n(1) We have cited these references\uff1a\n- Kweon, Hyeokjun, Yoon, Sung-Hoon, & Yoon, Kuk-Jin. (2023). Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 11329--11339).\n\n- Cheng, Zesen, Qiao, Pengchong, Li, Kehan, Li, Siheng, Wei, Pengxu, Ji, Xiangyang, Yuan, Li, Liu, Chang, & Chen, Jie. (2023). Out-of-candidate rectification for weakly supervised semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 23673--23684).\n\n- Rong, Shenghai, Tu, Bohai, Wang, Zilei, & Li, Junjie. (2023). Boundary-Enhanced Co-Training for Weakly Supervised Semantic Segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 19574--19584).\n\n- Ru, Lixiang, Zheng, Heliang, Zhan, Yibing, & Du, Bo. (2023). Token contrast for weakly-supervised semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 3093--3102).\n\n(2) We have carefully modified the introduction to make it more readable.\n\nWe have revised the sentence: \"Instance segmentation is a critical task in computer vision, where semantic segmentation estimation and instance discrimination are equally important.\"\n\nAdditionally, we have updated the phrase: \u201cand it aims not only to locate objects accurately but also to estimate their boundaries to differentiate.\u201d\n\nWe have also replaced the terms \u201csemantic segmentation\u201d and \u201cinstance segmentation\u201d with \u201csemantic information\u201d and \u201cinstance information\u201d in the second paragraph of the introduction to provide a clearer and more accurate description.\n\n(3) Dim-align in Fig.3: The term 'Dim-align' refers to 'dimension alignment,' a necessary step in our method. Prior to matrix multiplication, we rearrange the dimensions of our tensors for compatibility, a process accomplished using the transpose or permute functions in PyTorch. \n\n(4) Instance Adaptive Grouping in Sec.3.1: Our network creates final instance segmentation labels based on a combination of predicted semantic segmentation results, instance similarity matrices, and point annotations. If conflicts arise between semantic predictions and point-based instance segmentation, we prioritize the point annotations. For points lacking final instance predictions, we assign a pseudo-box of size 16x16. This method ensures that our semantic segmentation results, a fusion of network predictions and instance similarity matrices, are more representative of instance characteristics, thereby improving instance segmentation outcomes. \n\n(5) Green Arrow in Fig.3 (OffsetMap): The green arrow represents the OffsetMap, which, in conjunction with the Semantic Map, generates new instance segmentation results. This process is detailed in the right half of Fig. 5. \n\n(6) We have made corrections in the manuscript: change \u201cdenotes the \u03b2 times Hardmard power\u201d to \u201cdenotes the \u03b2 times Hadamard power\u201d, \u201cThe Hardmard power of the affinity matrix provides additional information\u201d to \u201cThe Hadamard power of the affinity matrix provides additional information\u201d.\n\n(7) We have already explained the 'instance ownership relationship' in Sec3.2: \u201cSpecifically, in the instance similarity matrix, if two pixels belong to the same instance, their value is set to 1, otherwise, it is set to 0. \u201d\n\n(8) We have made corrections in the Pg. 5, eq. (5), we have added an explanation that \u201cA is assumed to be generated by the network HRNet\u201d\n\n(9) We have made corrections in the Pg 6: \"COCO (i.e. COCO 2017) includes 118k images.\" The experiments of the test set have been supplemented.\n\n(10) We have made corrections in Pg 6: \"We assess the performance of instance segmentation using two measures. We measure the performance using the standard protocol mean Average Precision(mAP).\"\n\n(11) We have updated our paper to cite the HRNet paper (Yuan et al., 2019).\n- Sun, Ke; Xiao, Bin; Liu, Dong; Wang, Jingdong. \"Deep high-resolution representation learning for human pose estimation.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5693-5703. 2019.\n\n(12) We have added explanations in Sec4.3\uff1a \"In Table 5, the term \u201chard pixel ratio\u201d refers to the proportion of challenging samples used in loss computation.\""
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496987138,
                "cdate": 1700496987138,
                "tmdate": 1700496987138,
                "mdate": 1700496987138,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "92ozFBdyCW",
                "forum": "B4vzu2aokv",
                "replyto": "GW3BTKpBzl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Reviewer_ZJkY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Reviewer_ZJkY"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reply"
                    },
                    "comment": {
                        "value": "Thanks for the author's rebuttal. It addresses most of my concerns. However, the effect of S2I and I2S is still unclear for me. It seems that S2I improves little for the VOC dataset, while I2S improves little for COCO. I fell confused about this. I will remain my original rating now, marginally above the acceptance threshold. However, I am not particularly opposed to rejecting this paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719767117,
                "cdate": 1700719767117,
                "tmdate": 1700719767117,
                "mdate": 1700719767117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E1N32lxtg2",
            "forum": "B4vzu2aokv",
            "replyto": "B4vzu2aokv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission688/Reviewer_fR5S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission688/Reviewer_fR5S"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a novel approach \u201cmutual distillation\u201d for Point Supervised Instance Segmentation (PSIS). The method utilizes point-supervised semantic segmentation results as an initialization for guiding instance segmentation, and then use an affinity matrix that represents instance segmentation details to optimize the semantic information This process achieves mutual distillation between instance and semantic information to improve the final result of instance segmentation. They validated the effectiveness of the proposed method on the VOC and COCO datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe concept of \"MUTUAL DISTILLATION\" in point instance segmentation is both novel and interesting. In my experimental observations, BESTIE doesn't handle adjacent objects well, possibly because it relies solely on semantic segmentation results without fully utilizing point-represented instance information. The motivation behind this paper aligns with my observations, and therefore, the proposed concept of mutual information exchange between semantic and instance information seems sound to me.\n2.\tThe paper's experimental section is comprehensive, using two datasets, different backbones, segmentation architectures, and conducting essential ablation studies.\n3. It appears that the visualization is effectively presented."
                },
                "weaknesses": {
                    "value": "1.\tObservations from Figure 7 and Figure 8 suggest that the proposed mutual distillation method appears to perform well in segmenting adjacent objects and addressing missing object issues. It would greatly enhance the quality of this paper if the authors could provide a quantitative analysis of these cases.\n2.\tMinor issue\nThe writing of introduction should be improved somewhat.\n-  \"Instance segmentation is a critical task in computer vision and is equally important in semantic segmentation estimation and instance discrimination.\"\nto \n\"Instance segmentation is a critical task in computer vision, where semantic segmentation estimation and instance discrimination are equally important .\"\n- \u201cand it aims not only to locate objects accurately but also estimate their boundaries to differentiate\u201d\nto\n\u201cand it aims not only to locate objects accurately but also to estimate their boundaries to differentiate\u201d\nI understand the description in 2nd paragraph of introduction. But the \u672f\u8bed \u201csemantic segmentation\u201d and \u201cinstance segmentation\u201d may not suitbale, it should be \u201csemantic information\u201c and \u201cinstance information\u201d"
                },
                "questions": {
                    "value": "Please see the detailed information in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760691731,
            "cdate": 1698760691731,
            "tmdate": 1699635995759,
            "mdate": 1699635995759,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9fFgzKtB2R",
                "forum": "B4vzu2aokv",
                "replyto": "E1N32lxtg2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fR5S (Part 1)"
                    },
                    "comment": {
                        "value": "**Dear Reviewer,**\n\nThank you very much for your valuable comments and suggestions, which deeply improve our manuscript.\n>***Q1: It would greatly enhance the quality of this paper if the authors could provide a quantitative analysis of these cases.***\n\n**A1:** In Section 4.4 of the manuscript, we have added a quantitative analysis to complement the visual results presented in Figures 7 and 8:\n\nAs indicated in Table 10, this analysis is grounded in two key metrics: Missing Rate and Adjacent Rate. The Missing Rate quantitatively evaluates the instances where our method fails to segment objects as precisely as depicted in the ground truth. It is noteworthy that, in comparison to the BESTIE method, our approach demonstrates a significant reduction in the occurrence of object misses, with a Miss Rate that is lower by 3.9%. On the other hand, the Adjacent Rate metric assesses the proportion of successfully identified adjacent objects. Here, our method markedly enhances the capability to recognize neighboring objects, exhibiting a 30.4% higher Adjacency Rate than that achieved by BESTIE.\n\n| Method | Missing Rate | Adjacent Rate |\n| ------ | ------------ | ------------- |\n| BESTIE | 46.8%        | 22.2%         |\n| **Ours** | **42.9%**  | **52.6%**     |\n\n>***Q2: The writing of the introduction should be improved somewhat.***\n\n**A2:** Thanks. We have carefully modified the introduction to make it more readable.\n\nWe have revised the sentence from \"Instance segmentation is a critical task in computer vision and is equally important in semantic segmentation estimation and instance discrimination.\" to \"Instance segmentation is a critical task in computer vision, where semantic segmentation estimation and instance discrimination are equally important.\"\n\nAdditionally, we have updated the phrase \u201cand it aims not only to locate objects accurately but also estimate their boundaries to differentiate\u201d to \u201cand it aims not only to locate objects accurately but also to estimate their boundaries to differentiate.\u201d\n\nIn line with your advice, we have also replaced the terms \u201csemantic segmentation\u201d and \u201cinstance segmentation\u201d with \u201csemantic information\u201d and \u201cinstance information\u201d in the second paragraph of the introduction to provide a clearer and more accurate description.\n\nWe believe these revisions have improved the clarity and quality of our manuscript. Your insightful feedback has been instrumental in enhancing our paper, and we are grateful for your contribution to our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496366607,
                "cdate": 1700496366607,
                "tmdate": 1700496366607,
                "mdate": 1700496366607,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IF56n9eZZD",
                "forum": "B4vzu2aokv",
                "replyto": "E1N32lxtg2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fR5S(Part 2 Writing and Presentation improvement)"
                    },
                    "comment": {
                        "value": "***Writing and Presentation improvement***\n\n(1) We have carefully modified the introduction to make it more readable.\n\nWe have revised the sentence: \"Instance segmentation is a critical task in computer vision, where semantic segmentation estimation and instance discrimination are equally important.\"\n\nAdditionally, we have updated the phrase: \u201cand it aims not only to locate objects accurately but also to estimate their boundaries to differentiate.\u201d\n\nWe have also replaced the terms \u201csemantic segmentation\u201d and \u201cinstance segmentation\u201d with \u201csemantic information\u201d and \u201cinstance information\u201d in the second paragraph of the introduction to provide a clearer and more accurate description.\n\n(2) We have cited these references\uff1a\n- Kweon, Hyeokjun, Yoon, Sung-Hoon, & Yoon, Kuk-Jin. (2023). Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 11329--11339).\n\n- Cheng, Zesen, Qiao, Pengchong, Li, Kehan, Li, Siheng, Wei, Pengxu, Ji, Xiangyang, Yuan, Li, Liu, Chang, & Chen, Jie. (2023). Out-of-candidate rectification for weakly supervised semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 23673--23684).\n\n- Rong, Shenghai, Tu, Bohai, Wang, Zilei, & Li, Junjie. (2023). Boundary-Enhanced Co-Training for Weakly Supervised Semantic Segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 19574--19584).\n\n- Ru, Lixiang, Zheng, Heliang, Zhan, Yibing, & Du, Bo. (2023). Token contrast for weakly-supervised semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 3093--3102).\n\n(3) Dim-align in Fig.3: The term 'Dim-align' refers to 'dimension alignment,' a necessary step in our method. Prior to matrix multiplication, we rearrange the dimensions of our tensors for compatibility, a process accomplished using the transpose or permute functions in PyTorch. \n\n(4) Instance Adaptive Grouping in Sec.3.1: Our network creates final instance segmentation labels based on a combination of predicted semantic segmentation results, instance similarity matrices, and point annotations. If conflicts arise between semantic predictions and point-based instance segmentation, we prioritize the point annotations. For points lacking final instance predictions, we assign a pseudo-box of size 16x16. This method ensures that our semantic segmentation results, a fusion of network predictions and instance similarity matrices, are more representative of instance characteristics, thereby improving instance segmentation outcomes. \n\n(5) Green Arrow in Fig.3 (OffsetMap): The green arrow represents the OffsetMap, which, in conjunction with the Semantic Map, generates new instance segmentation results. This process is detailed in the right half of Fig. 5. \n\n(6) We have made corrections in the manuscript: change \u201cdenotes the \u03b2 times Hardmard power\u201d to \u201cdenotes the \u03b2 times Hadamard power\u201d, \u201cThe Hardmard power of the affinity matrix provides additional information\u201d to \u201cThe Hadamard power of the affinity matrix provides additional information\u201d.\n\n(7) We have already explained the 'instance ownership relationship' in Sec3.2: \u201cSpecifically, in the instance similarity matrix, if two pixels belong to the same instance, their value is set to 1, otherwise, it is set to 0. \u201d\n\n(8) We have made corrections in the Pg. 5, eq. (5), we have added an explanation that \u201cA is assumed to be generated by the network HRNet\u201d\n\n(9) We have made corrections in the Pg 6: \"COCO (i.e. COCO 2017) includes 118k images.\" The experiments of the test set have been supplemented.\n\n(10) We have made corrections in Pg 6: \"We assess the performance of instance segmentation using two measures. We measure the performance using the standard protocol mean Average Precision(mAP).\"\n\n(11) We have updated our paper to cite the HRNet paper (Yuan et al., 2019).\n- Sun, Ke; Xiao, Bin; Liu, Dong; Wang, Jingdong. \"Deep high-resolution representation learning for human pose estimation.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5693-5703. 2019.\n\n(12) We have added explanations in Sec4.3\uff1a \"In Table 5, the term \u201chard pixel ratio\u201d refers to the proportion of challenging samples used in loss computation.\""
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496532647,
                "cdate": 1700496532647,
                "tmdate": 1700496532647,
                "mdate": 1700496532647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "agjHEojgbp",
            "forum": "B4vzu2aokv",
            "replyto": "B4vzu2aokv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission688/Reviewer_enoR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission688/Reviewer_enoR"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses Point-level Supervised Instance Segmentation (PSIS). The authors argue that the existing PSIS methods usually suffer from the lack of contour annotations, and thereby precisely predicting boundaries is still challenging. As a remedy, this paper introduces the Mutual Distillation Module (MDM), leveraging the complementary benefits of semantic information and instance position. In MDM, Semantic to Instance (S2I) exploits the precise boundary information of semantic maps to enhance the instance contours. Meanwhile, Istance to Semantic (I2S) uses discriminative instances to differentiate the number of objects in the semantic map. Extensive experiments and comparisons are conducted on the PASCAL VOC and MS COCO datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea is sounding.\n2. The ablations studies are extensively conducted."
                },
                "weaknesses": {
                    "value": "[Major]\n1. Some important and recent WSSS methods are missing. Please refer to the following CVPR 2023 papers.\n* Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor\n* Out-of-Candidate Rectification for Weakly Supervised Semantic Segmentation\n* Boundary-Enhanced Co-Training for Weakly Supervised Semantic Segmentation\n* Token Contrast for Weakly-Supervised Semantic Segmentation\n\n2. The performance table is incomplete. First, please show the result of mAP25. Also, there have been several works using the transformer backbone, such as Point2Mask or AttentionShift. Please compare with them. In addition, where is BESTIE (with Res101)? Finally, please include the results of the COCO test set.\n\n3. The segment-anything model (SAM) can be an excellent option for solving PSIS. Please compare the proposed method with the SAM, from the performance perspective.\n\n[Minor]\nThe overall  writing should be enhanced."
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission688/Reviewer_enoR",
                        "ICLR.cc/2024/Conference/Submission688/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800396972,
            "cdate": 1698800396972,
            "tmdate": 1700594128766,
            "mdate": 1700594128766,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z714U4eWi6",
                "forum": "B4vzu2aokv",
                "replyto": "agjHEojgbp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer enoR (Part1)"
                    },
                    "comment": {
                        "value": "**Dear Reviewer,**\n\nThank you very much for your valuable and detailed feedback on our manuscript. We appreciate your insights and suggestions for improving our work.\n>***Q1: Some important and recent WSSS methods are missing.*** \n\n**A1:** Thanks, we cited the important and recent WSSS methods mentioned by you in our revised manuscript, and discussed them in the related work. Further, we also improved the writing and presentation of our paper.\n>***Q2\uff1aThe performance table is incomplete. First, please show the result of mAP25. Also, there have been several works using the transformer backbone, such as Point2Mask or AttentionShift. Please compare with them. In addition, where is BESTIE (with Res101)? Finally, please include the results of the COCO test set.***\n\n**A2:** \n(1) We updated Table 1 in the manuscript and added the result of mAP25 and the result of BESTIE (with Res101):\n\n**Table 1 Quantitative comparison of the state-of-the-art WSIS methods on VOC 2012 val-set(Part).**\n\nWe denote the supervision sources as: $\\mathcal{F}$ (full mask), $\\mathcal{B}$ (box-level label), $\\mathcal{I}$ (image-level label), $\\mathcal{P}$ (point-level label),  $\\mathcal{S}$ prompting SAM with ViT-Base for object mask annotations and $\\mathcal{C}$ (object count). The off-the-shelf proposal techniques are denoted as follows: $\\mathcal{M}$ (segment proposal), $\\mathcal{R}$ (region proposal), and $\\mathcal{S_I}$ (salient instance segmentor).\n\n| Method | Sup. | Backbone | Extra | mAP$_{25}$ | mAP$_{50}$ | mAP$_{70}$ | mAP$_{75}$ |\n|--------|------|----------|-------|------------------|------------------|------------------|------------------|\n| Mask R-CNN | $\\mathcal{F}$ | ResNet-50 | - | 76.7 | 67.9 | - | 44.9 |\n| **End-to-End weakly-supervised models.** | | | | | | | |\n| Point2Mask | $\\mathcal{P}$ | ResNet-101 | - | - | 48.4 | - | 22.8 |\n| **Multi-Stage weakly-supervised models.** | | | | | | | |\n| $\\text{BESTIE}^\u2020$ | $\\mathcal{P}$ | ResNet-101 | - | 60.8 | 52.3 | - | 30.3 |\n| $\\text{BESTIE}^\u2020$ | $\\mathcal{P}$ | HRNet-48 | - | 62.8 | 52.8 | - | 31.2 |\n| SAM | $\\mathcal{P+S}$ | ViT-S/22.1M | - | 59.4 | 39.9 | - | 19.0 |\n| **Ours** | **$\\mathcal{P}$** | ResNet-101 | - | **63.1** | **53.9** | **37.7** | **32.0** |\n| **Ours** | **$\\mathcal{P}$** | HRNet-48 | - | **66.0** | **55.6** | **40.2** | **34.4** |\n\n(2) Comparison with Point2Mask: As you rightly pointed out, we have included a comparison with Point2Mask in our original submission (Table 1). We believe this comparison showcases the strengths and unique aspects of our proposed method in the context of current advancements. Regarding AttentionShift: We carefully considered including a comparison with AttentionShift. However, we observed that AttentionShift employs a strong pre-trained model, which might not provide a fair basis for direct comparison with our approach.\n\n(3) BESTIE (with Res101) Results: We have also added the results for BESTIE with ResNet-101 in Table 1 in the manuscript. BESTIE achieved mAP50 of 52.3 on this setting.\n\n(4) COCO Test Set Results: Regarding the COCO test set results, our model achieved an AP of 17.4. We give the performance comparison in the Table 1*:\n\n**Table1\\*  Quantitative comparison of the state-of-the-art WSIS methods on MS COCO 2017 test-dev**\n\n| Method | Sup. | Extra | AP | AP$_{50}$ | AP$_{75}$ |\n| ------ | ---- | ----- | -- | --------- | --------- |\n| **COCO test-dev.** |\n| Mask R-CNN | $\\mathcal{F}$ | - | 35.7 | 58.0 | 37.8 |\n| $\\text{Fan $et\\;al.$}$ | $\\mathcal{I}$ | - | 13.7 | 25.5 | 13.5 |\n| LIID  | $\\mathcal{I}$ | $\\mathcal{M}$, $\\mathcal{S_I}$ | 16.0 | 27.1 | 16.5 |\n| $\\text{BESTIE}^\u2020$  | $\\mathcal{P}$ | - | 14.2 | 28.6 | 12.7 |\n| **Ours** | **$\\mathcal{P}$** | - | **17.4** | **33.3** | **16.4** |\n>***Q3\uff1aCompare the proposed method with the SAM***\n\n**A3:** We updated Table 1 and Table 2 in the manuscript and compared our method with SAM. Parts of Table 2 are given as follows:\n\n**Table 2 (Part)**\n| Method | Sup. | Backbone | Extra | AP | AP$_{50}$ | AP$_{75}$ |\n| ------ | ---- | -------- | ----- | -- | ---- | ---- |\n| $\\text{Point2Mask}$ | $\\mathcal{P}$ | ResNet-101 | - | 12.8 | 26.3 | 11.2 |\n| $\\text{BESTIE}^\u2020$  | $\\mathcal{P}$ | HRNet-48 | - | 14.2 | 28.4 | 22.5 |\n| $\\text{SAM}$ | $\\mathcal{P+S}$ | ViT-S/22.1M | - | **19.5** | **36.8** | 18.8 |\n| **Ours** | **$\\mathcal{P}$** | ResNet-101 | - | 15.2 | 30.1 | 24.6 |\n| **Ours** | **$\\mathcal{P}$** | HRNet-48 | - | 17.6 | 33.6 | **28.1** |\n\nAs shown in Table 1 in the manuscript, our experimental results demonstrate that our method outperforms SAM on the VOC dataset, indicating its effectiveness in handling diverse segmentation scenarios. \n\nFurthermore, we updated Table 2 in the manuscript, while evaluating the COCO dataset, we observed that SAM shows slightly higher performance in terms of AP and AP50 metrics. However, our method significantly surpasses SAM in the AP75 metric."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495728485,
                "cdate": 1700495728485,
                "tmdate": 1700495728485,
                "mdate": 1700495728485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2KstsOROgL",
                "forum": "B4vzu2aokv",
                "replyto": "Z714U4eWi6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission688/Reviewer_enoR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission688/Reviewer_enoR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response.\nMy concerns are clearly addressed.\nI raised my rating from 5 to 6.\n\nHowever, I am not particularly opposed to rejecting this paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594094726,
                "cdate": 1700594094726,
                "tmdate": 1700594094726,
                "mdate": 1700594094726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]