[
    {
        "title": "Language Models as Semantic Indexers"
    },
    {
        "review": {
            "id": "OsI288env2",
            "forum": "hJEMTDOwKx",
            "replyto": "hJEMTDOwKx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4028/Reviewer_2Nr6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4028/Reviewer_2Nr6"
            ],
            "content": {
                "summary": {
                    "value": "Semantic id is an interesting and promising topic.\nThis task reform counting id to the identifier with semantic, which will faciliate many intelligence application including e-commerce.\nThis work presents a self-supervised framework to learn semantic IDs with a generative language model.\nBy progressive learning and contrastive learning, the authors achieve sequential discrete semantic indexier.\nThe paper is well-motivated and well-written.\n\nMajor Concerns:\n1. We have not seen a full section to anlayze the semantic IDs by demonstrating all sorts of key examples.\nAs a rising domain, many readers may wonder what the semantic IDs exactly look like and how they benefit the downstream tasks.\n2. In your experiments, could you provide some human annotator evaluation for the semantic of your generated IDs?\nThis is a key experiment, though we have seen enough assessment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Semantic id is an interesting and promising topic.\nThis task reform counting id to the identifier with semantic, which will faciliate many intelligence application including e-commerce.\nThis work presents a self-supervised framework to learn semantic IDs with a generative language model.\nBy progressive learning and contrastive learning, the authors achieve sequential discrete semantic indexier.\nThe paper is well-motivated and well-written."
                },
                "weaknesses": {
                    "value": "Experimental analysis is not sufficient to support the contribution."
                },
                "questions": {
                    "value": "Major Concerns:\n1. We have not seen a full section to anlayze the semantic IDs by demonstrating all sorts of key examples.\nAs a rising domain, many readers may wonder what the semantic IDs exactly look like and how they benefit the downstream tasks.\n2. In your experiments, could you provide some human annotator evaluation for the semantic of your generated IDs?\nThis is a key experiment, though we have seen enough assessment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698640152871,
            "cdate": 1698640152871,
            "tmdate": 1699636365761,
            "mdate": 1699636365761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0lFCIkpthG",
                "forum": "hJEMTDOwKx",
                "replyto": "OsI288env2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Nr6"
                    },
                    "comment": {
                        "value": "Thank you so much for your thoughtful review!\n\nRegarding your questions:\n1. **What do semantic IDs look like?** We thank the reviewer for raising this comment. Accordingly, we add a semantic ID study section in Appendix A.7 to show what the IDs look like and what documents are assigned to those IDs. From the result, we can find that documents/products sharing similar IDs are semantically similar to each other. In addition, the first c^1 captures coarse-grained functions (e.g., Bath products) while the second c^2 will dig into more fine-grained semantics (e.g., Soaps, Shampoo). \n\n2. **Human evaluation.** We agree with the reviewer that a human evaluation can make the experiment on ID quality more comprehensive. We add a human evaluation by 1) Randomly selecting product pairs (20 pairs for each method) in the Amazon-sports dataset which share the first two IDs c^{<2}=c^1c^2. 2) Ask four trained annotators to evaluate if the two products in each pair are semantically related to each other. 3) Calculate the accuracy of each method. The results are shown below:\n\n| Model              | Accuracy |\n|--------------------|--------|\n| rq-VAE indexer     |  0.7375 | \n| HC Indexer         |  0.5375 | \n| Ours               | 0.7750 |\n\nFrom the result, our LMIndexer can outperform baseline methods by large margins. The human evaluation form can be found at https://docs.google.com/forms/d/e/1FAIpQLSfwtA4C6IY-YF4soWBWOqKMtJOso33p7DdUJic1_tE8utv5ww/viewform."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230656171,
                "cdate": 1700230656171,
                "tmdate": 1700230656171,
                "mdate": 1700230656171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h8NR14jrg7",
                "forum": "hJEMTDOwKx",
                "replyto": "OsI288env2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder"
                    },
                    "comment": {
                        "value": "We wish to express our sincere gratitude once again to the reviewers for their valuable contributions and considerate feedback. We would like to gently bring to the reviewers' attention that the interaction phase between authors and reviewers is nearing completion (within 12 hours).\n\nGiven the inclusion of the new experiments, we kindly inquire whether the reviewers might consider assigning a more favorable evaluation to our submission. Should you have any further insights to share, we are more than willing to sustain our discussion until the deadline."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703551527,
                "cdate": 1700703551527,
                "tmdate": 1700703551527,
                "mdate": 1700703551527,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ufgPDJGXSv",
            "forum": "hJEMTDOwKx",
            "replyto": "hJEMTDOwKx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4028/Reviewer_HnqK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4028/Reviewer_HnqK"
            ],
            "content": {
                "summary": {
                    "value": "Discrete semantic IDs are useful in information retrieval tasks. They are often learned by performing some sort of hierarchical clustering over off-the-shelf item representations which is not ideal as they may not be aligned with the downstream task. This paper presents an approach to learn discrete semantic IDs of items in a self-supervised manner. The proposed approach uses a transformer decoder architecture which encodes the item description and decodes it into semantic IDs which is further coupled with a small transformer model that consumes these semantic IDs, item description and tries to perform MLM task. The paper also describes and suggests ways to circumvent the challenges encountered while learning these semantic IDs."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is in general well-written and easy to follow\n- The approach is novel for learning semantic IDs in information retrieval, and the optimization challenges in such learning problems is highlighted"
                },
                "weaknesses": {
                    "value": "My major concerns with the paper are the weak evaluation and baselines, and overall the training seems to need a lot of bells and whistles to succeed.\n- DPR dual encoder is not a strong baseline; DPR is almost 10% behind SOTA dual-encoder approaches on standard benchmarks\n- Baselines in section 4.2 are weak since they are using an off-the-shelf text encoder and hence have no knowledge about the task; a very simple baseline that could be tried here is to train a dual-encoder model on this corpus and then cluster the embeddings"
                },
                "questions": {
                    "value": "- The learned semantic ID lengths seem to be very small (1-3), is this because of training instability when scaling to larger ID lengths? Do the other generative baselines also use the same ID lengths?  \n- Why not use the full MS-Marco dataset instead of the 1M sampled one?\n- How does the performance gets affected when using a more powerful reconstructor? perhaps an ablation on the number of layers in the reconstructor might be helpful here\n- Is the contrastive loss $\\mathcal{L}_{\\text{contrastive}}$ computed over all documents?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4028/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4028/Reviewer_HnqK",
                        "ICLR.cc/2024/Conference/Submission4028/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790056529,
            "cdate": 1698790056529,
            "tmdate": 1700676480438,
            "mdate": 1700676480438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SmQF5EqkQB",
                "forum": "hJEMTDOwKx",
                "replyto": "ufgPDJGXSv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HnqK"
                    },
                    "comment": {
                        "value": "Thank you so much for your thoughtful review! \n\nRegarding your questions:\n\n1. **Simple baseline in 4.2.** We thank the reviewer for proposing this baseline. According to your comment, we add experiments on Amazon datasets by first training a dual-encoder on the target corpus with contrastive learning [1] and adopting hierarchical clustering or rq-VAE to obtain semantic IDs based on the learned embeddings, denoted as rq-vae-contrastive and HC-contrastive, respectively. The results for ID quantitative study are shown below (can also be found in Appendix A.11):\n\n| Model              | Beauty | Sports | Toys   |\n|--------------------|--------|--------|--------|\n| rq-VAE-bert        | 0.2654 | 0.2774 | 0.3154 |\n| HC-bert            | 0.2428 | 0.2387 | 0.2729 |\n| rq-VAE-contrastive | 0.3100 | 0.2695 | 0.3126 |\n| HC-contrastive     | 0.2771 | 0.2622 | 0.2968 |\n| Ours               | 0.3563 | 0.4163 | 0.3536 |\n\nFrom the result, we can find that the semantic IDs generated by embeddings obtained after in-domain contrastive learning are better than those generated by embeddings from off-the-shelf text encoders. However, LMIndexer can outperform both baselines, which demonstrates its effectiveness in learning semantic IDs with self-supervision.\n\n2. **Ablation study on reconstructor.** We agree with the reviewer that adding such a study can make the experiments more comprehensive. We try reconstructors with 2 layers and 3 layers in Amazon-beauty dataset and the results are shown below (can also be found in Appendix A.12):\n\n| Model              | ID AMI |   Recommendation (R@5)  | Product Search (NDCG@5)   |\n|--------------------|--------|--------|--------|\n| Ours (recon-1)     | 0.3563 | 0.0415 | 0.3187 |\n| recon-2            | 0.2390 | 0.0284 | 0.2528 |\n| recon-3            | 0.1679 | 0.0281 | 0.2522 |\n\nFrom the result, we can find that as the reconstructor layer increases (the reconstructor becomes more powerful), the quality of the semantic indexer and its generated semantic IDs decreases. This is because more knowledge is learned inside the reconstructor rather than in the semantic indexer during self-supervised learning, which results in ID quality degeneration.\n\n\n3. **Dual encoder (DPR) baseline.** Thanks for your comments! We would like to argue this point from two perspectives: 1) *Focus*: The focus of our work is to explore a self-supervised learning method to learn semantic IDs for documents and study if the learned semantic indexer and semantic IDs can be adapted to various downstream tasks. We are happy to compare with more baseline methods but we would like to mention that our goal is not to design a model to compete on one specific task. 2) *DPR setting*: In our experiments for DPR, we are adopting the T5-base as the pretrained checkpoint rather than BERT-base in the original DPR setting (code can be found in https://anonymous.4open.science/r/ICLR24-submit-B2E7). This dual encoder setting is demonstrated to be much stronger [2]. \n\n4. **Semantic ID length.**  We illustrate from two perspectives why we set T to be 3. 1) *Performance*. As shown in Figure 5,  as T increases, the performance increase speed becomes smaller. This means that the gain of making T larger is smaller. Since a larger T will cost more time in training and inference, we adopt T=3 in our experiments. 2) *ID duplication*. As shown in Figure 7 in the Appendix, when we set T=3, the duplication issues of IDs are largely alleviated. It means that T=3 can make the ID for different documents distinguishable. \n\n5. **MS-MACRO dataset.** Thanks for your question. We sample a 1M MS-Macro document subset to evaluate our method following [3].\n\n6. **Contrastive loss.** The contrastive loss is designed to promote distinction on c^t between documents that previously shared the same c^{<t}, enabling the model to discern finer-grained hierarchical relationships between documents and alleviate ID duplication issues. The contrastive loss is computed over all documents. For each document, we serve other documents that share c^{<t} with it to be negative samples when learning c^t.\n\n\n[1] Gao, et al. Simple Contrastive Learning of Sentence Embeddings. EMNLP 2021.\n\n[2] Ni, et al. Large Dual Encoders Are Generalizable Retrievers. EMNLP 2022.\n\n[3] Pradeep, et al. How Does Generative Retrieval Scale to Millions of Passages?"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230572215,
                "cdate": 1700230572215,
                "tmdate": 1700248322057,
                "mdate": 1700248322057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2LoUYWYN1V",
                "forum": "hJEMTDOwKx",
                "replyto": "SmQF5EqkQB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4028/Reviewer_HnqK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4028/Reviewer_HnqK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response, it cleared some of my doubts but I am still not convinced about the effectiveness of the proposed approach. I think it's an interesting approach but there is not enough evidence to support its usefulness on downstream tasks. I am updating my score to 5 in light of the author's further clarifications."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676449298,
                "cdate": 1700676449298,
                "tmdate": 1700676449298,
                "mdate": 1700676449298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qBhsyYlNqD",
            "forum": "hJEMTDOwKx",
            "replyto": "hJEMTDOwKx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4028/Reviewer_dU5G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4028/Reviewer_dU5G"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose LMIndexer, a self-supervised method to learn the semantic IDs of documents using sequence-to-sequence models. A semantic ID of a document is a sequence of integers that are indexes/row numbers of a codebook embedding matrix. Three loss functions are designed and used to train the sequence-to-sequence model: a reconstruction loss, a contrastive loss, and a commitment loss. The proposed method is evaluated on three downstream tasks: sequential recommendation, product search, and document retrieval. The results show good improvement over some SOTA methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe proposed method formulates the semantic ID learning problem as a sequence-to-sequence learning method, which is novel according to related work discussed in the paper.\n-\tTechnical challenges are described clearly.\n-\tSOTA techniques are used in the proposed framework.\n-\tThe experimental results show that the proposed method outperforms some SOTA methods in the three downstream tasks."
                },
                "weaknesses": {
                    "value": "-\tThe paper says that the proposed method \"learns the document\u2019s discrete semantic embeddings and its hierarchical structure simultaneously\". But it is not clear what the authors mean by the hierarchical structure of a document, how the proposed method is guaranteed to learn such a structure, and whether the proposed method actually learns such a structure. \n\n-\tThe size of the semantic ID (T) is set to less than or equal to 3 in the experiments, which is surprisingly small. Figure 5 shows the performance increases with T. Why not trying bigger T values? Would an ID with length 3 be informative enough?\n\n-   How should the codebook size be determined? It seems that only three sizes are tried in the experiments. How does the size of the codebook affect the performance?\t\n\n-  The performance metric used in ID quantitative study, AMI (in Table 1), is not defined or explained. \n\n-   Not clear what the word clouds picture (Figure 3) is trying to show? The text explaining it is confusing. What do you mean by \u201ctwo semantic ID prefixes\u201d? Are the two prefixes from the same generated ID or two different IDs? \n\n-   It would be better if authors provided other performance metrics such as latency for the retrieval in comparison with baselines.\n\n-  The authors mentioned the model could be fine-tuned on downstream tasks such as retrieval or recommendation tasks. Fine-tuning would change the model weights and then the previously generated semantic IDs would be changed as well, which may affect the ground-truth ID used in fine-tuning. Does it cause any problem?"
                },
                "questions": {
                    "value": "Please see the questions in the above section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818037028,
            "cdate": 1698818037028,
            "tmdate": 1699636365587,
            "mdate": 1699636365587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "91HwE8csdB",
                "forum": "hJEMTDOwKx",
                "replyto": "qBhsyYlNqD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dU5G"
                    },
                    "comment": {
                        "value": "Thank you so much for your thoughtful review! \n\nRegarding your questions:\n1. **Hierarchical structure.** 1) *Meaning*: Learning the hierarchical structure of documents refers to understanding documents from different semantic granularity and reflecting them in their semantic IDs (The preliminary set of IDs should predominantly encapsulate coarse-grained semantics, with successive IDs delving deeper into nuanced specifics). A more detailed illustration is added in Appendix A.7. 2) *Techniques*: The LMIndexer is proposed to learn the semantics into the IDs with self-supervised sequential discrete auto-reconstruction and capture the different granularity of semantics into their IDs with progressive training and contrastive objective. 3) *Results*: We show the learned hierarchy in Figure 9 and Figure 10 in the Appendix, where we can find that the products sharing the same first ID have a similar coarse-grained function (e.g., Bath products), while products sharing the first and second ID have related fine-grained functions (e.g., Soaps, Shampoo). \n\n2. **Size of the semantic ID (T).** We illustrate from two perspectives why we set T to be 3. 1) Performance. As shown in Figure 5, the performance increases speed as T increase becomes smaller. This means that the gain of making T larger is smaller. Since a larger T will cost more time in training and inference, we adopt T=3 in our experiments. 2) ID duplication. As shown in Figure 7 in the Appendix, when we set T=3, the duplication issues of IDs are largely alleviated. It means that T=3 can make the ID for different documents distinguishable. \n\n3. **Codebook size.** The codebook size is set as a hyperparameter in our model design. We agree with the reviewer that showing the study of codebook size can make our research more comprehensive. The results are shown below (added in Appendix A.8):\n\n| Model              |   Recommendation (R@5)  |   Product Search (NDCG@5)   |\n|--------------------|--------|--------|\n| 128            | 0.0291 | 0.3071 |\n| 256            | 0.0355 | 0.3089 |\n| Ours (512)     | 0.0415 | 0.3187 |\n\nFrom the result, we can find that the downstream task performance generally increases as codebook size increases. It is intuitive since the larger the codebooks are (before overfitting), the more information they can represent. \n\n4. **Definition of AMI.** The Adjusted Mutual Information (AMI) score is a measure used in statistics and information theory to quantify the agreement between two clusters (in our experiments, the two clusters refer to ground truth category clusters and Semantic ID clusters) while correcting for chance. It is an adjustment of the Mutual Information (MI) score that accounts for the fact that MI is generally higher for clusters with a larger number of clusters, thus providing a normalized score that is more comparable across different clusters. We have added the definition of AMI in Appendix A.9 according to the reviewer's suggestion.\n\n5. **Word clouds.** The word clouds are shown for the semantic ID qualitative study. \u201cTwo semantic ID prefixes\u201d refers to two different c^{<=2}=c^1c^2 (e.g., ID (0,0,*) and ID (1,2,*)). We try to print out the word clouds to summarize what documents are sharing IDs (0,0,*) and what documents are sharing IDs (1,2,*) respectively. We can find that the two groups of documents are related to beach/sand set toys (corresponding to ID (0,0,*)), and star war toys (corresponding to ID (1,2,*)) respectively. This demonstrates that the IDs acquired through LMIndexer convey a higher degree of semantic relevance (enabling similar documents to have similar semantic IDs). We have added a more comprehensive ID qualitative study section in Appendix A.7.\n\n6. **Latency analysis.** We conduct latency analysis on baseline methods and LMIndex on Amazon-beauty dataset. We measure the total latency of product search on the whole Amazon-beatuy test set. The results are shown below (details can be found in Appendix A.10):\n\n| Model              | Beauty |\n|--------------------|--------|\n| rq-VAE indexer     |  13.66s | \n| HC indexer         |  12.85s | \n| SEAL               |  21min |\n| Ours               | 12.21s |\n\nFrom the result, the inference latency of our method is comparable with the rq-VAE indexer and HC indexer and is much smaller than SEAL.\n\n7. **Semantic ID & Fine-tuning.** This is a great point. After the self-supervised semantic ID learning, we have a semantic indexer that can capture document semantics into their IDs. The semantic IDs for all documents will be generated with this semantic indexer and will not be updated in downstream tasks. During downstream task fine-tuning, only the parameters inside the LMIndexer are updated (semantic IDs for documents are fixed), where the model tries to learn the mapping between new types of input text (e.g., user history in recommendation and query in retrieval) to fixed document semantic IDs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230395304,
                "cdate": 1700230395304,
                "tmdate": 1700230689321,
                "mdate": 1700230689321,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9sFcTmQsdu",
                "forum": "hJEMTDOwKx",
                "replyto": "qBhsyYlNqD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind reminder"
                    },
                    "comment": {
                        "value": "We wish to express our sincere gratitude once again to the reviewers for their valuable contributions and considerate feedback. We would like to gently bring to the reviewers' attention that the interaction phase between authors and reviewers is nearing completion (within 12 hours).\n\nGiven the inclusion of the new experiments and further explanation of our methods, we kindly inquire whether the reviewers might consider assigning a more favorable evaluation to our submission. Should you have any further insights to share, we are more than willing to sustain our discussion until the deadline."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703520847,
                "cdate": 1700703520847,
                "tmdate": 1700703520847,
                "mdate": 1700703520847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YFGCEvEzDb",
            "forum": "hJEMTDOwKx",
            "replyto": "hJEMTDOwKx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4028/Reviewer_4DWK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4028/Reviewer_4DWK"
            ],
            "content": {
                "summary": {
                    "value": "This paper formulates the problem of learning semantic IDs by simultaneously capturing the document's semantic representations and its hierarchical structure. It introduces an innovative self-supervised approach designed to acquire semantic IDs directly from the input document using a generative language model. Experimental results on five datasets from various domains demonstrate that the proposed method consistently outperforms competitive baselines by a significant margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper presents the \"LMINDEXER\" approach as a solution to the challenges inherent in generating semantic IDs from textual data. The approach is carefully crafted to capture both the semantic representations and hierarchical structure of documents simultaneously.\n\n2. The paper demonstrates the effectiveness of the LMINDEXER approach through empirical evidence gathered from experiments on three distinct downstream tasks, utilizing data from diverse domains.\n\n3. The paper exhibits a well-organized structure and offers an easily digestible reading experience."
                },
                "weaknesses": {
                    "value": "1. While this paper presents an approach termed LMINDEXER, it's important to note that the novelty of the method is somewhat limited. Additionally, the paper lacks a comprehensive discussion of related work, including notable prior efforts that have explored the use of encoders for text encoding and decoders for reconstruction in the context of information retrieval. Several works, such as [1], [2], and [3], have examined similar techniques and deserve acknowledgment for their contributions to the field.\n\n[1] Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder (EMNLP 2021)\n\n[2] A Contrastive Pre-training Approach to Learn Discriminative Autoencoder for Dense Retrieval (CIKM 2022)\n\n[3] RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder (EMNLP 2022)\n\n\n2. In the context of document retrieval, it would be beneficial to broaden the comparison to include more generative retrieval baselines. For instance, evaluating how the LMINDEXER approach compares to FM-index-based SEAL or other recent generative retrieval methods would provide a more comprehensive understanding of its performance. Focusing solely on comparisons with DSI, which may be considered a relatively weaker baseline, might not offer a complete picture of the method's capabilities.\n\n3. While the proposed self-supervised approach has the potential to address the \"new document problem\" by automatically acquiring semantic IDs, it would be valuable to see experimental results that explicitly address this issue. Incorporating experiments that involve adding new documents to the model and assessing its adaptability and performance in such scenarios would provide a more robust evaluation.\n\n4. An important consideration when using automatically generated semantic IDs is the possibility of duplication. It is crucial to include a discussion or explanation of how the LMINDEXER approach handles or mitigates this potential issue. A detailed exploration of the approach's robustness in preventing or addressing duplication would enhance the paper's completeness and practicality."
                },
                "questions": {
                    "value": "1. The authors should consider expanding their related work section to include the most recent developments in generative Information Retrieval (IR) techniques. Additionally, they should include a comparative analysis of these recent works alongside the proposed LMINDEXER method in the experimental section. This would provide a more comprehensive overview of how LMINDEXER stacks up against the state-of-the-art in generative IR.\n\n2. It is essential for the authors to address the issue of handling new documents within the LMIndexer framework. The paper should discuss how this framework manages the addition of new documents, what mechanisms or strategies are employed, and the performance of the LMIndexer approach in comparison to baseline methods when confronted with this \"new document\" scenario. This analysis would help assess the adaptability and robustness of the approach.\n\n3. To ensure that distinct semantic IDs are assigned to different documents, the paper should provide detailed explanations and discussions regarding the mechanisms and safeguards in place within the LMIndexer framework. Experimental results or case studies showcasing how the system maintains distinct semantic IDs for various documents would add substantial value to the paper, reinforcing its practicality and effectiveness in preventing semantic ID duplication."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824512477,
            "cdate": 1698824512477,
            "tmdate": 1699636365510,
            "mdate": 1699636365510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tOho4ZdJhi",
                "forum": "hJEMTDOwKx",
                "replyto": "YFGCEvEzDb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4DWK"
                    },
                    "comment": {
                        "value": "Thank you so much for your thoughtful review!\n\nRegarding your questions:\n1. **More related works.** We thank the reviewer for pointing out the related papers and we are happy to add them to the related work section (added to the \u201cSelf-supervised Learning with Language Models\u201d section). We would like to emphasize the novelty of LMIndexer compared with these works: 1) *Focus*: LMIndex mainly focuses on learning semantic IDs via self-supervised learning, which is different from existing works\u2019 focus on pretraining a text encoder for dense retrieval. 2) *Techniques*: Since semantic IDs lie in a discrete and sequential fashion which is different from a soft embedding for dense retrieval, we propose a new sequential discrete auto-reconstruction pipeline and introduce solutions for challenges including Reconstructor collapse, Posterior collapse, and ID diversity. 3) *Task*: We explore LMIndex on product recommendation, product retrieval, and document retrieval and existing works mainly focus on document retrieval.\n\n2. **Comparison with SEAL.** We thank the reviewer for pointing out the baseline. We add their results on Amazon product search datasets and NQ320k (code to reproduce is uploaded to https://anonymous.4open.science/r/ICLR24-submit-B2E7/):\n\n| Model              | Beauty | Sports | Toys   |  NQ320k |  \n|--------------------|--------|--------|--------|--------|\n| rq-VAE indexer        |  0.2710 |  0.2606 | 0.2511 |  0.6480  |\n| HC indexer     |  0.2172 | 0.1979 |  0.2379 |   0.6439 |\n| SEAL               | 0.1271 | 0.2011 | 0.1035 |   0.5698 |\n| Ours               | 0.3187 |  0.2870 | 0.2865 |   0.6631|\n\nWe also add the experimental results in Appendix A.5 in our paper. From the result, we can find that our method outperforms SEAL significantly. The main reason is that after the self-supervised semantic ID learning, LMIndexer can generate higher quality semantic ID as identifiers for documents than Ngrams used in SEAL.\n\n3. **New document problem.** We conduct experiments to study this problem in Section 5.2 (Table 4). After the semantic indexer is finetuned on the downstream product search task, we add 100 new documents into the corpus. We use the semantic indexers before fine-tuning on the downstream task (including rq-VAE indexer, HC indexer, and LMIndexer) to give semantic IDs for new documents and test how the fine-tuned semantic indexers (on the downstream task) can generalize to the new documents. The results are shown in Table 4, where LMIndexer outperforms the baselines significantly. The reason is that LMIndexer is trained to achieve semantic IDs on a large corpus with self-supervision. This enables the LMIndexer to learn the projection between text semantics and semantic IDs, which can be generalized to new documents and applied to downstream tasks. We will make this part more clear in our revision.\n\n4. **ID Duplication.** This is a great point. We add a section to discuss this issue in Appendix A.6. We agree with the reviewer that the duplication issue is very important here. To alleviate this issue, we propose a contrastive objective in Section 3.2 to promote distinction between documents that previously shared the same learned ID and encourage them to obtain different IDs for the next position (alleviate duplication). The effectiveness of this design is shown in Figure 8 in Appendix. We can find that during self-supervised learning, if the contrastive objective is added, the difference ratio on the next ID position of documents sharing learned ID is bigger and the diversity (perplexity) of IDs on the next position is larger, which means that the duplication issue is alleviated. We also show in Figure 7 in the Appendix that the semantic IDs learned by LMIndexer are quite distinguishable. While it is nearly impossible to guarantee zero duplication since there can be documents that have very similar semantics, we simply add another final ID position to distinguish them."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230145864,
                "cdate": 1700230145864,
                "tmdate": 1700230145864,
                "mdate": 1700230145864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hmMsVR3JxY",
                "forum": "hJEMTDOwKx",
                "replyto": "tOho4ZdJhi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4028/Reviewer_4DWK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4028/Reviewer_4DWK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors and the very detailed reply. \n\nAs the extended experiments on new-document and id-duplicate problems, I can confirm that these two problems exist in the proposed method. For example, the performance of the new-document setting is only 0.0455 on Recall@50 compared to the original 0.3187 on NDCG@5. Furthermore, the duplicate ID problem is significant, because 20% or more documents have the same IDs. I am not sure because the Figure 7 is over-smoothed. Finally, I think the performance of SEAL needs to be verified because I think it at least outperforms the DSI model."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574927747,
                "cdate": 1700574927747,
                "tmdate": 1700574927747,
                "mdate": 1700574927747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]