[
    {
        "title": "T-MARS: Improving Visual Representations by Circumventing Text Feature Learning"
    },
    {
        "review": {
            "id": "k9kaolMxDH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6128/Reviewer_NY9s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6128/Reviewer_NY9s"
            ],
            "forum": "ViPtjIVzUw",
            "replyto": "ViPtjIVzUw",
            "content": {
                "summary": {
                    "value": "This paper introduces T-MARS (Text Masking and Re-Scoring), a data-filtering method for curating image-text datasets. The method is built on the observation that a large portion of web-crawled image-text datasets such as LAION contain text that overlap significantly with the image (and often lack visual representations of what the text refers to). Intuitively, such datapoints could encourage models to prioritize learning OCR over learning the visual contents of the images. Their method, T-MARS attempt to filter such samples. Their experiments show strong empirical results, improving over strong baselines and competing methods in DataComp by a large margin. Overall, their method is simple, scalable and effective."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "There are many strengths to this paper.\n\n1. Firstly, understanding how to better design datasets is an important and timely problem, and many open problems remain. This paper presents a significant step forward in that direction. As such, I believe this paper would be of interest to many in the community and will have substantial impact for practitioners interested in building better multimodal models.\n2. The proposed method is simple and novel, and can be easily applied to any data curation pipeline.\n3. As shown by the authors, the method also scales well.\n4. The experimental results are very strong, providing large gains in accuracy over strong baselines and existing data curation methods.\n5. The paper is very clear and well written."
                },
                "weaknesses": {
                    "value": "The main weakness I see in this paper is the lack of large scale experiments. However, I do not think this should count against the authors, for a few reasons. Firstly, running large-scale CLIP pre-training experiments can be prohibitively expensive for many institutions. Secondly, the authors present clear scaling trends that show that their approach holds great promise for larger scales."
                },
                "questions": {
                    "value": "1. In Section 4.1, why exactly 50% of the pool is filtered?\n2. I'm sometimes a bit confused by the choice of downstream evaluation tasks. In particular, the DataComp defines a clear set of 38 downstream tasks, yet the authors evaluate on only subsets of these tasks (and often not even the same subsets, e.g. table 1 is on 17 datasets, Table 2 doesn't show the average over the 38, and in Section 5.3 it says 23 datasets are used). Why the inconsistencies?  \n3. Why are some of the stronger baselines (including ones from the DataComp paper) not present in some tables?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6128/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697311023594,
            "cdate": 1697311023594,
            "tmdate": 1699636663611,
            "mdate": 1699636663611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cKgsYfF5MU",
                "forum": "ViPtjIVzUw",
                "replyto": "k9kaolMxDH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your time, and valuable feedback, acknowledging \u201cvery strong results\u201d and \u201cpromising scaling trends\u201d. We are glad to see your recognition of our pilot study as a \u201csignificant step towards understanding multimodal datasets\u201d.  Below we provide clarifications to the questions raised. \n\n### **Why is exactly 50% of the data filtered** \nWhile designing the approach and experimenting on the LAION-400M dataset, we tried three choices for the amount of data to be filtered\u2014 40%, 50%, and 75%, with 50% working out to be the best. Note that these experiments further filtered an already filtered LAION dataset. One can definitely vary this and use other filtering metrics, say based on a threshold CLIP score after text masking. For example, on DataComp (filtering on the common crawl),  we use a CLIP-similarity threshold of 0.281, which is similar to that used in prior work, such as LAION filtering. Fine-tuning the threshold may bring further gains, but running such parameter searches is beyond our compute scope. We have clarified this in the updated manuscript as well. \n\n### **Downstream Datasets** \nWe apologize for the oversight on our part and have updated Table 2 (Datacomp results) with an evaluation on all 38 downstream tasks. We have fixed the typo on the number of datasets in Table 1 as well. Our experiments in Table 1 were on quite a small scale initially, due to which we did not evaluate on some tasks like WILDS in Datacomp on which all models get random performance.\n\n### **Some missing baseline from DataComp paper**\nThe DataComp paper also considered an \u2018image-based\u2019 filtering approach that uses information from the Imagenet dataset (ImageNet $\\cap$ CLIP Score). T-MARS outperforms this baseline as well, but we do not list it because we wanted to only consider methods that do not use any information of the downstream evaluation dataset to provide for a fair evaluation between all baselines.\n\n---\n\nWe again thank you for your time and hope that we have addressed the concerns. We are happy to provide any further clarifications if requested."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500651032,
                "cdate": 1700500651032,
                "tmdate": 1700500664352,
                "mdate": 1700500664352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HlV5XRkTAX",
                "forum": "ViPtjIVzUw",
                "replyto": "cKgsYfF5MU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Reviewer_NY9s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Reviewer_NY9s"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my comments. I have read the other reviews and the responses from the authors and stick to my original score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579762572,
                "cdate": 1700579762572,
                "tmdate": 1700579762572,
                "mdate": 1700579762572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wRJNB48kog",
            "forum": "ViPtjIVzUw",
            "replyto": "ViPtjIVzUw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6128/Reviewer_3vr7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6128/Reviewer_3vr7"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel data filtering method named T-MARS (Text Masking and Re-Scoring) tailored for extensive image-text datasets.\nAn analysis of the LAION dataset revealed that 40% of the images have text overlapping with the associated caption, leading models to rely on OCR instead of learning from visual features which is the motivation for building T-MARS.\nThe proposed methodology involves detecting text within images, masking it out, and subsequently re-scoring the image in relation to the caption using the CLIP model. Images that score low are discarded, ensuring the retention of images where visual features are still closely correlated with the text."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The overall idea it's straightforward and easy to understand.\n\nThe paper shows good empirical results. When using the proposed method to filter out data there is an increase in accuracy.\n\nThe filtering method was applied on the LAION dataset and the trained models on the newly curated dataset are tested on a decent amount of downstream tasks.\n\nThe paper findings are in line with other works (such as [1]) that show that data quality is important.\n\n[1] Gunasekar, Suriya, et al. \"Textbooks Are All You Need.\" arXiv preprint arXiv:2306.11644 (2023)."
                },
                "weaknesses": {
                    "value": "The motivation for the work is somehow weak and it lacks theoretical analysis of why text-only images degrade visual learning compared to mislabeled data.\n\nChapter 3: manually analyzes 500 sample images from the LAION dataset to categorize them based on the correlation between image features (text or visual) and the caption. It lacks some metrics to quantify how representative the 500 sample is of the whole dataset. I appreciate that additional details are given in the appendix, however the work would benefit for more experiments, more details and more analytics. For example, take a larger random sample with statistical estimates of error bars on proportions.\n\nChapter 6: it is very hard to follow. A rewriting of it to better present the experiments would be beneficial. \n\nThe whole method relies on CLIP score for filtering, which can be noisy and introduce additional biases. The current version of the paper is not tackling this."
                },
                "questions": {
                    "value": "What happens if the model is trained with the masked images? So instead of discarding them, you train the model with the masked images.\n\nAre there other datasets that might benefit from the proposed method? (maybe CC12M [1])\n\n\n[1] Changpinyo, Soravit, et al. \"Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[After rebuttal] The authors addressed my concerns especially regarding mislabeled data vs text-only images. Thus I will raise my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6128/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6128/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6128/Reviewer_3vr7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6128/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787142784,
            "cdate": 1698787142784,
            "tmdate": 1700681233279,
            "mdate": 1700681233279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q5gVTAFYbL",
                "forum": "ViPtjIVzUw",
                "replyto": "wRJNB48kog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and valuable feedback. We are happy to see that you appreciated the simplicity of our approach and the promising empirical results on a variety of tasks. We address all of your concerns below:\n\n### **Why text-only images hurt, but mislabeled data does not?** \nFirst, we want to clarify that the CLIP similarity metric is already good (not perfect) at capturing mislabeled data. In fact, most image-caption pairs below a CLIP score of 0.2 are already mislabeled or have random captions. They continue to have a low CLIP score even post-masking and are hence still removed by T-MARS. The natural CLIP score does not, however, help remove text-dominant image-caption pairs. As seen in our synthetic experiment in Section 6, mislabeled examples and text-dominant image-caption pairs hurt visual representations almost equally which supports your insight. \n\nHowever, we also highlight that in Section 4.2, we proposed two competitive data filtering baselines, C-RHO and C-SSFT, that filter out the mislabeled and ambiguous examples. These baselines have been proposed by drawing insights from the long line of work around mislabeled and hard example mining in the supervised classification regime. Further, as shown in Table 1 and Table 2 (see T-MARS $\\cap$ C-SSFT row), taking an intersection of data filtered by T-MARS (text-dominated images) and the proposed baselines (say C-SSFT, which removes additional mislabeled samples that CLIP score didn\u2019t capture) gives additive gains. This highlights the importance of filtering out both the text-dominated and mislabeled samples, as you rightly suggested.\n\n### **Larger pilot study beyond 500 images** \nWe agree with the reviewer that a larger scale extension of our pilot study will definitely be helpful for the community. In Appendix C.2, we talk about statistical estimates of error rates for the estimated proportions. Assuming that the sampling is unbiased, 500 samples are enough to estimate the proportions with a 5% margin of error (95% confidence).\nHowever, to verify whether the sampling itself is representative of the population (the unbiased assumption above), we performed a similar pilot study over 500 additional examples (once again randomly taken from the LAION dataset). The table below gives the estimated and the actual count of data points from the various categories.\n\n|           | Category 1 | Category 2 | Category 3 | Category 4 | Category 5 |\n| --------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| Estimated (based on previous study) | 25         | 225        | 50         | 100        | 100        |\n| Actual (new 500 examples study)   | 24         | 232        | 53         | 96         | 95         |\n\n\n### **Will there be gains on CC12M** \nCC12M is a small and already well-curated dataset. Hence we do not think that there will be significant gains using T-MARS or any other baselines for data filtering here.\nHowever, note that such small datasets cannot be used for training large-scale state-of-the-art CLIP models, which use noisy webscale image-caption datasets like Common Crawl. Our work focuses on building scalable and efficient curation approaches for these webscale datasets.\n\n### **Training on masked images** \n This is indeed an interesting question. We chose not to train on masked images to avoid distribution shifts and loss in performance potentially due to the aberrations introduced by masking (although they are indeed quite negligible, see Appendix E). Moreover, there is no reason to train on masked images from Category 5 (text only) because they will be data points where the text and the (now masked) input are uncorrelated. As far as Category 3 is concerned, the uncorrelated texts are already ignored by the model (as also seen in Section 6). Finally, let us come to the most important category in question (Category 4 with both text and visual features). Currently, we retain the unmasked input in such categories (that have both text and visual features). One proposal would be to only retain the visual features in these (as you suggest). We believe that keeping the text features can be helpful for retaining an understanding of certain visual features as well, such as brands and trademark logos (such as the Superman logo) which can be in-part textual, but also have a distinct visual pattern. Our masking algorithm will mask these patterns and potentially hinder the model\u2019s knowledge.\n\n### **Section 6**\nWe acknowledge your feedback about the difficulty of parsing Section 6. **We have rewritten the same** by focussing on the insights and results in the main paper and moving details to the Appendix that were previously hindering clarity in the interest of brevity. We hope the updated version reads better.\n\n---\n\nWe again thank you for raising these interesting questions and hope that the clarification helps you assess the work more positively. We are happy to provide any further clarifications if requested."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500065990,
                "cdate": 1700500065990,
                "tmdate": 1700501978272,
                "mdate": 1700501978272,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jVb8ocUF7r",
                "forum": "ViPtjIVzUw",
                "replyto": "wRJNB48kog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, \n\nWe worked on addressing all of your concerns in the response above. We hope to hear back from you. As a quick summary, in our rebuttal:\n\n- we highlighted that T-MARS already removes most of the mislabeled images and the paper also proposed 2 baselines focused on filtering out mislabeled images. Interestingly, T-MARS + proposed baselines give additive gains, highlighting the importance of removing mislabeled samples, as you also guessed.\n\n- we conducted an additional pilot study, to show that our estimates for different data type proportions are indeed within the error range estimated statistically (i.e. 2-3%).\n\n- we have rewritten Section 6 (on estimating the utility of each data type), focusing on the insights and key takeaways, as requested.\n\nWe believe that these points address all the concerns raised and please let us know if there is anything else that you are still concerned about."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619307307,
                "cdate": 1700619307307,
                "tmdate": 1700621207455,
                "mdate": 1700621207455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bQO4djJ1bE",
                "forum": "ViPtjIVzUw",
                "replyto": "wRJNB48kog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,    \nSince today is the last day of the author-reviewer discussion period, we kindly request you to please take a look at the rebuttal above, which we believe addresses all the concerns raised by you."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680817018,
                "cdate": 1700680817018,
                "tmdate": 1700680831969,
                "mdate": 1700680831969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f87wad7zNe",
                "forum": "ViPtjIVzUw",
                "replyto": "bQO4djJ1bE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Reviewer_3vr7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Reviewer_3vr7"
                ],
                "content": {
                    "comment": {
                        "value": "The rebuttal clarifies my concerns, thus I raised my score. After reading the rebuttal it's a lot more clear that the mislabeled data is also filtered. Please consider slightly re-writing the paper to clearly state that (even though it might feel like stating the obvious)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681453141,
                "cdate": 1700681453141,
                "tmdate": 1700681453141,
                "mdate": 1700681453141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NntCibpMTS",
                "forum": "ViPtjIVzUw",
                "replyto": "wRJNB48kog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for going through the rebuttal and raising the score. We will definitely work on incorporating the above suggestion about mislabeled examples in the manuscript. \n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697580206,
                "cdate": 1700697580206,
                "tmdate": 1700697619078,
                "mdate": 1700697619078,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IgcQs2YeHx",
            "forum": "ViPtjIVzUw",
            "replyto": "ViPtjIVzUw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6128/Reviewer_hNGm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6128/Reviewer_hNGm"
            ],
            "content": {
                "summary": {
                    "value": "## Summary\n\nThis paper aims to improve visual representations via a proposed data filtering approach. It is based on an observation that about 40% images contain overlapped text. The experimental results show the performance of the proposed method to some ext"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "## Strengths\n1. The motivation of this paper sounds reasonable.\n\n2. Some experimental results look good."
                },
                "weaknesses": {
                    "value": "## Weaknesses\n1. The writing of this paper is somewhat obscure, resulting in that it is some difficult to follow this paper.\n\n2. Is it possible to directly remove all the text in the images? This may avoid the distractions of the text.\n\n3. It would be better to conduct experiments on more datasets, except for LAION."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6128/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698978635006,
            "cdate": 1698978635006,
            "tmdate": 1699636663381,
            "mdate": 1699636663381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OOhtWyVaau",
                "forum": "ViPtjIVzUw",
                "replyto": "IgcQs2YeHx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and valuable inputs. We provide clarifications for the concerns raised in your review:\n\n### **Directly remove all the text in the images** \nThe first step of TMARS, i.e. text-detection can indeed be seen as trying to identify and remove all the texts in the images. However, note that TMARS then filters out the images with low CLIP score after masking. \nThis is because, as outlined in our pilot study (Section 3), a significant number of images primarily contain text as the sole feature correlating with the caption.  It would be suboptimal (waste of compute) to retain these images and spend training compute on them, as after masking, they do not have any visual features (correlated with caption) that could possibly help the model to learn better visual representations.\n\n### **Experiments on more datasets** \nWe point to Table 2, where we also show results on the DataComp, a latest benchmark for multimodal data filtering. For example, on the medium scale of datacomp, TMARS observes around $6.5\\%$ gains on ImageNet zeroshot accuracy compared to CLIP filtering and $4.4\\%$ compared to TextMatch[1]. The goal of our work is to improve training on noisy, web-crawled datasets, and in that regard, we have shown effectiveness on both, filtering the common crawl, and pre-filtered LAION datasets. The common crawl is pretty much all the data on the web, and captures any subset of data that may be created out of it. We hope this helps acknowledge the significance of our results.\n\n\n### **Writing**\n**We have re-written** parts of the paper that we felt might have been hard to follow. This especially included a re-write of Section 6. If you have any particular comments on any other parts of the paper, we would be happy to make amends. Thanks!\n\n---\n\n[1] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distillation, and hard negatives for vision-language pre-training. arXiv:2301.02280, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501291976,
                "cdate": 1700501291976,
                "tmdate": 1700501291976,
                "mdate": 1700501291976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uahwSKEbwM",
            "forum": "ViPtjIVzUw",
            "replyto": "ViPtjIVzUw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6128/Reviewer_ZjEa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6128/Reviewer_ZjEa"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims at filtering out irrelevant data based on text masking and re-scoring to help the learning of visual features for zero- and few-shot image recognition. The proposed method is simple and can improve the zero-shot performance by only modifying the subset of data and evaluating multiple tasks to show no bias issue in the filtered subset. In addition, the experimental results show that the proposed method brings promising high-quality data curation for data filtering."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Data cleaning is an important topic in the deep learning field. The proposed filtering data method shows the observation that nearly 40% of LAION\u2019s images contain text overlapping the caption and then designs a method to eliminate the noise for the data filtering. \n2. Instead of simply adding or removing data, the authors mask out the text in an image and restore the text regions by replacing the region with the average color of the surrounding pixels. Then, the similarity score between the image and the caption is calculated in order to filter out the low-score images. \n3. The proposed method is evaluated on multiple baselines ranging from 2 million to 128 million to demonstrate robustness."
                },
                "weaknesses": {
                    "value": "1. Even though the proposed method has been evaluated on multiple datasets and various tasks, the metric is only the accuracy, which may be narrow and bias may exist for other metrics. \n2. The image's text overlaps with the caption which may not be helpful for learning visual features, a subsection for the discussion with multiple ways to resolve the issue can help researchers get more insight into this topic instead of simply exploiting the masking technique."
                },
                "questions": {
                    "value": "1. The proposed filtering method is reasonable, but can this method be used for all different tasks with only one metric, i.e., accuracy? Is that possible that the method filtered some salient signals but isn't shown in this paper due to the single metric?\n2. When masking out the text of an image, Will the inpaint technique alter the original data? If it's not an important issue, is that possible to leverage the power of generative models for it? \n3. It'll be good if the authors provide the distribution scores (cosine similarity) before and after filtering out the data that can help understand the distribution of the good/bad data. \n4. In this paper, the proposed method removes the text of an image, will it be different if using different percent of the masking? \n5. After filtering out the data, is data augmentation used in the experiments? Will it provide a more significant improvement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6128/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6128/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6128/Reviewer_ZjEa"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6128/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699201178374,
            "cdate": 1699201178374,
            "tmdate": 1699636663231,
            "mdate": 1699636663231,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gUfgiSqLAJ",
                "forum": "ViPtjIVzUw",
                "replyto": "uahwSKEbwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6128/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and valuable feedback. We are happy to note that you appreciated the simplicity of our method, supported by promising results. We provide clarifications to your questions below:\n\n### **Accuracy as the only metric** \nSorry for not being precise about this, but we follow the standard evaluation metrics as used in the DataComp benchmark (https://github.com/mlfoundations/datacomp/blob/main/tasklist.yml). For example, on retrieval tasks (MSCOCO and Flickr) the metric is mean_recall@1 and on the WinoGAViL dataset it is jaccard_score. Further, we have updated Table 2 in the paper with an additional column of average performance across all the datasets in DataComp, which involves F1 score as the metric for some of the class imbalanced datasets like wilds-iWILDCam and worst region accuracy as the metric for wilds-FMoW. \n\n\n### **Distribution of scores before and after masking**\nThank you for the suggestion. Recall our 500-example pilot study where we hand-labeled examples into various categories, like those with only visual features or those with only text features. We calculated the CLIP similarity score of these image-caption pairs, before and after masking the text in the respective images. **A histogram of this distribution of scores (before and after masking) is now uploaded in Appendix F**. Our observations indicate that after masking, the CLIP score of images with just text features (OCR) drops significantly (indicating they will be filtered out by T-MARS). At the same time, CLIP scores of images with only visual features stay nearly the same. This demonstrates the efficacy of the filtering method for removing samples with only text features.\n\n### **Data augmentation after filtering** \nWe do not perform any additional data augmentation after filtering out the bad data to ensure a fair comparison with the baselines. We follow the standard and widely used CLIP training implementation given here[https://github.com/mlfoundations/open_clip]. Indeed, data augmentation may provide additional improvements in visual representation, but our work aims to improve representations by intervening on the data, not the training process.\n\n### **Does inpainting alter original data?**\nIf the text overlays a patch with significant visual features, inpainting might create some aberrations. However, in general, we observe that the text detection algorithm (FAST[3]) gives very tight bounding boxes around the text, leading to no major aberrations of visual features when inpainting the text area, serving our main purpose in this work well. In Appendix E, we have added some pairs of original and masked images to highlight the same. \n\nAdditionally, the histogram of CLIP score distribution in Appendix F (which you suggested) provides evidence that the method works well at masking out text features, thereby, lowering CLIP scores of only the images with just text features. However, we agree with you that exploring more nuanced ways to remove the text from the image is an interesting direction for future work. \n\n### **Impact of percent of Masking**\nTo decide the threshold of image-caption pairs to be removed post-masking, we use a CLIP-similarity threshold of 0.281, which is the same as that used in prior work, such as LAION filtering. Fine-tuning the threshold may indeed bring further gains, but running such parameter searches is beyond our compute scope. \n### **New Discussion: Appendix G (Other ways to resolve text overlap with caption)**\nWe have added a discussion section on other ways to circumvent text feature learning in Appendix G  (Page 26 and Page 27), as requested by you.  We give a brief summary here:\n- One can remove all inputs with any text features matching the caption. However, this is suboptimal as it removes datapoints with both visual and text features [1].\n- Alternatively, one can consider a much more expensive process of modifying the input captions of the dataset. One way for the same would be to remove identified text OCR from the input captions. Another way would be to generate new captions altogether, as recently explored in [2].\n- Finally, one could also consider modifying the input images to in-paint the OCR region. Note here that in T-MARS, we chose to use the masked images just to calculate new CLIP scores for filtering. A more elaborate discussion is provided in the appendix.\n---\n\nWe again thank you for raising these interesting questions and hope that the clarification helps you assess the work more positively. We are happy to provide any further clarifications if requested.\n\n[1] https://arxiv.org/abs/2301.02280.  \n[2] https://arxiv.org/abs/2307.10350.    \n[3] https://github.com/czczup/FAST."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493605519,
                "cdate": 1700493605519,
                "tmdate": 1700688780985,
                "mdate": 1700688780985,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]