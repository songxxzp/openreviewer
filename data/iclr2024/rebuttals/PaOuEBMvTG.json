[
    {
        "title": "Multiple Object Stitching for Unsupervised Representation Learning"
    },
    {
        "review": {
            "id": "arlHw1qi0C",
            "forum": "PaOuEBMvTG",
            "replyto": "PaOuEBMvTG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4979/Reviewer_gfC6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4979/Reviewer_gfC6"
            ],
            "content": {
                "summary": {
                    "value": "This paper indicates that contrastive learning for single object centric images has achieved impressive performance and suffered inferior performance on multiple objects. To this end, this paper proposes a method, i.e., Multiple Object Stitching (MOS), to refine the unsupervised representation for multi-object images. Particularly, they construct the multi-object images by stitching the single object centric ones, where the objects in the synthesized multi-object images are predetermined. In the experiments, the proposed method is evaluated on multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Applying contrastive learning to multiple object scenarios is meaningful."
                },
                "weaknesses": {
                    "value": "1. To the best of my knowledge, applying contrastive learning to multiple object scenarios has been explored by many works. There exist multiple works that aim to utilize contrastive loss for object detection and semantic segmentation. However, in the Introduction Section, the authors do not introduce these works. I recommend the authors modify their paper carefully, introduce these works, and make corresponding contrasts.\n\n2. To address the multiple object contrastive learning, the authors propose a stitching strategy. However, this strategy is somewhat simple. Meanwhile, I am not clear on how to use the proposed strategy to obtain object-related content.\n\n3. In Method Section, the authors should add a discussion section to discuss the advantages and disadvantages of the proposed method.  Using more stitched images may increase computational costs. The authors should give more interpretations. Finally, the authors should show some training curves, which is helpful for further understanding the proposed method. Meanwhile, the authors should show some T-SNE results to indicate the superiorities of the proposed method."
                },
                "questions": {
                    "value": "The authors should evaluate the proposed method on object detection and semantic segmentation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4979/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697533519641,
            "cdate": 1697533519641,
            "tmdate": 1699636485458,
            "mdate": 1699636485458,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y1JCClEBVZ",
                "forum": "PaOuEBMvTG",
                "replyto": "arlHw1qi0C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your constructive comments.  We address the concerns as follows:\n\n+ **More introduction of related work**\n\n  Thanks for your constructive comments. Following the suggestions, we supplement more related works in the Introduction section. \n\n+ **How to use the proposed strategy to obtain object-related content?**\n\n  The object-related images can be easily obtained from the off-the-shelf classification datasets, such as CIFAR10, CIFAR100 and ImageNet-1K. Due to single category label of each image from these datasets, the image context of these datasets can be regarded as single-object one.  \n\n+ **Discussion about advantages and disadvantages**\n\n  We supplement an additional section in the revision to discuss the advantages and disadvantages of our proposed method. The clarifications are presented as follows. \n\n  > Our proposed method synthesizes multi-object image by stitching off-the-shelf single object centric images, inevitably introducing artificiality produced by the boundary between stitched objects. Therefore, there is potential domain gap between our synthesized multi-object images and the natural ones, which may affect the multi-object representation learning. To this end, we introduce the contrastive objective between natural single object centric images to alleviate the above issue, which effectively improves the multi-object representation performance in the experiments. \n\n+ **The increased computational cost of our method**\n\n  Actually, stitching more images doesn't introduce additional computational cost. Specifically, all stitched images are resized into 224\u00d7224 size, the same image size as the one adopted by plain contrastive learning methods. Hence, compared to plain image view for contrastive learning, our stitched images doesn't increase the cost of computation.\n\n+ **Training curves of our method**\n\n  We supplement the pretraining curves in the appendix of our revised paper (**E Pretraining Curves** section). The clarifications are presented as follows. \n\n  > To further understand our method, we present the pretraining curves of our total loss and all loss items in Figure 6 and Figure 7. For CIFAR100 dataset in Figure 6, all losses achieves stable convergence during pretraining. Notably, the value of single-to-single loss is larger than the ones of multiple-to-single and multiple-to-multiple losses. I analyze the above results as follows.  Due to the supervision of multiple-to-single and multiple-to-multiple losses, the representations extracted by out method are encouraged to capture the similarities between different image instances, e.g., potential positive samples.  Therefore, single-to-single loss cannot simply reduce the similarities of other potential positive pairs (not the given positive pairs) for lower loss value. \n  >\n  > For more complicate ImageNet-1K dataset in Figure 7, the values of losses don't consistently decrease with the increment of pretraining epochs. However, we find that the performance of its representation keeps sustained improvement during the pretraining.   We plan to further explore the insight behind this result in future work.\n\n+ **T-SNE visualization results**\n\n  We supplement the T-SNE visualization results in the appendix of our revised paper (**F The Visualization of Pretrained Representations** section). The clarifications are presented as follows. \n\n  > To further assess the representation quality of our method, we visualize the representations of our method and iBOT using t-SNE. For better visualization, we select the representations of simple CIFAR10, which only contains 10 categories. For more complicated ImageNet-1K and CIFAR100 datasets, it's difficult to discriminate massive categories for better visualization by limited distinguished colored markers. As shown in Figure 8, our method can well group the samples from the same categories in the representation space, in spite of without supervised training. Compared to iBOT, our method achieves better representation consistency for \"green\" category.\n\n  \n\n+ **The evaluation results on object detection and semantic segmentation**\n\n  Indeed, we have evaluated our pretrained model on object detection and instance segmentation tasks of COCO dataset in Table 3. \n\n  To further validate the effectiveness of our method, we conduct transfer learning experiments with the pretrained ViT-S/16 backbone using FPN architecture on semantic segmentation of ADE20K dataset. All models are finetuned for 40k iterations. The results are listed as follows.\n\n  |     Method     |   mIoU   |   aAcc   |   mAcc   |\n  | :------------: | :------: | :------: | :------: |\n  |   SelfPatch    |   41.2   |   80.7   |   52.1   |\n  |     ADCLR      |   42.4   |   81.1   |   54.2   |\n  | **MOS (ours)** | **43.9** | **82.4** | **55.8** |\n\n  The experimental results demonstrate that our method significantly outperform the previous method."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049227012,
                "cdate": 1700049227012,
                "tmdate": 1700049227012,
                "mdate": 1700049227012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AnMP571gP2",
            "forum": "PaOuEBMvTG",
            "replyto": "PaOuEBMvTG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4979/Reviewer_AZJE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4979/Reviewer_AZJE"
            ],
            "content": {
                "summary": {
                    "value": "Contrastive learning suffers from the issue of semantic inconsistency caused by random cropping. This paper circumvents this problem by exclusively utilizing single-object images and their synthetic combinations, creating a 2x2 grid of a single image. This synthetic image serves as a pseudo-multi-object image, where the location of the positive object is known, allowing it to be trained similarly to a multi-object image. This technique enhances the learned representation, as evaluated in image classification and object detection benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Self-supervised learning, particularly learning from multi-object images, is an important problem.\n- The performance improvement is quite significant, both in image classification and object detection."
                },
                "weaknesses": {
                    "value": "**Movitation is not new**\n\nThe issue of semantic inconsistency in contrastive learning has been discussed in several prior works [1-4].\nThese works are not properly cited, which may lead readers to overestimate the contribution of this paper.\nThe efforts of prior work and the contributions of this paper should be clarified in the second paragraph of the introduction.\n\n[1] CASTing Your Model: Learning to Localize Improves Self-Supervised Representations. CVPR'21.\\\n[2] Unsupervised Object-Level Representation Learning from Scene Images. NeurIPS'21.\\\n[3] Object-aware Contrastive Learning for Debiased Scene Representation. NeurIPS'21. \\\n[4] Object-aware Cropping for Self-Supervised Learning. TMLR'22.\n\n\n---\n**Not addressing the raised issue**\n\nThis paper does not address the issue that has been raised: the problem of random cropping in multi-object images.\nInstead, the paper solely utilizes single-object images and their combinations, while ignoring multi-object images.\n\nThis raises two problems:\n- The paper can be considered a general augmentation strategy for contrastive learning, rather than specialized for multi-object images. This necessitates a proper revision of the storyline, as well as the baselines to compare with.\n- Restricting the training images to single-object images significantly limits the applicability of the method to the majority of unlabeled images containing multiple objects. Thus, the proposed method should be compared with vanilla contrastive learning trained on a larger superset of images that also includes multi-object images.\n\n\n---\n**Method is not new**\n\nCombining multiple images for data augmentation has been studied in multiple prior works [5-6].\nFurthermore, similar augmentation strategies have also been applied in the context of contrastive learning [7-8].\nThe paper should properly cite these prior works and make comparisons with them, particularly the most relevant MosRep [8].\n\n[5] RICAP: Random Image Cropping and Patching Data Augmentation for Deep CNNs. ACML'18.\\\n[6] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features. ICCV'19.\\\n[7] i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning. ICLR'21.\\\n[8] Mosaic Representation Learning for Self-supervised Visual Pre-training. ICLR'23.\n\n\n---\n**Method section could be revised**\n\nThe current method section is difficult to understand.\nSince the overall method is relatively simple, the authors should consider a more effective way to convey the message:\n- Super/subscripts are overused. Consider simplifying the notations.\n- Equations are overused. Consider moving less critical ones (e.g., straightforward definitions like Eq. (1)) into the inline text and using the equation mode exclusively for important formulas (e.g., the definitions of m2s/m2m/s2s losses) to emphasize them.\n- For Fig. 2, consider making the caption self-contained, possibly using natural language without math notations that are defined in the main text below."
                },
                "questions": {
                    "value": "1. Compare this method with prior works such as i-Mix and MosRep.\n2. This method can also be applied to multi-object images?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4979/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4979/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4979/Reviewer_AZJE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4979/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698284219972,
            "cdate": 1698284219972,
            "tmdate": 1700506835086,
            "mdate": 1700506835086,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Co0k4aNCoP",
                "forum": "PaOuEBMvTG",
                "replyto": "AnMP571gP2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your constructive feedback. We handle your main concerns as follows:\n\n+ **More introduction about related works**\n\n  Thanks for your constructive comments. Following the suggestions, we supplement more related works in the Introduction section and further refine the writing. \n  \n+ **The addressing of the issue about random cropping in multi-object images**\n\n  There are some misunderstandings of our proposed method. \n  \n  Our proposed method is designed to avoid random cropping issue suffered by directly conducting contrastive learning methods on natural multi-object images. To this end, we construct synthetic multi-object images by stitching the existing single-object-centric images, where the context of synthetic multi-object images can be predetermined. Therefore, our method can well address the random cropping issue on multi-object images.\n  \n+ **The comparison with MosRep**\n\n  Thanks for pointing out the missing references. We will carefully analyze and compare MosRep in the final revision. \n  \n  Compared to MosRep, the main contributions (differences) of our method can be summarized as follows.\n  \n  1. Our method provides an effective contrastive learning solution for multi-object images and achieves very competitive performance on both single-object-centric and multi-object tasks. In contrast, MosRep is only designed for the task on single-object-centric tasks.\n  2. Our loss function is significantly different from MosRep. Our proposed contrastive objective models the accurate correspondences of multiple-to-single, multiple-to-multiple and single-to-single relations. The stitched images are addressed as a whole in our method. However, MosRep handles the mosaic representations as single objects by ROI Align. \n  \n  Moreover, we compare our method with MosRep according to the official implementation **[R1]** (The official source code of [R1] fails to achieve the results reported in the paper). The results are listed as follows.\n  \n  |     Method      |   #Views    | Backbone  |  Linear   |    kNN    |\n  | :-------------: | :---------: | :-------: | :-------: | :-------: |\n  | MosRep **[R1]** | 2\u00d7224+4\u00d7112 | ResNet-50 |   72.3%   |   61.3%   |\n  | **MOS (Ours)**  | 2\u00d7224+4\u00d7112 | ResNet-50 | **75.6%** | **70.6%** |\n  \n  The above results demonstrate that our method consistently outperforms MosRep by a large margin on both linear probing and kNN probing protocols. \n  \n  **Moreover, we supplement the analysis of MosRep and cite it in the Related Work section.**\n  \n  **[R1]** https://github.com/DerrickWang005/MosRep\n  \n+ **The refinement of the writing about method section**\n\n  Thanks for your constructive suggestion.  We will refine these notations in the final version.\n  \n  For the notations in the paper, we mainly focus on the accurate expression of our method. We confirm that there is no useless notation or equation in our paper. In spite of somewhat complex notations, these notations can help authors to better understand the technique details of our method without confusion. Of course, we will further refine the writing of method section in the final revision. \n  \n+ **The application on multi-object images**\n\n  As clarified in the paper, our method is designed to simulate multi-object images by single-object-centric ones. Hence, the direct application of our method on multi-object images has not been considered in this paper. We would like to research this topic in future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049012153,
                "cdate": 1700049012153,
                "tmdate": 1700049012153,
                "mdate": 1700049012153,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iaM4rlohTz",
                "forum": "PaOuEBMvTG",
                "replyto": "AnMP571gP2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4979/Reviewer_AZJE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4979/Reviewer_AZJE"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the response. I have carefully read the rebuttal and revision. However, my original concerns have not been addressed.\n\n- **More introduction about related works**\\\nThe main idea of this paper is to restrict the positive pairs of contrastive learning to avoid semantic inconsistency. This idea is highly relevant to the suggested [1-4], which still have not been discussed in the paper. These approaches belong to a different category than the region-level and pixel-level contrastive learning.\n\n- **The addressing of the issue about random cropping in multi-object images**\\\nIn essence, the paper avoids using multi-object images to sidestep the issue of semantic inconsistency. This approach largely overlooks the potential information that could be learned from such images, and bypasses the problem rather than addressing it. Given that the majority of real-world images contain multiple objects, how can one collect single-object images in a scalable manner?\n\n- **The comparison with MosRep**\\\nThank you for the additional experiments. The rebuttal claims that \"Our method provides an effective contrastive learning solution for multi-object images and achieves very competitive performance on *both single-object-centric and multi-object* tasks. In contrast, MosRep is only designed for the task on single-object-centric tasks.\" However, I cannot find the experiments supporting this claim. Can the authors elaborate and demonstrate which parts of MOS make it effective for multi-object tasks, while MosRep cannot?\n\n- **The refinement of the writing about method section**\\\nI did not claim that there are unnecessary notations or equations. However, providing excessive details, including those that are less essential, can distract the readers. For example, overloading equations with numerous superscripts and subscripts hampers readability. As one example, the paper uses subscripts $_i$ to indicate the $i$-th sample of the batch, but this $i$ is unnecessary when describing other parts; one can simply denote the sample as $x$ instead of $x_i$. The readers can still understand that the same approach can be extended to other samples in the batch. Delivering the core message in a readable form while maintaining precision is the art of mathematical writing."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508408336,
                "cdate": 1700508408336,
                "tmdate": 1700508854209,
                "mdate": 1700508854209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jmn0hGDmiY",
            "forum": "PaOuEBMvTG",
            "replyto": "PaOuEBMvTG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4979/Reviewer_Kxdm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4979/Reviewer_Kxdm"
            ],
            "content": {
                "summary": {
                    "value": "The research paper introduces a novel method for unsupervised multi-object representation learning in computer vision. It addresses two significant challenges in this field: the semantics inconsistency between views and the difficulty in establishing accurate object correspondences. The proposed method tackles these challenges by synthesizing single-object images into multi-object images. This unique approach is demonstrated to be highly effective in experimental evaluations, excelling in image classification and outperforming existing methods in object detection and instance segmentation. The research opens up new possibilities for unsupervised learning in various modalities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe paper presents an innovative approach to unsupervised multi-object representation learning, which is an increasingly important area in computer vision.\n-\tThe method's technique for multi-object image stitching through data augmentation, scaling, and tensor operations is efficient and leads to improved representation learning."
                },
                "weaknesses": {
                    "value": "-\tWhile the method excels in multi-object representations, it may not have been extensively tested in scenarios with highly dynamic or cluttered objects."
                },
                "questions": {
                    "value": "-\tIn table 3, Selfpatch is trained for only 200 epochs and is compared against this work which is trained for 300 epochs? What is the rationale behind this comparison?\n-\tHave you tested the method in scenarios with highly dynamic or cluttered objects or occluded objects?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4979/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4979/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4979/Reviewer_Kxdm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4979/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739956332,
            "cdate": 1698739956332,
            "tmdate": 1699636485284,
            "mdate": 1699636485284,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FEOZN2n0Pk",
                "forum": "PaOuEBMvTG",
                "replyto": "jmn0hGDmiY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your thoughtful comments and resolve your main concerns as follows:\n\n+ **Experiments in scenarios with highly dynamic or cluttered objects**\n\n  Thanks for your constructive suggestion.\n\n  We evaluate our pretrained ViT-S/16 model (with FPN architecture) on WIDER FACE dataset for 80 epochs as [R1], where face objects present high degree of variability in scale, pose, expression, occlusion and illumination. The results (AP when IoU=0.5) are listed as follows. \n\n  |     Method      |  Easy AP  | Medium AP |  Hard AP  |   mAP    |\n  | :-------------: | :-------: | :-------: | :-------: | :------: |\n  | RetinaFace [R1] |   96.9%   |   96.2%   |   91.9%   |   52.3   |\n  |   MOS (ours)    | **97.3%** | **96.8%** | **92.8%** | **52.9** |\n\n  Compared to the previous best method RetinaFace, our method achieves consistent performance gain on easy, medium and hard subsets of WIDER FACE dataset. \n\n  [R1] RetinaFace: Single-stage Dense Face Localisation in the Wild. CVPR 2020.\n\n+ **The comparative results about SelfPath pretrained for 300 epochs**\n\n  Thanks for pointing out this typo error. The actual number of pretraining epochs for SelfPatch is 300 epochs. We revise this typo error in the revised paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048731428,
                "cdate": 1700048731428,
                "tmdate": 1700048731428,
                "mdate": 1700048731428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E5V5q0yDSC",
            "forum": "PaOuEBMvTG",
            "replyto": "PaOuEBMvTG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4979/Reviewer_R2AJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4979/Reviewer_R2AJ"
            ],
            "content": {
                "summary": {
                    "value": "Most existing algorithms for self-supervised learning use single object-centric images for contrastive learning. The paper claims they failed to perform well on images with multiple objects. To resolve the problem, the paper proposes to stitch multiple object-centric images to become multi-object images for contrastive learning. More specifically, the learning objective contains singe-to-single, single-to-multi, and multi-to-multi contrastive pairs. The paper shows improved performance on multiple benchmarks including ImageNet-1K, CIFAR, and COCO."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper achieves state-of-the-art results on multiple benchmarks. \n1. The idea is simple but effective."
                },
                "weaknesses": {
                    "value": "1. The motivation is unclear. The paper claims that contrastive learning for single object-centric images \"suffer inferior performance on\nthe widespread images with multiple objects\", but it doesn't provide enough evidence to support the claim. For example, ImageNet-1K and CIFAR are recognition problems with single objects, why do we need to stitch multiple together? I understand that, for example, even in ImageNet-1K a lot of times images do contain multiple objects. If this is the case, I suggest the paper show the number of objects vs classification performance by applying some existing detectors, which can prove that the performance of existing algorithms degrades on multi-object images. However, somehow it is counter-intuitive because the proposed method may strengthen the minor objects in the images which distracts the recognition of the main object.\n1. Similar to the above, detection and segmentation tasks should be the main and right benchmarks to explain the proposed algorithm because they definitely contain multiple objects in a scene. However, the paper still needs to provide more analysis to support the main assumption about multi-objects. For example, it will be very convincing if the paper shows the proposed algorithm can already achieve a better class-agnostic recall of objects on the attention map on COCO before even fine-tuning on it. In addition, I think it's better if it can show the ablation in table 4 with the detection and segmentation task. \n1. To me the idea is simple and I totally understand the proposed algorithm, but I think section 3 is overcomplex with math expression. To me math expression is to help explanation not to make it harder to read. One problem might be there are so many notations. I believe there is a simpler way to make it clearer. For example, stitching itself might not be that important. More important is how to learn from stiched images, e.g. proposed three losses. I hope I can see more underlying reasons for learning objectives instead of just showing the formula. For example, I am not quite convinced by the claim of L_s2s, \"to alleviate the domain gap between synthesized multi-object images and natural images\". Isn't it just the same as the existing algorithms without stitching?\n1. The paper uses ViT as the backbone. I am curious about the performance with CNN because to my understanding this assumption should not be related to the architecture."
                },
                "questions": {
                    "value": "1. In table 4, what is the main difference between s2s and existing unsupervised algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4979/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816567558,
            "cdate": 1698816567558,
            "tmdate": 1699636485174,
            "mdate": 1699636485174,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gk3LI5z1xO",
                "forum": "PaOuEBMvTG",
                "replyto": "E5V5q0yDSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your constructive comments. We address your main concerns as follows:\n\n+ **The evidence for the performance degradation on multi-object images**\n\n  Thanks for pointing out the issue. \n\n  The previous research works **[R1, R2]** indeed support that plain contrastive learning methods for single object-centric images suffer inferior performance on the multi-object images. \n\n  For example, the introduction of **[R1]** claims that \"for typical contrastive learning models, the pretraining pretext task considers an image holistically in instance discrimination, without explicit spatial modeling over regions. Though it enhances transferability for classification, this practice is less compatible with spatial reasoning tasks, such as object detection\". This clarification is also validated in the experiments, such as Figure 1 of **[R1]**.\n\n  Moreover, our experimental results in Table 5 also demonstrate that our method achieves significant performance gain over the baseline pretrained by single object-centric images on both object detection and instance segmentation. \n\n  We will supplement and emphasize this point in the final revision. \n\n  \n\n  **[R1]** Instance Localization for Self-supervised Detection Pretraining. CVPR 2021. \n\n  **[R2]** Self-EMD: Self-Supervised Object Detection without ImageNet. Arxiv 2021.\n\n  \n\n+ **Why applying image stitching strategy on single-object images, such as ImageNet-1K and CIFAR?**\n\n  Although the images from both ImageNet-1K and CIFAR are single object-centric ones, we surprisingly  find that our method consistently achieve very competitive results on image classification task for single object-centric images. Compared to [R1], our method also boosts the performance on image classification, instead of only spatial-sensitive tasks as [R1]. We speculate that  our proposed stitching method introduces more image context information (include other objects) into training targets, thus alleviating the potential representation degradation (the plain methods may only focus on the most discriminative parts but miss other contents)  and improving the generalization of representations.\n\n  \n\n+ **More analysis about the assumption on multi-object images**\n\n  Thanks for your constructive suggestion.\n\n  We visualize the self-attention maps of our pretrained model on COCO dataset in Figure 11 (**G The Visualization of Self-Attention Map section**) and object correspondences between multi-object images with the same categories in Figure 12 (**H Object Correspondence**  section). \n\n  As shown in Figure 11, our method can well discriminate different objects in images. For example, the baby and teddy bear of the second row image about can be well distinguished from each others, yet the counterpart, iBOT, fails to discriminate them. \n\n  In Figure 12, our method achieves rough matching between different multi-object images with the same categories. In the first image pair (both images contains dog and person), the correspondences successfully point out the semantic matching for the objects in the images. \n\n  The above visualization results indeed demonstrate the effectiveness of our method on multi-object image representations.\n\n+ **More ablation study on detection and segmentation tasks**\n\n  We supplement ablation results on object detection and instance segmentation tasks in Table 4 as follows.\n\n  | $L_{s2s}$ | $L_{m2s}$ | $L_{m2m}$ | COCO AP$^{bb}$ | COCO AP$^{mk}$ |\n  | :-------: | :-------: | :-------: | :------------: | :------------: |\n  |     \u2713     |    --     |    --     |      42.8      |      38.9      |\n  |    --     |     \u2713     |    --     |      43.1      |      38.8      |\n  |    --     |    --     |     \u2713     |       NA       |       NA       |\n  |    --     |     \u2713     |     \u2713     |      43.6      |      39.2      |\n  |     \u2713     |     \u2713     |    --     |      44.3      |      39.7      |\n  |     \u2713     |     \u2713     |     \u2713     |    **45.6**    |    **40.6**    |\n\n  When the model is pretrained by all three losses $L_{s2s}$, $L_{m2s}$ and $L_{m2m}$, our method achieves the best results on both object detection and instance segmentation of COCO dataset. It indeed supports the effectiveness of the above three loss items. We supplement these results in the revised paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048491261,
                "cdate": 1700048491261,
                "tmdate": 1700048491261,
                "mdate": 1700048491261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jyUA1vWilJ",
                "forum": "PaOuEBMvTG",
                "replyto": "E5V5q0yDSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4979/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "+ **More explanations about $L_{s2s}$**\n\n  The loss $L_{s2s}$ is just the normalized temperature-scaled cross entropy loss, which is widely adopted by the plain contrastive learning methods. \n\n  We explain the design of the contrastive losses $L_{s2s}$ as follows.\n\n  1. The patch boundaries of the stitched images adopted in our method are discontinuous, thus inevitably producing artificiality, which are different from the natural images. We call it domain gap between the stitched images and natural images.\n  2. If we trained the model with only the stitched images, such as $L_{m2s}$ or $L_{m2m}$, the data for training and the ones for testing would be significantly different. Hence, the model would degrade the adaption  of representations on natural images. The results in Table 4 support this clarification, where the training without $L_{s2s}$ obviously degrades the performance on downstream tasks. \n  3. The application of $L_{s2s}$ can introduce natural images during pretraining, thus improving the representation adaption on natural images. It's also supported by the ablation results in Table 4.\n\n+ **The performance on CNN**\n\n  As clarified in Introduction section, our method works better on vision transformer architectures. For CNNs, the artificiality produced by the boundary of image stitching may partially affect the representation performance. We evaluate our method on CNN architecture (ResNet50 pretrained for 200 epochs) as follows.\n\n  |       Method        | AP$^{\\rm bb}$ | AP$_{50}^{\\rm bb}$ | AP$_{75}^{\\rm bb}$ | AP$^{\\rm mk}$ | AP$_{50}^{\\rm mk}$ | AP$_{75}^{\\rm mk}$ |\n  | :-----------------: | :-----------: | :----------------: | :----------------: | :-----------: | :----------------: | :----------------: |\n  |  DetCo (ResNet 50)  |     40.1      |        61.0        |        43.9        |     36.4      |        58.0        |        38.9        |\n  | **MOS (ResNet 50)** |     41.2      |        62.4        |        44.7        |     37.3      |        59.2        |        39.5        |\n  | **MOS (ViT-S/16)**  |   **45.6**    |      **65.9**      |      **48.9**      |   **40.6**    |      **62.8**      |      **42.6**      |\n\n  The results demonstrate that our method outperforms the previous state-of-the-art on CNN: DetCo, but worse than the ViT counterpart. It's a promising line to eliminate the gap caused by stitching artificiality. We plan to explore this topic in future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048511774,
                "cdate": 1700048511774,
                "tmdate": 1700549429647,
                "mdate": 1700549429647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]