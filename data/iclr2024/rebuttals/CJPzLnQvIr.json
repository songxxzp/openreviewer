[
    {
        "title": "QuickDrop: Efficient Federated Unlearning by Integrated Dataset Distillation"
    },
    {
        "review": {
            "id": "ILZtCARadG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1297/Reviewer_BiDM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1297/Reviewer_BiDM"
            ],
            "forum": "CJPzLnQvIr",
            "replyto": "CJPzLnQvIr",
            "content": {
                "summary": {
                    "value": "This work studies how to extract a much smaller dataset for federated unlearning (FU) using dataset distillation (DD), which speeds up the FU time. The method is incremental to (Zhao et al., 2021), adapting it to the federated setting with many clients. This method reuses the gradient updates produced during FL training for DD, so the overhead of creating distilled datasets is very small. However, the result quality is not as good as the more expensive baselines, but finetuning steps on the original dataset can mitigate this issue to achieve comparable accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The approach is effective especially in speeding up the unlearning request processing, and the idea makes sense and is intuitive.\nThe experiments are comprehensive."
                },
                "weaknesses": {
                    "value": "The idea is very incremental to (Zhao et al., 2021). In particular, the main technical sections are Sec 3.2 and 3.3, but Sec 3.2 is basically reviewing (Zhao et al., 2021). Sec 3.3 is short and describes the small changes of (Zhao et al., 2021) in Algorithm 1 to achieve Algorithm 2, where the changes are very standard migration to the FedAvg setting.\n\nThe accuracy of QUICKDROP is lower than that of the baselines, and a solution of finetuning is proposed in Figure 4 to be effective. However, finetuning is claimed to be done on the original dataset. Does that mean you still need all clients to collaborate to complete the finetuning?\n\nFinally, while the method is intuitive, it is very simple without much theory."
                },
                "questions": {
                    "value": "Finetuning is claimed to be done on the original dataset. Does that mean you still need all clients to collaborate to complete the finetuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1297/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1297/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1297/Reviewer_BiDM"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697309755994,
            "cdate": 1697309755994,
            "tmdate": 1699636056675,
            "mdate": 1699636056675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZsvWZBbJQU",
                "forum": "CJPzLnQvIr",
                "replyto": "ILZtCARadG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "About the incremental  contribution on  (Zhao et al., 2021)."
                    },
                    "comment": {
                        "value": "We understand the perceived limitation of our technical contribution. While it is true that our algorithm builds upon the gradient matching algorithm introduced by Zhao et al. (2021), it is not immediately evident if DD performs optimally in the context of FU. For instance, Zhao et al. (2021) train over several model initializations deemed necessary for good distillation, whereas only one model initialization is available for distillation in QuickDrop. \n\nOur contributions extend beyond the combination of existing methods in extensively demonstrating the viability, challenges, and modifications necessary to achieve efficient unlearning. Furthermore, our work emphasizes minimizing the overhead associated with DD generation, addressing a critical aspect often overlooked in previous studies."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064590895,
                "cdate": 1700064590895,
                "tmdate": 1700064590895,
                "mdate": 1700064590895,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HdFqFTlHTH",
                "forum": "CJPzLnQvIr",
                "replyto": "ILZtCARadG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Fine-tuning issues"
                    },
                    "comment": {
                        "value": "We want to clarify that each client performs fine-tuning independently using its local dataset. That is, each client first generates its own distilled dataset during model training with FL and later locally fine-tunes after the FL training is over. Therefore, no collaboration is needed during the fine-tuning phase. We will clarify this in the final version of the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064639343,
                "cdate": 1700064639343,
                "tmdate": 1700064639343,
                "mdate": 1700064639343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JscVUKzkJR",
                "forum": "CJPzLnQvIr",
                "replyto": "HdFqFTlHTH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Reviewer_BiDM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Reviewer_BiDM"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674851796,
                "cdate": 1700674851796,
                "tmdate": 1700674851796,
                "mdate": 1700674851796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L1uNnhppwr",
            "forum": "CJPzLnQvIr",
            "replyto": "CJPzLnQvIr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1297/Reviewer_ug1m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1297/Reviewer_ug1m"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of federated unlearning and proposes an efficient method based on dataset distillation. It is used to accelerate both the training and unlearning stages The authors conduct unlearning by stochastic gradient ascent. The proposed method is evaluated experiments on three datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.\tFederated unlearning is an interesting research topic. The authors also focus on a general framework of sample-wise unlearning, which is also applicable to class-wise and client-wise.\n2.\tThe paper is clearly written and well organized."
                },
                "weaknesses": {
                    "value": "1.\tThe technical contribution is incremental. The proposed method uses a gradient ascent + fine-tuning framework, which has been introduced in [1, 2]. The main contribute of this paper is the use of data distillation (a direct application of existing algorithm), which is similar to the idea of representative data selection in [3]. Thus, this paper seems to be a combination of existing methods.\n2.\tThe proposed method is applicable to sample-wise unlearning, but the experiments are conducted on class-wise and client-wise unlearning. I was expected to see the evaluation of sample-wise unlearning, since it has broader applicability.\n3.\tLack of baseline methods. I suggest that the authors consider including some representative ones or at least one method from more different approaches. Additionally, PU-MP is a CNN-specific method. It may not be appropriate to limit the target model to CNN and compare it with the proposed method, which is more general. I list some of the approaches for your consideration:\ni.\tEfficient retraining: [4, 5]\nii.\tInfluence function: [6]\niii.\tGradient ascent + regularization: [7, 8] ([8] is compared in the paper)\niv.\tRoll back gradient + knowledge distillation: [1]\nv.\tScaling: [9]\n\n[1] 2022, Federated Unlearning with Knowledge Distillation\n[2] AAAI\u201921, Amnesiac Machine Learning\n[3] IJCAI\u201922, ARCANE\uff1aAn Efficient Architecture for Exact Machine Unlearning\n[4] IWQOS\u201921, FedEraser: Enabling Efficient Client-Level Data Removal from Federated Learning Models\n[5] WSDM\u201923, Federated Unlearning for On-Device Recommendation\n[6] INFOCOM\u201922, The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining\n[7] ICML\u201922, Federated Unlearning: How to Efficiently Erase a Client in FL\n[8] IEEE Network, Federated Unlearning: Guarantee the Right of Clients to Forget\n[9] 2023, VERIFI: Towards Verifiable Federated Unlearning"
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698474216539,
            "cdate": 1698474216539,
            "tmdate": 1699636056607,
            "mdate": 1699636056607,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hfZA5xur42",
                "forum": "CJPzLnQvIr",
                "replyto": "L1uNnhppwr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Technical contribution"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's examination of our proposed method and references to related work. While some aspects of our approach are built upon previous work, we argue that our work extends beyond a mere combination of existing methods. Although SGA and DD techniques are well-known, it is not immediately evident if their combination can perform well for FU. We conducted thorough empirical assessments on three diverse datasets to understand and demonstrate the effectiveness and adaptability of our approach across different unlearning scenarios, e.g., unlearning on class and client levels. Furthermore, our work emphasizes minimizing the overhead associated with DD generation, addressing a critical aspect often overlooked in previous studies.\n\n**Essential difference between DD and representative data selection**:\n\nBesides, we have carefully studied the related work indicated by the reviewer ([3]) about representative data selection and find there is a key difference between our approach and the representative data selection. \nDD in QuickDrop can absorb all the crucial information from the whole original dataset by condensation, while the approach in [3] can only choose part of the dataset and lose extensive information in those unselected data. The unlearning performance with representative data selection therefore has a worse accuracy degradation compared with QuickDrop. For example, Figure 6 in [3] reveals that if 25% data are selected from MNIST as representative dataset, the accuracy on the representative dataset is merely 60%, compared with 95% when using the full original dataset. However, in QuickDrop with scale=100 (i.e., the size of the distilled dataset is only 1% compared with the original dataset), the achieved test accuracy with the distilled dataset is 91%."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064240348,
                "cdate": 1700064240348,
                "tmdate": 1700064240348,
                "mdate": 1700064240348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qJw93IqkV3",
                "forum": "CJPzLnQvIr",
                "replyto": "L1uNnhppwr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sample-wise unlearning"
                    },
                    "comment": {
                        "value": "We agree that sample-wise unlearning presents a more general scenario compared to class- and client-wise unlearning. While it would be valuable to tackle sample-wise unlearning, our framework in its current form requires additional adaptations to achieve this, and we should have been more clear about this limitation. We currently generate class-wise distilled samples that can be reused for downstream class-wise or client-wise unlearning. To achieve sample-wise unlearning, we would need to understand how to generate targeted distilled data for specific forgetting samples, and we left this out of this submission due to the lack of space and to keep the discussion more focused. We look forward to extending our work with sample-wise unlearning."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064397544,
                "cdate": 1700064397544,
                "tmdate": 1700064397544,
                "mdate": 1700064397544,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kP7GwB9vj0",
                "forum": "CJPzLnQvIr",
                "replyto": "L1uNnhppwr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Lack of baseline methods."
                    },
                    "comment": {
                        "value": "Because of the time limitation, we can only add one more baseline for comparison during this rebuttal period, which is \u201cIJCAI\u201922, ARCANE\uff1aAn Efficient Architecture for Exact Machine Unlearning\u201d. We will report these experiment results soon."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064515766,
                "cdate": 1700064515766,
                "tmdate": 1700064515766,
                "mdate": 1700064515766,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UhV2GoWcPv",
            "forum": "CJPzLnQvIr",
            "replyto": "CJPzLnQvIr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1297/Reviewer_AkUq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1297/Reviewer_AkUq"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes QuickDrop, a new federated unlearning (FU) approach that uses dataset distillation to efficiently remove specific data from a collaboratively trained machine learning model. QuickDrop has each client create a highly condensed \"distilled\" dataset that preserves the key features of their local training data using dataset distillation. Clients then use this compact distilled dataset during the unlearning and recovery phases rather than their full training datasets. This drastically reduces the computation cost of unlearning. The paper also proposes integrating dataset distillation into the regular federated learning training process by reusing gradients, avoiding extra overhead. Evaluations on MNIST, CIFAR-10, and SVHN show QuickDrop reduces unlearning time by 463.8x compared to full model retraining and 65.1x compared to prior FU techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Proposes QuickDrop, which integrates dataset distillation into regular federated learning training by reusing gradients, avoiding extra overhead.\n2. Experiments on 3 datasets conclusively show QuickDrop reduces unlearning time by 65-464x over baselines."
                },
                "weaknesses": {
                    "value": "1. The paper does not discuss communication overhead of exchanging distilled datasets, which is the main concern I have regarding whether the proposed QuickDrop could be useful.\n2. Some of the important details are not clear, for example, how non-IID data affects QuickDrop and the impact of distillation hyperparameters."
                },
                "questions": {
                    "value": "I don't have other questions in addition to the questions in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698556949789,
            "cdate": 1698556949789,
            "tmdate": 1699636056534,
            "mdate": 1699636056534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FllgcoA4ux",
                "forum": "CJPzLnQvIr",
                "replyto": "UhV2GoWcPv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Communication Overhead Issue"
                    },
                    "comment": {
                        "value": "There may be a misunderstanding here. \n\nQuickDrop does not exchange distilled datasets: each distilled dataset is generated locally and remains local. Therefore, this should not be a concern at all. Unlearning, however, does proceed in a federated manner involving the exchange of updated and global models between clients and the server. \n\nWe have provided in Section 3.2 a detailed explanation of the unlearning algorithm that might help clarify the point. If specific points still need to be clarified, we would be happy to provide additional details or discuss them further."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700063399023,
                "cdate": 1700063399023,
                "tmdate": 1700063399023,
                "mdate": 1700063399023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ebsYdEUTSD",
                "forum": "CJPzLnQvIr",
                "replyto": "UhV2GoWcPv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "How non-IID data affects QuickDrop and the impact of distillation hyperparameters"
                    },
                    "comment": {
                        "value": "**The effect of Non-IIDness on QuickDrop**:\n\nFrom an algorithm perspective, the data distribution of the dataset used does not impact how distilled data is generated on each client. From a performance perspective, in Appendix C (Table 7 and 8), we have conducted experiments with IID and non-IID data distributions, showing that QuickDrop can still achieve similar accuracy compared to baselines while maintaining its significant efficiency benefit.\n\nIn general, non-IIDness affects us the same as in other unlearning algorithms/baselines. Understanding the impact of data distribution on unlearning algorithms and their proper assessment with well-defined metrics is still an open question for the machine unlearning community.\n\n**Impact of other hyperparameters for distillation**:\n\n**Scale**: An important hyperparameter that influences our distillation is the scale, which defines the size of the distilled dataset on each client. We currently use a scale of 100, i.e., we condense client local datasets by a hundred times. If we reduce the scale (e.g., to 10), it is obvious that we will obtain a larger distilled dataset, which can absorb more information to represent the characteristics of the original dataset and improve the effectiveness of unlearning.\n\nGiven the importance of this parameter, we conduct additional experiments to test the influence of scale on unlearning accuracy and computation cost of QuickDrop (on CIFAR-10, alpha=0.1, 10 clients). We list the results in the tables below, where the first table shows the accuracy on the R-set after recovery and the second table shows the compute time to conduct unlearning:\n\n| Scale | Acc. on R-set  (QuickDrop) | Acc.on R-set  (Retraining) |\n|:-----:|:------------------------------:|:-------------------------------:|\n|  100  |             70.48%             |              74.95%             |\n|   10  |             73.69%             |              74.95%             |\n\n| Scale | Compute time  (QuickDrop) | Compute time (Retraining) |\n|:-----:|:-------------------------:|:-------------------------:|\n|  100  |           15.61s          |          7239.58s         |\n|   10  |          154.79s          |          7239.58s         |\n\nIn conclusion, the scale is a trade-off between the unlearning accuracy and efficiency, and can be adjusted according to the different demands of clients or the FU environment. We will add these experimental results to the paper for a final version.\n\nWe also identify other hyperparameters that influence the performance of QuickDrop, like the current global round K and local update steps T. In our work, we fix K=200 and T=50. Since QuickDrop combines DD and FL, the distilled dataset is a by-production of the normal FL training process. The local update step T and global round K required to obtain a trained FL model is much larger than the requirements of DD. Therefore, we are guaranteed that the distilled dataset with K=200 and T=50 are of high quality."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064081970,
                "cdate": 1700064081970,
                "tmdate": 1700064081970,
                "mdate": 1700064081970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FfNYdswUZD",
            "forum": "CJPzLnQvIr",
            "replyto": "CJPzLnQvIr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1297/Reviewer_um5Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1297/Reviewer_um5Z"
            ],
            "content": {
                "summary": {
                    "value": "This research is primarily focused on the development of a method designed for the removal of specific training data from a Machine Learning (ML) model that has been trained using Federated Learning (FL). This specialized technique, known as Federated Unlearning (FU), is the central objective of the study.\n\nTo achieve this, the authors introduce an innovative and highly efficient FU methodology named QUICKDROP. QUICKDROP leverages the power of Dataset Distillation (DD) to streamline the unlearning process, resulting in a significant reduction in the computational resources required when compared to conventional approaches. The fundamental principle of QUICKDROP involves each client generating a concise and representative dataset referred to as a \"distilled dataset\" using DD techniques. These distilled datasets play a crucial role in the subsequent unlearning phase.\n\nFurthermore, the authors demonstrate their ingenuity by seamlessly integrating Dataset Distillation (DD) into the Federated Learning (FL) training process. This integration enhances the overall efficiency of QUICKDROP by capitalizing on the reuse of gradient updates generated during FL training for DD purposes. Consequently, the overhead associated with creating distilled datasets is effectively minimized.\n\nThe empirical evaluation of QUICKDROP's performance, conducted across three standard datasets, conclusively demonstrates its capacity to deliver remarkable efficiency gains in the context of federated unlearning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The development of QUICKDROP significantly improves the efficiency of the Federated Unlearning (FU) process. By incorporating Dataset Distillation (DD) and Stochastic Gradient Ascent, the method considerably reduces the computational resources and time required for unlearning compared to existing approaches.\n\n2. The research demonstrates the scalability of QUICKDROP by evaluating its performance with a large number of clients (100 clients). This scalability is vital in practical FL scenarios involving numerous participants."
                },
                "weaknesses": {
                    "value": "The integration of Dataset Distillation (DD) and Federated Learning (FL) is not a novel concept. Previous research presented at conferences such as ICLR and CVPR has explored similar strategies involving the replacement of original data with distilled data for FL training. Applying this strategy to a new application problem, such as federated unlearning, does not represent a sufficiently novel contribution.\n\nThe experimental results presented in the research do not demonstrate strong evidence of effectiveness. While the findings indicate a significant improvement in efficiency, there are concerns about the effectiveness of the method, particularly in terms of accuracy. Sacrificing effectiveness for increased efficiency may not be a favorable trade-off in practical applications.\n\nThe organization of the paper could benefit from improvement. For instance, the extensive space dedicated to explaining dataset distillation through gradient matching is unnecessary. This concept is derived from existing work and is not a novel contribution of this research."
                },
                "questions": {
                    "value": "Please refer to \"Weaknesses\" part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662775753,
            "cdate": 1698662775753,
            "tmdate": 1699636056463,
            "mdate": 1699636056463,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ncGOF8wiBO",
                "forum": "CJPzLnQvIr",
                "replyto": "FfNYdswUZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty about the integration of Dataset Distillation (DD) and Federated Learning (FL)"
                    },
                    "comment": {
                        "value": "We acknowledge that Dataset Distillation (DD) within Federated Learning (FL) algorithms has been explored in prior research. However, our work introduces a meaningful extension by focusing on the application of DD to Federated Unlearning (FU), which presents distinct challenges compared to standard FL. The differences between FL and FU are non-trivial, and it is not immediately evident that DD performs well for FU. Additionally, we carefully reuse the gradients produced by FL to generate the distilled datasets, an improvement beyond simply applying DD to FU. Gradient reusage brings along a unique set of challenges. For instance, Zhao et al. (2021) train over several model initializations deemed necessary for good distillation, whereas only one model initialization is available for distillation in QuickDrop since we leveraged FL. We will elaborate on these challenges in the final version of our work.\n\nOur contributions also extend beyond the combination of existing methods in extensively demonstrating the viability, challenges, and modifications necessary to achieve efficient unlearning. We showcased this through thorough empirical assessments on three diverse datasets, each exhibiting varying levels of heterogeneity. Our work emphasizes minimizing the overhead of generating distilled datasets, addressing a critical aspect often overlooked in previous studies. While we agree that QuickDrop is inspired by and relies to some extent on existing techniques, our research is the first to apply DD to FU."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700062993359,
                "cdate": 1700062993359,
                "tmdate": 1700062993359,
                "mdate": 1700062993359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qy2sNsrRM5",
                "forum": "CJPzLnQvIr",
                "replyto": "FfNYdswUZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The effectiveness of QuickDrop on the trade-off between accuracy and efficiency."
                    },
                    "comment": {
                        "value": "We agree with the reviewer that our approach does not match the performance achieved by retraining from scratch baseline without additional fine-tuning steps. Like any methodology, inherent tradeoffs are inevitable, and it would be unrealistic to expect our method to consistently outperform retraining from scratch across all dimensions. We intend to present these results to offer a nuanced understanding of the efficiency-accuracy tradeoff. \n\nEven with a potential decrease in accuracy, the significant improvement in efficiency addresses a specific need in practical applications where computational resources might be limited (e.g., edge settings such as mobile phones). Furthermore, the efficiency gains of QuickDrop become more pronounced when doing multiple unlearning requests, as discussed in Section 4.3. \n\nWe believe that our work contributes to this by providing a comprehensive picture of the tradeoff landscape, offering practitioers the flexibility to make informed decisions based on the specific requirements of their applications."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700063142232,
                "cdate": 1700063142232,
                "tmdate": 1700063142232,
                "mdate": 1700063142232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FdNd6DjhCI",
                "forum": "CJPzLnQvIr",
                "replyto": "FfNYdswUZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Paper Organization"
                    },
                    "comment": {
                        "value": "Through the current organization, we intend to provide a comprehensive comparison of our proposed method to previous work, highlighting the key differences and modifications. Furthermore, this explanation is helpful for reviewers unaware of the inner workings of gradient matching. However, based on the reviewer\u2019s feedback, we understand that this description might be too elaborate, and we intend to shorten this section in the final version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700063190097,
                "cdate": 1700063190097,
                "tmdate": 1700063190097,
                "mdate": 1700063190097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]