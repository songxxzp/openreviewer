[
    {
        "title": "GTMGC: Using Graph Transformer to Predict Molecule\u2019s Ground-State Conformation"
    },
    {
        "review": {
            "id": "lqEhvzchy1",
            "forum": "F7QnIKlC1N",
            "replyto": "F7QnIKlC1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5387/Reviewer_1WnG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5387/Reviewer_1WnG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes GTMGC, a transformer-based architecture for end-to-end prediction of 3D groundstate molecules' conformations from their 2D graph.\n\nThe method makes use of MoleBERT for initial embedding as well as LPE for positional encoding.\nThe inputs are then processed by a Transformer-based model, where the self-attention modules are augmented with the adjacency matrix and learned/predicted atomic distance matrix in a weighted sum fashion. \n\nThe loss is augmented by a regularization of the middle distance matrix prediction.,"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is very clear and pleasant to read.\n2. The integration of various existing frameworks for the task of molecules' ground state conformation is original to some extent.\n3. The performances compared to the baselines are significant."
                },
                "weaknesses": {
                    "value": "I believe there are two main weaknesses:\n\n1. Novelty: There are plenty of previous works (not cited) implementing Transformer architectures that implement the elements of the proposed self-attention, e.g., [1] uses the adjacency matrix, [2] makes use of the distance matrix (even cited in the manuscript), while the weighted summation is ubiquitous in many fields using Transformers [3]. The initial encoding is obviously not a technical contribution.\n\nThus, the contribution may be summarized as the integration of existing methods/approaches, augmented with the regularization loss on the distance matrix (the \\beta seems to bring very minor improvement).\n\n2. Strange results: I may be wrong, but according to Table 3 (ablation study) a *simple Transformer* architecture without any addition to the self-attention reaches 0.4395 (MAE) which is already better than all the other baselines. This is problematic. Also, the final improvement is only ~1%. Finally, the reported results seem to take the best-ablated model results for each metric, which is wrong.\n\nMoreover,  we have:\n\n3. The advantage of MoleBert over the standard atomic encoding is extremely shallow or even worse.\n4. Lack of comparison with other Transformer based methods.\n\n[1] *Molecule attention transformer.*\n\n[2] *Geometric transformer for end-to-end molecule properties prediction.*\n\n[3] *Axial-DeepLab*"
                },
                "questions": {
                    "value": "Currently, it seems the proposed contributions don't bring any advantage over a simple (large) transformer model.\n\n1. One needs to know the capacity of the model in order to assess the origin of the good performance. \nAccording to weakness 2, it seems the good performances are obtained *almost solely* from a large/powerful standard Transformer model. From Table 5, the model seems much bigger than other methods. Also, the discrepancies between the tables are disturbing (one single metric should be used for the model validation).\n\n2. Ablating the initial LPE. \n\n3. It would be beneficial to have a comparison performance with (at least one) other molecule transformer-based methods such as [4,5,6] or others at a similar capacity.\n\n4. It would be interesting to look at the learned weighting parameters (\\gamma) of the self-attention to better understand the contribution of each (maybe even adding a weighting to the global attention).\n\n\n[4] Relative molecule self-attention transformer.\n\n[5] 3dtransformer: Molecular representation with transformer in 3d space.\n\n[5] Geometric transformer for end-to-end molecule properties prediction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5387/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698246140324,
            "cdate": 1698246140324,
            "tmdate": 1699636545151,
            "mdate": 1699636545151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mG5yN958Xp",
                "forum": "F7QnIKlC1N",
                "replyto": "lqEhvzchy1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1WnG's Weakness1"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe greatly appreciate you taking the time to review our manuscript. Your feedback and suggestions are extremely valuable to us and will help us improve and enhance our work.\n\n**Weakness 1:** \n\n> Novelty: There are plenty of previous works (not cited) implementing Transformer architectures that implement the elements of the ......\n\n**Response:**\n\nAs you mentioned, numerous studies have attempted to utilize $\\mathbf{A}$ and $\\mathbf{D}$ to enhance the Transformer network structure in molecular modeling. We believe that $\\mathbf{A}$ and $\\mathbf{D}$  are the most prevalent inductive biases for 3D molecular structures. Therefore, our innovation lies not in augmenting self-attention with A and D per se, but in the manner we integrate A and D into the network. \n\nFirst, we list our core formulas and the core formulas of the two works [1] [2]:\n\n**Ours**:  $\\mathbf{S}_l^\\prime=\\mathbf{Q}_l{\\mathbf{K}_l^T}\\odot{[\\mathbb{E}_n+{\\gamma_l^A\\times{\\mathbf{A}}}+\\gamma_l^D\\times{(\\vec{\\beta}^{\\max}-\\mathbf{D})}]}$ and $\\mathbf{O}_l=\\text{softmax}(\\frac{\\mathbf{S}_l^\\prime}{\\sqrt{d_k}})\\mathbf{V}_l$, where $\\gamma_l$ is learnable parameters.\n\n**[1]**: $\\mathbf{O}_l=(\\lambda_a\\text{softmax}(\\frac{\\mathbf{Q}_l{\\mathbf{K}_l^T}}{\\sqrt{d_k}})+\\lambda_dg(\\mathbf{D})+\\lambda_g\\mathbf{A})\\mathbf{V}_l$, where $\\lambda$ is hyperparameters and $g$ is a function.\n\n**[2]**: $\\mathbf{O}_l=\\text{softmax}(\\frac{\\mathbf{Q}_l{\\mathbf{K}_l^T}}{\\sqrt{d_k}})\\odot{f({\\mathbf{D}}^{-1})^2}\\mathbf{V}_l$,  where $f$ is a shallow, fully connected neural network.\n\nAlthough our method has certain similarities with theirs, we emphasize the following differences to enhance our novelty\uff1a\n\n1. Figure 1(b) in our manuscript demonstrates our main idea. We propose using three masks, namely $\\mathbb{E}_n$, $\\mathbf{A}$, and $\\vec{\\beta}^{\\max}-\\mathbf{D}$, and adjusting the different combinations between each head with learnable parameters $\\gamma_l^A$ and $\\gamma_l^D$. Ultimately, we aim to adjust $\\mathbf{Q}_l{\\mathbf{K}_l^T}$ according to the final attention-mask, ${\\mathbb{E}_n+{\\gamma_l^A\\times{\\mathbf{A}}}+\\gamma_l^D\\times{(\\vec{\\beta}^{\\max}-\\mathbf{D})}}$ , that has learned and integrated \u2018global\u2019, \u2018nearby\u2019, and \u2018spatial\u2019 information **in each attention head **. This approach differs significantly from [1]. In [1], the self-attention mechanism is also enhanced through the use of $\\mathbf{A}$ and $\\mathbf{D}$ as biases. However, [1] directly adds $\\mathbf{A}$ and $g(\\mathbf{D})$ to the attention-weights $softmax(\\frac{\\mathbf{Q}_l{\\mathbf{K}_l^T}}{\\sqrt{d_k}})$ with global hyperparameters $\\lambda$ which means they can't learn diffrent attention patterns (as dipected in Figure 4 in our manuscript) across different  attention heads.\n2. We have innovatively adopted the $\\vec{\\beta}^{\\max}-\\mathbf{D}$ approach to incorporate the hypothesis that \"the farther the distance, the smaller the interaction\". This approach has shown some improvement in performance in both table 3 and table 4 of our manuscript when compared to the original $\\mathbf{D}$ approach, although not in all cases. It is worth noting that this approach bears some similarity to $g(\\mathbf{D})$ in [1] and $f({\\mathbf{D}}^{-1})^2$ in [2] with $f$ being a neural network.  But in terms of details, the importance we place on \u201cdistance\u201d varies in different places, but [1] is the same everywhere. In addition, [2] may have parameterized different $f$ in each Trans block, but in each block, the importance they focus on  \u201cdistance\u201d for different attention heads is still the same. And we only introduce h (num attention heads) learnable parameters in each block, while [2] introduces a fully connected neural network.\n3. We strictly follow that the attention-weights $\\mathbf{O}_l=\\text{softmax}(\\frac{\\mathbf{S}_l^\\prime}{\\sqrt{d_k}})$ are normalized, but they might not have.\n\nWe wish that the aforementioned explanation angle could enhance our novelty. To summarize, our innovative approach to introduce $\\mathbf{A}$ and $\\mathbf{D}$ into self-attention is that: \n\n In each attention head, we capture different attention patterns by automatically learning the weights among \u201cglobal\u201d, \u201cnearby\u201d, and \u201cspatial\u201d information. The final attention-mask $\\mathbb{E}_n+{\\gamma_l^A\\times{\\mathbf{A}}}+\\gamma_l^D\\times{(\\vec{\\beta}^{\\max}-\\mathbf{D})}$, which is obtained after fusion, is used to adjust the attention-score $\\mathbf{Q}_l{\\mathbf{K}_l^T}$. Then, through softmax, we obtain the final attention-weights $\\text{softmax}(\\frac{\\mathbf{S}_l^\\prime}{\\sqrt{d_k}})$  to weight the node features $\\mathbf{V}_l$. The key point is that the patterns learned in each attention head are different. We don\u2019t need to adjust additional hyperparameters and introduce very few learnable parameters to get performance improvement (as demonstrated in tables 3 and 4). Moreover, it allows for a more interpretable attention map (as depicted in Figure 4)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220382704,
                "cdate": 1700220382704,
                "tmdate": 1700556644176,
                "mdate": 1700556644176,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wwvS2PKhzD",
                "forum": "F7QnIKlC1N",
                "replyto": "lqEhvzchy1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1WnG's Weakness2&3&4"
                    },
                    "comment": {
                        "value": "**Weakness 2:** \n\n> Strange results: I may be wrong, but according to Table 3 (ablation study) a *simple Transformer* architecture without any addition to the self-attention reaches 0.4395 (MAE) which is already better than all the other baselines. This is problematic. Also, the final improvement is only ~1%. Finally, the reported results seem to take the best-ablated model results for each metric, which is wrong.\n\n**Response:**\n\nThere is indeed such a problem, and the simple Transformer structure has indeed achieved good results. However, from an application standpoint, it is also considered an \"innovation\" to use such a structure for the first time in completing this task. What we would like to emphasize more is our focus on the C-RMSD indicator (as mentioned in our manuscript) since it is widely used in the field of bioinformatics to measure the difference between two spatial structures, thus making it more convincing. The D-MAE and D-RMSE indicators are derived from the Molecule3D benchmark. In order to ensure a fair comparison, we still use these two indicators as references. Overall, our proposed structure has demonstrated an improvement of more than 3% in C-RMSD compared to Simple Trans, thereby proving its effectiveness. It is possible that this task is more difficult to optimize, hence we have strongly demonstrated in Table 4 that our proposed module has been successfully applied to molecular property prediction, resulting in significant performance improvements compared to simple Trans.\n\nIn addition, we corrected our careless error reporting results in Table 1 (best model on Molecule3D Random Test, D-MAE: 0.4325; D-RMSE:0.7210; C-RMSD:0.7129).\n\n**Weakness 3 & 4:** \n\n> The advantage of MoleBert over the standard atomic encoding is extremely shallow or even worse.\n>\n> Lack of comparison with other Transformer based methods.\n\n**Response:**\n\nAs with the response to Weakness2, we are more focused on the results on C-RMSD. The introduction of MoleBert indeed brought improvements on C-RMSD, so we retained it as a small innovation.  In addition, we have taken into account the opinions of another reviewer and added a set of comparison ablation experiments in Table 2 for another input format (Orb-style embeddings). The results show that its performance on C-RMSD is far inferior to our method.\n\nWe compared our method with the GPS method in Table 1, which is a Graph Transformer (GT) method  (as mentioned in Sec. 4.3). In Table 6, both SE(3)-Transformer and Equiformer are GT methods. And also MAT in table4."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220653064,
                "cdate": 1700220653064,
                "tmdate": 1700220653064,
                "mdate": 1700220653064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "De9f84aRbd",
                "forum": "F7QnIKlC1N",
                "replyto": "lqEhvzchy1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1WnG's Questions"
                    },
                    "comment": {
                        "value": "**Question 1:** \n\n> One needs to know the capacity of the model in order to assess the origin of the good performance. According to weakness 2, it seems the good performances are obtained *almost solely* from a large/powerful standard Transformer model. From Table 5, the model seems much bigger than other methods. Also, the discrepancies between the tables are disturbing (one single metric should be used for the model validation)\n\n**Response:**\n\nAs for the responses to Weakness 2&3, Pure Trans indeed has brought very good performance. However, as shown in the updated Table 3 and Table 4, our MSRSA module introduces extremely few extra parameters (close to 0%) and achieves 3% and 60% performance improvements on the main task, ground-state conformation prediction, and the transfer task, molecular property prediction, respectively. \n\nHere, we express our apologies. We used wrong calculation results in Table 5 of our previous manuscript. Therefore, we reorganized the performance of the MSRSA module on molecular property prediction in Table 5 and Table 6 in Sec. A.2. The new correct results show that we achieved considerable results on Molecule3D with fast inference time. The results on Qm9 also achieved a medium-to-high level on Gap, Homo, and Lumo. \n\nWhile our $GTMGC_{large}$ does have a relatively large parameter size, it introduces less inductive bias compared to other baselines (bond angles, dihedral angles and so on). Moreover, its inference speed is impressively efficient.\n\n**Question 2:** \n\n> Ablating the initial LPE.\n\n**Response:**\n\nwe add the ablation experiments on LPE  in Table 3 and Table 4. However, the results show that the performance is very poor after removing LPE. This is because PE is an important component in the Transformer network (as is the case in NLP). Therefore, we regard PE as a member of Pure Trans.\n\n**Question 3:** \n\n> It would be beneficial to have a comparison performance with (at least one) other molecule transformer-based methods such as [4,5,6] or others at a similar capacity.\n>\n> [4] Relative molecule self-attention transformer.\n>\n> [5] 3dtransformer: Molecular representation with transformer in 3d space.\n>\n> [6] Geometric transformer for end-to-end molecule properties prediction.\n\n**Response:**\n\nDue to time and resource constraints, we have added a comparison experiment with [2] in Table 4 (MAT). We did not fully reproduce the model in [2], but instead directly replaced the attention formula in our code with MAT to corroborate the response to weakness1.\n\n**Question 4:** \n\n> It would be interesting to look at the learned weighting parameters (\\gamma) of the self-attention to better understand the contribution of each (maybe even adding a weighting to the global attention).\n\n**Response:**\n\nThe learned weights $\\gamma$ are demonstrated in Section A.3.3. The corresponding weights are obtained and displayed by normalizing (1, $\\gamma_l^A$ and $\\gamma_l^D$).\n\nPlease let us know if you have any further questions or concerns. If we have addressed your concerns, we would appreciate it if you could consider increasing the score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220777024,
                "cdate": 1700220777024,
                "tmdate": 1700556612856,
                "mdate": 1700556612856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Aw91nT3751",
                "forum": "F7QnIKlC1N",
                "replyto": "lqEhvzchy1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Urgent Request for Reviewer 1WnG's Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1WnG,\n\nWe apologize for the interruption! We understand that you may be very busy, but we hope that you can take some time to review and respond to our rebuttal. As the rebuttal phase is about to end, we believe that our response can adequately address your doubts and questions to a certain extent, and we have made corresponding modifications based on your suggestions.\n\nWe very much hope to receive your reply, which is very important for the improvement of our paper. If you have any other questions, we hope you can communicate with us again.\n\nLooking forward to your reply.\n\nThank you!\n\nBest regards,\nAuthors of Paper ID 5387"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535770475,
                "cdate": 1700535770475,
                "tmdate": 1700535770475,
                "mdate": 1700535770475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kqpWOoy2pU",
                "forum": "F7QnIKlC1N",
                "replyto": "Aw91nT3751",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5387/Reviewer_1WnG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5387/Reviewer_1WnG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your rebuttal and detailed answers.\n\nWeakness 1: I am aware of the differences in formulation. I meant that the proposed method integrates existing self-attention approaches, which greatly reduces the paper's novelty.\n\nBaselines: The problem with MAT is that it is the worst transformer-based model by very large margins. I suggested a few but there are many other Transformer based works with much better performance. Table 3 shows a very low 3% improvement.\nAlso, Table 6 shows very bad results (in fact almost every existing Transformer-based prediction model would beat it) in molecular property prediction compared to the baselines while having a much higher model capacity.\nGiven Table 6, the new jump in performance in Table 4 is very strange and hard to understand.\n\nQuestion 4: This is a new very interesting analysis. We can observe $\\gamma^D$ is almost always the lowest and close to zero (not sure why they have been normalized). It does not really support the importance of its contribution. How do you explain that?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568965234,
                "cdate": 1700568965234,
                "tmdate": 1700568965234,
                "mdate": 1700568965234,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xxe0kbsP5M",
            "forum": "F7QnIKlC1N",
            "replyto": "F7QnIKlC1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5387/Reviewer_Po8N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5387/Reviewer_Po8N"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes GTMGC, a graph transformer model for ground-state molecular conformation prediction. GTMGC uses a novel self-attention module to achieve effective molecular structure modeling. Experiments show that the proposed GTMGC model achieves state-of-the-art performance in ground-state molecular conformation prediction benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The proposed graph transformer model is novel, with many novel technical contributions in effectively capturing spatial structures by self-attention mechanism.  \nQuality: The effectiveness has been effectively demonstrated by experiments.  \nClarify: The writing and presentation of this paper is good and well-organized.  \nSignificance: The contribution of this work is very useful and meaningful to chemical and molecular biological science fields as the proposed method can significantly accelerate the computation of finding ground-state molecular conformations."
                },
                "weaknesses": {
                    "value": "There are actually many prior studies about formulating the mapping from 2D molecular graphs to 3D molecular conformations as a generative problem. Though they are different from the problem studied in this work, these models can be trained on the used Molecule3D datasets and evaluated by generating only one molecular conformation. However, authors do not compare with any of these methods. Authors are recommended to compare with at least one molecular conformation generation method, such as [1].\n\n[1] Torsional Diffusion for Molecular Conformer Generation. NeurIPS 2022."
                },
                "questions": {
                    "value": "No additional questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5387/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633401901,
            "cdate": 1698633401901,
            "tmdate": 1699636545032,
            "mdate": 1699636545032,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yfNTkyEUlM",
                "forum": "F7QnIKlC1N",
                "replyto": "xxe0kbsP5M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Po8N"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nWe would like to express our sincere gratitude for the time and effort you have dedicated to reviewing our manuscript.\n\n**Weakness 1:** \n\n> There are actually many prior studies about formulating the mapping from 2D molecular graphs to 3D molecular conformations as a generative problem. Though they are different from the problem studied in this work, these models can be trained on the used Molecule3D datasets and evaluated by generating only one molecular conformation. However, authors do not compare with any of these methods. Authors are recommended to compare with at least one molecular conformation generation method, such as [1].\n>\n> [1] Torsional Diffusion for Molecular Conformer Generation. NeurIPS 2022.\n\n**Response:**\n\nIndeed, a substantial amount of contemporary research focuses on using generative models to generate multiple potential conformations with low energy, conditioned on the 2D graph structure of molecules. As you correctly pointed out, Molecule3D could potentially utilize these models for a comparison in single-generation.\n\nHowever, it is unfortunate that the currently effective algorithms for conformation generation, including TorDiff[1], ConfGF[2], GeoDiff[3], among others, mainly rely on score-based models and diffusion models. These models require multiple iterations during the sampling process. For example, GeoDiff necessitates a denoising process of 5000 steps to sample a single instance. This approach is impractical for our dataset, which consists of 700,000 large-scale molecules in the test set. To the best of our knowledge, these generative benchmarks only cover a mere 200 molecules within the test set.\n\n[1] Torsional Diffusion for Molecular Conformer Generation\n\n[2] Learning Gradient Fields for Molecular Conformation Generation\n\n[3] GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation\n\nPlease let us know if you have any further questions or concerns."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219945520,
                "cdate": 1700219945520,
                "tmdate": 1700219945520,
                "mdate": 1700219945520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rzXKdGkm4Z",
                "forum": "F7QnIKlC1N",
                "replyto": "yfNTkyEUlM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5387/Reviewer_Po8N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5387/Reviewer_Po8N"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Response"
                    },
                    "comment": {
                        "value": "I understand that some conformation generation methods, such as ConfGF and GeoDiff, requires thousands of diffusion steps to generate molecular conformations. Nonetheless, some other methods may not require such a large cost. According to Section 4.3 of Torsional Diffusion paper [1], Torsional Diffusion requires only 5~20 diffusion steps for generation, and the runtime of Torsional Diffusion and GeoMol [2] is not high compared with RDKit (Table 2). Do authors think it is computationally practical to run Torsional Diffusion or GeoMol on your datasets?\n\n[1] Torsional Diffusion for Molecular Conformer Generation. NeurIPS 2022.  \n[2] GeoMol: Torsional Geometric Generation of Molecular 3D Conformer Ensembles. NeurIPS 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321594738,
                "cdate": 1700321594738,
                "tmdate": 1700321594738,
                "mdate": 1700321594738,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0HwdezqDf1",
            "forum": "F7QnIKlC1N",
            "replyto": "F7QnIKlC1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5387/Reviewer_BbTi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5387/Reviewer_BbTi"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel graph transformer specifically designed for 3D ground state prediction. Notably, the proposed architecture is versatile and applicable to a wide range of 3D supervised tasks. The key contributions of this work include:\n\nA novel architectural proposal that elegantly extends the classical attention mechanism to 3D molecular graphs. This extension incorporates edge and interatomic distances as biases for the attention mechanism, enhancing its capabilities.\n\nA successful demonstration of the effectiveness of this architecture in the realm of 3D ground state prediction, as well as its application to predict various other 3D molecular properties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\nDespite numerous unsuccessful attempts to construct graph transformers using principles akin to the original transformer, this work stands out as it elegantly achieves the intended goal with minimal architectural complexity. This work avoids unnecessary biases that often detract from the model's effectiveness, making it easier for most researchers to apply their existing intuitions from sequence transformers to this novel architecture.\n\nQuality:\nThe architectural design and its application in ground state conformation prediction are well executed, as reflected in the results, solidifying its position as a favorable solution compared to alternative methods. The iterative refinement of $G_{cache}$ within the decoder represents a notable innovation that enhances model performance in conformer prediction. Ablation studies further clarify the significance of each component within the network, facilitating an understanding of their contributions to this specific modeling task.\n\nClarity:\nThe paper is well-written and maintains a high level of clarity, making it easily comprehensible for readers.\n\nSignificance:\nWhile predicting the ground state of a molecule remains relatively underexplored due to its limited relevance in specific applications, this work serves as a foundational step that can be extended to tackle the broader challenge of full conformer generation. Such an extension holds are very significant, especially in the context of drug discovery."
                },
                "weaknesses": {
                    "value": "The assertion regarding the innovative utilization of the MoleBERT Tokenizer might be overstated, especially in light of the results presented in Table 2. Previous molecular graph papers, such as the MolGPS paper, have explored various atomic featurizations that could potentially outperform the approach presented in this work.\n\nTo allocate more space for related works and experiments, it would be beneficial to consider shortening or omitting certain sections, such as those in the introduction (implementation) and preliminary sections (multi-head and transformer). \n\nThe related work should be integrated into the main text rather than relegated to the appendix. It is crucial to comprehensively cover the various attempts to construct graph transformers and elucidate why they are ill-suited for the tasks at hand."
                },
                "questions": {
                    "value": "Was molecular property prediction approached as a single-task or multi-task endeavor?\n\nCould you clarify the rationale behind placing the molecular property prediction results in the appendix, especially considering that they do not outperform SOTA across the board?\n\nIt could be valuable to assess the scalability of your architecture across various graph sizes, thereby determining where it potentially outperforms existing methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5387/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780196807,
            "cdate": 1698780196807,
            "tmdate": 1699636544931,
            "mdate": 1699636544931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zO6T4GqN4Q",
                "forum": "F7QnIKlC1N",
                "replyto": "0HwdezqDf1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5387/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BbTi"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to express our sincere gratitude for the time and effort you have dedicated to reviewing our manuscript. Your comments and suggestions have been invaluable in improving the quality of our work. In the following sections, we will address each of the points you have raised in your review. We hope that our responses and the revisions we have made will meet your approval.\n\n**Weakness 1:** \n\n> The assertion regarding the innovative utilization of the MoleBERT Tokenizer might be overstated, especially in light of the results presented in Table 2. Previous molecular graph papers, such as the MolGPS paper, have explored various atomic featurizations that could potentially outperform the approach presented in this work.\n\n**Response:**\n\nThank you for your valuable comments! We would like to clarify that our primary focus in this task is the C-RMSD evaluation metric, which is a widely accepted measure in bioinformatics that quantifies the 3D spatial difference between two structures. The D-MAE and D-RMSE metrics, which were derived from the Molecule3D benchmark, were incorporated to facilitate a fair comparison. The introduction of MoleBert, particularly in comparison with atom ids, has significantly enhanced our performance on the C-RMSD metric. We consider this to be a noteworthy innovation in terms of performance improvement. \n\nIn response to your suggestion, we\u2019ve conducted an additional  ablation experiment using Ogb-style embeddings. These embeddings are node features parsed by rdkit and are represented as a vector of length 9, with each element signifying a different atomic property. The outcomes can be found in the updated version of Table 2. While this method of atomic featurization shows good results in terms of D-MAE and D-RMSE, it unfortunately falls short when evaluated against the C-RMSD indicator. Despite this setback, we maintain our belief that the Molebert Tokenized IDs, which we proposed, offer a beneficial approach to predicting molecular conformation.\n\n**Weakness 2:** \n\n> To allocate more space for related works and experiments, it would be beneficial to consider shortening or omitting certain sections, such as those in the introduction (implementation) and preliminary sections (multi-head and transformer).\n>\n> The related work should be integrated into the main text rather than relegated to the appendix. It is crucial to comprehensively cover the various attempts to construct graph transformers and elucidate why they are ill-suited for the tasks at hand.\n\n**Response:**\n\nAccording to your suggestions, we have appropriately discussed some relevant literature in the Introduction section. However, considering the natural flow of the text and the impact of the notation and formulas in the preliminary sections on the subsequent Method chapter, as well as the limited amount of work directly related to our task, ground-state conformation prediction (which is not widely studied), we have still chosen to present it in the Appendix.\n\nNonetheless, we have also included an additional section on the related work of \"graph transformers\". Regarding the feasibility of other GT methods, we mainly emphasize the simplicity and elegance of our structure (as mentioned in the text), which has already demonstrated good performance. We have also compared it with GPS, a kind of GT method, in Table 1.\n\n**Question 1:** \n\n> Was molecular property prediction approached as a single-task or multi-task endeavor?\n\n**Response:**\n\nSingle-task.\n\n**Question 2:** \n\n> Could you clarify the rationale behind placing the molecular property prediction results in the appendix, especially considering that they do not outperform SOTA across the board?\n\n**Response:**\n\nThe purpose is to illustrate that our proposed MSRSA module can be easily migrated to molecular property prediction and achieve remarkable results. Although not outstanding, our structure is simpler and easier to implement.\n\nWe would like to apologize and clarify that the data used in Table 5 of our previous manuscript was incorrectly calculated. As a result, we have made corrections in Sec. A.2 and reorganized our molecular property prediction experiments in Tables 4, 5, and 6. The experimental results show:\n\n1. The ablation study validates the effectiveness of the components of the MSRSA module.\n2. Table 5 demonstrates that our method achieves considerable performance on large-scale macromolecular datasets while maintaining desirable inference speed.\n3. On Qm9, we achieved above-average performance on three properties: Gap, Homo, and Lumo. The performance on the other three properties is not significantly different from other baselines.\n\nThis fully demonstrates that our structure, while simple and elegant, can be quickly transferred to other tasks and achieve good performance.\n\nPlease let us know if you have any further questions or concerns. Thanks again!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5387/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219607305,
                "cdate": 1700219607305,
                "tmdate": 1700219607305,
                "mdate": 1700219607305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]