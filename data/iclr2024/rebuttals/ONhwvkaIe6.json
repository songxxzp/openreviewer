[
    {
        "title": "Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy"
    },
    {
        "review": {
            "id": "5YXcmXJ6fT",
            "forum": "ONhwvkaIe6",
            "replyto": "ONhwvkaIe6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8163/Reviewer_4BDw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8163/Reviewer_4BDw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to evaluate the hypernymy understanding of text-to-image models using the WordNet hierarchy and ImageNet classifiers. The paper introduces two metrics, In-Subtree Probability (ISP) and Subtree Coverage Score (SCS), that measure the generation precision and coverage of the WordNet tree. The paper also compares several popular text-to-image models, such as GLIDE, Latent Diffusion, and Stable Diffusion, using the proposed metrics and analyzes their language understanding capabilities and limitations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces a novel framework for evaluating text-to-image models using the WordNet hierarchy and ImageNet classifiers, which is a unique approach in the field. The work is of high quality, with rigorous methodology and comprehensive evaluation of several popular models. The research is significant as it provides valuable insights into the language understanding capabilities of text-to-image models, which can guide future research in this area."
                },
                "weaknesses": {
                    "value": "The paper could include more diverse models for comparison to provide a more comprehensive evaluation. Meanwhile, the proposed metrics (ISP and SCS) are innovative, but their interpretation and implications could be explained more clearly."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698138663663,
            "cdate": 1698138663663,
            "tmdate": 1699637011323,
            "mdate": 1699637011323,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "65PLooYeC0",
                "forum": "ONhwvkaIe6",
                "replyto": "5YXcmXJ6fT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer 4BDw"
                    },
                    "comment": {
                        "value": "Thank you for your review! Below, you can find our responses to your comments:\n\n> The paper could include more diverse models for comparison to provide a more comprehensive evaluation.\n\nThank you for the suggestion! We included **3 more models** (currently considered state-of-the-art among openly available ones) in the new revision of the paper; please see the **updated PDF and the general response** for details.\n\n> the proposed metrics (ISP and SCS) are innovative, but their interpretation and implications could be explained more clearly\n\nWe explain the motivation behind our metrics in **Section 3** and show how to use them in **Sections 5.1 and 5.2**; in the introduction of the paper, we have tried to outline the need for hypernymy metrics to the best of our ability. If you have **specific questions or suggestions** that would help the reader understand ISP and SCS better, we would be happy to follow them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440581855,
                "cdate": 1700440581855,
                "tmdate": 1700440581855,
                "mdate": 1700440581855,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HGJcJwmins",
                "forum": "ONhwvkaIe6",
                "replyto": "65PLooYeC0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8163/Reviewer_4BDw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8163/Reviewer_4BDw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response! I do not have further questions and keep the rating unchanged."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633056976,
                "cdate": 1700633056976,
                "tmdate": 1700633056976,
                "mdate": 1700633056976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VhNFg2E4pv",
            "forum": "ONhwvkaIe6",
            "replyto": "ONhwvkaIe6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8163/Reviewer_pgJu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8163/Reviewer_pgJu"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents metrics, In-Subtree Probability (ISP) and Subtree Coverage Score (SCS), based on WordNet and ImageNet, for evaluating the hypernymy comprehension capabilities of popular text-to-image models. The proposed method also can be used to provide insights into text-to-image models' limitations for downstream applications. The authors evaluate publicly available models and analyze the hypernymy understanding of existing text-to-image models, validating the effectiveness of their proposed method in this work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This study investigates the problem of assessing the hypernymy understanding of text-to-image models. The authors propose an effective approach that combines the hypernymy knowledge from WordNet with the classification capabilities of established image classifiers. This integration demonstrates a successful application in evaluating the hypernymy understanding of the models.\n\n2. The paper was, in general, easy to follow. In particular, I feel condent that, based on the description, I can reimplement the model and reproduce the results. \n\n3. The proposed evaluation framework and its motivation are reasonable (but see the weakness)."
                },
                "weaknesses": {
                    "value": "1. The efficacy of the proposed method presented in this study is heavily dependent on the performance of the image classifier. Both the In-Subtree Probability (ISP) and Subtree Coverage Score (SCS) metrics rely on the probabilities generated by the classifier. To achieve reliable results, it is crucial to obtain a highly accurate classifier capable of effectively covering a wide range of real-world object classes. The metrics' effectiveness is contingent on the classifier's performance, meaning that if the classifier demonstrates low prediction accuracy, the reliability of the ISP and SCS metrics may be compromised.\n\n2. Pretrained generic large language models (e.g., T5, Llama2), trained on text-only corpora, demonstrate proficiency in text encoding for image synthesis. These models inherently possess semantic understanding and effectively acquire knowledge of hypernymy. More experiments of applying the proposed metrics into text-to-image models equipped with LLMs need to be conducted."
                },
                "questions": {
                    "value": "1. Please consider extending the experiments with more LLMs-equiped text-to-image models, e.g. Imagen (Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding) (but not limited to.)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677476781,
            "cdate": 1698677476781,
            "tmdate": 1699637011195,
            "mdate": 1699637011195,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jjubnzlXJW",
                "forum": "ONhwvkaIe6",
                "replyto": "VhNFg2E4pv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer pgJu"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback and insightful comments! Our responses to your questions and concerns can be found below:\n\n> The efficacy of the proposed method presented in this study is heavily dependent on the performance of the image classifier.\n\nWe agree, which is why **we use an accurate and robust pretrained ImageNet classifier** (ViT-B/16) in our evaluation and in the reference implementation of the metrics we propose. For a detailed discussion about the validity of using pretrained classifiers, please see the **general response**; overall, we argue that this dependence should not be viewed as a disadvantage of our approach.\n\n> Pretrained generic large language models (e.g., T5, Llama2), trained on text-only corpora, demonstrate proficiency in text encoding for image synthesis\n> Please consider extending the experiments with more LLMs-equiped text-to-image models, e.g. Imagen\n\nThank you for this suggestion! We would like to note that **Imagen is not publicly available**; therefore, it is not possible to run experiments on this model. Moreover, large language models **do not necessarily have much better hypernymy knowledge**: as shown in [1], models like ChatGPT can fall behind BERT, CLIP, or other smaller neural networks in terms of hypernym discovery, and GPT embeddings are only marginally better when compared to CLIP (which is used in the majority of the models that we study).\n\nStill, to address your concern, we conducted experiments on DeepFloyd IF XL (which has **T5-XXL as its backbone**) and included results in the updated revision of the paper. We kindly ask you to see the new versions of **Tables 1, 9, and 10** and tell us if these results address your concern. In the camera-ready version, we will improve the fidelity metrics of DeepFloyd with super-resolution (that was harder to run within the limits of the response period), but other results should remain similar.\n\n[1] Concept Understanding in Large Language Models: An Empirical Study. Liao et al., ICLR 2023 Tiny Papers track."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440508750,
                "cdate": 1700440508750,
                "tmdate": 1700440508750,
                "mdate": 1700440508750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9WHyhFsUjZ",
                "forum": "ONhwvkaIe6",
                "replyto": "jjubnzlXJW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8163/Reviewer_pgJu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8163/Reviewer_pgJu"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response! Glad to see the experimental results of those models equipped with LLM. I have no further questions and keep the rating unchanged."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669793505,
                "cdate": 1700669793505,
                "tmdate": 1700669793505,
                "mdate": 1700669793505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CxpRcjxrf6",
            "forum": "ONhwvkaIe6",
            "replyto": "ONhwvkaIe6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8163/Reviewer_npm8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8163/Reviewer_npm8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to evaluate the hypernymy understanding abilities of text-to-image models, including two distinct and complementary metrics. The evaluation process is fully automated. The authors also show some merits of the evaluation framework such as finding unknown concepts and conducting granular comparison of models. Overall, this paper presents a focused analysis on a specific aspect of text-to-image generation (i.e., hypernymy understanding)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This experiments presented in this paper are comprehensive and elaborate.\n- The evaluation metrics are automated and could be useful for future research."
                },
                "weaknesses": {
                    "value": "- This paper focuses on a very specific aspect of text-to-image generation.\n- The proposed evaluation framework relies on a well-trained image classifier. It performance depends on the accuracy and coverage of the image classifier. Particularly, its usefulness may be limited by the coverage of existing image classifiers.\n- The results of some experiments  (e.g., the influence of the classifier-free guidance scale, the number of diffusion steps, and the number of generated samples) seem to be self-evident and provide little new insights."
                },
                "questions": {
                    "value": "- For Subtree Coverage Score, why not use simpler formula such as the entropy of the average distribution.\n- Do you anticipate any more applications of the proposed evaluation framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8163/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8163/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8163/Reviewer_npm8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698922645524,
            "cdate": 1698922645524,
            "tmdate": 1700623601567,
            "mdate": 1700623601567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KP5savs8le",
                "forum": "ONhwvkaIe6",
                "replyto": "CxpRcjxrf6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer npm8"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work and for your feedback! Please find our answers to your concerns below:\n\n> This paper focuses on a very specific aspect of text-to-image generation.\n\nWhile we agree that we study a particular aspect of generation abilities, we believe that the **hypernymy relation is a sufficiently abstract concept** that can be broadly useful. As we notice in the paper, there are multiple applications of hypernymy that encompass different questions faced by text-to-image generation practitioners. Moreover, as shown by prior work [1], **even specific aspects could be of significant interest** to the community. Our metrics offer a general foundation for a more semantics-driven approach to evaluation, which makes it possible for the research community to explore nuances not captured by existing methods.\n\n> The proposed evaluation framework relies on a well-trained image classifier. It performance depends on the accuracy and coverage of the image classifier. Particularly, its usefulness may be limited by the coverage of existing image classifiers.\n\nThank you for this observation! We believe that the existence of many **high-quality and robust pretrained ImageNet classifiers** makes our approach viable; for more details, see our **general response**.\n\n> The results of some experiments (e.g., the influence of the classifier-free guidance scale, the number of diffusion steps, and the number of generated samples) seem to be self-evident and provide little new insights.\n\n**These experiments were designed to have unsurprising results:** their goal is to show that our metrics **behave as expected** when sampling hyperparameters are changed and that the default hyperparameters we use for evaluation provide sensible results. In other words, these experiments are **necessary to validate the design choices** of our evaluation protocol, which, in our opinion, should be helpful for intended users of ISP and SCS.\n\n> For Subtree Coverage Score, why not use simpler formula such as the entropy of the average distribution.\n\nThank you for this question! In general, our motivation is the same as in the Inception Score paper: using the entropy of the average distribution **does not allow us to distinguish** between diverse images of specific objects and images covering mixed objects in one sample. More concretely, we have measured the entropy of the average distribution for setups from Section 4.3 and added its correlation with human preferences to **Appendix H**. As you can see there, measuring entropy in each synset has lower agreement than SCS.\n\n> Do you anticipate any more applications of the proposed evaluation framework?\n\nApart from **3 applications that we already show in the paper** (finding unknown/low-diversity concepts, conducting pairwise comparisons, and running per-subtree evaluation), we think it is also possible to leverage the hypernymy relation in the following ways:\n\n1. We can derive a new measure of similarity between two synsets by dividing the total probability of the intersection of classifiable subtrees by the probability of their union. Intuitively, \u201ccanine\u201d and \u201cdog\u201d should be considered similar, whereas \u201canimal\u201d and \u201cdog\u201d will be less similar due to \u201canimal\u201d containing additional concept subtrees.\n2. Similarly, one can compare how two models \u201cview\u201d the same synset by considering its classifiable subtree and computing the Jensen-Shannon divergence (or any other symmetric distribution similarity measure) between hyponym distributions induced by these models.\n3. We are able to evaluate the multilingual capabilities of models out of the box by employing multilingual WordNet variations [2].\n\n[1] Benchmark for Compositional Text-to-Image Synthesis. Park et al., NeurIPS 2021 Datasets and Benchmarks\n\n[2] Open Multilingual WordNet. https://omwn.org/"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440398528,
                "cdate": 1700440398528,
                "tmdate": 1700440398528,
                "mdate": 1700440398528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SorJGZjGnZ",
                "forum": "ONhwvkaIe6",
                "replyto": "CxpRcjxrf6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8163/Reviewer_npm8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8163/Reviewer_npm8"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thoughtful response.\n\nI raise my score from 5 to 6. I would like the ACs to know that (1) overall, I think this paper is a solid work and (2) I just doubt its impact on our research community."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623561411,
                "cdate": 1700623561411,
                "tmdate": 1700623576835,
                "mdate": 1700623576835,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]