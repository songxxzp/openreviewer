[
    {
        "title": "SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity"
    },
    {
        "review": {
            "id": "39kLvY3eDg",
            "forum": "lK4QHgjUU8",
            "replyto": "lK4QHgjUU8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3205/Reviewer_4Z6k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3205/Reviewer_4Z6k"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a more general solution to reduce variance for score distillation, termed Stein Score Distillation (SSD). SSD incorporates control variates constructed by Stein identity, allowing for arbitrary baseline functions. The experiment results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written.\n2. The experiments show that the results are better than the baselines.\n3. This work proposes to rethink the SDS/VSD in the way of variance, which is interesting.\n4. The idea is novel."
                },
                "weaknesses": {
                    "value": "1. My main concern is that why a lower variance during optimization is helpful for the final quality? Although there are some empirical results in Sec 3 show that there are some corelation between variance and generated quality, I think a more convincing justification should be given. Maybe a more detailed theoretical or intuitive explanation should be given.\n2. No quantitative results are given to compare the proposed method and baselines in terms of the visual quality."
                },
                "questions": {
                    "value": "1.  How will the baseline function effects the final results? Since the Stein identity always holds, what is the relationship between the baseline function and the final performance?\n2. Can you show a 2D experiment? (Using SSD to directly optimize an image.) This will strengthen the effectiveness of SSD."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3205/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3205/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3205/Reviewer_4Z6k"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698579776750,
            "cdate": 1698579776750,
            "tmdate": 1699636268811,
            "mdate": 1699636268811,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "paGzszRP7k",
                "forum": "lK4QHgjUU8",
                "replyto": "39kLvY3eDg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4Z6k"
                    },
                    "comment": {
                        "value": "We thank Reviewer 4Z6k for appreciating our theoretical reevaluation of SDS and VSD. Regarding your questions, please see our detailed responses below:\n\n**1. Why a lower variance during optimization is helpful for the final quality?**\n\nIn optimization theory, variance plays a crucial role in determining the convergence rate of SGD algorithms (refer to Theorem 5.3 in [1]). With a finite number of optimization steps and a standard learning rate, maintaining low variance is pivotal to ensure convergence. Training with noisy gradients introduces high instability, potentially resulting in suboptimal solution or even divergence, as illustrated in Fig. 4(a).\n\n[1] Garrigos & Gower, Handbook of Convergence Theorems for (Stochastic) Gradient Methods\n\n**2. Quantitative results.**\n\nWe present a qualitative comparison among DreamFusion, ProlificDreamer, and SteinDreamer across all showcased scenes, utilizing the CLIP score as our metric. \n\n| method              |   tulip |   sushi car |   lionfish |\n|:--------------------|--------:|------------:|-----------:|\n| DreamFusion      |   0.777 |       0.862 |      0.751 |\n| ProlificDreamer     |   0.751 |       0.835 |      0.749 |\n| SteinDreamer  |   0.734 |       0.754 |      0.735 |\n\n| method              |   cheesecake castle |   dragon toy |   michelangelo dog |\n|:--------------------|--------------------:|-------------:|-------------------:|\n| DreamFusion      |               0.902 |        0.904 |              0.789 |\n| ProlificDreamer     |               0.843 |        0.852 |              0.775 |\n| SteinDreamer |               0.794 |        0.806 |              0.769 |\n\nOur observations indicate that SteinDreamer consistently outperforms all other methods, which improves CLIP score by ~0.5 over ProlificDreamer. This superior result suggests our flexible control variates are more effective than the one adopted by ProlificDreamer.\n\n\n**3. The relationship between the baseline function and the final performance.**\n\nVarious baseline functions can yield differing correlations between the constructed control variate and the target random variable for estimation. As outlined in Section 2.2, it is advisable to employ a baseline function that exhibits a stronger correlation with the 2D diffusion model to minimize variance. Typically, we notice that baseline functions with a more robust prior tend to demonstrate higher correlations. From another perspective, the baseline function $\\phi$ can be regarded as a guidance introduced into the distillation process. Intuitively, enforcing priors and constraints on the gradient space can also stabilize the training process by regularizing the optimization trajectory. In our empirical design, the inclusion of geometric information expressed by a pre-trained MiDAS estimator is expected to result in superior variance reduction compared to SSD and VSD."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701229811,
                "cdate": 1700701229811,
                "tmdate": 1700701229811,
                "mdate": 1700701229811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N1ByZO8j1Q",
            "forum": "lK4QHgjUU8",
            "replyto": "lK4QHgjUU8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3205/Reviewer_XeQE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3205/Reviewer_XeQE"
            ],
            "content": {
                "summary": {
                    "value": "This work propose a novel regularization term to help SDS achieve low variance, named Stein Score Distillation (SSD). The proposed SSD is based on Stein's identity, which is natually zero-means to serve as variance control. The method starts from the insights that lower variance produces better performance with emprical results from DreamFusion and ProfilicDreamer. The experimental results demonstrate better shape, texture, and details on object and scene level generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- originality: the proposed method for text-to-3d generation is relatively novel, with inspiration from Stein's identity to reduce the variance in  the training process of text-to-3d.\n- quality: this work starts from the emprical insights and combine with the proper mathematical solution to promote text-to-3d application. The experimental results outperform the previous reprsentative works, such as DreamFusion and ProfilicDreamer.\n- clarity: the presentation of this work is good, with clear formluation and structure organization. It is reader friendly.\n- significance: this work is of significance, especially in an age of AIGC."
                },
                "weaknesses": {
                    "value": "- The experimental results are not as extensive as ProfilicDreamer. Compared with 10 object and 8 sence level generated content on https://ml.cs.tsinghua.edu.cn/prolificdreamer/, there are only 6 object and 4 sence level in this work and supplementary demo.\n- Looking at the Figure 5, there existing the following confusions: 1) DreamFusion seems not converge for some cases, especially for \"A Car made out of Sushi.\" and \"A Lion Fish\". Maybe DreamFusion needs more iterations? 2) The proposed SteinDreamer can be overfitting after ~120k training steps, with gradually increasing variance. How could this happen?\n- Also, for comparing the convergence speed, it is much better to compare when all the methods are fully converged, especially for DreamFusion and ProfilicDreamer.\n- Possible typos: 4th para. in Inroduction, 'aligns with with that' --> 'aligns with that'"
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3205/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3205/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3205/Reviewer_XeQE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738039629,
            "cdate": 1698738039629,
            "tmdate": 1699636268716,
            "mdate": 1699636268716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f4PMz6v9EP",
                "forum": "lK4QHgjUU8",
                "replyto": "N1ByZO8j1Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XeQE"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer XeQE for recognizing our motivation and the clarity of our mathematical exposition. We have fixed all the typos and addressed your concerns as follows.\n\n**1. More experiment results.**\n\nWe additionally showcase four qualitative results on scene generation in Appendix C.4. Aligning with our prior empirical observations, our method consistently produces sharp and visually coherent results.\n\n**2. DreamFusion results seem not converged.**\n\nEvery qualitative result, including DreamFusion's, was obtained after 25k training steps, which follows the standard training configuration in the threestudio repository. To be more convincing, we train DreamFusion with the prompt \"a car made out of sush\" for an additional 10k steps, there were no observable improvements in the results. See our Appendix C.3 for more details.\n\n\n**3. Why does the variance of SteinDreamer gradually increase after 12k steps.**\n\nAs depicted in Fig. 5, not all scenarios witness an increase in variance, as seen in examples like 'a blue tulip' and 'a plush dragon toy'. We emphasize that the optimization trajectory of score distillation exhibits high non-smoothness, leading to a typical fluctuation phenomenon in variance. Even in instances where variance appears to rise, its final value remains upper bounded, ensuring the convergence of our results. In contrast, other methods, such as ProlificDreamer with the prompt 'a plushy dragon toy', display potential variance explosion, evident in the training instability depicted in Fig. 4(a).\n\n**4. Compare convergence after more training steps.**\n\nWe express our apologies for the mistake in Fig. 7, where the maximum of x-axis was incorrectly set to 100 instead of the intended 25k steps. This initial error occurred as we mistakenly used the indices of data points for the x-axis. We have rectified this issue in our revised version. It is important to note that 25k is the default setting suggested by threestudio, typically resulting in converged results with ProlificDreamer. To verify this claim, we train ProlificDreamer for an additional 10k steps. Both visual quality and the CLIP score exhibit negligible changes. See Appendix C.3 for more details."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701046958,
                "cdate": 1700701046958,
                "tmdate": 1700701128541,
                "mdate": 1700701128541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UbSaYHhTCu",
            "forum": "lK4QHgjUU8",
            "replyto": "lK4QHgjUU8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3205/Reviewer_D8i1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3205/Reviewer_D8i1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a variance reduction method for training text-to-3d synthesis models based on pretrained diffusion models. The authors first point out that prior work in this domain share the the same gradient in expectation, but differs in the Monte Carlo estimator. Then, they show that the performance difference can be explained by the variance of the estimator. Given this observation, they proposes a new variance reduction technique based on Stein's identity that generalizes the estimator in a previous work (SSD). The new estimator is shown to have lower variance and improve the visual quality of 3d scenes in experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed method is well-motivated. Given the observation that prior gradient estimators have the same expectation and the performance is highly dependent on variance, it is very sensible to investigate better variance reduction techniques for this approach.\n\n* The method is shown to have less variance than prior methods in experiments. \n\n* The generated 3d scenes are visually better than prior methods (e.g., in Figure 3)."
                },
                "weaknesses": {
                    "value": "* Introducing another network for variance reduction increases the training cost per iteration, which might outweigh the benefit brought by having lower variance per iteration. It would be more convincing to include a wall-clock time comparison between different methods. \n\n* One biggest weakness of this work is the insufficient empirical evaluation. The variance plot is one face of the story. However, it would greatly strengthen the work if the authors can show the 3d synthesis model learned by SSD is quantitatively better than the baselines given the same compute. In Figure 7, the improvement is very marginal considering the additional cost of training the control variates.\n\n* The work can be positioned better in the literature. Sticking the landing  is not cited. The citations on Stein's identity can be improved. The generalized form of the Stein's identity in (7) is first introduced in Gorham & Mackey (2015).\n\n\nRef: \n[1] Roeder, G., Wu, Y., & Duvenaud, D. K. (2017). Sticking the landing: Simple, lower-variance gradient estimators for variational inference. Advances in Neural Information Processing Systems, 30.\n\n[2] Gorham, Jackson, and Lester Mackey. \"Measuring sample quality with Stein's method.\" Advances in neural information processing systems 28 (2015)."
                },
                "questions": {
                    "value": "I would potentially raise the score if the wall-clock time comparison is included."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698867766587,
            "cdate": 1698867766587,
            "tmdate": 1699636268630,
            "mdate": 1699636268630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h0cwLprDCi",
                "forum": "lK4QHgjUU8",
                "replyto": "UbSaYHhTCu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D8i1"
                    },
                    "comment": {
                        "value": "We are grateful to Reviewer D8i1 for recognizing our motivation. We have conducted benchmarking of the wall-clock time and provided detailed per-point answers below.\n\n**1. Wall-clock time comparison between different methods.**\n\nBelow, we list the average per-iteration wall-clock time for all methods, each benchmarked using the same GPU device with 10k training steps.\n\n| Method   | Per-iteration Time (s)  |\n| -------- | ------- |\n| DreamFusion  | 1.063 $\\pm$ 0.002    |\n| ProlificDreamer | 1.550 $\\pm$ 0.004     |\n| SteinDreamer    | 1.093 $\\pm$ 0.005    |\n\n\nIn summary, SteinDreamer exhibits comparable per-iteration speed to DreamFusion while significantly outperforming ProlificDreamer in terms of speed. The trainable component $\\mu$ in SSD comprises only thousands of parameters, which minimally increases computational overhead and becomes much more efficient than tuning a LoRA in VSD. Notably, given that SSD can reach comparable visual quality in fewer steps, SteinDreamer achieves significant time savings for 3D score distillation.\n\n**2. Quantitative comparison with other methods.**\n\nWe present a qualitative comparison among DreamFusion, ProlificDreamer, and SteinDreamer across all showcased scenes, using the CLIP score as our metric. \n\n| method              |   tulip |   sushi car |   lionfish |\n|:--------------------|--------:|------------:|-----------:|\n| DreamFusion      |   0.777 |       0.862 |      0.751 |\n| ProlificDreamer     |   0.751 |       0.835 |      0.749 |\n| SteinDreamer  |   0.734 |       0.754 |      0.735 |\n\n| method              |   cheesecake castle |   dragon toy |   michelangelo dog |\n|:--------------------|--------------------:|-------------:|-------------------:|\n| DreamFusion      |               0.902 |        0.904 |              0.789 |\n| ProlificDreamer     |               0.843 |        0.852 |              0.775 |\n| SteinDreamer |               0.794 |        0.806 |              0.769 |\n\nOur observations indicate that SteinDreamer consistently outperforms all other methods, demonstrating an approximate 0.5 improvement in CLIP distance over ProlificDreamer.\n\n**3. Missing citations.**\n\nThank you for highlighting these two works. We have updated our citation to include mention of these references.\n\n[1] Roeder et al. Sticking the landing: Simple, lower-variance gradient estimators for variational inference. NeurIPS 2017.\n\n[2] Gorham et al. Measuring sample quality with Stein's method. NeurIPS, 2015."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700884423,
                "cdate": 1700700884423,
                "tmdate": 1700700884423,
                "mdate": 1700700884423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8G2I9n1rBK",
            "forum": "lK4QHgjUU8",
            "replyto": "lK4QHgjUU8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3205/Reviewer_HqJT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3205/Reviewer_HqJT"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an interpretation of score distillation sampling by integrating the Stein identity. This interpretation underscores the significance of selecting appropriate control variates for reducing variance to achieve efficient convergence during training. The authors propose the use of a monocular depth estimator for efficiency and demonstrate that their method effectively reduces variance during updates, leading to faster convergence compared to other score distillation sampling techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to understand.\n-The adoption of a monocular depth estimator is a simple yet effective approach to reducing variance in text-to-3D synthesis."
                },
                "weaknesses": {
                    "value": "**Limited novelty**\n- It is noted that the primary focus of this paper is to interpret score distillation sampling using the Stein identity and to reduce variance through the incorporation of monocular depth estimation. However, Kim et al [1] have already discussed a similar interpretation of the Stein identity and the importance of baseline function selection for convergence and efficiency. Although the specific focus of Kim et al. is different (visual editing), the core inspiration appears to be similar. Therefore, it is recommended for the authors to acknowledge and discuss the already incorporated findings of Kim et al. In this context, while the inclusion of monocular depth estimation for efficient text-to-3D synthesis is intriguing, the novelty of the proposed method may be considered weak. Thus, given the similarities in findings and interpretation, the distinguishing factor appears to be only the choice of baseline functions for specific tasks, the adoption of a monocular depth estimator as the control variate. \n\n**Lack of Quantitative Evaluation**\n- To support their claims of qualitative improvement over Score Distillation Sampling (SDS) and Variational Score Distillation (VSD), the authors should provide a quantitative evaluation. Although the video demonstration provided by the authors indicates some improvement, issues such as the \"Janus problem\" in the dog statue are still present. While Figure 5 does demonstrate a reduction in variance compared to other methods, it does not necessarily guarantee improved quality. Figure 5 suggests that SSD reduces variance during training when compared to other methods, which may lead to fewer artifacts at similar training iterations, especially in comparison to ProlificDreamer. However, quality improvements are not guaranteed after fully training ProlificDreamer until convergence, as fine-tuning the diffusion model may require more computational time. Hence, it is recommended that the authors claim that the proposed method effectively reduces variance during training, which ensures better quality at an \u201cearly stage\u201d not overall quality improvements. This concept is also supported by Figure 7 (although additional figures may be necessary for validation, just one example is severely not enough), clearly indicating that the reduced variance achieved by the proposed method results in faster convergence rather than overall quality improvement.\n- Additionally, the comparison in the paper appears to be based on a limited number of samples. It is beneficial to conduct experiments with a broader range of prompts to assess the robustness of the proposed method in reducing variance.\n\n[1] Kim et al., Collaborative Score Distillation for Consistent Visual Editing. NeurIPS 2023."
                },
                "questions": {
                    "value": "As demonstrated in the Weaknesses section, the authors should make a comparison with the prior work of Kim et al. and discuss any differences if there exists new interpretation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3205/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699354787024,
            "cdate": 1699354787024,
            "tmdate": 1699636268542,
            "mdate": 1699636268542,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qRuCkO5eKD",
                "forum": "lK4QHgjUU8",
                "replyto": "8G2I9n1rBK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3205/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3205/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HqJT"
                    },
                    "comment": {
                        "value": "We thank Reviewer HqJT for the time and effort in reviewing our paper. Per your questions, please see our response below:\n\n**1. Lack of novelty and idea coincides Kim et al.**\n\nBefore submission, we were aware of Kim et al.'s work [1]. However, we respectfully disagree regarding the similarity of their method to ours. While both methods are grounded in Stein's method, the underlying principles significantly differ. In their approach [1], the SVGD-based update takes the form of the Stein discrepancy: $\\max_{\\phi \\in \\mathcal{F}} \\mathbb{E}\\_{x\\sim q(x)} [\\phi(x) \\nabla \\log p(x) + \\nabla \\phi(x)]$, where $\\phi(x)$ is often interpreted as an update direction constrained by a function class $\\mathcal{F}$ (RBF RKHS in [1]). In contrast, our update rule appends a zero-mean random variable via the Stein identity after the raw gradient of the KL divergence: $\\mathbb{E}_{x \\sim q(x)} [\\nabla \\log p(x) + \\phi(x) \\nabla \\log q(x) + \\nabla \\phi(x)]$, where $\\phi(x)$ typically represents a pre-defined baseline function. The potential rationale behind [1]'s approach to reducing variance lies in introducing the RBF kernel as a prior to constrain the solution space by modeling pairwise relations between data samples. Our approach to reducing variance is centered around constructing a more general control variate that correlates with the random variable of interest, featuring zero mean but variance reduction. We have cited [1] and included the above discussion in our revised submission (Sec. 4.3).\n\n[1] Kim et al., Collaborative Score Distillation for Consistent Visual Editing. NeurIPS 2023.\n\n\n**2. Clarification on training steps and quantitative results.**\n\nThe qualitative results presented in this study were generated using a default setting of 25k training steps, consistent with the threestudio repo's default configuration. An error was identified in the original Fig. 7, where the x-axis required scaling to 25k due to our oversight in using the number of data points as the x-axis reference. We have rectified this error in the updated Fig. 7. To verify 25k is sufficient for convergence, we extended the training of ProlificDreamer by an additional 10k steps. During this extension, the CLIP score ranged between 0.74 and 0.75. Furthermore, a quantitative comparison is provided below.\n\n\n| method              |   tulip |   sushi car |   lionfish |\n|:--------------------|--------:|------------:|-----------:|\n| DreamFusion      |   0.777 |       0.862 |      0.751 |\n| ProlificDreamer     |   0.751 |       0.835 |      0.749 |\n| SteinDreamer  |   0.734 |       0.754 |      0.735 |\n\n| method              |   cheesecake castle |   dragon toy |   michelangelo dog |\n|:--------------------|--------------------:|-------------:|-------------------:|\n| DreamFusion      |               0.902 |        0.904 |              0.789 |\n| ProlificDreamer     |               0.843 |        0.852 |              0.775 |\n| SteinDreamer |               0.794 |        0.806 |              0.769 |\n\nThe notably superior performance suggests that our method not only enhances quality at the early stages but also improves overall quality with full training.\n\n**3. Correction on Janus results.**\n\nWe apologize for the inclusion of an incorrect video demo in the previous supplementary material, an oversight that occurred during the manuscript preparation rush. The updated supplementary material now accurately demonstrates that our method can potentially mitigate the Janus problem, although it is not the primary goal of this paper.\n\n**4. The number of examples is limited.**\n\nTo enhance our experiment results, we present an additional set of four qualitative results on scene generation in Appendix C.4. As previously noted, our method consistently delivers smooth and visually consistent outcomes. Their video demos can be found in our updated supplementary materials."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3205/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700772487,
                "cdate": 1700700772487,
                "tmdate": 1700701098840,
                "mdate": 1700701098840,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]