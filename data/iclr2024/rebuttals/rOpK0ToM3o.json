[
    {
        "title": "V-Former: Offline RL with Temporally-Extended Actions"
    },
    {
        "review": {
            "id": "3T2pGJT4i6",
            "forum": "rOpK0ToM3o",
            "replyto": "rOpK0ToM3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6695/Reviewer_ZdXy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6695/Reviewer_ZdXy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a transformer-based offline RL method. It first introduces an \"implicit V-learning\" algorithm (similar to IQL) that can be extended to multiple timesteps. It then learns a transformer-based policy via a weighted behavior cloning objective, where the weight depends on the temporally extended learned value function. The method is evaluated on several continuous control benchmarks including the Robomimic and Franka Kitchen."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**originality**: Although the idea of \"implicit V-learning\" has been used in several prior works, this paper proposes to extend the learning objective to multiple timesteps. Moreover, the combination of IVL and transformer-based policy learning is novel.\n\n**clarity**: I find this paper generally well written and easy to follow."
                },
                "weaknesses": {
                    "value": "My main concern is on the experiments, which I don't find sufficient enough to demonstrate the strength of the proposed method as an offline RL algorithm. \n\nFirst, the evaluation was conducted against some variants of the proposed method (which seems more like ablation studies to me) but didn't consider any existing offline RL baselines (which I don't see any limitations in the settings that prevent one from doing so). \n\nSecond, the experiments were conducted only on expert datasets, and *suboptimal datasets* (expert + random data) which were a bit artificial. While it's known that the performance of offline RL / imitation learning methods varies drastically depending on the data quality, it's important to evaluate the method on datasets of various optimality, and especially on those which are similar to real-world settings, e.g. the multi-human datasets from Robomimic. \n\nLastly, the proposed method seems sensitive (task depedent) to certain critical hyperparameters, including the \"action chunk size\". But experiments only cover a small range of those parameters. I believe more extensive ablation studies would be helpful to show if the method is robust and generally applicable to various continuous control problems."
                },
                "questions": {
                    "value": "1. Why not including established baselines, e.g. BC-RNN, BCQ, CQL (which were used as baselines in the Robomimic paper), and transformer-based baselines like Decision Transformer and Trajectory Transformer?\n\n2. How are the success rates in Fig 3 normalized? Why are some greater than 1?\n\n3. The expert + random datasets seem a bit artificial. Why not evaluating on the existing multi-human datasets (which were generated by human operators of different level of proficiency on the tasks) from Robomimic instead?\n\n4. What's your intuition on selecting an optimal range of N?\n\n5. As noted in the appendix, different weight functions f(x) were used in the Robomimic and FrankaKitchen experiments. It would be nice to include an ablation table for both f(x) on both environments to show how sensitive the method is to f(x). \n\n6. Have you tried evaluating the method on tasks with discrete action spaces, e.g. maze?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6695/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6695/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6695/Reviewer_ZdXy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698328367583,
            "cdate": 1698328367583,
            "tmdate": 1700683733617,
            "mdate": 1700683733617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hHKgAUMxPo",
                "forum": "rOpK0ToM3o",
                "replyto": "3T2pGJT4i6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough review and constructive feedback about this work. Below, we describe how we have added an additional comparison with $3$ existing offline RL baselines, new results on different types of suboptimal datasets, and a new ablation study with different action chunk sizes. We believe that these changes strengthen the paper, and welcome additional suggestions for further improving the work.\n\n* **$\\mathbf{3}$ new baselines - CQL, IQL, and DT**\n\nThank you for the suggestion. We would like to first note that we have (already) included comparisons with three existing offline RL and BC methods, (1) BC, (2) BC+Transformer, and (3) IQL (which correspond to $BC (1, 1)$, $BC (3, 3)$, and $VF (1, 1)$, respectively (Tables 1, 2, Fig. 3); these methods use the same action discretization as V-Former). However, following the suggestion, below, we compare V-Former with $\\mathbf{3}$ additional existing offline RL methods: **CQL**, **(Original) IQL**, and **Decision Transformer**.\n\n**(1) Results on expert (PH) datasets:**\n| Task | BC | BC+Transformer | IQL | CQL | (Original) IQL | Decision Transformer | V-Former (ours) |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $90.0$ $\\tiny{\\pm 3.5}$ | $91.3$ $\\tiny{\\pm 9.0}$ | $92.7$ $\\tiny{\\pm 6.1}$ | $10.0$ $\\tiny{\\pm 11.1}$ | $37.3$ $\\tiny{\\pm 9.5}$ | $93.3$ $\\tiny{\\pm 1.2}$ | $\\mathbf{95.3}$ $\\tiny{\\pm 3.1}$ |\n| $\\texttt{lift}$ | $88.7$ $\\tiny{\\pm 9.5}$ | $94.7$ $\\tiny{\\pm 2.3}$ | $94.7$ $\\tiny{\\pm 4.2}$ | $70.7$ $\\tiny{\\pm 11.5}$ | $74.0$ $\\tiny{\\pm 8.7}$ | $\\mathbf{98.0}$ $\\tiny{\\pm 2.0}$ | $93.3$ $\\tiny{\\pm 3.1}$ |\n| $\\texttt{square}$ | $62.7$ $\\tiny{\\pm 5.0}$ | $\\mathbf{78.0}$ $\\tiny{\\pm 7.2}$ | $65.3$ $\\tiny{\\pm 8.1}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $31.3$ $\\tiny{\\pm 9.0}$ | $30.7$ $\\tiny{\\pm 6.4}$ | $67.3$ $\\tiny{\\pm 1.2}$ |\n| $\\texttt{tool\\\\_hang}$ | $14.7$ $\\tiny{\\pm 8.3}$ | $36.7$ $\\tiny{\\pm 8.1}$ | $14.7$ $\\tiny{\\pm 5.8}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $6.0$ $\\tiny{\\pm 4.0}$ | $4.0$ $\\tiny{\\pm 2.0}$ | $\\mathbf{38.0}$ $\\tiny{\\pm 7.2}$ |\n| $\\texttt{transport}$ | $22.7$ $\\tiny{\\pm 3.1}$ | $\\mathbf{35.3}$ $\\tiny{\\pm 8.1}$ | $23.3$ $\\tiny{\\pm 5.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $2.0$ $\\tiny{\\pm 0.0}$ | $2.0$ $\\tiny{\\pm 2.0}$ | $32.0$ $\\tiny{\\pm 7.2}$ |\n| **average** | $55.7$ | $\\mathbf{67.2}$ | $58.1$ | $16.1$ | $30.1$ | $45.6$ | $65.2$ |\n\n**(2) Results on suboptimal (mixed) datasets:**\n| Task | BC | BC+Transformer | IQL | CQL | (Original) IQL | Decision Transformer | V-Former (ours) |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $52.0$ $\\tiny{\\pm 7.2}$ | $59.3$ $\\tiny{\\pm 10.3}$ | $54.7$ $\\tiny{\\pm 10.3}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $10.7$ $\\tiny{\\pm 8.3}$ | $33.3$ $\\tiny{\\pm 17.5}$ | $\\mathbf{66.7}$ $\\tiny{\\pm 5.8}$ |\n| $\\texttt{lift}$ | $50.0$ $\\tiny{\\pm 9.2}$ | $58.7$ $\\tiny{\\pm 4.2}$ | $58.7$ $\\tiny{\\pm 9.9}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $26.0$ $\\tiny{\\pm 4.0}$ | $60.0$ $\\tiny{\\pm 3.5}$ | $\\mathbf{70.0}$ $\\tiny{\\pm 15.6}$ |\n| $\\texttt{square}$ | $37.3$ $\\tiny{\\pm 9.5}$ | $\\mathbf{66.0}$ $\\tiny{\\pm 8.0}$ | $47.3$ $\\tiny{\\pm 5.8}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $18.0$ $\\tiny{\\pm 6.0}$ | $6.0$ $\\tiny{\\pm 7.2}$ | $\\mathbf{66.0}$ $\\tiny{\\pm 3.5}$ |\n| $\\texttt{tool\\\\_hang}$ | $11.3$ $\\tiny{\\pm 5.0}$ | $28.0$ $\\tiny{\\pm 14.4}$ | $15.3$ $\\tiny{\\pm 4.2}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $\\mathbf{28.7}$ $\\tiny{\\pm 7.0}$ |\n| $\\texttt{transport}$ | $8.0$ $\\tiny{\\pm 2.0}$ | $16.0$ $\\tiny{\\pm 6.9}$ | $14.7$ $\\tiny{\\pm 5.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $\\mathbf{25.3}$ $\\tiny{\\pm 2.3}$ |\n| **average** | $31.7$ | $45.6$ | $38.1$ | $0.0$ | $10.9$ | $19.9$ | $\\mathbf{51.3}$ |\n\nThe table above shows the comparison results of V-Former with three additional baselines on both expert and mixed (suboptimal) datasets (at 500K steps, 3 seeds each, $\\pm$ denotes standard deviations). The results suggest that V-Former mostly outperforms the three new baselines, especially on challenging mixed datasets. In particular, previous offline RL methods that do not use temporally extended actions (i.e., CQL and IQL) struggle on these narrow, suboptimal datasets since their policies cannot fully represent highly non-Markovian and multi-modal behavioral policies. We will add these results to the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376359921,
                "cdate": 1700376359921,
                "tmdate": 1700376604634,
                "mdate": 1700376604634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "USFgQoBOum",
                "forum": "rOpK0ToM3o",
                "replyto": "3T2pGJT4i6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "* **\u201cThe expert + random datasets seem a bit artificial.\u201d**, **Evaluation on multi-human datasets**\n\nThank you for the suggestion. Following the suggestion, we evaluate V-Former on **two** additional types of datasets: (1) the original multi-human (MH) datasets from Robomimic and (2) more \u201cnatural\u201d suboptimal datasets consisting of original expert trajectories and additional \u201cdiversity\u201d trajectories, where each diversity trajectory is obtained by concatenating initial steps from an expert trajectory to another goal-reaching trajectory toward a randomly sampled goal.\n\n**(1) Results on multi-human (MH) datasets**\n\n| Task | BC | BC+Transformer | IQL | V-Former (ours) |\n| :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $\\mathbf{82.7}$ $\\tiny{\\pm 4.2}$ | $81.3$ $\\tiny{\\pm 4.6}$ | $80.7$ $\\tiny{\\pm 3.1}$ | $80.0$ $\\tiny{\\pm 5.3}$ |\n| $\\texttt{lift}$ | $92.7$ $\\tiny{\\pm 1.2}$ | $94.7$ $\\tiny{\\pm 3.1}$ | $94.7$ $\\tiny{\\pm 3.1}$ | $\\mathbf{97.3}$ $\\tiny{\\pm 3.1}$ |\n| $\\texttt{square}$ | $32.7$ $\\tiny{\\pm 4.6}$ | $\\mathbf{40.7}$ $\\tiny{\\pm 1.2}$ | $33.3$ $\\tiny{\\pm 9.5}$ | $\\mathbf{40.7}$ $\\tiny{\\pm 5.0}$ |\n| $\\texttt{transport}$ | $2.7$ $\\tiny{\\pm 3.1}$ | $4.7$ $\\tiny{\\pm 1.2}$ | $2.0$ $\\tiny{\\pm 2.0}$ | $\\mathbf{8.7}$ $\\tiny{\\pm 3.1}$ |\n| **average** | $52.7$ | $55.3$ | $52.7$ | $\\mathbf{56.7}$ |\n\n**(2) Results on more \"natural\" suboptimal datasets**\n\n| Task | BC | BC+Transformer | IQL | V-Former (ours) |\n| :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $7.3$ $\\tiny{\\pm 1.2}$ | $10.0$ $\\tiny{\\pm 4.0}$ | $6.7$ $\\tiny{\\pm 6.4}$ | $\\mathbf{13.3}$ $\\tiny{\\pm 8.1}$ |\n| $\\texttt{lift}$ | $10.7$ $\\tiny{\\pm 4.2}$ | $20.0$ $\\tiny{\\pm 2.0}$ | $\\mathbf{40.0}$ $\\tiny{\\pm 2.0}$ | $38.7$ $\\tiny{\\pm 7.0}$ |\n| $\\texttt{square}$ | $4.7$ $\\tiny{\\pm 1.2}$ | $4.0$ $\\tiny{\\pm 2.0}$ | $12.0$ $\\tiny{\\pm 5.3}$ | $\\mathbf{13.3}$ $\\tiny{\\pm 5.0}$ |\n| $\\texttt{tool\\\\_hang}$ | $0.7$ $\\tiny{\\pm 1.2}$ | $6.0$ $\\tiny{\\pm 4.0}$ | $2.7$ $\\tiny{\\pm 1.2}$ | $\\mathbf{6.7}$ $\\tiny{\\pm 5.0}$ |\n| $\\texttt{transport}$ | $5.3$ $\\tiny{\\pm 3.1}$ | $6.0$ $\\tiny{\\pm 5.3}$ | $7.3$ $\\tiny{\\pm 5.0}$ | $\\mathbf{7.3}$ $\\tiny{\\pm 1.2}$ |\n| **average** | $5.7$ | $9.2$ | $13.7$ | $\\mathbf{15.9}$ |\n\nThe tables above compare V-Former with three baselines on the two additional datasets (at 500K steps, 3 seeds each, $\\pm$ denotes standard deviations). The results suggest that, in both the MH datasets and the new suboptimal datasets, V-Former achieves the best or near-best performance, showing the effectiveness of temporally extended actions in offline RL. We will add these results to the final version of the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376401029,
                "cdate": 1700376401029,
                "tmdate": 1700376628743,
                "mdate": 1700376628743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f5Yua1jjn3",
                "forum": "rOpK0ToM3o",
                "replyto": "D4vOpAOGSO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6695/Reviewer_ZdXy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6695/Reviewer_ZdXy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response and additional experiments. My major concern on the baselines have been addressed. I've thus increased my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685921222,
                "cdate": 1700685921222,
                "tmdate": 1700685921222,
                "mdate": 1700685921222,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YLJyzpUDZg",
            "forum": "rOpK0ToM3o",
            "replyto": "rOpK0ToM3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6695/Reviewer_yZ1P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6695/Reviewer_yZ1P"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an offline RL approach built on implicit Q/V-learning, extending the original formulation to n-steps, enabling offline value learning from arbitrary-length actions. They further formulate the Bellman equation in a continuous-time MDP, therefore supporting learning over multiple datasets with distinct temporal frequencies, as is commonly available in robotics. The authors train open loop, multistep transformer policies, taking temporally extended actions, with advantage weighted regression, learning from suboptimal data, outperforming prior approaches on robotic benchmarks w/wo multiple temporal frequencies."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Extending implicit V-learning to n-steps is intuitive and well motivated.\n2) Results are impressive, especially on increasingly suboptimal datasets.\n3) Table 4 ablation study is appreciated.\n4) Paper is well written and the approach should be simple to implement and adopt by the wider community."
                },
                "weaknesses": {
                    "value": "1) The authors do not report confidence intervals in many of their results\n2) Only 3 random seeds were ran, which is very low\n3) Setting the action chunking length as a hyperparameter seems restrictive. Wouldn\u2019t it be better to learn dynamic chunking lengths, based on the task and state? E.g., wouldn\u2019t something more akin to \u201coptions\u201d [1,2] work better here?\n\n[1] - Sutton, Richard S., Doina Precup, and Satinder Singh. \"Intra-Option Learning about Temporally Abstract Actions.\" ICML. Vol. 98. 1998.\n\n[2] - Salter, Sasha, et al. \"Mo2: Model-based offline options.\" Conference on Lifelong Learning Agents. PMLR, 2022."
                },
                "questions": {
                    "value": "Could the authors comment on what the choice of \u2018n\u2019 in Eq 9 has during learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6695/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6695/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6695/Reviewer_yZ1P"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698532607742,
            "cdate": 1698532607742,
            "tmdate": 1699636768395,
            "mdate": 1699636768395,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fpjY4gnivZ",
                "forum": "rOpK0ToM3o",
                "replyto": "YLJyzpUDZg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough review and constructive feedback about this work. Below, we provide answers to the questions. We welcome additional suggestions for further improving the work.\n\n* **More random seeds and confidence intervals**\n\nThanks for the suggestion. While we reported two aggregation plots with confidence intervals (Fig. 3), in which V-Former shows the best performance by statistically significant margins, we completely agree with the reviewer\u2019s suggestion and will add at least three more seeds (i.e., six seeds in total) as well as statistics to the final version of the paper (for the statistics of our current results, please see our response to Reviewer 7pis). Please understand that we were not able to run all $6$ Transformer methods on $5 \\times 2$ datasets with $3$ more seeds within the limited time of the rebuttal period.\n\n* **\u201cSetting the action chunking length as a hyperparameter seems restrictive. Wouldn\u2019t it be better to learn dynamic chunking lengths, based on the task and state?\u201d**\n\nThanks for raising this point. In an earlier version of this work, we made initial attempts at devising an algorithm to select action lengths dynamically based on learned advantages. Specifically, we tried adding a `STOP` token to the action space and training a policy to output `STOP` if there are no suitable open-loop actions. However, we didn\u2019t find this version to be particularly better than the current fixed-step policy in our experiments and thus omitted it from the paper. That being said, we believe extending V-Former to handle dynamic chunk lengths is an exciting future research direction.\n\n* **Could the authors comment on what the choice of \u2018n\u2019 in Eq 9 has during learning?**\n\nAs stated in L4 of Algorithm 1, we sample $n$ from the uniform distribution over $\\{1, 2, \\dots, N\\}$. We use $N=3$ for Robomimic and $N=12$ for Kitchen.\n\n\nWe thank the reviewer again for the helpful feedback and please let us know if there are any additional concerns or questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376251842,
                "cdate": 1700376251842,
                "tmdate": 1700376251842,
                "mdate": 1700376251842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MeIMUkIJmm",
            "forum": "rOpK0ToM3o",
            "replyto": "rOpK0ToM3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6695/Reviewer_B2yt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6695/Reviewer_B2yt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes V-Former, an offline RL algorithm to learn from suboptimal, multi-modal, and non-Markovian data with different control frequencies. To address these challenges, the authors first extend implicit V-learning to arbitrary frequencies, and then train a Transformer policy with advantage reweighting to produce temporally extended actions. The empirical results show that V-Former can adapt to time-heterogeneous datasets and outperform its per-timestep or BC variants."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper aims to address some important questions in offline RL. It is clearly written and the proposed algorithm is novel to my knowledge. The idea of using Transformer to generate temporally extended \"action chunks\" sounds interesting."
                },
                "weaknesses": {
                    "value": "While well-motivated, I have some major concerns about the methodology and experiments of this work:\n\n1. What value is $V_\\psi(s)$ modeling in value learning? According to Equation 5-9, it seems that $V_\\psi(s)$ is trying to approximate the value of the optimal single-step policy with the n-step Bellman equation. However, $V_\\psi(s)$ is proposed to model the value of arbitrary action frequencies or action lengths that should have different values, which is confusing to me.\n2. Compared to previous offline hierarchical RL works, what's the advantage of the proposed method? These works [1, 2, 3] also aim to solve similar tasks.\n3. I am worried that the experiments are insufficient to support that V-Former is a strong baseline for offline RL, as we can only see ablations of V-Former on action chunking and advantage weighting, missing the performance of other state-of-the-art offline RL and offline HRL baselines. Moreover, Section 5.3 shows that the optimal action chunk size is around 3 in Robomimic tasks, which makes it hard to distinguish the effect of temporally extended actions. Therefore, I suggest authors compare the performance of V-Former and other baselines on tasks that may benefit from longer horizon control, such as *antmaze* and *kitchen* in D4RL.\n\nMinor questions:\n\n1. How to choose the hyper-parameter N? It is unclear to me the criteria for choosing N in different environments during evaluation.\n2. The results in Section 5.1 and 5.3 indicate that open-loop control can achieve the best performance. However, Section 5.2 uses a close-loop VF for evaluation. Can authors provide some intuitions behind this choice?\n\nOverall, I am unable to recommend acceptance at this stage given the questions mentioned above. However, I would consider raising my score if the authors could address my concerns.\n\n[1] Pertsch, Karl, Youngwoon Lee, and Joseph Lim. \"Accelerating reinforcement learning with learned skill priors.\" Conference on robot learning. PMLR, 2021.\n\n[2] Ajay, Anurag, et al. \"Opal: Offline primitive discovery for accelerating offline reinforcement learning.\" arXiv preprint arXiv:2010.13611 (2020).\n\n[3] Yang, Yiqin, et al. \"Flow to control: Offline reinforcement learning with lossless primitive discovery.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 9. 2023."
                },
                "questions": {
                    "value": "There are some questions and concerns, which I have outlined in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811071754,
            "cdate": 1698811071754,
            "tmdate": 1699636768282,
            "mdate": 1699636768282,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oAx7uCCYdb",
                "forum": "rOpK0ToM3o",
                "replyto": "MeIMUkIJmm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough review and constructive feedback about this work. Below, we describe how we have added an additional comparison with $3$ new baselines and a new ablation study with different action chunk sizes. We believe that these changes strengthen the paper, and welcome additional suggestions for further improving the work.\n\n* **$\\mathbf{3}$ new baselines - CQL, IQL, and DT**\n\nThank you for the suggestion. We would like to first note that we have (already) included comparisons with three existing offline RL and BC methods, (1) BC, (2) BC+Transformer, and (3) IQL (which correspond to $BC (1, 1)$, $BC (3, 3)$, and $VF (1, 1)$, respectively (Tables 1, 2, Fig. 3); these methods use the same action discretization as V-Former). However, following the suggestion, below, we compare V-Former with $\\mathbf{3}$ additional existing offline RL methods: **CQL**, **(Original) IQL**, and **Decision Transformer**.\n\n**(1) Results on expert (PH) datasets:**\n| Task | BC | BC+Transformer | IQL | CQL | (Original) IQL | Decision Transformer | V-Former (ours) |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $90.0$ $\\tiny{\\pm 3.5}$ | $91.3$ $\\tiny{\\pm 9.0}$ | $92.7$ $\\tiny{\\pm 6.1}$ | $10.0$ $\\tiny{\\pm 11.1}$ | $37.3$ $\\tiny{\\pm 9.5}$ | $93.3$ $\\tiny{\\pm 1.2}$ | $\\mathbf{95.3}$ $\\tiny{\\pm 3.1}$ |\n| $\\texttt{lift}$ | $88.7$ $\\tiny{\\pm 9.5}$ | $94.7$ $\\tiny{\\pm 2.3}$ | $94.7$ $\\tiny{\\pm 4.2}$ | $70.7$ $\\tiny{\\pm 11.5}$ | $74.0$ $\\tiny{\\pm 8.7}$ | $\\mathbf{98.0}$ $\\tiny{\\pm 2.0}$ | $93.3$ $\\tiny{\\pm 3.1}$ |\n| $\\texttt{square}$ | $62.7$ $\\tiny{\\pm 5.0}$ | $\\mathbf{78.0}$ $\\tiny{\\pm 7.2}$ | $65.3$ $\\tiny{\\pm 8.1}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $31.3$ $\\tiny{\\pm 9.0}$ | $30.7$ $\\tiny{\\pm 6.4}$ | $67.3$ $\\tiny{\\pm 1.2}$ |\n| $\\texttt{tool\\\\_hang}$ | $14.7$ $\\tiny{\\pm 8.3}$ | $36.7$ $\\tiny{\\pm 8.1}$ | $14.7$ $\\tiny{\\pm 5.8}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $6.0$ $\\tiny{\\pm 4.0}$ | $4.0$ $\\tiny{\\pm 2.0}$ | $\\mathbf{38.0}$ $\\tiny{\\pm 7.2}$ |\n| $\\texttt{transport}$ | $22.7$ $\\tiny{\\pm 3.1}$ | $\\mathbf{35.3}$ $\\tiny{\\pm 8.1}$ | $23.3$ $\\tiny{\\pm 5.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $2.0$ $\\tiny{\\pm 0.0}$ | $2.0$ $\\tiny{\\pm 2.0}$ | $32.0$ $\\tiny{\\pm 7.2}$ |\n| **average** | $55.7$ | $\\mathbf{67.2}$ | $58.1$ | $16.1$ | $30.1$ | $45.6$ | $65.2$ |\n\n**(2) Results on suboptimal (mixed) datasets:**\n| Task | BC | BC+Transformer | IQL | CQL | (Original) IQL | Decision Transformer | V-Former (ours) |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $52.0$ $\\tiny{\\pm 7.2}$ | $59.3$ $\\tiny{\\pm 10.3}$ | $54.7$ $\\tiny{\\pm 10.3}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $10.7$ $\\tiny{\\pm 8.3}$ | $33.3$ $\\tiny{\\pm 17.5}$ | $\\mathbf{66.7}$ $\\tiny{\\pm 5.8}$ |\n| $\\texttt{lift}$ | $50.0$ $\\tiny{\\pm 9.2}$ | $58.7$ $\\tiny{\\pm 4.2}$ | $58.7$ $\\tiny{\\pm 9.9}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $26.0$ $\\tiny{\\pm 4.0}$ | $60.0$ $\\tiny{\\pm 3.5}$ | $\\mathbf{70.0}$ $\\tiny{\\pm 15.6}$ |\n| $\\texttt{square}$ | $37.3$ $\\tiny{\\pm 9.5}$ | $\\mathbf{66.0}$ $\\tiny{\\pm 8.0}$ | $47.3$ $\\tiny{\\pm 5.8}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $18.0$ $\\tiny{\\pm 6.0}$ | $6.0$ $\\tiny{\\pm 7.2}$ | $\\mathbf{66.0}$ $\\tiny{\\pm 3.5}$ |\n| $\\texttt{tool\\\\_hang}$ | $11.3$ $\\tiny{\\pm 5.0}$ | $28.0$ $\\tiny{\\pm 14.4}$ | $15.3$ $\\tiny{\\pm 4.2}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $\\mathbf{28.7}$ $\\tiny{\\pm 7.0}$ |\n| $\\texttt{transport}$ | $8.0$ $\\tiny{\\pm 2.0}$ | $16.0$ $\\tiny{\\pm 6.9}$ | $14.7$ $\\tiny{\\pm 5.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $\\mathbf{25.3}$ $\\tiny{\\pm 2.3}$ |\n| **average** | $31.7$ | $45.6$ | $38.1$ | $0.0$ | $10.9$ | $19.9$ | $\\mathbf{51.3}$ |\n\nThe table above shows the comparison results of V-Former with three additional baselines on both expert and mixed (suboptimal) datasets (at 500K steps, 3 seeds each, $\\pm$ denotes standard deviations). The results suggest that V-Former mostly outperforms the three new baselines, especially on challenging mixed datasets. In particular, previous offline RL methods that do not use temporally extended actions (i.e., CQL and IQL) struggle on these narrow, suboptimal datasets since their policies cannot fully represent highly non-Markovian and multi-modal behavioral policies. We will add these results to the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376089125,
                "cdate": 1700376089125,
                "tmdate": 1700376593262,
                "mdate": 1700376593262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ko8fYUJY6f",
                "forum": "rOpK0ToM3o",
                "replyto": "9TgeMUBsT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6695/Reviewer_B2yt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6695/Reviewer_B2yt"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your clarification and additional experiments. While the comparison with new baselines demonstrates V-Former's better performance on Robomimic tasks, my major concerns may not be fully addressed in the response. Here are my further questions:\n\n* **Comparison to new baselines.** I appreciate these results presented by authors, which show that V-Former is better than some state-of-the-art offline RL algorithms in some cases. However, comparing the performance of IQL(VF(1,1) in the paper) and IQL(Original), I feel that it is the Transformer rather than the temporal abstraction that contributes to the most performance gain. Moreover, given that original IQL, CQL, and DT are not evaluated on Robomimic tasks, I think it is more fair to compare these methods on the standard offline RL benchmark such as D4RL.\n* **Main contribution of this work.** The authors claim that the significance of this work is \"the effectiveness of temporally extended actions in the offline RL setting\" in their response to Reviewer 7pis. However, such an idea has been explored in many offline hierarchical RL methods (see my review part point 2 and citation [1, 2, 3]). It seems that neither theoretical justification nor empirical results could show that V-Former is better than these offline hierarchical RL methods.\n* **The action chunk size $N$.** I agree that the table reported can show that $N$ is not sensitive to 5 tasks in Robomimic, possibly due to the agent controlling the same robot arm. However, according to Table 3 on Kitchen environment, the optimal $N$ may vary across different robots, which may need us to sweep the hyperparameter.\n* **Open-loop vs. closed-loop action execution.** I appreciate this discussion. However, it is unclear to me why closed-loop actions (at evaluation time) cannot simulate a non-Markovian policy. For instance, you can model the low-level policy as DT, which is non-Markovian but can perform closed-loop control."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588536557,
                "cdate": 1700588536557,
                "tmdate": 1700588536557,
                "mdate": 1700588536557,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2kxUmILeVf",
            "forum": "rOpK0ToM3o",
            "replyto": "rOpK0ToM3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6695/Reviewer_7pis"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6695/Reviewer_7pis"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on handling data with different qualities (suboptimal) and different frequencies. The paper proposes a method, V-former, which utilizes the idea of \u201caction trunks\u201d and a transformer-based policy. Concretely, it extends the value function of Implicit Value Learning to bootstrap with multiple steps actions and uses the transformer policy to roll out multiple steps. In five robomimic tasks with different data qualities and kitchen tasks with different data frequencies, the proposed method shows better performance than baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-organized and clear.\n\n - The method section is easy to follow."
                },
                "weaknesses": {
                    "value": "- The method is straightforward, and the contribution is limited. The main technical contribution of the paper is extending the value function of IVL and making it consider the outcome of multiple timesteps, which I believe is not significant enough. The underlying insight that modeling multiple steps to help handle multimodality is already known in the literature. \n\n - The experiment evaluation is not thorough enough. The baselines are mostly ablation of the proposed method. Moreover, there are other existing offline RL methods that also can be applied to the problem of interest, such as IQL. The limited set of experiments makes the significance of the proposed method hard to evaluate."
                },
                "questions": {
                    "value": "- The evaluation in Table 1 is interesting and shows that VF can achieve good performance if proper N and k are selected. However, the optimal N and k may be quite different for different tasks. Instead of manually selecting them, will there be a general way to derive them from the offline dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699329824264,
            "cdate": 1699329824264,
            "tmdate": 1699636768128,
            "mdate": 1699636768128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5vAPVYfJBf",
                "forum": "rOpK0ToM3o",
                "replyto": "2kxUmILeVf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough review and constructive feedback about this work. Below, we describe how we have added an additional comparison with $3$ existing offline RL methods and a new ablation study with different action chunk sizes. We believe that these changes strengthen the paper, and welcome additional suggestions for further improving the work.\n\n* **$\\mathbf{3}$ new baselines - CQL, IQL, and DT**\n\nThank you for the suggestion. We would like to first note that we have (already) included comparisons with three existing offline RL and BC methods, (1) BC, (2) BC+Transformer, and (3) IQL (which correspond to $BC (1, 1)$, $BC (3, 3)$, and $VF (1, 1)$, respectively (Tables 1, 2, Fig. 3); these methods use the same action discretization as V-Former). However, following the suggestion, below, we compare V-Former with $\\mathbf{3}$ additional existing offline RL methods: **CQL**, **(Original) IQL**, and **Decision Transformer**.\n\n**(1) Results on expert (PH) datasets:**\n| Task | BC | BC+Transformer | IQL | CQL | (Original) IQL | Decision Transformer | V-Former (ours) |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $90.0$ $\\tiny{\\pm 3.5}$ | $91.3$ $\\tiny{\\pm 9.0}$ | $92.7$ $\\tiny{\\pm 6.1}$ | $10.0$ $\\tiny{\\pm 11.1}$ | $37.3$ $\\tiny{\\pm 9.5}$ | $93.3$ $\\tiny{\\pm 1.2}$ | $\\mathbf{95.3}$ $\\tiny{\\pm 3.1}$ |\n| $\\texttt{lift}$ | $88.7$ $\\tiny{\\pm 9.5}$ | $94.7$ $\\tiny{\\pm 2.3}$ | $94.7$ $\\tiny{\\pm 4.2}$ | $70.7$ $\\tiny{\\pm 11.5}$ | $74.0$ $\\tiny{\\pm 8.7}$ | $\\mathbf{98.0}$ $\\tiny{\\pm 2.0}$ | $93.3$ $\\tiny{\\pm 3.1}$ |\n| $\\texttt{square}$ | $62.7$ $\\tiny{\\pm 5.0}$ | $\\mathbf{78.0}$ $\\tiny{\\pm 7.2}$ | $65.3$ $\\tiny{\\pm 8.1}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $31.3$ $\\tiny{\\pm 9.0}$ | $30.7$ $\\tiny{\\pm 6.4}$ | $67.3$ $\\tiny{\\pm 1.2}$ |\n| $\\texttt{tool\\\\_hang}$ | $14.7$ $\\tiny{\\pm 8.3}$ | $36.7$ $\\tiny{\\pm 8.1}$ | $14.7$ $\\tiny{\\pm 5.8}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $6.0$ $\\tiny{\\pm 4.0}$ | $4.0$ $\\tiny{\\pm 2.0}$ | $\\mathbf{38.0}$ $\\tiny{\\pm 7.2}$ |\n| $\\texttt{transport}$ | $22.7$ $\\tiny{\\pm 3.1}$ | $\\mathbf{35.3}$ $\\tiny{\\pm 8.1}$ | $23.3$ $\\tiny{\\pm 5.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $2.0$ $\\tiny{\\pm 0.0}$ | $2.0$ $\\tiny{\\pm 2.0}$ | $32.0$ $\\tiny{\\pm 7.2}$ |\n| **average** | $55.7$ | $\\mathbf{67.2}$ | $58.1$ | $16.1$ | $30.1$ | $45.6$ | $65.2$ |\n\n**(2) Results on suboptimal (mixed) datasets:**\n| Task | BC | BC+Transformer | IQL | CQL | (Original) IQL | Decision Transformer | V-Former (ours) |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $52.0$ $\\tiny{\\pm 7.2}$ | $59.3$ $\\tiny{\\pm 10.3}$ | $54.7$ $\\tiny{\\pm 10.3}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $10.7$ $\\tiny{\\pm 8.3}$ | $33.3$ $\\tiny{\\pm 17.5}$ | $\\mathbf{66.7}$ $\\tiny{\\pm 5.8}$ |\n| $\\texttt{lift}$ | $50.0$ $\\tiny{\\pm 9.2}$ | $58.7$ $\\tiny{\\pm 4.2}$ | $58.7$ $\\tiny{\\pm 9.9}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $26.0$ $\\tiny{\\pm 4.0}$ | $60.0$ $\\tiny{\\pm 3.5}$ | $\\mathbf{70.0}$ $\\tiny{\\pm 15.6}$ |\n| $\\texttt{square}$ | $37.3$ $\\tiny{\\pm 9.5}$ | $\\mathbf{66.0}$ $\\tiny{\\pm 8.0}$ | $47.3$ $\\tiny{\\pm 5.8}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $18.0$ $\\tiny{\\pm 6.0}$ | $6.0$ $\\tiny{\\pm 7.2}$ | $\\mathbf{66.0}$ $\\tiny{\\pm 3.5}$ |\n| $\\texttt{tool\\\\_hang}$ | $11.3$ $\\tiny{\\pm 5.0}$ | $28.0$ $\\tiny{\\pm 14.4}$ | $15.3$ $\\tiny{\\pm 4.2}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $\\mathbf{28.7}$ $\\tiny{\\pm 7.0}$ |\n| $\\texttt{transport}$ | $8.0$ $\\tiny{\\pm 2.0}$ | $16.0$ $\\tiny{\\pm 6.9}$ | $14.7$ $\\tiny{\\pm 5.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $0.0$ $\\tiny{\\pm 0.0}$ | $\\mathbf{25.3}$ $\\tiny{\\pm 2.3}$ |\n| **average** | $31.7$ | $45.6$ | $38.1$ | $0.0$ | $10.9$ | $19.9$ | $\\mathbf{51.3}$ |\n\nThe table above shows the comparison results of V-Former with three additional baselines on both expert and mixed (suboptimal) datasets (at 500K steps, 3 seeds each, $\\pm$ denotes standard deviations). The results suggest that V-Former mostly outperforms the three new baselines, especially on challenging mixed datasets. In particular, previous offline RL methods that do not use temporally extended actions (i.e., CQL and IQL) struggle on these narrow, suboptimal datasets since their policies cannot fully represent highly non-Markovian and multi-modal behavioral policies. We will add these results to the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375811392,
                "cdate": 1700375811392,
                "tmdate": 1700376570381,
                "mdate": 1700376570381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iMJ4HKRtsp",
                "forum": "rOpK0ToM3o",
                "replyto": "2kxUmILeVf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "* **\u201cThe method is straightforward, and the contribution is limited\u201d**, **\u201cThe underlying insight that modeling multiple steps to help handle multimodality is already known in the literature\u201d**\n\nThe significance of our work is that we demonstrate the effectiveness of temporally extended actions in the **offline RL** setting, especially when the dataset consists of narrow demonstration data and broader suboptimal data, a setting very common in the real world (e.g., large task-agnostic + small task-specific demonstrations). While V-Former is indeed straightforward to implement, we believe that this simplicity is a strength of the approach, not a weakness. Also, we are not aware of any prior work that uses such a multi-step open-loop policy for offline RL. Applying the idea of action chunking to offline RL is not straightforward as it requires fitting a multi-step Q function, which we resolve by generalizing an in-sample value maximization algorithm in a novel way. Finally, we would like to note that the V-Former can even handle **time-heterogeneous** datasets thanks to our generalized IVL, outperforming previous work (Burns et al., 2022) by a significant margin (Table 3).\n\n\n* **How to select the action chunk size $N$?**\n\nAs the reviewer pointed out, the action chunk size $N$ is a hyperparameter that we need to tune, as in most previous works in hierarchical RL and multi-step BC. However, we found that the optimal action chunk size $N$ is *not* very sensitive to individual tasks. We present the ablation results of V-Former (VF) on the five environments from Robomimic below:\n\n| Method ($N$, $k$) | VF (1, 1) | VF (3, 1) | VF (3, 3) | VF (5, 1) | VF (5, 3) | VF (8, 1) | VF (8, 3) |\n| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| $\\texttt{can}$ | $92.7$ $\\tiny{\\pm 6.1}$ | $90.7$ $\\tiny{\\pm 3.1}$ | $\\mathbf{95.3}$ $\\tiny{\\pm 3.1}$ | $91.3$ $\\tiny{\\pm 4.2}$ | $91.3$ $\\tiny{\\pm 2.3}$ | $92.0$ $\\tiny{\\pm 3.5}$ | $94.7$ $\\tiny{\\pm 2.3}$ |\n| $\\texttt{lift}$ | $\\mathbf{94.7}$ $\\tiny{\\pm 4.2}$ | $94.0$ $\\tiny{\\pm 2.0}$ | $93.3$ $\\tiny{\\pm 3.1}$ | $93.3$ $\\tiny{\\pm 3.1}$ | $\\mathbf{94.7}$ $\\tiny{\\pm 1.2}$ | $43.3$ $\\tiny{\\pm 8.3}$ | $71.3$ $\\tiny{\\pm 8.1}$ |\n| $\\texttt{square}$ | $65.3$ $\\tiny{\\pm 8.1}$ | $73.3$ $\\tiny{\\pm 3.1}$ | $67.3$ $\\tiny{\\pm 1.2}$ | $66.0$ $\\tiny{\\pm 8.7}$ | $\\mathbf{75.3}$ $\\tiny{\\pm 6.1}$ | $68.0$ $\\tiny{\\pm 13.1}$ | $64.7$ $\\tiny{\\pm 5.8}$ |\n| $\\texttt{tool\\\\_hang}$ | $14.7$ $\\tiny{\\pm 5.8}$ | $19.3$ $\\tiny{\\pm 2.3}$ | $\\mathbf{38.0}$ $\\tiny{\\pm 7.2}$ | $14.7$ $\\tiny{\\pm 4.2}$ | $37.3$ $\\tiny{\\pm 8.3}$ | $6.7$ $\\tiny{\\pm 4.6}$ | $22.0$ $\\tiny{\\pm 8.7}$ |\n| $\\texttt{transport}$ | $23.3$ $\\tiny{\\pm 5.0}$ | $20.7$ $\\tiny{\\pm 4.2}$ | $\\mathbf{32.0}$ $\\tiny{\\pm 7.2}$ | $8.0$ $\\tiny{\\pm 2.0}$ | $14.7$ $\\tiny{\\pm 8.3}$ | $2.7$ $\\tiny{\\pm 1.2}$ | $5.3$ $\\tiny{\\pm 2.3}$ |\n| **average** | $58.1$ | $59.6$ | $\\mathbf{65.2}$ | $54.7$ | $62.7$ | $42.5$ | $51.6$ |\n\n\nThe table above compares the performances from different action chunk sizes $N$ on the five Robomimic tasks (at 500K steps, 3 seeds each, $\\pm$ denotes standard deviations). The results suggest that, as long as $N$ is in an appropriate range (mostly within 3-5), V-Former with open-loop control ($k = 3$) achieves the best or near-best performance consistently across the five different environments. As such, we may sweep the optimal hyperparameter $N$ in some representative tasks, and reuse the best $N$ for the other tasks. We will add the ablation results above to the paper.\n\nWe thank the reviewer again for the helpful feedback and please let us know if there are any additional concerns or questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375828309,
                "cdate": 1700375828309,
                "tmdate": 1700375909673,
                "mdate": 1700375909673,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]