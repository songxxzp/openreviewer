[
    {
        "title": "BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity"
    },
    {
        "review": {
            "id": "AJl5bBkYWU",
            "forum": "mQYHXUUTkU",
            "replyto": "mQYHXUUTkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission298/Reviewer_G8Co"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission298/Reviewer_G8Co"
            ],
            "content": {
                "summary": {
                    "value": "The authors regressed fMRI neural activities onto CLIP embedding of images, after projecting the weight vector onto the \u201cnatural image distribution\u201d, they use the projected weights to generate natural language description and image visualizations.  Using this method, they validated many functional properties of the human visual cortices and defined visual properties for some less charted lands on human cortex."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This is a fast-paced field. The authors spend the time to add comprehensive references to previous literature, which form a strong foundation for evaluating this work.\n- Simple but principled method. Noticing and addressing the modality gap with embedding projection seems like a key advance to make this work.\n- The close comparison to BrainDiVE is interesting, which kind of suggests that the text caption captures the \u201cessence\u201d of high activation images, without the need for direct gradient ascent, i.e. the selectivity at least for the activation maximizing images is compressed in the words.\n- The authors showed additional applications for neuroscience discovery which is super cool. This visualization/caption tool will help define the visual property of many uncharted lands. Evaluating the separation of clusters with human subjects is convincing.\n- I can see this work\u2019s approach is broadly applicable to multiple neuroscience modalities, e.g. ephys or imaging data from animals. Though language caption may not be a proper medium for them."
                },
                "weaknesses": {
                    "value": "- Not much a weakness but more like a c**omment.** I think it seems common in this domain (e.g. neuron guided image generation), to pipe together several large-scale pre-trained multi-modal models with brain data and then train linear adaptors between them and then it will work.  So not quite sure how technical challenging this work is comparing to previous ones."
                },
                "questions": {
                    "value": "### Questions\n\n- In Eq. 4 why do you choose to use score to weighted average norm and direction separately, instead of averaging the vectors themselves? I can see arguments for both ways, but why do you choose this one?\n- In Fig. 5, the Word Voxel visualizations using BrainSCUBA is always some round cake like object with text on it \u2014\u2014 which is kind of strange, while the previous method (BrainDIVE) and NSD samples don\u2019t have this feature. Where do you think this bias could come from?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission298/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission298/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission298/Reviewer_G8Co"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission298/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813363392,
            "cdate": 1698813363392,
            "tmdate": 1699635956167,
            "mdate": 1699635956167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ph5CQauHrN",
                "forum": "mQYHXUUTkU",
                "replyto": "AJl5bBkYWU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to G8Co (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the helpful and concrete suggestions by G8Co. We gladly welcome any questions and have provided our responses to your specific comments below.\n\n> **Q1) Difference between our work and (linear) decoding**\n\nWe wanted to quickly summarize some conceptual differences between our work and the traditional decoding tasks -- which seek to reconstruct a stimulus from recorded brain activations. Recent decoding models indeed typically map from brain activation to a latent embedding using a linear model, and the result is coupled to a large image generative model. \n\nIn contrast, our work seeks to understand the semantic selectivity of regions of the brain by generating natural language captions. Our work uses a linear probe which takes as input a CLIP image embedding and maps to voxel-wise brain activations. By constraining the encoding backbone to be unit-norm, the voxel-wise weight can be efficiently used to derive the optimal CLIP image embedding. \n\nA problem with this particular embedding is that is does not necessarily lie in the space of CLIP image embeddings for natural images. To that end, we propose an efficient parameter free method to project this embedding via a convex operation. \n\nOur method falls under the broad umbrella of work like [1,2,3,4,5,6]. These methods without exception rely on expensive gradient based optimization. An example includes BrainDiVE which requires **20~30 seconds per-sample**, compared to **2~3 seconds per-sample** for our method (including both caption and image generation) on the same hardware. This restricts these methods to analyzing broad swathes of the brain (typically ROIs on the order of hundreds to thousands of voxels).\n\nOur gradient free approach to selectivity analysis differentiates our work from previous gradient based models. **This huge speed improvement can facilitate efficient data-driven exploration of the broader visual cortex**, and we demonstrate this in section 4.3 **Figure 8**, where we perform voxel-wise caption generation and tagging, backed up by additional human studies on ground truth top images (1600 responses in **Table 3** and 2560 responses in **Table S.3/S.4**).\n\n[**1**] Inception loops discover what excites neurons most using deep predictive models (2019)\n\n[**2**] Neural population control via deep image synthesis(2019)\n\n[**3**] Computational models of category-selective brain regions enable high-throughput tests of selectivity (2021)\n\n[**4**] NeuroGen: Activation optimized image synthesis for discovery neuroscience (2022)\n\n[**5**] Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models (2023)\n\n[**6**] Energy Guided Diffusion for Generating Neurally Exciting Images (2023)\n\n> **Q2) Decoupled norm and direction weighting**\n\nThis is an excellent question that highlights a subtle aspect of our proposed method.\n\nOur goal in the projection step is to map the encoder weights to the domain of CLIP embedding of natural images. We decouple the norm and direction weighting because if we performed simultaneous weighted sum, the vectors could cancel out and end up with a different norm.\n\nConsider the following scenario, where we have 2 natural images, and their respective vectors are $a=[0.5, -0.5]$ and $b=[0.5, +0.5]$, both of norm ~$0.707$. Consider the case where the weights on a and b are $w_a=0.5$ and $w_b=0.5$, in this case, the resulting vector if we do not decouple norm/direction is $[0.5, 0]$ with norm $0.5$.\n\nInstead, if we perform decoupled norm and direction weighting, the result is $[0.5,0]/0.5 * 0.707 = [0.707,0]$, which would match the norm of the two input vectors.\n\nWe further illustrate this effect in **Figure S.19 on page 38 of the revision**, we plot a histogram of the norms of 1 million natural CLIP image embeddings, the norms of encoder weights projected without decoupled norm, and the norms of encoder weights projected with decoupled norm. This figure shows that decoupled projection yields a better aligned norm distribution.\n\nWe hope this clarifies our approach."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169905583,
                "cdate": 1700169905583,
                "tmdate": 1700169905583,
                "mdate": 1700169905583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q67PGL5f60",
                "forum": "mQYHXUUTkU",
                "replyto": "AJl5bBkYWU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Q3) Cake and food selectivity**\n\nBrainDiVE is guided by gradient signal of an encoder on a stochastic image diffusion model, while our approach works by generating captions conditioned on encoder model weights, which can be further used for diffusion image generation. Looking at the images in **Figure 5** and **Figure S.4 to S.11**, both BrainDiVE and our method generate a high frequency of cake-like objects, although our method indeed appears to have a higher frequency.\n\nWe believe the difference primarily comes from the GPT based language model in one of two possible ways:\n1. As BrainDiVE is not text-conditioned, it is free to generate images that are not quite recognizable objects, and yields supra-natural predicted activations even when evaluated with a second encoding model backbone. In contrast, our method typically yields coherent images with recognizable objects.\n2. It is possible that a distribution difference exists in the language model itself, because cake is a more frequently mentioned food, and this is reflected in the captions, which is further reflected in the generated images.\n\n\u2800 \n\n\u2800 \n\n\nThank you for your comments and suggestions! We welcome any additional questions and discussion."
                    },
                    "title": {
                        "value": "Response to G8Co (2/2)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169964393,
                "cdate": 1700169964393,
                "tmdate": 1700176057666,
                "mdate": 1700176057666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k8d2bt1mDC",
            "forum": "mQYHXUUTkU",
            "replyto": "mQYHXUUTkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission298/Reviewer_79Ny"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission298/Reviewer_79Ny"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a data-driven method, BrainSCUBA, to generate natural language descriptions for images that are predicted to maximally activate individual voxels. This is achieved by first training a neural encoding model to predict the fMRI response from the image embeddings, then projecting the encoding weights into the representational space in a contrastive vision-language model, and finally producing interpretable captions for individual voxels. The results aligned with the previous findings about semantic selectivity in higher-order visual regions. The authors further demonstrated finer patterns that represent the \"person\" category in the brain."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This method provides more interpretable neural encoding results.\n- The generated text and synthesized images for individual voxels in the higher visual regions align well with previous research findings."
                },
                "weaknesses": {
                    "value": "- The scientific contribution and significance of this work are unclear. The paper didn't provide much new insights into the neuroscience findings. \n- The method of this paper is not sufficiently evaluated. There are multiple steps in the analysis, including training the encoding model, projecting weights, and transforming them into text or images, each of these steps (except encoding model accuracy) lacks a clear ground truth for comparison. Thus, we can hardly know how much the result deviated from the original voxel representation."
                },
                "questions": {
                    "value": "- How does the encoding weight vector $W_i$ change regarding to the model's training procedure? For example, if different splits of training and testing datasets are used, to what extent does the $W_i$ fluctuate? This is concerned because all the following analyses depend on a robust and accurate $W_i$. And for a voxel that has poor prediction accuracy (e.g., correlation=0.2), we don't know how well $W_i$ can be trusted as the representative embedding of that voxel.\n- The projection method is intuitive and smart, but there's no direct validation of its effectiveness. Is there a specific reason for using cosine similarity as the distance metric?\n- The map in Fig.3 seems to be very categorical compared to the continuous and distributed brain map resulting from natural speech data (Huth et al. 2016). Does this imply the existence of more separable clusters in higher visual areas? Is there any finer organization within these clusters if you look at more UMAP dimensions, e.g., under the \"human-related\" group (blue voxels) or \"scene-related\" group (pink voxels)?\n- The generated words in Fig. 4 mostly align with expectations. However, certain words, like \"close\" in the \"Faces\" and \"Food\" categories, are strange. Do the authors have an explanation for that?\n- The findings in Fig.7 are interesting. However, I am concerned that the identified regions don't necessarily represent the concept of \"people\". A region that represents another semantic category (for example, \"home\") that often co-occurs with \"people\" words in natural language captions might also be highlighted with this method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission298/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission298/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission298/Reviewer_79Ny"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission298/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818636411,
            "cdate": 1698818636411,
            "tmdate": 1700330980355,
            "mdate": 1700330980355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b7EBhXsugc",
                "forum": "mQYHXUUTkU",
                "replyto": "k8d2bt1mDC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 79Ny (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer 79Ny for the detailed and constructive review! We look forward to further discussion, and are happy to answer any questions. We will address all questions below, and incorporate your suggestions into the paper.\n\n> **Q1) Neuroscience contributions**\n\nOur paper presents four key experiments, detailed in **sections 4.2, 4.3, and 4.4**, along with supplemental results. The first two experiments confirm existing knowledge (**4.2 and 4.3**), while the third and fourth (**4.4** and **supplemental**) offer **new insights into the neuroscience of person representation in the brain.** **Following your suggestion, we have clarified this in the revision**.\n\nThere is no doubt that visual representations of the \"person\" class are extremely important in the human visual cortex, with representations of people emerging very early in infancy. In these two experiments, we seek to investigate the finer grained distribution of person representations in the brain.\n\nUsing voxel-wise caption generation and part-of-speech tagging, we distinguish between person-related and non-person-related nouns. Our regions of interest (ROIs) were identified using NSD functional localizer experiments with an independent stimulus set.\n\nExperiment three visualizes the presence of the \"person\" class in different ROIs in **Figure 7** and **Table 2**, revealing a clear distinction between regions traditionally associated with person perception (like EBA and FFA) and those not (such as RSC/PPA/OPA). We also note increased selectivity in the word region, aligning with previous studies on the visual word form area (VWFA). Significantly, we find a high density of person representations in the precuneus visual area (PCV) and the temporoparietal junction (TPJ), as shown in **Figure 7** and quantitatively evaluated in **Table 2**, suggesting a role for these areas in processing people. The PCV has been implicated in first- versus third-person perspective judgements, while the TPJ has been implicated to play a role in theory of mind. **We believe we are the first to provide evidence for PCV and TPJ's role in person representations using a natural image viewing dataset, in contrast to experimenter-designed stimulus datasets.**\n\nExperiment four focuses on the extrastriate body area (EBA). We identify two distinct clusters in EBA related to single and multiple person representations, which have not been reported in past work. **A human study with 1600 responses (Tables 3) and 5120 responses (Table S.3 and S.4) supports our computational findings, indicating that EBA-1 is more selective for social images, while EBA-2 favors single person images. We believe this is a novel neuroscientific insight that has not been observed previously.**\n\nThese findings, particularly regarding the EBA, offer novel insights and lay groundwork for future hypothesis-driven neuroscience research.\n\n> **Q2) Evaluation setup**\n\nThank you for you suggestion!\n\nIn the paper we perform the following validations:\n* In the encoder training step, we evaluate the $R^2$ on a **held out test set** which offers evidence that our encoder has strong performance. This test set is the ~$1000$ images viewed by all subjects. We find that we can achieve high explained variance across the higher order visual areas comparable to other works [1] using the NSD dataset, reaching 0.75 $R^2$.\n* In the projection step, we visualize the cosine similarity of the pre- and post-projection embeddings, and use this result to select the softmax temperature vector where cosine similarity plateaus in **Figure 2**. **In response to your suggestion, we have further added the standard deviation of cosine similarity in Figure 2 of the revision**\n\nWe would like to further clarify how we perform evaluation of our entire system end-to-end. Key to our evaluation is the use of ground truth region of interest (ROI) masks. These masks ***are not*** derived from the stimulus set (10000 images), and instead they are derived from a functional localizer task. This task uses images with the presence of a semantic category (faces, places, words, bodies) using the Stanford VPNL fLoc dataset. This yields ground truth ROIs.\nWe evaluate against these ROIs in a few ways:\n1. UMAP dimensionality reduction shows that we recover the category selective regions. **In response to your suggestion, we have added visualization of the ground truth t-statistics for the fLoc experiments in Figure S.17 on page 36** This provides a further point of reference to compare **Figure 3** against.\n2. We perform caption conditioned image synthesis for each ROI. Using CLIP for classification like BrainDiVE, we find our synthesized images have high semantic specificity matching the region's true semantic selectivity in **Table 1**.\n3. (see next response)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169709236,
                "cdate": 1700169709236,
                "tmdate": 1700169739548,
                "mdate": 1700169739548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7nngwp8fOe",
                "forum": "mQYHXUUTkU",
                "replyto": "k8d2bt1mDC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 79Ny (2/2)"
                    },
                    "comment": {
                        "value": "3. In the person experiments, we perform a human study and collect 4160 total responses (1600 in **Table 3**, 2560 in **Table S.3 and S.4**) on the ground truth top 1% fMRI images. The responses provide evidence that the caption analysis in **Figure 8b** and **8c** are valid.\n\nWe hope this response helps clarify our evaluation setup.\n\n[**1**] Natural language supervision with a large and diverse dataset builds better models of human high-level visual cortex\n\n> **Q3) Stability of W***\n\n**In response to your suggestion, we have added two plots evaluating the stability of encoder weights in 10-fold cross validation in Figure S.15 in page 34** and the semantic stability of post-projection encoder weights in **Figure S.16 in page 35**. We find that our encoder is very stable across different training sets and initializations.\n\n> **Q4) Use of cosine similarity**\n\nTo clarify, cosine similarity (dot product) is the officially recommended [text-image retrieval metric (link to github)](https://github.com/openai/CLIP/blob/main/README.md?plain=1#L117) for CLIP [2]. This is because CLIP embeddings naturally lie on the surface of the sphere.\n\n[**2**] Learning Transferable Visual Models From Natural Language Supervision\n\n> **Q4) Distribution of concepts in visual cortex**\n\nWe are deeply inspired by the Huth et al. paper, and cite it explicitly in **Figure 3**. Huth et al. did in fact propose categorical regions with clear boundaries in Figure 3 of their 2016 paper using a cross-subject clustering-based approach.\n\nIn addition, higher order *visual* regions are typically believed to have a clear categorical selectivity of voxels [3]. We further illustrate this in the revision by plotting the ground truth t-statistics from the NSD fLoc experiments in **Figure Figure S.17 on page 36** of the supplemental. The categories for fLoc were selected based on categories for which selective regions had been found as of 2015, with recent research from 2022 revealing an additional food-selective region.\n\n[**3**] The functional architecture of the ventral temporal cortex and its role in categorization\n\n> **Q4.1) Finer grained distribution of concepts** \n\nBeyond the representation of single people versus groups of people in extrastriate body area (EBA, blue voxels), there indeed exists a finer grained distribution of concept selectivity as revealed by UMAP dimension 4 in the occipital place area (OPA), which shows an indoor/outdoor split. This has been alluded to in BrainDiVE, and prior work on gradients in OPA [4], but to our knowledge this is the first time this has been independently confirmed using a language-based data-driven model. In the **revision we have added the OPA results in Figure S.18 on page 37**, using first, second, and fourth UMAP component as color components instead of the first three in **Figure 3**.\n\n[**4**] Processing of different spatial scales in the human brain\n\n\n> **Q5) \"close\" in part-of-speech tagging**\n\nThis is due to the nature of the Spacy part-of-speech tagging engine, we checked the generated captions and \"close\" always occurs in the form of \"A **close up** photo/picture of ...\", where \"close\" is tagged as a noun. We believe this is reflective of how faces/food are depicted in our datasets. \n\n> **Q6) Co-occurrence statistics**\n\nWe agree that identifying pure semantic selectivity is an important question. A significant advantage of our method compared to traditional functional localizer experiments is that our method processes naturalistic images, and does not need the images with a singular isolated concept. \n\nWe perform a quantitative study of the density of \"person\" representation in different ROIs, and find that it is indeed much lower in voxels that do not code for person. Note for the word region has been demonstrated to exhibit person selectivity in prior work, in line with the higher person density shown in **Table 7**.\n\nWe also extensively validate our results using a human study on the ground truth top fMRI images. In addition to the two-alternative forced choice human study to pick among clusters (**Table 3**, 1600 responses total), we also perform a human study where each attribute is judged to be present or not in a group of images (**Table S.3, S.4**, 2560 responses total). We believe this is a robust evaluation as it directly evaluates attributes. We find the human study provides additional evidence for the computational results in **Figure 8**.\n\u2800 \n\n\u2800 \n\nThank you for the detailed and thoughtful feedback! We look forward to additional discussions. We hope that our clarifications have been insightful, and hope you could consider a more positive assessment of our work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169826365,
                "cdate": 1700169826365,
                "tmdate": 1700178677458,
                "mdate": 1700178677458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SCATelN1Uz",
                "forum": "mQYHXUUTkU",
                "replyto": "7nngwp8fOe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_79Ny"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_79Ny"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response to my questions. I think most of my concerns have been addressed. I will raise my rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330946796,
                "cdate": 1700330946796,
                "tmdate": 1700330946796,
                "mdate": 1700330946796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k6704HH7eb",
            "forum": "mQYHXUUTkU",
            "replyto": "mQYHXUUTkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission298/Reviewer_xxME"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission298/Reviewer_xxME"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to find natural language description that maximally activates fMRI response of a voxel  to assess semantic-selectivity of different brain regions.\n\nKey idea is to first train a linear encoder to predict voxel activations from a pretrained CLIP model. Then  Voxel weights are projected to CLIP embedding space to generate captions from a pretrained text decoder (CLIPCap).\n\nThe proposed approach is validated by comparing the selectivity obtained by the proposed method with brain regions which show selectivity for places, food, bodies, words and faces. The authors perform additional analysis to find a person specific region in body-selective areas demonstrating new scientific discovery from this approach."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Use of pre-trained modules (CLIP, CLIPCap) to generate captions which maximize a voxel\u2019s response (Section 3, Figure 1)\n2. Confirmation of BrainSCUBA\u2019s findings on well-known category selective brain regions (Figure 4,5)\n3. Demonstration of category selectivity through a text to image generation model. Figure 5 and Figure 6 show how this method can generate maximally activating images 10 times faster than gradient based maximization (BrainDIVE). The images generated using BrainSCUBA are also more human-interpretable as compared to BrainDIVE.\n4. Finding person and non-person specific cluster within EBA  (Table 3, Figure 8)\n5. Overall the paper is well written and easy to follow. The approach is presented in a simple yet comprehensive manner thus making it easier for new readers to follow.\n6. The approach is validated both qualitatively (figure 5) and quantitatively using different metrics (Figure 6, Table 1,2)\n7. The approach has potential to be extended to investigating more complex semantic specificity which are not feasible using category selective images only."
                },
                "weaknesses": {
                    "value": "1. Minor: In Figure 3, color code used is not shown in the legend but is there in text in page 6. I recommend to add legend in the Figure also for clarity.\n2. I believe this paper does not fully leverage the potential of BrainSCUBA. The captions generated are currently restricted to Nouns. Semantic selectivity using images is limiting as we have to find natural images that consist of only one concept without confounds. BrainSCUBA can allow a deeper investigation of social interaction through verbs , subject-object pairs and finding which regions are selective for specific interactions/emotions. I emphasize that I mentioned this in the weakness section as I would have loved to see more new results of semantic specificity (other than confirmations)."
                },
                "questions": {
                    "value": "1. Is the semantic selectivity found here limited by training set (NSD)? Can we expect to see semantic specificity that is present in CLIP training data but not in NSD images?\n2. What was the intuition behind the choice  \u201cA photo of a/an [NOUN]\u201d. Did you consider other prompts ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission298/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840916411,
            "cdate": 1698840916411,
            "tmdate": 1699635955954,
            "mdate": 1699635955954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3UwZNEEzxb",
                "forum": "mQYHXUUTkU",
                "replyto": "k6704HH7eb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to xxME"
                    },
                    "comment": {
                        "value": "We thank the Reviewer xxME for the helpful review and the positive evaluation of our work. We address your suggestions below, and update the revision to provide further clarification.\n\n> **Q1) Figure 3 clarity**\n\nWe appreciate the suggestion! Indeed, we should provide additional context in the caption of Figure 3 using the color code.\n\nTo clarify, the color code is derived from data without explicit access to the region of interest (ROI) masks. The purpose of **Figure 3** is to show that our method can recover the semantic selectivity across the broader higher order visual regions by aligning the embedding of a brain encoder (which uses a contrastive vision-language CLIP image backbone) with text embeddings from a CLIP text encoder.\n\nIn **Figure 3**, the trace of ROIs are derived from an independent functional localizer task (stanford VPNL fLoc) provided in NSD for different semantic categories, including faces (FFA/aTL-faces)/places (RSC/OPA/PPA)/bodies (EBA).\n\n**We have clarified the ROIs using the color codes in the revision following your suggestions.**\n\n> **Q2) Potential of BrainSCUBA**\n\nWe want to clarify that our system generates full natural language captions. These captions include nouns, adjectives, and subject/object interactions. We include examples of these captions in **Figure 1c**. \n\nOur purpose for focusing on noun based analyses in experiment 1 **(section 4.2)** and experiment 3 **(section 4.4 Table 7)** is primarily because nouns correspond to semantic categories, and enables a \"sanity check\" on the broader results given the broad consensus on category selectivity in higher order visual areas. \n\nIndeed our UMAP results, show that our methods can independently recover regions of food selectivity (yellow) corresponding to the food area (discovered in 2022 by three groups independently in [1,2,3]) in humans without using food specific stimulus.\n\nWe further analyze the occurrence frequency of multiple-people and a single person in Figure 8B, and perform human behavioral study in **Table 3 and Supplemental section A.6** to show that our method corresponds well with human perceptual judgements. **We believe this observation of disjoint semantic coding in the EBA region is a novel discovery.**\n\nIn response to your suggestion, **we have added additional plots of adjective frequency and example sentences in Figure S.14 on page 33 of the revision**, extracted from the BrainSCUBA generated captions using the Spacy part-of-speech engine. We hope this provides additional insight into the power of our method.\n\n> **Q3) Limits of semantic selectivity**\n\nThis is a very interesting question. NSD is the largest 7T fMRI visual dataset, with 10000 images shown to each of 8 subjects. However 10000 is probably small relative to human visual experience. \n\nIn theory it is possible that the CLIP embeddings could reveal selectivity to concepts not present in the NSD stimulus set, even when the encoder is fit on only NSD images. \n\nConsider the case where a voxel is perfectly monotonic w.r.t. orthodromic distance in CLIP space, and that the CLIP embeddings of all natural images form a (geodesic) convex set. Given the monotonic condition, it is possible that an image on the boundary of this set is predicted to be highly activating, even when this image is not present in the stimulus set. However, the monotonicity assumption is very strong and not true in practice, as it basically assumes the human brain reacts to visual stimuli exactly the same as a CLIP neural network. The language model itself is also an area of potential improvement. So to summarize, while it is possible in theory, the assumptions needed are strong.\n\nWe believe collection of larger fMRI datasets and use of more powerful language models would help reveal additional selectivity.\n\n> **Q3) Prompt format**\n\nOur prompts used for Figure 3 follow the practice established by [1] (see section **3.1.4** of their paper), which popularized the use of this prompt format for zero-shot ImageNet classification. Recent work by CuPL [2] has established more sophisticated prompt formats which leverage GPT-3 augmentation. The original format is more commonly used, and we stuck with it in our work.\n\n[**1**] Learning Transferable Visual Models From Natural Language Supervision\n\n[**2**] What does a platypus look like? Generating customized prompts for zero-shot image classification\n\n\u2800\n\u2800\n\nThank you again for your wonderful suggestions, and we\u2019re eager to hear your thoughts on our clarifications. Please let us know if you have any additional questions or comments!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169554579,
                "cdate": 1700169554579,
                "tmdate": 1700169554579,
                "mdate": 1700169554579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6118BeBDUb",
                "forum": "mQYHXUUTkU",
                "replyto": "k6704HH7eb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_xxME"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_xxME"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for clarification and new results"
                    },
                    "comment": {
                        "value": "I would like to thank authors for detailed responses to feedback from all the reviewers. I appreciate the additional plots authors generated for adjectives. The results in the plot (Figure S14, page 33) show a strong bias of white , young adjective in faces,  bodies and word regions. While this is not a strong concern, this bias should be mentioned and I would like to read authors thoughts on following\n1. Where does this bias come from e.g. datasets (MS-COCO, NSD) ; models (CLIPCap, CLIP) ; or participants (were all participants in NSD from certain ethnicity); or other possible reasons?\n2. How to address the above bias and other potential biases which may not be evident yet due to this approach not being fully explored?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488479598,
                "cdate": 1700488479598,
                "tmdate": 1700488575646,
                "mdate": 1700488575646,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tnSCXCQgQD",
                "forum": "mQYHXUUTkU",
                "replyto": "k6704HH7eb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on adjectives"
                    },
                    "comment": {
                        "value": "Thank you for your suggestions! \n\nTo clarify, we checked our generated captions, and were not able to identify any instance where \"white\" or \"black\" occurred in a racial sense. \"White\" and \"black\" primarily occurred in the form \"A black and white photo of ...\", and occurring otherwise as a generic non-racial color descriptor. \n\nIn the food region, \"white\" primarily occurs in the form \"... on a white plate\".\n\nThat said, we agree with you that there exist a racial imbalance in MS-COCO. And in such datasets, the percentage of people with a lighter skin color is very over-represented [1]. We believe the frequency of \"young\" likely also comes from caption data.\n\nSuch a problem is also propagated to the diffusion generated images (Figure **S.4** to **S.11**). In the absence of racial imbalance the captions themselves, we believe the diffusion image bias can come from two sources:\n* The CLIP based caption embedding network (trained on a dataset, then frozen for use in diffusion)\n* The images used to train the cross-attention and diffusion model\n\nIn particular, from private correspondence with contributors to Stable Diffusion, we understand that the frozen CLIP embedding model can have a large influence on the final diffusion output, even when the dataset for training the diffusion model is fixed. As an example, we were told that OpenAI's CLIP allows the diffusion model to better model artist styles, while Open-CLIP leads to a relatively poor ability to mimic artist styles, despite the larger CLIP training set used by Open-CLIP. And this happened when the Diffusion model was trained on the same dataset (but just swapped CLIP models). We understand this was one of the motivating reasons why Stable Diffusion XL moved to a dual embedding model that combines OpenAI CLIP with Open-CLIP.\n\nIn addition, the diffusion model itself is trained on [LAION-Aesthetics, scroll down to browse example images](https://laion.ai/blog/laion-aesthetics/), which seems to also over-represent people with light skin.\n\nWe believe one potential mitigation is to adopt weakly constrained captioning models such as ZeroCap [2], which score poorly on objective measures, but generate very diverse captions. Recent work has also explicitly proposed ways to balance racial and gender diversity of diffusion model output [3].\n\n**We have added additional discussion to the ethics section** on potential sources of dataset bias. Thank you for your suggestions! Please let us know if you have any additional comments. \n\n[**1**] Understanding and Evaluating Racial Biases in Image Captioning\n\n[**2**] ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic\n\n[**3**] Unified Concept Editing in Diffusion Models"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517277088,
                "cdate": 1700517277088,
                "tmdate": 1700517392995,
                "mdate": 1700517392995,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FdltX82Lvn",
                "forum": "mQYHXUUTkU",
                "replyto": "k6704HH7eb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_xxME"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_xxME"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for considering the suggestion to include a discussion on biases. I have no more concerns."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603721562,
                "cdate": 1700603721562,
                "tmdate": 1700603754199,
                "mdate": 1700603754199,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U6D1oTXM6W",
            "forum": "mQYHXUUTkU",
            "replyto": "mQYHXUUTkU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission298/Reviewer_aznk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission298/Reviewer_aznk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to combine fMRI encoding models with pretrained vision-language models to find captions for different voxels in visual cortex. They use this method to perform exploratory data analysis on an existing fMRI data (Natural Scenes Dataset, NSD). They find that the method can reconstruct existing known distinctions in preferences in visual cortex (places, faces, bodies, food, words), and can uncover heretofore unrecognized distinctions in the extrastriate body area and in the social network of the brain."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, this is an interesting application of pretrained vision-language models for better understanding how the brain is organized. The exploration of fine-grained distinctions in the social network of the brain (section 4.4) is quite convincing, especially given the human evaluation results. The paper is clearly written and the demonstration of the use case would, I believe, be of substantial interest to neuroscientists and cognitive scientists."
                },
                "weaknesses": {
                    "value": "I don't believe that this submission is well-suited for a machine-learning-focused conference such as ICLR. It uses off-the-shelf, pretrained models to find information about visual cortex, which would be primarily of interest to neuroscientists and cognitive scientists. I cannot find substantial methodological advances here that would be of general interest to a conference aimed at machine learning researchers."
                },
                "questions": {
                    "value": "My concerns are not about the soundness of the work\u2013which is fine\u2013but about the appropriateness for publication in a machine learning conference. I don't think that there is much extra work the authors could do to convince me otherwise. I'm open to re-evaluation of the paper's merits if the other reviewers deem it appropriate for this conference. \n\nNevertheless, I do have some minor comments:\n\nPage 2: acitvations -> activations\nPage 3-4: I found Figure 2b and its accompanying justification inscrutable. If the point is that there are no images close to unit sphere of captions, and hence blending (eq. 4) must be used to find something closer to the manifold of natural images, this does a poor job of conveying that, and text would be a better way of communicating that. If there is a different point they're trying to make, the authors should take a few sentences to explain what it is. \nPage 7-8: I found the attempt at quantification conveyed in Figures 6 and Table 1 of dubious relevance. If the point of the method is to find visually coherent images that are easy for a human to understand the gist of, using sample images from the NSD itself would do just as well (e.g. Borowski et al. 2021). If the point is to get a better view of what an entire area is selective to, then it seems BrainDiVE works better. The authors should clearly state what they're trying to convey in these figures and tables, they take up a lot of space in the paper but don't add much, in my opinion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission298/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission298/Reviewer_aznk",
                        "ICLR.cc/2024/Conference/Submission298/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission298/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871626399,
            "cdate": 1698871626399,
            "tmdate": 1700285288390,
            "mdate": 1700285288390,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2jBrNXhHsD",
                "forum": "mQYHXUUTkU",
                "replyto": "U6D1oTXM6W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to aznk (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer aznk for the constructive review. We appreciate your recognition of the paper's technical soundness and potential interest to the fields of neuroscience and cognitive science - we address our paper's domain appropriateness for ICLR below. We also value your specific suggestions, which we have carefully considered in our responses below and in revisions to our paper.\n\n> **Q1) Suitability for ICLR**\n\nWhile we acknowledge your assessment regarding the suitability of our paper for ICLR, we argue that consistent with prior presentations at ICLR, BrainSCUBA contributes to the field by showcasing an *innovative application* of pre-trained vision-language models in field of neuroscience. \n\n\nOur paper was submitted under the **applications to neuroscience & cognitive science** primary area on OpenReview, one of the **primary areas officially recognized by ICLR 2024 in the [call for papers](https://iclr.cc/Conferences/2024/CallForPapers#:~:text=applications%20to%20neuroscience%20%26%20cognitive%20science)**. Since its inception in 2013, ICLR has included applications in neuroscience in the [topics of interest](https://iclr.cc/archive/2013/call-for-papers.html). The increasing prominence of computational neuroscience and cognitive science at ICLR is evidenced by a growing body of neuroscience papers published at ICLR ([see this anonymous list of ICLR 2020~2023 papers](https://github.com/anon-0518/ICLR_neuro_papers/tree/main)).\n\n\nIn 2023, the following applications in neuroscience paper appeared at ICLR as an \"oral\": Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness. It is important to note that this paper's primary contribution was in the field of computational neuroscience and, methodologically, relied on straightforward fine-tuning of a pre-trained model.\n\n\n\nKey contributions of our paper include:\n* By using an encoder trained on a large dataset, we recover a *less noisy* estimate of neural semantic alignment (see **Figure 5**). As the encoder is parameterized as a linear probe of a unit-norm embedding, we can derive the optimal CLIP embedding efficiently with a normalization operation, bypassing the slow gradient-based optimization used in BrainDiVE.\n* We demonstrate that these embeddings diverge from natural CLIP image embeddings using UMAP. This motivates us to use a parameter-free method for aligning encoder weights with natural embeddings and generating voxel-specific captions. We show that our method can yield high categorical specificity, high predicted activations, and yield novel insights.\n* Our work pioneers voxel-wise semantic explanations in the brain setting, moving beyond traditional region-based approaches like NeuroGen/BrainDiVE. We have adapted and applied vision-language models in a way that has not been done before, particularly in the context of understanding the human brain's visual cortex.\n\nWe do note with appreciation your willingness to reconsider based on broader reviewer feedback. As the other reviewers have not disagreed with our paper's suitability for ICLR, and two reviewers are positive about our work's contribution, **we hope you will reconsider the alignment of our work with ICLR's commitment to interdisciplinary research and the explicit call for papers on \"applications to neuroscience & cognitive science\"**. \n\n> **Q2) Typos**\n\nThank you, we have corrected the typos in the revision of our work.\n\n> **Q3) Clarification of Figure 2b**\n\n**Figure 2b** shows that even when both the encoder weights and CLIP Image embeddings are projected to the *same unit-sphere*, there still exists a gap between the two embeddings. This plot was specifically motivated by [1](NeurIPS 2022).\n\nWe aim to be consistent with prior work on identifying CLIP Image & CLIP Text embedding gaps, and use the same technique -- performing UMAP on two sets of embeddings simultaneously -- in showing a brain voxel embedding & CLIP Image embedding gap.\n\n**Following your suggestion, we have further clarified our motivation in the revision text.**\n\n[**1**] Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning (NeurIPS 2022)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169247348,
                "cdate": 1700169247348,
                "tmdate": 1700230134561,
                "mdate": 1700230134561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YhyyioWfrk",
                "forum": "mQYHXUUTkU",
                "replyto": "U6D1oTXM6W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to aznk (2/2)"
                    },
                    "comment": {
                        "value": "> **Q4) Clarification of Figure 6 and Table 1**\n\nThank you for bringing to our attention the two papers by Borowski and Zimmermann (2021), we agree they are interesting papers **and have included them as a citation in our revision.**\n\n**In neuroscience, novel captions/images provide a purpose beyond top-images, as they can act as a basis for future hypothesis based studies.** This has been demonstrated in [2,3,4] which utilized computationally identified selectivity to perform additional fMRI studies.\n\nThe two figures you reference seek to explore the performance tradeoffs in BrainSCUBA versus BrainDiVE in achieving a 10x speedup. A major postive of BrainSCUBA compared to BrainDiVE is the magnitude improvement in speed. BrainDiVE has extremely high computational costs due to the explicit gradient based optimization used. This poses an obstacle in using BrainDiVE for computational characterization of the entire visual cortex which contains tens of thousands of voxels.\n\nOn the same hardware with a V100 GPU, BrainDiVE uses around **20~30 seconds per image** (Page 41 of their paper), while our approach uses **2~3 seconds per caption + image**. Note both times ignore the training of the encoder, which is amortized for all future use. This magnitude improvement in performance is desirable if you want to explore finer grained regions in the brain. \n\n**Figure 6** qualitatively shows despite the 10x speed improvement, when evaluated using a different brain encoder backbone with state of the art imagenet performance (EVA02-CLIP trained using masked image modeling + CLIP objective), our generated images still achieve very high predicted activations that are only slightly lower than BrainDiVE.\n\n**Table 1** quantifies the semantic specificity using the exact same metrics in BrainDiVE. This shows that on average our method yields only a 3.8% and 9.6% gap in semantic specificity in S2 and S5. **We have further clarified our reasoning in the revision.**\n\n[**2**] Computational models of category-selective brain regions enable high-throughput tests of selectivity\n\n[**3**] Human brain responses are modulated when exposed to optimized natural images or synthetically generated images\n\n[**4**] Selectivity for food in human ventral visual cortex\n\n\u2800\n\u2800\n\n\nWe respect your expertise and perspective, and we appreciate the opportunity to address your concerns. We hope that you might be willing to consider a more positive evaluation of our work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169283344,
                "cdate": 1700169283344,
                "tmdate": 1700230154346,
                "mdate": 1700230154346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y9CdksmWHw",
                "forum": "mQYHXUUTkU",
                "replyto": "YhyyioWfrk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_aznk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_aznk"
                ],
                "content": {
                    "comment": {
                        "value": "I think the list of previous neuroscience publications is pretty convincing. I had looked up 2020-2021 and couldn't find many hits, but it looked like they ramped up in 2022\u20132023. Since the other reviewers think it fits, and I have no qualms about the science, I've bumped up the rating to a 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission298/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285497360,
                "cdate": 1700285497360,
                "tmdate": 1700285497360,
                "mdate": 1700285497360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]