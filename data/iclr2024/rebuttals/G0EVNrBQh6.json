[
    {
        "title": "Investigating Human-Identifiable Features Hidden in Adversarial Perturbations"
    },
    {
        "review": {
            "id": "PRxBSqrlYg",
            "forum": "G0EVNrBQh6",
            "replyto": "G0EVNrBQh6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_MnMs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_MnMs"
            ],
            "content": {
                "summary": {
                    "value": "This work identifies the presence and effect of human-identifiable features in adversarial perturbations. The authors recognize that individual perturbations on a single input, while successful at fooling a model, do not produce distinct features that can be readily interpreted by humans. They posit that this is due to the presence of noise in the perturbations, and introduce a methodology to help overcome this by averaging many perturbations on the same image. The result produces perturbations that are significantly more human understandable, as demonstrated through a human evaluator experiment. With these new perturbations, they identify two different effects that these perturbations have on their input: masking, which covers prominent features of the true class of the image, and generation, which creates prominent features of the target class. Overall, this work provides insights into features created in adversarial examples, introduces methodology that can increase explainability in the presence of adversarial examples, and provides explanations from their findings for well known phenomena in adversarial training, transfer ability attacks, and interpretability."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Thank you for your submission! I thoroughly enjoyed reading this paper; the results were compelling, the methodology was sound, the contributions and findings are novel and useful, explanations were clear, and I was surprised at how recognizable the generated perturbations were.\n\nSome specific highlighted results/conclusions/contribution:\n- As mentioned in the paper, there is a significant need for work that provides explanations for reasons as to why attacks are as successful as they are and why models are as vulnerable to adversarial examples as they are. This work bridges these two approaches by (a) evaluating a variety of attacks and (b) creatively extracting portions of perturbations that are well aligned across models and thus represent features that transfer across models\n- The perturbations generated with this method were significantly clearer/more recognizable to me as a reader. Additionally, I felt that the claim of generating human recognizable perturbations was well supported by also incorporating the results showing that (a) human evaluators were able to recognize perturbations without associated inputs from the MM+G method at a rate significantly higher than random guessing and (b) the perturbations generated in the MM+G setting yield far more successful adversarial examples than the standard SM case\n- The discussion section connected multiple trends in transferability, adversarial training, and clean/robust accuracy tradeoffs to reasonable explanations based on insights from this work."
                },
                "weaknesses": {
                    "value": "The breadth of experiments done was extensive, but I felt that in certain places, the depth of individual experiments could have been improved. Specifically:\n- I would have preferred to see more samples per class evaluated (10 seems quite small to me)\n- In the human evaluator test, I understand the limitation of testing all the attacks/settings but at the very least both settings under one attack should have been evaluated. At present, it is hard to give meaning to the 80.7% human evaluator accuracy under the BIM MM+G setting since there is not a BIM SM setting to compare it to. It would also be helpful to provide some justification for why BIM (over the other attacks) was chosen for this experiment.\n- Similar to the previous point, including SM settings in the cosine similarity experiment would have been helpful to get a baseline sense of how similar perturbations usually are to each other and to see if the MM+G setting yields significantly different values.\n\nAdditionally, the paper is clear and concise as written, but there were some portions that could benefit from additional details, explanations, or citations, mainly in Section 4 (Experimental Method). \n\nSpecific (minor) suggestions for improvement:\n- The notion of \"incomplete components of the associated features\" was lacking definition/explanation, adding some details around what this is supposed to represent would be helpful.\n- The problem of \"the number of available neural networks being limited\" didn't feel clear/well motivated. There are many parameters that can be adjusted to produce different models (seeds, hyperparameters, optimizer, architecture, etc.). Further, it wasn't clear how the solution of applying noise to produce more inputs solved this problem. \n- Some more citations to help support the contour extraction experiment would be helpful, particularly for claims that make statements about portions of the image that humans use for classification."
                },
                "questions": {
                    "value": "- How were the subset of classes chosen?\n- How were the 200 inputs chosen? Were there any constraints or conditions for these inputs? Were all samples chosen correctly classified by all models?\n- While it does appear that adding noise to produce additional inputs works well, the inspiration/motivation for doing this wasn't exactly clear. Why add noise rather than performing some kind of data augmentation? \n- Why was the standard deviation of noise added to the inputs different for the different attack algorithms?\n- Why were 270 models chosen for generating perturbations? Were these experiments tried with fewer models (besides the single model case)?\n- It is mentioned in the human evaluator test that the lowest and highest accuracy in each subset was discarded before calculating the average. What was the purpose of this? And can you clarify exactly what was discarded (e.g., was data for a single sample removed from all participants or was data from a single participant removed from all samples?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695861140,
            "cdate": 1698695861140,
            "tmdate": 1699636432539,
            "mdate": 1699636432539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SxeKk7Nj98",
                "forum": "G0EVNrBQh6",
                "replyto": "PRxBSqrlYg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank You for Your Comment"
                    },
                    "comment": {
                        "value": "We wholeheartedly appreciate your encouragement and suggestions. Thank you very much.\n\n\n>How were the subset of classes chosen and 200 inputs chosen?\n\nWe carefully selected 20 classes from the ImageNet dataset to ensure maximal diversity in our study, covering a wide array of subjects such as animals, plants, architecture, toys, transportation, and utilities. To avoid human bias, we adopted a systematic approach, selecting the first 10 images from each class rather than hand-picking them.\n\n>While it does appear that adding noise to produce additional inputs works well, the inspiration/motivation for doing this wasn't exactly clear. Why add noise rather than performing some kind of data augmentation?\n\nWe believe that perturbations contain noise that may result in meaningless local variations [1]. As a consequence, introducing different Gaussian noises to the same input image may lead to different local variation patterns. Therefore, averaging those perturbations can effectively reduce the noise residing within them. \n\nWe believe that the changes in local variations are independent of specific data augmentation techniques because their variation is meaningless. Consequently, we anticipate that different augmentation techniques will yield similar results. However, adding noise offers the advantage of easily controlling the strength of augmentation by adjusting its standard deviation. This is the reason why we choose to add noise instead of performing data augmentation.\n\nReference:\n\n[1] Smoothgrad: Removing noise by adding noise. Daniel Smilkov, et al. Workshop on Visualization for Deep Learning. 2017.\n\n> Why was the standard deviation of noise added to the inputs different for the different attack algorithms?\nIn our research, Gaussian noise serves as an additional tool to further reduce the noise in perturbations. We found that different attack algorithms may need different levels of Gaussian noise to result in optimal clarity for human-identifiable features.\n\nFor BIM attacks, Gaussian noise with a standard deviation of 0.02 produces clear and pronounced human-identifiable features. CW and DeepFool attacks with a standard deviation of 0.02 also exhibit clear human-identifiable features. However, with a standard deviation of 0.05, the clarity of these features will be further enhanced. Consequently, we set the standard deviation for BIM attacks to 0.02 and for CW and DeepFool attacks to 0.05.\n\nWe would like to emphasize that averaging perturbations from different models is sufficient for the emergence of human-identifiable features, as discussed in Appendix A. Adding Gaussian noise only serves as an additional tool to further reduce perturbation noise.\n\n>Why were 270 models chosen for generating perturbations? Were these experiments tried with fewer models (besides the single model case)?\n\nWe have conducted additional experiments to investigate the impact of the number of models used for averaging perturbations on the mean squared error (MSE) compared to perturbations from the MM setting. This analysis may give us a better insight into the number of models required to observe the emergence of human-identifiable features.\n\nPrior to calculating the MSE score, we normalized the averaged perturbations using standard deviations and means derived from ImageNet datasets. This normalization ensures that the resulting MSE is on a comparable scale to the MSE scores calculated from images sampled from the ImageNet dataset, which have input values ranging between 0 and 1.\n\nOur findings reveal that to achieve MSE convergence within 0.05, an average of 25 models is required for the three attack algorithms. For an MSE of 0.02, 90 models are necessary, while an MSE of 0.01 needs 157 models. For detailed information, please refer to Appendix I.\n\n>It is mentioned in the human evaluator test that the lowest and highest accuracy in each subset was discarded before calculating the average. What was the purpose of this? And can you clarify exactly what was discarded (e.g., was data for a single sample removed from all participants, or was data from a single participant removed from all samples?)\n\nTo eliminate outliers, we excluded data from two participants in each group: one with the highest accuracy and another with the lowest accuracy. This decision was made based on statistical considerations, as we observed that within each group, some individuals may perform significantly better or worse than the norm. By removing these outliers, we aim to provide an averaged human perspective on the evaluation of human assessment."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720145116,
                "cdate": 1700720145116,
                "tmdate": 1700720271775,
                "mdate": 1700720271775,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mAjwoDCgTu",
            "forum": "G0EVNrBQh6",
            "replyto": "G0EVNrBQh6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the human-identifiable features that are concealed within adversarial perturbations. To this end, this paper utilizes 270 models as surrogate models, introduces Gaussian noise to the input, and identifies the human-identifiable features. This paper shows that in targeted attacks, these features typically demonstrate a \"generation effect\" by producing features or objects of the target class. In contrast, in untargeted attacks, these features exhibit a \"masking effect\" by hiding the features or objects of the original class. This paper further claims the revealed phenomenon can interpret some properties of adversarial perturbations."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This paper revisits a critical concept in the context of adversarial robustness: the underlying mechanism of adversarial perturbations.\n2. This paper conducted human tests to verify that the emergence of semantic features is not coincidental, which is of importance.\n3. This paper validates the hypothesis across targeted and untargeted attacks and includes search-based attacks."
                },
                "weaknesses": {
                    "value": "This paper challenges a well-acknowledged phenomenon in the context of adversarial robustness: the *perceptual aligned gradient* (PAG), which refers to the **human-identifiable features** that align with human perception in adversarial perturbations, only exists in robust models [1-3]. However, this paper claims that such features are also hidden in the perturbations of standardly trained (non-robust) models, which contradicts the current understanding of PAG. This concept of PAG has been well supported by various empirical and theoretical analyses in the follow-up works, along with its various applications. Therefore, in my opinion, to challenge the existing theories that contradict the claim made, this paper should provide sufficient theoretical and empirical evidence to support the proposed claims. Unfortunately, not only has the evidence in this paper already been discovered or directly deduced by previous work, but they also cannot explain the contradicted theories, which I specify below.\n\n1. The experiment uses Gaussian noise to average the perturbations to reveal the human-identifiable features. However, this phenomenon has already been revealed in [4], which shows that randomized smoothing (adding Gaussian noises to the input and calculating the averaged gradient) on a single standardly trained model can lead to PAG and generate these features. Therefore, it's not a newly discovered phenomenon claimed in this paper that averaging gradient among perturbations with different noises can lead to human-identifiable features.\n2. The experiment also averages different models to reveal the human-identifiable features. However, this phenomenon is expected based on existing work [5, 6], which shows that a little adversarial robustness of the models can lead to PAG. Specifically, as ensembling more non-robust models can still enhance adversarial robustness to a certain extent, though not as robust as adversarially trained models, it can be inferred that the ensembled model can lead to such PAG and identifiable features. Even if this paper shows that the robust accuracy of the ensembled model against adversarial attacks is still low (in Figure 3), the enhanced robustness may still be sufficient to bring such PAG.\n3. In addition, it has also been shown [7] that the distribution of non-robust features [17] varies across different model architectures. Therefore, intuitively, the gradient (perturbation) of a single model (or a single kind of model architecture) may be noisy, but by averaging the gradients from different models, it is possible to converge toward the robust features.\n\nBased on these discussions, the discovery made in this paper is somewhat trivial, since the observed phenomenons have already been revealed in existing work or can be directly deducted from them. Furthermore, the evidence presented in this paper is insufficient to challenge the well-established theories of PAG, as this paper does not provide a clear explanation of the contradictions or confusions, which I specify below.\n\n4. There exist several works [8-10] aim to explain the reason PAG only exists in robust models by characterizing the decision boundaries between different models, which is well supported by theoretical analysis. These works show the fundamental difference of decision boundaries between standard and adversarially trained models leads to the (non-)existence of PAG, which contradicts the claim made in this paper in Section 7(2) that human-identifiable features also exist in non-robust models. Unfortunately, this paper does not discuss this viewpoint and does not conduct a theoretical analysis to overturn these theories.\n5. There also exist theories interpreting the existence of PAG in robust models by modeling adversarial training as energy-based models [11-12]. Additionally, the robust model also provides better guidance during the generation process of diffusion models [13-14], indicating the importance of robust models with PAG for better gradient and generation guidance. Since such a generation process requires multi-step sampling, which can be regarded as applying an **average (ensemble)** of gradients (perturbations) to the standardly trained model, this also contradicts the viewpoint in this paper and should be well-explained.\n6. In Section 7(1), the explanation for the transferability of adversarial examples contradicts existing works. This paper attributes the transferability to the human-identifiable (robust) features, but existing works [15-16] show that robust features may not be always helpful for adversarial examples transferring between models and non-robust features still play a crucial role in transferring adversarial examples. Therefore, the claims made in this paper fail to explain the transferability of adversarial examples across models.\n7. The explanation of non-trivial accuracy for classifiers trained on a manipulated dataset [17] made in Section 7(3) is flawed. It is clear that in the manipulated dataset, which includes perturbations claimed as human-identifiable features in this paper, the features from the original class are still dominant over the perturbations. According to the interpretation within this paper, the model should still learn the features from the original class and cannot achieve clean accuracy in this noisy training setting. This contradicts the explanation proposed in this paper.\n8. In Appendix A, Figure 7, it appears that the masking effect of the perturbation without Gaussian noise significantly reduces the identifiability of human-identifiable features, compared to the results in the main paper (with Gaussian noise). Therefore, it can be inferred that ensembling Gaussian noise plays a more crucial role in generating the human-identifiable features than ensembling different models, which undermines the soundness of the claim that the presence of human-identifiable features is inherent in the perturbations themselves, rather than being a result of added Gaussian noise.\n9. There is a lack of ablation studies on the number of models to further support their claims. It is suggested to add experiments to analyze how many models or noises are required to emerge such human-identifiable features, which can provide a more intuitive view of how noisy the gradients are in the adversarial perturbations.\n10. For transfer attacks, this paper only compares BIM, CW, and DF, which are not specifically designed for transfer attacks. It is suggested to add a comparison with existing state-of-the-art transfer attackers, e.g., MI-FGSM [18], DI-FGSM [19], and ensemble attacker CWA [20], to substantial the claims regarding transfer attacks. Since this paper claims that the success of transfer attacks is based on hidden human-identifiable features, it can be inferred that transfer attacks can emerge with more human-identifiable features, which should be supported by experiments on evaluating these attacks designed for transferring.\n11. There is no statement on open sourcing and reproducibility. Since finding such 270 surrogate models is challenging to reproduce, I strongly suggest releasing the code.\n\n[1] Robustness May Be at Odds with Accuracy. ICLR 2019\n\n[2] Image Synthesis with a Single (Robust) Classifier. NeurIPS 2019\n\n[3] Adversarial Robustness as a Prior for Learned Representations. arxiv 1906.00945\n\n[4] Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?. NeurIPS 2019 Workshop\n\n[5] On the Benefits of Models with Perceptually-Aligned Gradients. ICLR 2020 Workshop\n\n[6] A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks. NeurIPS 2021\n\n[7] Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets. ICLR 2021\n\n[8] Bridging Adversarial Robustness and Gradient Interpretability. ICLR 2019 Workshop\n\n[9] On the Connection Between Adversarial Robustness and Saliency Map Interpretability. ICML 2019\n\n[10] Robust Models Are More Interpretable Because Attributions Look Normal. ICML 2022\n\n[11] Towards Understanding the Generative Capability of Adversarially Robust Classifiers. ICCV 2021\n\n[12] A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training. ICLR 2022\n\n[13] Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance. TMLR\n\n[14] BIGRoC: Boosting Image Generation via a Robust Classifier. TMLR\n\n[15] Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently. WACV 2023\n\n[16] Why Does Little Robustness Help? Understanding and Improving Adversarial Transferability from Surrogate Training. S&P 2024\n\n[17] Adversarial Examples are not Bugs, they are Features. NeurIPS 2019\n\n[18]  Boosting adversarial attacks with momentum. CVPR 2018.\n\n[19] Improving transferability of adversarial examples with input diversity. CVPR 2019.\n\n[20] Rethinking Model Ensemble in Transfer-based Adversarial Attacks. arXiv:2303.09105"
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4550/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4550/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742370600,
            "cdate": 1698742370600,
            "tmdate": 1699636432431,
            "mdate": 1699636432431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TKdjIReOSt",
                "forum": "G0EVNrBQh6",
                "replyto": "mAjwoDCgTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We Respectfully Disagree With the Reviewer's Comment (Part 1)"
                    },
                    "comment": {
                        "value": "We respectfully disagree with the reviewer's comment, yet we appreciate the reviewer's time. The reviewer clearly has an opposite view from ours on the origin of adversarial perturbations. This contrast in views, in our opinion, has led to a very critical tone in the reviewer\u2019s comments. In the following, we respond to each of the reviewer's points, providing clarifications and counterarguments where necessary.\n\nPlease note that references starting with R are referred to as the reviewer's references.\n\n>1. The experiment uses Gaussian noise to average the perturbations to reveal the human-identifiable features. However, this phenomenon has already been revealed in [R4], which shows that randomized smoothing (adding Gaussian noises to the input and calculating the averaged gradient) on a single standardly trained model can lead to PAG and generate these features. Therefore, it's not a newly discovered phenomenon claimed in this paper that averaging gradient among perturbations with different noises can lead to human-identifiable features. \n\nThe reviewer confused our findings with those in [R4]. Our research reveals that human-identifiable (robust) features naturally occur in adversarial perturbations of 'standard-trained' neural networks, even without adding Gaussian noise. This stands in contrast to [R4], which associates perceptually aligned gradients with the robustness of a classifier, leading to a markedly different conclusion.\n\nA method for creating a robust classifier involves randomized smoothing, which adds Gaussian noises with a significant standard deviation (0.5 as mentioned in [R4]) to the original image and implicitly averages the model's predictions. The effectiveness of this technique lies in its certified radius, which assures the classifier's robustness within a certain range of perturbations and is proportional to the added Gaussian noises' standard deviation, as confirmed by earlier research work[1].\n\nIn [R4], it is noted that as the standard deviation of incorporated Gaussian noise increases, human-perceptible features become observable and evident in perturbations. Hence, the author suggests that the increase of model's robustness may lead to the presence of perceptually aligned gradients.\n\nOur study, however, presents a different scenario. We demonstrate that human-identifiable (robust) features in adversarial perturbations are discernible when averaging perturbations from various models, even without Gaussian noise. This suggests that robust features inherently exist in standardly trained models, as averaging perturbations do not add new features to the original perturbation. Therefore, the distinction between our work and the study in [R4] is evident."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639986204,
                "cdate": 1700639986204,
                "tmdate": 1700639986204,
                "mdate": 1700639986204,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5RNIk8XnwH",
                "forum": "G0EVNrBQh6",
                "replyto": "mAjwoDCgTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We Respectfully Disagree With the Reviewer's Comment (Part 3)"
                    },
                    "comment": {
                        "value": ">5. There also exist theories interpreting the existence of PAG in robust models by modeling adversarial training as energy-based models [R11-R12]. Additionally, the robust model also provides better guidance during the generation process of diffusion models [R13-R14], indicating the importance of robust models with PAG for better gradient and generation guidance. Since such a generation process requires multi-step sampling, which can be regarded as applying an average (ensemble) of gradients (perturbations) to the standardly trained model, this also contradicts the viewpoint in this paper and should be well-explained. \n\nIn our research, we show, without a shred of doubt, that human-identifiable features exist in perturbations of standard-trained models. We firmly believe that theory is always capable of explaining experimental findings.\n\n>6. In Section 7(1), the explanation for the transferability of adversarial examples contradicts existing works. This paper attributes the transferability to the human-identifiable (robust) features, but existing works [R15-R16] show that robust features may not be always helpful for adversarial examples transferring between models and non-robust features still play a crucial role in transferring adversarial examples. Therefore, the claims made in this paper fail to explain the transferability of adversarial examples across models. \n\nIn our research, we isolated human-identifiable features from perturbations and discovered their significant attack strength, leading us to conclude that these features contribute to the misclassification of a model. We see no conflict between our findings and those in [R15, R16], as [R15, R16] do not deny the strong transferability of robust features to deceiving models.\u2003\n>7. The explanation of non-trivial accuracy for classifiers trained on a manipulated dataset [R17] made in Section 7(3) is flawed. It is clear that in the manipulated dataset, which includes perturbations claimed as human-identifiable features in this paper, the features from the original class are still dominant over the perturbations. According to the interpretation within this paper, the model should still learn the features from the original class and cannot achieve clean accuracy in this noisy training setting. This contradicts the explanation proposed in this paper. \n\nThe term \"dominant\" should be used with caution. While the original images' features are larger in pixel value, the perturbations generated by the models may align more closely with what the models learn, potentially playing a more significant role during training than that of the original features. The discussion of which feature dominates in training is not the focus of this research. Since we have yet to demonstrate what types of features are more dominating, we put an emphasis in our paper that only partial explanation can be offered."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640429454,
                "cdate": 1700640429454,
                "tmdate": 1700641619320,
                "mdate": 1700641619320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "67eUbDNGLi",
                "forum": "G0EVNrBQh6",
                "replyto": "mAjwoDCgTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We Respectfully Disagree With the Reviewer's Comment (Part 4)"
                    },
                    "comment": {
                        "value": ">8. In Appendix A, Figure 7, it appears that the masking effect of the perturbation without Gaussian noise significantly reduces the identifiability of human-identifiable features, compared to the results in the main paper (with Gaussian noise). Therefore, it can be inferred that ensembling Gaussian noise plays a more crucial role in generating the human identifiable features than ensembling different models, which undermines the soundness of the claim that the presence of human-identifiable features is inherent in the perturbations themselves, rather than being a result of added Gaussian noise. \n\nThe statement made by the reviewer that the presence of human-identifiable features is due to the incorporation of Gaussian noise is incorrect. Two pieces of information illustrate the incorrectness: \n\n1. In the search-based attack, no Gaussian noise is added, yet human-identifiable features still emerge, as shown in Section 5.2.4 and Appendix D.\n2. In Appendix A, we have performed an experiment on perturbations generated without the incorporation of Gaussian noise (MM setting). It is clear that human-identifiable features is still observed from perturbations, as demonstrated in Appendix A1.\n\nAdditionally, we disagree with the reviewer's assertion that averaging perturbations from various models (MM setting) significantly diminishes the perceptibility of human-identifiable features. The cosine similarity between perturbations generated separately from MM+G and MM setting for BIM attack reaches as high as 0.80 based on an average of 200 perturbations. This high similarity indicates that Gaussian noise is not critical to influencing the perceptibility of perturbations. \n\n> 9. There is a lack of ablation studies on the number of models to further support their claims. It is suggested to add experiments to analyze how many models or noises are required to emerge such human-identifiable features, which can provide a more intuitive view of how noisy the gradients are in the adversarial perturbations.\n\nWe have added experiments analyzing the model count and the convergence for perturbations, please refer to Appendix I. Thank you for your suggestion.\n\nReferences:\n\n[1] Certified Adversarial Robustness via Randomized Smoothing. Jeremy Cohen, et al. ICML 2019.\n\n[2] Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks, Nicolas Papernot, et al. 37th IEEE Symposium on Security and Privacy."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641677117,
                "cdate": 1700641677117,
                "tmdate": 1700641982281,
                "mdate": 1700641982281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G5GOxRvoYt",
                "forum": "G0EVNrBQh6",
                "replyto": "mAjwoDCgTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
                ],
                "content": {
                    "title": {
                        "value": "Further response to the authors (Part 1/n)"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response. I appreciate your time. However, I respectfully maintain my disagreement with your clarifications, which I will explain in detail below.\n\n---\n\n1. *The experiment uses Gaussian noise to average the perturbations to reveal the human-identifiable features. However, this phenomenon has already been revealed in [4], which shows that randomized smoothing (adding Gaussian noises to the input and calculating the averaged gradient) on a single standardly trained model can lead to PAG and generate these features.*\n\nI maintain my viewpoint on this concern. I believe that your clarifications have not yet distinguished your method from [4].\n\n> Our research reveals that human-identifiable (robust) features naturally occur in adversarial perturbations of 'standard-trained' neural networks, even without adding Gaussian noise. This stands in contrast to [R4], which associates perceptually aligned gradients with the robustness of a classifier, leading to a markedly different conclusion.\n\nWhile I acknowledge that [4] focuses on randomized smoothing in [21], the leveraged methods are exactly the same. Specifically, adding Gaussian noise to the sample and averaging the adversarial perturbations, both used in randomized smoothing [4] and in your proposed ``+G``, have the exact same underlying mechanism. Additionally, the model used in [4] is also a standardly trained model, which is still the same as your setting. Therefore, it is difficult to argue that your study presents a different scenario. The claim that adding Gaussian noise and then averaging over adversarial perturbations on standardly trained models can derive robust features has already been discovered.\n\n---\n\n2. *The experiment also averages different models to reveal the human-identifiable features. However, this phenomenon is expected based on existing work [5, 6], which shows that a little adversarial robustness of the models can lead to PAG. Specifically, as ensembling more non-robust models can still enhance adversarial robustness to a certain extent, though not as robust as adversarially trained models, it can be inferred that the ensembled model can lead to such PAG and identifiable features. Even if this paper shows that the robust accuracy of the ensembled model against adversarial attacks is still low (in Figure 3), the enhanced robustness may still be sufficient to bring such PAG.*\n\n> The reviewer then states that our results are expected extension of [5], based on the assumption that a slight increase in the model\u2019s robustness will also increase perturbations\u2019 perceptibility. There is no indication that such assumption can be realized. Verifying such assumption clearly does not fall within our work\u2019s domain.\n\nStill, I maintain my viewpoint that ensembling multiple models into one can improve robustness, albeit not significantly, and thus improve perceptibility. \n\nFirst, it has been demonstrated that model ensemble can enhance adversarial robustness [22-23], thus leading to robust features. While the mentioned works focus on adversarially trained models, their assertion about the connection between ensembling and adversarial robustness can be easily extended to non-robust models. \n\nFurthermore, as stated in [5], the emergence of perceptual aligned gradients (for a single image, without being perturbed by noise) exhibits a strong correlation with model robustness. Therefore, since both methods of averaging over models and introducing perturbations contribute to improving robustness, recognized as randomized smoothing and model ensemble, these conclusions are undoubtedly anticipated.\n\n---\n\n3. *In addition, it has also been shown [7] that the distribution of non-robust features [17] varies across different model architectures. Therefore, intuitively, the gradient (perturbation) of a single model (or a single kind of model architecture) may be noisy, but by averaging the gradients from different models, it is possible to converge toward the robust features.*\n\n> The paper in [R7] proposes an algorithm - the Skip Gradient Method (SGM) - that can increase the transferability of adversarial perturbations. In the paper, nothing is discussed about \u201cthe distribution of non-robust features\u201d. We would appreciate it if the reviewer would show where the paper the claim is written.\n\nThe intuition here is that SGM [7] modifies the algorithm for generating adversarial examples based on the design of the architecture, which is specifically tailored for residual connection models. From this, it can be inferred that different model architectures exhibit different non-robust features. Intuitively, averaging various models leads to convergence towards robust features.\n\n---\n\nIt seems that you missed Weakness 4. I would like to further discuss with you after you fulfill\n> In the following, we respond to each of the reviewer's points, providing clarifications and counterarguments where necessary.\n\nBest Regards,"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643983643,
                "cdate": 1700643983643,
                "tmdate": 1700644858022,
                "mdate": 1700644858022,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LGXnTGbXcK",
                "forum": "G0EVNrBQh6",
                "replyto": "mAjwoDCgTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
                ],
                "content": {
                    "title": {
                        "value": "References (to be updated)"
                    },
                    "comment": {
                        "value": "[1] Robustness May Be at Odds with Accuracy. ICLR 2019\n\n[2] Image Synthesis with a Single (Robust) Classifier. NeurIPS 2019\n\n[3] Adversarial Robustness as a Prior for Learned Representations. arxiv 1906.00945\n\n[4] Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?. NeurIPS 2019 Workshop\n\n[5] On the Benefits of Models with Perceptually-Aligned Gradients. ICLR 2020 Workshop\n\n[6] A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks. NeurIPS 2021\n\n[7] Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets. ICLR 2021\n\n[8] Bridging Adversarial Robustness and Gradient Interpretability. ICLR 2019 Workshop\n\n[9] On the Connection Between Adversarial Robustness and Saliency Map Interpretability. ICML 2019\n\n[10] Robust Models Are More Interpretable Because Attributions Look Normal. ICML 2022\n\n[11] Towards Understanding the Generative Capability of Adversarially Robust Classifiers. ICCV 2021\n\n[12] A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training. ICLR 2022\n\n[13] Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance. TMLR\n\n[14] BIGRoC: Boosting Image Generation via a Robust Classifier. TMLR\n\n[15] Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently. WACV 2023\n\n[16] Why Does Little Robustness Help? Understanding and Improving Adversarial Transferability from Surrogate Training. S&P 2024\n\n[17] Adversarial Examples are not Bugs, they are Features. NeurIPS 2019\n\n[18] Boosting adversarial attacks with momentum. CVPR 2018.\n\n[19] Improving transferability of adversarial examples with input diversity. CVPR 2019.\n\n[20] Rethinking Model Ensemble in Transfer-based Adversarial Attacks. arXiv:2303.09105\n\n[21] Certified Adversarial Robustness via Randomized Smoothing. ICML 2019.\n\n[22] Improving Adversarial Robustness via Promoting Ensemble Diversity. ICML 2019\n\n[23] Self-Ensemble Adversarial Training for Improved Robustness. ICLR 2022"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644109167,
                "cdate": 1700644109167,
                "tmdate": 1700644109167,
                "mdate": 1700644109167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oDJx7wq6cV",
                "forum": "G0EVNrBQh6",
                "replyto": "mAjwoDCgTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Disagree With the Reviewer's Response (Part 1-2)"
                    },
                    "comment": {
                        "value": "We respectfully disagree with the reviewer's comment, yet we appreciate the reviewer's time.\n\n>While I acknowledge that [R4] focuses on randomized smoothing in [R21], the leveraged methods are exactly the same. Specifically, adding Gaussian noise to the sample and averaging the adversarial perturbations, both used in randomized smoothing [R4] and in your proposed +G, have the exact same underlying mechanism. Additionally, the model used in [R4] is also a standardly trained model, which is still the same as your setting. Therefore, it is difficult to argue that your study presents a different scenario. The claim that adding Gaussian noise and then averaging over adversarial perturbations on standardly trained models can derive robust features has already been discovered.\n\nPlease note that references starting with R are referred to as the reviewer's references.\n\nThe perturbations in [R4] are derived from a robust model, as the authors indicate in their abstract: \"In this paper, we show that these perceptually aligned gradients (human-identifiable features) also occur under randomized smoothing, an alternative means of constructing adversarially-robust classifiers.\" The authors observe that as the model's robustness increases (increase the standard deviation of incorporated Gaussian noise), the human-identifiable features in the perturbations become pronounced. They also reported that human-identifiable features are not observed when noise is not incorporated. Consequently, they suggest that human-identifiable features are a consequence of a robust model.\n\nIn contrast, our study reveals that human-identifiable (robust) features are inherently present in standard models. By averaging perturbations from different standard models, even those without Gaussian noise, we uncover human-identifiable features. Notably, this averaging process does not introduce any new information, implying that these robust features are originally embedded in perturbations from standard models. This finding is very different from the conclusions drawn in [R4]. \n\nApparently, our method and conclusion differ from [R4].\n\n\u2003\n>Still, I maintain my viewpoint that ensembling multiple models into one can improve robustness, albeit not significantly, and thus improve perceptibility. First, it has been demonstrated that model ensemble can enhance adversarial robustness [R22-R23], thus leading to robust features. While the mentioned works focus on adversarially trained models, their assertion about the connection between ensembling and adversarial robustness can be easily extended to non-robust models. Furthermore, as stated in [R5], the emergence of perceptual aligned gradients (for a single image, without being perturbed by noise) exhibits a strong correlation with model robustness. Therefore, since both methods of averaging over models and introducing perturbations contribute to improving robustness, recognized as randomized smoothing and model ensemble, these conclusions are undoubtedly anticipated.\n\n[R5] does not make the claim that \u201cthe emergence of perceptual aligned gradients exhibits a strong correlation with model robustness.\u201d Instead in the abstract of [R5], it states \u201cWe perform experiments to show that interpretable and perceptually aligned gradients are present even in models that do not show high robustness to adversarial attacks.\u201d. **This statement differs significantly from the reviewer\u2019s assertion** and, on the contrary, implicitly suggests that perceptually aligned gradients may not necessarily be related to the model\u2019s robustness.\n\nThere is no indication in the papers provided by the reviewer regarding the reviewer\u2019s viewpoint that a slight increase in the model\u2019s robustness will necessarily improve perturbations\u2019 perceptibility. Furthermore, it is noteworthy that several defensive algorithms, such as defensive distillation, marginally enhance the model's robustness. We would be interested to know if the reviewer could offer thoughts on whether perturbations in these defensively augmented models exhibit a perception-aligned gradient."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734178933,
                "cdate": 1700734178933,
                "tmdate": 1700734178933,
                "mdate": 1700734178933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EGscN1Al1O",
                "forum": "G0EVNrBQh6",
                "replyto": "mAjwoDCgTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Disagree With the Reviewer's Response (Part 2-2)"
                    },
                    "comment": {
                        "value": ">The intuition here is that SGM [R7] modifies the algorithm for generating adversarial examples based on the design of the architecture, which is specifically tailored for residual connection models. From this, it can be inferred that different model architectures exhibit different non-robust features. **Intuitively, averaging various models leads to convergence towards robust features**.\n\nA key contribution of our paper is demonstrating the existence of robust features in perturbations derived from standard models, a finding previously unrecognized in the community, as highlighted by the reviewer's previous comment. In a previous comment, the reviewer noted \"This paper challenges a well-acknowledged phenomenon in the context of adversarial robustness: the perceptual aligned gradient (PAG), which refers to the human-identifiable features that align with human perception in adversarial perturbations, **only** exists in robust models [1-3].\" The reviewer also noted \u201cThere exist several works [8-10] that aim to explain the reason PAG **only** exists in robust models by characterizing the decision boundaries between different models, which is well supported by theoretical analysis.\u201d\n\nIn the previous comment, the reviewer stated: \"In addition, it has also been shown [R7] that the distribution of non-robust features [R17] varies across different model architectures.\" As far as we are aware, this does not appear to have been addressed in [R7].\n\n>It seems that you missed Weakness 4. I would like to further discuss with you after you fulfill.\n\nWe have conducted experiments and the results lead to our conclusion. It would be nice to build a theoretical model on what we have found, which is not done as confined by limited manpower and resources."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735234750,
                "cdate": 1700735234750,
                "tmdate": 1700735624947,
                "mdate": 1700735624947,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bLclegtPXn",
            "forum": "G0EVNrBQh6",
            "replyto": "G0EVNrBQh6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_3kT3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_3kT3"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the exploration of the underlying reasons for adversarial perturbations. Specifically, the authors hypothesize that human-identifiable features are present within the perturbations, forming part of the inherent properties of these perturbations. To validate this hypothesis, the authors average perturbations generated by various neural networks to uncover the human-identifiable features."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This work finds that perturbations generated by existing methods statistically contain some human-identifiable features. These are clearly illustrated in the provided qualitative results.\n\n+ To uncover these human-identifiable features, the authors use a simple method which averages extensive generated perturbations, which is reasonable. \n\n+ This paper demonstrates that perturbations produced by certain attack methods converge at the object region.\n\n+ This paper provides a clear narrative, supplemented by analytical insights."
                },
                "weaknesses": {
                    "value": "- In the first paragraph of Section 4, on what basis do you assert that (1) the noise in perturbations is independent and (2) two perturbations from different models display distinct human-identifiable features? I couldn't find any references or evidence supporting the claims. \n\n- The gradient-based attacks, proposed five years ago, aren't sufficiently contemporary to test the paper's hypothesis. There exist many newer gradient-based attacks, such as [1, 2].\n\n- I observed that detecting human-identifiable features necessitates 2,700 samples (270 models and 10 noise-infused seed samples). These may suggest that the averaged perturbation, generated by the three attacking methods, gravitates towards the object region. However, they don't confirm that in every model, the generated perturbations house human-identifiable features. Hence, a deeper experimental analysis regarding model selection and the integration of Gaussian noise would be beneficial, perhaps including more ablation studies (like MM, MM+G, SM+G).\n\n- Why choose only 20 fixed classes out of 1,000? And a mere 200 samples seem insufficient to substantiate the claims made in the paper\n\n- It's noted that perturbations of identical images from varying attack algorithms are presumably alike. However, the results don't include background noise similarity or image perturbation similarity. Providing experimental evidence for this would enhance the argument.\n\n- The experimental analysis concerning the two distinct types of human-identifiable features (masking effect and generation effect) appears limited. Visualizing the perturbation for targeted attacks would be beneficial.\n\n-  Does the visual perturbation come from cases where the attack was successful? How does the perturbation behave in the case of an unsuccessful attack?\n\n- While the paper asserts findings across three different datasets, I could only locate a detailed attack accuracy comparison for ImageNet in Appendix E Table 1. It is not clear why the NOISE performance surpass that of IMAGE?\n\n[1] Rony J, Hafemann L G, Oliveira L S, et al. Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses. ICCV 2019\n\n[2] Wang X, He K. Enhancing the transferability of adversarial attacks through variance tuning. CVPR 2021."
                },
                "questions": {
                    "value": "See the questions in the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848838059,
            "cdate": 1698848838059,
            "tmdate": 1699636432349,
            "mdate": 1699636432349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1wrSLpCpJt",
                "forum": "G0EVNrBQh6",
                "replyto": "bLclegtPXn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3kT3 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing valuable feedback on our work. In the following, we will individually respond to each question from the reviewer.\n>1. In the first paragraph of Section 4, on what basis do you assert that (1) the noise in perturbations is independent and (2) two perturbations from different models display distinct human-identifiable features? I couldn't find any references or evidence supporting the claims.\n\n\n(1) and (2) are assumptions based on existing literature or empirical observations, which inspired us to uncover human-identifiable features in perturbations through the process of averaging.\n\nFor assumption (1), the existing literature suggests that the noise observed in gradients relative to the loss function might be meaningless local variations in partial derivatives [1]. Therefore, it is reasonable to assume that the local variations of gradients trained independently from different models should not be aligned or dependent on each other.\n\nRegarding assumption (2), our empirical observations show that two robust models can generate distinct human-identifiable features for the same image using the same attack algorithm. This leads to our hypothesis that perturbations from standard-trained models also contain distinct human-identifiable features.\n\n[1] Smoothgrad: Removing noise by adding noise. Daniel Smilkov, et al. Workshop on Visualization for Deep Learning. 2017.\n\u2003\n>2. I observed that detecting human-identifiable features necessitates 2,700 samples (270 models and 10 noise-infused seed samples). These may suggest that the averaged perturbation, generated by the three attacking methods, gravitates towards the object region. However, they don't confirm that in every model, the generated perturbations house human-identifiable features. Hence, a deeper experimental analysis regarding model selection and the integration of Gaussian noise would be beneficial, perhaps including more ablation studies (like MM, MM+G, SM+G).\n\nThank you for your comment. We would like to note that we have included the MM setting in our ablation study, as detailed in Appendix A1. The results consistently align with those from the MM+G setting and demonstrate human-identifiable features. \n\nBased on your suggestion, we have conducted additional experiments to explore the contribution of each model towards the human-identifiable feature. This involved varying the number of models used to average perturbations and comparing their mean square error to the perturbations from the MM setting. We found that, to achieve Mean Square Error (MSE) convergence within 0.05, an average of 25 models are needed for the three attack algorithms. For a MSE of 0.02, 90 models are required, while an MSE of 0.01 necessitates 157 models, for detailed information, please refer to Appendix I. \n\n\n>3. Why choose only 20 fixed classes out of 1,000? And a mere 200 samples seem insufficient to substantiate the claims made in the paper.\n\nWe have carefully selected 20 classes from the ImageNet validation set to maximize the diversity of our study, including classes on animals, plants, architecture, toys, transportation, utility, etc. We chose the first 10 images, instead of hand-picking, in each class to eliminate the possibility of human bias. Additionally, we tested the results across three different datasets and with up to five attack algorithms. We consistently observed the emergence of human-identifiable features from perturbations in each and every case.\n\nWhile we are eager to expand our experimental scope, the computational costs are prohibitively high. For instance, the DeepFool attack in our experiments alone required over 250 hours on a Tesla V100 GPU. Thus, further expanding the experiment's scale is challenging. However, we are confident that the consistent results obtained across various settings are sufficient to support our argument."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638474664,
                "cdate": 1700638474664,
                "tmdate": 1700638549550,
                "mdate": 1700638549550,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8YHm59Oa2G",
            "forum": "G0EVNrBQh6",
            "replyto": "G0EVNrBQh6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_G5Xu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_G5Xu"
            ],
            "content": {
                "summary": {
                    "value": "This paper conducted interesting analysis on the human-identifiable features concealed in adversarial perturbations crafted by different attack algorithms. In order to obtain the visual-recognizable patterns from gradient-driven adversarial perturbations, multi-samplings on different threat models was used based on the independence assumption. In experiment sections, thsi paper conducted such analysis on various threat models (274 in total) with various attack algorithms (gradient-based, search-based), which is efficient and solid. While the resulting denoised adversarial perturbations seem to have some clear pattern which can be recognized by human, the pure adversarial perturbation cannot reveal any information regarding the image itself. This paper also contains following discussion on the denoised adv perturbation by quantitatively analyzing its recognizability, checking its attack strength, and applying contour extraction. The overall analysis is plentiful and the results looks interesting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Evaluation on a large amount of threat models and attack algorithms make the whole experimental results to be reliable.\n- Motivation on exploring the human-identifiable features directly, instead of applying XAI methods to interpret, looks efficient and interesting.\n- Overall written is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "While I do appreciate such important and intense work on exploring the explainability in adversarial perturbations, I still have some major concerns about the whole paper. \n\n- Human-identifiable features looks vague: I still remain unclear about how to logically define the \"human-identifiable\" here: In section 5.2.1 authors conducted recognizability experiments on these denoised adv perturbations but it can only prove they are \"model-identifiable\". We cannot make such claim by showing part of (or even all) extracted adv perturbations and they are all human-identifiable. Some human-labeling experiments is required as a strong evidence to prove this.\n\n- The overall finding is not surprising: while it is good to see that denoised adversarial perturbation is similar to its corresponding raw image, I'm not surprising to see because gradient-based attacks perturb models' prediction by optimizing the objective function following the pixel-gradient direction --- larger pixel gradients indicate pixels here are important for threat model to identify this input image. Thus the outcome of gradient optimization, adversarial perturbation, should contain some important features to identify this image. And for search-based attacks, it still tend to follow the important pixels to craft their perturbation. I think this paper should focus more on the target-attack scenario - so we have our raw-image key features and our targeted label --- how would the adversarial perturbation be to reflect both concept? Currently it only has a very short paragraph discussing such scenario (Section 6)."
                },
                "questions": {
                    "value": "I put all my concerns to the weakness part and I do think this paper has a lot of space to improve. \n\nHowever, I think the overall results is plentiful and interesting for other researchers to know (especially on denoised perturbation under targeted attack scenario). It could be a very interesting workshop paper after reorganizing it into a logical way.\n\n\n======================================================\n\nUpdates after reading authors' rebuttal:\n\nI really appreciate authors efforts on further elaborating the importance of their findings - now I tend to believe this is an interesting finding to me and it could inspire several future papers for further theoretical analysis. However, after checking Reviewer ZsSi's comments, there could be some literatures implicitly discussing such scenario but this paper lacks contribution on further exploring the underlying reasons. I would like to raise my score to 5 but reduce my confidence to 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4550/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4550/Reviewer_G5Xu",
                        "ICLR.cc/2024/Conference/Submission4550/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699501161106,
            "cdate": 1699501161106,
            "tmdate": 1700712774921,
            "mdate": 1700712774921,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CZT7rn3W2Z",
                "forum": "G0EVNrBQh6",
                "replyto": "8YHm59Oa2G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank You for Your Comment"
                    },
                    "comment": {
                        "value": "First, we would like to thank the reviewer for their detailed review of the paper. Please find our response below.\n\n>1. Human-identifiable features looks vague: I still remain unclear about how to logically define the \"human-identifiable\" here: In section 5.2.1 authors conducted recognizability experiments on these denoised adv perturbations but it can only prove they are \"model-identifiable\". We cannot make such claim by showing part of (or even all) extracted adv perturbations and they are all human-identifiable. Some human-labeling experiments is required as a strong evidence to prove this.\n\nWe would like to note that we have conducted a human evaluation test on perturbations generated under the MM+G setting, as detailed in Section 5.2.1. An average accuracy of 80.7% is achieved by human participants, indicating that adversarial perturbations are highly classifiable/identifiable by humans.\n\n>2. The overall finding is not surprising: while it is good to see that denoised adversarial perturbation is similar to its corresponding raw image, I'm not surprising to see because gradient-based attacks perturb models' prediction by optimizing the objective function following the pixel-gradient direction --- larger pixel gradients indicate pixels here are important for threat model to identify this input image. Thus the outcome of gradient optimization, adversarial perturbation, should contain some important features to identify this image. And for search-based attacks, it still tend to follow the important pixels to craft their perturbation. I think this paper should focus more on the target-attack scenario - so we have our raw-image key features and our targeted label --- how would the adversarial perturbation be to reflect both concept? Currently it only has a very short paragraph discussing such scenario (Section 6).\n\nWe thank the reviewer for providing us with the chance to clarify our work. In the following discussion, we will introduce the prevailing perspective on adversarial perturbations, and then explain the contribution of our work.\n\nAlthough adversarial perturbations are expected to contain features beneficial to identifying images, there is no guarantee that these features will be human-understandable. In fact, the prevailing view in the field is that human and machine rely on different types of features for classification. Adversarial perturbations, as a direct consequence of human-AI misalignment, are a type of feature (non-robust feature) that is highly informative for models yet incomprehensible to humans [1]. There is even a stronger version of the assumption stating that human-identifiable (robust) features should only exist in robust models, as suggested by reviewer 4 (ZsSi).\n\nThe reviewer might think that, with the invention of explainable AI tools, it should appear straightforward that gradient/adversarial perturbations must include features identifiable by human. However, these tools often require preprocessing that largely alters the image, which could mask the actual features relied upon by the model. Consequently, the reported results may be ad-hoc and not accurately reflect reality [2].\n\nOur research reveals that human-identifiable features inherently reside in perturbations from a standardly trained model, offering fresh insights into explaining the properties of perturbations including transferability, the relation between adversarial training and increased model explainability, and the human-AI misalignment experiment, all of which are crucial in understanding the mechanisms behind adversarial perturbations.\n\nReferences:\n\n[1] Adversarial Examples Are Not Bugs, They Are Features. Andrew Ilyas, et al. NeurIPS, 2019.\n\n[2] Adversarial Examples and Human-ML Alignment. Alexender Madry. MIT CBMM Talks (https://www.youtube.com/watch?v=AvcRBuFreFg, 22:52-23:13)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638302210,
                "cdate": 1700638302210,
                "tmdate": 1700736380387,
                "mdate": 1700736380387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q8h2aPnp0g",
            "forum": "G0EVNrBQh6",
            "replyto": "G0EVNrBQh6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_QGNn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4550/Reviewer_QGNn"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to extract human-identifiable features from adversarial examples. Based on the fact that DNN models are trained on human-labeled datasets, the authors assume that adversarial perturbations should also contain human-identifiable features. \n\nThe authors first claify that two factors, excessive gradient noise and incomplete features, hinder feature extraction. Therefore, the authors propose to utilize noise augmentations and model ensembling to mitigate these negative effects. The authors find two interesting phenomenons: masking effect (untargeted attacks) and generation effect (targeted attacks)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This problem is interesting. I like this topic.\n\n2. The visualization results are also promising."
                },
                "weaknesses": {
                    "value": "1. Although this problem is interesting, the authors do not provide more surprising findings and insights compared with previous works.  \n  1.1 Adversarial perturbations contain meanful or human-identifiable features have been studied in these works [1,2]. They may correspond to \"robust\" features.  \n  1.2 The proposed methods, noise augmentations and model ensembling are widely used in transfer attacks. More transfeable perturbations contain more \"robust\" features (human-identifiable features) and share more non-robust features. The previous work have shown this point [1]  \n  1.3 Although the visualizations are very promising, we are uncertain about the extent of assistance this can provide.\n\n2. Some claims in the article are unclear:  \n  2.1 The two obtacles are not very clear. The first one (noisy gradient) is easy to understand. Lots of transfer attacks also propose to mitigate this negative effect to improve adversarial tranferability. However, there is insufficient evidence to support the second claim about incomplete learned features. Could you please provide more details about the second one?  \n  2.2 Meanwhile, the comparison between these two points is also unclear. Which factor has a greater negative impact on extracting human-identifiable features? As shown in experimental setting, the authors need to use lots of ensembling models. This has made is method less practical.  \n  2.3 The findings from Section 5.2.3 are interesting. The authors use the contour features to attack models. It also shows that contour features are important than background information. Could the authors please discuss connections and differences between this phenomenon and this work [3]?  \n\n3. Could the authors please provide more results about generation effect on targeted attacks?\n\n[1]. Adversarial Examples Are Not Bugs, They Are Features.  \n[2]. Image Synthesis with a Single (Robust) Classifier.  \n[3] ImageNet-trained CNNs are biased towards texture: increasing shape bias improves accuracy and robustness"
                },
                "questions": {
                    "value": "Please see Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699530649221,
            "cdate": 1699530649221,
            "tmdate": 1699636432138,
            "mdate": 1699636432138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LtR3CaHZWD",
                "forum": "G0EVNrBQh6",
                "replyto": "q8h2aPnp0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QGNn (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing valuable feedback on our work. In the following, we will individually respond to each question from the reviewer.\n\n>1. Although this problem is interesting, the authors do not provide more surprising findings and insights compared with previous works.\\\n1.1 Adversarial perturbations contain meanful or human-identifiable features have been studied in these works [1,2]. They may correspond to \"robust\" features.\\\n1.2 The proposed methods, noise augmentations and model ensembling are widely used in transfer attacks. More transfeable perturbations contain more \"robust\" features (human-identifiable features) and share more non-robust features. The previous work have shown this point [1].\\\n1.3 Although the visualizations are very promising, we are uncertain about the extent of assistance this can provide.\n>\n\nNote: References cited by the reviewer are cited as R#. For example, the first reference cited by the\nreviewer is indicated as R1.\n\nWe thank the reviewer for giving us the chance to clarify our contributions. Our contribution is not to re-discover robust (human-identifiable) features but to demonstrate that robust features inherently exist in adversarial perturbations generated from a standard-trained model. In fact, the prevailing view in this field is that human and machine rely on different types of features for classification. This view leads to the claim that adversarial perturbations from standard models, as a direct consequence of human-AI misalignment, are a type of feature (non-robust feature) that is highly informative for models yet incomprehensible to human [R1]. \n\n\nWith our experiment, we demonstrate that perturbations from standard models inherently contain robust features, instead of mere non-robust features. To our best knowledge, this finding is yet to be discussed in the literature and provides a new perspective on understanding the mechanism governing the attack of perturbations on models.\n\nReference [R2] illustrates the effectiveness of robust models in image generation and restoration tasks. A related study [1] by the same research group delves into the reasons behind this, which is closely related to our work. The study shows that gradients and perturbations generated by robust models align more closely with human perception, while perturbations from standard networks are significantly noisier and may not be human-identifiable. According to the authors, standard models learn different features than robust models. \n\nWe show that, by averaging perturbations from standard models, robust features are still observable, indicating a shared learning aspect between standard and robust models. Furthermore, we offer a new yet plausible explanation for the increased perceptibility in adversarial training: Adversarial training could act as gradient regularization, reducing noise and homogenizing gradients, thus making perturbations more perceptible to humans, as stated in Section 7.2. \n\nOur study uncovered human-identifiable (robust) features in perturbations from standard training, shedding new light on perturbations\u2019 properties, such as transferability, the link between adversarial training and enhanced model explainability, and the human-AI misalignment experiment. These aspects are key to understanding the mechanism of adversarial perturbations, allowing us to pursue adversarial-perturbations related applications. \n\nReference:\n\n[1] Robustness May Be at Odds with Accuracy. Dimitris Tsipras, et al. ICLR 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636060131,
                "cdate": 1700636060131,
                "tmdate": 1700736111907,
                "mdate": 1700736111907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eGpoHTCc5K",
                "forum": "G0EVNrBQh6",
                "replyto": "q8h2aPnp0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4550/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QGNn (Part \uff12)"
                    },
                    "comment": {
                        "value": ">2. Some claims in the article are unclear:\\\n2.1 The two obtacles are not very clear. The first one (noisy gradient) is easy to understand. Lots of transfer attacks also propose to mitigate this negative effect to improve adversarial tranferability. However, there is insufficient evidence to support the second claim about incomplete learned features. Could you please provide more details about the second one? \\\n2.2 Meanwhile, the comparison between these two points is also unclear. Which factor has a greater negative impact on extracting human-identifiable features? As shown in experimental setting, the authors need to use lots of ensembling models. This has made is method less practical.\\\n2.3 The findings from Section 5.2.3 are interesting. The authors use the contour features to attack models. It also shows that contour features are important than background information. Could the authors please discuss connections and differences between this phenomenon and this work [3]?\n>\n\n**2.1** The obstacles hindering the presence of robust features are assumptions made in the paper based on existing literature or empirical observations. These obstacles motivate us to propose the method of averaging perturbations as a method to uncover these robust features.\n\nRegarding assumption (2), our empirical observations indicate that gradient/perturbations from robust models only contain a subset of human-identifiable features. This leads to the hypothesis that robust features from standard models should also contain incomplete human-identifiable features.\n\nWe believe the reason behind is due to the capacity limit and challenge in learning optimum, neural networks may not completely utilize all robust feature for classification. Perturbations, designed to maximize the model's loss function, impact only those features the model recognizes. Consequently, these perturbations might encompass only a subset of features identifiable by humans.\n\n\n**2.2** Our research aims to investigate robust features in perturbations from standard models. Assessing which factors, noise or incompleteness, more significantly hinder robust features is not our primary focus. We recognize the value of such evaluations.  However, discerning the dominant factor is challenging due to the concurrent presence of noise and the incompleteness of robust features in perturbations. Separating one factor without altering the perturbations' nature may be challenging. This issue deserves our attention for further investigation.\n\n**2.3** In [R3], the authors demonstrates that Convolutional Neural Networks (CNNs) is used to classify images using both texture and contour, with texture playing a more significant role than in human classification, which relies more on contour.\n\nWe segment the perturbations into those within the contour of the object and those in the background. We find that the contour parts contain both the object\u2019s edges and textures that are similar to the original image. Figure 2 illustrates those perturbations from MM+G display texture-like features. For instance, one can observe a green, granular texture on the frog's back and a stripe-like texture on the baseball. These observations align with the findings in [R3] because, to deceive the model, perturbations may need to reduce the pixel values of both texture and contour features, which serve as critical factors influencing the model's decision.\n\n>3. Could the authors please provide more results about generation effect on targeted attacks?\n\n**3** Additional results about targeted attack have been provided in Appendix H. Thank you for your suggestion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4550/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636879230,
                "cdate": 1700636879230,
                "tmdate": 1700637177993,
                "mdate": 1700637177993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]