[
    {
        "title": "LASER: Linear Compression in Wireless Distributed Optimization"
    },
    {
        "review": {
            "id": "zavFpxIRbq",
            "forum": "TCJbcjS0c2",
            "replyto": "TCJbcjS0c2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_dVrm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_dVrm"
            ],
            "content": {
                "summary": {
                    "value": "This paper  introduces the LASER scheme, a novel communication-efficient distributed optimization approach utilizing plain SGD. In contrast to most existing literature, LASER incorporates considerations for communication noise. The compression method implemented within LASER involves both low-rank representation and gradient scaling, with a detailed algorithmic description provided. Theoretical analyses are included to demonstrate the convergence assurance of the proposed scheme, while experimental evaluations validate the complexity improvements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The algorithm design considers the noise in communication, which is more realistic than most works in literature. The design enables the level of noise to decrease when the norm gradient is smaller. This is important to guarantee good performance of SGD.\n2. The convergence analysis of the proposed scheme is comprehensive and the result is reasonable. The convergence rate in quansi-convex and non-convex setting is comparable to literature and standard distributed optimization methods.\n2. The experiments setup is closely related to the theoretical analysis and the result is convincing."
                },
                "weaknesses": {
                    "value": "1. The major concern is the novelty of this work. The low-rank approximation of gradient or weight matrix is a common method in literature. And this method has been applied in distributed learning, especially Federated learning setting, e.g, Zhou, Huachi, et al. \"Low rank communication for federated learning.\" , 2020,Kone\u010dn\u00fd, Jakub, et al. \"Federated learning: Strategies for improving communication efficiency.\" arXiv preprint arXiv:1610.05492 (2016). Thus, I do not think the proposed framework if of great contribution.\n2. The power allocation step is also not novel. The method introduced in equation (2) is a standard noise variance minimization method in 'Guo, Huayan, An Liu, and Vincent KN Lau. \"Analog gradient aggregation for federated learning over wireless networks: Customized design and convergence analysis.\" IEEE Internet of Things Journal 8.1 (2020): 197-210.' \n3. The theoretical analysis is based on standard distributed optimization with error feedback. The only difference is the low-rank compression and noise are considered."
                },
                "questions": {
                    "value": "1. Looks like LASER is a combination of well-known methods(low-rank approximation and noise variance minimization), so is there any major difference or difficulty in theoretical analysis besides the ones are already sound in literature?\n2. Could you provide intuitive understanding about why low-rank approximation can induce lower noise variance? Specifically, explain the intuition on the comparison of noise variance in equation (5)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Reviewer_dVrm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7274/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796708644,
            "cdate": 1698796708644,
            "tmdate": 1700679794580,
            "mdate": 1700679794580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jvPhaT38YQ",
                "forum": "TCJbcjS0c2",
                "replyto": "zavFpxIRbq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Authors",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty and theoretical contributions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and insightful comments. We refer to the common response about the  theoretical and  algorithmic novelty of LASER. \n\n- **Theoretical analysis:** As highlighted in the common response, a major difficulty in the theoretical analysis of LASER is handling the communication noise and the power budget. Since the channel noise interacts in a non-linear way with the gradients (Eq. (noisy channel)), we tackle this through introducing **channel-influence** factor which gives a handle to control the second-moments of the noisy gradients, which are crucial for convergence analysis. On the power front, Lemmas 7 and 8 in Appendix B.1 establish the optimal power allocation scheme among the rank components in order to obtain a tight characterization of the channel influence for various compression schemes. This further facilitates a thorough comparison with baselines such as Z-SGD and showcases the benefits of compression algorithms through convergence rates.\n\n- **Intuition behind Eq. (5):** As demonstrated in Section 3 of the paper, the underlying intuition here is that under a fixed power budget, if we transmit the full gradient matrix of size $m \\times m$, this budget has to be utilized for $m^2$ entries. On the other hand, if we capitalize on its low-rank structure we effectively need to transmit $O(m)$ entries, thus we can allocate more power-per-entry hence resulting in a roughly $O(m)$ boost in the signal-to-noise ratio (SNR). This intuition mathematically translates into the reduction of the gradient variance, as captured by the channel-influence factor in Eq. (4). In view of this, Eq. (5) compares these noise variances for Z-SGD and LASER and confirms the above intuition."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221322828,
                "cdate": 1700221322828,
                "tmdate": 1700221568589,
                "mdate": 1700221568589,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pqq5b99FMt",
                "forum": "TCJbcjS0c2",
                "replyto": "zavFpxIRbq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and encouraging comments and reconsideration of the score!"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729933386,
                "cdate": 1700729933386,
                "tmdate": 1700729933386,
                "mdate": 1700729933386,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7pac1wdUWg",
            "forum": "TCJbcjS0c2",
            "replyto": "TCJbcjS0c2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_DPBP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_DPBP"
            ],
            "content": {
                "summary": {
                    "value": "The authors address the communication bottleneck issue in data-parallel SGD, which is widely used for distributed optimization in large-scale machine learning. The authors highlight that existing compression schemes either assume noiseless communication links or fail to perform well in practical tasks. To bridge this gap, the authors propose LASER, a gradient compression scheme that transmits gradients over noisy channels by leveraging the inherent low-rank structure."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors demonstrate that LASER consistently outperforms baseline methods across various benchmarks, outperforming state-of-the-art compression schemes in computer vision and GPT language modeling tasks."
                },
                "weaknesses": {
                    "value": "The novelty of the paper is not well stated. It is unclear whether low-rank matrix decomposition techniques already exist and if the contribution of the paper lies solely in the utilization of low-rank decomposition to reduce gradient transmission traffic.\n\nThe practicality of the compression method is questionable. Adopting low-rank decomposition on gradients requires massive computations. It is unclear whether this operation introduces additional overhead to the distributed training procedure."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7274/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699250114548,
            "cdate": 1699250114548,
            "tmdate": 1699636868603,
            "mdate": 1699636868603,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fTQmlCXOeE",
                "forum": "TCJbcjS0c2",
                "replyto": "7pac1wdUWg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty of LASER and efficiency of its rank decomposition"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and insightful comments. We refer to the common response about the theoretical and algorithmic novelty of LASER, which also illustrates that it's extremely efficient and practical."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221530089,
                "cdate": 1700221530089,
                "tmdate": 1700221530089,
                "mdate": 1700221530089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a4xNGSysTP",
            "forum": "TCJbcjS0c2",
            "replyto": "TCJbcjS0c2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_t6dq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_t6dq"
            ],
            "content": {
                "summary": {
                    "value": "Considering the randomness in the communication environment, to reduce the power needs of sending information from the local client, the author proposed a new distributed algorithm called LASER. Different from the previous algorithm, the proposed algorithm uses low-rank compression to reduce the vector dimension that will be sent to the server. Combined with error feedback, the algorithm's convergence is established in quasi-convex, convex, and nonconvex cases. The experiments show the proposed algorithm can achieve a similar performance with lower power."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Consider the power that is needed to transmit vectors from the client to the server.\n\n2. To use the power efficiently, the authors introduce a low-rank compressor to make the SNR larger than transmitting the full matrix.\n\n3. To eliminate the error from the compressors, the authors introduce the error feedback into the proposed algorithm.\n\n4. The authors show the convergence of the proposed algorithm under quasi-convex, convex, and non-convex cases.\n\n5  The experimental results show that the proposed algorithm can achieve a similar performance but requires less power."
                },
                "weaknesses": {
                    "value": "1. In the distributed setting, how can each client know $max ||g||$?\n\n2. The low-rank compressor seems to be time-consuming. Why not use top-k or other compressors?\n\n3. It seems to be unfair to compare the energy budget in each iteration, because adding a compressor even with the error feedback, the algorithm needs more iterations to converge. Thus, it would be fair to compare total energy costs. Otherwise, the most energy-saving algorithm in this framework will be the top-1 compressor (sending only one value each time)."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7274/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699358704852,
            "cdate": 1699358704852,
            "tmdate": 1699636868504,
            "mdate": 1699636868504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xjdFjDfVDW",
                "forum": "TCJbcjS0c2",
                "replyto": "a4xNGSysTP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Processing time for PowerSGD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the encouraging feedback and insightful comments. \n\n- **Power scalars:**  As highlighted in [1, 2], in each communication round the nodes first transmit the gradient norms $\\|\\| \\boldsymbol{g}_i \\|\\|$, which are scalars, to the server which then computes their maximum and transmits it back. Since this procedure only involves transmission of real scalars, it has negligible communication overhead as compared to the gradients being transmitted. \n\n- **Processing time for PowerSGD and baselines:** As highlighted in Section 4.3 of the paper, PowerSGD [3], the low-rank compressor behind LASER, takes significantly smaller processing time as compared to other compressors such as Top-K or Random-K. For example, Table 4 in [3] highlights that PowerSGD achieves a significant 42 % reduction in processing time/batch as compared to Top-K, whereas it\u2019s 55 % compared to Random-K for CIFAR-10. Similar computational gains of PowerSGD over the baselines are demonstrated further in [3] over various challenging datasets.\n\n- **Energy budget:** Indeed, our current comparison already considers the total energy costs in an implicit manner. Since we fix the power budget per each iteration across various baselines and compare the final accuracy after a specific number of epochs, this is equivalent to the former scenario. We observe similar gains for LASER even when the final number of epochs is increased further. \n\n**References:**\n\n\n- [1] Huayan Guo, An Liu, and Vincent KN Lau. Analog gradient aggregation for federated learning over wireless networks: Customized design and convergence analysis. *IEEE Internet of Things Journal*, 8(1):197\u2013210, 2020.\n\n- [2] Wei-Ting Chang and Ravi Tandon. Mac aware quantization for distributed gradient descent. *In GLOBECOM 2020-2020 IEEE Global Communications Conference*, pages 1\u20136. IEEE, 2020.\n\n- [3]  Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient compression for distributed optimization. *Advances in Neural Information Processing Systems*, 32, 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700256417469,
                "cdate": 1700256417469,
                "tmdate": 1700256417469,
                "mdate": 1700256417469,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lxIaWYldJs",
                "forum": "TCJbcjS0c2",
                "replyto": "xjdFjDfVDW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_t6dq"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_t6dq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response, my concerns are well addressed."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641831948,
                "cdate": 1700641831948,
                "tmdate": 1700641831948,
                "mdate": 1700641831948,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bB7GBD8QMn",
            "forum": "TCJbcjS0c2",
            "replyto": "TCJbcjS0c2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_ykHv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_ykHv"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a low-rank compression method for distributed optimization. The emphasis is on wireless communication systems where the averaging is done \"over the air\" in a noisy channel. The performance is evaluated in experiments with both language modeling and image classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The incorporation of a power budget and power allocation in the compression algorithm is somewhat new and interesting. \n- The consideration of a GPT model in the experiments is good."
                },
                "weaknesses": {
                    "value": "- The considered wireless communication system with \"over the air\" averaging is essentially doing the averaging in the analog domain. This is impractical as all modern wireless systems are digital and data transmission involves channel coding, which is likely incompatible with \"over the air\" averaging. I am aware that this concept has been presented in many papers published in wireless communications journals and conferences, but it is still impractical. It is unlikely that wireless communication standards will incorporate a specific physical layer technique only for the sake of computing averages, which in any foreseeable future represents only a very small fraction of data transmitted in common wireless systems. Moreover, this mechanism restricts the averaging to clients communicating with a single basestation, while in practice, the coverage of a single basestation is quite small and having all the clients connecting to the same basestation is very unlikely.\n- The paper focuses on low-rank compression, which by itself is a known technique. Yet, the result in Theorem 1 and its assumptions seem to be a standard error-feedback result. In particular, Assumption 4 is standard and does not capture anything specific to the fact of using low-rank compression instead of other compression methods such as top-k and random-k. It seems the only difference from standard error-feedback compression analysis is the noise term, which, however, is a straightforward extension. \n- It is not quite clear how power reduction is achieved as shown in the experiments. The power budget is fixed according to Algorithm 1. If there is a fixed power budget, the same budget should apply to both the proposed algorithm and the baselines. The results in Table 1 and Table 3 either do not enforce a fixed power budget, which contradicts with the description in Algorithm 1, or there is something else wrong."
                },
                "questions": {
                    "value": "Please refer to the weaknesses. \n\nIn addition, the ICLR template suggests that the appendix should be included at the end of the same PDF as the main paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Reviewer_ykHv"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7274/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699550512851,
            "cdate": 1699550512851,
            "tmdate": 1700582096426,
            "mdate": 1700582096426,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uoSZMZnCVq",
                "forum": "TCJbcjS0c2",
                "replyto": "tIJ7MZjeP2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_ykHv"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Authors",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_ykHv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. Regarding novelty in the analysis, it would be helpful if you could point to specific steps (e.g., equation numbers) where the analysis is different from standard error-feedback analysis.\n\nMy following comments remain unaddressed:\n- The considered wireless communication system with \"over the air\" averaging is essentially doing the averaging in the analog domain. This is impractical as all modern wireless systems are digital and data transmission involves channel coding, which is likely incompatible with \"over the air\" averaging. I am aware that this concept has been presented in many papers published in wireless communications journals and conferences, but it is still impractical. It is unlikely that wireless communication standards will incorporate a specific physical layer technique only for the sake of computing averages, which in any foreseeable future represents only a very small fraction of data transmitted in common wireless systems. Moreover, this mechanism restricts the averaging to clients communicating with a single basestation, while in practice, the coverage of a single basestation is quite small and having all the clients connecting to the same basestation is very unlikely.\n- It is not quite clear how power reduction is achieved as shown in the experiments. The power budget is fixed according to Algorithm 1. If there is a fixed power budget, the same budget should apply to both the proposed algorithm and the baselines. The results in Table 1 and Table 3 either do not enforce a fixed power budget, which contradicts with the description in Algorithm 1, or there is something else wrong."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282555669,
                "cdate": 1700282555669,
                "tmdate": 1700282555669,
                "mdate": 1700282555669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DfoeOAaLBX",
                "forum": "TCJbcjS0c2",
                "replyto": "bB7GBD8QMn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Compatibility with Wireless Communication Standards"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the engaging and insightful comments. \n\nIt is true that in today\u2019s communication systems, the encoders need to follow standards. Related to that, we (1) explain how we could make LASER compatible with the current modulation scheme, (2) discuss potential (minimal) changes in the wireless systems required to implement LASER, and (3) make final remarks on our thoughts on academic research. \n\n**1) Digital vs. Analog transmissions:** LASER transmission as a high-order QAM. \nThere are ways to adapt LASER to a digital transmission framework if needed. Perhaps one straightforward way would be to quantize the transmitted values of LASER to make it compatible with the modulation schemes used in today\u2019s communication systems (e.g., QAM). The typical range of QAM levels varies from system to system. For WiFi, a very high modulation order (e.g., 4096-QAM) is one of the key enhancements of WiFi 7 (IEEE 802.11be). It is shown in the literature that the quantization of real-valued codewords to 6 bits usually does not affect the reliability much. The quantification of LASER is an interesting future research topic. \n\n**2) LASER and Comm. Standards.** \n\nSmall modification: We would like first to note that using LASER (+ quantization) in today\u2019s wireless systems will only require \u2018bypassing\u2019 channel coding/modulation and directly generating modulation symbols, which is not a complicated modification. \n\nLASER as a joint-source channel coding algorithm: regarding the reviewer\u2019s comment that \u201cWireless communication standards will (not) incorporate a specific physical layer technique only for the sake of computing averages\u201d, on the one hand, we\u2019ll note that given that we do not need major changes to the wireless system as noted above, we do not think LASER is completely ruled out solely for the compatibility reason. On the other hand, we would like to note that LASER belongs to a broad family of joint source-channel coding algorithms from the perspective of which part of the standards needs to change. Communication systems, to date, relied on source-channel separation for several reasons; it\u2019s simple; it\u2019s known to be optimal if we have a lot of (asymptotically many) IID sources; and there are not many good joint source-channel coding algorithms. With the advance of ML, there are several reasons to explore neural joint source-channel codecs, some of which includes that not all part of the data source is equally important; we finally have the capability to design analytically intractable joint source-channel coding algorithms in a data-driven manner, etc. (cf. There are several studies demonstrating the efficacy of data-driven short-blocklength neural codecs and semantic communications such as NECST [1]). Joint source-channel coding, also called semantic communications, for wireless is definitely of interest to industry and academia; the communities as a whole are at the stage of exploring what is feasible and how much gain we can achieve from it. \n\nRecent trends in Wireless: Given the extensive interest from industry and academia in neural-based codecs, joint source-channel coding, ML-driven wireless, wireless for ML applications, end-to-end ML-driven communication blocks, and virtualized RAN, we do not think LASER (and more generally directly mapping data to modulation symbols) is completely ruled out in future. An interesting discussion would be on which communication modes (cellular, wifi, or private networks) can benefit most by deploying LASER.\n\n\nFinal remark: As a final remark, we believe, as academics, we should not bind our research only to what is available with today\u2019s technology. We believe it's an important and exciting area of research to evaluate the capability of aggregation-leveraging communication algorithms.\n\n**References:**\n- [1] NECST: https://arxiv.org/abs/1811.07557"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302378050,
                "cdate": 1700302378050,
                "tmdate": 1700302378050,
                "mdate": 1700302378050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zzMXYBf6Rj",
                "forum": "TCJbcjS0c2",
                "replyto": "bB7GBD8QMn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Base station"
                    },
                    "comment": {
                        "value": "**1)** Please note that the central node does not have to be a base station; the central node could be the edge device, such as a router in a concert hall.  As a concrete example, let's consider the following scenario.  \n\nLocation: a concert hall with a few hundred to several thousands of audience (or more) \n\nClients: cell phone\n\nCentral node: edge device \n\nML models to be learned: music-related neural network in an app (e.g., Spotify) \n\nHere, several clients - whose data will be most relevant to training a music-related ML model- are physically co\u2013located and thus naturally share the wireless medium. (Cf. The clients here would be anyone willing to share their gradients and need not be chosen beforehand.)  Thus in this example, the central node could be the edge device, such as a router in the concert hall. \n\n**2)** Applying our algorithm to \u201ccellular\u201d communication scenarios (i.e., Base Stations = central node, User Equipment (UE) = clients ) will potentially require more coordination and adjustments. First of all, we agree that not all cells would be suitable to perform LASER-based federated learning. Instead, only some base stations (with sufficiently large clients participating in the federated learning) will allocate some (wireless) resource blocks to the LASER operation. There are several other aspects to coordinate such as resource allocation (data traffic vs. federated learning traffic). These are all interesting potential research problems although they are not within the scope of our work. We would like to briefly mention that supporting ML applications from wireless is of increasing interest."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303560991,
                "cdate": 1700303560991,
                "tmdate": 1700303560991,
                "mdate": 1700303560991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YjpBUf9HDE",
                "forum": "TCJbcjS0c2",
                "replyto": "iguKDH8Sel",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_ykHv"
                ],
                "readers": [
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Authors",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_ykHv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the answers. I'm happy to raise my score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582053370,
                "cdate": 1700582053370,
                "tmdate": 1700582053370,
                "mdate": 1700582053370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ry1yDDaZVz",
            "forum": "TCJbcjS0c2",
            "replyto": "TCJbcjS0c2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_yf4L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_yf4L"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a novel technique for distributed optimization: LASER, which is mostly composed of a low-rank compression step, and an over-the-air aggregation step. The authors present the theoretical advantage of such method over vanilla over-the-air SGD, and the convergence of the method is ensured. Finally, extensive experiments show the advantage of the proposed method, including on large scale tasks such as GPT language modeling tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "## Originality: \n\nThe work is original, as it is the first one to study over-the-air gradient aggregation using a low-rank compression of the gradients. The results proven regarding the channel influence factor are novel and seem not trivial to show. The experiments show an improved performance over state of the art algorithms.\n\n## Quality: \n\nThe quality is good, with Theorems and their assumption clearly stated. Detailed proofs are provided for the results exposed. Additionally, the code is provided in the Supplementary material.\n\n## Clarity: \n\nI believe the contributions are clear, and well organized.\n\n## Significance: \n\nI believe the work is significant, in particular given the extensive experimental comparison with state of the art algorithms, including a very encouraging experimental results on large scale tasks such as GPT language modeling, which have become prominent in machine learning. Additionally, the results proven regarding the channel influence factor seem non-trivial and therefore should be very useful for the research community to build upon."
                },
                "weaknesses": {
                    "value": "I just have a few questions, see \u201cQuestions\u201d below."
                },
                "questions": {
                    "value": "I just have a few questions below:\n\n1. In algorithm 1, the local error $e_i$ is only $\\boldsymbol{M}_i - \\text{DECOMPRESS}(\\mathcal{C}_r(\\boldsymbol{M}_i))$, without ever being multiplied by $\\gamma$: is this correct ? It seems that if so, the error may not be compensated correctly ? (I may be mistaken)\n2. If I understand correctly, Assumption 5 is a new assumption introduced in the paper: unless I missed it in the paper, I think it would be good to elaborate a bit further on why such assumption should be verified in the settings considered, theoretically and/or experimentally (or even just with a discussion, just to provide some intuition on why such assumption should be verified): it seems that to verify it, either the compression should be very accurate (i.e. $\\delta_r$ large), or $\\lambda_{\\text{LASER}}$ should be small, but however, while taking a larger $r$ would make $\\delta_r$ larger (more accurate compression), it would also make the bound on $\\lambda_{\\text{LASER}}$ larger, according to eq. (5), therefore it seems a bit unclear whether Assumption 5 can be verified (or how to make it verified in practice).\n3. For the experiments, I believe it would be useful to just recall how one can deal with parameters of DNNs, which are not, per say, 1 matrix: looking into Vogels 2019, it seems that, to do that, the low-rank decompositions are done layer-wise (since each layer can be seen as a matrix, even in the convolutional case), but just a quick recall about this would be useful.\n4. Minor remarks/typos:\n- in 5. \u201cdecompose gradient matrix\u201d -> \u201cdecompose the gradient matrix\u201d\n- In the supplemental, just before D.2: \u201cexperssion\u201d \u2014> \u201cepxression\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Reviewer_yf4L"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7274/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699613408769,
            "cdate": 1699613408769,
            "tmdate": 1699636868282,
            "mdate": 1699636868282,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jYbKDxlRJK",
                "forum": "TCJbcjS0c2",
                "replyto": "Ry1yDDaZVz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Intuition behind Assumption 5"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the encouraging feedback and valuable suggestions. We will fix the typos in our revised version. We address the individual questions below.\n\n**Local error:** Following the standard convention in [1], in Line 7 of Algorithm 1 we add the local error $ \\boldsymbol{e}_i $ to the scaled gradient $\\gamma  \\boldsymbol{g}_i $  to get the new gradient $  \\boldsymbol{M}_i $. In Line 13, after its low-rank reconstruction through $ \\boldsymbol{g}$, a normal SGD step is performed on $ \\boldsymbol{\\theta}$. We can see that when there is no compression error, $ \\boldsymbol{e}_i =0 $, this falls back to the classical SGD. As suggested, one could in principle multiply the learning rate $\\gamma$ to the local-error but this requires a slight change in the definition of local error and the SGD descent step, so that we could recover the classical SGD in the special-case as above.\n\n**Assumption 5:** Thanks for your insightful comment. We indeed highlight the interplay between $\\lambda_{\\mathrm{LASER}}$ and $\\delta_r$ in Appendix B.2. Since the channel corruption follows the rank-compression, Assumption 5 roughly ensures that the overall compression is still benign. Mathematically, as derived in B.2, even in the hypothetical scenario where the compressor has access to the channel output, we see that a condition of the form $\\lambda_{\\mathrm{LASER}} \\leq \\delta_r$ is needed to ensure that the overall compression factor is still less than $1$. In view of this, Assumption 5 can be roughly treated as $\\lambda_{\\mathrm{LASER}} \\leq O(\\delta_r)$ in the general scenario. \n\nOn the other hand, while the above assumption is needed only for theoretical analysis, empirically we observe an interesting rank-accuracy tradeoff as demonstrated in Appendix F.5. We observe that either with low-rank or high-rank decomposition, the final accuracy is worse than that of the medium-rank. We believe this phenomenon is perhaps an empirical manifestation of the interplay between  $\\lambda_{\\mathrm{LASER}}$ and $ \\delta_r$. Given the difficulty in characterizing $\\delta_r$ precisely, establishing a clear mathematical justification for the aforementioned phenomena is an interesting topic of future research.\n\n**Layer-wise decomposition:** We fully agree with your comment. Indeed, we already specify this feature just below Algorithm 1 on page 4 in the paper, but we will further make it clear.\n\n**References:**\n\n- [1] Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350, 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259500826,
                "cdate": 1700259500826,
                "tmdate": 1700259500826,
                "mdate": 1700259500826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TXaLabBdPM",
                "forum": "TCJbcjS0c2",
                "replyto": "Ry1yDDaZVz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_yf4L"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_yf4L"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Dear Authors, thanks a lot for your answer, \n\n- **Local error:** That makes sense, thanks for your answer (also, this has become clearer for me upon inspection of equation (20) in [1]).\n- **Assumption 5:** Thank you, that makes it clearer for me. Though, regarding the bound on $\\delta\\_r$, in general, wouldn't $\\delta\\_r$ be equal to $\\frac{r}{d}$ ? (since, assuming the worst case scenario where $\\boldsymbol{M}$ has all singular values equal to its largest ($\\sigma\\_{max}$), we have for the Frobenius norm $\\\\| \\mathcal{C}\\_r (\\boldsymbol{M}) -\\boldsymbol{M} \\\\|\\_{F}^2 = (d-r)\\sigma\\_{max}^2 = (1 - \\frac{r}{d}) d \\sigma\\_{max}^2 =  (1 - \\frac{r}{d}) \\\\| \\boldsymbol{M} \\\\|\\_F^2 $ (I may be mistaken). With such bound, this means, according to equation (5), that it would be necessary to enforce an SNR large enough to ensure validity of Assumption 5 (e.g. it is not verified if SNR=1) ? However, I guess this bound on $\\delta\\_r$ is loose in practice since DNNs for instance, are low-rank compressible so $\\delta\\_r$ will be smaller. But I guess it could be interesting to deepen this aspect: for instance if there are some common assumptions on the \"low-rank compressiveness\" of DNNs in the literature which would give some tight enough bound on $\\delta\\_r$, those would in turn result in the necessary value for the SNR; another option could be to just verify experimentally, by computing the maximum $\\delta\\_r$ over all $\\boldsymbol{M}$ encountered during training, and checking whether it verifies Assumption 5.  In any case, such analysis does not need to be done (as the remaining time is limited), and so this will not impact my evaluation of the paper, I am just checking whether my understanding of the technique is correct.\n- **Layer-wise decomposition:** Thank you, I had missed the reference on page 4.\n\nI have read the reviews and responses, and I keep my positive impression on the paper and will preserve my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640091504,
                "cdate": 1700640091504,
                "tmdate": 1700640217803,
                "mdate": 1700640217803,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G4us4OySnP",
                "forum": "TCJbcjS0c2",
                "replyto": "sIj1SKqrS6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_yf4L"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewer_yf4L"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks a lot for your answer, that makes sense. I keep my positive impression on the paper and will preserve my score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735170363,
                "cdate": 1700735170363,
                "tmdate": 1700735170363,
                "mdate": 1700735170363,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mIcPpzxEDK",
            "forum": "TCJbcjS0c2",
            "replyto": "TCJbcjS0c2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_71LW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7274/Reviewer_71LW"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a scheme for efficient and reliable uplink communication over a noisy channel in a federated environment. In particular, clients submit rank factors of the gradients, where owing to the inherent low-rank structure of gradients, reconstruction can be performed reliably server-side, leveraging error-feedback if necessary. The authors demonstrate the effectiveness of this communication strategy, dubbed LASER, by performing experiments such as an image-classification, a GPT language-modeling task, and a 1-layer NN MNIST task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The narrative and motivation are very well-written. Indeed, uplink communication is often assumed to be noiseless in many FL setups. \n2. To the best of my knowledge, the theory is sound, and all assumptions are fair, with well-cited support regarding the low-rank nature of gradients. \n3. The large-scale GPT-2 experiment was quite impressive. Challenging large-scale language tasks are seldom seen in FL literature."
                },
                "weaknesses": {
                    "value": "1. In my opinion, this work does not introduce anything truly novel or interesting to the subdomain of communication efficiency. Gradient compression via sketching (Rabbani et al., 2023, Rothchild et al., 2020; Haddadpour et al., 2020), quantization (Zakerinia et al., 2023), and even the blanket consideration of general contraction/compression operators (Dorfman et al., 2023) along with many other works indicate that the research area of uplink efficiency is saturated. Introducing a low-rank decomposition of the gradient as a form of compression is not a particularly novel technique, especially since it is handled under the general error-feedback recovery. Bi-directional compression is a much more challenging and relevant problem in the modern landscape of FL communication efficiency. \n\n2. The following lines concern me: \"Rank compression methods (Yu et al., 2018; Cho et al., 2019; Wang et al., 2018) spectrally decompose gradient matrix (often via SVD) and transmit these factors. Since SVD is computationally prohibitive, we rely on the state-of-the-art\nlight-weight compressor PowerSGD (Vogels et al., 2019).\" Since gradient rank decomposition has been performed in distributed settings, using a SoTA method for decomposition is simply a plug-and-play extension which further restricts the novelty of LASER."
                },
                "questions": {
                    "value": "1. In what scenario would the uplink communication be noisy but the downlink communication would be noiseless? \"For the downlink\ncommunication from the server to the clients (broadcast channel), we assume that it is noiseless and thus the clients exactly receive what the server transmits...\"\n\n2.  (Chang & Tandon, 2020; Guo et al., 2020) imply that clients must first transmit information regarding the norms of their gradients before constructing a power budget policy -- does this imply 2 rounds of communication would be necessary assuming a dynamic power schedule is employed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7274/Reviewer_71LW",
                        "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7274/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699780239358,
            "cdate": 1699780239358,
            "tmdate": 1699780285303,
            "mdate": 1699780285303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0rnUGAw0dA",
                "forum": "TCJbcjS0c2",
                "replyto": "mIcPpzxEDK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7274/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty and communication links"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and insightful comments. We refer to the common response about the novelty of LASER.\n\n- **Uplink and downlink noise:** In the uplink channel the resource-constrained workers communicate with the server whereas in the downlink communication the server to the clients. The downlink is a broadcast channel which is typically assumed in the literature to be noiseless [1-3]. The underlying intuition here is that the central server (e.g. base station) is not as resource/power-constrained as the individual clients (e.g. mobile phones) and hence it can ensure its transmission error-free relatively speaking. While few works [8, 9] study the impact of downlink noise on the convergence of Z-SGD, it\u2019s an interesting future direction to extend these results for LASER-like compression algorithms.\n\n- **Transmission of power scalars:** Yes, indeed. As highlighted in [4, 5], the nodes transmit only the gradient norms $\\|\\| \\boldsymbol{g}_i \\|\\|$, which are scalars, to the server which then computes their maximum and transmits it back. Since this procedure only involves transmission of real scalars, it has negligible communication overhead as compared to the gradients being transmitted. \n\n**References:**\n\n-  [1] H Brendan McMahan and Daniel Ramage. Federated learning: Collaborative machine learning without centralized training data. *https://ai.googleblog.com/2017/04/federated-learning-collaborative.html*, 2017.\n\n  - [2] Jakub Kone\u02c7cn `y, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. *arXiv preprint arXiv:1610.05492*, 2016.\n\n   - [3] Jakub Kone\u02c7cn `y, H Brendan McMahan, Daniel Ramage, and Peter Richt\u00e1rik. Federated optimization: Distributed machine learning for on-device intelligence. *arXiv preprint arXiv:1610.02527*, 2016.\n\n- [4] Huayan Guo, An Liu, and Vincent KN Lau. Analog gradient aggregation for federated learning over wireless networks: Customized design and convergence analysis. *IEEE Internet of Things Journal*, 8(1):197\u2013210, 2020.\n\n- [5] Wei-Ting Chang and Ravi Tandon. Mac aware quantization for distributed gradient descent. *In GLOBECOM 2020-2020 IEEE Global Communications Conference*, pages 1\u20136. IEEE, 2020."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219190979,
                "cdate": 1700219190979,
                "tmdate": 1700219190979,
                "mdate": 1700219190979,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]