[
    {
        "title": "Learning to Jointly Understand Visual and Tactile Signals"
    },
    {
        "review": {
            "id": "HFWqNnJFH1",
            "forum": "NtQqIcSbqv",
            "replyto": "NtQqIcSbqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2955/Reviewer_RFKG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2955/Reviewer_RFKG"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of learning representations for modeling visual and tactile sensor data. A large multi-modal visual-tactile dataset is presented, and a straightforward pipeline is proposed for learning the data. Experiments are performed on cross-modal prediction tasks to validate the idea."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a substantial dataset comprising paired visual and tactile sensor data, which holds the potential for significant advancements in cross-modal research; \n\nThe paper's organization is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "The proposed approach has been exclusively assessed in the context of cross-modal prediction tasks, with no concrete verification of its applicability in downstream manipulation tasks; \n\nMoreover, it is worth noting that a single video observation could potentially correspond to a wide range of tactile signals, such as variations in the force applied when touching dough. Regrettably, the study does not appear to account for the inherent multimodality in the distribution of data in this respect; \n\nThe paper lacks technical details, e.g., the learning rate, batch size, etc. \n\nThe video prediction results for new instances and new categories seem not promising."
                },
                "questions": {
                    "value": "Providing additional technical details would enhance the reproducibility of the work, e.g., model architecture, training details, etc. \n\nIt would be interesting to see how the learned representations can be applied to downstream manipulation tasks, adding such results would further strengthen the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Reviewer_RFKG",
                        "ICLR.cc/2024/Conference/Submission2955/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2955/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698030685247,
            "cdate": 1698030685247,
            "tmdate": 1700803696435,
            "mdate": 1700803696435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kdp0IHUa30",
                "forum": "NtQqIcSbqv",
                "replyto": "HFWqNnJFH1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update on 1) added experiments on a robotic-manipulation-inspired task, 2) variational inference experiment, 3) details on model architecture and training setup"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback. We have updated our draft with 1) added experiment on a robotic-manipulation-inspired task, 2) variational inference, 3) details on model architecture and training setup \n\n**1. The proposed approach has been exclusively assessed in the context of cross-modal prediction tasks, with no concrete verification of its applicability in downstream manipulation tasks; (Question 2) It would be interesting to see how the learned representations can be applied to downstream manipulation tasks, adding such results would further strengthen the paper.** \n    \nWe thank the reviewer for this question, we added a robotic-manipulation-inspired experiment in our draft in Appendix Sec. A3. \n\nTo motivate downstream robotic applications, we show **two experiments** inspired by robotic perception applications. The high-level intuition is that if our learned embedding space trained with human-collected data can sufficiently represent data collected by robotic arms, we can transfer knowledge from human demonstration to robotic tasks. First, we collected some data using a robotic hand to show the cross-modal prediction results leveraging the pretrained embedding with human data. Additionally, we would like to see how much knowledge is transferrable from human demonstrations to data to a robotic hand. To achieve this goal, we train a simple classifier using our trained embedding space with human data to enable manipulation and object classification through tactile sequences. While we only show two applications here, other downstream tasks can be enabled by our proposed\nmethod and dataset. \n\nTo minimize the domain gap between robotic hand and human hand, we use a robotic motor to drive a tenden-driven OpenBionics hand, and record a tactile sequence shown in Fig. 8 in appendix. We use a test-time optimization approach to find a latent code in the shared embedding space to decode into other modalities. We show that we can perform cross-modal inference with robotic data. \n\nObject classification with shared embedding. One other potential application is to help robotic tasks in low-light settings, by equipping robotic hands with tactile sensors. We can leverage our pretrained cross-modal manifold to recognize objects and even operate objects when vision is impaired. With our trained shared latent space we train a classifier to classify the corresponding\nobject category and manipulation type from the signal sequence. We withhold some latent codes from the classifier to test the baseline performance on human data. We use test-time optimization to obtain a latent code in the shared embedding space using the robotic tactile data. We show the performance in Table. 2 in Appendix. We notice that human manipulation and robotic manipulation share a significant amount of transferrable and common knowledge.\n\nWhile these are simple robotic applications of our proposed dataset and method, other more advanced robotic applications are possible. We focus on learning a joint representation for different modalities of signals and leave more challenging robotic tasks as future work\n    \n**2. Moreover, it is worth noting that a single video observation could potentially correspond to a wide range of tactile signals, such as variations in the force applied when touching dough. Regrettably, the study does not appear to account for the inherent multimodality in the distribution of data in this respect;**\n    \nThank you for the question. Our method is able to handle and accommodate variational inference / multimodal predictions. This is actually the strength of our method. we have modified the manuscript to reflect this in section 4.3. Specifically, our framework learns the joint manifold of visual and tactile signals. This allows us to handle multimodal settings, as each possible pairing is in the manifold. This can be extracted by running test-time optimization with different latent initializations. By initializing different latent codes for test-time optimization, we can allow variational inference for a fixed sequence that is given. We added an experiment to reflect shown in the added Section A3 in the manuscript. \n    \nEven though touching dough is a very interesting and challenging topic indeed, our work primarily focuses on articulated object manipulation and cross-modal inference on these objects. We leave touching dough an interesting future work.\n\n\n**3. The video prediction results for new instances and new categories seem not promising.**\n\nThank you for the comment. Generalizing manipulation to new categories of objects is a very challenging task. When compared to the baseline methods, our proposed method achieves much better generalizability. With more data, our proposed method should do even better. We hope our work can shed some light on the research direction of generalizing manipulation to novel instances and categories."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461682271,
                "cdate": 1700461682271,
                "tmdate": 1700469319744,
                "mdate": 1700469319744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nn0E9CvlCT",
                "forum": "NtQqIcSbqv",
                "replyto": "QpHTRkj1KY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_RFKG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_RFKG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2019 response, which clarifies the majority of my inquiries."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619035027,
                "cdate": 1700619035027,
                "tmdate": 1700619035027,
                "mdate": 1700619035027,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "da5r30XzoL",
            "forum": "NtQqIcSbqv",
            "replyto": "NtQqIcSbqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2955/Reviewer_h9Bd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2955/Reviewer_h9Bd"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on understanding everyday objects and tools from a manipulation standpoint. The authors have constructed a multi-modal visual-tactile dataset, consisting of paired full-hand force pressure maps and manipulation videos. Additionally, they introduce a unique method to learn a cross-modal latent manifold. This manifold facilitates cross-modal prediction and uncovers latent structures in various data modalities. The extensive experiments establish the efficacy of the proposed approach approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper tackles a common issue, manifold learning, through a pragmatic lens within robotics applications. The study aims to address multi-modal learning problems in the visual-tactile sensory observation context, a highly practical setup for manipulation tasks. The proposed representation learning method can be beneficial for a multitude of downstream applications within visuotactile learning in robotics.\n\n2. The method proposed in the paper is straightforward, suggesting that it does not place a heavy computational load on the system.\n\n3. The authors have gathered a substantial paired dataset for visual and tactile signals. If made publicly accessible, this dataset could prove to be a valuable resource for further research."
                },
                "weaknesses": {
                    "value": "1. The method operates under the assumption that the sum of shape and tactile information equates to visual information. This assumption is manifested in the authors' approach of creating video latents by combining the latents of manipulation and the latents of canonical shapes. However, the tactile sequence may also encapsulate the object's geometric information. As suggested in the referenced paper 'Learning human\u2013environment interactions using conformal tactile textiles,' tactile information can be employed to classify object geometry. Consequently, it's worth questioning the efficacy of combining shape and tactile embeddings to produce the video embedding.\n\n2. The cross-modality query necessitates an optimization process. Therefore, it's crucial to provide information regarding the time cost of these experiments. For instance, how much time would be required to employ the neural field in this inverse manner?\n\n3. The absence of videos in the paper is a notable limitation. Including video content could significantly enhance the understanding of the tasks and experiments conducted in the study."
                },
                "questions": {
                    "value": "1. Do you want to claim the dataset as one of the contribution? In another word, would you open source the dataset once the paper is accepted?\n\n2. Could you clarify the symbol $\\gamma$ used in Equation 2? I was unable to locate a definition for it within the text.\n\n3. Your elaboration on $I_i$ and $I_j$ would be appreciated, specifically in relation to the following sentence. How is the distance within the space of $I$ quantified, and how is the subtraction operation defined in Eq3 for $I_i$ and $I_j$, given that $I$ is a three-modality tuple? While I recognize that the manifold isometry loss is a standard loss in the manifold learning field, I would like to confirm if the subtraction operation is a simple reduction operation in the raw data format.\n> any two samples sampled from the signal agnostic manifold $\\{z_i, z_j \\} \u2286 M$ respects the distance between the samples $I_i, I_j$ \n\nMinor issues:\n1. There appears to be a typographical error in the first line of the 'Dataset: Data Collection Setup' section \u2013 the citation parentheses are empty.\n2. It would enhance clarity if Figure 1 was referenced in Section 4.2 and if further details about the components in the figure were provided. This issue is also applicable to Figure 2.\n3. Please use last names when citing authors. For instance, it should be 'Chen et al.' instead of 'Peter et al.'."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Reviewer_h9Bd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2955/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698101356302,
            "cdate": 1698101356302,
            "tmdate": 1699636239431,
            "mdate": 1699636239431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bNhPVOSeVW",
                "forum": "NtQqIcSbqv",
                "replyto": "da5r30XzoL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Updated our draft to reflect 1) intuition on our proposed method 2) test-time optimization time cost, 3) added videos to supplementary website, 4) more details our open-source datset, 5) clarification of equations, 6) corrected typos."
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We have updated our draft to reflect 1) intuition on the method 2) test-time optimization time, 3) added videos to supplementary website, 4) more details on dataset to be open-sourced, 5) clarification of equations, 6) corrected typos. \n\n**1. The method operates under the assumption that the sum of shape and tactile information equates to visual information. This assumption is manifested in the authors' approach of creating video latents by combining the latents of manipulation and the latents of canonical shapes. However, the tactile sequence may also encapsulate the object's geometric information. As suggested in the referenced paper 'Learning human\u2013environment interactions using conformal tactile textiles,' tactile information can be employed to classify object geometry. Consequently, it's worth questioning the efficacy of combining shape and tactile embeddings to produce the video embedding.**\n\nThank you for the feedback. Reconstruction and classification tasks are fundamentally different. Classification can rely on local geometric information to predict object classes when objects are sufficiently different, as shown in the paper \u201cLearning\u2026. \u201c, whereas constructing the video sequence requires detailed information on the color of the object, the full geometry of the object, including the areas of the object that is not touched. The tactile force maps obtained from the tactile glove are not informative about the color of the object, the untouched area of the object, and the texture appearance of the object. \nWe thank the reviewer for the comment. We have updated the writing of our paper from \u201ctactile sequences contain less information about the object geometry\u201d  **to** \u201ctactile sequences contain limited information about the object appearance. It is not informative about the color of the object, the untouched area of the object, and the texture appearance of the object. We leverage complementary information from canonical images that contains complementary information on the color/texture/full geometry of the objects to construct video sequences.\u201c \n\nAdditionally, under our projection framework, shared information in both tactile and shape will be projected in both latent spaces, so this is not a problem.  \n\n**2. The cross-modality query necessitates an optimization process. Therefore, it's crucial to provide information regarding the time cost of these experiments. For instance, how much time would be required to employ the neural field in this inverse manner?**\n\nWe thank the reviewer for bringing this to our attention. We have included section A6 for more detail on the time for test time optimization.\n\nSpecifically, for test-time optimization of the latent code of our proposed approach, we use Adam optimizer to run 1000 time steps with a batch size of 32. For 1000 time steps, we use 56 seconds in total, 17.87 iteration/second, on average every example uses 1.75 seconds. With better computational resources or faster optimizers, the inference speed can be faster.\n\n\n**3. The absence of videos in the paper is a notable limitation. Including video content could significantly enhance the understanding of the tasks and experiments conducted in the study.**\n\nThank you for this constructive feedback. We have updated our website to include videos on our [supplementary website](https://sites.google.com/view/iclr-submission-force-vision)\n\n**4. Will the dataset be open-sourced once the paper is accepted?**\n\nThank you for the question, and yes, we will open-source the dataset once the paper is accepted. We have updated our details on the dataset that will be open-sourced for public use. \n\n**5. Could you clarify the symbol\u00a0$\\gamma$\u00a0used in Equation 2? I was unable to locate a definition for it within the text.**\n\nThank you for bringing this up. This is a typo. We have updated our draft with the correct formulation. Specifically, $L_{Rec} = | \\phi_{c} (z_{c, i}) - c_i|^2 + | \\phi_{h} (z_{h, i}) - h_i|^2 + | \\phi_{x} (z_{x, i}) - x_i|^2 $"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460362160,
                "cdate": 1700460362160,
                "tmdate": 1700460736864,
                "mdate": 1700460736864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HNwoM2ThNM",
                "forum": "NtQqIcSbqv",
                "replyto": "da5r30XzoL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**6. Your elaboration on\u00a0$\\mathcal{I}_i$\u00a0and\u00a0$\\mathcal{I}_j$\u00a0would be appreciated, specifically in relation to the following sentence. How is the distance within the space of\u00a0$I$\u00a0quantified, and how is the subtraction operation defined in Eq3 for\u00a0$I_j$ and $I_i$, given that\u00a0$I$\u00a0is a three-modality tuple? While I recognize that the manifold isometry loss is a standard loss in the manifold learning field, I would like to confirm if the subtraction operation is a simple reduction operation in the raw data format.**\n        \nThank you for this question. We have updated our draft for more clarity on the manifold isotropy loss. Specifically, we allow the distance between two sampled latent codes to be equal to the distance between the explicit signals expressed by the latent codes, using $d_z(i, j)$ to denote the latent distance between two latent codes $z_i,z_j$, and use $d_\\mathcal{I}(i, j),\\mathcal{I} \\in \\{x, c, h\\}.$ to denote distance between 2 signals:  $\\| c_i - c_j \\|$, $\\| x_i - x_j \\|$, or $\\| h_i - h_j \\|$. In our formulation: \n    \n$$L_{Iso} = d_z(i, j) - d_{\\mathcal{I} (i, j)} \\\\ =  \\left(d_z(z_{c,i}, z_{c, j}) - d_{\\mathcal{I}} (c_i, c_j)\\right) +\\left(d_z(z_{h,i}, z_{h, j}) - d_{\\mathcal{I}} (h_i, h_j)\\right)+ \\left(d_z(z_{x,i}, z_{x, j}) - d_{\\mathcal{I}} (x_i, x_j)\\right)$$\n$$= \\left( \\| \\|(z_{c, i}) - (z_{c, j})\\| - \\| c_i - c_j \\|   \\| \\right)+  \\left(\\| \\|(z_{x, i}) - (z_{x, j})\\| - \\| x_i - x_j \\|   \\| \\right) + \\left(\\| \\|(z_{h, i}) - (z_{h, j})\\| - \\| h_i - h_j \\|  \\| \\right) $$\n $$= \\left( \\| \\|\\Gamma_c({z_i}) - \\Gamma_c({z_j})\\| - \\| c_i - c_j \\|   \\| \\right)+  \\left(\\| \\|\\Gamma_x({z_i}) - \\Gamma_x({z_j})\\| - \\| c_i - c_j \\|   \\| \\right) + \\left(\\| \\|\\Gamma_h({z_i}) - \\Gamma_h({z_j})\\| - \\| h_i - h_j \\|  \\| \\right) $$\n    \nWe have updated our manuscript to reflect this in equations 4 and 5 in Section 4.2.\n\n**7. typos** \n\nWe thank the reviewer for pointing out our typos. We have corrected these issues and reflected in our updated manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460574611,
                "cdate": 1700460574611,
                "tmdate": 1700460688375,
                "mdate": 1700460688375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zUCmrFPfC1",
                "forum": "NtQqIcSbqv",
                "replyto": "HNwoM2ThNM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_h9Bd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_h9Bd"
                ],
                "content": {
                    "title": {
                        "value": "Replay to Author Rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' substantial effort in addressing the points raised during the review. The detailed explanation provided for the derivation has enhanced my understanding of the issue at hand. Additionally, the new webpage is truly impressive. While I am inclined towards the acceptance of this paper, considering the application and overall novelty, it's challenging to advocate for a strong acceptance."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631253588,
                "cdate": 1700631253588,
                "tmdate": 1700631253588,
                "mdate": 1700631253588,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iC17Qng9Kd",
            "forum": "NtQqIcSbqv",
            "replyto": "NtQqIcSbqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper purposed a force maps and RGB paird visual-tactile dataset. And further purpose to first represent each signal in a shared latent space, and then project the global manifold to local submanifold for each signal for reconstruction. The results demonstrate the effectiveness of purposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper tactles a more challenging visual-tactile prediction task which is harder compare to previous works.\n2. The design of preoject global manifold to local submanifold force the model to capture different signals, and to futher incorporate with the test time optimization method to improve the prediction results.\n3. The experiments are comprehensive.The TSNE results also show the model learned with some semantic meaningful infomation."
                },
                "weaknesses": {
                    "value": "1. My major concern is since the training set : testing set is aroud 12:1, and only include 4 categoreis, can this method really generalize to unseen objects? How is the diversity of the training and testing set?\n2. Presentation with Figure1: It would be better to also draw the process of getting shared latent space in the Figure1 for better understanding, it's quiet hard to undertand how to get a shared latent space from signals that are different dimension, could the author illustrate more about this ? Also projection layer of x seems missing in Figure 1."
                },
                "questions": {
                    "value": "1. If as the author statement, force maps and RGB id many-to-many mapping, what is the advantage of using force maps as tactile signal?  Why different object will not have similar surface texture property? And the challenge of disparity in spatial scale of different signals seems also exist even if it is one-one mapping?\n2. How is the robustness of the tactile glove, will it need to calibrate a lot to make sure the tactile data is accurate?\n3. How long will it take for test time optimization? Since this might be important for robotic application?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2955/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698568428996,
            "cdate": 1698568428996,
            "tmdate": 1700655760727,
            "mdate": 1700655760727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6zQiHXGXZ1",
                "forum": "NtQqIcSbqv",
                "replyto": "iC17Qng9Kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Updated 1) draft with clarification on dataset, 2) more details on proposed shared latent, 3) 4) details on data calibration 5) robotic experiment"
                    },
                    "comment": {
                        "value": "Thank you for the constructive feedback. We have updated our draft with 1) better clarification of train-test split, 2)clarification on shared manifold construction, 3) discussion and history on force maps as tactile signal, 4) details on calibration, 5) test-time optimization time. \n\n**1. My major concern is since the training set : testing set is aroud 12:1, and only include 4 categoreis, can this method really generalize to unseen objects? How is the diversity of the training and testing set?**\n\nThank you for raising this concern, we have revised our manuscript for increased clarity on the dataset statistics, shown in Section 3.  we provide more details and more description on the details of our dataset and train-test split. Our training and test set is 6:1, with 124,000 frames of data for training, (10,000 + 6,000 + 5,000) frames of data for testing. The details are as follows\n\n- Our training set contains 81 objects spanning 4 different categories, with paired tactile and video recordings of manipulation, containing 124000 frames of data.\n- Our test set is constructed with 3 different subsets:\n    - in-distribution test-set: constructed with withheld sequences of the same 81 objects seen during training, containing 10,000 frames of data.\n    - in-category test-set: contains withheld 4 objects unseen during training, containing around 6,000 frames of data.\n    - out-of-category test set: we collect data on 4 objects of unseen categories, containing 5,000 frames of data.\n\nWe test our proposed method on novel instances of a seen category as well as a novel category as shown in Section 5.3 in our paper. We do want to note that with more data, our proposed method can achieve better generalization. \n\n**2. Presentation with Figure1: It would be better to also draw the process of getting shared latent space in the Figure1 for better understanding, how to get a shared latent space from signals that are different dimension, where is projection layer of x?**\n\nThank you for the constructive feedback. We have updated the figure and writing for more clarity, reflected in Section 4.1 and 4.2 in blue. \n\nWith regard to the shared latent space, Existing methods, such as TouchandGo, ObjectFolder, VisGel, etc., employ encoder-decoder techniques where the latent codes are constructed and obtained from encoding a specific modality of information. However, we use an auto-decoder technique, using different branches of decoders to construct different modalities of information (tactile/vision). In this way, we allow gradients from different modalities of information to simultaneously update and optimize the latent embedding. In our implementation, we follow the DeepSDF implementation and randomly initialize our shared latent embedding to be trainable. During training, we sample a latent code $z_i$ to decode to different through different neural fields (coordinate networks), tactile signal $I_h = \\phi_h (\\Gamma_h(z_i))$ and video signal $I_x = \\phi_x(\\Gamma_x(z_i))$. \n\nIn order to handle information of different dimensions, we use neural fields or coordinate networks. As described in Section 3.1 in our paper, neural fields are different from traditional convolution networks whose construction is significantly bound by the dimension of signals. Neural fields take in coordinates $u$ and some latent code $z_{k}$ that contains information about the signal $k \\in \\{ c, x, h\\}$  to decode value at that coordinate $u$.  Formally, the value of the $i$-th sample for signal $k$ at coordinate $c$ is obtained: \n$I_{k, i, u} = \\phi_k ( z_{k, i}, u )$. \nIn our implementation, we use MLP that takes in input of \n$d_z + d_u$ \ndimensional to decode signal values \n$R^{d_{I}}$\n at coordinate \n$u$, $\\phi_j$ \nwhere \n$\\phi_j: R^{d_z + d_u} \\rightarrow R^{d_I}$. \nFor more information and interest in neural fields, please refer to [3]\n\nWe have updated our text and visual presentation on the construction of the video projection layer in our manuscript to provide more detail and clarity. \nThe video projection layer $\\Gamma_x$ is constructed by concatenating the image projection layer and tactile projection layer, For the details of each of our projection layers:\n\n$z_{c, i} = \\Gamma_c(z_i), \\Gamma_c \\in \\mathbb{R}^{256 \\times 256}$ \n\n$z_{h, i} = \\Gamma_h(z_i), \\Gamma_h \\in \\mathbb{R}^{256 \\times 16}$ \n\n$z_{x, i} = \\{\\Gamma_c; \\Gamma_h\\}(z_i) = \\{\\Gamma_c(z_i) ; \\Gamma_h(z_i)\\} = \\{z_{c, i} ; z_{h, i}\\}$   \nor equivalently, $\\Gamma_x = \\{\\Gamma_c; \\Gamma_h\\} \\in \\mathbb{R}^{256 \\times 272}$\n\n[1] Gao et al. Objectfolder: A dataset of objects with implicit visual, auditory, and tactile representations. CoRL, 2021.\n[2] Yang et al. Touch and go: Learning from human-collected vision and touch. NeuIPS\n[3] Li et al. Connecting touch and vision via cross-modal prediction. CVPR June 2019\n[4] Xie, Y., Neural fields in visual computing and beyond. In\u00a0*Computer Graphics Forum*."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458533926,
                "cdate": 1700458533926,
                "tmdate": 1700461765347,
                "mdate": 1700461765347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NKqd06wKvl",
                "forum": "NtQqIcSbqv",
                "replyto": "iC17Qng9Kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**3. If as the author statement, force maps and RGB id many-to-many mapping, what is the advantage of using force maps as tactile signal? Why different object will not have similar surface texture property? And the challenge of disparity in spatial scale of different signals seems also exist even if it is one-one mapping?**\n\nThank you for the question. Tactile signal might be a slightly overloaded term for referring to different types of signals, such as GelSight surface textures [1, 2, 3] and force responses from touch [5, 6]. To clarify, we use force maps as tactile signals, and we are not concerned with surface textures, by following previous Subramian et al.[5] and Luo et al [6]. Tactile, in general, refers to the responses of our skin when touching and making contact with the external environment. In this work, we only consider force maps instead of surface textures as our touch/haptics signal, because we are interested in different types of object manipulation. For this purpose, force maps are more informative about the manipulation action than knowing the surface texture of various objects. We believe surface texture is a very interesting topic indeed, but it is less relevant to our focus on hand-object manipulation.\n\nThe large disparity in spatial scale, in general, incurs challenges. Tasks become more difficult when the relation between different modalities is many-to-many mapping. In the cases with force-map tactile signals, dependent on the deformability of objects, the force map will be very different. In general, there could exist many pairings between RGB images and force maps. Our paper is interested in discovering how we can discover the shared structure across such disparate settings.\n\n[5] Sundaram et al. Learning the signatures of the human grasp using a scalable tactile glove.\u00a0Nature, 2019\n\n[6] Luo, et al. Learning human\u2013environment interactions using conformal tactile textiles.\u00a0Nature Electronics, 2021\n\n**4. How is the robustness of the tactile glove, will it need to calibrate a lot to make sure the tactile data is accurate?**\n\nThank you for the question. we have included more details on the background of the gloves and glove calibration in the dataset section in Appendix section A.1. \nSpecifically, the glove only needs to be calibrated once. The glove is made using piezoresistive film. The working principle of this material is when the material stretches, it causes the deformation of carbon nanotubes, which then induces the relative electrical resistance changes and thus difference in voltage readouts. The material in general is sturdy and the material property does not change much with wear and tear. Therefore, the glove only needs to be calibrated once to convert the sensor readouts in voltages to force magnitudes in Newton before use. \n\n**5. How long will it take for test time optimization? Since this might be important for robotic applications?**\n\nWe thank the reviewer for bringing this to our attention. We have included section A6 for more detail on the time for test time optimization.\n\nSpecifically, for test-time optimization of the latent code of our proposed approach, we use Adam optimizer to run 1000 time steps with a batch size of 32. For 1000 time steps, we use 56 seconds in total, 17.87 iteration/second, on average every example uses 1.75 seconds. With better computational resources or faster optimizers, the inference speed can be faster."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458725437,
                "cdate": 1700458725437,
                "tmdate": 1700468658126,
                "mdate": 1700468658126,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vSls3JIQ7j",
                "forum": "NtQqIcSbqv",
                "replyto": "iC17Qng9Kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply! \nI have three further questions:\n1. I am still curious what type of objects you are using for out-of-category tests, can you provide more details on this? (for example, provide the object list of your whole dataset in website)\n2. Consider your current design, Is this method robust for input video that has a different background? if during training, include a video image that has a different background, will it has a side effect for training, since the static image may not have information that the video image requires?\n3. although force map can offer more information about manipulation, will using additional texture can make this question easy, since the texture will make object classification easier, which seems can help map force and RGB?\n\nAlso for question 5: such time seems hard for real-time robotic manipulation, which might need to be improved.\n\nAlthough there are still few. question I am curious, these question may not eliminate the contribution of this work, thus, I would like to raise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530125436,
                "cdate": 1700530125436,
                "tmdate": 1700655739472,
                "mdate": 1700655739472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kmsCVRYIvY",
                "forum": "NtQqIcSbqv",
                "replyto": "iC17Qng9Kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
                ],
                "content": {
                    "comment": {
                        "value": "1. As you mentioned, articulation type is important for object manipulation, and the same object type usually has the same articulation type, from my perspective, if you know the object type, you will also know more information about how to manipulate it.\n2. I am also curious about how to use such a method for RL training. For example, for every RL step, you will get an new image, and then you have to do optimization to get correspond latent, how can you avoid these?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691191830,
                "cdate": 1700691191830,
                "tmdate": 1700691395414,
                "mdate": 1700691395414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "duSScue3I1",
                "forum": "NtQqIcSbqv",
                "replyto": "iC17Qng9Kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comment. \n\n**As you mentioned, articulation type is important for object manipulation, and the same object type usually has the same articulation type, from my perspective, if you know the object type, you will also know more information about how to manipulate it.**\n\nIndeed, knowing the object type will help with knowing the general articulation type. However, in addition to object type, the actual action that will induce the desired articulation varies based on the size of the object, and physical properties of parts inside the object (e.g. stiffness or strength of the springs), and etc. Force maps are more informative on these aspects as compared to texture. The video sequences used in the method are very informative about the object type, and as shown in the TSNE of the latent visualization, the learned latent manifold clearly clusters data based on both the object type and manipulation type. \n\n**I am also curious about how to use such a method for RL training. For example, for every RL step, you will get a new image, and then you have to do optimization to get the corresponding latent, how can you avoid these?**\n\nThe most naive way to use our method in a robotic setting is to use it as a pretraining strategy for RL. For example, each latent code in the pre-trained latent embedding can be projected and decoded into state spaces for the robot arm to conduct similar actions as the decoded human manipulation sequences to mimic human manipulation. With such a pretraining stage, sample efficiency can be improved as training the RL policy is now dealing with a subset of possibly higher-success state spaces instead of all possible state spaces.  We leave efforts on policy training for robotic manipulation a future work, and study the hand-object manipulation synergy in the current effort. \n\nAdditionally, we want to note that decoding each latent code into video and tactile sequences is instantaneous, the optimization takes a long time because many optimization steps are taken, and our method is still relevant as long as the test-time optimization is not used during the online testing stage."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693462368,
                "cdate": 1700693462368,
                "tmdate": 1700693684024,
                "mdate": 1700693684024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WXH0WIXrAU",
            "forum": "NtQqIcSbqv",
            "replyto": "NtQqIcSbqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2955/Reviewer_ksmF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2955/Reviewer_ksmF"
            ],
            "content": {
                "summary": {
                    "value": "The authors have curated a unique visual-tactile dataset and introduced a manifold algorithm to explore the cross-modal relationship between objects and their manipulation. By visualizing the cross-modal latent structures, they showcase that their approach outperforms current methods and effectively generalizes manipulations to unfamiliar objects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is articulately written, offering clarity and ease of comprehension, making it accessible even to readers unfamiliar with the subject matter.\n2. A significant contribution of this research is the introduction of a novel visual-tactile dataset, especially noteworthy given the limited datasets available in this domain.\n3. The innovative manifold learning approach presented has the potential to pave the way for subsequent research.\n4. Through experiments, the paper effectively showcases the promise of the cross-modal retrieval, prediction, and the latent structure. Compared to existing methodologies, the proposed approach holds considerable promise."
                },
                "weaknesses": {
                    "value": "1. The dataset would benefit from enhanced visualization and in-depth details, possibly within the appendix.\n2. There's a typographical error on page 5 after equation 2; \"Additioanlly\" should be corrected to \"Additionally.\"\n3. Based on observations from figures 3 and 4, the sequences appear to have minimal variation across different frames. Displaying greater variation would add value. Additionally, considering a baseline that utilizes only the initial frame, as opposed to the entire sequence data, could provide intriguing insights."
                },
                "questions": {
                    "value": "Please see the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2955/Reviewer_ksmF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2955/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698650775658,
            "cdate": 1698650775658,
            "tmdate": 1699636239273,
            "mdate": 1699636239273,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZMIdXtbgTX",
                "forum": "NtQqIcSbqv",
                "replyto": "WXH0WIXrAU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Updated draft and website 1)  include more visuals and videos, 2) corrected typos, and 3) more experimental results"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments. We have updated our manuscript to include more visuals and videos, corrected typos, and added more results with more frame-wise variations. \n\n**1) The dataset would benefit from enhanced visualization and in-depth details, possibly within the appendix.**\n\nThank you for this feedback. We have updated our manuscript to include more details about our dataset both in Section A2 in the Appendix and on our [supplementary website](https://sites.google.com/view/iclr-submission-force-vision/). We added a demo figure of video sequences of our tactile and RGB signal from 12 objects sampled from our dataset, as well as some video sequences of experiments. We have also added more details about our dataset collection and dataset statistics in Section 3 of our paper. We also added a Section to cover more background on the tactile glove and sensor calibration. We hope these modifications improve the clarity of our paper about our dataset.\n\n**2) There's a typographical error on page 5 after equation 2; \"Additioanlly\" should be corrected to \"Additionally.\"**\n\nWe have corrected this typo. Thank you for bringing this to our attention. \n\n**3) Based on observations from figures 3 and 4, the sequences appear to have minimal variation across different frames. Displaying greater variation would add value. Additionally, considering a baseline that utilizes only the initial frame, as opposed to the entire sequence data, could provide intriguing insights.**\n    \nThank you for the feedback. We hope that the video demo of our dataset on our supplementary webpage helps provide a better illustration of the signal variation across different frames. \n    \nWe added a test that only the first frame as the test-time optimization energy signal and added it to our ablation experiments. Empirically, we observe that it only using first frame decreases cross-modal retrieval accuracy. We believe that this difference is caused by the increase of variational inference, or in other words, with less optimization signal, there are now more local minima during test-time optimization that fit the optimization objective. More details can be found in Sec. A4 in our appendix. We would appreciate more details on the kind of insights or the kind of first frame setup you may be looking for."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462119527,
                "cdate": 1700462119527,
                "tmdate": 1700468631745,
                "mdate": 1700468631745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fSLecPNNM4",
                "forum": "NtQqIcSbqv",
                "replyto": "ZMIdXtbgTX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_ksmF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_ksmF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply! My concerns have been addressed, but I will keep the original rating given the whole contribution."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2955/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510997133,
                "cdate": 1700510997133,
                "tmdate": 1700510997133,
                "mdate": 1700510997133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]