[
    {
        "title": "Graph Generation with  $K^2$-trees"
    },
    {
        "review": {
            "id": "zEGhSuvsp0",
            "forum": "RIEW6M9YoV",
            "replyto": "RIEW6M9YoV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4327/Reviewer_XLS2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4327/Reviewer_XLS2"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on generating graphs from a target distribution, which is a vital task in several areas like drug discovery and social network analysis. The paper introduces a new framework termed as \"Hierarchical Graph Generation with K^2\u2212Tree\" (HGGT). This model (1) uses a K^2\u2212Tree representation, which was originally designed for lossless graph compression, enabling a compact graph representation while also capturing the hierarchical structure of the graph. (2) Incorporates pruning, flattening, and tokenization processes in the K^2\u2212Tree representation. (3) Introduces a Transformer-based architecture, optimized for generating sequences by using a specialized tree positional encoding scheme."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper's emphasis on hierarchically capturing graph structures using K^2\u2212Tree is technically sound. Hierarchies are crucial for many real-world graphs, and K^2\u2212Tree, with its inherent structure, naturally offers this advantage.\n\n2. The introduction of pruning, flattening, and tokenization processes aims to achieve a compact representation. This can lead to both storage and computational efficiencies, which are pivotal when dealing with large-scale graph data."
                },
                "weaknesses": {
                    "value": "1. I have doubts about the motivation, which is not strong enough to drive the development of such work. \n\n2. Related work missing, such as [1, 2]\n\n3. The paper doesn't detail the computational resources required, which raises concerns about its practicality for very large graphs.\n\n4. Tokenization can sometimes lead to loss of information, and without details, it's uncertain how this impacts the overall graph representation.\n\n[1] Kong, Lingkai, et al. \"Autoregressive Diffusion Model for Graph Generation.\" (2023).\n\n[2] Chen, Xiaohui, et al. \"Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling.\" (2023)"
                },
                "questions": {
                    "value": "See weakness, I'd like to raise my score if concern is addressed"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4327/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4327/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4327/Reviewer_XLS2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4327/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698521555418,
            "cdate": 1698521555418,
            "tmdate": 1699636401958,
            "mdate": 1699636401958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MGRJzCPC6K",
                "forum": "RIEW6M9YoV",
                "replyto": "zEGhSuvsp0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer XLS2,\n\nWe sincerely appreciate your comments and efforts in reviewing our paper. We address your question as follows. We also updated our manuscript which is highlighted in $\\color{red}{\\text{red}}$.\n\n**W1. Motivation is not strong enough.**\n\nTo alleviate your concern, we first restate our motivation stated in our introduction (development of compact and hierarchical representation) and then strengthen it by providing the underlying idea (exploiting large zero-filled submatrices) that is more explicit. \n\nThe motivation stated in our introduction is the benefits of compact and hierarchical representation for graph generation. Compact representation reduces the complexity of graph generation and simplifies the search space over graphs. Hierarchical representation allows generating the graph in a coarse-to-fine way, which aligns with the hierarchical nature of real-world graphs.\n\nOur idea beneath this motivation is to map the patterns (zero-filled blocks) being repeated across the whole dataset into simple objects (nodes in the $K^2$-tree) that the model can easily generate. This mapping removes the burden of the generative model to learn how to generate large patterns and allows the model to focus on learning instance-specific details. \n\nWe also note that our $K^2$-tree being hierarchical allows us to handle various patterns (zero-filled blocks with varying sizes) in a systematic way. Our $K^2$-tree being compact for a dataset implies that the patterns (zero-filled blocks) indeed frequently appear across the whole dataset.\n\nWe believe this idea is strong enough to drive our work since similar ideas showed success in different domains in different forms. \n- Most of the successful language models [1] generate a sentence word-by-word instead of character-by-character. Here, the recurring patterns of characters are mapped into word tokens.\n- Several successful molecular generative models [2, 3] also generate molecules fragment-by-fragment. Here, the recurring patterns of atoms and bonds are mapped into fragments.\n\n---\n\n**W2. Related works are missing, such as GraphARM and degree-guided diffusion.**\n\nThank you for pointing out the references. We added the references about GraphARM and the degree-guided diffusion model in our updated manuscript. \n- GraphARM (ICML 2023) is a very recent work that improves over the existing diffusion models based on the forward and reverse diffusion process that iteratively masks and unmasks the graph elements in an autoregressive way. \n- Degree-guided diffusion (ICML 2023) is also a very recent work that improves the scalability of the diffusion models for graph generation. It exploits graph sparsity based on adding edges between only a small portion of nodes at each graph construction (reverse diffusion) step.\n\n---\n\n**W3. The paper doesn't detail the computational resources.**\n\nWe used a single GeForce RTX 3090 GPU to train and generate samples with HGGT. This was detailed in Appendix E.1 in our original manuscript. We added the description to our main paper for better visibility in the updated manuscript. \n\n---\n\n**W4. Tokenization can sometimes lead to loss of information, and without details, it's uncertain how this impacts the overall graph representation.**\n\nWe would like to clarify that our tokenization does not lose any information. Our tokenization is a mapping from $K^2$ binary digits to a vocabulary of size 24 where one can perfectly reconstruct any $K^2$-tree after the tokenization process. We systematically designed our tokenization process so that there is no unknown token (<UNK>) that may lead to loss of information as the reviewer mentioned.\n\n    \n**References**\n\n[1] Devlin, J., et al., BERT: Pre-training of deep bidirectional transformers for language understanding. Association for Computational Linguistics 2018.\n    \n[2] Jin, W., et al., Junction tree variational autoencoder for molecular graph generation. ICML 2018.\n    \n[3] Jin, W., et al., Hierarchical generation of molecular graphs using structural motifs. ICML 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4327/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033195974,
                "cdate": 1700033195974,
                "tmdate": 1700033195974,
                "mdate": 1700033195974,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JldT93F1Sm",
            "forum": "RIEW6M9YoV",
            "replyto": "RIEW6M9YoV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4327/Reviewer_K9N3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4327/Reviewer_K9N3"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new graph generative model which capitalizes on the $K^2-$tree representation, a  representation of graphs that is more compact than the adjacency matrix. The $K^2-$tree representation is transformed into a sequence and then a Transformer-based architecture is employed which predicts one token at a time based on the previously generated sequence. The Transformer is also equipped with positional encodings that take into account the structure of the tree. The proposed model is trained on synthetic and real-world datasets. The results indicate that in most cases, the generated graphs better preserve graph properties than the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed model shows strong empirical performance over previous baselines on both synthetic and real-world datasets. Thus, HGGT could be a useful addition to the list of graph generative models.\n\n- The $K^2-$tree representation seems interesting and the proposed model has some novelty. Even though there are previous works that have proposed autoregressive models for graph generation, in my view the main components of HGGT are different from those of previous works.\n\n- The model supports node and edge features, while the results reported in Figure 7 suggest that HGGT is much more efficient than competing models."
                },
                "weaknesses": {
                    "value": "- The paper claims that the employed representation is hierarchical, however, I do not fully agree with this claim. In case a hierarchical community structure is present in the graph, a hierarchical representation is supposed to capture this community structure. However, the proposed $K^2-$tree representation would not necessarily capture this (since it depends on the node ordering). On the other hand, the hierarchical clustering algorithm would produce a proper hierarchical representation. I thus think that this claim needs rephrasing to avoid misunderstanding.\n\n- The proposed model is conceptually similar to GRAN [1] which sequentially generates blocks of nodes and associated edges. A detailed discussion of how HGGT differs from GRAN is missing from the paper.\n\n- One of my main concerns with this work is that it is not clearly explained in the paper why the proposed model significantly outperforms the baselines. This is not the first autoregressive model for graph generation, and previous models also came up with different schemes to reduce the time and space complexity (such as BFS ordering and generation of blocks of the adjacency matrix in [2] and [1], respectively). Thus, I would not expect such a significant difference in performance between HGGT and those previous models. I would like the authors to comment on this.\n\n- In Table 2, we can observe that the novelty of the generated molecules is low compared to those of the baselines (mainly on QM9). I would expect the authors to provide some explanation or intuitions about why the proposed model fails to produce novel graphs.\n\n- In section 5.2, it is mentioned that \"Each metric is measured between the 10,000 generated samples and the test set\". I do not think that this is actually true. If I am not wrong the validity and the uniqueness have nothing to do with the samples of the test set. Furthermore, the Frechet ChemNet Distance and the novelty are commonly computed by comparing the generated samples against those of the training set and not those of the test set.\n\n[1] Liao, R., Li, Y., Song, Y., Wang, S., Hamilton, W. L., Duvenaud, D., Urtasun, R., & Zemel, R. \"Efficient graph generation with graph recurrent attention networks\". In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 4255-4265, 2019.\\\n[2] You, J., Ying, R., Ren, X., Hamilton, W., & Leskovec, J. \"Graphrnn: Generating realistic graphs with deep auto-regressive models\". Proceedings of the 35th International Conference on Machine Learning, pp. 5708-5717, 2018."
                },
                "questions": {
                    "value": "In p.5, why $K^2$ elements are not enough and $K(K + 1)/2$ more elements are added, thus increasing the vocabulary size for each token?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4327/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698620614730,
            "cdate": 1698620614730,
            "tmdate": 1699636401858,
            "mdate": 1699636401858,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I7yiaxn2Jy",
                "forum": "RIEW6M9YoV",
                "replyto": "JldT93F1Sm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer K9N3,\n\nWe sincerely appreciate your comments and efforts in reviewing our paper. We address your question as follows. We also updated our manuscript which is highlighted in $\\color{red}{\\text{red}}$.\n\n---\n**W1. The claim \"capturing the hierarchical graph structure\" and the description \"hierarchical representation\" need rephrasing.**\n\nThank you for the insightful comment. We agree that our claims on \"capturing the hierarchical graph structure\" can be ambiguous. In our updated manuscript, we added a discussion on how our HGGT does not necessarily capture the exact hierarchical community structure in Appendix H.\n\nHowever, we believe it is still appropriate to describe our $K^2$-tree as a hierarchical representation, hence choose to precisely specify the terminology instead of entirely rephrasing it. Our precise description is incorporated in the updated manuscript and explained in what follows.\n\nNodes in the $K^2$-tree form a parent-child hierarchy between the nodes. To be specific, each node in our $K^2$-tree corresponds to a block (i.e., submatrix) in the adjacency matrix. Given a child and its parent node, the child node block is a submatrix of the parent node block, hence the tree also represents a hierarchical structure between the blocks. While this hierarchy may differ from the hierarchical community structure, the $K^2$-tree representation still represents a valid hierarchy present in the adjacency matrix.  \n\nFinally, we also point out prior works [1,2] that imply how the Cuthill\u2013McKee node ordering (used in our HGGT) partially captures the underlying community structure. These prior works additionally support our statement that our $K^2$-tree representation is a hierarchical representation. This is incorporated in Appendix H of our updated manuscript.\n\n---\n\n**W2. A detailed discussion of how HGGT differs from GRAN is missing.**\n\nThank you for pointing this out. We compare our HGGT with GRAN in the updated manuscript (Appendix H) and in what follows.\n\nFirst, the main conceptual difference is in the representation generated by HGGT and GRAN. \n- HGGT generates $K^2$-tree representation for graphs with large zero-filled blocks. This is based on summarizing the large zero-filled blocks into a single node in the $K^2$-tree. \n- GRAN generates the conventional adjacency matrix representation. The *block matrices are conceptually irrelevant for the graph representation* and instead used for parallel decoding. \n\nNext, as the reviewer mentioned, both HGGT and GRAN use block-wise decoding. However, the decoding processes of the block matrix elements are different. \n- HGGT sequentially specifies the block matrix elements in a hierarchical way. It first specifies whether the whole block matrix is filled with zeros at the upper level of the $K^2$-tree, then proceeds to specify smaller submatrices in the lower levels of the $K^2$-tree. \n- GRAN can optionally decode the block matrix elements in parallel. All the elements in the block matrices are generated at once to speed up the decoding process (at the cost of slightly lower performance).\n\nFinally, the semantics and shapes of the block matrix in HGGT and GRAN are also different.\n- The square-shaped HGGT block defines connectivity between a pair of equally-sized node sets. \n- The rectangular-shaped GRAN block defines connectivity between a set of newly added nodes and the existing nodes at each decoding step. \n\nThis is incorporated in Appendix H of our updated manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4327/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700032894414,
                "cdate": 1700032894414,
                "tmdate": 1700032894414,
                "mdate": 1700032894414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bUPPSgfFuk",
                "forum": "RIEW6M9YoV",
                "replyto": "JldT93F1Sm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W3. It is not clearly explained in the paper why the proposed model significantly outperforms the baselines. Previous models also came up with different schemes to reduce the time and space complexity.**\n\nOur main intuition on why HGGT outperforms the baselines is that only our HGGT maps the recurring patterns in the dataset (large zero-filled block matrices) into a simple object (a zero-valued node in the $K^2$-tree) that the model can easily generate. This mapping allows the generative model to focus on learning instance-specific details rather than the generation of the whole pattern that is common across the dataset. The baseline space and memory reduction techniques do not yield this benefit, e.g., GRAN and GraphRNN propose parallel decoding and constraining the search space, respectively.\n\nIn what follows, we make the one-to-one comparison between our scheme and the existing space and memory reduction schemes. Note that we do *not* claim HGGT to strictly improve over the baselines in a conceptual way. In fact, it is hard to make an apple-to-apple comparison between the reduction techniques since the ideas are orthogonal, e.g., the $K^2$-tree can be generated via parallel decoding (like GRAN) and made smaller by constraining the adjacency matrix (like GraphRNN). \n\n**Comparison with GRAN.** As already discussed in **W2**, GRAN employs block-wise parallel decoding to reduce the number of decoding steps for generating the whole adjacency matrix. The parallel decoding does not reduce representation size and the search space, hence lowering the performance for faster generation.\n\n**Comparison with GraphRNN.** While GraphRNN also reduces the representation size (and alleviates the long-range dependency problem), our reduction is higher. \n\nTo be specific, GraphRNN limits the search space for adjacency matrices by constraining the \"upper-corner\" elements of the adjacency matrix to be consecutively zero, based on the maximum size of the BFS queue. See Figure 15 in Appendix H for an illustration. \n\nIn comparison, while our HGGT does not put constraints on the graphs being generated, both HGGT and GraphRNN reduce the representation size; this removes the burden of the graph generative models learning long-range dependencies. Empirically, our reduction is higher as shown in the below table that reports the average length of representations for the Community, Planar, Enzymes, and Grid datasets. Note that the representation size of HGGT and GraphRNN indicates the number of tokens and the number of elements limited by the maximum size of the BFS queue, respectively. While the comparison is not fair due to different vocabulary sizes, one could expect HGGT to suffer less from the long-range dependency problem due to the shorter representation.\n\n\n|             | Comm. | Planar | Enzymes |    Grid |\n| ----------- | -----:| ------:| -------:| -------:|\n| Full matrix | 241.3 | 4096.0 |  1301.6 | 50356.1 |\n| GraphRNN    |  75.2 | 2007.3 |   234.7 |  4106.3 |\n| HGGT (Ours) |  30.3 |  211.7 |    67.3 |   419.1 |\n\nThis is incorporated in Appendix H of our updated manuscript.\n\n---\n\n**W4. Why is the result on QM9 not very good, especially for Novelty? Does it indicate the inferiority of HGGT on novel graph generation?**\n\nWe first note that graph generative models that faithfully learn the training dataset are more likely to assign high likelihoods on the training dataset, hence the trade-off between the quality and the novelty of graph generation is inevitable. This trade-off is especially significant for the QM9 dataset since it consists of a large number of molecules (134k) despite the small search space (molecules with up to only nine heavy atoms). Indeed, no baseline has successfully avoided this trade-off in the QM9 dataset, e.g., Digress achieves a good FCD at the cost of low novelty in Table 2. We chose to achieve high scores for faithfully learning the underlying distribution (FCD and NSPDK) since we consider them to be more meaningful as evidence for high-quality molecule generation.\n\nInstead of the QM9 results, we would like to put more emphasis on the ZINC250k dataset with a larger search space and higher relevance to real-world applications (drug discovery). In the corresponding result, our HGGT achieves much better FCD and NSPDK metrics compared to the baselines, at the cost of a very small decrease in uniqueness and novelty. We also note that one can compensate for the non-unique and non-novel molecules by filtering them out and generating new molecules."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4327/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700032990507,
                "cdate": 1700032990507,
                "tmdate": 1700035801895,
                "mdate": 1700035801895,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y0i4NIAxy9",
                "forum": "RIEW6M9YoV",
                "replyto": "JldT93F1Sm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W5. There exists some mis-explanation for molecular evaluation metrics.**\n\nThank you for pointing this out. Indeed, the validity and the uniqueness are computed with the generated samples themselves and the novelty is computed by comparing with the training set. We fixed our description in Section 5.2. of our updated manuscript.\n\nOtherwise, following the MOSES benchmark [3], FCD and NSPDK are measured between the 10,000 generated samples and the test set. It is also notable that we followed other prior works on graph generative models [4, 5] that reported FCD between generated samples and test sets.\n\n---\n\n**Q1. Why do you choose the $2^{K^2} + 2^{K(K+1)/2}$ vocabulary instead of $2^{K^2}$ which leads to the larger vocabulary size?**\n\nOur main insight is that the semantics of the diagonal blocks are different from non-diagonal blocks in the $K^2$-tree, as described in the fourth paragraph of Section 4.1. To incorporate this idea, we assign different vocabulary for the diagonal (size $2^{K(K+1)/2}$) and non-diagonal blocks (size $2^{K^{2}}$), resulting in a larger vocabulary size. In practice, we choose $K=2$ and the increase in vocabulary size ($2^{K(K+1)/2}=8$) is not significant.\n\n\n**References**\n\n[1] Barik, R., et al., Vertex reordering for real-world graphs and applications: An empirical evaluation. IISWC 2020.\n\n[2] Mueller, C., et al., Sparse matrix reordering algorithms for cluster identification. Machine Learning in Bioinformatics 2004.\n\n[3] Polykovskiy, D., et al., Molecular sets (MOSES): a benchmarking platform for molecular generation models. Frontiers in pharmacology 2020.\n\n[4] Jo, J., et al., Score-based generative modeling of graphs via the system of stochastic differential equations. ICML 2022.\n\n[5] Luo, T., et al., Fast graph generative model via spectral diffusion. arXiv preprint 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4327/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033050077,
                "cdate": 1700033050077,
                "tmdate": 1700033097026,
                "mdate": 1700033097026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Id77yvTGjL",
                "forum": "RIEW6M9YoV",
                "replyto": "y0i4NIAxy9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4327/Reviewer_K9N3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4327/Reviewer_K9N3"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer K9N3"
                    },
                    "comment": {
                        "value": "I thank the authors for their response. After reading the response, I think some of my concerns are not well addressed.\n\nI still feel that the use of the term \"hierarchical representation\" is somewhat misleading. Even though, this is actually a hierarchical representation, this term has a different meaning in this context. For example, there is a large body of works that propose models which learn hierarchical representations such as [1] and [2], and those representations capture the community structure of the input graph. I feel that the discussion added in the appendix does not address this issue.\n\nThe explanation given in my question on why the proposed model significantly outperforms the baselines makes sense in general. However, it assumes that the imposed node ordering is optimal. How likely is this to happen? Does the employed algorithm provide any guarantees? Also $K$ is set equal to small values, and the emerging trees are not that small in practice. I wonder whether the increase in performance provided by HGGT is due to the Transformer (which is much more complex than the RNNs employed in previous works).\n\nIn case of very dense graphs, how would the authors expect the HGGT model to perform? Would it still be more efficient than the baselines?\n\nI am not fully convinced by the claim that the search space is small on the QM9 dataset. There exist models that achieve very high values of novelty. Moreover, in some applications, generative models are expected to produce only novel samples. How did the authors choose to achieve high scores for FCD and NSPDK? Is there some hyperparameter that allows the model to put more emphasis on them than on validity? More importantly, why do the authors consider FCD and NSPDK to be more meaningful as evidence for high-quality molecule generation? Can the authors provide some reference to back up their claim?\n\n[1] Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., & Leskovec, J. (2018). Hierarchical graph representation learning with differentiable pooling. Advances in Neural Information Processing Systems.\\\n[2] Cangea, C., Veli\u010dkovi\u0107, P., Jovanovi\u0107, N., Kipf, T., & Li\u00f2, P. (2018). Towards sparse hierarchical graph classifiers. arXiv:1811.01287."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4327/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608289450,
                "cdate": 1700608289450,
                "tmdate": 1700608289450,
                "mdate": 1700608289450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u4TTrh9jWw",
            "forum": "RIEW6M9YoV",
            "replyto": "RIEW6M9YoV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4327/Reviewer_JeGt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4327/Reviewer_JeGt"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a new graph generative model Hierarchical Graph Generation with $K^2$\u2013Tree (HGGT). $K^2$-tree is a lossless graph representation and the authors compress it by pruning, flattening and tokenizing operations such that it fits to Transformer with $K^2$-tree positional encoding for graph generation. The effectiveness and efficiency of HGGT are evaluated on six datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The approach of combining $K^2$-tree compressed representation with Transformer is new.\n\n(2) The performance of HGGT is superior to the SOTA baselines on most datasets."
                },
                "weaknesses": {
                    "value": "(1) The performance of HGGT (Table 2) is not so satisfactory for molecular graph generation which is probably the most important application of this graph generative model.\n\n(2) It lacks the worst case time complexity analysis for the algorithms."
                },
                "questions": {
                    "value": "(1) Why is the performance of HGGT on molecular datasets not so good as that on the generic graph datasets? It seems that HGGT achieves the worst score on three metrics of the two molecular benchmarks (Uniqueness on QM9 and Novelty on both).\n\n(1) What are the time complexities of Algorithms 1-4 and HGGT?\n\n(2) Is the $K^2$-representation still lossless after pruning, flattening and tokenization? I guess yes, but is there a simple proof for this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4327/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4327/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4327/Reviewer_JeGt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4327/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698686553536,
            "cdate": 1698686553536,
            "tmdate": 1699636401726,
            "mdate": 1699636401726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LZUL3mrzs0",
                "forum": "RIEW6M9YoV",
                "replyto": "u4TTrh9jWw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer JeGt,\n\nWe sincerely appreciate your comments and efforts in reviewing our paper. We address your question as follows. We also updated our manuscript which are highlighted in $\\color{red}{\\text{red}}$.\n\n\n---\n\n**W1/Q1. The performance of HGGT (Table 2) is not so satisfactory for molecular graph generation. Why is the performance not so good as that on the generic graph datasets?** \n\nWe believe our molecular graph generation performance to be satisfactory since it achieves better NSPDK, FCD, QED, and SA as reported in Section 5.2 and Appendix G.2. However, we resonate with your concern since the HGGT scores low novelty and uniqueness for the QM9 dataset, which may yield the impression that our scores are not so satisfactory. To alleviate your concern, we explain why (a) low novelty is a natural consequence of faithfully learning the molecular distribution and (b) one should put more emphasis on the ZINC250k dataset when interpreting our results.\n\nFirst, we note that the models make a tradeoff between the quality (e.g,. NSPDK and FCD) and novelty of the generated graph since the graph generative models that faithfully learn the distribution put a high likelihood on the training dataset. In particular, the tradeoff is more significant in QM9 due to the large dataset size (134k) compared to the relatively small search space (molecular graphs with only up to nine heavy atoms). It is noteworthy that no existing baseline has successfully avoided this trade-off on the QM9 dataset. \n\nInstead of the QM9 results, we would like to put more emphasis on our results on the ZINC250k with a larger search space and higher relevance to the real-world application, i.e., drug discovery. Here, one can observe our HGGT to achieve better FCD and NSPDK metrics compared to the baselines, while trading off for a very small decrease in uniqueness and novelty. \n\n---\n\n**Q2/W2. What is the time complexity of $K^2$-tree construction and graph construction from $K^2$-tree?**\n\nThe time complexity of $K^2$-tree construction from the adjacency matrix is $O(N^2)$ where $N$ is the number of vertices in the graph, as stated in the original paper [1] that proposed the $K^2$-tree for graph compression. The adjacency matrix construction algorithm also takes $O(N^2)$ time complexity since it requires querying for each element in the adjacency matrix. This is incorporated in Apepndix A and B of our updated manuscript.\n\n---\n\n**Q3. Is the $K^2$-representation still lossless after pruning, flattening and tokenization?**\n\nYes. Given a $K^2$-tree, the pruning process iteratively removes each $(i, j)$-th child node associated with a redundant submatrix, i.e., a submatrix consisting of matrix elements positioned above the matrix diagonal. Each removal can be inverted by duplicating the $(j, i)$-th child node as a new $(i, j)$-th child node. \n\nTo implement the lossless reconstruction in a simple way, one could apply Algorithm 2 (in Appendix B) to the pruned $K^2$-tree to obtain an incomplete adjacency matrix $\\hat{A}$ with zero entries above the diagonal. Then one can recover the full matrix by duplicating the entries below the diagonal, i.e., set the adjacency matrix by $A = \\hat{A}+\\hat{A}^{\\top}$.\n\nIn addition, the flattening process is lossless since it flattens the tree with breadth-first traversal. The un-flattening process is defined by adding all the child nodes for each token to a uniquely defined parent node, i.e., the oldest node with label one and without a child node. This process is lossless since it uniquely maps a token to each sequence to a unique $K^2$-tree structure.\n\nTo further verify our claim, we conducted experiments on the reconstruction of the original adjacency matrices from the pruned $K^2$-trees, and the results showed a 100% reconstruction rate. This finding confirms that pruned $K^2$-tree is a lossless compression method.\n\n**References**\n\n[1] Brisaboa et al., $K^2$-Trees for Compact Web Graph Representation. International Symposium on String Processing and Information Retrieval 2009."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4327/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700032757877,
                "cdate": 1700032757877,
                "tmdate": 1700032757877,
                "mdate": 1700032757877,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CJ5QXoo1aR",
            "forum": "RIEW6M9YoV",
            "replyto": "RIEW6M9YoV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4327/Reviewer_Nzff"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4327/Reviewer_Nzff"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel algorithm to generate graphs based upon $K^2$-tree representation. One of the positive sides of $K^2$-tree representation lays in the fact that it ensures the compactness of the obtained representation without losing the hierarchical information from the nodes and edges in the original graph. After having described how $K^2$-tree representation works, the authors outline the generation algorithm built upon it. Specifically, the algorithm prunes redundant nodes from the representation (e.g., given its symmetrical nature); the, it flattens and tokenizes the pruned $K^2$-tree; finally, it exploits a Transformer architecture to generate the new graph through positional encoding. Results on various graph learning tasks and domains against other state-of-the-art graph generation solutions outline the efficacy of the proposed approach. The evaluation is complemented through an extensive ablation study which further validates the goodness of the algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is well-written and easy-to-follow.\n+ The proposed algorithm is simple but effective.\n+ The proposed algorithm is also able to generate featured graphs (e.g., molecular structures which come with features on graph edges).\n+ The experimental analysis is extensive and supports the efficacy of the proposed solution.\n+ The code is released at review time."
                },
                "weaknesses": {
                    "value": "- To the best of my knowledge, I cannot see any specific weakness."
                },
                "questions": {
                    "value": "* Could it be possible to adopt the proposed graph generation algorithm to create graphs with specific topological properties (e.g., node degree or clustering coefficient)?\n\n**After the rebuttal.** The rebuttal answered all questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4327/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4327/Reviewer_Nzff",
                        "ICLR.cc/2024/Conference/Submission4327/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4327/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699258954343,
            "cdate": 1699258954343,
            "tmdate": 1700651532865,
            "mdate": 1700651532865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P0kPmreymr",
                "forum": "RIEW6M9YoV",
                "replyto": "CJ5QXoo1aR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4327/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer Nzff,\n\nWe sincerely appreciate your comments and efforts in reviewing our paper. We address your question as follows. We also updated our manuscript which is highlighted in $\\color{red}{\\text{red}}$.\n\n---\n\n**Q1. Could it be possible to adopt the proposed graph generation algorithm to create graphs with specific topological properties (e.g., node degree or clustering coefficient)?**\n\nThank you for the suggestion. Yes, one can train a conditional generative model (parameterized by a Transformer) on our representation with the specific topological properties as conditions. One could even extend our framework to controllable generation [1], which would be an interesting avenue for our future research.\n\n**References**\n\n[1] Keskar, N. S., et al., CTRL: A conditional transformer language model for controllable generation. arXiv preprint 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4327/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700032298355,
                "cdate": 1700032298355,
                "tmdate": 1700032298355,
                "mdate": 1700032298355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2aJJAsZ8zP",
                "forum": "RIEW6M9YoV",
                "replyto": "P0kPmreymr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4327/Reviewer_Nzff"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4327/Reviewer_Nzff"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nthank you for your rebuttal and thank you for answering to my question/suggestion."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4327/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484801700,
                "cdate": 1700484801700,
                "tmdate": 1700484801700,
                "mdate": 1700484801700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]