[
    {
        "title": "Learning Sequence Attractors in Recurrent Networks with Hidden Neurons"
    },
    {
        "review": {
            "id": "tiqn86bUta",
            "forum": "biNhA3jbHc",
            "replyto": "biNhA3jbHc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3994/Reviewer_qcYh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3994/Reviewer_qcYh"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an algorithm to let a recurrent neural network learn sequences of patterns with convergence guarantees for the algorithm.\nFurthermore, the importance of hidden neurons in the proposed architecture and activation function is shown for producing some of the sequences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The effectiveness of the approach is demonstrated against two datasets, highlighting its applicability. The theorems and claims are sound.\nFurthermore, the paper is well written and the contributions are clearly explained with possible implications of the work for neuroscience. These implications for neuroscience might provide a new perspective on the contribution of some neurons to neural computation."
                },
                "weaknesses": {
                    "value": "\\paragraph{Comparison of encoding efficiency to other methods/experiments}\nThe authors don't make considerable efforts to compare their work to previous ones relating to sequence encoding.\nThere are many works on sequence learning, see for example (Lipton, 2015). Your work is a particular subclass of networks that can perform sequence-to-sequence processing (namely where you have a sequence with an single input and the start and zero inputs afterwards). Make comparison with other methods to measure the performance of your framework in terms of robustness to noise, memory capacity (number of recallable pattern sequences), etc.\n\n\\paragraph{Other experiments to demonstrate the effectiveness of the method}\nThe the demonstration contribution of this framework could benefit from some additional experiments.\nExperiments for effect of noise level of the first patter on the performance.\nFurther, the paper doesn't address how inputs to the network influence the output or performance. Experiments that track performance of the networks as a function of injected noise during sequence generation would better demonstrate the usefulness of this framework for neuroscience.\n\n\n\\paragraph{Figures}\nOverall, the information in the figures is very low.\nFor Figure 4 and 11 for example it is unclear what is to be gained from seeing the convergence of the algorithm, if it is not compared to other algorithms to see which converges in less epochs for example.\n\nFor Figure 7 and 8 it would be beneficial to see how increasing $T$ influences retrieval success as $M$ and $N$ are changed.\n\n\\paragraph{Some remarks on concepts and notation}\nThe section on the robustness hyperparameter is unclear. Better describe how the margin of the margin perceptron is related to the robustness here.\n\nRelating to the derivations:Why does such a $U^\\star$ exist?\nAfter Equation (19): \"due to (1)\". Should this be \"due to (17)\"?\n\nThere are a couple of unintroduced variables/notation.\nIn Eq. (33) $q$ is appearing without introduction. Should this be $p$? Clarify step (32) to (33) in the derivation.\nIn Eq. (39) $\\Omega$ has not been introduced. Did you mean $O$?\n\n\n\\paragraph{Neuroscience implications}\nThe justification of the relevance of this work to V1 neurons is insufficient.\nThe authors should justify why this particular activation function is a good model for V1 neural activity. Furthermore, the claim about unexplained V1 neural activity relies on the limit of the Heaviside activation function, for other activation functions hidden neurons would not be necessary.\nBut more fundamentally, the authors seem to understand V1 neural activity dynamics in a very different way than the general con census. Although there is sufficient recurrence in V1, the main function of these neurons is not sequence generation. Hippocampal circuits (that are mentioned in the beginning of the paper) would be a better justification.\nFinally, for V1 neurons in particular, but for all subnetworks in the brain really, inputs are a very important contribution to the dynamics.\n\n\nLipton, Z.C., Berkowitz, J. and Elkan, C., 2015. A critical review of recurrent neural networks for sequence learning. arXiv preprint arXiv:1506.00019."
                },
                "questions": {
                    "value": "What is the expressivity of the Heaviside function for a fixed $M$ and $N$ in terms of $T$?\n\nOn page 8, just above Figure 4, when is $M$ large enough? When is the solution the pseudo-inverse and when the transpose of $\\mathbf{P}$? How can high-dimensional probability theory explain this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Reviewer_qcYh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3994/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698590821137,
            "cdate": 1698590821137,
            "tmdate": 1699636361367,
            "mdate": 1699636361367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PDt91e6wxM",
                "forum": "biNhA3jbHc",
                "replyto": "tiqn86bUta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the dedicated proofreading and helpful suggestions on improving our paper. Below, we address each point of yours.\n\n* **Comparison of encoding efficiency to other methods/experiments**  \n    In the revised paper, we have compared our approach with networks of continuous neurons trained with backpropagation and modern Hopfield networks. Please see Appendix A and B in the revised paper.\n\n* **Experiments for effect of noise level of the first patter on the performance.**  \n    We observed a trade-off between the noise level that can be tolerated and the length of the stored sequence in our experiments. Additionally, we provide the code of our experiments in the supplementary material for further exploration.\n    \n* **Further, the paper doesn't address how inputs to the network influence the output or performance.**  \n    We agree that the inputs are important. However, in this paper, we focus on spontaneous neural activities and sequence memory for which the inputs are absent. The exploration of inputs to the network is beyond the scope of this paper.\n\n* **For Figure 4 and 11 for example it is unclear what is to be gained from seeing the convergence of the algorithm, if it is not compared to other algorithms to see which converges in less epochs for example.**  \n    Figure 4 is to demonstrate that why the algorithm works despite that $\\mathbf{P}$ is a random matrix and fixed during learning. Figure 11 is to demonstrate that both errors $\\mu_i(t)$ and $\\nu_j(t)$ would jointly reduce to zero, rather than conflicting with each other. Additionally, we provide comparison experiments in Figure 12 in Appendix A.\n\n* **For Figure 7 and 8 it would be beneficial to see how increasing  influences retrieval success as and are changed.**  \n    Due to space limitation, we cannot provide all possible experiments suggested by the reviewer in the paper, as the number of combinations of the hyperparameters is enormous. Nevertheless, we provide the code of our experiments in the supplementary material for further exploration.\n\n* **Better describe how the margin of the margin perceptron is related to the robustness here.**  \n    In margin perceptron, the margin is disproportional to the magntitude of the weights, which is related to robustness, as can be been in eq (12) in Proposition 4. We improved the presentation in the revised paper.\n    \n* **Why does such a** $\\mathbf{U}^*$ **exist?**  \n    The assumption that such a $\\mathbf{U}^*$ exists is a condition that the algorithm converges in finite steps. This condition is also in convergence proof of the perceptron learning algorithm.\n\n* **After Equation (19): \"due to (1)\". Should this be \"due to (17)\"?**  \n    We corrected this typo in the revised paper.\n\n* **In Eq. (33) $q$ is appearing without introduction**  \n    $q$ is an index which ranges from $1$ to $p$. It should be clear from the context.\n\n* **Clarify step (32) to (33) in the derivation.**  \n    We improved our writing of the derivation in the revised paper.\n\n* **In Eq. (39) $\\Omega$ has not been introduced.**  \n    $\\Omega$ (Big-Omega) is a standard notation in for analyzing complexity of algorithms. It is defined as the asymptotic lower bounds.\n\n* **The justification of the relevance of this work to V1 neurons is insufficient.**  \n    The reviewer made good points on our speculation on V1. After reconsideration, we found our speculation on V1 is too premature and therefore removed it in the revised paper.\n\n* **What is the expressivity of the Heaviside function for a fixed $M$ and $N$ in terms of $T$?**  \n    In the revised paper, we derived the lower bound of the capacity of our network model. The maximal length of sequences that can be stored is at least $M$ (number of hidden neurons).  Please see the updated Proposition 1 in the revised paper.\n\n* **On page 8, just above Figure 4, when is $M$\n large enough?**  \n Empirically, we found when $M$ is slightly larger than the sequence length $T$, the network can learn to store the sequence. Interestingly, we also found that larger $M$ leads to faster convergence in learning.\n\n* **When is the solution the pseudo-inverse and when the transpose of $P$?**  \n Both works fine when $M$ is large enough as shown in Figure 4.\n\n* **How can high-dimensional probability theory explain this?**  \nThe phenomenon that $\\mathbf{x} = \\text{sign}(\\mathbf{V} \\text{sign}(\\mathbf{Px}))$ for $\\mathbf{V}=\\mathbf{P}^\\top$ or $\\mathbf{V}=\\mathbf{P}^+$ only happens when $M$ is sufficiently large. We cannot prove it at the moment but offer our intuition and conjecture for the interested reader to explore further."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281496448,
                "cdate": 1700281496448,
                "tmdate": 1700538188412,
                "mdate": 1700538188412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KVuZAN4y0c",
                "forum": "biNhA3jbHc",
                "replyto": "PDt91e6wxM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_qcYh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_qcYh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I appreciate the additional details and the new version is taking steps in the right direction.\n\n> Comparison of encoding efficiency to other methods/experiments\n\nIt seems like the trained neural networks for the comparison are feedforward ones. Why did the authors not choose RNNs, which would also make the comparison more biologically relevant. \n\n>> Our algorithm is proven to converge and lead to sequence attractors. Such theoretical results are missing for networks of continuous neurons trained with backpropagation\n\nAs RNNs have universal approximation properties (at least for such tasks as considered in the paper), the two frameworks seem to have the same expressivity for the limit where the number of neurons go to infinity.\n\nFinally, being able to respond to incoming stimuli is an important aspect of biological networks, and it is possible to account for with RNNs, which is missing for the proposed framework.\n\n> Further, the paper doesn't address how inputs to the network influence the output or performance.\n\nFor the neuroscientific relevance having experiments that measure sequence memory ability in the presence of continuously incoming stimuli or noise would have been essential.\n\n> Proposition 1\n\nThis seems like an inefficient way to encode sequences by using almost the number of neurons for the sequence length. Also, this is considering a single sequence, which limits the applicability of this result.\n\n\n> The justification of the relevance of this work to V1 neurons is insufficient.\n\nWithout this neuroscientific justification, the proposed framework is missing a clear contribution for neuroscience."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674713653,
                "cdate": 1700674713653,
                "tmdate": 1700674713653,
                "mdate": 1700674713653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KIm7lUP96a",
                "forum": "biNhA3jbHc",
                "replyto": "tiqn86bUta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It seems like the trained neural networks for the comparison are feedforward ones. Why did the authors not choose RNNs, which would also make the comparison more biologically relevant.\n\nWithout inputs, RNN is equivalent to a feedforward network when it is unfolded in time. Please see Figure 1 and 3. \n\n> Finally, being able to respond to incoming stimuli is an important aspect of biological networks, and it is possible to account for with RNNs, which is missing for the proposed framework.\n\nWe focus on spontaneous neural activities and sequence memory for which the inputs are absent. Spontaneous or internally generated activities have been observed in hippocampus [Pastalkova, et al. 2008]. Our comparison work does not consider inputs either [Amari 1972, Bressloff and Taylor 1992, Chaudhry et al., 2023].\n\n> This seems like an inefficient way to encode sequences by using almost the number of neurons for the sequence length. \n\nEncoding sequences by using almost the number of neurons for the sequence length also appears in our comparison work [Chaudhry et al., 2023]. In the future, when considering multi-layer hidden neurons, we might be able to derive more efficient capacity bound.\n\n> Also, this is considering a single sequence, which limits the applicability of this result.\n\nOne can concatenate multiple sequences into a single sequence as in our comparison work [Chaudhry et al., 2023].\n\n> Without this neuroscientific justification, the proposed framework is missing a clear contribution for neuroscience.\n\nAs discussed in the Abstract and Introduction, it remains largely unclear how the brain learns to store and retrieve sequence memories. Our work provides a possible biologically plausible mechanism in elucidating sequence memory in the brain. We have added this in the revised paper. Thanks for the suggestion.\n\n\nReferences  \n\nInternally generated cell assembly sequences in the rat hippocampus. Pastalkova, et al. Science, 2008.\n\nLearning patterns and pattern sequences by self-organizing nets of threshold elements. Amari. IEEE Transactions on Computers, 1972.\n\nPerceptron-like learning in time-summating neural networks. Bressloff and Taylor. Journal of Physics A: Mathematical and General, 1992.\n\nLong sequence hopfield memory. Chaudhry et al, NeurIPS, 2023."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709022105,
                "cdate": 1700709022105,
                "tmdate": 1700730148272,
                "mdate": 1700730148272,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VnpBEFcdpx",
                "forum": "biNhA3jbHc",
                "replyto": "tiqn86bUta",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "title": {
                        "value": "External inputs"
                    },
                    "comment": {
                        "value": "We agree with the reviewer that external inputs are crucial in neural information processing. However, it is beyond the scope of this paper. We have acknowledged this limitation in the Conclusion and Discussion section of the revised paper and plan to explore this important topic in the future work."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730420878,
                "cdate": 1700730420878,
                "tmdate": 1700730618045,
                "mdate": 1700730618045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sUfMV2uI9N",
            "forum": "biNhA3jbHc",
            "replyto": "biNhA3jbHc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3994/Reviewer_TBVF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3994/Reviewer_TBVF"
            ],
            "content": {
                "summary": {
                    "value": "Attractor networks can be considered all-to-all connected feedforward networks without any hidden layer. These \u2018visible neurons\u2019 must do both the computational and representational work simultaneously. This limits the expressibility of such networks, particularly for sequence attractors. This submission shows one way around this limitation is to add hidden neurons with a dedicated computational role and no (direct) representational role. A learning rule is introduced to learn the necessary parameters for these hidden neurons in the case of artificial and naturalistic data, with good recall shown."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Integrates some nice ideas to solve an identified problem."
                },
                "weaknesses": {
                    "value": "*Novelty*\n\nI question how or in what way this is new or different to past work (see question 2 below), and for that reason am concerned about novelty. However, my concern might be misplaced and I would appreciate the authors clarifying this point.\n\n*Imprecise or incorrect statement*\n\nFrom page 1: \u201cthe Hopfield model and related works typically only consider static attractors\u201d\n\nNo, there exist many works looking at non-static attractors. Some of these are cited at the end of the first sentence in section 2 on page 2. A few additional examples include:\n\nH. Gutfreund and M. Mezard. Processing of temporal sequences in neural networks. Phys. Rev. Lett., 61:235\u2013238 1998.\n\nArjun Karuvally, Terrence Sejnowski, and Hava T Siegelmann. General sequential episodic memory model, ICML 2023.\n\nHamza Chaudhry, Jacob Zavatone-Veth, Dmitry Krotov, Cengiz Pehlevan, Long Sequence Hopfield Memory, NeurIPS 2023.\n\n*Lack of comparisons and practical applications*\n\nPrevious work on encoding sequences in attractor networks have taken many different approaches. Additionally, there exist many methods for learning sequences in the machine learning literature. There is a lack of comparison with these prior methods."
                },
                "questions": {
                    "value": "1. Where is the evidence that the examples shown in Figure 2 cannot be generated by a network without hidden neurons?\n\n2. How is your approach conceptually different to hierarchical attractor network architectures? E.g.,\n\nDmitry Krotov, Hierarchical Associative Memory, arXiv:2107.06446\n\nKunihiko Fukushima, A hierarchical neural network model for associative memory, Biological Cybernetics volume 50, pages105\u2013113 (1984)\n\n3. What is the memory capacity of this network? How does this depend on memory load?\n\n4. In the second paragraph of the conclusion, there is some speculation that neurons in V1 with unknown function may be akin to the hidden neurons of this model. How would a neuroscientist test for this? How should the tuning properties be studied, i.e., what should be measured? Does there exist some structure(s) in the hidden neuron activity data from your own model which you would expect in the aforementioned V1 neurons?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Reviewer_TBVF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3994/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779747637,
            "cdate": 1698779747637,
            "tmdate": 1700759557365,
            "mdate": 1700759557365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T8mc0Y8Up3",
                "forum": "biNhA3jbHc",
                "replyto": "sUfMV2uI9N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback and helpful suggestions on improving our paper. Below, we address each point of yours.\n\n* **No, there exist many works looking at non-static attractors.**  \n    What we meant is most previous works on attractor networks of Hopfield type considered only static attractors. Although there are also some other works on sequence attractors as the reviewer pointed out and we cited, the amount of them is smaller compared to those on static attractors. We have clarified this in the revised paper.\n\n* **There is a lack of comparison with these prior methods.**  \nIn the revised paper, we have compared our approach with networks of continuous neurons trained with backpropagation and modern Hopfield networks. Please see Appendix A and B in the revised paper.\n\n* **Where is the evidence that the examples shown in Figure 2 cannot be generated by a network without hidden neurons?**  \n\"In Figure 2, we show additional examples of\nsequences that cannot be generated by the network. The sequences are synthetically constructed. We\nthen test if the perceptron learning algorithm can learn the sequences. Since the algorithm converges\nif the linear separability condition is met [Minsky \\& Papert, 1969], the divergence of the algorithm\nimplies that the sequences cannot be generated by the networks.\" This is stated in the text above Figure 2.\n\n* **How is your approach conceptually different to hierarchical attractor network architectures?**  \nWe do not claim our approach is conceptually different to hierarchical attractor network architectures such as [Chaudhry et al, NeurIPS 2023] for sequence learning. However, our model and algorithm are more biologically plausible, as stated in Appendix B in the revised paper.\n\n* **What is the memory capacity of this network? How does this depend on memory load?**  \nIn the revised paper, we derived the lower bound of the capacity of our network model. The maximal length of sequences that can be stored is at least $M$ (number of hidden neurons).  Please see the updated Proposition 1 in the revised paper.\n\n* **In the second paragraph of the conclusion, there is some speculation that neurons in V1 with unknown function may be akin to the hidden neurons of this model.**  \nThe reviewer raised good questions for us to reflect on our speculation. After reconsideration, we found our speculation on V1 is too premature and therefore removed it in the revised paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279944738,
                "cdate": 1700279944738,
                "tmdate": 1700283522398,
                "mdate": 1700283522398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rtY1xJm8m8",
            "forum": "biNhA3jbHc",
            "replyto": "biNhA3jbHc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3994/Reviewer_9sz4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3994/Reviewer_9sz4"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors extend Hopfield networks by adding one hidden layer and together with their proposed learning rules, show that this architecture can store and retrieve binary sequences. They provide convergence guarantees and empirically demonstrate, on various toy and real world examples, that their network learns to store sequences and retrieve them even in the presence of (a particular type of) noise."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper provides a relatively simple extension to Hopfield networks and corresponding learning rules to store and retrieve sequences that works well, which is novel to my knowledge. The problem of being able to store and retrieve sequences is an important one, both for understanding biological brains and for machine learning. Therefore having such a method is very useful and significant for the community.\n\nThe paper provides convergence proofs and empirical evaluation on a variety of targeted and relevant tasks, which also makes the paper a high quality contribution to the community. I particularly found the specific examples provided to demonstrate the problems with storing sequences in a fully visible recurrent network very useful to understand the motivation behind their approach."
                },
                "weaknesses": {
                    "value": "A discussion of the capacity of the proposed architecture is missing, and is pretty important to be able to meaningfully connect this approach with biology and apply it in machine learning. This, in my opinion, is the biggest weakness of the paper.\n\nThe experiments section also does not have sufficient details about hyper-parameters ($\n\\eta, \\tau$). The language and clarity of the exposition in the paper could be improved significantly.  See examples in \"Questions\"."
                },
                "questions": {
                    "value": "## Questions related to points mentioned above:\n- How many times is each sequence presented to the model?\n- What's the capacity of the model? How many sequences can it store?\n\n## Clarity issues in the paper:\n- The sign function and the Heaviside function seem to be identical the way it's been defined in the paper.\n- The first sentence of Related work just lists a bunch of papers, which seems redundant, since these papers are explained later anyway.\n\n### Minor:\nWould have been useful to have Fig. 2 and Fig. 5 side by side for comparison. Merge the two figures perhaps?\nBar plots in Fig. 7 and 8 are hard to read. Having concrete values for each bar, and mentioning the specific values of $M$ and $T$ used would be very useful.\n\n### Grammar/Language:\n\n**Abstract**\n- \"The brain is targeted...\" is not well formed.\n- \"We demonstrate our model...\" -> \"We demonstrate that our model...\"\n\n**Sec 1:**\n- \"as we experience the 'mental time travel'\" -> \"as we experience 'mental time travel'\"\n- \"By a sequence attractor, it means\"\n- \"The algorithm is proved to converge\" -> \"The algorithm is proven to converge\"\n- ...\n\nThere are many more. I would suggest passing the text through a grammar check."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3994/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698861423693,
            "cdate": 1698861423693,
            "tmdate": 1699636361181,
            "mdate": 1699636361181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JJz2kU5vIg",
                "forum": "biNhA3jbHc",
                "replyto": "rtY1xJm8m8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the encouragement and helpful suggestions on improving our paper. Below, we address each point of yours.\n\n\n* **A discussion of the capacity of the proposed architecture is missing**  \nIn the revised paper, we derived the lower bound of the capacity of our network model. The maximal length of sequences that can be stored is at least $M$ (number of hidden neurons).  Please see the updated Proposition 1 in the revised paper.\n\n * **The experiments section also does not have sufficient details about hyper-parameters**  \nWe have tested different hyper-parameters and found the results are consistent. Meanwhile, we provide the code in the supplementary material for reproduction of our experiments.\n\n* **How many times is each sequence presented to the model?**  \n    In training, we ran our algorithm for maximal 500 epochs. Each epoch correspond to presenting the sequences to the model once. Empirically, we found the algorithm usually converges within 100 epochs.\n\n* **What's the capacity of the model? How many sequences can it store?**  \n    The maximal length of an arbitrary sequence that can be stored in the model is at least $M$ (number of hidden neurons). Please see the updated Proposition 1 in the revised paper. The model can store as many sequences as possible as long as the total length of the sequences is within the model capacity.\n\n* **The sign function and the Heaviside function seem to be identical the way it's been defined in the paper.**  \n    Both functions are needed. The sign function is to simplify the math such that one does not have to explicitly convert the values of network states from $\\{0,1\\}$ to $\\{-1,1\\}$, as in the classical Hopfield networks. The Heaviside function is needed as $\\mu_i(t)$ and $\\nu_j(t)$ are errors which need to be non-negative.\n\n* **The first sentence of Related work just lists a bunch of papers, which seems redundant, since these papers are explained later anyway.**  \n    We modified the writing as you suggested in the revised paper.\n\n* **Would have been useful to have Fig. 2 and Fig. 5 side by side for comparison. Merge the two figures perhaps?**  \n    We considered this suggestion but found merging two figures would break the logic line of our presentation for the first-time readers.\n\n* **Bar plots in Fig. 7 and 8 are hard to read. Having concrete values for each bar, and mentioning the specific values of and used would be very useful.**  \n    We considered this suggestion but found adding specific values in the figure would make the figures visually messy.\n\n\n* **I would suggest passing the text through a grammar check.**  \n    We thank the reviewer for the writing suggestions and incorporated them in the revised paper. We will polish our paper further by a large language model later."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279462383,
                "cdate": 1700279462383,
                "tmdate": 1700279462383,
                "mdate": 1700279462383,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rCcKjjkciw",
                "forum": "biNhA3jbHc",
                "replyto": "JJz2kU5vIg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_9sz4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_9sz4"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses and clarifications. Proposition 1 further strengthens the paper.\n\nReg. Fig. 7 and 8, I would encourage the authors to at least have a table in the supplement with all the values so that the readers are not left to guess the specific values based on the bar plots."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476700952,
                "cdate": 1700476700952,
                "tmdate": 1700476700952,
                "mdate": 1700476700952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xdVCrLyLBU",
            "forum": "biNhA3jbHc",
            "replyto": "biNhA3jbHc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new binary RNN in which there is a hidden layer in between RNN transitions. They develop a learning rule for training the two weight matrices in the transition. They then train these RNNs on some simple sequences and show they can be learned."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is clearly presented, and the work is original to my knowledge. My main concern is its significance (see below)."
                },
                "weaknesses": {
                    "value": "You can always add more neurons to solve these problems with a standard RNN. See Siegelmann and Sontag (1992). You\u2019d need to actually show that the other RNNs can\u2019t actually learn these sequences, but it\u2019s clear that they could\u2026(maybe not a size matched RNN, but a larger one could\u2026)\n\nYou\u2019re training a new network for each sequence? This is pretty extreme, and there are lots of much simpler ways of solving that problem (e.g. a HMM). \n\nIn sum, it\u2019s not really clear that the proposed method actually solves a real problem. You\u2019d need to show it by comparing performance to other RNN architecture (as well as non sized matched networks). You\u2019d also need to test against networks that don\u2019t just have binary outputs. I understand that you would like to relate it to the brain, however the brain communicates in spike rates, not just a single spike\u2026"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3994/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699246266449,
            "cdate": 1699246266449,
            "tmdate": 1699636361099,
            "mdate": 1699636361099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3NADai9Krw",
                "forum": "biNhA3jbHc",
                "replyto": "xdVCrLyLBU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission3994/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback. However, the reviewer seems to misunderstand our paper in several aspects.\n\n1. Fundamentally, we are not trying to solve a \"real problem\" in the sense of engineering applications, but to answer a biological question: how does the brain learn to store and retrieve sequence memory? Storing the sequences without biologically plausible mechanisms is not the interest of our paper.\n\n2. Given a sequence $\\mathbf{x}(1),...,\\mathbf{x}(T)$, our goal is not simply storing it as it is, but storing it **as an attractor** such that given a perturbed state $\\hat{\\mathbf{x}}(t)$, the network can recover the rest of the stored sequence robustly. Please refer to Figure 9 and 10 and the supplementary material for such a demonstration.\n\n3. While the standard RNN in deep learning literature can certainly store the sequences, it is trained with backpropagation, whose biological plausibility is in question. While in our work, the algorithm has a clear biological interpretation.\n\n4. Our focus is on theoretical analysis. We proved how the algorithm can converge and give rise to sequence **attractors**. Such results are missing for the standard RNN trained with backpropagation.\n\nBelow, we address several of your points.\n\n> You\u2019re training a new network for each sequence?\n\nNo. For the Moving MNIST experiment, we use a single network for storing 20 sequences.\n\n> the brain communicates in spike rates, not just a single spike\u2026\n\nIt is questionable that the brain communicates only in spike rates. There is a classic debate between rate coding and temporal coding. As to single spikes, please refer to the following paper for their role in neural coding:\n\nThe Role of Spike Timing in the Coding of Stimulus Location in Rat Somatosensory Cortex (Panzeri et al., Neuron 2001)"
                    },
                    "title": {
                        "value": "The reviewer misunderstood the paper"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699679505925,
                "cdate": 1699679505925,
                "tmdate": 1699868157958,
                "mdate": 1699868157958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LX6iGlMu8o",
                "forum": "biNhA3jbHc",
                "replyto": "xdVCrLyLBU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission3994/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparison experiments on networks of continuous neurons"
                    },
                    "comment": {
                        "value": "We conducted experiments on networks of continuous neurons trained with backpropagation, in response to the reviewer's concern. The results are in Appendix A of the revised paper. Our algorithm converges to zero training error much faster empirically."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282830104,
                "cdate": 1700282830104,
                "tmdate": 1700283248649,
                "mdate": 1700283248649,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0II1bOWD8U",
                "forum": "biNhA3jbHc",
                "replyto": "LX6iGlMu8o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission3994/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your responses."
                    },
                    "comment": {
                        "value": "Thanks for your responses. I still don't understand how you manage to do multiple sequence when there may be the same sequence element across sequences? Or are sequence elements not repeated? This was the crux of my complaint, you are mapping x(t) -> x(t+1) so it's not possiible to build a latent representations that can deal with aliased sequence elements. Otherwise you're basiacally just mapping x(t) -> x(t+1) with a hidden layer inbetween. Perhaps I'm wrong, in which case I'd be happy to raise my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700299637,
                "cdate": 1700700299637,
                "tmdate": 1700700299637,
                "mdate": 1700700299637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ijqct18ELW",
                "forum": "biNhA3jbHc",
                "replyto": "xdVCrLyLBU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission3994/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Our current work cannot handle multiple sequences when there may be the same sequence element across sequences. The related work in comparison cannot handle such case either. For example,\n\nLearning patterns and pattern sequences by self-organizing nets of threshold\nelements. Amari. IEEE Transactions on Computers, 1972.\n\nPerceptron-like learning in time-summating neural networks. Bressloff and Taylor. \nJournal of Physics A: Mathematical and General, 1992.\n\nLong sequence hopfield memory. Chaudhry et al. NeurIPS, 2023.\n\nThis can be handled by considering time delay which maps $\\mathbf{x}(t), \\mathbf{x}(t-1),...,\\mathbf{x}(t-\\tau)$ to $\\mathbf{x}(t+1)$ or introducing stochastic neurons and is left to the future work."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707924448,
                "cdate": 1700707924448,
                "tmdate": 1700712563989,
                "mdate": 1700712563989,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D23UbqviIu",
                "forum": "biNhA3jbHc",
                "replyto": "Ijqct18ELW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission3994/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
                ],
                "content": {
                    "comment": {
                        "value": "OK now I get it. It's not about RNNs at all. It's a way of increasing the capacity of stored paired associations in a Hopfield-like networks, and just that the paired associations come in a sequence. You shouldn't really call it RNNs as RNNs were developed precisely for dealing with latent states (i.e. not confusing two identical sequence elements from two different sequences). I'm happy to incease my score provided the authors make it clear that it's not a RNN, i.e. by removing 'recurrent networks' from the title, and making clear in the main text that the setup is rapid binding of paired associations and not anyhting like a RNN."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709439289,
                "cdate": 1700709439289,
                "tmdate": 1700709439289,
                "mdate": 1700709439289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x2E2JgDI9S",
                "forum": "biNhA3jbHc",
                "replyto": "xdVCrLyLBU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission3994/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We use the term \"recurrent\" to refer to a neuron's output is feedback to itself in a biological sense. As it causes confusion, we would like to consider changing the title to, for example,\n\nLearning Sequence Attractors in Associative Networks with Hidden Neurons\n\nThank you so much for the suggestion!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710765944,
                "cdate": 1700710765944,
                "tmdate": 1700712617063,
                "mdate": 1700712617063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x3AM1qck6F",
                "forum": "biNhA3jbHc",
                "replyto": "x2E2JgDI9S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission3994/Authors",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3994/Reviewer_ZADc"
                ],
                "content": {
                    "comment": {
                        "value": "Yes that would be better - thanks!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3994/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713833726,
                "cdate": 1700713833726,
                "tmdate": 1700713833726,
                "mdate": 1700713833726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]