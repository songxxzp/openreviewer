[
    {
        "title": "Wording Image for Domain-Invariant Representation in Domain Generalization"
    },
    {
        "review": {
            "id": "cDzm0fvvHX",
            "forum": "WJfbQulYPz",
            "replyto": "WJfbQulYPz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6269/Reviewer_wwhy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6269/Reviewer_wwhy"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies domain generalization, where the key challenge is to learn domain-invariant visual representations for each category. The authors argue that language embeddings of a particular category are naturally domain-invariant. Moreover, the difference between the pseudo-language embedding (prompted with the input image) and the original language embedding (prompted with the class description) represents the domain-specific counterpart. To this end, the authors propose WIDIn to learn domain-invariant visual representations using language embeddings. Empirical evaluation under various domain generalization benchmarks demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow.\n2. The motivation is intuitive and seems reasonable.\n3. The figures vividly illustrate the pipeline of the proposed method, especially the left part of Figure 1 and Figure 2b.\n4. The improvements over baselines are relatively significant.\n5. The proposed method is also capable of long-tailed image classification."
                },
                "weaknesses": {
                    "value": "1. **Lack of evidence to support the core insight.**  As illustrated in Figure 2b, $t_x - t_c$ is parallel to $x - x_e$, but in Figure 3, no such phenomenon can be observed. It is encouraged to draw a parallelogram composed of $x, x_e, t_x, t_c$ for each category both on the source domain and the target domain.\n\n2. **Missing ablations.** There are three objectives in total, including $L_{ia}$, $L_{ca}$, and $L_{feat}$. It is better to study the effectiveness of each component step by step. \n\n3. **The underlying motivation of $L_{ca}$.** Specifically, $L_{ca}$ aims to minimize the distance between the domain-specific text embedding and the domain-invariant text embedding. However, as the optimization goes on, the difference between these two embeddings becomes small, which is used to measure the domain-specific parts. However, domain-specific parts always exist and *never* go small. Therefore, minimizing $L_{ca}$ becomes strange to me."
                },
                "questions": {
                    "value": "I do not have further questions, please refer to the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6269/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6269/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6269/Reviewer_wwhy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6269/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698131905283,
            "cdate": 1698131905283,
            "tmdate": 1699636686412,
            "mdate": 1699636686412,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "S8bHnUWHK9",
            "forum": "WJfbQulYPz",
            "replyto": "WJfbQulYPz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6269/Reviewer_m3y6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6269/Reviewer_m3y6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method called WIDIn, which connects visual and language information to create domain-invariant features. This is achieved by representing images as word tokens and using the difference between the image's and its class description's language embeddings to promote domain-invariant representation learning. Experimental results show the effectiveness of WIDIn on benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method is novel. With the added F_p, F_D, and F_C, it add learnable parameters to LADS and thus lead to better performance.\n2. The experimental results show the power of proposed method.\n3. Ablation study with different prompts, contrastive learning w/ w/ot labels, and training/freezing language model is interesting.\n4. The extension experiments on long tail case also better support the power of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Is the image encoder trainable? If so, it might be unfair to compare with Linear Clf. and MLP Clf.\n2. It would be great if the author could compare to other more general methods in domain generalization, where only the training domain is available and the domain descriptions of testing domains remain unknown.\n3. It would be great if the proposed method could be evaluated on bigger benchmarks (DomainNet and Office-Home) with more domains as the proposed methods do not require any access to the target domain.\n4. The presentation of this paper is a little bit awkward. For example, the training details, such as loss for each step, are not presented in the main paper."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6269/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785620816,
            "cdate": 1698785620816,
            "tmdate": 1699636686266,
            "mdate": 1699636686266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "9MytkCInUK",
            "forum": "WJfbQulYPz",
            "replyto": "WJfbQulYPz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6269/Reviewer_z1uR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6269/Reviewer_z1uR"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose to project images into language space by representing each image as a word token, which is attached with hand-crafted prompt and fed into language encoder, where the difference between the extracted embedding and the language embedding of its class description is used to estimate the domain-specific counterpart, which facilitates the domain-invariant representation learning. Experiments demonstrate the effectiveness of this approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Reported results outperform baselines: experimental studies on two domain generalization benchmark datasets and two long-tail benchmark datasets demonstrate the effectiveness of this approach.\n2. The presentation is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "1.  The novelty is limited. Representing an image as a word token is very common in many recent multimodal models (e.g., BLIP, LLava, CM3, RA-CM3, CM3Leon, etc.). The overall idea and pipeline is still similar to LADS. The difference is that, this paper learns from domain-invariant representations while LADS learns from domain-specific representations, and the way a domain-specific or domain-invariant is obtained is similar.\n\ntechnical details:\n\n2. From my understanding, $t_x$ is the \"unified\" representation of domain-invariant and domain-specific features. In this case, I wonder why you use a text encoder to encode \"an image of [V]\" to represent $t_x$? Given that you claim \"an image\" represents an invariant domain, it would involve some biases (towards invariant domain) this way. Why not directly encode the [V] token alone? \n\n3. I am concerned about using \"image\" to represent the domian-invariant space. Have you tried using other words to replace \"image\"? For example, using \"painting\" instead: projecting everything into the painting-domain. I doubt \"an image of {...}\" works due to the authors' claim that, \"image\" corresponds to an invariant space. I guess other words could have the similar results.\n\n4. Code is not provided."
                },
                "questions": {
                    "value": "please see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6269/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806504717,
            "cdate": 1698806504717,
            "tmdate": 1699636686134,
            "mdate": 1699636686134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]