[
    {
        "title": "Rethinking Decision Transformer via Hierarchical Reinforcement Learning"
    },
    {
        "review": {
            "id": "GmhsYZKE19",
            "forum": "7v3tkQmtpE",
            "replyto": "7v3tkQmtpE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7223/Reviewer_UBLF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7223/Reviewer_UBLF"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces the Autotuned Decision Transformer (ADT), a novel approach that employs a hierarchical structure, substituting the traditional returns-to-go (RTG) with prompts derived from a high-level policy. The paper presents two specific variants of this innovative prompting mechanism: V-ADT, which utilizes prompts designed to optimize learned value functions; G-ADT, where the prompts provide subgoals, strategically directing the policy toward the ultimate objective. Proposed methods demonstrates superior performance compared to conventional DT-based techniques and hierarchical methods on the standard D4RL benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Strength 1: Effective Approach to a Critical Issue**\n\nThis paper addresses a crucial issue in the realm of Decision Transformers (DT), particularly the challenge associated with handling the returns-to-go (RTG) and integrating value functions during the DT training phase. There is widespread agreement on the necessity of this challenge within the field. The proposed solution, which involves learning a prompt to feed into the policy of the DT, is not only a plausible approach but also one that has been empirically substantiated, showing enhanced effectiveness over existing DT-based methodologies. This validation underscores the method's potential impact and applicability within the discipline.\n\n**Strength 2: Noteworthy Innovation in Synthesis**\n\nWhile the individual components utilized in the proposed method might not be pioneering in isolation\u2014such as the employment of in-sample optimal values (akin to value-based approaches like IQL), the adoption of hierarchical structures for subgoals (seen in strategies like HIQL), or the application of weighted regression techniques (as in AWR)\u2014the paper's true innovation lies in the synthesis of these elements within the framework of Decision Transformers. \n\nThe authors have skillfully amalgamated these various concepts, producing a methodology that, in its entirety, presents a significant departure from conventional approaches. This fusion of ideas, culminating in a cohesive and well-articulated study, embodies substantial novelty. \n\n**Strength 3: Insightful Ablations**\n\nGiven the many components of the proposed method, it's important to empirically validate the source of the gain the proposed method may offer. In this sense, the authors made a great effort on additional experiments from page 7 to 9."
                },
                "weaknesses": {
                    "value": "**Weakness 1: Limited Empirical Superiority**\n\nThe primary data presented in Table 1 indicate that while V-ADT demonstrates a notable advancement over other DT-based methods, it fails to consistently surpass or significantly differentiate itself from value-based strategies. This observation is critical, particularly since V-ADT incorporates a value function trained via IQL during the formation of its high-level policy, suggesting that its performance should be directly contrasted with these value-based counterparts. The authors' choice to emphasize the superiority of V-ADT within the DT category, marked in bold, might inadvertently misguide readers regarding the method's comparative effectiveness. It remains unclear why V-ADT, despite leveraging the advantages of pre-trained IQL values, does not achieve a substantial performance lead over IQL itself. A more in-depth discussion on this aspect would enhance the paper's credibility and help readers understand the practical implications and potential limitations of integrating value-based components within a DT framework.\n\n**Weakness 2: Insufficiency of Information for Replicability**\n\nThe absence of shared code alongside the paper significantly hampers the research's transparency and the reproducibility of its findings. The explanation provided in Appendix A, particularly the vague descriptions in the concluding section, lacks the detailed guidance necessary for readers to independently replicate and verify the proposed method's effectiveness. Considering the empirical foundation upon which the method stands, it is imperative for the authors to release the corresponding code, ensuring that peers can thoroughly evaluate, validate, or even potentially improve upon the methodology."
                },
                "questions": {
                    "value": "Question 1) The comprehensive ablation studies provided are certainly insightful. Could the authors elaborate on their specific choice of using antmaze for these ablations (not MuJoCo), considering its characteristic sparse rewards and diverse objectives?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7223/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7223/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7223/Reviewer_UBLF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698233563658,
            "cdate": 1698233563658,
            "tmdate": 1699636859854,
            "mdate": 1699636859854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HRGlmeZG12",
                "forum": "7v3tkQmtpE",
                "replyto": "GmhsYZKE19",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UBLF"
                    },
                    "comment": {
                        "value": "### Q: Limited Empirical Superiority\n\nA:  Our ablation studies reveal some interesting findings that might be able to explain why the overall performance of ADT falls short of IQL and HIQL. **Our main conjecture is that Transformer as a function approximator is hard to optimize for standard RL algorithms.** To verify this, in Section C in the appendix of the revised paper, we first implement an *oracle algorithm*, which distill the IQL policy using a transformer with supervised learning objective (V-ADT Oracle in Figure 6). The oracle algorithm matches the performance of IQL, suggesting that the transformer architecture is not the bottleneck. We then implement another baseline, named IQL-Trans , by replacing the MLP policy with a transformer policy for IQL. As shown in Figure 3, the performance of IQL-Trans (V-ADT w/o prompt in Figure 3) cannot match the original IQL, further supporting our conjecture. The advantage of ADT over IQL-Trans is mainly contributed to the joint optimization of hierarchical policies (the high-level policy optimizes the prompt and the the low-level policy is optimized based on the prompt), since this is the key difference between these two algorithms. To help readers clearly get this information, we provide the results of V-ADT, V-ADT Oracle, IQL-Trans and IQL in the table below:\n\n|                           |     V-ADT      |   IQL-Trans    |  V-ADT Oracle  |       IQL       |\n| :------------------------ | :------------: | :------------: | :------------: | :-------------: |\n| antmaze-medium-play-v2    | 62.2 $\\pm$ 2.5 | 50.6 $\\pm$ 6.6 | 69.0 $\\pm$ 1.8 | 71.2 $\\pm$ 7.3  |\n| antmaze-medium-diverse-v2 | 52.6 $\\pm$ 1.4 | 38.6 $\\pm$ 5.4 | 64.8 $\\pm$ 6.5 | 70.0 $\\pm$ 10.9 |\n| antmaze-large-play-v2     | 16.6 $\\pm$ 2.9 | 19.4 $\\pm$ 3.6 | 50.0 $\\pm$ 1.7 | 39.6 $\\pm$ 5.8  |\n| antmaze-large-diverse-v2  | 36.4 $\\pm$ 3.6 | 5.0 $\\pm$ 5.2  | 33.4 $\\pm$ 5.3 | 47.5 $\\pm$ 9.5  |\n\nFinally, we note that there is still a performance gap between ADT and the oracle algorithm. This motivates the investigation of other techniques to improve  transformer-based decision models, which we leave as our future work.\n\n\n\n### Q: Insufficiency of Information for Replicability\n\nA: We promise that we will release our codes soon. \n\n\n\n### Q: why using antmaze for these ablations (not MuJoCo), considering its characteristic sparse rewards and diverse objectives?\n\nA: One of the most important ADT's contribution is giving transformer-based models (DT) the stitching ability, referring to the capability to integrate suboptimal trajectories from the data. The Antmaze datasets consist of suboptimal trajectories, requiring the agent use portions of existing trajectories in order to solve a task, is widely considered to be the benchmark for evaluating the stitching ability [1]. Therefore, we choose to ablate the efficacy of each component of ADT in improving the stitching ability using Antmaze datasets.\n\n### Reference\n\n[1] Fu, Justin, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. \"D4rl: Datasets for deep data-driven reinforcement learning.\" arXiv preprint arXiv:2004.07219 (2020)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699955664965,
                "cdate": 1699955664965,
                "tmdate": 1700042767081,
                "mdate": 1700042767081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6IHtt7PjTV",
                "forum": "7v3tkQmtpE",
                "replyto": "HRGlmeZG12",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7223/Reviewer_UBLF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7223/Reviewer_UBLF"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for the response, which has helped clarify several aspects of the work.\n\nI now have a better understanding of the rationale behind selecting the antmaze environment for ablations and its relevance in evaluating the stitching ability. This aspect of the research is well-justified and adds value to the study.\n\nRegarding the difficulty in optimizing the Transformer with standard offline RL algorithms, I agree with the author's response. However, it is noteworthy that even with the implementation of the V-ADT Oracle, its performance lags behind IQL in the Antmaze environment. This observation leads to a lack of compelling reasons for readers to prefer V-ADT over IQL in this specific context.\n\nConsequently, while I acknowledge the potential and innovativeness of the hierarchical transformer-based approach, these current limitations lead me to maintain my initial rating. Nevertheless, I see significant promise in the methodology. I encourage the authors to improve the approach, particularly in bridging the performance gap observed."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528892907,
                "cdate": 1700528892907,
                "tmdate": 1700528892907,
                "mdate": 1700528892907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EO6EU4aa5y",
            "forum": "7v3tkQmtpE",
            "replyto": "7v3tkQmtpE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7223/Reviewer_xoB5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7223/Reviewer_xoB5"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates transformer-based decision models through a hierarchical decision-making framework, and proposes two new transformer-based decision models for offline RL. Specifically, the high-level policy suggests a prompt, following which a low-level policy acts based on this suggestion. Empirical studies show some improvements over DT on several control and navigation benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The study of integrating decision transformers into a hierarchical setting is interesting to the HRL community, for understanding the benefits as well as limitations of the DT based approach. The two proposed models seem to be technically sound. The empirical studies seem to be comprehensive."
                },
                "weaknesses": {
                    "value": "1. Integrating decision transformers into a hierarchical setting has already been studied in an earlier paper [*], especially corresponding to the goal-conditioned version. This paper wasn\u2019t cited or discussed. \n2. The paper doesn\u2019t dive into the analysis of the reason why the proposed approach outperforms or underperforms the baseline methods. For instance, when comparing with HRL baselines, it was simply mentioning that \u201cGiven that V-ADT and G-ADT is trained following the IQL and HIQL paradigm, respectively, the achieved performance nearing or inferior to that of IQL and HIQL is anticipated\u201d - but why?  Is it due to the generated subgoals or non-stationarity issues? The reasons behind the observations are more valuable to the community. \n3. In V-ADT, does the high level provide prompt at every time step? If it does, it is arguably a hierarchical setting since there is no clear decomposition of a global goal or task. What is the reward function of the high level?\n4. It\u2019s not clear to me how the high level generates returns which are not covered in the offline dataset either? i.e., how could you avoid the issues you stated in Sec. 2\nReference:\n[*] Correia, A. and Alexandre, L.A., 2022. Hierarchical decision transformer. arXiv preprint arXiv:2209.10447."
                },
                "questions": {
                    "value": "Please address my questions in the Weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667448132,
            "cdate": 1698667448132,
            "tmdate": 1699636859735,
            "mdate": 1699636859735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ALFSzHWC2C",
                "forum": "7v3tkQmtpE",
                "replyto": "EO6EU4aa5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xoB5: Part 1"
                    },
                    "comment": {
                        "value": "### Q: Integrating decision transformers into a hierarchical setting has already been studied in an earlier paper\n\nA: Thanks for pointing this out. We have cited and discussed this paper and other DT Enhancements in Discussions section in our revised paper. ADT differs from HDT mainly from two aspects: (1) The high-level policy of ADT is more general that could output different kinds of prompts, including sub-goals, target values, latent skills and options. (2) For the low-level policy, we apply advantage weighted regression instead of conditioned prediction as in DT to enable stitching ability. \n\n### Q: why the proposed approach outperforms or underperforms the baseline methods\n\nA: Thanks for your suggestion! Our ablation studies are designed to explain the reasons behind the empirical results. But we totally agree that analysis of these results should be discussed more carefully in the paper.   \n\nOur ablation studies reveal some interesting findings that might be able to explain why the overall performance of ADT falls short of IQL and HIQL. **Our main conjecture is that Transformer as a function approximator is hard to optimize for standard RL algorithms.** To verify this, in Section C in the appendix of the revised paper, we first implement an *oracle algorithm*, which distill the IQL policy using a transformer with supervised learning objective (V-ADT Oracle in Figure 6). The oracle algorithm matches the performance of IQL, suggesting that the transformer architecture is not the bottleneck. We then implement another baseline, named IQL-Trans , by replacing the MLP policy with a transformer policy for IQL. As shown in Figure 3, the performance of IQL-Trans (V-ADT w/o prompt in Figure 3) cannot match the original IQL, further supporting our conjecture. The advantage of ADT over IQL-Trans is mainly contributed to the joint optimization of hierarchical policies (the high-level policy optimizes the prompt and the the low-level policy is optimized based on the prompt), since this is the key difference between these two algorithms. To help readers clearly get this information, we provide the results of V-ADT, V-ADT Oracle, IQL-Trans and IQL in the table below:\n\n|                           |     V-ADT      |   IQL-Trans    |  V-ADT Oracle  |       IQL       |\n| :------------------------ | :------------: | :------------: | :------------: | :-------------: |\n| antmaze-medium-play-v2    | 62.2 $\\pm$ 2.5 | 50.6 $\\pm$ 6.6 | 69.0 $\\pm$ 1.8 | 71.2 $\\pm$ 7.3  |\n| antmaze-medium-diverse-v2 | 52.6 $\\pm$ 1.4 | 38.6 $\\pm$ 5.4 | 64.8 $\\pm$ 6.5 | 70.0 $\\pm$ 10.9 |\n| antmaze-large-play-v2     | 16.6 $\\pm$ 2.9 | 19.4 $\\pm$ 3.6 | 50.0 $\\pm$ 1.7 | 39.6 $\\pm$ 5.8  |\n| antmaze-large-diverse-v2  | 36.4 $\\pm$ 3.6 | 5.0 $\\pm$ 5.2  | 33.4 $\\pm$ 5.3 | 47.5 $\\pm$ 9.5  |\n\nFinally, we note that there is still a performance gap between ADT and the oracle algorithm. This motivates the investigation of other techniques to improve  transformer-based decision models, which we leave as our future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699955596320,
                "cdate": 1699955596320,
                "tmdate": 1700042747574,
                "mdate": 1700042747574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bkj7BnK7C0",
                "forum": "7v3tkQmtpE",
                "replyto": "EO6EU4aa5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xoB5: Part 2"
                    },
                    "comment": {
                        "value": "### Q: does the high level provide prompt at every time step?\n\nA: Yes, we describe this in Figure 1. \n\n\n\n### Q: V-ADT is arguably a hierarchical setting since there is no clear decomposition of a global goal or task\n\nA: The term \"hierarchical\" is mainly referred to the policy decomposition $\\pi(a|s) = \\int_{\\rho\\in \\mathcal{P}} \\pi(\\rho | s) \\pi(a|s,\\rho) d \\rho$  (Eq 3). The prompt is a general concept, includes but not limits to sub-goals used in HRL. Other choices includes returns-to-go (values), option, and skills. For example, return-conditioned prediction [1] can also be included as a special case of hierarchical policy framework. \n\nAlthough V-ADT does not decompose a global goal into explicit sub-goals, it operates hierarchically by having the high-level policy generate context-specific prompts (values in this case) which guide the decision-making of the low-level policy. This separation of prompt generation (high-level) and action determination (low-level) based on these prompts aligns with the fundamental concept of hierarchy in decision-making processes.\n\n\n\n### Q: What is the reward function of the high level?\n\nA: There is no reward function of the high-level in V-ADT. The low-level policy of V-ADT makes return-conditioned predictions as in [1]. The key innovation of V-ADT is the value prompt given by the high-level policy is learned from data. \n\n\n\n### Q: how could you avoid the issues you stated in Sec. 2?\n\n V-ADT can solve the motivating example. The value prompt used by V-ADT represents the maximum achievable value given the offline data. This value prompt then informs the V-ADT low-level policy to produce an action towards achieving the value. \n\nGiven this in mind, let's consider the motivating example.  The dataset \\( D \\) contains three trajectories $a \u2192 b \u2192 c$ leading to a return of 0, $d \u2192 b \u2192 e$ leading to a return of 10, and another trajectory starting from *$a$* and leading to a return of 1. As for this particular example, the value prompt for $a$ is 10. To achieve this value, at state $a$, V-ADT takes a step towards $b$, as the other path would lead a worse trajectory with return 1. \n\n\n\n\n\n### Reference\n\n[1] Emmons, Scott, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. \"RvS: What is Essential for Offline RL via Supervised Learning?.\" In *International Conference on Learning Representations*. 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699955613890,
                "cdate": 1699955613890,
                "tmdate": 1699955613890,
                "mdate": 1699955613890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iRNwdPr7qa",
                "forum": "7v3tkQmtpE",
                "replyto": "ALFSzHWC2C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7223/Reviewer_xoB5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7223/Reviewer_xoB5"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' efforts in addressing the concerns raised in my initial review. However, I still have reservations regarding the distinction between this work and the prior work of the Hierarchical Decision Transformer.\n\nThe authors' response suggests that the high-level policy of ADT is more general, capable of outputting various types of prompts. This statement implies that the prior work might be limited to certain types of subgoals. For a clearer understanding of the novelty and contributions of this paper, it would be beneficial to have a more detailed explanation of how this generalization differs from or improves upon the capabilities of the Hierarchical Decision Transformer.\n\nAdditionally, I noticed that the revised manuscript does not include direct experimental comparisons with the Hierarchical Decision Transformer. Such comparisons are crucial for substantiating the claimed advancements and understanding the relative strengths and limitations of both approaches. \n\nI believe that further clarifications and possibly additional experimental comparisons are needed to fully address these concerns. These additions would significantly strengthen the paper and provide the necessary context for its positioning within the existing body of work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668911354,
                "cdate": 1700668911354,
                "tmdate": 1700668911354,
                "mdate": 1700668911354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OTNqvarxAF",
            "forum": "7v3tkQmtpE",
            "replyto": "7v3tkQmtpE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7223/Reviewer_XUDW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7223/Reviewer_XUDW"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an in-depth examination of a hierarchical reinforcement learning (HRL) approach applied to decision transformers (DT). The study begins by shedding light on various challenges within the realm of DT, primarily focusing on issues like inaccurate return-to-go (RTG) estimates resulting from initial estimations and the inability perform stitching. The authors propose a solution by introducing a simple network, conditioned solely on the current state, which generates a prompt for the high-level policy. Furthermore, they refine the low-level policy by conditioning it on both the state, historical data (excluding returns), and the generated prompt to enhance in context learning.  Advantage regression is used to train the low-level policy since the value tokens are not conditioned on the actions. The paper explores two distinct prompt styles: one based on value, learned using in-sample optimal value, to address the RTG estimation issues, and another that outputs a goal state prompt, learned using HIQL.\n\nThe experimental component encompasses a variety of D4RL environments, with particular emphasis on hierarchical envs. The results show improvements mainly in the hierarchical environments. It also shows issues related to variance when tuning target returns, showcasing consistent performance using the proposed approaches. Additionally, the paper provides insights through various ablations, which involve removing RL losses and prompts, among other factors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work gives a well-thought-out approach compared to most existing research involving HRL in the context of decision transformers. It addresses specific challenges, such as the problems associated with RTG estimates, in a systematic manner, such as by introducing value tokens.\n- The introduction of value-based prompts represents a novel contribution, and the adjustment of the low-level policy loss to accommodate this is a good improvement.\n- The paper includes a substantial number of offline baselines and conducts numerous experiments in hierarchical environments, as well as those involving complex stitching operations.\n- The ablation studies effectively illustrate the significance of each component within the proposed approach.\n- The paper is easy to read and very well written."
                },
                "weaknesses": {
                    "value": "I have two main concerns about this work:\n\n- The experimental results presented in the paper appear to lack significance. For instance, in Table 2, the performance of G-ADT is nearly indistinguishable from that of the waypoint transformer. In Table 1, V-ADT demonstrates significant improvements over DT only in the of antmaze environments, with results still falling short of IQL.\n- While the application of various RL losses to DT within a hierarchical framework has elements of novelty, the significance of these contributions may be undermined by the underwhelming experimental outcomes.\n\nIn light of the concerns regarding the lack of experimental significance, it is recommended to consider a weak rejection. The work remains principled and thought-provoking; however, additional experimental evaluations are essential to further substantiate its claims and contributions."
                },
                "questions": {
                    "value": "Can more HRL environment experiments be shown?\n\nCan the authors put the significance of the work into context better (especially experimentally)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7223/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7223/Reviewer_XUDW",
                        "ICLR.cc/2024/Conference/Submission7223/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766220627,
            "cdate": 1698766220627,
            "tmdate": 1700568409167,
            "mdate": 1700568409167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H88dMy8FKb",
                "forum": "7v3tkQmtpE",
                "replyto": "OTNqvarxAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XUDW"
                    },
                    "comment": {
                        "value": "### Q: The experimental results presented in the paper appear to lack significance\n\nA: We respectfully disagree with this. V-ADT outperforms DT by **20.5%** on MuJoCo datasets, **227.5%** on Antmaze datasets, and **42.5%** on Kitchen datasets.  G-ADT  outperforms WT by **14.2%** on Antmaze datasets.   \n\nThe overall performance of ADT falls short of IQL could be attributed to the transformer architecture of the policy. In fact, it is still an open problem if Transformer is more efficient than standard architectures in single task RL. Previous studies show that MLP is competitive with and sometimes more effective than Transformer in offline RL benchmarks (see Table 1 of RvS[1] for example, where RvS-R outperforms DT on Antmaze datasets in the reward-conditioned behavior cloning setting). One conjecture is that high-capacity modles only make sense when we train on large and diverse datasets [2]. We would like to explore this conjecture in future works by applying ADT for multi-modal and multi-task domains. \n\n\n\n### Q: Can more HRL environment experiments be shown?\n\nA: Our method is not designed specifically for HRL. Instead, we use hierachical framework to rethink transformer-based decision algorithms, and derive joint policy optimization algorithms from it. Our method is essentially an offline RL method and we follow standard benchmarks to prove the effectiveness of ADT.\n\n### Reference\n\n[1] Emmons, Scott, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. \"RvS: What is Essential for Offline RL via Supervised Learning?.\" In *International Conference on Learning Representations*. 2021.\n\n[2] Chebotar, Y., Vuong, Q., Hausman, K., Xia, F., Lu, Y., Irpan, A., ... & Levine, S. (2023, August). Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. In *7th Annual Conference on Robot Learning*."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699955459807,
                "cdate": 1699955459807,
                "tmdate": 1700040954561,
                "mdate": 1700040954561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TywbilKjaZ",
                "forum": "7v3tkQmtpE",
                "replyto": "OTNqvarxAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7223/Reviewer_XUDW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7223/Reviewer_XUDW"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the authors for the clarification.  I also believe the authors have given good justification for the IQL performance gap in my response and to other reviewer responses.  I have chosen to adjust my score to marginal acceptance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568470528,
                "cdate": 1700568470528,
                "tmdate": 1700568498885,
                "mdate": 1700568498885,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HxQ3i4dMYa",
            "forum": "7v3tkQmtpE",
            "replyto": "7v3tkQmtpE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7223/Reviewer_RLH1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7223/Reviewer_RLH1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to integrate a hierarchical architecture with the decision transformer architecture to automatically tune the \u201cprompts\u201d. The prompt in this paper mainly corresponds to the reward-to-go in the original DT paper, and is also extended to the notion of \u201cgoal\u201d. The paper proposed an automated decision transformer (ADT), and its two versions, including V-ADT and G-ADT. Experimental results in continuous control tasks show that ADT shows better performances than baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper focuses on a solid problem which is manually selecting a pre-defined expected reward is often difficult and can result in suboptimal outcomes.\n\n - The ablation study is interesting."
                },
                "weaknesses": {
                    "value": "- The paper is missing key related works, such as prompt decision transformer studying the prompting mechanism for continuous control tasks in multi-task setting and hierarchical decision transformer, which has a similar motivation of using a hierarchical architecture.\n\n    - Prompting decision transformer for few-shot generalization: https://arxiv.org/pdf/2206.13499.pdf\n   \n    - Hierarchical decision transformer: https://arxiv.org/pdf/2209.10447.pdf\n\n - Although the problem is quite solid, and the paper is trying to propose a general hierarchical framework to tackle the problem, the paper is still focusing on single-task learning, which is a relatively narrow scope.\n\n - The methodology is hard to follow, and the design choice is not fully discussed. \n\n - The motivating example in Section 2.2.1 does not quite connect to the proposed methodology and leads to confusion. The example provided in section 2.2.1 is mainly about the data coverage issue. The proposed method seems to still learning Q, V, or goal from the offline dataset. How the proposed method can help solve the problem is still unclear."
                },
                "questions": {
                    "value": "- For the hierarchical policy, why condition on the current state is sufficient? \n\n - Does the method\u2019s performance heavily depend on the learned Q and V function from IQL?\n\n - How the proposed method can help solve the example problem in section 2.2.1 (no trajectory a->b->c)?\n\n - How is the proposed method different from the hierarchical decision transformer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698960601092,
            "cdate": 1698960601092,
            "tmdate": 1699636859481,
            "mdate": 1699636859481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5hZ79o8p7d",
                "forum": "7v3tkQmtpE",
                "replyto": "HxQ3i4dMYa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RLH1"
                    },
                    "comment": {
                        "value": "### Q: The paper is missing key related works\n\nA: Thanks for pointing out this. We have cited and discussed these two papers and other DT Enhancements in Discussions section in our revised paper\n\n\n\n### Q: The paper is still focusing on single-task learning.\n\nA: Actually, some of  the tasks in our experiments include several sub-tasks. For example, the kitchen datasets are collected based on the 9 degrees of freedom [Franka robot](https://www.franka.de/). The Franka robot is placed in a kitchen environment containing several common household items: a microwave, a kettle, an overhead light, cabinets, and an oven. The environment is a `multi-goal reaching` problem in which the robot has to interact with the previously mentioned items in order to reach a desired goal configuration. Detailed descriptions can be found at https://robotics.farama.org/envs/franka_kitchen/franka_kitchen/ and https://github.com/Farama-Foundation/d4rl/wiki/Tasks#frankakitchen. We leave multi-task learning on more benchmarks as our future work.\n\n\n\n### Q: The methodology is hard to follow, and the design choice is not fully discussed.\n\nA: To help understand our methodology better, we provide more detailed descriptions in Section 3 of the revision. As for the design choices, we ablate the effectiveness of the most important components of ADT including the efficacy of the prompt, the efficacy of the RL loss and the efficacy of tokenization in Section 5.4. If you could please point out the specific design choices that need to be discussed, we would happy to respond to them.\n\n\n\n### Q: The motivating example in Section 2.2.1 does not quite connect to the proposed methodology and leads to confusion.  How the proposed method can help solve the example problem in section 2.2.1 (no trajectory a->b->c)?\n\nA: V-ADT can solve the motivating example. The value prompt used by V-ADT represents the maximum achievable value given the offline data. This value prompt then informs the V-ADT low-level policy to produce an action towards achieving the value. \n\nGiven this in mind, let's consider the motivating example.  The dataset \\( D \\) contains three trajectories $a \u2192 b \u2192 c$ leading to a return of 0, $d \u2192 b \u2192 e$ leading to a return of 10, and another trajectory starting from *$a$* and leading to a return of 1. As for this particular example, the value prompt for $a$ is 10. To achieve this value, at state $a$, V-ADT takes a step towards $b$, as the other path would lead a worse trajectory with return 1. \n\n\n\n### Q: For the hierarchical policy, why condition on the current state is sufficient?\n\nA: We think a memoryless policy is sufficient for the Markov environment. However, we certainly agree that applying an auto-regressive model for the high-level policy as did in HDT is also possible. We would like to explore this design in our future work. Thanks for your suggestions. \n\n\n\n### Q: Does the method\u2019s performance heavily depend on the learned Q and V function from IQL?\n\nA: Yes, V-ADT requires joint optimization of high and low level policies. The output of the high-level policy, which is produced by IQL, significantly affects the performance of V-ADT. \n\nWe want to emphasize that this does not mean that V-ADT is  simply reinventing IQL by replacing the policy with a transformer. Our ablation shows that leveraging the prompts from a high-level policy significantly boost the performance of V-ADT on some datasets. \n\n\n\n### Q: How is the proposed method different from the hierarchical decision transformer?\n\nA: ADT differs from HDT mainly from two aspects: (1) The high-level policy of ADT is more general that could output different kinds of prompts, including sub-goals, target values, latent skills and options. (2) For the low-level policy, we apply advantage weighted regression instead of conditioned prediction as in DT to enable stitching ability."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699955426208,
                "cdate": 1699955426208,
                "tmdate": 1699955426208,
                "mdate": 1699955426208,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]