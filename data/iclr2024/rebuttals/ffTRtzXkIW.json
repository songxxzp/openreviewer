[
    {
        "title": "PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward"
    },
    {
        "review": {
            "id": "elixQpU4ux",
            "forum": "ffTRtzXkIW",
            "replyto": "ffTRtzXkIW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4732/Reviewer_58y7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4732/Reviewer_58y7"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the reward misalignment problem in IRL, which refers to the problem that reward maximization does not guarantee task success. To address this problem, PAGAR introduces both an antagonist policy - an optimal policy trained to perform similarly to the expert - and a protagonist policy that is trained to be closer to the antagonist policy using a PPO-style objective function. PAGAR then derives a reward function that maximizes the performance difference between the protagonist policy and the antagonist policy. When the performance margin remains within a certain bound, the protagonist policy's performance with the updated reward function does not fall below a certain threshold, indicating reward alignment. By training the protagonist policy in a minimax manner, the authors argue that an aligned reward function can be identified and used to optimize the policy. The authors demonstrate PAGAR's performance in partially observable navigation tasks and transfer environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors conducted a thorough analysis of reward function alignment and successfully applied their insights within the IRL framework. They demonstrated that the PAGAR-(IRL) algorithms outperform baselines, requiring fewer samples and enabling zero-shot imitation learning in transfer environments."
                },
                "weaknesses": {
                    "value": "There are a few minor typos present:\n\n- On the 2nd line from the bottom in Figure 1, $U_{r^-}(\\pi^-) \\rightarrow U_{r^-}(\\pi^*)$.\n- On the 5th line from the bottom, please confirm whether the direction of the inequality sign is correct.\n- On the first line in section 2, Related works, contextx should be changed to contexts."
                },
                "questions": {
                    "value": "I'm curious if the antagonist policy can be a non-optimal policy. In Figure 2 (b), VAIL does not appear to have enough successful episodes to qualify as an optimal policy and serve as a proxy. Would it be acceptable for the policy to have non-zero returns instead of being fully optimal?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4732/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4732/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4732/Reviewer_58y7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735094370,
            "cdate": 1698735094370,
            "tmdate": 1699636455074,
            "mdate": 1699636455074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pvMftOWnPT",
                "forum": "ffTRtzXkIW",
                "replyto": "elixQpU4ux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4732/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## 5th line from the bottom ... inequality sign\n\nThe sign is correct, when $U_{r^-}(E)-max_\\pi U_{r^-}(\\pi) > U_{r^+}(E) - max_\\pi U_{r^+}(\\pi)$, reward function $r^-$ will be more 'optimal' than $r^+$. \nWe use this inequality to show that we must consider the case where a sub-optimal reward function can align with the task while the optimal reward function is misaligned.\n\n\n## ... non-optimal policy ...\n\nIn theory, the antagonist policy has to be optimal, while in practice, we have to train the antagonist policy together with the protagonist policy from scratch.\nThe fact that VAIL alone not being able to produce successful episodes does not directly relate to whether running VAIL in the PAGAR framework can produce better results.   \nThis is because the reward function in the PAGAR framework is optimized to increase the protagonist antagonist induced regret in Eq (2). This optimization incentivizes searching for a reward function that can makes the antagonist policy outperforms the protagonist policy.\nGiven that inverse RL can have multiple solutions -- multiple reward functions aligned with the demonstrations, PAGAR-based IL avoids those reward functions that are difficult for any policy to achieve a high utility, since this will result in both the antagonist and the protagonist policy having low performance."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166372358,
                "cdate": 1700166372358,
                "tmdate": 1700166372358,
                "mdate": 1700166372358,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tdr4WGkZTN",
            "forum": "ffTRtzXkIW",
            "replyto": "ffTRtzXkIW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4732/Reviewer_rmTC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4732/Reviewer_rmTC"
            ],
            "content": {
                "summary": {
                    "value": "The paper is inspired by unsupervised environment design (UED) [Dennis et al. 2020] which optimizes a protagonist policy an antagonist policy and the dynamics parameters with respect to the regret (performance difference between antagonist and protagonist; protagonist aims to minimize the regret, antagonist and \"environment\" aim to maximize regret). However, in contrast to that prior work, the current submission tackles the imitation learning setting, by maximizing the regret with respect to a reward function instead of dynamics parameter. The motivation for this approach is to learn policies that perform well across a distribution of reward function making them less susceptible to reward misalignment, where a policy performs well on an inferred reward function but still fails in the underlying task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Although there is a close connection to UED, the application to imitation learning is quite original.\n\nI also think that the problem setting, focusing on solving the actual underlying task, is very relevant."
                },
                "weaknesses": {
                    "value": "1. Clarity\n------------\nI'm struggling to make sense of some parts of the work. For now, I view these as clarity issues that I hope the authors can address in the rebuttal, but there may also be issues regarding soundness or relevance:\n\n- I'm not fully convinced of the motivation for the method. The paper argues that existing IL/IRL methods may fail in optimizing the underlying task because they learn a policy that optimizes a misaligned reward function. However, I do not think that the typically used problem formulation suffer from such problems. For example IL methods are often formulated as divergence minimization problems, and assuming that they succeed in bringing the divergence to zero, they would perform as well as the expert on any reward function. Of course, in practice we will typically not achieve zero divergence due function approximations, limited data and potentially unstable optimization, but I don't think that these challenges are specific to the problem formulation and they affect the proposed method in similar way. For some IRL methods, it was even shown that the optimal policy for the learned reward function may outperform the (suboptimal) expert, in the sense that it performs no worse but potentially better on every reward function within the hypothesis space. With that in mind, I also find Example 1 quite misleading. The example argues that IRL would lead to a bad policy in a simple toy problems, by arguing that a constant reward function would solve the IRL problem. However, already Ng & Russell [2000] noted, that such IRL formulation would be illposed, and additional constraints / objectives are required. For example, MaxEnt-IRL would maximize the likelihood for a MaxEnt-policy, where a constant reward function (leading to a uniform policy) would certainly not be a solution in the toy problem.\n\n- The conditions used in Theorem 1 are not satisfiable if the hypothesize space of the reward function includes a reward function for which there is no gap between the best performing failing policy and a succeeding policy. For example, if a task is considered to succeed when the distance to a goal is below a certain threshold, and the reward uses a quadratic penalty on the distance to the threshold as cost, we can get arbitrary close in terms of reward to a succeeding policy while still failing. Potentially related to this, it not clear to me how the method optimizes with respect to a distribution $\\mathcal{P}\\_{\\pi}(r)$ over reward functions. Where does this distribution appear in Algorithm 1?\n\n- Section 4.2 defines a set $R\\_{E,\\delta} =  \\\\{ r \\| U\\_{r}(E) - \\underset{\\pi \\in \\Pi}{\\max} U\\_{r}(\\pi) \\ge \\delta \\\\}$ where $\\delta$ is chosen to be strictly positive. Wouldnt this set be empty if the expert policy is in $\\Pi$?\n\n- I am wondering how the success of a policy or its performance can be deterministic, given that both policy and environment are stochastic.\n\n- The transfer experiment is not clear to me. Is the learned policy transferred to the new environment, or is it learned from scratch in the new environment, using demonstration from the source environment?\n\n2. Evaluation\n-------------\nThe experimental results in the continuous domain are not convincing. For example, IQ-Learn should clearly outperform GAIL in these environments. The paper states that it should perform better if a bigger replay buffer was used, but then I wonder why no adequate hyperparameter tuning was performed for the comparisons,"
                },
                "questions": {
                    "value": "For questions, please refer to my comments under \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764235773,
            "cdate": 1698764235773,
            "tmdate": 1699636454967,
            "mdate": 1699636454967,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RYP7i0WuJT",
                "forum": "ffTRtzXkIW",
                "replyto": "tdr4WGkZTN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4732/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ## ... maximize the likelihood for a MaxEnt-policy ...\n\nMaximum entropy is a heuristic used to reduce reward ambiguity and not reward misalignment. \nIn fact, MaxEnt cannot eliminate reward ambiguity even if we have infinite demonstrations ([Skalse. et al., 2022], which we cite in the first paragraph of Introduction).\n\nReward ambiguity and reward-task misalignment are two different concepts.  \nReward ambiguity is the problem of many different reward functions in the reward hypothesis space can explain the expert demonstrations equally well.\nOn the other hand, reward-task misalignment \nis the problem of policies that achieve a high utility under a reward function failing to finish the task (the well-known notion of reward hacking is a manifestation of this [Pan. et al., 2022], which we cite at the end of first paragraph).\n\nMaximum entropy likelihood may lead to a policy that is highly probable in reproducing the demonstrated trajectories. \nIt is equivalent to assuming that the underlying task is simply to generate the demonstrated trajectories with the highest probability.\nOur work does not make such an assumption. As shown by our definition of $R_{E,\\delta}$, we include not only the optimal but also the sub-optimal solutions of inverse RL as candidate reward functions. \n\n> ## ... The conditions used in Theorem 1 are not satisfiable ... #\n\nFirstly, in Definition 1, the intervals $S_r$ and $F_r$ are defined such that a policy's utility falling within $S_r$ or $F_r$ is deemed a sufficient but not necessary condition for determining success or failure. Put differently, if a policy $\\pi$ achieves a utility in $S_r$, it is guaranteed to be successful, and if $\\pi$ achieves a utility in $F_r$, it is guaranteed to be a failure. Note that $S_r$ and $F_r$ are not required to encompass all successful or failing policies' utilities. These intervals may also be very small for certain task. In the extreme scenario, $S_r$ could be a singleton, representing the maximum achievable policy utility under $r$, if the optimal policy under $r$ can successfully complete the task. Meanwhile, $F_r$ can be empty.\nThe impact is that if all reward functions in the set $R$ have very small $S_r$ and $F_r$ intervals, $MinimaxRegret$ may have to achieve a low regret to produce a successful policy.\n\nAlso, the conditions about the width of S and F intervals are sufficient conditions for $MinimaxRegret$ to learn a successful policy. Suppose all the reward functions in the candidate reward function set are task-aligned. Then, even if they all match the description \n mentioned by the reviewer, i.e., `no gap between  $S_r$ and $F_r$`, $MinimaxRegret$ can still induce a successful policy in the task. \n> Proof\n   * There must be a policy $\\pi^+$ that makes $Regret(\\pi^+, r)\\leq|S_r|$ hold for all reward function $r$'s. Because its utility cannot be in the S interval under a reward function and in the F interval under another.\n   * For any policy $\\pi^-$ that fails the task, $Regret(\\pi^+, r)>|S_r|$ holds for all reward function $r$'s. Because if there exists an $r$ under which $Regret(\\pi^-, r) \\leq  |S_r|$, $\\pi^-$ is guaranteed to succeed in the task, contradicting the assumption. \n   * $MinimaxRegret$ will pick $\\pi^+$ over $\\pi^-$ as its solution because $Regret(\\pi^+, r)<Regret(\\pi^-, r)$ under all $r$'s\n\n> ## ... Potentially related to this, it not clear to me how the \n\nTable 1 shows an equivalent objective function of $MinimaxRegret$\nIt only describes the analytical solution of PAGAR. \nOur approach to PAGAR is not based on Table 1.\n\n> ## ... $\\delta$  is chosen to be strictly positive ...\n\nWe mentioned after Corollary 2 that $\\delta$ cannot be larger than the negative minimal loss of inverse RL. \nIf the minimal loss of inverse RL is non-negative, i.e., the expert policy is in the policy set, $\\delta$ will be non-positive.\n\n> ## ...the success of a policy or its performance can be deterministic ...\n\nThe 'success' here indicates the ability of the policy to finish the task, not whether the policy finishes the task in a single run.\nFor instance, a success criterion can be \"the probability of reaching the state must be higher than 'p'\". \nIf a reward function is well-designed, the success of this policy can be guaranteed if the policy's expected return exceeds a certain threshold. \n\n\n> ## ...  IQ-Learn should clearly outperform ...\n\nIn our continuous control experiments, the x-axis represents policy update iterations, not policy roll-out steps. \nIQ-learn's implementation is based on soft-actor-critic which is an off-policy algorithm that conducts several policy updates between consecutive interactions with the environment.\nThe size of the replay buffer allocated for IQ-learn is consistent with the sample size used for PPO which is the RL algorithm used in our implementation.\n\n> ## ... transfer experiment\n\nThe demonstrations are sampled in one environment, Algorithm 1 (reward learning, policy learning) is conducted in another environment."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166282838,
                "cdate": 1700166282838,
                "tmdate": 1700166282838,
                "mdate": 1700166282838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7GQjVOl4G7",
                "forum": "ffTRtzXkIW",
                "replyto": "RYP7i0WuJT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4732/Reviewer_rmTC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4732/Reviewer_rmTC"
                ],
                "content": {
                    "title": {
                        "value": "Concerns have not been adequately addressed"
                    },
                    "comment": {
                        "value": "> Maximum entropy is a heuristic used to reduce reward ambiguity and not reward misalignment. In fact, MaxEnt cannot eliminate > reward ambiguity even if we have infinite demonstrations ([Skalse. et al., 2022], which we cite in the first paragraph of Introduction).\n\nI would not reduce the principle of Maximum Entropy to a mere heuristic. MaxEnt-IRL chooses among all models of the expert the one that makes least assumption, which is arguably the most principled choice! Please also note that MaxEnt-IRL is well-posed. The dual function is convex, which implies that there is a single reward that solves the MaxEnt-IRL optimization problem. If we are talking about more modern approaches that maximize the likelihood of MaxEnt-RL policy, but optimize over a NN reward function instead of a linear reward function, then I agree that there is reward ambiguity since potential-based reward shaping does not affect the (MaxEnt)-optimal policy. However, even then, the different reward functions all result in the same behavior on the given environment, and hence, reward ambiguity only becomes an issues when we want to apply the learned reward function to MDPs with different dynamics.  \n\nI do not see how the concept of reward misalignment can apply to IRL. If the agent reproduces the behavior of the expert and we assume that the expert solves the task, the agent will also solve the task. Typical IRL formulation, will find a reward function that when optimized either matches the expert exactly, or derivates if this provably does not diminish reward on all reward functions in the hypothesis space.\n\n>Maximum entropy likelihood may lead to a policy that is highly probable in reproducing the demonstrated trajectories. It is >equivalent to assuming that the underlying task is simply to generate the demonstrated trajectories with the highest probability. \n\nThis claim is not substantiated and wrong. The underlying task would always be to maximize the reward function. Maximizing the MaxEnt reward function would not necessarily result in the demonstrated trajectories to obtain highest probability. Even when MaxEnt-optimizing the learned reward function, the maximum likelihood trajectory generated of the policy might in general not coincide with any demonstration.\n\nAlso, my comments regarding Example 1 have not been addressed!  \n\n> We mentioned after Corollary 2 that $\\delta$ cannot be larger than the negative minimal loss of inverse RL. If the minimal loss of inverse RL is non-negative, i.e., the expert policy is in the policy set, $\\delta$ will be non-positive.\n\nThe text after Corollary 2 mentions that $R\\_{E,\\delta}$ is learned for a given $\\delta$. Table 2, also lists $\\delta$ as a hyperparameter. These hyperparamters are non-sensical, at least if we assume the expert policy to be in the hypothesis space.\n\n> In our continuous control experiments, the x-axis represents policy update iterations, not policy roll-out steps. IQ-learn's implementation is based on soft-actor-critic which is an off-policy algorithm that conducts several policy updates between consecutive interactions with the environment. The size of the replay buffer allocated for IQ-learn is consistent with the sample size used for PPO which is the RL algorithm used in our implementation.\n\nThis does not address my complain that the evaluation does not show a fair comparison. For a fair comparison, the hyperparameters should be chosen suitable to the method. Is also not fair to compare SAC based method with PPO based method based with respect to the number of policy updates, since SAC usually updates the policy after a single transition. Instead the comparison should be with respect to the number of transitions. For evaluating the computational overhead of the single-step updates, additionally the computational time could be shown. \n\nPlotting over the number of policy updates is severely misguiding the reader to overestimate the relative performance between the proposed method and IQ-Learn!\n\nI will reduced my score from 5 to 3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726165353,
                "cdate": 1700726165353,
                "tmdate": 1700726165353,
                "mdate": 1700726165353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a4lCmSv5pa",
                "forum": "ffTRtzXkIW",
                "replyto": "tdr4WGkZTN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4732/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">  If the agent reproduces the behavior of the expert and we assume that the expert solves the task, the agent will also solve the task.\n\nA policy being able to reproduce the demonstrated trajectories does not imply the policy is ready for the task. In a stochastic environment, any stochastic policy can reproduce the demonstrated trajectories ... We have reiterated that the outcomes of individual attempts or instances do not solely determine success. If by `reproduces the behavior` you mean to recover the expert policy, the problem is that IRL can fail to recover expert policy for so many reasons ... We have cited the literature in the paper and in our rebuttal.\n\n> This claim is not substantiated and wrong ...\n\nOur claim is substantiated and correct. We recommend revisiting `Ziebart, Brian D., et al. Maximum entropy inverse reinforcement learning` and searching for the keyword 'likelihood'. It is beyond doubt that max-ent IRL determines the reward weight by maximizing the likelihood of the max-ent policy generating the demonstrated trajectories. Max-Ent principle is not the only factor at play.\n\n> The underlying task would always be to maximize the reward function.\n\nThis does not contradict our argument. The policy is trained with a reward function that is learned based on a maximum likelihood objective function.\n\n> ... at least if we assume the expert policy to be in the hypothesis space\n\nOur paper never made such an assumption at all.\nIt is common knowledge that IRL can be affected by various factors, including a limited number of expert demonstrations, expert policy not being in the hypothesis space, learning reward function in MDPs with different dynamics ... \nAnd our method is to mitigate the reward misalignment issue in challenging and varied conditions.\nWhile Theorem 3 implicitly covers the ideal condition where IRL can reach Nash Equilibrium, all other theories do not account for the ideal condition.\nOur experiments in MiniGrid aim to exhibit PAGAR's ability when those factors are present."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731201874,
                "cdate": 1700731201874,
                "tmdate": 1700731738718,
                "mdate": 1700731738718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qypCZRHCEf",
            "forum": "ffTRtzXkIW",
            "replyto": "ffTRtzXkIW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4732/Reviewer_BsED"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4732/Reviewer_BsED"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method for reducing reward misalignment in RL and IL settings. The method follows the paradigm introduced in PAIRED, where there is a triple-level objective: the protagonist tries to minimize regret, a design agent selects a reward function to maximize regret, and the antagonist also tries to maximize regret. The paper first formalizes task-reward alignment using the introduction of success and failure intervals. Regret is defined as the difference in expected return achieved by the best-case antagonist and the protagonist. Then the authors provide a theoretical result demonstrating that, under certain conditions, the protagonist policy optimizing for minimax regret can guarantee avoiding failing the task and possibly succeed as well. Next the authors analyze the IL setting, where expert demos are provided in lieu of a reward function. Then they provide an algorithm for PAGAR-based IL, which alternates between the policy optimization steps (for P and A) and reward selection step. The policy optimization step is fairly straightforward using a PPO-style objective. Reward selection for regret maximization uses bounds on the difference in expected return between P and A using only the samples of one policy. Experiments are conducted in maze navigation and MuJoCo continuous control tasks, both demonstrating PAGAR can mitigate reward misalignment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this paper are 1) the novelty in applying PAIRED's paradigm to reward misalignment and 2) the very appreciable amount of technical novelty. Regarding the latter, the authors formalize task-reward misalignment (which is generally only explained intuitively), define regret for the reward misalignment mitigation problem, and give algorithms for both RL and IL with PAGAR. The authors provide theoretical results showing that under certain conditions, PAGAR's minimax regret objective, if optimized perfectly, is able to guarantee avoiding task failure and possibly succeed, in both the RL and IL settings. These are important and strong results. \n\nExperiments are provided in both gridworlds/discrete envs and continuous envs."
                },
                "weaknesses": {
                    "value": "The paper was very difficult to follow. There was quite a lot of new notation introduced for a fairly intuitive concept, and it was prohibitively difficult to keep track of all the new notation, definitions, and theorems. I was not able to fully appreciate the implications of the theory. I would be willing to raise my score if the paper were made significantly more readable, since this is an interesting and novel idea.\n\nIn particular, the paper does not answer the question of how the set R (under which reward optimization is performed) is chosen in the RL setting, instead redirecting to the IL setting. Is PAGAR meant to be applied in the RL setting or is the selection of R prohibitively difficult and therefore PAGAR as described is only applicable in practice to IL? It would help to make this more explicit.\n\nIn the IL setting, it seems that generation of the R_E,delta set is difficult, but it is not discussed how to obtain such a set. Example 1 does not provide intuition for the basis functions it chose to parameterize R_E so it was difficult to understand. Is the discussion at the bottom of pg 5 meant to be a construction of R_E or just a non-constructive description? \n\nThe paper is missing a discussion of the computational complexity of PAGAR, both theoretically and empirically. This is a crucial property of the method. \n\nExperiments are only conducted with a very limited number of demonstrations (10). I don't find this realistic for real-world deployment where we care about alignment. Is this because providing more demonstrations would reduce misalignment and thus the need for a complicated method like PAGAR?"
                },
                "questions": {
                    "value": "- Is PAGAR meant to be applied in the RL setting or is the selection of R prohibitively difficult and therefore PAGAR as described is only applicable in practice to IL?\n- Is the discussion at the bottom of pg 5 meant to be a construction of R_E or just a non-constructive description? \n- how difficult is it to specify success/failure for non-binary-outcome tasks in general? \n- definition 2 is not a definition as written, it is a claim about the method.\n- how strong of a condition is (2) in theorem 1?\n- some of the lines in figure 2 look like they are still increasing and not yet converged. can you run them for longer?\n- in figure 2 does timesteps count just the protagonist or everyone? if not, can you compare with # FLOPS as well? I imagine PAGAR is quite a bit more expensive. \n- is AIRL a suitable baseline? it's also disentangled from transition dynamics which is related to your objective for the zero-shot transfer experiments. \n- can you provide more details what exactly the transfer is? is it transferring from one random instantiation of a family of envs to another?\n- It is odd to put a definition in the intro, I would suggest keeping the intro at a high level and moving the definition to a following section. \n- I noticed that often a definition would introduce many new objects at once and require reading an equation given later in order to appreciate a previous sentence. I would recommend re-ordering the presentation of material such that the flow of understanding is always moving forward, which can be accomplished by introducing new terms/definitions/concepts one step at a time and building up from the first object the reader should know (and can understand without needing to pull in other objects first)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4732/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4732/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4732/Reviewer_BsED"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825138693,
            "cdate": 1698825138693,
            "tmdate": 1699636454901,
            "mdate": 1699636454901,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k7TU9wmrBS",
                "forum": "ffTRtzXkIW",
                "replyto": "qypCZRHCEf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4732/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> ## does not answer the question of how the set $R$ is chosen in the RL setting ... Is PAGAR meant to be applied in the RL setting ... Only applicable in practice to IL\n\nWe would like to clarify that \nSection 4.1, \"RL with PAGAR\", is about examining the conditions for $R$ under which $MinimaxRegret(R)$ can generate a policy to avoid task failure or guarantee task success. \nIt does not deal with how to construct such an $R$ in an RL training scenario.\nIt is meant to support the main objective of the paper, which is mitigating the reward alignment problem in IRL-based IL, since IRL-based IL involves inferring a reward from the demonstrations and performing RL with the inferred reward.  \n\nApplying PAGAR to IRL-based IL is equivalent to letting $R$ be the solution set of IRL. \nIn this context, human demonstrations can be viewed as a supervision signal that help us identify $r$ that satisfies the conditions in Section 4.2 which are extensions of the conditions in Section 4.1 for a setting with expert demonstrations. \n \n \n> ## construction of the $R_E$ set\n\nWe do not directly construct this set; rather, we utilize the aforementioned conditions to restrict the exploration of $r$.  \nOur definition of $R_{E, \\delta}$ is that for every $r\\in R_{E,\\delta}$, the difference between the optimal utility and the expert demonstration utility is within $\\delta$. \nThis difference equals the negative IRL loss under $r$. \nWe can enforce selecting $r$ from $R_{E,\\delta}$ by restricting the inverse RL loss of $r$ to be no greater than $-\\delta$.\n\n> ## Example 1 ... parameterize $R_E$\n\nWe mention in the paragraph above Example 1 that $\\delta$ cannot be larger than the negative minimal inverse RL loss.\nWhen $\\delta$ reaches its maximum, $R_{E,\\delta}$ equals the optimal solution set of inverse RL.\nIn this case, we simplify the notation as $R_E$.\nWe then use Example 1 to show that in some instances, inverse RL treats the entire reward space $R$ as its optimal solution set, in other words, $R_E=R$.\nThe basis function is picked to illustrate the misalignment problem in inverse RL, as the choice of the basis functions can be arbitrary.\nHowever, by applying PAGAR, we can identify the ground-truth reward function. \n\n> ## Is the discussion at the bottom of pg 5 meant to be a construction of $R_E$ or just a non-constructive description \n\nThis discussion is about the formula in Table 1, which is an alternative of $MinimaxRegret$. \nIt is equivalent to $MinimaxRegret$ in the sense that it induces the same result as $MinimaxRegret$. \nIt is meant to provide more insights about PAGAR.\nIt is not a constructive description. \n\n\n> ## Complexity of PAGAR\n\nIn Algorithm 1, we show that PAGAR involves optimizing two policies (protagonist and antagonist) and one reward function.\nAlgorithm 1 leverages existing inverse RL algorithm to perform the reward optimization and the PPO algorithm for the policy optimization.\nTherefore, the complexity of PAGAR equals the complexity of the reward optimization algorithm plus the complexity of the PPO algorithm. \nIn our experiments, we use the same reward optimization algorithms as those of GAIL and VAIL in PAGAR.\n\n> ## limited number of demonstrations (10)\n\nThe effective utilization of a limited number of demonstrations is widely recognized as a crucial metric for evaluating Inverse Reinforcement Learning (IRL) and Imitation Learning (IL) algorithms in the literature. It is often used to asess the performance of these algorithms, as demonstrated in notable works such as the GAIL paper [Ho. et al., 2016], the AIRL paper [Fu. et al., 2018], and various other references cited in our study.\nIn GAIL [Ho. et al., 2016], which is a seminal work in imitation learning with generative adversarial networks, the algorithm demonstrates the ability to generate high-performance policies with just a single demonstration. \n\n> ## More demonstrations would reduce misalignment\n\nAs we mentioned in our introduction, a reward function aligning with the expert demonstrations does not imply aligning with the underlying task.\nThese two concepts are orthogonal.\nNotice that the definition of Task-Reward Alignment (Definition 1) is independent of demonstrations.\nMore demonstrations may help reduce reward ambiguity (i.e. multiple demonstration-aligned rewards), which, however, cannot be eliminated even with an infinite number of data [Ng & Russell (2000); Cao et al. (2021);\nSkalse et al. (2022a;b); Skalse & Abate (2022)]."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156680019,
                "cdate": 1700156680019,
                "tmdate": 1700162026839,
                "mdate": 1700162026839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]