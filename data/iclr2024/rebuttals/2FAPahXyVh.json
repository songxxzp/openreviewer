[
    {
        "title": "OptiMUS: Optimization Modeling Using mip Solvers and large language models"
    },
    {
        "review": {
            "id": "HHK4LoCvP8",
            "forum": "2FAPahXyVh",
            "replyto": "2FAPahXyVh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
            ],
            "content": {
                "summary": {
                    "value": "This introduction highlights the wide-ranging prevalence of optimization problems in operations, economics, engineering, and computer science. It emphasizes their significance in applications like improving energy efficiency in smart grids, refining supply chains, and enhancing profits in algorithmic trading. The paper underlines the critical role of algorithm selection and problem modeling in achieving successful solutions.\nThe expertise required to navigate these challenges creates a barrier for many sectors, including supermarkets, hospitals, municipalities, solar farms, and small businesses, limiting their access to optimization benefits.\nThe paper proposes leveraging Large Language Models (LLMs) to democratize access to optimization. LLMs have demonstrated proficiency in understanding and generating natural language, offering a means to simplify problem formulation and disseminate expert knowledge. However, their role in optimization remains underexplored due to their novelty and the lack of comprehensive benchmarks.\nThe paper introduces three key contributions:\n1. The NLP4LP dataset comprises 40 expert-formulated linear programming (LP) and mixed integer linear programming (MILP) problems. It includes annotated solutions, optimality-checking code, and sample formulations in markdown and code formats. The dataset uses a standardized format for representing optimization problems in natural language.\n2. OptiMUS, an LLM-based agent designed for formulating and solving optimization problems, was introduced.\n3. Developing techniques to enhance OptiMUS's performance, including automated data augmentation through problem rephrasing and self-improvement of solutions via automated testing and debugging. These techniques lead to a 67% increase in the solve rate compared to direct prompting.\nIn summary, the paper's contributions aim to democratize access to optimization techniques across various domains, extending their reach and utility."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this paper are as follows.\n1. Proposed a new framework for solving optimization problems using natural language.\n2. Proposed SNOP, a new format for expressing optimization problems in natural language.\n3. This paper proposed NLP4LP, a new dataset of optimization problems expressed in SNOP."
                },
                "weaknesses": {
                    "value": "The final sentence of the first paragraph on page 4 and the enumeration at the beginning of page 7.\n\nWeaknesses regarding the content of the text are as follows:\n3. The results of solving 32 different LPs and 8 different MILPs are summed for the experiment. This bais of problem types makes it difficult to compare whether the results of the experiments are more influenced by the nature of the problem (LP or MILP) or the nature of the method.\n4. The experiment in Figure 6 should describe the problems used. It is difficult to determine whether the results are the average of multiple problems or the results of solving one problem.\n5. There is no description of the solver used in the experiments. We believe that the choice of solver is important to improve the success rate and execution rate. Therefore, the experiment section should state what solver was used and, if OptiMUS selected it, how it was selected.\n\nThis study covers an exciting subject. I hope that future studies will improve the weaknesses mentioned above."
                },
                "questions": {
                    "value": "In addition to the above weaknesses, I would like to have the following questions answered:\n1. Is there any difference between different types of problems for variation in success rate and execution rate; please tell us about the experiment results in Figure 5, focusing only on LP (MILP).\n2. What is the maximum number of iterations with debugging?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6499/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6499/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698294317399,
            "cdate": 1698294317399,
            "tmdate": 1699636729000,
            "mdate": 1699636729000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QPZUyypGNg",
                "forum": "2FAPahXyVh",
                "replyto": "HHK4LoCvP8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 8JDZ"
                    },
                    "comment": {
                        "value": "**Response to reviewer 8JDZ**\n\nWe thank the reviewer for the time spent on our paper and valuable suggestions.\n\n----\n**Weaknesses**\n\n1. Reviewer\u2019s concern: This bias of problem types makes it difficult to compare whether the results of the experiments are more influenced by the nature of the problem (LP or MILP) or the nature of the method\n\n   We thank the reviewer for pointing this out. We have included plots for the performance of OptiMus on LP and MILP separately (Please see appendix B in the revised paper). We observe that OptiMUS performs the best among all the methods we evaluated in both cases, solving more than 80% of the problems. \n\n2. Reviewer\u2019s concern: The experiment in Figure 6 should describe the problems used.\n\n   Sorry for the confusion. Figure 6 is plotted based on the whole dataset. We added more details to the figure caption.\n\n3. Reviewer\u2019s concern: There is no description of the solver used in the experiments\n\n   We used Gurobi for all our tests. The reason for this choice is that we noticed that there are abundant internet resources on the Gurobi Python interface. Moreover, according to our preliminary tests, Gurobi performs better than CVXPY in general. We also updated the experiment section in the revised version to include this information.\n\n4. Reviewer\u2019s concern: typos and formatting issues\n\n   Thanks for pointing them out. We have fixed the issues in our revision.\n\n----\n**Questions**\n\n1. Reviewer\u2019s question: Is there any difference between different types of problems for variation in the metric?\n\n   Please refer to our response to concern 1 in the weaknesses section. \n\n2. What is the maximum number of iterations with debugging\n\n   The maximum number of iterations means the maximum number of times OptiMUS is allowed to debug the code if an execution error or a test error is raised. We use 5 debugging iterations in our experiments. We added more details on this to the revised version.\n\nWe sincerely appreciate the valuable suggestions of the reviewer. We have addressed all of the reviewer\u2019s concerns. If the reviewer finds our response and revision satisfactory, we politely request the reviewer to update the score accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275264157,
                "cdate": 1700275264157,
                "tmdate": 1700275264157,
                "mdate": 1700275264157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1AcP7NwI1p",
                "forum": "2FAPahXyVh",
                "replyto": "QPZUyypGNg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your courteous response. \n\nI am an expert in optimization problems such as LP (Linear Programming) and MILP (Mixed Integer Linear Programming). However, LP and MILP differ in both algorithms and computational complexity. For instance, in MILP, simply changing the order of variables and constraints can significantly alter the performance of solvers. As an expert in optimization problems, I am very interested in how much this method can mitigate computational difficulties, but regarding MILP, considering its NP-hardness, the outcome seems negative.\n\nOf course, I do not intend to deny the results of this experiment. However, since it is impossible to judge whether the results are limited to certain instances, it feels premature to conclude the effectiveness of this method in LP and MILP."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639867418,
                "cdate": 1700639867418,
                "tmdate": 1700639867418,
                "mdate": 1700639867418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lywGuOM759",
                "forum": "2FAPahXyVh",
                "replyto": "8xfeoR02Of",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the quick reply.\nSince it is not feasible to experiment with an infinite number of instances, using a benchmark set is naturally the course of action. The question is whether this benchmark set is considered sufficient in its type, quantity, and difficulty level in that field. For this reason, benchmark sets like MIPLIB have been proposed. For example, in the case of MILP, I believe that it cannot be considered sufficient with as few as eight instances.\n> 2. In terms of benchmarking, we politely disagree with saying \"it is impossible to judge whether the results are limited to certain instances\". In terms of measuring MILP/LP softwares, it is common to test them on a large set of benchmark instances."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645520029,
                "cdate": 1700645520029,
                "tmdate": 1700645520029,
                "mdate": 1700645520029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WLiKKKYFSA",
                "forum": "2FAPahXyVh",
                "replyto": "VVKH2AeQTi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_8JDZ"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to add to my earlier comments.\nIt's not mandatory to use MIPLIB, but I strongly suggest referring to the MIPLIB homepage and papers at https://miplib.zib.de/.\n\nIn the world of deep learning, researchers debate over appropriate datasets like ImageNet and CIFAR.\nSimilarly, in the world of MIP (Mixed Integer Programming), there has been a long-standing discussion about what kind of datasets are necessary. It's not just about categorizing problems; factors like the type of formulation and problem size are also carefully considered in the selection of problems.\n\nIt's fine to prepare a new dataset, but when claiming its generality, I think careful consideration should also be given to discussions in the optimization field, similar to those surrounding MIPLIB."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699811299,
                "cdate": 1700699811299,
                "tmdate": 1700699811299,
                "mdate": 1700699811299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "URaoZAgPT0",
            "forum": "2FAPahXyVh",
            "replyto": "2FAPahXyVh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a system to acquire the formal definition of an optimization\nmodel from a natural language description using LLMs. The authors describe their\napproach and evaluate it empirically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed system is very interesting and potentially makes solving technology\nmuch more accessible."
                },
                "weaknesses": {
                    "value": "The acquisition of MIP and similar types of problems from high-level\ndescriptions and examples of solutions has long been investigated, see for\nexample\nBeldiceanu, N., Simonis, H. (2012). A Model Seeker: Extracting Global Constraint Models from Positive Examples. In: Milano, M. (eds) Principles and Practice of Constraint Programming. CP 2012. Lecture Notes in Computer Science, vol 7514. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-33558-7_13\nThis work should at least be mentioned, as it is highly relevant here.\n\nThere are multiple broken references (??) throughout the paper."
                },
                "questions": {
                    "value": "How was the ground truth for experiments obtained? -- answered in rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6499/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6499/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689332694,
            "cdate": 1698689332694,
            "tmdate": 1700506390959,
            "mdate": 1700506390959,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QoyLzJJ33B",
                "forum": "2FAPahXyVh",
                "replyto": "URaoZAgPT0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Bafe"
                    },
                    "comment": {
                        "value": "**Response to reviewer Bafe**\n\nWe thank the reviewer for the time spent on our paper.\n\n----\n**Weaknesses**\n\n1. Reviewer\u2019s concern: There are no ground truth models for the considered optimization problems; There is nothing to ensure that the modeled problem corresponds to the natural language description\n\n   Many mathematical optimization problems can be modeled in several different ways. Therefore, we believe it is not reasonable to limit the evaluation process to a single ground-truth model. Instead, OptiMUS focuses on verifying that the output solution is valid. As we discussed in Section 3.4 of the paper, we use a human-written testing script to benchmark each instance. An instance is solved *only if* the solution passes the human-written test. Essentially, if the model does not correspond to the actual problem, the tests will fail.\n\n2. Reviewer\u2019s concern: comparison to previous work\n\n   We thank the reviewer for pointing out this valuable branch of missing references, and we have included discussions in our revision. Compared to using LLMs to extract a mathematical model, using a structured system can bring great stability, accuracy, and explainability. But such systems are also less flexible. OptiMUS can accept any natural language input, whereas the automated constraint acquisition literature requires a user to select from a library of constraints. \n\n3. Reviewer\u2019s concern: Broken references\n\n   Thank you for pointing this out. We fixed it in our revision.\n----\n**Questions**\n\n1. Reviewer\u2019s question: How was the ground truth for experiments obtained?\n\n   We obtain the optimal value of each instance either from textbook solution manuals or by manually modeling and solving the instance. Each instance has a supervised testing script that guarantees the validity of the solution. We added more details to section 4 (dataset) in the revised version.\n\nWe hope that our response can address the concern of the reviewer, and thanks again for your constructive reviews.\n\n----\n**References**\n\n[1] Beldiceanu, N., & Simonis, H. (2012, October). A model seeker: Extracting global constraint models from positive examples. In *International Conference on Principles and Practice of Constraint Programming* (pp. 141-157). Berlin, Heidelberg: Springer Berlin Heidelberg."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275260219,
                "cdate": 1700275260219,
                "tmdate": 1700275260219,
                "mdate": 1700275260219,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H4STNmf3nx",
                "forum": "2FAPahXyVh",
                "replyto": "QoyLzJJ33B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification. I completely agree that there are multiple ways to model a problem and this is not my concern here. Could you elaborate on what the human-written tests scripts check? Do they check the value of the objective function for the returned solution, or do they perform a \"semantic\" check to ensure that the solution satisfies the original problem?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500342904,
                "cdate": 1700500342904,
                "tmdate": 1700500342904,
                "mdate": 1700500342904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iE52dNbqcb",
                "forum": "2FAPahXyVh",
                "replyto": "NoudL3ZiDT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks -- what exactly does this semantic check look like? In particular, how do you ensure that the solution is actually a solution for the original problem, and how do you get the ground truth for what is a solution?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504637728,
                "cdate": 1700504637728,
                "tmdate": 1700504637728,
                "mdate": 1700504637728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hO6QBa0nqf",
                "forum": "2FAPahXyVh",
                "replyto": "OOiY3PV98T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_Bafe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you, this answers my question and addresses my main concern. I'll revise my review accordingly."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506321697,
                "cdate": 1700506321697,
                "tmdate": 1700506321697,
                "mdate": 1700506321697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JiwxKgACtP",
            "forum": "2FAPahXyVh",
            "replyto": "2FAPahXyVh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6499/Reviewer_GUNL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6499/Reviewer_GUNL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes using large language model (LLM) to automatize optimization modeling and solving: formulate MILP from natural language description, generate the solver code and test the output. A package OptiMUS is developed, along with a new dataset NLP4LP as benchmark set. The experiments demonstrate the potential of large language model to help model and solve optimization problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work poses an intriguing question: how can large language model help optimization modeling and solving, and makes an initial exploration on this topic. The paper demonstrates the potential of LLM to assist modeling and generating solver code. Compared with direct prompting as baseline, the augmentations developed, such as iterative debugging, are demonstrated to effectively increase the execution and success rates."
                },
                "weaknesses": {
                    "value": "- Though the question posed is interesting, the contribution of this work is less significant. Basically, OptiMUS leverages LLM to generate the mathematical formulation and test code from the SNOP and to iteratively debug the solver code. If so, I'm afraid this work is kind of engineering without much novelty.\n- The experiments are far from exhaustive. In the paper, only execution and success rates are reported but there are many aspects of solving optimization problems to discuss. See Questions for more detail on this point.\n- The instances in the benchmark set NLP4LP are collected from optimization textbooks and lecture notes. They are more like toy examples (which means they are not representative of real problems) and typically with small sizes. And conclusions drawn on 40 instances can be unreliable."
                },
                "questions": {
                    "value": "Major comments: I do have several concerns on OptiMUS from the optimization solving perspective.\n\n- If I understand correctly, the mathematical formulation of the problem is generated from the SNOP provided by users. I'm wondering how to guarantee the correctness of the mathematical formulation. Moreover, the test code is also generated by LLM. It could happen that LLM misunderstands the SNOP, generates the wrong mathematical formulation and corresponding wrong test script. In such case the solver solves a wrong problem but passes the test. Is OptiMUS able to detect and circumvent such scenarios?\n- Typically, modern solvers have many parameters and options to set. Different options can generate very different outputs. Is the output of OptiMUS stable or not?\n- As mentioned in the paper, OptiMUS checks correct formatting and constraint satisfaction, namely feasibility. Does OptiMUS check optimality of the output?\n- The instances in NLP4LP are collected from classic optimization textbooks and lecture notes. There are chances that these materials are included in the training data of LLM, which makes the numerical results here less convincing. Moreover, these instances can be very different with real problems and they have relatively small scale. My suggestion is that experiments on real-world problems should be conducted to further demonstrate the effectiveness of OptiMUS.\n\nOther comments: \n- I find it difficult to read Figure 5. Why are success rates always higher than the execution rates? In other words, what is the definition of success rate? In Page 8, the authors write ``success rate (the ratio of outputs satisfying all constraints and finding the optimal solution)\". Is it the ratio of success number and all instances, or the ratio of success number and execution count?\n- In the abstract, there is a typo \"MLIP\" which should be \"MILP\".\n- The references to the figures in the manuscript (without appendix) need double-check. For example, there are some missing references in page 7.\n- I'm also concern about the reproducibility of the results because the output of LLM can sometimes be irreproducible. Can the authors comment on this point?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713764253,
            "cdate": 1698713764253,
            "tmdate": 1699636728766,
            "mdate": 1699636728766,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "78w5uiIeCa",
                "forum": "2FAPahXyVh",
                "replyto": "JiwxKgACtP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer GUNL"
                    },
                    "comment": {
                        "value": "**Response to reviewer GUNL**\n\nWe express our gratitude to the reviewer for their helpful feedback. \n\n----\n**Weaknesses**\n\n1. Reviewer\u2019s concern: The contribution of this work is not significant enough.\n\n   We refer the reviewer to Section **Novelty** in our global response. \n\n2. Reviewer\u2019s concern: The experiments are not exhaustive, only execution and success rates are reported.\n\n   We conducted more experiments and included them in the revised version. We will further address the reviewer's questions in the **Questions** section of the response.\n\n3. Reviewer\u2019s concern: instances in the benchmark set NLP4LP are collected from optimization textbooks, and are toy examples. Conclusions drawn on 40 instances can be unreliable.\n\n   The dataset includes a variety of instances, from toy examples to real-world and industry problems (please see the **Novelty** section of our global response for more details). \n\n----\n**Questions**\n\n1. Reviewer\u2019s question: Correctness of mathematical formulation. Are error metrics reliable?\n\n   Our evaluation of OptiMUS ensures that only correct solutions are counted as solved by ensuring a proposed solution passes human-supervised tests and matches the ground truth optimal value. In practice, we imagine domain experts using OptiMUS would use LLM-generated testing scripts as a starting point to write supervised tests, saving time compared to writing tests (or solver code) directly.\n\n2. Reviewer\u2019s question: How does OptiMUS set internal solver parameters? Is the output of OptiMUS stable?\n\n   Indeed, modern optimization solvers do have many parameters to set that control heuristics that are used internally and can affect the speed of finding the solution. However, one of the glories of standard optimization solvers is that they are guaranteed to find a solution if one exists. OptiMUS uses default parameters. Setting these parameters better to minimize solve time is an interesting problem that has a large literature. We have discussed the stability of OptiMUS in response to question 7.\n\n3. Reviewer\u2019s question: Does OptiMUS check optimality of the output?\n\n   In our evaluations, we do check for optimality of the output (Please see answer to question 1). However, in practice, it is not possible to check for optimality until we have solved the problem.\n\n4. Reviewer\u2019s question: The instances in NLP4LP are from textbooks. Chance of being in the training data; Instances can be very different from real problems and they have relatively small scale. \n\n   It is possible that the textbooks appeared in the training set of GPT4. However, \n\n   a) the SNOPs do not appear in the training dataset, as they were developed more recently (by the present authors)\n\n   b) Code solutions to these problems do not exist in the textbooks (or, to our knowledge, on the internet). Moreover, as discussed in the **Novelty** section of our global response, our dataset does include real-world problems.\n\n5. Reviewer\u2019s question: Why are success rates higher than execution rates? \n\n   Apologies for the confusion here. We swapped two legends of Figure 5 by mistake. We have corrected the mistake in our revision.\n\n6. Reviewer\u2019s question: missing references and Latex issues\n\n   Thank you for pointing these out. We have fixed them in the revised version\n\n7. Reviewer\u2019s question: Can the authors comment on reproducibility?\n\n   In our experiments, we use the \"temperature\" parameter of LLM to control the stability of output. Setting the temperature to 0 improves stability but reduces performance, as OptiMUS uses the diversity of augmentations to improve success rate. However, since we are calling APIs, the output in general will be different when the underlying LLM changes. We do not find this source of variability to have had a significant impact during the period since submission of this paper.\n\nThanks again for your valuable feedback. All the concerns posed by the reviewer have been thoroughly addressed. If the reviewer find our subsequent response and revisions satisfactory, we kindly ask for an updated evaluation of our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275258360,
                "cdate": 1700275258360,
                "tmdate": 1700275258360,
                "mdate": 1700275258360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c5YPEcSWe0",
            "forum": "2FAPahXyVh",
            "replyto": "2FAPahXyVh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6499/Reviewer_QHcC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6499/Reviewer_QHcC"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackled a challenging task of helping the modeling part of optimization problems via LLMs: they try to collect data, give some solid examples and prompting concepts, and show the performance of using the concept."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Well-motivated problem and explanations of the paper content.\n- The dataset collection for the challenging task with the SNOP (structured natural-language optimization problem), with experimental evaluations of several aspects (GPT-3.5 and GPT-4, some ablation)."
                },
                "weaknesses": {
                    "value": "- Although the LLM-based framework has good performance, the contributions in this paper seem to be experimental findings (rather than some new methods or theoretical analytics)."
                },
                "questions": {
                    "value": "- Please clarify or comment on the two metrics: success rate and execution rate. At first glance, the success rate seems to include execution rates (i.e., successes are only achievable when executable). Is this correct? In addition, in Fig. 5 of CPT4 + Prompt + Debug + Supervised Test, the execution rate coincides with the success rate. This bar is completely different from others. So, the authors are better to give some explanations (or intuitions).\n- Some basic questions in the pipeline to follow the concept of OptiMUS:\n    - In each part involving LLMs (e.g., the formulation in markdown, code generations, test generations), do LLMs (i.e., GPT-3.5, GPT-4) always succeed? Give some errors in practice.  Of course, I can believe that they `can` do them, but I\u2019m interested in how we can believe their outputs and how often we should take care of them in the pipeline.\n- Minor comments\n    - I\u2019m not exactly sure the reason, but some LaTeX links are not correctly inserted (some points are ??, maybe related to the appendix link). They should be fixed for readability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698715428888,
            "cdate": 1698715428888,
            "tmdate": 1699636728648,
            "mdate": 1699636728648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B5UZo9GbXa",
                "forum": "2FAPahXyVh",
                "replyto": "c5YPEcSWe0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer QHcC"
                    },
                    "comment": {
                        "value": "**Response to reviewer QHcC**\n\nWe thank the reviewer for the constructive suggestions.\n\n----\n**Weaknesses**\n\n1. Reviewer\u2019s concern: The contributions in this paper seem to be experimental findings (rather than some new methods or theoretical analytics)\n\nWe respectfully refer the reviewer to the **Novelty** section in our global response.\n\n----\n**Questions**\n\n1. Reviewer\u2019s question: Success rate and execution rate labels are confusing\n\n   We are sorry for the confusion here. The legend labels were swapped by mistake. We have fixed this error in the revised version and we kindly refer the reviewer to the **Clarification** section in our global response.\n\n2. Reviewer\u2019s question: Do LLMs always succeed in different parts of the pipeline?\n\n   We appreciate the great question. Here is a more detailed list of how and where failures happen in our tests:\n\n   -  Modeling: 3 instances where the optimization model was missing constraints or had incorrect constraints.\n   - Unit mismatch: 1 instance where the output\u2019s unit (cost per click) was different from the desired unit (cost per 1000 clicks). This could be resolved by clarifying the unit in the objective.\n   - Code omission: 1 instance where the model simply replaced some parts of the code with \u201c...\u201d during revision, resulting in an incorrect output\n   -  Debugging failure: 3 instances where the model was not able to debug the code and make it executable\n   -  Output format: 1 instance where the format of the generated output JSON did not match the desired format presented in the SNOP \n\n3. Reviewer\u2019s question: Latex link issues\n\n   Thank you for pointing this out. We fixed the links in the revision.\n\nThanks again for reading the paper and providing us with valuable feedback. We have addressed all of the reviewer\u2019s questions and listed our methodological and scientific contributions in our global response. If the reviewer is satisfied with our response and the revision, we respectfully request that the reviewer update the score of this paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275255513,
                "cdate": 1700275255513,
                "tmdate": 1700275255513,
                "mdate": 1700275255513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fPdIk1Louc",
                "forum": "2FAPahXyVh",
                "replyto": "B5UZo9GbXa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_QHcC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6499/Reviewer_QHcC"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your responses"
                    },
                    "comment": {
                        "value": "I appreciate the feedback from the authors.\n\nTo be honest, they answered my questions and partially clarified my concerns (e.g., explanations of experiments). However, I will keep my score because I have some concerns about the novelty and contributions (i.e., a new paragraph written in blue and some new figures) after the revision. Particularly, NLP4LP and the improvements are interesting, but they seem to be particularly new results, so we should carefully review and discuss them in my personal view."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6499/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483985171,
                "cdate": 1700483985171,
                "tmdate": 1700483985171,
                "mdate": 1700483985171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]