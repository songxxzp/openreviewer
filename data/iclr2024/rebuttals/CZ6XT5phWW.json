[
    {
        "title": "Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance"
    },
    {
        "review": {
            "id": "Dsarncg8NW",
            "forum": "CZ6XT5phWW",
            "replyto": "CZ6XT5phWW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8350/Reviewer_bonY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8350/Reviewer_bonY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the quailty of prompts and how to improve them for better capabilities of zero-shot and few-shot in-context learning. To ahieve this, authors first define the characteristics that good prompts should have, and then proposes a method to rewrite prompts to improve their quality based on this. Effectiveness is evaluated on mathematical reasoning, code generations and those tasks in BigBench."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think the tageted issue, i.e., the quality and improvements of prompts, is currently needed by both the industrial and academic communities. Improving the zero-shot or few-shot in-context learning ability of large-scale models by improving the quality of prompts still has extremely high research value in the short term. This work could be viewed as a start point for this aspect.\n\nHowever, we have to realize that the research space in prompt engineering also reflects the shortcomings of the current large models. The improvement of large models in the near future will be reflected in their stronger robustness to prompts. I suggest researchers to look at this issue with a more long-term developmental perspective, instead of being satisfied with the immediate results."
                },
                "weaknesses": {
                    "value": "I very much agree with the starting point of this article, but at the same time, I regret that this research work lacks the necessary depth. From the definition of the quality of prompts to the method of improving the quality of prompts, most of the content is confined to quantitative analysis, lacking more in-depth and specific method design. There is also no larger scale quantitative evaluation on more general downstream tasks. For readers, it is somewhat difficult to catch the technical insights and contribution so that the current version seems premature."
                },
                "questions": {
                    "value": "Could you further clarify how you rewrite the prompts and clearly hightlight the insights in them?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830244304,
            "cdate": 1698830244304,
            "tmdate": 1699637038714,
            "mdate": 1699637038714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ESsuR1yObN",
                "forum": "CZ6XT5phWW",
                "replyto": "Dsarncg8NW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bonY"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for acknowledging the value of our work and suggesting the long-term developmental perspective about LLMs\u2019 robustness to prompts! We agree with the reviewer and found it an important direction to pursue. \n\n\n**Regarding the reviewer\u2019s suggestion on improving LLMs\u2019 robustness to prompts as future direction**\n\nWe agree with the reviewer on the importance of improving LLMs\u2019 robustness to prompts. In fact, this is also our motivation for this work. In our submission, we contribute to this aim by rewriting prompts that can be executed across a diverse set of tasks on different models. That is, instead of improving the robustness nature of an individual LLM, we approach this \u201crobustness to prompts\u201d by bridging the gap between human-written prompts (which could be suboptimal) and the LLM-preferable prompts through PROMPTD\u2019s optimization. We did three experiments on our proposed method\u2019s capability on the three points: robustness to unseen tasks, adaptability on weaker LLMs, and sustainability to adversarial prompting through the interpretability of PROMPTD.\n\n\nWe show that PROMPTD could rewrite test instances with specialized instructions and hints across unseen task types such as implicit reasoning, symbolic reasoning, and sentiment classification. For adaptability, we show the efficacy of our approach with evolving LLMs, including GPT-3.5-turbo, and LLAMA2 70B. Finally, we present a use-case of PROMPTD on its applicability to avoid adversarial prompting because of its interpretable approach to rewriting. PROMPTD not only aligns with the current trajectory of LLM development but also actively contributes to enhancing their robustness to a diverse set of prompts aligning with the future direction suggested by the reviewer. \n\n\n**Clarification on how to rewrite prompts and their insights**\n\nThank you for seeking further clarification on PROMPTD's methodology and insights. In summary, PROMPTD improves the zero-shot performance of LLMs by tailoring prompts to each test instance based on criteria such as specificity and structure, as demonstrated through our extensive experiments. This approach not only enhances clarity and interpretability but also showcases PROMPTD's adaptability across diverse task types.\n\n\n**Rewriting Criteria:** PROMPTD employs specificity, non-ambiguity, completeness, and structuredness as its rewriting criteria, ensuring each prompt is optimally customized for its corresponding test instance.\n\n**In-Context Learning for Rewriting:** Using a few-shot in-context learning method, PROMPTD contrasts good and bad prompts to guide the LLM in generating better prompts.\n\n**Instance-Level Customization:** Each test instance is individually considered, allowing PROMPTD to address unique requirements effectively, contrasting with generic, one-size-fits-all prompts.\n\n\nWe picked the ten rewriting demonstrations for PROMPTD based on the most frequently researched task types such as Mathematical Reasoning, Logical Reasoning, Code Generation, and Instruction Induction to probe LLMs. Since we aim to improve on broad task types such as general queries to chatGPT, we also included content generation prompts. The intuition behind including such diverse tasks in demonstrations was to cover the diverse shortcomings of different task types and generalize to unseen task types. Moreover, we hypothesize that different task types could learn from different errors and when reasoning over a test instance they can learn from both the similar types of tasks and general content generation tasks. Guiding through in-context learning examples and harnessing the text generation capabilities of LLMs enables us to not only identify the limitations but also add instance-specific hints and examples in the rewritten prompts.\nWe also pick several examples from the Instruction Induction task that would encourage LLMs to learn from input-output pairs and generate examples. As shown in Table 5, for the sentiment classification task, PROMPTD was able to generate several examples to learn the task. Finally, for some task types such as Code Generation and Content Generation, we refrain from generating a structured output such as \u201cThe answer is \u2026\u201d enabling LLMs to learn when a structured output is required.\nEach of these prompt types addresses a unique set of challenges in prompt rewriting. Together, they provide a comprehensive training ground for the model to learn a wide array of skills necessary for generating high-quality, instance-specific prompts. These diverse examples enable the model to generalize these skills across tasks, leading to better overall performance and adaptability to a range of unseen task types."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451424458,
                "cdate": 1700451424458,
                "tmdate": 1700451424458,
                "mdate": 1700451424458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cRcXxgY8C9",
                "forum": "CZ6XT5phWW",
                "replyto": "ESsuR1yObN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8350/Reviewer_bonY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8350/Reviewer_bonY"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for the response. After reading the reviews and responses, I tend to keep my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718884594,
                "cdate": 1700718884594,
                "tmdate": 1700718884594,
                "mdate": 1700718884594,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "S4dLMQ1Ig1",
            "forum": "CZ6XT5phWW",
            "replyto": "CZ6XT5phWW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8350/Reviewer_xGQ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8350/Reviewer_xGQ6"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses a significant challenge in the application of large language models (LLMs) to zero-shot tasks \u2013 the design of task prompts that are sufficiently informative and unambiguous to guide the model to the correct solution without task-specific annotations. The authors propose PROMPTD, an innovative approach that generates customized prompts for each test instance by designing some pre-defined prompts, enhancing the LLM's ability to handle tasks across various domains including arithmetics, logical reasoning, and code generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of dynamically rewriting prompts for individual instances is a novel and interesting approach that represents a significant departure from the more static strategies employed in prior works.\n\n2. The reported results indicate that PROMPTD provides a substantial boost in performance, achieving an improvement of 10% on the MATH dataset and 5% on the code generation tasks in HumanEval, which is impressive and suggests that the approach has practical value.\n\n3. By applying PROMPTD across eight different datasets, the authors demonstrate the method's general applicability, an essential characteristic for real-world deployments.\n\n4. The paper presents an additional benefit of using PROMPTD \u2013 the rewritten prompts not only aid in task resolution but also enhance the interpretability of the LLM's decision-making process, which could be crucial for trust and reliability in AI systems."
                },
                "weaknesses": {
                    "value": "1. The paper does not sufficiently discuss the computational overhead of the PROMPTD method. Since the approach involves generating custom prompts for each instance, there may be a significant increase in the computational cost that could limit its scalability. More importantly, the PROMPTD is quite long; can the authors make some ablation studies about it?\n\n2: The efficacy of PROMPTD is likely highly dependent on the initial quality of the prompts it is based upon. The paper could better address how the system performs with suboptimal base prompts and the robustness of the method to variations in prompt quality.\n\n3: While the performance improvements are impressive, the evaluation might benefit from a deeper analysis of where and why the approach fails. Understanding the limitations of PROMPTD is as important as understanding its strengths.\n\n4: There have been so many zero-shot prompting methods recently. The paper would be strengthened by including a more comprehensive comparison with the recent state-of-the-art methods for zero-shot learning.\n\n\n5. The evaluation of PROMPTD on a single new task type (sentiment classification) is a significant limitation. Given the length and complexity of the original PromptD, it is unclear how well the method would generalize to other task types. The reviewer's expectation of a more general prompt applicable to a wide array of task types is unmet. This is a critical aspect, as the creation of highly specialized prompts may not be feasible in many real-world applications."
                },
                "questions": {
                    "value": "Same as before"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698995967130,
            "cdate": 1698995967130,
            "tmdate": 1699637038613,
            "mdate": 1699637038613,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kr7ORzr2kI",
                "forum": "CZ6XT5phWW",
                "replyto": "S4dLMQ1Ig1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xGQ6"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s acknowledgment of PROMPTD on its novelty to dynamically rewrite prompts and recognition of its interpretability to perform prompt optimization for zero-shot approaches.\n\n**W1: Computational overhead of PromptD**\n\nWe refer the reviewer to our general response to this comment.\n\n\n**W2: Reliance on Prompt Quality and Robustness**\n\nWe clarify that PROMPTD does not rely on the initial quality of the prompts it is based upon. We note that, as we focus on zero-shot prompting, the \u201cbase prompt\u201d in our experiments is simply the task instruction from the original test benchmarks (see examples in Table 2), and we make no assumption on its quality.\n\n\nOur approach PROMPTD does not assume optimal \u201cgood prompts\u201d in prompt rewriting either. Instead, PROMPTD relies on its ability to reason over the shortcomings of a given \u201cbad prompt\u201d and overcome them in rewriting. This ensures robust performance even with suboptimal starting prompts, as demonstrated across various tasks and datasets, including unseen ones.\n\n\n\n\n**W3: A deeper analysis of where and why the approach fails**\n\nWe refer the reviewer to the general discussion on in-depth analysis.\n\n**W4: Comparison with other zero-shot approaches**\n\nThank you for your comment regarding the comparison with other zero-shot approaches. Indeed, the approaches for zero-shot learning in LLMs are rich and rapidly expanding. To address your point, we have identified several recent papers that present innovative zero-shot techniques. Wang et al., 2023 introduced a two-step approach that plans out subtasks and then executes the plans to derive the final answer. On the contrary, our work focuses on enriching prompts with hints and encouraging LLMs to generate rationales before deriving the final answer. Reynolds and McDonell2021 suggested a meta-prompt \u201cLet's solve this problem by splitting it into steps.\u201d to facilitate multi-step reasoning in simple arithmetic problems. However, they did not evaluate quantitatively on diverse reasoning tasks against baselines. In this paper, we evaluate PROMTPD on a diverse set of tasks beyond reasoning. Wei et al., 2021 proposed instruction-tuning which gives models an instruction describing a task while with PROMPTD, on the other hand, instead of tuning an instruction we aim to optimize task-specific instructions and test instances. Recent approaches, similar to PROMPTD,  have also proposed to optimize task-level instructions (Hong et al., 2023) enriching task-specific hints. Through experiments, we show that such task-specific instruction optimization does not lead to stable performance across different tasks motivating the need for instance-level prompt optimization. Huang et al., 2022 show that reasoning performance improves by multi-task finetuning a model on several self-generated synthetic CoT datasets. We, on the other hand, do not perform finetuning and in addition to mathematical and logical reasoning, also focus on content generation and code generation tasks.\n\n\n**References:**\n\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim \"Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.\" In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.\n\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, & Jiawei Han. (2023). Large Language Models Can Self-improve. International Conference on Learning Representations. 2023.\n\nSun, Hong, et al. \"AutoHint: Automatic Prompt Optimization with Hint Generation.\" Workshop on Foundations and Applications in Large-scale AI Models -Pre-training, Fine-tuning, and Prompt-based Learning, colocated with the 29TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD), August 6-10, 2023, Long Beach, CA, USA\n\nWei, Jason, et al. \"Finetuned Language Models are Zero-Shot Learners.\" International Conference on Learning Representations. 2021.\n\nLaria Reynolds and Kyle McDonell. 2021. Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (CHI EA '21). Association for Computing Machinery, New York, NY, USA, Article 314, 1\u20137. https://doi.org/10.1145/3411763.3451760"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450550390,
                "cdate": 1700450550390,
                "tmdate": 1700450550390,
                "mdate": 1700450550390,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IwRD7HJ9gW",
            "forum": "CZ6XT5phWW",
            "replyto": "CZ6XT5phWW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose to rewrite a specific prompt for each test point by prompting GPT4 with demonstrations that show how to rephrase a bad prompt into a better one with a rationale and task type. With the refined prompt, we can achieve better zero-shot performance on several benchmark datasets than other relevant baselines including zero-shot Chain-Of-Thought. Moreover, such prompt rewriting method generalizes to refine tasks that are not included the demonstrations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is simple and effective. It rewrite a prompt into clear, specific, complete and more structure prompts, which leads to improved performance.\n\n- For rewriting a prompt, we need 10 demonstrations, which is really practical for real-world applications.\n\n- The authors performed human evaluation to verify that the quality of the rewritten prompts becomes better.\n\n- The authors performed ablation study to show that task type and reasons are crucial component for prompt rewriting."
                },
                "weaknesses": {
                    "value": "-  Compared to zero-shot prompt models, it requires extra forward pass of LLMs to rewrite a prompt. It would be better to show how much more computational cost is required than other baselines.\n\n- It is not clear such GPT4 written prompt would be transferred to other LLMs such as Llama."
                },
                "questions": {
                    "value": "Please see weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8350/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8350/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8350/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699450477930,
            "cdate": 1699450477930,
            "tmdate": 1700702362903,
            "mdate": 1700702362903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vCjw3PSwK2",
                "forum": "CZ6XT5phWW",
                "replyto": "IwRD7HJ9gW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8350/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8350/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vep9"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s recognition of the simplicity and applicability of PROMPTD on real-world applications and the recognition of experiments performed to study the efficacy of our approach.\n\n**Computational Cost**\n\nWe refer the reviewer to our general response to this comment.\n\n\n**Transferability to Other LLMs such as LLAMA**\n\nThank you for raising this good question! We investigated the effect of applying rewritten prompts from PROMPTD to three other task LLMs, including GPT-3.5-turbo, LLAMA2 70B, and LLAMA 2-13B. Due to the resource and time limitation during rebuttal, the experiments were conducted on a subset of 250 randomly sampled questions from GSM8k, and LLAMA2 70B was implemented using together AI (https://www.together.ai/). \n\n\n|               | Zero-Shot | Zero-Shot-CoT | PROMPTD |\n|---------------|-----------|---------------|---------|\n|   LLAMA-13B   |   6.500   |     **11.600**    |  4.800  |\n|   LLAMA-70B   |   7.600   |     13.400    |  **14.800** |\n| GPT-3.5-turbo |   65.200  |     68.400    |  **71.600** |\n\n\nOur results showed that the rewritten prompts of PROMPTD can enhance GPT-3.5-turbo\u2019s performance consistently. However, there\u2019s only a slight improvement for LLAMA2 70B and a reduced effect for LLAMA2 13B. We note a potential implementation issue with LLAMA2 70B due to the restriction from using together AI\u2019s API, which we hope to figure out in the remaining rebuttal period or post-rebuttal. However, the current results have revealed interesting observations that the rewritten prompts of PROMPTD can only be effective when the task LLM itself has a strong enough capability of language understanding and instruction following. To overcome it, future work could improve PROMPTD to take into account the performance of the downstream task LLM or consider fine-tuning the task LLM for better instruction following or understanding."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450350643,
                "cdate": 1700450350643,
                "tmdate": 1700450366445,
                "mdate": 1700450366445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3qh1hzPB5i",
                "forum": "CZ6XT5phWW",
                "replyto": "vCjw3PSwK2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Authors"
                    },
                    "comment": {
                        "value": "Thanks for answering the question. The results look a bit disappointing and show limitation of the proposed method since not everyone can access to large language models and many of them resort to relatively small open source language models such as LLAMA. If the authors cannot provide reasonable justification for this, I am inclining to decrease the score to 5."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631686630,
                "cdate": 1700631686630,
                "tmdate": 1700631686630,
                "mdate": 1700631686630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4WbyMbBwmw",
                "forum": "CZ6XT5phWW",
                "replyto": "iyoysEc3rq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8350/Reviewer_vep9"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thank you for your response, but I remain unconvinced that the additional results do not highlight limitations of the proposed method. Instead, they suggest a need for careful tuning of the method for each language model with different sizes, aligning with the authors' claim that it is designed for models like GPT-3.5. Additionally, there is a failure to tune the method for the relatively smaller language model LLAMA-2-7B, resulting in a performance degradation. Therefore, I am adjusting the rating to 5."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8350/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702344841,
                "cdate": 1700702344841,
                "tmdate": 1700702344841,
                "mdate": 1700702344841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]