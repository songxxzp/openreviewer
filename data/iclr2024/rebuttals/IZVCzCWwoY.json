[
    {
        "title": "Completion Consistency for Point Cloud Completion Enhancement"
    },
    {
        "review": {
            "id": "ruVk1GuPFl",
            "forum": "IZVCzCWwoY",
            "replyto": "IZVCzCWwoY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2420/Reviewer_uAuJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2420/Reviewer_uAuJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the task of point cloud completion (PCC). Due to the ill-posed nature of CD distance, the current training signal for PCC could be noisy and contradictory, which hinders the resulting completion quality.\n\nThe authors thus propose a consistency loss such that a many-to-many loss calculation is established. For each prediction, it is evaluated against a set of ground truth complete point clouds; and for each ground truth complete point cloud, it is also evaluated with a set of predictions. The consistency among these computed losses is encouraged."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written.\n\nThe proposed method is simple and is demonstrated to be effective to some extent."
                },
                "weaknesses": {
                    "value": "Regarding the analysis of the consistency loss at the end of Sec.3, I think it is not clear and could be introduced in more detail. Specifically, the authors claim that \"The effect of the contradictory supervision signal to the gradient descent step can then be suppressed by $L_{k}^{c-{tg}}$ and $L_{k}^{c-{s g}}$\", while I think the two losses $L_{k}^{c-{tg}}$ and $L_{k}^{c-{s g}}$ might provide very close constraints. And the results in Tab. 6 could also prove that, such as Row#2 v.s. Row#5 and Row#3 v.s. Row#4. We can observe that the weight of $L_{k}^{c-{s g}}$ should be small, if $L_{k}^{c-{tg}}$ and $L_{k}^{c-{s g}}$ have the same weight, the performance even drops. I think more explanations are required. Otherwise, the motivation behind the solution is not clear. How the method suppresses such signals in practice should be analyzed more, such as from the perspective of gradients."
                },
                "questions": {
                    "value": "- Please give more analysis about the consistency loss, such as from the perspective of gradients.\n\n- Please provide more analysis about the performance comparisons in Tab. 6.\n\n- Please explain the differences and similarities between the two losses $L_{k}^{c-{tg}}$ and $L_{k}^{c-{s g}}$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698501102532,
            "cdate": 1698501102532,
            "tmdate": 1699636177539,
            "mdate": 1699636177539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "SmtN7X7MSy",
            "forum": "IZVCzCWwoY",
            "replyto": "IZVCzCWwoY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2420/Reviewer_CxXc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2420/Reviewer_CxXc"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of one-to-many mapping in point cloud completion networks, where an incomplete point cloud can have multiple valid completion solutions. This issue can lead to contradictory supervision signals during training, potentially hindering network optimization. To tackle this, the authors propose a novel completion consistency loss that enforces the generation of a consistent completion solution for incomplete objects from the same source point cloud. Experimental results across different datasets and benchmarks showcase the effectiveness of this approach in improving the completion performance of existing networks without requiring modifications to their design."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well organized.\n2. The motivation of the paper is important and interesting."
                },
                "weaknesses": {
                    "value": "1. In Tables 4 and 5, the proposed loss functions are plugged into several methods but not all of them. Is there a performance gain for the rest of the methods?\n2. Does the marginal performance improvement of AdaPointTr imply that the current state-of-the-art models have somehow implicitly learned to handle ambiguity in supervised signals, proving the proposed losses unnecessary?\n3. The primary contribution of the paper lies in the proposed loss functions, which have not been conclusively demonstrated to be highly effective. Therefore, it is believed that the paper has not met the acceptance bar of ICLR."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632363744,
            "cdate": 1698632363744,
            "tmdate": 1699636177464,
            "mdate": 1699636177464,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "vwGRtrWZeD",
            "forum": "IZVCzCWwoY",
            "replyto": "IZVCzCWwoY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2420/Reviewer_MC7Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2420/Reviewer_MC7Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for point cloud completion using a novel completion consistency loss to mitigate the issue that one-to-many mapping can cause contradictory supervision signals for point cloud completion training. More specifically, the authors propose a self-guided consistency and a target-guided consistency to ensure that the network generates a consistent completion for incomplete objects. To verify the effectiveness, extensive experiments are conducted, and promising results are achieved."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clearly written and well organized.\n2. Extensive experiments are tested together with promising performances."
                },
                "weaknesses": {
                    "value": "1. It is highly encouraged that the experiments should cover the widely used scenario to predict the whole completion points instead of only predicting the missing parts, since in most cases, the input parietal points itself could be quite noisy and inaccurate. Hence, predicting the whole point sets can provide more accurate and evenly distributed outputs. \n2. The paper aims to solve the one-to-many issue, but it seems that self-guided consistency is designed as a many-to-one mechanism. Hence, how does the many-to-one approach help the one-to-many issue? Moreover, self-guided consistency seems not novel, and it has been studied in [A] before.\n3. It seems that a SOTA method [B] in ShapeNet55 and ShapeNet34 is missing. Hence, it would be great if authors could provide experimental results based on [B] as well to show the effectiveness of the method.\n\n[A] Gu, Jiayuan, et al. \"Weakly-supervised 3D shape completion in the wild.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16. Springer International Publishing, 2020.\n[B]Chen, Zhikai, et al. \"AnchorFormer: Point Cloud Completion From Discriminative Nodes.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "questions": {
                    "value": "1. How are the results in Table 2 obtained? Are the CD values calculated from the whole point set (both partial and missing parts)? If so, it is obvious that predicting the missing points alone is better than the other one, since there is always a subset of points that has zero CDs. It would be better to also show the qualitative comparisons to support the authors\u2019 assumption.\n2. Why the PCN result (10.55) in Table 4 is different from the original paper\u2019s (9.636)?\n3. Why is AXFormNet not tested on the PCN dataset in Table 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2420/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2420/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2420/Reviewer_MC7Y"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679069457,
            "cdate": 1698679069457,
            "tmdate": 1699636177385,
            "mdate": 1699636177385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "VdhmZU9wFl",
            "forum": "IZVCzCWwoY",
            "replyto": "IZVCzCWwoY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2420/Reviewer_3ARx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2420/Reviewer_3ARx"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a consistency constraint to address the one-to-many mapping issue in the point cloud completion task. In order to mitigate this problem, the authors propose a consistency loss that consists of a Self-guided Consistency loss and a Target-guided Consistency loss, which serve as guiding factors during the training process. However, the novelty of the proposed method is limited, as the authors merely observe the one-to-many mapping issue without providing a solution. Furthermore, the logic and coherence of the manuscript are lacking, and the arrangement of the content is unreasonable. In the experimental section, although the combination of the proposed consistency constraint with existing methods leads to improved performance, there are issues with the experimental settings and evaluation metrics."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I can't see obvious strengths."
                },
                "weaknesses": {
                    "value": "1)\tThe novelty is limited.\n\n2)\tThe arrangement of the manuscript is unreasonable.\n\n3)\tSome claims are casual and cannot be proved.\n\n4)\tThe experiments are problematic and insufficient to support the proposed method's efficiency. And some significant experiments are missing."
                },
                "questions": {
                    "value": "1)\tThe novelty is limited. The main contribution of the proposed method is concentrated on the items of Eq. (3); however, the target-guided consistency has been proposed in previous work SeedFormer [1] (see Eq. (7)). \n\n2)\tIn Section 2.3, the authors argue that learning to predict only the missing points can improve completion performance and validate this hypothesis using a single model, i.e., AxForm. However, the results from a single model are insufficient to establish the validity of this inference. Further experimentation with more models is required to support this claim. \n\n3)\tThe consistency loss requires the utilization of n incomplete point clouds for each object. However, this manuscript does not mention how these incomplete point clouds are generated. In datasets like PCN, each object is associated with 8 partial point clouds. Do these partial observations overlap with the incomplete point clouds used for the consistency loss?\n\n4)\tThere is no evidence and theoretical analysis about how the proposed self-guided consistency could solve the one-to-many mapping issue, which is mentioned in Sec.1. The proposed method seems to deal with the many-to-one issue. It is recommended to provide corresponding experiments and results to support this claim.\n\n5)\tThe arrangement of the manuscript is unreasonable. For example, Sec 2.3 should introduce the related works rather than the experiments. Sec 2.3 is suggested to be put in Sec. 1. The motivation of Sec 2.2 is not clear. The proposed method is about loss function instead of the optimizer in training, but Sec 2.2 introduces and carries the experiments about the optimizer.\n\n6)\tSome claims cannot be proved with enough evidence. In the last paragraph in Sec \u201cComplete Loss Function\u201d, why CD(P_{a}^{com}, P_{b}^{com})>>CD(P_{a1}^{inc}, P_{b1}^{inc}) since the CD calculates the average error between point clouds?\n\n7)\tThe evaluation metrics are problematic. The evaluation metric should include F-Score besides CD. \n\n8)\tEvaluating the performance on real-world datasets is crucial to validate the effectiveness of point cloud completion methods. While this paper demonstrates the effectiveness of the proposed method on synthetic datasets, it does not extend the evaluation to real-world datasets such as KITTI.\n\n9)\tWhen combined with the newest frameworks, the increment seems to be unobvious. Does it illustrate that the proposed method is not necessary in these frameworks? \n\n10)\tThere is no ablation study. \n\n11)\tIn Fig.3, the improvement in the \u201cchair\u201d is not obvious. \n\n12)\tThere is no time and GPU memory analysis.\n\n[1] SeedFormer: Patch Seeds Based Point Cloud Completion with Upsample Transformer."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763009962,
            "cdate": 1698763009962,
            "tmdate": 1699636177305,
            "mdate": 1699636177305,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]