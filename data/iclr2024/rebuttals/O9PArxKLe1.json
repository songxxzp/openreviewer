[
    {
        "title": "Leveraging Optimization for Adaptive Attacks on Image Watermarks"
    },
    {
        "review": {
            "id": "7WYeBeluqb",
            "forum": "O9PArxKLe1",
            "replyto": "O9PArxKLe1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2831/Reviewer_2NFv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2831/Reviewer_2NFv"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an objective function and approaches adaptive attacks as an optimization problem to evaluate the robustness of watermarking algorithms.  The core idea of the proposed adaptive attack is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack\u2019s parameters. The experiments reveal that this type of attacker can successfully compromise all five surveyed watermarking methods with minimal degradation in image quality. These findings underscore the necessity for more comprehensive testing of the robustness of watermarking algorithms against adaptive, learnable adversaries."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes a practical method of empirically testing the robustness of different watermarking methods. The proposed adaptive attack is shown to be effective against different watermarking methods."
                },
                "weaknesses": {
                    "value": "1. The attack requires the knowledge of the watermarking algorithm. \n2 in the experiments the surrogate generator and the watermark generator exhibited a high degree of similarity, which may not be as practical in real-world scenarios."
                },
                "questions": {
                    "value": "1 The paper shows that generates a single surrogate key and can evade watermark verification which indicates that the private key seems to have little impact. Is it possible that the less impact comes from the high similarity between the surrogate generator and the watermark generator?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2831/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2831/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2831/Reviewer_2NFv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654033789,
            "cdate": 1698654033789,
            "tmdate": 1699636226875,
            "mdate": 1699636226875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kMBT0Qg4Bv",
                "forum": "O9PArxKLe1",
                "replyto": "7WYeBeluqb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and positive assessment of our paper. Please find detailed responses below. \n\n> The attack requires the knowledge of the watermarking algorithm.\n\nKnowledge of the watermarking algorithm may only sometimes be given in a real-world attack (as outlined in our limitations on page 9). Still, it remains a valuable tool for the provider to _test_ the robustness of a watermarking method. Robustness against adaptive attackers extends to robustness against non-adaptive attackers (as they have fewer capabilities). This forms our core argument for why studying a method\u2019s robustness requires considering adaptive attacks. We will emphasize this argument in the revised paper. \n\n> The paper shows that generates a single surrogate key and can evade watermark verification which indicates that the private key seems to have little impact. Is it possible that the less impact comes from the high similarity between the surrogate generator and the watermark generator?\n\nWe thank the reviewer for this interesting question! Our work shows that an attacker can remove any watermark by generating a single surrogate key. If the KEYGEN procedure were sufficiently randomized, this should not be possible even if the attacker had access to **the same** generator as the defender. Our attacks assume less because we use a _surrogate_ generator that is not the same, because the generator\u2019s abuse by the attacker may otherwise not be well motivated, but show that these restricted attacks are still successful with only a single surrogate key. From these results, we conclude that the KeyGen procedure is not sufficiently randomized, which indicates a design flaw. We will add this to the revised paper\u2019s discussion.  \n\nWe are happy to respond to any further questions or suggestions the reviewer may have."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699894028452,
                "cdate": 1699894028452,
                "tmdate": 1699894028452,
                "mdate": 1699894028452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nc2XU96cKr",
            "forum": "O9PArxKLe1",
            "replyto": "O9PArxKLe1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2831/Reviewer_LPhx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2831/Reviewer_LPhx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for performing adaptive attacks against image watermarking methods, allowing for more accurate evaluations of the robustness of watermarking methods against motivated attackers. To enable standard adversarial optimization attacks against known but non-differentiable watermarking methods, the authors propose to train surrogate (differentiable) watermark detection networks. Experiments show that adaptive attacks crafted with the proposed method significantly degrade the effectiveness of all evaluated watermarking methods and outperform non-adaptive attacks while preserving the overall perceptual quality of attacked images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The issue of watermarking the outputs of generative models is timely and interesting.\n\nThe idea of training differentiable surrogates for arbitrary watermarking methods is an interesting threat model.\n\nThe selection of baseline watermarking methods is reasonable and includes both \"post-hoc\" (low-perturbation) and \"semantic\" (high-perturbation) methods.\n\nThe autoencoder/compression-based attack is interesting and seems to effectively remove watermarks while retaining high perceptual quality."
                },
                "weaknesses": {
                    "value": "I think there is a terminology issue in the paper that could be confusing for readers. It appears the watermark \"key\" referenced in the paper more closely matches the concept of a watermark \"detector\" algorithm in methods such as RivaGAN and Tree-Rings; many methods often use \"key\" and \"message\" interchangeably to refer to the hidden signal. If this is true, the authors' proposed training of differentiable surrogate \"keys\" can be understood as training differentiable surrogate detector networks that predict the key/message concealed in a watermarked image --  allowing for gradient-based optimization attacks on non-differentiable detectors. The pseudocode in Algorithm 1 strongly suggests this. If this is the case, I urge the authors to revise their terminology to make this more clear.  \n  \n- - - -\n\nIt should be clarified that in the general no-box watermarking scenario, the second step $\\mathrm{EMBED}$ need not modify the parameters of the generator model, just endow it with watermarking capabilities (e.g. by applying a post-hoc watermarking algorithm to its outputs). As far as I can tell, none of the methods evaluated in the paper modify the generator parameters directly. Overall, the no-box watermarking steps and their instantiations for each of the evaluated watermarking methods are not clearly explained.\n  \n- - - -\n\nThe proposed method is similar to that of Jiang et al. [5] in that it adversarially parameterizes image transformations to remove watermarks. The authors claim that the method of Jiang et al. requires access to a watermarking \"key;\" however, Jiang et al. propose attacks under the explicit assumption that the attacker does not have access to the ground-truth key/message (which is either approximated via the detector model's predictions or sampled at random) -- from pp.6, \"the attacker does not have access to the ground-truth watermark $w$\".  On the other hand, if I am correct that the authors take \"key\" to mean \"detector,\" this claim makes more sense in that Jiang et al. use full access (white-box) or query access (black-box) to the detector to craft attacks. \n\nThe authors should clarify their statements about the prior work of Jiang et al. and the distinctions between their methods. And while additional experiments may not be feasible at this point for various reasons, I think the paper would be much stronger if it included comparisons between the proposed approach and either or both attack variants proposed by Jiang et al. This would also require modifying the proposed attack to evade two-tailed detection, as discussed by Jiang et al.\n\n- - - -\n\nIf the attacker \"does not need to invoke GKEYGEN\" for TRW/WDM/RivaGAN, as stated in section 4.2, does this mean the attacker does not train a differentiable surrogate detector? In this case, doesn't the proposed approach just become a standard white-box attack, as in Jiang et al.?\n\n- - - -\n\nThe authors do not provide any details of the architecture of the surrogate detector networks $\\theta_D$ trained by the adversary. This seems like a crucial aspect of the proposed approach, so it is strange that it is not discussed.\n\n- - - -\n\nThe related work section mixes references to image classifier watermarks [1][2] and watermarks for generative models [3], which are very different: the former aims to protect the intellectual property of a model developer, typically through query- or trigger-based verification, while the latter is embedded in all outputs of a generative model to distinguish real from fake content. This paper is concerned with the latter kind of watermark, so I'm confused by the emphasis on works in the former area. At the very least, these two different types of works should be clearly distinguished from one another in the related work section.\n\n- - - -\n\nAs far as I can tell, the term \"Adaptive Attack\" comes from the adversarial example literature -- the authors should explain what distinguishes an adaptive attack from a non-adaptive attack and probably cite the original work [4].\n\n- - - -\n\nFigure 1 is missing step #8 (it skips from 7 to 9).\n\n- - - -\n\nThis is a much smaller concern, but the substitution of the Stable Diffusion v1 generator for v2 does not seem like a very difficult obstacle for the attacker to overcome, given the general similarities in architecture and training. Attacks on post-hoc methods probably shouldn't be affected too much by the choice of surrogate generator, but Tree-Rings is deeply intertwined with the generator structure. Therefore, it would be interesting to see how attacks on TRW fare when there is a more substantial mismatch between the actual and surrogate generator.\n\n- - - -\n\nOverall, I think the central idea -- no-box watermark attacks with differentiable surrogates -- is very interesting, and the experimental results look very strong. However, I think the paper has many issues that still need to be addressed.\n\n\n[1] Nils Lukas, Edward Jiang, Xinda Li, and Florian Kerschbaum. Sok: How robust is image classification deep neural network watermarking? In 2022 IEEE Symposium on Security and Privacy\n(SP), pp. 787\u2013804. IEEE, 2022.\n\n[2] Arpit Bansal, Ping-yeh Chiang, Michael J Curry, Rajiv Jain, Curtis Wigington, Varun Manjunatha, John P Dickerson, and Tom Goldstein. Certified neural network watermarks with randomized smoothing. In International Conference on Machine Learning, pp. 1450\u20131465. PMLR, 2022.\n\n[3] Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030, 2023.\n\n[4] Nicholas Carlini and David Wagner. Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods. In AISec '17: Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp.3-14. 2017.\n\n[5] Zhengyuan Jiang, Jinghuai Zhang, and Neil Zhenqiang Gong. Evading watermark based detection of ai-generated content. arXiv preprint arXiv:2305.03807, 2023."
                },
                "questions": {
                    "value": "Did the authors train surrogate detectors for all the evaluated watermarking methods to create true no-box attacks?\n\nDid the authors experiment with different degrees of attacker knowledge -- e.g., what if the attacker does not know the length of the watermark embedded by the actual generator? Would training on 32-bit messages cause attacks on a 64-bit message system to fail?\n\nRather than training surrogate detectors to reconstruct embedded messages, did the authors consider simply training a binary classifier on watermarked and un-watermarked images from the surrogate generator and then performing an adversarial attack on the binary classifier? This seems like the simplest no-box approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2831/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2831/Reviewer_LPhx",
                        "ICLR.cc/2024/Conference/Submission2831/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734072838,
            "cdate": 1698734072838,
            "tmdate": 1700717547896,
            "mdate": 1700717547896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2BqSewGd8r",
                "forum": "O9PArxKLe1",
                "replyto": "nc2XU96cKr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s detailed comments and suggestions for further improvements of our work and will carefully incorporate them in the revised paper. Please find our point-by-point responses below. \n\n> Terminology issue between a \"key\" and a \"message\". \n\nOur terminology is the following: A watermarking key refers to secret random bits of information used in the randomized \u201cdetector\u201d algorithm (called VERIFY in the paper) to detect a watermark. A message is a hidden signal in the image. This notation is inspired by related works [A, B] that also distinguish the terms \u201ckey\u201d and \u201cmessage\" like our work. The Tree-Rings paper instead uses the word \u201ckey\u201d to refer to a \u201cmessage\u201d, which we agree is confusing. \n\nWith our terminology, the _key_ in Tree-Rings are the parameters of the diffusion model, the VERIFY function corresponds to the forward diffusion process, and the _messages_ are initial noise vectors for the diffusion process in the frequency domain. We hope that this explanation and example clear up any confusion. We thank the reviewer for bringing this to our attention and will clarify it in the revised paper.\n\n> The proposed method is similar to that of Jiang et al. [5] in that it adversarially parameterizes image transformations to remove watermarks. The authors claim that the method of Jiang et al. requires access to a watermarking \"key;\" however, Jiang et al. propose attacks under the explicit assumption that the attacker does not have access to the ground-truth key/message [..]. On the other hand, if I am correct that the authors take \"key\" to mean \"detector,\" this claim makes more sense in that Jiang et al. use full access (white-box) or query access (black-box) to the detector to craft attacks.\nThe authors should clarify their statements about the prior work of Jiang et al. and the distinctions between their methods. And while additional experiments may not be feasible at this point for various reasons, I think the paper would be much stronger if it included comparisons between the proposed approach and either or both attack variants proposed by Jiang et al.\n\nAs the reviewer correctly points out, there are fundamental differences between our attacks and those proposed by Jiang et al.. The threats studied by Jiang et al. and those studied in our works are related since both undermine robustness but are orthogonal due to different threat models. \n\n**Jiang et al.**: In all attacks, Jiang et al. assume some (at least indirect) access to the secret key (e.g., black-box access to VERIFY, whose response depends on the secret key). They craft an adversarial perturbation that fools the specific instance of the detector for one key-message pair, which is a special case of Eq. 1, where they optimize \\theta_A with a single, fixed key \\tau and message m (i..e, they remove the expectation term). In their black-box attacks, access to VERIFY is also limited, requiring them to instantiate gradient-free optimization or train surrogate decoders to extract the VERIFY functionality.\n\n**Our work**: Our attacks make no assumptions about the key-message pair used by the provider since they optimize the expected evasion rate over _any_ pair and any instance of the detector. Consequently, Jiang et al. should find \u201cbetter\u201d attacks (i.e., less visible ones) to evade detection since they make more assumptions about the optimization problem in some regards (optimize against one key-message pair with access to VERIFY) and fewer assumptions in other regards (no knowledge of the watermarking method in the black-box case). Still, unlike our work, their attacks require the attacker to access the provider\u2019s secret key by calling VERIFY. \n\nBoth threats are orthogonal because robustness against our attacks does not imply robustness against Jiang et al. 's attacks and vice-versa. To defend against Jiang et al., a provider could restrict their access to the secret key (e.g., allowing only trusted users to access VERIFY or not replying to \u201csimilar\u201d queries), but they would remain vulnerable to our attacks. Similarly, a defense against our attack may not defend against Jiang et al.\u2019s attacks when the attacker has additional capabilities, such as access to the provider\u2019s VERIFY procedure. Providers need to defend against both types of attacks independently.  \n\nWe will highlight the differences between both types of attacks, extend the difference between our work to Jiang et al.\u2019s work, and emphasize the necessity to defend against _both_ threats in the revised paper. \n\n---------\n[A] Zhao, Xuandong, et al. \"Provable robust watermarking for ai-generated text.\" arXiv preprint arXiv:2306.17439 (2023).\n\n[B] Christ, Miranda, Sam Gunn, and Or Zamir. \"Undetectable Watermarks for Language Models.\" arXiv preprint arXiv:2306.09194 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893514139,
                "cdate": 1699893514139,
                "tmdate": 1699893514139,
                "mdate": 1699893514139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oa4RiDoCHY",
                "forum": "O9PArxKLe1",
                "replyto": "nc2XU96cKr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2831/Reviewer_LPhx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2831/Reviewer_LPhx"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed reply. I broadly agree with the authors' characterization of the differences between the proposed method and that of Jiang et al. I also appreciate the additional experiments performed against two-tailed detectors. I am willing to increase my score, but I'd like some clarification on points 1-3 below first. Point 4 is less important.\n\n__1.__ I still think the ResNet description is insufficient. For instance, line 6 of Algorithm 1 is doing a lot of heavy lifting. For each of the watermarking methods considered, how is the ResNet configured to extract the encoded message? Does it predict a vector corresponding to the watermark message? Is it trained with cross-entropy or mean squared error loss? This would be good to see in the appendix.\n\n__2.__ Regarding my comment: \n>If the attacker \"does not need to invoke GKEYGEN\" for TRW/WDM/RivaGAN, as stated in section 4.2, does this mean the attacker does not train a differentiable surrogate detector?...\n\nFrom the authors' reply, it sounds like a ResNet-based surrogate detector network (or \"key\") is only trained for DCT and DCT-SVD. For the remaining methods (TRW, WDM, and RivaGAN) the authors presumably train the detector network (or \"key\") from scratch, as using a provided pre-trained detector network would constitute a white-box attack. So my understanding of the attack is as follows:\n* For DCT/DCT-SVD the attacker must train a ResNet detector from scratch\n* For WDM/RivaGAN the attacker must train a pair of watermark embedder/detector networks from scratch using the corresponding published architectures and training procedures\n* For TRW the attacker uses a pretrained surrogate Stable Diffusion model\n\nCan the authors confirm if this is correct?\n\n__3.__ The actual objective functions for each watermark are not specified in the paper. The VERIFY portion of the objective function will presumably differ from method to method depending on the key and message format -- for example RivaGAN's detector predicts a vector corresponding to the embedded message, while TRW requires measuring the similarity between predicted and known diffusion noise patterns in Fourier space. Again, this would be nice to see in the appendix, as the formulation of an appropriate objective function is a crucial part of any adversarial attack.\n\n\n__4.__ Regarding my comment on the hypothetical surrogate binary classifier attack, I agree that such an attack would operate with less knowledge of the watermark than the proposed method, and should be expected to perform worse. That said, the adversarial attack literature is littered with examples of supposedly naive attacks outperforming more informed baselines due to simplified optimization [1]. I think it would be very interesting to see how such a naive baseline fares, but I don't think such an experiment would be critical to the paper given that the proposed method is already effective.\n\n[1] F. Tramer, N. Carlini, W. Brendel, and A. Madry, \"On adaptive attacks to\nadversarial example defenses,\" in Proc. NIPS, 2020, pp. 1\u201344."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422373679,
                "cdate": 1700422373679,
                "tmdate": 1700422544087,
                "mdate": 1700422544087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P8qbHHE7Eg",
                "forum": "O9PArxKLe1",
                "replyto": "ndLwfC76ub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2831/Reviewer_LPhx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2831/Reviewer_LPhx"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for addressing my concerns. I believe the proposed revisions will significantly strengthen the paper, and I have updated my score accordingly."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432561264,
                "cdate": 1700432561264,
                "tmdate": 1700432561264,
                "mdate": 1700432561264,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9VXGOMwie7",
            "forum": "O9PArxKLe1",
            "replyto": "O9PArxKLe1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2831/Reviewer_Agid"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2831/Reviewer_Agid"
            ],
            "content": {
                "summary": {
                    "value": "Authors emphasize the significance of watermarking in countering misuse by marking generated content with hidden messages. The core security property of watermarking, robustness, is investigated in this paper. The authors assert that evaluating robustness involves creating adaptive attacks tailored to specific watermarking algorithms. To this end, one of the paper's contributions is the proposed approach to assess the optimality of adaptive attacks by framing them as an optimization problem and defining an objective function. The paper presents evidence that such attackers can effectively break all five surveyed watermarking methods with negligible degradation in image quality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "First, on the aspect of the paper\u2019s organization, this manuscript is well-organized and easy to follow. Second, on the aspect of clarity, the proposed method is clearly defined using schematics and pseudo-code descriptions. Third, this paper provides an approach to evaluating adaptive attacks and the demonstration of their effectiveness provide a fresh perspective on the challenges faced in countering image manipulation."
                },
                "weaknesses": {
                    "value": "The motivation and importance of the proposed method are not clear enough, e.g., what problems did the previous works exist? Besides, the experiments comparison and discussion are weak. Experiment section should expand the scope of discussion, compare with more advanced methods, and provide in-depth discussions."
                },
                "questions": {
                    "value": "1.\tABSTRACT: The text should include more details to the proposed methodology, numerical results achieved, and comparison with other methods\n2.\tCould you tell me the limitations of the proposed method? How will you solve them? Please add this part to the manuscript.\n3.\tThe abbreviations must appear at the very first place that the terminology is introduced and the way of introducing the terms must be consistent throughout the manuscript from abstract to conclusion.\n4.\tThe conclusions should be improved such that the authors add some analytical terms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2831/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2831/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2831/Reviewer_Agid"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737017410,
            "cdate": 1698737017410,
            "tmdate": 1700710922914,
            "mdate": 1700710922914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PJWDOW9f0X",
                "forum": "O9PArxKLe1",
                "replyto": "9VXGOMwie7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We appreciate that the reviewer thinks we provided a \u201cfresh perspective\u201d and for their suggestions to improve our paper further. Please find our detailed response below.\n\n> The motivation and importance of the proposed method are not clear enough, e.g., what problems did the previous works exist?\n\nWatermarking can only effectively control image generator misuse if it is robust. Watermarking methods that claim robustness have been proposed, but we refute their claims and show that their approach to testing robustness is flawed. These methods demonstrate robustness by testing their watermark against attacks that know nothing about the watermarking method, such as JPEG compression, blurring, etc. The issue is that these attacks could perform better if they knew more about the method used to watermark the image, and this leads to a cat-and-mouse game where defenses that claim robustness are proposed, only to be broken later by better (adaptive) attacks. \n\nAn attacker who only knows the watermarking method\u2019s algorithmic descriptions, which we refer to as (KEYGEN, EMBED, and VERIFY), can instantiate far more powerful attacks. As our paper shows, instantiating such attacks requires no handcrafting but can be done by leveraging optimization. We define robustness as an objective function, which makes our method generally applicable to any watermarking method proposed in the future. Robustness against adaptive attacks extends to robustness against non-adaptive attacks, which makes studying the former interesting. In cryptography, this principle is known as Kerckhoff\u2019s principle, where an algorithm\u2019s security should not rely on obscurity. Previous works did not consider such adaptive, learnable attackers. We will expand on this discussion in the revised paper. \n\n> Experiment section should expand the scope of discussion, compare with more advanced methods, and provide in-depth discussions.\n\nWe will gladly expand the scope of discussion and add comparisons with advanced methods, but we kindly ask for clarification about the methods the reviewer refers to. To our knowledge, there are no strong, comparable attacks without access to the secret watermarking key, as evidenced by watermarking papers [A,B] that only evaluate against attacks such as JPEG compression and quantization. We would greatly appreciate it if the reviewer could point us to \u201cmore advanced methods\u201d or detail what part of the discussion we should focus on more in-depth, and we would be happy to revise our paper. \n\n> 1. ABSTRACT: The text should include more details to the proposed methodology, numerical results achieved, and comparison with other methods\n> 4. The conclusions should be improved such that the authors add some analytical terms.\n\nWe thank the reviewer for their suggestions and will revise the abstract and conclusion of the revised paper as requested. \n\n > 2. Could you tell me the limitations of the proposed method? How will you solve them? Please add this part to the manuscript.\n\nWe describe our method's limitations to attacking real systems in Section 6. In essence, an adaptive attacker requires access to a (less capable) surrogate generator and the algorithmic description of the watermarking method (meaning KEYGEN, EMBED, and VERIFY). As stated in the paper, we took a best-effort approach. We used the least capable public diffusion model (Stable Diffusion v1.1) to remove watermarks from the most capable public diffusion model (Stable Diffuson v2.0). We also discuss in the paper that the watermarking algorithm may only sometimes be released. Still, the provider relies on security through obscurity and remains vulnerable to attacks by anyone to whom this information is released.  \n\nFurthermore, Section 6 also describes limitations on the attacks that we consider and that more powerful attacks may exist that could be considered in future work.\n\n> 3. The abbreviations must appear at the very first place that the terminology is introduced and the way of introducing the terms must be consistent throughout the manuscript from abstract to conclusion.\n\nWe apologize for any confusion if an abbreviation was used before defining it. We are happy to revise and clarify the paper if the reviewer could point us to the specific abbreviations that have caused confusion. \n\nWe are happy to resolve any other questions the reviewer may have. If these are the only concerns, we kindly ask the reviewer to consider increasing their score.\n\n---------\n\n[A] Wen, Yuxin, et al. \"Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust.\" arXiv preprint arXiv:2305.20030 (2023).\n\n[B] Zhao, Yunqing, et al. \"A recipe for watermarking diffusion models.\" arXiv preprint arXiv:2303.10137 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699892547389,
                "cdate": 1699892547389,
                "tmdate": 1699892547389,
                "mdate": 1699892547389,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dokA3TomtD",
            "forum": "O9PArxKLe1",
            "replyto": "O9PArxKLe1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2831/Reviewer_f7pQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2831/Reviewer_f7pQ"
            ],
            "content": {
                "summary": {
                    "value": "Untrustworthy users can misuse image generators to synthesize high-quality deep-\nfakes and engage in online spam or disinformation campaigns. Watermarking de-\nters misuse by marking generated content with a hidden message, enabling its\ndetection using a secret watermarking key. A core security property of water-\nmarking is robustness, which states that an attacker can only evade detection by\nsubstantially degrading image quality. Assessing robustness requires designing\nan adaptive attack for the specific watermarking algorithm. A challenge when\nevaluating watermarking algorithms and their (adaptive) attacks is to determine\nwhether an adaptive attack is optimal, i.e., it is the best possible attack. We solve\nthis problem by defining an objective function and then approach adaptive attacks\nas an optimization problem. The core idea of our adaptive attacks is to replicate\nsecret watermarking keys locally by creating surrogate keys that are differentiable\nand can be used to optimize the attack\u2019s parameters. The authors demonstrate for Stable\nDiffusion models that such an attacker can break all five surveyed watermarking\nmethods at negligible degradation in image quality. These findings emphasize the\nneed for more rigorous robustness testing against adaptive, learnable attackers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Paper is well formatted\n\nTopic is interesting\n\nGood balance of theory and experiments"
                },
                "weaknesses": {
                    "value": "Please improve readability\n\nPlease number all equations\n\nPlease discuss figures, tables and algorithms clearly in the text\n\nPlease add a security analysis to known attacks in this domain"
                },
                "questions": {
                    "value": "Why is this topic important?\n\nWhat are the future work directions of this work?\n\nWhy is the comparative analysis limited\n\nWhat is the complexity of the algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752008543,
            "cdate": 1698752008543,
            "tmdate": 1699636226653,
            "mdate": 1699636226653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AEPuX6Zf2p",
                "forum": "O9PArxKLe1",
                "replyto": "dokA3TomtD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comment!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive assessment of our work and comments which will help further improve our paper. Please find answers to all questions below. \n\n> Please improve readability. Please number all equations. Please discuss figures, tables and algorithms clearly in the text.\n\nThank you for bringing to our attention that some equations are not labeled. We will revise the paper to label all equations and ensure that all Figures, Tables, and Algorithms are discussed in the text. \n\n> Why is this topic important? \n\nAs stated in the introduction, high-quality image generators can be misused by untrustworthy users, leading to the proliferation of deepfakes and online spam. This is already a problem on social media [A] and has led to companies such as Google announcing their image watermark, such as SynthID [B]. Also, the US government has released an \u201cAI executive order\u201d [C] asking providers to watermark generated outputs. Watermarking needs to be robust; otherwise, attackers can easily evade it by removing the watermark from an image with imperceptible modifications. We propose a better method to test the robustness of watermarking methods using adaptive attacks that are designed against a specific watermarking method. We will include the above references in the revised paper. \n\n> What are the future work directions of this work?\n\nThe future direction of our work is to use our adaptive attacks to design watermarking methods that withstand them. Such a watermarking method\u2019s claim to robustness is more convincing, as robustness to adaptive attacks extends to robustness against non-adaptive attacks. We will include this point in the revised paper\u2019s discussion section. \n\n> Why is the comparative analysis limited?\n\nWe believe that we have mentioned all related work in this field. Our work is the first to instantiate _adaptive_ attacks against image watermarking where the attacker knows the watermarking method but not the secret key or the message. Our attacks require no access to the provider\u2019s watermark verification procedure (unlike other works, such as Jiang et al.). The proposed attacks can remove a watermark from a single image, regardless of which key-message pair was used to watermark it. We would appreciate it if the reviewer would further specify in what regards they believe our comparative analysis is limited. We will be happy to address it in the revised paper.\n\n> What is the complexity of the algorithms?\n\nThank you for this question. From a computational perspective, generating a surrogate watermarking key with methods such as RivaGAN or WDM is the most expensive operation, as it requires training a watermark encoder-decoder pair from scratch. Generating a key for these two methods takes around 4 GPU *hours* each on a single A100 GPU, which is still negligible considering the total training time of the diffusion model, which takes approximately 150-1000 GPU *days* [D]. The optimization of Adversarial noising takes less than 1 second per sample, and tuning the adversarial compressor\u2019s parameters takes less than 10 minutes on a single GPU. We will revise the paper to include the measured running times in the Appendix of the revised paper. \n\nWe hope that we have addressed all of the reviewer\u2019s questions and would kindly ask the reviewer to increase their score if these are the only questions. \n\n------\n\n[A] Barrett, Clark, et al. \"Identifying and mitigating the security risks of generative ai.\" arXiv preprint arXiv:2308.14840 (2023).\n\n[B] Sven Gowal and Pushmeet Kohli. Identifying ai-generated images with SynthID, 2023. URL https://www.deepmind.com/blog/identifying-ai-generated-images-with-synthid. Accessed: 2023-09-23.\n\n[C] \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\" Federal Register, 2023, https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence. Accessed 4 Nov. 2023.\n\n[D] Dhariwal, Prafulla, and Alexander Nichol. \"Diffusion models beat gans on image synthesis.\" Advances in neural information processing systems 34 (2021): 8780-8794."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699892258065,
                "cdate": 1699892258065,
                "tmdate": 1699892258065,
                "mdate": 1699892258065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2PcyYRVBtR",
                "forum": "O9PArxKLe1",
                "replyto": "AEPuX6Zf2p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2831/Reviewer_f7pQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2831/Reviewer_f7pQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for addressing my comments"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656178452,
                "cdate": 1700656178452,
                "tmdate": 1700656178452,
                "mdate": 1700656178452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]