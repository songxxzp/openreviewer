[
    {
        "title": "BEEF: Building a BridgE from Event to Frame"
    },
    {
        "review": {
            "id": "AzHVcCnYMr",
            "forum": "IHedM0Zem9",
            "replyto": "IHedM0Zem9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_erhw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_erhw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel pre-processing framework (i.e., BEEF) to split continuous event streams into event slices in an adaptive manner. BEEF mainly adopt an energy-efficient SNN to trigger the slicing time. Technically, a new dataset is first split into event slices by SNN, which is robust to high-speed or low-speed scenarios. Then, event slices are used to finetune the ANN to verify the performance in downstream event-based vision tasks. The experiments show that the proposed BEEF achieves SOTA performance in event-based object tracking and event-based object recognition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "i) The topic of adaptively splitting event streams using SNN is very interesting and attractive.\n\nii) The authors sufficient experiments in the main paper and the supplemental material to help reader better understand the main contributions of this work.\n\niii) The writing is straightforward, clear, and easy to understand."
                },
                "weaknesses": {
                    "value": "i) While fixed windows or a fixed event count may not offer optimal performance for event partitioning pre-processing, they do provide a quick processing option for collaboration with subsequent vision tasks. The authors also adapt the SNN for event stream division, but it's crucial to determine if this process is time-consuming across different platforms (CPU, GPU) and if it's suitable for downstream tasks, particularly those requiring low-latency responses for agile robots. Although the authors give the analysis of processing speed, it should be given the computational analysis in CPU.\n\nii) The authors have conducted a comparison experiment with a fixed number of times, as shown in Table 3. Nevertheless, it is advisable for the authors to include experiments with a fixed time window. Furthermore, the authors should investigate how various parameters for fixed events or fixed time windows compare to BEEF. Additionally, it would be beneficial for the authors to provide more visual comparison results of event representations.\n\niii) There are articles exploring adaptive event stream splitting strategies. The author should consider citing some relevant references [1, 2] that utilize hyperparameters for implementation.\n\n[1] EDFLOW: Event driven optical flow camera with keypoint detection and adaptive block matching, IEEE TCSVT 2022.\n\n[2] Asynchronous spatio-temporal memory network for continuous event-based object detection, IEEE TIP 2022."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3307/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3307/Reviewer_erhw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698151213258,
            "cdate": 1698151213258,
            "tmdate": 1699636280148,
            "mdate": 1699636280148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XXfZ72szmn",
                "forum": "IHedM0Zem9",
                "replyto": "AzHVcCnYMr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to erhw (Reply 1)"
                    },
                    "comment": {
                        "value": "**Weakness 1 (Computational Analysis in both GPU and CPU)**\n\n**Q1:** *While fixed windows or a fixed event count may not offer optimal performance for event partitioning pre-processing, they do provide a quick processing option for collaboration with subsequent vision tasks. The authors also adapt the SNN for event stream division, but it's crucial to determine if this process is time-consuming across different platforms (CPU, GPU) and if it's suitable for downstream tasks, particularly those requiring low-latency responses for agile robots. Although the authors give the analysis of processing speed, it should be given the computational analysis in CPU.*\n\n**A1:**\nThank you for your insightful comments! We have conducted a computational analysis of the BEEF framework on both CPU and GPU platforms:\n\n|  |Slicing Latency |FPS |\n|:------|:----------------|:----------------|\n| SNN on GPU | 0.009s per img | 111 Hz| \n| SNN on CPU | 0.430s per img | 2.3 Hz| \n\n\n The results indicate that SNN processing of event streams on a GPU offers the advantage of low latency, while there is a noticeable delay when processing on a CPU.\n\nHowever, it's important to note that SNNs are primarily designed to operate on neuromorphic hardware, where they can leverage their low power consumption and low latency advantages for efficient data processing in scenarios demanding quick responses. There is significant research demonstrating the benefits of SNNs in processing event streams on neuromorphic hardware. For instance, the use of the Loihi hardware for event-based vision tasks [1,2] and the Tianji chip [3] for robotics applications [4] are notable examples.\n\nWe will include these details in our revised manuscript to provide a comprehensive understanding of the processing capabilities of SNNs across different hardware platforms. Thanks for your suggestion!\n\n***Reference:***\n\n[1] Roy A, Nagaraj M, Liyanagedera C M, et al. Live Demonstration: Real-time Event-based Speed Detection using Spiking Neural Networks. CVPRW 2023.\n\n[2] Viale A, Marchisio A, Martina M, et al. Carsnn: An efficient spiking neural network for event-based autonomous cars on the loihi neuromorphic research processor. IJCNN 2021.\n\n[3] Brain-inspired multimodal hybrid neural network for robot place recognition. Science Robotics 2023\n\n[4] Viale A, Marchisio A, Martina M, et al. LaneSNNs: Spiking Neural Networks for Lane Detection on the Loihi Neuromorphic Processor. IROS 2022."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230510400,
                "cdate": 1700230510400,
                "tmdate": 1700230510400,
                "mdate": 1700230510400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rl8cwyiMiQ",
                "forum": "IHedM0Zem9",
                "replyto": "AzHVcCnYMr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to erhw (Reply 2)"
                    },
                    "comment": {
                        "value": "**Weakness 2 (Comparisons on Different Fixed Slicing Methods)**\n\n**Q2:** *The authors have conducted a comparison experiment with a fixed number of times, as shown in Table 3. Nevertheless, it is advisable for the authors to include experiments with a fixed time window. Furthermore, the authors should investigate how various parameters for fixed events or fixed time windows compare to BEEF. Additionally, it would be beneficial for the authors to provide more visual comparison results of event representations.*\n\n**A2:**\nThank you very much for your valuable suggestions! In the original experiments presented in Table 3, we used the slicing of fixed event count for comparison. We will include more specific details about this in the revised version of our manuscript. To facilitate a more complete comparison between dynamic slicing and traditional fixed slicing methods, we have supplemented our experiments in the object recognition task. These include slicing based on a fixed number of events and a fixed duration of events. We have also compared three different event representation methods, with the results as follows:\n\n| DVSGesture |Event Frame | Event Spike Tensor | Voxel Grid |\n|:------|:----------------|:--------|:----|\n| Fix Duration | 93.75% | 93.75% | 88.54% |\n|Fix Event Count |93.06%|94.79%|88.19%|\n|**BEEF(ours)** |**94.79%**|**95.49%**|**89.24%**|\n\n\nThe results show that our dynamic slicing approach, BEEF, outperforms the fixed slicing approach in downstream tasks across different event representations. This highlights the efficacy of BEEF in handling event streams.\n\nAdditionally, we plan to include more illustrative figures in the revised version to vividly depict the differences and respective advantages of dynamic and fixed slicing. More visualization results related to the experiments will also be added to the appendix.\n\nThank you again for your suggestion! Your feedback is instrumental in enhancing the clarity and comprehensiveness of our research."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230554940,
                "cdate": 1700230554940,
                "tmdate": 1700230554940,
                "mdate": 1700230554940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UbyGo8lsrw",
                "forum": "IHedM0Zem9",
                "replyto": "AzHVcCnYMr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to erhw (Reply 3)"
                    },
                    "comment": {
                        "value": "**Weakness 3 (Relevant References)**\n\n**Q3:** *There are articles exploring adaptive event stream splitting strategies. The author should consider citing some relevant references [1, 2] that utilize hyperparameters for implementation.*\n\n**A3:**\nThank you very much for your suggestion! We acknowledge the importance of incorporating relevant references, particularly those that explore adaptive event stream splitting strategies. We will make sure to include the references you have mentioned [1,2] in the revised version of our manuscript! Thanks!\n\n***Reference:***\n\n[1] Liu M, Delbruck T. EDFLOW: Event driven optical flow camera with keypoint detection and adaptive block matching[J]. IEEE TCSVT 2022.\n\n[2] Li J, Li J, Zhu L, et al. Asynchronous spatio-temporal memory network for continuous event-based object detection[J]. IEEE TIP 2022."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230588590,
                "cdate": 1700230588590,
                "tmdate": 1700230588590,
                "mdate": 1700230588590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FonEw9HS5w",
                "forum": "IHedM0Zem9",
                "replyto": "UbyGo8lsrw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Reviewer_erhw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Reviewer_erhw"
                ],
                "content": {
                    "title": {
                        "value": "I stuck with the original score."
                    },
                    "comment": {
                        "value": "The author has addressed all my queries through the experimental responses. However, I suggest incorporating these experiments into the supplementary material to make them accessible to a broader audience of authors."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533721306,
                "cdate": 1700533721306,
                "tmdate": 1700533721306,
                "mdate": 1700533721306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LnTSOxxXjF",
            "forum": "IHedM0Zem9",
            "replyto": "IHedM0Zem9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_689Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_689Y"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose BEEF, a novel-design event processing framework that can slice the event streams in an adaptive manner. To achieve this, BEEF employs an SNN as the event trigger to dynamically determine the time at which the event stream needs to be split, rather\nthan requiring hyper-parameter adjustment as in traditional methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: papers dealing with spiking related algorithms should be of interest to the subset of the machine learning community investigating on-the-edge computing algorithms.\n\nS2: the paper is relatively well written"
                },
                "weaknesses": {
                    "value": "W1: I am aware that with event and spiking cameras it is quite popular to convert the event/spike streams into a sort of frame based representation. However I have a fundamental objection with this type of an approach (which is shared by quite a few of my colleagues around the world, in private conversations at least) as to why should these fundamentally asynchronous\nevent streams representations should be converted to a rather synchronous representation, simply to be able to map them into algorithms that were originally developed for synchronous frame like data. I think a more thorough discussion on this is needed in the paper to better motivate the work\n\nW2: clarify better what are the alternative methods to which this is being compared? What exactly is meant by \"fixed slice\" approaches to which this is being compared? Many approaches for producing frame like representations (such as getting the max or union of all events in a time window) result in the introduction of significant amounts of noise. In contrast morphological operands like erosion and dilation can introduce much better quality frames. To what extent is the good performance of the algorithm attributable simply to noisy frame generation in competing approaches?\n\nW3: unless i missed it, will source code be provided?"
                },
                "questions": {
                    "value": "See my questions above. Addressing them would improve the paper's relevance"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698326756128,
            "cdate": 1698326756128,
            "tmdate": 1699636280074,
            "mdate": 1699636280074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "buz9MAwyI7",
                "forum": "IHedM0Zem9",
                "replyto": "LnTSOxxXjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 689Y (Reply 1)"
                    },
                    "comment": {
                        "value": "**Weakness 1 (Asynchronous Event and Synchronous Representation)**\n\n**Q1:** *I am aware that with event and spiking cameras it is quite popular to convert the event/spike streams into a sort of frame based representation. However I have a fundamental objection with this type of an approach (which is shared by quite a few of my colleagues around the world, in private conversations at least) as to why should these fundamentally asynchronous event streams representations should be converted to a rather synchronous representation, simply to be able to map them into algorithms that were originally developed for synchronous frame like data. I think a more thorough discussion on this is needed in the paper to better motivate the work.*\n\n**A1:**\nThank you for your insightful comment. First, I wholeheartedly agree with your statement regarding the conversion of asynchronous event data into synchronous formats, which indeed can undermine the inherent advantages of the asynchronous nature. This is not an optimal representation, and we acknowledge this limitation. However, the reasons for converting asynchronous event data into synchronous representations in current practices can be summarized as follows:\n\n1. Why choose synchronous over asynchronous processing? Given that existing GPU hardware architectures and programming models are designed for highly synchronous and parallel tasks, the conversion of asynchronous event stream data into a synchronous representation becomes a necessity for algorithm simulations performed on GPUs. In our paper, we aim to slice the event stream into very fine event cells to represent the original event data as closely as possible (Sec.4.1), hoping to minimize the errors introduced by synchronous representation.\n\n2. The synchronous simulations conducted on GPUs in our work do not preclude the possibility of future asynchronous implementations on neuromorphic hardware. Both SNNs and event stream data are inherently asynchronous. However, due to hardware constraints, current software simulations are temporarily unable to achieve true asynchronous processing. If deployed on neuromorphic hardware (e.g., Loihi [1], TrueNorth [2]), the asynchronous processing of event streams by SNNs would be extremely low-energy and low-lantancy[3,4,5]. Therefore, this work lays the theoretical groundwork for future hardware deployment, and efficiently implementing SNN processing of asynchronous event streams in conjunction with ANNs on neuromorphic hardware is one of our future goals.\n\nWe hope this explanation addresses your concerns. We are committed to further discussing this topic in our manuscript to provide a comprehensive motivation for our work. \n\n\n***Reference:***\n\n[1] Davies M, Srinivasa N, Lin T H, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro 2018.\n\n[2] Akopyan F, Sawada J, Cassidy A, et al. Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE TCAD 2015.\n\n[3] Roy A, Nagaraj M, Liyanagedera C M, et al. Live Demonstration: Real-time Event-based Speed Detection using Spiking Neural Networks. In CVPRW 2023.\n\n[4] Viale A, Marchisio A, Martina M, et al. Carsnn: An efficient spiking neural network for event-based autonomous cars on the loihi neuromorphic research processor. In IJCNN 2021.\n\n[5] Yu F, Wu Y, Ma S, et al. Brain-inspired multimodal hybrid neural network for robot place recognition. Science Robotics 2023."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230328918,
                "cdate": 1700230328918,
                "tmdate": 1700230328918,
                "mdate": 1700230328918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rM3GAFm7PM",
                "forum": "IHedM0Zem9",
                "replyto": "LnTSOxxXjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 689Y (Reply 2)"
                    },
                    "comment": {
                        "value": "**Weakness 2 (Clarification and Discussion)**\n\n**Q2:** *clarify better what are the alternative methods to which this is being compared? What exactly is meant by \"fixed slice\" approaches to which this is being compared? Many approaches for producing frame like representations (such as getting the max or union of all events in a time window) result in the introduction of significant amounts of noise. In contrast morphological operands like erosion and dilation can introduce much better quality frames. To what extent is the good performance of the algorithm attributable simply to noisy frame generation in competing approaches?*\n\n**A2:**\n\n*Symbol Description: the total event stream $E$; the resulting sliced sub-event stream list by BEEF: $E_{beef}=[E^b_{1},..E^b_{N_1}]$; the resulting sliced sub-event stream list by fixed slicing method: $E_{fixtime}=[E^f_{1},..E^f_{M_1}]$.*\n\nThank you for your insightful suggestions! First, let me clarify our dynamic slicing approach, which allows the event stream to be sliced at any timestamp and then converted into frames for downstream tasks. In Table 1, the term \"fixed slice\" refers to a method that we employ a fixed slicing approach with a constant number of events per sub-event stream to segment the entire event stream into $E_{fixtime}=[E^f_{1},..E^f_{M_1}]$. These sub-event streams are then transformed into different representations using the same event representation method $F$ (we used Event Frame [1]) and fed into downstream tasks (tracking/recognition) for testing. We will revise the terminology to \"slicing with fixed event number\" in our updated manuscript to make the experimental settings clearer.\n\nWe greatly appreciate your mention of the potential use of morphological operands. These methods are more commonly found in event stream denoising algorithms or event representation algorithms [2], and our paper primarily focuses on the event stream slicing process. The techniques you mentioned, like erosion or dilation, could be integrated with our current approach; for example, denoising the original event stream before applying BEEF\u2019s dynamic slicing. We intend to investigate it further in future research. Thank you for this valuable suggestion!\n\n***Reference:***\n\n\n[1] Maqueda A I, Loquercio A, Gallego G, et al. Event-based vision meets deep learning on steering prediction for self-driving cars. CVPR 2018.\n\n[2] Baldwin R W, Liu R, Almatrafi M, et al. Time-ordered recent event (TORE) volumes for event cameras. TPAMI 2022."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230401604,
                "cdate": 1700230401604,
                "tmdate": 1700230401604,
                "mdate": 1700230401604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZFn23hl7pk",
                "forum": "IHedM0Zem9",
                "replyto": "LnTSOxxXjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 689Y (Reply 3)"
                    },
                    "comment": {
                        "value": "**Weakness 3 (Code Available)**\n\n**Q3:** *unless i missed it, will source code be provided?*\n\n**A3:**\nAbsolutely\uff01We will provide the source code upon acceptance."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230450641,
                "cdate": 1700230450641,
                "tmdate": 1700230450641,
                "mdate": 1700230450641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ljWj2FGFBn",
                "forum": "IHedM0Zem9",
                "replyto": "LnTSOxxXjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Reviewer_689Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Reviewer_689Y"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the comments by the authors and reviewers. The overall evaluation of the paper seems to be consistent more or less across reviewers. As I indicated in my comments the conversion of asynchronous events to a frame based representation defeats the purpose of event/spiking cameras (in my personal opinion at least). Indicating that this is done because we need to run it on GPUs is not convincing to me, so I keep my ranking unchanged."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678821433,
                "cdate": 1700678821433,
                "tmdate": 1700678854993,
                "mdate": 1700678854993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2yk1QhhXpy",
                "forum": "IHedM0Zem9",
                "replyto": "LnTSOxxXjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 689Y (Clarification)"
                    },
                    "comment": {
                        "value": "Thank you for your response and thank you for taking the time and effort to review our paper. Although BEEF demonstrate superior performance compared with SOTA, but we believe that the flow in the original paper may have caused confusion, for instance, most reviewers confused our method with representation method. We notice that and emphasize our method is not one type of representation methods but compatible with any representation method (including asynchronous representation) in the revised paper. In addition, thanks to the suggestions made by the reviewers, we have added a number of experiments that have significantly increased the reliability of our paper.\n\nBack to your query, to the best of our knowledge, there is no commonly used SNN framework that can directly handle asynchronous inputs; current frameworks can only handle frame-based inputs. However, we believe that the SNN trained with current frameworks can potentially be utilized to process asynchronous input. To demonstrate this, we quickly add one experiment. When the number of slices ($N$) increases, the frame-like input will gradually become more similiar to asynchronous inputs. Therefore, we cut the event stream into up to 1000 slices, each slice has only ~100 events and almost all pixels are 0 or 1. Considering the total pixel number is 180 x 240 = 43200, it is quite sparse. From following table, the same SNN will fire at similar percentages (i.e., the resulting sub-stream always contains similar event point), demonstrating that SNN is capable of perceiving event information. Indeed, it is still not fully asynchronized input, but we hope it would provide some insights here.\n\n| $N$  |          30 |          50 |          100 |          200 |         300 |         400 |         500 |         600 |         700 |         800 |         900 |         1000 |\n|:-------------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|\n| Spike Position     |   15 | 25 | 51 | 102  | 153 | 213 | 256 | 324  | 387  | 444 | 504 | 560 |\n| Percentage of Containing Event   | 50.00%    | 50.00%  | 51.00% | 51.00% | 51.00% | 53.25% | 51.20%  | 54.00%  | 55.29% | 55.50% | 56.00% | 56.00% |"
                    }
                },
                "number": 38,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717525973,
                "cdate": 1700717525973,
                "tmdate": 1700717822211,
                "mdate": 1700717822211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "58XNGKolir",
            "forum": "IHedM0Zem9",
            "replyto": "IHedM0Zem9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_YMHz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_YMHz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient way for event representation. Specifically, they introduce SNN for adaptive event slicing, which can choose appropriate slicing times considering the events\u2019 temporal feature and downstream task. The authors present several losses to further improve the adaptiveness, and a strategy to let SNN better assist the of ANN in an iterative and cooperative manner."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The overall writing of this work is clear and easy to follow.\n+ The three observations and solutions seem to work well and improve the adaptation for slicing time.\n+ Using SNN in event representation is rational considering the similar feature for SNN and event."
                },
                "weaknesses": {
                    "value": "- This paper fails to fully review the topic of this work: event representation. As suggested in [1][2], there are several existing event representation strategies including stacking based on time/event counts, voxel grid, histogram of time surfaces, event spike tensor, and a recent work introduces neural representation [3]. However, this paper only mentions two of them. In addition, the motivation to consider temporal information is similar with event counts integration, which is mentioned by the authors. \n- The necessity of a very lightweight SNN is not clear. Since SNN works with ANN cooperatively, SNN has only very limited contribution to the overall computational cost. As implied in Table 2, considering the ANN is the major cost for the process, the contribution and necessity for low energy and fast speed of SNN is reduced.\n- The compared methods in the experiment are not sufficient. More event representation/stacking methods should be considered to compare with the proposed methods, including the methods mentioned in [1-3].\n- I wonder whether such iterative optimization of SNN and ANN work better than joint optimization, like we regard the whole process as an end-to-end task and optimize the SNN loss and downstream task loss together.\n- More details about the experimental settings are required. The proposed methods use adaptive slicing time, how to create GT accordingly? And how to compare with fixed-sliced methods that have different timestamps for event frames?\n\n[1] End-to-End Learning of Representations for Asynchronous Event-Based Data, ICCV 2019\n[2] Event-based High Dynamic Range Image and Very High Frame Rate Video Generation using Conditional Generative Adversarial Networks, CVPR 2019\n[3] NEST: Neural Event Stack for Event-based Image Enhancement, ECCV 2022"
                },
                "questions": {
                    "value": "See the weakness above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3307/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3307/Reviewer_YMHz"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746854466,
            "cdate": 1698746854466,
            "tmdate": 1699636279988,
            "mdate": 1699636279988,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZxpHVYgv7c",
                "forum": "IHedM0Zem9",
                "replyto": "58XNGKolir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to YMHz (Reply 1)"
                    },
                    "comment": {
                        "value": "**Weakness 1 (Difference between Event Slicing and Event Representation)**\n\n**Q1:** *This paper fails to fully review the topic of this work: event representation. As suggested in [1][2], there are several existing event representation strategies including stacking based on time/event counts, voxel grid, histogram of time surfaces, event spike tensor, and a recent work introduces neural representation [3]. However, this paper only mentions two of them. In addition, the motivation to consider temporal information is similar with event counts integration, which is mentioned by the authors.*\n\n**A1:**\nThank you for your recommendations. I understand your concern regarding why our paper does not address a broader range of event representation methods. The reason is that **our focus is on event slicing rather than event representation.** Event stream data conversion into frames/representations involves two steps: Step 1: slicing the event stream into multiple sub-event streams, and Step 2: converting these sub-streams into frames using different event representation methods. There is a significant body of work dedicated to optimizing event representation (Step 2), including the voxel grid, time surface, and other methods you mentioned. However, these do not address the issues arised with fixed slicing (e.g., resulting non-uniform event in scenarios with changing motion speed). Hence, our paper primarily addresses the first step: event slicing.\n\nAfter the dynamic slicing with BEEF, the events can indeed be transformed into different representational formats using various event representation methods. Although event slicing and representation are different processing steps, either better slicing or representation method benefits the feature extraction with neural network, thus improving performance. Experiments with different slicing methods and different event representation have been supplemented in **Reply 3**. \n\nThe references you have mentioned will also be included in the updated version of our manuscript. And the confusing description of motivation in the original article that you pointed out will also be revised shortly!\n\nI hope this explanation addresses your query. Thank you once again!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229730380,
                "cdate": 1700229730380,
                "tmdate": 1700229730380,
                "mdate": 1700229730380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vAn04Sr95X",
                "forum": "IHedM0Zem9",
                "replyto": "58XNGKolir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to YMHz (Reply 2)"
                    },
                    "comment": {
                        "value": "**Weakness 2 (Necessity of Using SNN)**\n\n**Q2:** *The necessity of a very lightweight SNN is not clear. Since SNN works with ANN cooperatively, SNN has only very limited contribution to the overall computational cost. As implied in Table 2, considering the ANN is the major cost for the process, the contribution and necessity for low energy and fast speed of SNN is reduced.*\n\n**A2:**\nThank you for your question. Let me elucidate the necessity of using SNN.\n\n1.The reason **why we choose SNN as the event slicing trigger** is twofold:\n- Utilizing SNNs on neuromorphic hardware for processing event streams is low-energy and low-latency [1,2].\n\n- Deployed on neuromorphic hardware, SNNs can process event streams asynchronously [3,4,5], conserving energy when there is no data input\u2014a capability that GPUs, operating synchronously, lack.\n\nDue to the aforementioned reasons, there is a considerable amount of research [6,7,8,9,10] employing Spiking Neural Networks (SNNs) for event data. Although these SNNs are simulated on GPU platforms, the models resulting from such simulations could be deployed on neuromorphic hardware [3,4].\n\n\n2.The rationale behind our aim for **low-energy and fast-speed SNN processing** is:\n\nWe design the BEEF as a **plug-and-play** algorithm, intending for the SNN to dynamically slice the event stream without adversely affecting the latency or energy consumption of the main network.\n\nRegarding the comparison of resource consumption between SNNs and ANNs (Table 2), it is crucial to note that our dynamic slicing is performed in real-time, not in an offline manner.\nBy employing a low-energy SNN model as a dynamic event stream slicer, we ensure that the speed does not impede the downstream processing rate and also enhances the overall performance of downstream tasks. This is one of the core motivations of our paper, and we hope it addresses your concerns.\n\n***Reference:***\n\n[1] Davies M, Srinivasa N, Lin T H, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro 2018.\n\n[2] Akopyan F, Sawada J, Cassidy A, et al. Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE TCAD 2015.\n\n[3] Roy A, Nagaraj M, Liyanagedera C M, et al. Live Demonstration: Real-time Event-based Speed Detection using Spiking Neural Networks. In CVPRW 2023.\n\n[4] Viale A, Marchisio A, Martina M, et al. Carsnn: An efficient spiking neural network for event-based autonomous cars on the loihi neuromorphic research processor. In IJCNN 2021.\n\n[5] Yu F, Wu Y, Ma S, et al. Brain-inspired multimodal hybrid neural network for robot place recognition. Science Robotics 2023.\n\n[6] Hagenaars J, Paredes-Vall\u00e9s F, De Croon G. Self-supervised learning of event-based optical flow with spiking neural networks. NeurlPS 2021.\n\n[7] Yao M, Gao H, Zhao G, et al. Temporal-wise attention spiking neural networks for event streams classification. ICCV 2021.\n\n[8] Zhu L, Wang X, Chang Y, et al. Event-based video reconstruction via potential-assisted spiking neural network. CVPR 2022\n\n[9] Kosta A K, Roy K. Adaptive-spikenet: event-based optical flow estimation using spiking neural networks with learnable neuronal dynamics. ICRA 2023.\n\n[10] Hussaini S, Milford M, Fischer T. Spiking neural networks for visual place recognition via weighted neuronal assignments. RAL 2023."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229814628,
                "cdate": 1700229814628,
                "tmdate": 1700229814628,
                "mdate": 1700229814628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DvQjtpBBRV",
                "forum": "IHedM0Zem9",
                "replyto": "58XNGKolir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to YMHz (Reply 3)"
                    },
                    "comment": {
                        "value": "**Weakness 3 (Comparisions of Different Event Representation)**\n\n**Q3:** *The compared methods in the experiment are not sufficient. More event representation/stacking methods should be considered to compare with the proposed methods, including the methods mentioned in [1-3].*\n\n**A3:**\nThanks for your suggestion! Event representation refers to the process of event information extraction that is performed after the event stream has been sliced into sub-event stream, and the resulting event representation meets the neural network input requirements. Thus, our dynamic slicing process and event representation can be used at the same time. \n\nTo validate the effectiveness of our slicing approach, we assess the downstream task performance using three distinct event representation methods, namely Event Frame [1], Event Spike Tensor (EST) [2], and Voxel Grid [3], on the DVSGesture dataset. We measure these against both fixed (slice by fixed duration and fixed event count) and dynamic slicing approaches to provide a comprehensive analysis:\n\n| DVSGesture |Event Frame | Event Spike Tensor | Voxel Grid |\n|:------|:----------------|:--------|:----|\n| Fix Duration | 93.75% | 93.75% | 88.54% |\n|Fix Event Count |93.06%|94.79%|88.19%|\n|**BEEF(ours)** |**94.79%**|**95.49%**|**89.24%**|\n\n\nThe results show that our dynamic slicing approach, BEEF, outperforms the fixed slicing approach in downstream tasks across different event representations. This highlights the efficacy of BEEF in handling event streams. \n\nThanks for your suggestion! We will cite the event representation methods you mentioned in the revised paper and try to integrate them with BEEF in the future.\n\n***Reference:***\n\n[1] Maqueda A I, Loquercio A, Gallego G, et al. Event-based vision meets deep learning on steering prediction for self-driving cars. CVPR 2018.\n\n[2] Gehrig D, Loquercio A, Derpanis K G, et al. End-to-end learning of representations for asynchronous event-based data. ICCV 2019.\n\n[3] Zhu A Z, Yuan L, Chaney K, et al. Unsupervised event-based learning of optical flow, depth, and egomotion. CVPR 2019."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229865583,
                "cdate": 1700229865583,
                "tmdate": 1700229865583,
                "mdate": 1700229865583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JEYLhYhHUv",
                "forum": "IHedM0Zem9",
                "replyto": "58XNGKolir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to YMHz (Reply 4)"
                    },
                    "comment": {
                        "value": "**Weakness 4 (End-to-end Optimization)**\n\n**Q4:** *I wonder whether such iterative optimization of SNN and ANN work better than joint optimization, like we regard the whole process as an end-to-end task and optimize the SNN loss and downstream task loss together.*\n\n**A4:**\nThanks for your constructive suggestion! We have explored your idea of training the SNN and ANN from scratch and optimizing them together. However, as shown in the table below, the results of ResNet18 have a similar performance to the original results, while ResNet34 even has a performance degradation:\n\n| DVSGesture | Random Slice| Fixed Slice | BEEF | BEEF (optimize both from scratch) |\n|:------|:----------------|:--------|:----|:----|\n| ResNet18 | 93.06% | 93.40% | 93.49% |93.75% |\n|ResNet34 |95.14%|93.40%|96.18%| 92.36%|\n\nThus, we believe that further in-depth exploration is needed to fully realize the concept you've mentioned and the training strategies need to be improved.\n\nThank you for interesting comments!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229979302,
                "cdate": 1700229979302,
                "tmdate": 1700229979302,
                "mdate": 1700229979302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L0HjDoRMeX",
                "forum": "IHedM0Zem9",
                "replyto": "58XNGKolir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to YMHz (Reply 5)"
                    },
                    "comment": {
                        "value": "**Weakness 5 (Experiment Details)**\n\n**Q5:** *More details about the experimental settings are required. The proposed methods use adaptive slicing time, how to create GT accordingly? And how to compare with fixed-sliced methods that have different timestamps for event frames?*\n\n**A5:**\nThank you for your question. Below are more detailed explanations of our experimental settings and the methodology for generating Ground Truth (GT). We will include these details in the appendix of the revised version:\n\n**Experimental settings\uff1a**\n\n1. *Network Structure*:\nWe used a Spiking Neural Network architecture comprising {16C3-IF-AP2-32C3-IF-AP2-64C3-IF-AP2-LNIF-LN-IF}, where IF denotes the use of integrated-fire neurons. (Appendix E)\n\n2. *Training Setup*:\nWe adopted the SGD optimizer with an initial learning rate of 1e-4, complemented by a cosine learning rate scheduler. SNN models were trained for 50 epochs with a batch size of 32. (Appendix E)\n\n3. *Data Processing Setup:*\nFor single object tracking tasks, we utilized the FE108 dataset, while for object recognition, we used the DVS-gesture and N-caltech101 datasets. Each dataset was divided into training, testing, and validation sets. The validation set was used to infer the ANN model to provide feedback for supervising SNN training. All results in the table represent the ANN's performance on the test set. \n\n4. *GT Setting:*\nSince the SNN might choose to spike and segment the event stream at any position, the GT at any timestamp was obtained through linear interpolation from the GT provided by the original dataset. \n\n**Details of Fixed-sliced method:**\n\n*Symbol Description: the total event stream $E$; the resulting sliced sub-event stream list by BEEF: $E_{beef}=[E^b_{1},..E^b_{N_1}]$; the resulting sliced sub-event stream list by fixed slicing method: $E_{fixtime}=[E^f_{1},..E^f_{M_1}]$.*\n\nSuppose an event stream (duration = $T$) is sliced into $N'$ slices ($E_{beef}=[E^b_{1},..E^b_{N_1}]$) after dynamic slicing. Then for a fair comparison, the number of sub-event stream generated by the fixed slicing method should also be $N'$ (i.e., $len(E_{fixtime})=N'$), where the duration of each sub-event stream in $E_{fixtime}$ is $\\frac{T}{N'}$. Since it was mentioned earlier that we know the GT of each timestamp, we can likewise obtain the GT corresponding to each sub-event stream after fixed slicing. Then we can compare the results of the fixed slicing method with our dynamic slicing method."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230197484,
                "cdate": 1700230197484,
                "tmdate": 1700230197484,
                "mdate": 1700230197484,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2GkRi9v1S4",
            "forum": "IHedM0Zem9",
            "replyto": "IHedM0Zem9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_q3dC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_q3dC"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about the slicing step in the conversion from events to binned representations that can yield frames for classical image processing.\nThe goal of this paper is to make the event-slicing step adaptive instead of fixed over time as it is now in the majority of the approaches that use slicing/binning/bucketing where events are assigned to slices with slices being constant time length or containing equal numbers of events. \n\nThe way it works is that events are fed to a spiking neural network with Leaky Integrate and Fire neurons. The SNN fires more sparsely than the original events.\nA new slice is created containing all events between the timings of two output spikes. \n\nTo control the desired time offset of the slice a membrane potential loss is introduced. Authors give a formal proof for the sufficient conditions. \nMoreover, a linear assuming loss resolves the dependence between neighboring membrane potentials.\n\nExperiments are conducted on object tracking and gesture/object recognition with impressive results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The dynamic slicing of events using the output spikes of an SNN.\n\n2. The connection between slicing and downstream task expressed in the additional two loss terms determining the hyperparameters of the SNN.\n\n3. The theoretical treatment of the sufficient condition of firing at a desired time (given in the appendix)."
                },
                "weaknesses": {
                    "value": "1. Frame-like inputs to transformers or CNNs where frames have been derived from events may be sensitive to slicing. We need a toy experiment to study this hypothesis with a smaller network and different slicing techniques.\n\n2. The exposition is really hard to follow. As stated directly after eq. 4, the slicing is done by grouping together events whose timestamps are between two output spikes of the SNN. Here, an experiment is needed on the statistics of this slicing and why such an approach makes sense.\n\n3. 4.3.1 has to be elaborated. While the math derivations are sound, it is not clear to the reader why the starting point of the derivations is the desire for $S_{out}$ to spike at $n^{*}$. I tried to understand it also through the observations in 4.3.2 but could not.\n\n4. The beginner's arena was meant to explain the above but is incomprehensible. What does it mean ``to slice at a specified time step $T^{*}'' ?\n\n5. It is not clear what purpose the energy computations of the SNN serve when the task will be solved with ultra consuming GPUs. \n\n6. The experimental comparison should be with approaches that are asynchronous end to end like HOTS or HATS or Cannici'19, Perot'20 etc. or approaches like the Event Transformer.\n\n7. Table 3: It is not discussed why the transformer tracker performs almost the same or better without BEEF. Why does BEEF not add anything significant when an attention mechanism is used?\n\n8. The feedback strategy is learnt during training. I understand that in this sense it is adaptive to the task rather than during inference to the event stream when the hyperparameters will be fixed.\n\n9. It is unclear whether events are treated differently according to their polarity.\n\n10. There is some problem with the definition of ${\\cal D}$ because $n_q$ is not defined anywhere but mentioned ``where $n_q$ denotes the time of the last spike''.\n\n11. It would be worth listing the latency from event to GPU output for the particular architectures on tracking and recognition. This is much more critical here than the power consumption of the CNN.\n\nSummary: The authors need to explain the slicing method more clearly (possible misreadings are listed above). My main concern is the lack of any experimental analysis or motivation for the particular quite elaborate slicing method. There is no motivation to use an SNN since the slicing is only a minimal energy and latency fraction of a pipeline that uses transformers or regression.\nThere is no comparison with architectures that use other event representations like time surfaces."
                },
                "questions": {
                    "value": "Weaknesses are numbered and should be considered as questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751944998,
            "cdate": 1698751944998,
            "tmdate": 1699636279918,
            "mdate": 1699636279918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5jnWiU7mQ5",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 1)"
                    },
                    "comment": {
                        "value": "**Weakness 1 (Slicing Sensitivity)**\n\n**Q1:** *Frame-like inputs to transformers or CNNs where frames have been derived from events may be sensitive to slicing. We need a toy experiment to study this hypothesis with a smaller network and different slicing techniques.*\n\n**A1:**\nThank you very much for your constructive suggestion! In response, we have conducted total 60 experiments with **different models** to investigate the impact of **different slicing techniques** and **different number of slices** on the performance in downstream tasks, thereby affirming the hypothesis that event streams are sensitive to slicing.\n\nIn our experiment, we employed two fixed slicing methods: (1). *Slicing with a fixed number of events* and (2). *Slicing with a fixed duration*. $N$ denotes the number of resulting event slices. Experimental results are detailed as follows: \n\n| NCaltech101   | $N$  |          2 |          4 |          6 |          8 |         10 |         12 |         14 |         16 |         18 |         20 |         22 |         24 |         26 |         28 |         30 |     Mean|     Var|\n|:---------------|:-------------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|\n| ResNet18       | Fixed Count     |   70.96 | 75.26 | 75.39 | 75.30  | 76.09 | 73.95 | 74.09 | 73.80  | 76.40  | 75.39 | 75.45 | 73.60  | 71.94 | 71.01 | 71.17 | **73.98**|**3.33**|\n| ResNet18       | Fixed Time     | 62.90  | 72.64 | 76.38 | 74.48 | 74.91 | 73.70  | 74.30  | 74.69 | 76.95 | 74.75 | 74.46 | 74.42 | 71.61 | 71.52 | 69.69 |**73.16**|**10.80**|\n| ResNet34       | Fixed Count| 72.19 | 75.55 | 76.98 | 78.22 | 77.14 | 77.40  | 76.78 | 76.90  | 78.14 | 77.06 | 76.91 | 74.85 | 74.76 | 76.91 | 73.07 |**76.19**|**2.90**|\n| ResNet34       | Fixed Time| 65.42 | 75.92 | 78.29 | 78.20  | 78.48 | 76.22 | 77.76 | 76.57 | 75.94 | 76.80  | 76.61 | 75.91 | 75.11 | 74.76 | 74.19 |**75.74**|**9.15**|\n\nThe results indicate significant fluctuations (large variance) in downstream performance based on the slicing method and the number of slices used. We believe this addition effectively demonstrates the sensitivity of event streams to fixed slicing techniques, confirming the need for our motivation to propose dynamic slicing of event streams. \nAdditionally, the accuracy achieved using the dynamic slicing method (82.54% by ResNet34) surpasses that of any fixed slicing approach (with the highest being 78.48%), further substantiating the efficacy of the dynamic method in our study.\n\nThanks for your suggestion!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226881123,
                "cdate": 1700226881123,
                "tmdate": 1700470987660,
                "mdate": 1700470987660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rXM9jNSf7l",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 2.1)"
                    },
                    "comment": {
                        "value": "**Weakness 2 (Statistics of Slicing Method)**\n\n**Q2:** *The exposition is really hard to follow. As stated directly after eq. 4, the slicing is done by grouping together events whose timestamps are between two output spikes of the SNN. Here, an experiment is needed on the statistics of this slicing and why such an approach makes sense.*\n\n**A2:**\nTo demonstrate the effectiveness of our proposed dynamic slicing method, we provide the following statistical results:\n\n### **1. Statistics of dynamic slicing (BEEF) vs. fixed slicing.**\n*Symbol Description: the total event stream $E$; the resulting sliced sub-event stream list by BEEF: $E_{beef}=[E^b_{1},..E^b_{N_1}]$; the resulting sliced sub-event stream list by fixed slicing method: $E_{fixtime}=[E^f_{1},..E^f_{M_1}]$.*\n\nIn the tracking task, the average duration of each sub-event stream $E^b_{k}(k\\in[1,N_1])$ is 65ms (corresponding to 13 event cells, and the duration of the event stream contained in each event cell is 5ms). The maximum duration of each sub-event stream is 100ms, and the minimum duration is 30ms, while for our comparison of the slicing-by-fixed-time approach, the duration of each sub-event stream $E^f_{j}(j\\in[1,M_1])$ is fixed at 75ms. The following are specific statistics:\n\n|Method| Avg Cell Num| Var Cell Num | Avg Duration| Min Duration | 25th Duration| 75th Duration| Max Duration|\n|:--------|:--------------------------------------|:---------------------------------------|:------------------------------|:--------|:-----|:-----|:----|\n| BEEF                                  |  12.99                          | 3.96    | ~65ms                                  |25ms    | 50ms   | 80ms  | 100ms |\n| Slice by fixed duration               |  15                             | 0       | 75ms                                  |//   | //   | //  | //  |\n\nWe will put the visualization of the statistic results in the appendix of the revised version.\n\n### **2. Statistics of event density.**\n\nWe also counted the average density of sub-event streams after fixed slicing and dynamic slicing for comparison.\n\n*Symbol Description: For each sub-event stream $E^b_{k}$ or $E^f_{j}$, it contains several event points $e_i=[x_i,y_i,t_i,p_i]$. We define a matrix $C$ to represent the event count, where $C_{xy}$ represents the number of events at coordinates $(x,y)$. Given a threshold $T$, we define the event density to be: $D = \\frac{\\sum_{x,y}\\mathbb{1}\\{C_{x,y}\\geq T\\}}{\\sum_{x,y}\\mathbb{1}\\{C_{x,y}> 0\\}}$. The threshold $T$ is determined based on the percentile of the event count $C_{xy}$.*\n\n**Density Analysis**: We define the density of events as a metric reflecting the amount of event information within sub-event streams. The smaller the fluctuation in the density of each sub-stream (the smaller the variance), the more stable the event information contained. The stability of the event density is crucial for scenarios where the distribution of events is not uniform (e.g., scenarios with changing motion speed).\n\n\nWe chose $T = 30$% and $90$% (lower $T$ to observe overall event activity, including those less frequent events; higher $T$ for focusing on repetitive or frequent events) to validate the effectiveness of BEEF to dynamically slice the event stream. Below are the statistics of event density, where the data (left vs right) on the left are statistics of BEEF and on the right for statistics of fixed slicing. We select 4 classes in the FE108 tracking dataset.\n\n| T=30% | airplane_mul222 | box_hdr | dog | tank_low |\n|:------|:----------------|:--------|:----|:---------|\n| Mean  | 0.9196 vs 0.9074 | 0.9850 vs 0.9850 | 0.9208 vs 0.8984 | 0.9584 vs 0.9575 |\n| Var $\\downarrow$| **0.0151** vs 0.0164 | **0.0038** vs 0.0039 | **0.0144** vs 0.0168 | **0.0096** vs 0.0100 |\n| Std $\\downarrow$ | **0.1231** vs 0.1283 | **0.0622** vs 0.0626 | **0.1200** vs 0.1299 | **0.0984** vs 0.1003 |\n\n| T=90% |airplane_mul222 | box_hdr | dog | tank_low|\n|:------|:----------------|:--------|:----|:---------|\n| Mean | 0.1256 vs 0.1263 | 0.1338 vs 0.1405 | 0.1120 vs 0.1131 | 0.9584 vs 0.9575 |\n| Var $\\downarrow$| 0.0004 vs 0.0004 | **0.0007** vs 0.0053 | 0.0001 vs 0.0001 | **0.0096** vs 0.0100 |\n| Std $\\downarrow$| 0.0215 vs 0.0215 | **0.0267** vs 0.0732 | **0.0073** vs 0.0078 | **0.0984** vs 0.1003 |\n\nThe results show that the event stream after BEEF's dynamic slicing has a more stable event density (low variance), which verifies that BEEF has a certain ability to perceive the event information, and ensures that BEEF is robust in different motion scenarios, as shown in Table 3."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227267943,
                "cdate": 1700227267943,
                "tmdate": 1700227267943,
                "mdate": 1700227267943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TY5wH1j25T",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 3)"
                    },
                    "comment": {
                        "value": "**Weakness 3 (Explaination of our method)**\n\n**Q3:** *4.3.1 has to be elaborated. While the math derivations are sound, it is not clear to the reader why the starting point of the derivations is the desire for $S_{out}$ to spike at $n^*$. I tried to understand it also through the observations in 4.3.2 but could not.*\n\n**A3:**\nThank you for your query which provides us with an opportunity to clarify our methodology. \n\nContrary to fixed slicing methods (such as predetermined time intervals or event counts), our approach dynamically determines event stream slicing based on spike occurrences in the SNN, detailed in Figure 1.\n\nIn order to supervise the SNN to slice the event stream at the optimal position, we feed the events sliced at the SNN-determined position as well as the events sliced at its neighboring positions into the downstream model.\nThe downstream model then returns the loss (e.g., classification loss, tracking loss), where the position corresponding to the minimum loss indicates the optimal slicing point (Eq.10). Thus, we obtain a position label $n^*$, which in turn guides the SNN to make the best slicing strategy through supervised learning. \n\nIn summary, when ANN feedback indicates that slicing at the $n^*$ position can enhance model performance, the SNN discriminator is directed to spike at the $n^*$. Section 4.3 of our paper delves into formulating the SPA-CE loss function, which aids the SNN to spike at this specified $n^*$ position. \n\nWe hope this response clarifies our approach and methodology. We are committed to improving the manuscript in its revised version for better understanding. Should there be any points needing further clarification, please feel free to reach out."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228098257,
                "cdate": 1700228098257,
                "tmdate": 1700228098257,
                "mdate": 1700228098257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bqxvIMY2HH",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 4)"
                    },
                    "comment": {
                        "value": "**Weakness 4 (Explaination of $T$*)**\n\n**Q4:** *The beginner's arena was meant to explain the above but is incomprehensible. What does it mean ``to slice at a specified time step $T$ * '' ?*\n\n\n**A4:**\nFollowing up on the explanation of Reply 3, consider the downstream ANN's feedback indicates that the SNN should spike at a specific moment $n^*$ to slice the event stream. In this case, our goal is to supervise the SNN to ensure that it spikes precisely at $n^*$. The purpose of \"The beginner's arena\" is to demonstrate that our proposed loss function SPA-CE can effectively guide the SNN to pulse at $n^*$, in contrast to common loss functions like Cross-Entropy (CE) or Mean Squared Error (MSE), which could not achieve this.\n\nThe term $T^*$ used in the text may have caused some confusion. In future revisions of our manuscript, we will standardize this notation to $n^*$ to avoid any ambiguity. We hope this clarification helps and apologize for any confusion caused. Thank you for the reminder."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228247204,
                "cdate": 1700228247204,
                "tmdate": 1700228247204,
                "mdate": 1700228247204,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kdtuU2LdTt",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 5)"
                    },
                    "comment": {
                        "value": "**Weakness 5 (Energy Computation)**\n\n**Q5:** *It is not clear what purpose the energy computations of the SNN serve when the task will be solved with ultra consuming GPUs.*\n\n**A5:**\nThank you for your comment. Indeed, you are correct that the implementation of Spiking Neural Networks (SNNs) on GPUs does not currently confer a significant energy advantage. This is largely due to the limitations inherent in current SNN simulation platforms (such as spikingjelly[9] and tonic[10]), which are primarily GPU-based.\n\nHowever, it is important to note that SNNs demonstrate significant energy efficiency when operated on neuromorphic hardware, such as Loihi [7] and TrueNorth [8]. This advantage is well-established and extensively discussed in recent literature [1,2]. The energy calculations in our paper [3] are intended to provide an estimation of the potential energy efficiency of SNNs with future advancements in hardware and algorithms. In addition, comparing the theoretical energy consumption of SNNs with that of traditional Artificial Neural Networks (ANNs) is commonly-adopted in this field [4,5,6], aiding in the understanding of the energy dynamics of these systems.\n\n***Reference:***\n\n[1] Yin B, Corradi F, Boht\u00e9 S M. Accurate online training of dynamical spiking neural networks through Forward Propagation Through Time. Nature Machine Intelligence 2023.\n\n[2] Schuman C D, Kulkarni S R, Parsa M, et al. Opportunities for neuromorphic computing algorithms and applications. Nature Computational Science 2022.\n\n[3] Yao M, Zhao G, Zhang H, et al. Attention spiking neural networks. TPAMI 2023.\n\n[4] Kim S, Park S, Na B, et al. Spiking-yolo: spiking neural network for energy-efficient object detection. AAAI 2020. \n\n[5] Zhou Z, Zhu Y, He C, et al. Spikformer: When spiking neural network meets transformer. ICLR 2023.\n\n[6] Wang Z, Fang Y, Cao J, et al. Masked Spiking Transformer. ICCV 2023.\n\n[7] Davies M, Srinivasa N, Lin T H, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro 2018.\n\n[8] Akopyan F, Sawada J, Cassidy A, et al. Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE TCAD 2015.\n\n[9] Fang W, Chen Y, Ding J, et al. SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence. Science Advances 2023.\n\n[10] Eshraghian J K, Ward M, Neftci E O, et al. Training spiking neural networks using lessons from deep learning. Proceedings of the IEEE 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228370707,
                "cdate": 1700228370707,
                "tmdate": 1700228370707,
                "mdate": 1700228370707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hBtUqYeCPA",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 6)"
                    },
                    "comment": {
                        "value": "**Weakness 6 (Comparisions of Different Event Representation)**\n\n**Q6:** *The experimental comparison should be with approaches that are asynchronous end to end like HOTS or HATS or Cannici'19, Perot'20 etc. or approaches like the Event Transformer.*\n\n**A6:**\nThank you for your suggestion! The above methods you mentioned are all about event representation methods. It is worth noting that **our work focuses on the slicing of the event stream rather than focusing on event representation.** Event representation refers to the process of event information extraction that is performed after the event stream has been sliced into sub-event stream, and the resulting event representation meets the neural network input requirements. Thus, our dynamic slicing process and event representation can be used at the same time, either better slicing or representation method benefits the feature extraction with neural network, thus improving performance.\n\nTo validate the effectiveness of our slicing approach, we supplement the event-based recognition task below. We compare the downstream performance of three different event representation methods (including Event Frame [1], Event Spike Tensor (EST [2]) and Voxel Grid [3]) on the DVSGesture dataset under fixed slicing and dynamic slicing: \n\n\n| DVSGesture |Event Frame | Event Spike Tensor | Voxel Grid |\n|:------|:----------------|:--------|:----|\n| Fix Duration | 93.75% | 93.75% | 88.54% |\n|Fix Event Count |93.06%|94.79%|88.19%|\n|**BEEF(ours)** |**94.79%**|**95.49%**|**89.24%**|\n\n\nThe results demonstrate that across different event representation methods, our dynamic slicing approach, BEEF, outperforms fixed slicing methods in downstream tasks. This underscores the efficacy of BEEF in handling event streams. \n\nThanks for your suggestion! We will cite the event representation methods you mentioned in the revised paper and try to integrate them with BEEF in the future.\n\n***Reference:***\n\n[1] Maqueda A I, Loquercio A, Gallego G, et al. Event-based vision meets deep learning on steering prediction for self-driving cars. CVPR 2018.\n\n[2] Gehrig D, Loquercio A, Derpanis K G, et al. End-to-end learning of representations for asynchronous event-based data. ICCV 2019.\n\n[3] Zhu A Z, Yuan L, Chaney K, et al. Unsupervised event-based learning of optical flow, depth, and egomotion. CVPR 2019."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228582909,
                "cdate": 1700228582909,
                "tmdate": 1700228582909,
                "mdate": 1700228582909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8QCTuUHyiq",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 7)"
                    },
                    "comment": {
                        "value": "**Weakness 7 (Improvement over baseline)**\n\n**Q7:** *Table 3: It is not discussed why the transformer tracker performs almost the same or better without BEEF. Why does BEEF not add anything significant when an attention mechanism is used?*\n\n**A7:**\nThanks! In fact, for TransT, there are only very few metrics where BEEF does not perform better than baselines, and here's a specific analysis of the amount of improvement in TransT results:\n\n|| HDR     |      |      |      | LL      |      |      |      | FWB     |      |      |      | FNB     |      |      |      | ALL     |      |      |      |\n|:--------|:--------|:----|:----|:----|:--------|:----|:----|:----|:--------|:----|:----|:----|:--------|:----|:----|:----|:--------|:----|:----|:----|\n|| RSR     | OP.50| OP.75| RPR  | RSR     | OP.50| OP.75| RPR  | RSR     | OP.50| OP.75| RPR  | RSR     | OP.50| OP.75| RPR  | RSR     | OP.50| OP.75| RPR  |\n| TransT (1)   | 55.9 | 71.0   | 24.6 | **84.5**    | 66.8 | 88.9 | 34.3 | 96.5    | 74.1 | **98.6** | 54   | **99.9**    | 55.8 | 69.2 | 24.9 | **85.4**    | 59.6 | 76.4 | 29   | 88.8 |\n| TransT+fixed slice (2)| 51.4 | 67.8 | 11.1 | 81.2    | 63.2 | 80.2 | 28.3 | 89.3    | 41.5 | 28   | 2.5  | 57.7    | 50.6 | 57.9 | 12.7 | 78.9    | 51   | 59   | 12   | 78.8 |\n| TransT+BEEF (3) | **57.7** | **75.2** | **28.1** | 82.6    | **70.7** | **93.6** | **42.7** | **99.0**      | **74.9** | 97.7 | **61.1** | 98.6    | **58.7** | **75.6** | **29.6** | 84.6    | **62.4** | **81.2** | **34.1** | **88.9** |\n| *Improvement* (3)-(1)     | 1.8  | 4.2  | 3.5  | -1.9     | 3.9  | 4.7  | 8.4  | 2.5      | 0.8  | -0.9 | 7.1  | -1.3     | 2.9  | 6.4  | 4.7  | -0.8     | 2.8  | 4.8  | 5.1  | 0.1  |\n|  *Improvement* (3)-(2)        | 6.3  | 7.4  | 17   | 1.4      | 7.5  | 13.4 | 14.4 | 9.7      | 33.4 | 69.7 | 58.6 | 40.9     | 8.1  | 17.7 | 16.9 | 5.7      | 11.4 | 22.2 | 22.1 | 10.1 |\n\nThe results demonstrates that 16 out of 20 metrics outperform baselines, and the total average improvement is 6.61%. The bolding value in Table 3 of our original submitted paper indicates a global optimum, which may cause confusion. We'll change this in our revised version shortly, thanks!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228628903,
                "cdate": 1700228628903,
                "tmdate": 1700228628903,
                "mdate": 1700228628903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C6Os3AjwE2",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 8)"
                    },
                    "comment": {
                        "value": "**Weakness 8 (Learnable Feedback Strategy)**\n\n**Q8:** *The feedback strategy is learnt during training. I understand that in this sense it is adaptive to the task rather than during inference to the event stream when the hyperparameters will be fixed.*\n\n**A8:**\nThat's right! The feedback strategy is indeed learned during the training process. Therefore, during inference, we utilize the trained SNN to slice the event stream instead of using a fixed set of hyperparameters for this task. The event frames sliced by the SNN are then directly fed into the downstream ANN model for testing."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228666755,
                "cdate": 1700228666755,
                "tmdate": 1700228666755,
                "mdate": 1700228666755,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EOM6nCQBuY",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 9)"
                    },
                    "comment": {
                        "value": "**Weakness 9 (Polarity Process)**\n\n**Q9:** *It is unclear whether events are treated differently according to their polarity.*\n\n\n**A9:**\nWe apologize for not talking about the handling of event polarity in the main text. Our approach retains the polarity information of the events. The information for each polarity is accumulated separately. Additionally, we have compared our method with other event representation techniques, which are presented in Reply 6. We will ensure to clearly articulate these experimental details in the subsequent version of our manuscript."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228711519,
                "cdate": 1700228711519,
                "tmdate": 1700228711519,
                "mdate": 1700228711519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DFJnSyYkJB",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 10)"
                    },
                    "comment": {
                        "value": "**Weakness 10 (Missing Definition)**\n\n**Q10:** *There is some problem with the definition of $D$ because $n_q$ is not defined anywhere but mentioned ``where \n$n_q$ denotes the time of the last spike''.*\n\n\n**A10:**\nMy apologies for the confusion caused by the writing error in Section 4.2. The phrase \"where $n_q$ denotes the time of the last spike\" should indeed be removed as it was not previously defined. Thank you for bringing this to our attention. We will correct this error shortly!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228752819,
                "cdate": 1700228752819,
                "tmdate": 1700228752819,
                "mdate": 1700228752819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gONp0t1o2H",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 11)"
                    },
                    "comment": {
                        "value": "**Weakness 11 (Latency Computation)**\n\n**Q11:** *It would be worth listing the latency from event to GPU output for the particular architectures on tracking and recognition. This is much more critical here than the power consumption of the CNN.*\n\n\n**A11:**\nI agree. It's of great significance to consider the latency of in the processing of events, especially in practical applications. As documented in Table 2 of our manuscript, the Frames Per Second (FPS) rate achieved by utilizing the SNN to slice the event stream is 111Hz, which corresponds to processing once every 0.009 seconds. This rate is significantly lower than the latency experienced by the downstream ANN models during tracking tasks, which have an FPS rate of 39. Consequently, our BEEF model can facilitate real-time dynamic event slicing. \n\n\n|  |Latency | FPS |\n|:------|:----------------|:--------|\n| SNN | 0.009s per img | 111 Hz| \n|ANN |0.025s per img|39 Hz|"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228806685,
                "cdate": 1700228806685,
                "tmdate": 1700228806685,
                "mdate": 1700228806685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XUYQx9Dm8b",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 12.1 Summary 1)"
                    },
                    "comment": {
                        "value": "Thanks again for all your suggestions and questions! Your valuable suggestions greatly help us to improve the content and quality of our articles! Please allow me to revisit and clarify the motivation behind our dynamic event stream slicing algorithm and how we've verified its effectiveness.\n\n### **1. Motivation for Proposing a Dynamic Event Stream Slicing Algorithm**\n\nLet's start by clarifying the process of event-to-frame conversion, which is mainly divided into two steps: **Step 1. Slice the raw event stream into multiple sub-event stream** and **Step 2. convert these sub-event streams into frames using various event representation methods.** While many works has focused on optimizing event representation (Step 2) to extract better event information, including time surface and EST, they do not address the issues arised with fixed slicing (e.g., resulting non-uniform event in scenarios with changing motion speed). Despite event slicing being a small part of the overall pipeline, it is a critical point. This is because the event stream is very sensitive to slicing, and the model performance fluctuates very much for different slicing methods, as proved by extensive experiments in Reply 1.\n\nTo better address this issue, we introduced the dynamic slicing framework BEEF. Meanwhile, BEEF is guided by downstream task feedback to ensure that the new sub-streams could enhance downstream task performance.\n\n\n\n\n### **2. Motivation for Using SNN as a Slicing Trigger**\n\nThe reason why we choose SNN as the event slicing trigger is twofold:\n\n- Utilizing SNNs on neuromorphic hardware for processing event streams is low-energy and low-latency [1,2].\n\n- Deployed on neuromorphic hardware, SNNs can process event streams asynchronously [3,4,5], conserving energy when there is no data input\u2014a capability that GPUs, operating synchronously, lack.\n\nDue to the aforementioned reasons, there is a considerable amount of research [6,7,8,9,10] employing Spiking Neural Networks (SNNs) for event data. Although these SNNs are simulated on GPU platforms, the models resulting from such simulations could be deployed on neuromorphic hardware [3,4].\n\n### **3. Contribution for Using SNN as a Slicing Trigger**\nWe propose a new cooperative paradigm where SNN acts as an efficient, low-energy data processor to assist the ANN in improving downstream\nperformance. This is a brand-new SNN-ANN cooperation way, paving the way for future event-related implementation on neuromorphic chips.\n\n### **4. Comparison with Other Event Representations**\nAlthough our article focuses on dynamic event stream slicing, not event representation, we appreciate your suggestion that verifying the effectiveness of dynamic slicing BEEF through different event representations is necessary. As I mentioned earlier, event slicing and event representation are not in conflict, and a fusion of the two may have potentially better enhancements. Experiments with different slicing methods and different event representation have been supplemented in Reply 6."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229111422,
                "cdate": 1700229111422,
                "tmdate": 1700493062199,
                "mdate": 1700493062199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Am2MEdXbPd",
                "forum": "IHedM0Zem9",
                "replyto": "2GkRi9v1S4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to q3dC (Reply 12.2 Summary 2)"
                    },
                    "comment": {
                        "value": "**Continued from Reply 12.1.**\n\n### **Summary**\nThis article focuses on dynamic event slicing methods and also proposes a cooperative ANN-SNN paradigm for future deployment on hardware.\nWe are deeply grateful for your advice! We are making every effort to resolve any confusion and will improve sections that could cause misunderstandings in the updated version, such as why we guide the SNN to pulse at $n^*$ and the difference between event slicing and event representation. If anything is still unclear, we will promptly revise and respond! Thanks!\n\n\n***Reference:***\n\n[1] Davies M, Srinivasa N, Lin T H, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro 2018.\n\n[2] Akopyan F, Sawada J, Cassidy A, et al. Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE TCAD 2015.\n\n[3] Roy A, Nagaraj M, Liyanagedera C M, et al. Live Demonstration: Real-time Event-based Speed Detection using Spiking Neural Networks. In CVPRW 2023.\n\n[4] Viale A, Marchisio A, Martina M, et al. Carsnn: An efficient spiking neural network for event-based autonomous cars on the loihi neuromorphic research processor. In IJCNN 2021.\n\n[5] Yu F, Wu Y, Ma S, et al. Brain-inspired multimodal hybrid neural network for robot place recognition. Science Robotics 2023.\n\n[6] Hagenaars J, Paredes-Vall\u00e9s F, De Croon G. Self-supervised learning of event-based optical flow with spiking neural networks. NeurlPS 2021.\n\n[7] Yao M, Gao H, Zhao G, et al. Temporal-wise attention spiking neural networks for event streams classification. ICCV 2021.\n\n[8] Zhu L, Wang X, Chang Y, et al. Event-based video reconstruction via potential-assisted spiking neural network. CVPR 2022\n\n[9] Kosta A K, Roy K. Adaptive-spikenet: event-based optical flow estimation using spiking neural networks with learnable neuronal dynamics. ICRA 2023.\n\n[10] Hussaini S, Milford M, Fischer T. Spiking neural networks for visual place recognition via weighted neuronal assignments. RAL 2023."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229197211,
                "cdate": 1700229197211,
                "tmdate": 1700229230739,
                "mdate": 1700229230739,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZWQWYq4fXc",
            "forum": "IHedM0Zem9",
            "replyto": "IHedM0Zem9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_48Pk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3307/Reviewer_48Pk"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies to learn event splits by using SNN. The triggered spikes from SNN are treated as signals for splitting event streams and constructing event frames. The proposed architecture is evaluated with object recognition and single object tracking datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The motivation of the paper is well demonstrated. Fixed event stream fixed slicing methods potentially fail to generalize in different motion scenarios.\n* How the paper finds optimal spike time, $n_{s}$, is interesting.\n* The paper shows relative improvements over different baseline methods when using their proposed BEEF framework."
                },
                "weaknesses": {
                    "value": "* The paper claims a fixed event split method fails to generalize. However,  event cell $C[N]$ is a discrete 2D representation generated from a fixed event split, and is used as the input for SNN. \n* BEEF can be used in ANN-based 3D CNN/Transformer seamlessly. Event cameras and SNN are all bio-inspired but do not necessarily imply that SNN is a good fit to event data."
                },
                "questions": {
                    "value": "* Why not experiment with the latest event recognition/single object tracking framework? The latest methods in Tab. 1 and Tab. 3 were published in 2021?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3307/Reviewer_48Pk",
                        "ICLR.cc/2024/Conference/Submission3307/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851899521,
            "cdate": 1698851899521,
            "tmdate": 1700719409160,
            "mdate": 1700719409160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2yOgg1RsIG",
                "forum": "IHedM0Zem9",
                "replyto": "ZWQWYq4fXc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 48Pk (Reply 1)"
                    },
                    "comment": {
                        "value": "**Weakness 1 (Event Cell)**\n\n**Q1:** *The paper claims a fixed event split method fails to generalize. However, event cell $C[N]$ is a discrete 2D representation and used as the input for SNN is generated from a fixed event split.*\n\n**A1:**\nThank you for your comment! You are correct in noting that our event cells adopt a 2D format. Ideally, our goal is for the Spiking Neural Network (SNN) to process raw event inputs asynchronously. However, due to the constraints that CNN-based deep learning frameworks can only accept 2D inputs, we also adopted 2D format, which is a commonly-used method for processing event [1,2,3].\n\nIn our work, we have endeavored to slice the entire event stream into very fine event cells along the timeline as much as possible, since we want to find the optimal and accurate event slices. \nThese event cells contain very little information, and our SNN is able to process the event cell and supervised to slice the event stream adaptively.\n\nIn the future, we expect to further our research by deploying SNNs on neuromorphic hardware. This would enable the asynchronous processing of event streams using BEEF, potentially overcoming current constraints and enhancing the efficiency of our approach.\n\n***Reference:***\n\n[1] Zhang J, Yang X, Fu Y, et al. Object tracking by jointly exploiting frame and event domain. ICCV 2021.\n\n[2] Baldwin R W, Liu R, Almatrafi M, et al. Time-ordered recent event (TORE) volumes for event cameras. TPAMI 2022.\n\n[3] Maqueda A I, Loquercio A, Gallego G, et al. Event-based vision meets deep learning on steering prediction for self-driving cars. CVPR 2018."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226563308,
                "cdate": 1700226563308,
                "tmdate": 1700226563308,
                "mdate": 1700226563308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NttLk9PNVs",
                "forum": "IHedM0Zem9",
                "replyto": "ZWQWYq4fXc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 48Pk (Reply 2)"
                    },
                    "comment": {
                        "value": "**Weakness 2 (Match between SNN and Event)**\n\n**Q2:** *BEEF can be used in ANN-based 3D CNN/Transformer seamlessly. Event cameras and SNN are all bio-inspired but do not necessarily imply that SNN is a good fit to event data.*\n\n**A2:**\nThank you for your comment!\nIn the original article, we mentioned that ``both SNN and event stream are brain-like inspired, which motivates us to use SNN to process events'', and there is indeed a problem with the logic of this statement. I appreciate you reminding us of this point. Therefore, here we have reorganized the motivation for using SNN to process events:\n\n- Utilizing SNNs on neuromorphic hardware for processing event streams is low-energy and low-latency [1,2].\n\n- Deployed on neuromorphic hardware, SNNs can process event streams asynchronously [3,4,5], conserving energy when there is no data input\u2014a capability that GPUs, operating synchronously, lack.\n\nDue to the aforementioned reasons, there is a considerable amount of research [6,7,8,9,10] employing Spiking Neural Networks (SNNs) for event data. Although these SNNs are simulated on GPU platforms, the models resulting from such simulations could be deployed on neuromorphic hardware [3,4].\n\nWe hope this explanation satisfactorily addresses your concerns and illustrates the rationale behind our methodology.\n\n***Reference:***\n\n[1] Davies M, Srinivasa N, Lin T H, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro 2018.\n\n[2] Akopyan F, Sawada J, Cassidy A, et al. Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip. IEEE TCAD 2015.\n\n[3] Roy A, Nagaraj M, Liyanagedera C M, et al. Live Demonstration: Real-time Event-based Speed Detection using Spiking Neural Networks. In CVPRW 2023.\n\n[4] Viale A, Marchisio A, Martina M, et al. Carsnn: An efficient spiking neural network for event-based autonomous cars on the loihi neuromorphic research processor. In IJCNN 2021.\n\n[5] Yu F, Wu Y, Ma S, et al. Brain-inspired multimodal hybrid neural network for robot place recognition. Science Robotics 2023.\n\n[6] Hagenaars J, Paredes-Vall\u00e9s F, De Croon G. Self-supervised learning of event-based optical flow with spiking neural networks. NeurlPS 2021.\n\n[7] Yao M, Gao H, Zhao G, et al. Temporal-wise attention spiking neural networks for event streams classification. ICCV 2021.\n\n[8] Zhu L, Wang X, Chang Y, et al. Event-based video reconstruction via potential-assisted spiking neural network. CVPR 2022\n\n[9] Kosta A K, Roy K. Adaptive-spikenet: event-based optical flow estimation using spiking neural networks with learnable neuronal dynamics. ICRA 2023.\n\n[10] Hussaini S, Milford M, Fischer T. Spiking neural networks for visual place recognition via weighted neuronal assignments. RAL 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226673472,
                "cdate": 1700226673472,
                "tmdate": 1700226673472,
                "mdate": 1700226673472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O0rVB1CuOu",
                "forum": "IHedM0Zem9",
                "replyto": "ZWQWYq4fXc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 48Pk (Reply 3)"
                    },
                    "comment": {
                        "value": "**Weakness 3 (Latest Methods)**\n\n**Q3:** *Why not experiment with the latest event recognition/single object tracking framework? The latest methods in Tab. 1 and Tab. 3 were published in 2021?*\n\n**A3:**\nTo enhance the credibility and robustness of our results, we have incorporated state-of-the-art models: Swin Transformer (SwinT) and Vision Transformer (ViT), to further validate the efficacy of our BEEF algorithm in event-based recognition tasks:\n\n\n| **Method** | Random Slice | Fixed Slice | **Ours** |\n|:-------|:-------|:-------|:------|\n| SwinT | 88.19 | 89.93 | **91.67(+1.74%)**|\n| ViT | 87.50 | 85.07 |**88.54(+3.47%)**|\n\n*Experiment Settings: We choose SwinT-small and ViT-small for comparisons on DVSGesture dataset. Other settings are consistent with the main experiments.*\n\nBEEF is designed as a **plug-and-play** algorithm for dynamic event stream slicing. It is benchmarked against baselines that employ fixed event stream slicing methods. Our approach is versatile and can be applied in any event recognition or single object tracking framework (including both classical and latest). \n\nWe appreciate your suggestion and hope that this response adequately addresses your query."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226763564,
                "cdate": 1700226763564,
                "tmdate": 1700487098400,
                "mdate": 1700487098400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yL0skxEpN3",
                "forum": "IHedM0Zem9",
                "replyto": "ZWQWYq4fXc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3307/Reviewer_48Pk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3307/Reviewer_48Pk"
                ],
                "content": {
                    "title": {
                        "value": "Event Cell"
                    },
                    "comment": {
                        "value": "Thank you for the clarification. I will maintain my rating. Alternative approaches such as representing event data as a graph could potentially address the conflict."
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703229011,
                "cdate": 1700703229011,
                "tmdate": 1700703550369,
                "mdate": 1700703550369,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]