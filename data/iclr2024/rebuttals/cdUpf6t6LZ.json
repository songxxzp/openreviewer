[
    {
        "title": "Robust NAS benchmark under adversarial training: assessment, theory, and beyond"
    },
    {
        "review": {
            "id": "XGj9oOJEmH",
            "forum": "cdUpf6t6LZ",
            "replyto": "cdUpf6t6LZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3918/Reviewer_AZRS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3918/Reviewer_AZRS"
            ],
            "content": {
                "summary": {
                    "value": "For fast and standardized evaluation of Neural Architecture Search algorithms, it is important to develop various benchmarks that contain a large set of architectures and their quality measured with diverse performance metrics. In this vein, the paper builds and releases a new NAS benchmark that considers not only the standard accuracy of evaluated architectures but also their robustness against adversarial attacks. The authors extend the existing NAS-Bench-201 benchmark by adversarially training the NAS-Bench-201 architectures and evaluating them on clean and perturbed data. The proposed robust NAS benchmark is named NAS-RobBench-201. Additionally, the authors theoretically characterize the robust generalization performance of architectures using the Neural Tangent Kernel (NTK) framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- As the authors note, adversarially training takes considerably more time than standard training with the cross-entropy loss. Given the enormous computational cost of adversarial training and evaluating a large number of architectures, I believe the authors really have done an impressive set of experiments.\n- I generally agree with the necessity to create a NAS benchmark that targets robustness and generalization beyond standard test data.\n- I do not feel confident enough to review and comment on the more theoretical parts of the paper. The proofs were not reviewed thoroughly, and thus I cannot vouch for its correctness. I am, however, familiar with the NTK theory, and I find the idea of extending the NTK-based analysis to explain adversarial robustness interesting."
                },
                "weaknesses": {
                    "value": "- Currently, the robust accuracy is measured only under FGSM and PGD attacks. I think the performance under stronger attack methods (e.g., AutoAttack) is necessary to make the benchmark more reliable.\n- The authors observe that the standard and robust accuracies exhibit a meaningful level of correlation. If they are, do we really need a separate robustness benchmark that includes robust accuracies? Wouldn\u2019t searching for the optimal architecture in a conventional sense work as a good proxy and naturally result in a more robust architecture?\n- I believe that NAS-Bench-201 is too constrained of a search space to accurately reflect the vastness of the potential architecture pool. For instance, [1] shows that many NTK-based methods fail outside the boundary of NAS-Bench-201. While this is a good first step towards building a more comprehensive robustness benchmark, it would be great if the authors could show that the experimental and theoretical analyses hold outside the tested search space.\n- It would be nice to include the performance of trained architectures on out-of-distribution data and/or datasets with distribution shift in the benchmark. Incorporating these other types of robustness would make the benchmark more comprehensive, especially considering that in real deployment scenarios, the architectures will encounter generic OOD data more often than adversarially-perturbed data.\n- In section 4.4, the authors study the correlation between the minimum eigenvalue of the NTK matrix and robust accuracy. I understand that this NTK-score is derived from theoretical resultss, but have the authors studied the relationship between other NTK-based scores (e.g., condition number [2]) and robust accuracy?\n\n[1] Mok, Jisoo, et al. \"Demystifying the neural tangent kernel from a practical perspective: Can it be trusted for neural architecture search without training?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[2] Chen, Wuyang, Xinyu Gong, and Zhangyang Wang. \"Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective.\" International Conference on Learning Representations. 2020."
                },
                "questions": {
                    "value": "Please refer to the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3918/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3918/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3918/Reviewer_AZRS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3918/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697993581461,
            "cdate": 1697993581461,
            "tmdate": 1699636351943,
            "mdate": 1699636351943,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wsuvnmYp4W",
                "forum": "cdUpf6t6LZ",
                "replyto": "XGj9oOJEmH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1 out of 2) to reviewer AZRS"
                    },
                    "comment": {
                        "value": "We thank the reviewer AZRS for the insightful feedback. We address the concerns below and add necessary revisions highlighted in red color in the updated version.\n\n---\n\n> **Q1:** [The performance under stronger attack methods (e.g., AutoAttack), out-of-distribution data and/or datasets with distribution shift.]\n\n**A1:** Thanks for the suggestion, the discussion on the AutoAttack can be found in [general response](https://openreview.net/forum?id=cdUpf6t6LZ&noteId=FRw76YCx2K) for details.\n\nFurthermore, to make the benchmark more comprehensive in a realistic setting, we evaluate 6466 architectures in the search space under distribution shift. We choose CIFAR-10-C [3], which includes 15 visual corruptions with 5 different severity levels, resulting in 75 new metrics. In the supplementary material, we have provided the result in a JSON file. \n\nWe add a new section Appendix F.4 to describe the evaluation and discuss the result. In Fig.7 of the revised version (or the [link](https://imgur.com/a/iVolRzr)), we show the boxplots of the accuracy. We can find that all architectures are robust towards corruption with lower severity levels. When increasing the severity levels to five, the models are no longer robust to fog and contrast architectures. Moreover, similar to robust accuracy under FGSM and PGD attacks, we can see a non-negligible gap between the performance of different architectures, which motivates robust architecture design. In Table 6 of the revised version (or the [link](https://imgur.com/a/iVolRzr)),  we show the Spearman coefficient between various accuracies on CIFAR-10-C and adversarial robust accuracy on CIFAR-10. Interestingly, we can see that the adversarial robust accuracy has a relatively high correlation with all corruptions except for contrast corruptions. As a result, performing NAS on the adversarially trained architectures in the benchmark can achieve a robust guarantee of distribution shift.\n\n---\n\n> **Q2:** [The authors observe that the standard and robust accuracies exhibit a meaningful level of correlation. If they are, do we really need a separate robustness benchmark that includes robust accuracies? Wouldn\u2019t searching for the optimal architecture in a conventional sense work as a good proxy and naturally result in a more robust architecture?]\n\n**A2:**  Let us explain why we do need a separate benchmark. We agree with the reviewer that there exists a partial correlation between clean accuracy and robust accuracy, as suggested by our benchmark. This is one of the findings from our benchmark: demonstrating that the correlation is not extremely low under adversarial training, compared to the benchmark under standard training [6]. We have included the discussion in Appendix B.2 of the paper. \n\nNotice that before building the benchmark, we did not know the relationship between clean accuracy and robust accuracy under adversarial training. To be specific, in the existing benchmark [6] that was constructed in the same space but under standard training, the correlation was not high. This correlation was enhanced only when adversarial training was involved. This is one finding from our benchmark. Moreover, we can see a notable difference between these two benchmarks in terms of top architecture id and selected nodes, see Fig. 4 and Fig. 8. This suggests that employing the benchmark under standard training to identify a resilient architecture may not guarantee its robustness under adversarial training.\n\n---\n\n> **Q3:** [NAS-Bench-201 is too constrained of a search space to accurately reflect the vastness of the potential architecture pool. For instance, [1] shows that many NTK-based methods fail outside the boundary of NAS-Bench-201. While this is a good first step towards building a more comprehensive robustness benchmark, it would be great if the authors could show that the experimental and theoretical analyses hold outside the tested search space.]\n\n**A3:** We thank the reviewer for pointing out this interesting question. When checking previous literature on NTK train-free algorithms, we find that NTK-based methods perform well on different search spaces beyond NAS-Bench-201, e.g., TE-NAS [2] and  EigenNAS [5] on DARTS search spaces. Nevertheless, the applicability of such results under adversarial training remains uncertain on other search spaces, as it requires expensive construction of the benchmark under adversarial training."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428337599,
                "cdate": 1700428337599,
                "tmdate": 1700428337599,
                "mdate": 1700428337599,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ShknvefTI5",
            "forum": "cdUpf6t6LZ",
            "replyto": "cdUpf6t6LZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3918/Reviewer_dbFp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3918/Reviewer_dbFp"
            ],
            "content": {
                "summary": {
                    "value": "The paper first presents an adversarially trained benchmark based on NASBench 201, and present NASRobBench 201."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "There is not much literature that presents a unified robust NAS benchmark, in that sense the work targets a unique space of research that needs exploration.\n\nThe NTK score vs accuracy correlation sounds interesting."
                },
                "weaknesses": {
                    "value": "1. It has been well known that adversarial robustness often may occur due to gradient obfuscation and is applicable for different sparse and dense model architectures [1,2]. Thus, a discussion on that would be necessary. In specific, is there a way to benchmark based on a subnet's susceptibility to be more prone towards obfuscation?\n\n2. Di you use ImageNet or ImageNet-16-120? As the Fig. 2 and the contribution section has mention of each.\n\n3. Please demonstrate the efficacy of the NASRobBench on Autoattack.\n\n4. The related work of Adversarial example generation is not up to date, the author should discuss more about the auto attacks and other contemporary attacks.\n\n[1] Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples, ICML 2018.\n\n[2] DNR: A Tunable Robust Pruning Framework Through Dynamic Network Rewiring of DNNs, ASP-DAC 2021."
                },
                "questions": {
                    "value": "1. Apart from the doubts in weakness, I have the following question:\n\na. How you add noise \"twice\"?\n\nb. Please extend the NTK score vs accuracy correlation with more recent and stronger attack scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3918/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804439436,
            "cdate": 1698804439436,
            "tmdate": 1699636351853,
            "mdate": 1699636351853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x9fkRvfOzi",
                "forum": "cdUpf6t6LZ",
                "replyto": "ShknvefTI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1 out of 2) to reviewer dbFp"
                    },
                    "comment": {
                        "value": "We thank the reviewer dbFp for the constructive feedback. We address the concerns below and incorporate necessary revisions highlighted in red color in the updated version.\n\n---\n\n> **Q1:** [It has been well known that adversarial robustness often may occur due to gradient obfuscation and is applicable for different sparse and dense model architectures [1,2]. Thus, a discussion on that would be necessary. In specific, is there a way to benchmark based on a subnet's susceptibility to be more prone towards obfuscation?]\n\n**A1:** We are thankful to the reviewer for the suggestion of this ablation experiment. Upon the suggestion of the reviewer, we add a new section in Appendix F.3 to discuss this.\n\nWe agree with the reviewer that gradient obfuscation might be a cause for the overestimated robustness of several adversarial defense mechanisms, e.g., parametric noise injection defense [3] and ensemble defenses [6], and can exist in the defense of several architectures [2]. However, in this work, we employ vanilla adversarial training as a defense approach, which does not suffer from gradient obfuscation, as originally demonstrated in [1] and mentioned in later literature [3].\n\n\nWe investigate whether the sparsity of the architecture design might have an impact on gradient obfuscation. Specifically, based on the search space in Figure 1 of the paper, we group all architectures in terms of the number of operators \u201cZeroize\u2019\u2019, which can represent the sparsity of the network (the more  \u201cZeroize\u201d operations, the more sparse the resulting network). Next, we select the network with the highest robust accuracy in each group and check whether this optimal architecture satisfies the following characteristics of non-gradient obfuscation [1]:\n-  a) One-step attack performs worse than iterative attacks.\n-  b) Black-box attacks perform worse than white-box attacks. \n-  c) Unbounded attacks fail to obtain $100\\%$ attack success rate.\n- d) Increasing the perturbation radius $\\rho$ does not increase attack success rate.\n- e) Adversarial examples can not be found by random sampling if gradient-based attacks do not.\n\nThe result below is the evaluation of these architectures under FGSM, Square attack, and PGD attack with varying perturbation radius. We can see that all of these architectures clearly satisfy a), c), and d). Regarding e), all of the networks still satisfy it because gradient-based attacks can find adversarial examples. For b), only the sparse network with 4 ``Zeroize\" does not satisfy the condition. Therefore, we can see that these architectures can not be considered as suffering gradient obfuscation, which is consistent with the original conclusion for adversarial training in [1].\n\n|Number of \u201cZeroize\"|Clean | FGSM, 3/255 | Square, 3/255 |PGD, 3/255|  PGD, 8/255 | PGD, 16/255| PGD, 32/255| PGD, 64/255|PGD, 128/255|PGD, 255/255|\n|--------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| 0  |0.7946|0.6975|0.6929|0.6924|0.4826|0.1774|0.0083|0.0000|0.0000|0.0000|\n| 1  |0.7928|0.6933|0.6884|0.6874|0.4798|0.1744|0.0077|0.0000|0.0000|0.0000|\n| 2 | 0.7842|0.6832|0.6786|0.6779|0.4711|0.1739|0.0080|0.0000|0.0000|0.0000|\n| 3 | 0.7456|0.6515|0.6482|0.6472|0.4442|0.1599|0.0079|0.0000|0.0000|0.0000|\n| 4 |0.5320|0.4560|0.4427|0.4528|0.3191|0.1393|0.0193|0.0010|0.0000|0.0000|\n\nLastly, under the aforementioned defense mechanisms e.g., noise injection defense and ensemble defenses, some of these architectures in the search space might be more prone to gradient obfuscation. Thus, we believe it is possible to further develop robust NAS benchmarks under other defense mechanisms, but this is out of the scope of the current work.\n\n\n---\n\n\n> **Q2:** [Did you use ImageNet or ImageNet-16-120? As the Fig. 2 and the contribution section has mention of each.]\n\nA2: Following NAS-Bench-201 [4], we conduct an evaluation on ImageNet-16-120, which is a well-established benchmark. In the updated version, we revised the contribution on page 2 as follows: \n\n\"107k GPU hours are required to build the benchmark on three datasets (CIFAR-10/100, ImageNet-16-120) under adversarial training.\"\n\n\n---\n\n> **Q3:** [Please demonstrate the efficacy of the NASRobBench on Autoattack.]\n\n**A3:**  Thanks for the suggestion, we have provided more evaluation of the benchmark, see [general response](https://openreview.net/forum?id=cdUpf6t6LZ&noteId=FRw76YCx2K) for details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241545238,
                "cdate": 1700241545238,
                "tmdate": 1700241545238,
                "mdate": 1700241545238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7CsVuiSDJs",
                "forum": "cdUpf6t6LZ",
                "replyto": "ShknvefTI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2 out of 2) to reviewer dbFp"
                    },
                    "comment": {
                        "value": "---\n\n> **Q4:** [The related work of Adversarial example generation is not up to date, the author should discuss more about the auto attacks and other contemporary attacks.]\n\n**A4:** In the revised version, we update the related work from the classical adversarial attack to up-to-date ways as follows:\n\nTo mitigate the effect of hyper-parameters in PGD and the overestimation of robustness [1], [7]  propose two variants of parameter-free PGD attack, namely, APGD_CE and APGD_DLR, where CE stands for cross-entropy loss and DLR indicates difference of logits ratio (DLR) loss. Both APGD_CE and APGD_DLR attacks dynamically adapt the step-size of PGD based on the loss at each step. Furthermore, to enhance the diversity of robust evaluation, [7] introduce Auto-attack, which is the integration of APGD_CE, APGD_DLR, Adaptive Boundary Attack (FAB) [4], and black-box Square Attack [5].\n\n---\n\n> **Q5:** [How you add noise \"twice\"?]\n\n**A5:** We perform one PGD/FGSM attack on the raw image data to generate the adversarial data, and then we perform the same attack on this adversarial data again. We have added such clarification in page 9 of the revised version.\n\n---\n\n> **Q6:** [Please extend the NTK score vs accuracy correlation with more recent and stronger attack scenarios.]\n\n**A6:**  We test the correlation between the robust accuracy under APGD_CE and the various NTK metrics in CIFAR-10. On one hand, we can still see that the robust NTK enables the increasing correlation under adversarial training, which is consistent with our observation in the original version for FGSM and PGD attacks. On the other hand, we can see the correlation decrease only ~0.5% for APGD accuracy compared to PGD accuracy. We have updated the result on the APGD attack In the revised version, see Fig.5 in page 9 or [the anonymous link](https://imgur.com/a/WcnEzOR).\n\n|Clean | FGSM, 3/255 | PGD, 3/255|  APGD, 3/255 | FGSM, 8/255| PGD, 8/255| APGD, 8/255|\n|--------|-------|-------|-------|-------|-------|-------|\n| KNAS, clean | 0.527 | 0.546 | 0.543 | 0.541 | 0.601 | 0.598 | 0.593 |\n| KNAS, PGD, 3/255 | 0.533 | 0.552 | 0.550 | 0.548 | 0.608 | 0.604 | 0.599 |\n| KNAS, PGD, 8/255 | 0.540 | 0.559 | 0.556 | 0.554 | 0.614 | 0.610 | 0.605 |\n| KNAS, twice-PGD, 3/255| 0.537 | 0.556 | 0.553 | 0.551 | 0.611 | 0.608 | 0.602 |\n| KNAS, twice-PGD,  8/255 | 0.549 | 0.568 | 0.565 | 0.564 | 0.622 | 0.619 | 0.615 |\n\n\n---\n\nIf the reviewer dbFp has any remaining concerns, we are happy to clarify further. \n\n---\n\n### Refs\n\n[1] Athalye, et al. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" ICLM, 2018. \n\n[2] Kundu, et al. \"Dnr: A tunable robust pruning framework through dynamic network rewiring of dnns.\" ASP-DAC, 2021. \n\n[3] He, et al. \"Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack.\" CVPR, 2019.\n\n[4] Dong, et al. \"NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search.\" ICLR, 2020.\n\n[5] Andriushchenko, et al. \"Square attack: a query-efficient black-box adversarial attack via random search.\" ECCV, 2020.\n\n[6] Gao ,et al. \"MORA: Improving Ensemble Robustness Evaluation with Model Reweighing Attack.\" NeurIPS, 2022.\n\n[7] Croce, et al. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" ICML, 2020.\n\n[8] Croce, et al. \"Minimally distorted adversarial examples with a fast adaptive boundary attack.\" ICML, 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241858081,
                "cdate": 1700241858081,
                "tmdate": 1700241858081,
                "mdate": 1700241858081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ogq46bJcJj",
                "forum": "cdUpf6t6LZ",
                "replyto": "ShknvefTI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer dbFp"
                    },
                    "comment": {
                        "value": "We appreciate the constructive feedback from the reviewer dbFp. During the rebuttal, we have already:\n- evaluated all 6466 architectures under the APGD_CE attack on CIFAR-10 and plotted their correlation with the NTK score. Additionally, we evaluated 2000 architectures using the complete AutoAttack. Based on the suggestion from reviewer AZRS, we also assessed 6466 architectures using 75 additional corruption metrics on the CIFAR-10-C dataset and analyzed the results.\n\n\n- added a new section to analyze gradient obfuscation in the proposed benchmark.\n\n\n- incorporated more attacks in the related work, clarified the usage of ImageNet-16-120, and explained the implementation of adding noise twice.\n\n\nWe hope that our additional experiments and clarifications address the reviewer\u2019s concerns. We sincerely appreciate it if the reviewer could reconsider the evaluation.\n\n\nBest regards,\n\n\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509992480,
                "cdate": 1700509992480,
                "tmdate": 1700509992480,
                "mdate": 1700509992480,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zPRViYFFWX",
            "forum": "cdUpf6t6LZ",
            "replyto": "cdUpf6t6LZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3918/Reviewer_QErk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3918/Reviewer_QErk"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents two primary contributions. Firstly, the authors have developed a search space encompassing over 6,000 adversarially trained architectures, addressing the limitation of prior studies that lacked such a comprehensive search space. Secondly, the authors demonstrate that robust architectures can indeed be identified within this search space by employing robust NTK."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper's principal strength lies in its creation of an adversarially trained search space, which required 107k GPU hours to construct. This significant advancement is poised to benefit researchers focusing on robust architecture search in the future immensely.\n2. The analyses conducted within this adversarial search space are considered to be novel. It is particularly noteworthy to observe cross-sectional evidence in the NAS search space, confirming the expectation that a 3x3 kernel size CNN layer contributes to robustness. Additionally, the findings concerning the correlation between clean accuracy and robustness in the face of adversarial attacks are intriguing.\n3. The paper is well-written, offering clarity and ease of comprehension."
                },
                "weaknesses": {
                    "value": "1. The paper lacks a detailed explanation and justification for the necessity of employing twice perturbation, as mentioned at the end of page 7. It remains unclear why robust NTK requires a double perturbation when conventional adversarial training typically examines generalization from a single perturbation.\n2. While it is posited that robust accuracy is influenced by the adversarial term, the theoretical analysis provided appears to be a reiteration of what was presented by Zhu et al. and Cao et al. in the context of clean NTK. Consequently, the contribution in this area seems minimal. Notably, the robust term A*(x,W), which is contingent on W and x, was not accounted for in the theoretical framework and was instead treated as an independent input variable. This oversight could impact the applicability of the theoretical analysis.\n3. The paper employs an evaluation metric that is not standard, raising questions about its normalization. The last part of page 4, in the dataset paragraph, should clarify whether the metric is normalized before the attack and then inputted into the model. Additionally, it would be beneficial if the authors specified whether the same perturbation radius (32x32) used for ImageNet was applied during training."
                },
                "questions": {
                    "value": "1. Could you please clarify what 'optimal' refers to in Table 2?\n2. In the first column of Table 2, what is the definition of 'criterion' used for?\n3. It would be interesting to learn about the correlation results when measured with the existing clean NTK, such as in previous works by Zhu et al. and Cao et al. (which could correspond to beta being 0).\n4. Referring to Figure 3, it appears that there is a high correlation between clean and robust accuracy. However, I am curious about the implications of using just clean NTK values for adversarial robust search and how that might affect the interpretation of the results.\n\n\nMinor Comments\n1. In Table 2, the entries in the first column are not center-aligned. This misalignment could be corrected for improved readability and a more professional presentation of the data.\n2. The interpretations of Figure 3, specifically parts (b) and (c), pose some difficulties. Enhanced specificity in the explanations accompanying these figures would greatly facilitate understanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3918/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699338026765,
            "cdate": 1699338026765,
            "tmdate": 1699636351784,
            "mdate": 1699636351784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P539j4gw51",
                "forum": "cdUpf6t6LZ",
                "replyto": "zPRViYFFWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1 out of 3) to reviewer QErk"
                    },
                    "comment": {
                        "value": "We thank the reviewer QErk for the insightful feedback. Below, we address the concerns pointed out by the reviewer QErk and revise the text in red in the updated version.\n\n---\n> **Q1:** [Explanation for employing twice perturbation. Why robust NTK requires a double perturbation when conventional adversarial training typically examines generalization from a single perturbation.]\n\n**A1:** We explain the usage of twice-robust NTK both theoretically and empirically.\n\n- **Theoretically**, In the neural tangent kernel theory, we need to define a linearized neural network around the initialization of its weight. Specifically, in equation 15, we consider the linearized network\n$F_{\\bf W^{[1]},\\bf W^\\star}(\\bf x) :=\nf(\\bf x,\\bf W^{[1]})  + \\langle\n(1-\\beta)\\nabla f_{\\bf W}(\\bf x,\\bf W^{[1]}) +\n\\beta \\nabla f_{\\bf W}(\\mathcal{A}^*_\\rho(\\bf x,\\bf W^{[1]}),\\bf W^{[1]})  , \\bf W^\\star - \\bf W^{[1]} \\rangle$.\nThen at the end of the proof of Theorem 1, we have\n$F_{\\bf W^{[1]},\\bf W^\\star}(\\mathcal{A}^*_\\rho(\\bf x_i,\\bf W^{[1]}))=    f(\\mathcal{A}^*_\\rho(\\bf x_i,\\bf W^{[1]}),\\bf W^{[1]})  + \\langle\n   (1-\\beta)\\nabla f_{\\bf W}(\\mathcal{A}^*_\\rho(\\bf x_i,\\bf W^{[1]}),\\bf W^{[1]}) +  \\beta \\nabla f_{\\bf W}(\\mathcal{A}^*_\\rho(\\mathcal{A}^*_\\rho(\\bf x_i,\\bf W^{[1]}),\\bf W^{[1]}),\\bf W^{[1]})  , \\bf W^\\star - \\bf W^{[1]} \\rangle. $\nAs a result, the Jacobian is given by\n$\\widetilde{\\bf J}\\_\\mathrm{all}  := (1-\\beta)\\hat{\\bf J}\\_\\rho+\\beta \\hat{\\bf J}\\_{2\\rho}$, where the twice-NTK appears.\n\n\n-  **Empirically**, we have demonstrated the **superiority of twice-robust NTK when compared to the (single perturbation) robust NTK in Figure 5**, which shows that the correlation between the accuracy ranking and twice-robust NTK is higher than the robust NTK. \n\nWe agree with the reviewer that the intrinsic motivation of twice perturbation is not as straightforward as classical adversarial training works in the single perturbation style. However, this twice perturbation approach shares a similar style with adding noise before the adversarial attack [5]. More precisely, [5] demonstrate that augmenting clean data with noise before executing adversarial attack has the benefit of avoiding catastrophic overfitting, compared to a classical adversarial attack on the clean data. The proposed Noise-FGSM achieves promising performance across several datasets (e..g, CIFAR-10, and SVHN)  and mitigates robust overfitting. This demonstrates that introducing a stronger attack is beneficial to obtain better performance in practice. We believe that the development of algorithms based on the twice perturbation style will be a viable and beneficial avenue.\n\n\n---\n\n> **Q2:** [Not standard evaluation raises questions about its normalization. Whether the metric is normalized before the attack and then inputted into the model. Whether the same perturbation radius (32x32) used for ImageNet was applied during training.]\n\n**A2:** In this work, the data is normalized with zero-mean and unit-variance before the attack. We have added such clarification in the dataset paragraph in the revised version. We emphasize that **we are using standard tools and evaluation metrics**, as used also in several ML papers [4,11] and open-source robustness library [3] developed by MadryLab. Regarding the implementation detail, we employ the Torchattacks library [7]. Lastly, we will release the pre-trained weight of 6444* 3* 3 architectures to encourage the practitioners to evaluate with more metrics.\n\nWe emphasize the same perturbation radius $\\rho = 8/255$ is used for ImageNet in the training procedure of the revised version.\n\n---\n\n> **Q3:** [What 'optimal' refers to in Table 2.]\n\n**A3:** Optimal refers to the architecture with the highest average robust accuracy among the benchmarks. We have updated the caption of Table 2 in the revised version.\n\n---\n\n> **Q4:** [The definition of 'criterion' in the first column of Table 2.]\n\n**A4:**  The first column of Table 2 is referred to as the \"attack scheme\" and we updated it in the revised version to avoid misunderstanding. The \"criterion\" used in our previous version indicates how the accuracy is measured during the searching phase of these baseline methods. We have already updated it in the caption of Table 2 for a better understanding in our new version. We are thankful to the reviewer for the attentive study of our work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087036151,
                "cdate": 1700087036151,
                "tmdate": 1700088407688,
                "mdate": 1700088407688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E3IjTm3D1n",
                "forum": "cdUpf6t6LZ",
                "replyto": "zPRViYFFWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2 out of 3) to reviewer QErk"
                    },
                    "comment": {
                        "value": "> **Q5:** [The theoretical analysis provided appears to be a reiteration of what was presented by [1,2] in the context of clean NTK. Consequently, the contribution in this area seems minimal. The robust term was not accounted for in the theoretical framework. ]\n\n**A5:**  Let us explain why the previous work cannot be directly applied to our setting. How to handle the **multi-objective** trade-off between clean/robust accuracy under the more general robust architecture search framework is still unclear in theory. Here, we clarify the technical difficulties behind this. \n\nPrevious work [2] builds the generalization bound of fully-connected neural networks via NTK and [1] extends this result under the activation function search and skip connection search. Both of them focus on the standard training. Instead, our work studies the robust generalization bound under *multi-objective adversarial training* of the searched architecture. Here we list the following reasons why previous work cannot be employed to our setting under multi-objective training (including adversarial training):\n-   **Trade-off and analysis of multi-objective training** are unclear: We focus on a multi-objective training scheme by introducing a regularization parameter to balance the standard training and adversarial training. In this case, the coupling relationship during training as well as the clean/robust accuracy trade-off makes the analysis difficult. For example, how to tackle the robust term $A^\\star(x,W)$ for analysis is questionable from previous work [1,2].\n-   **NTK formulation under multiple-objective training** is unclear: To our knowledge, only [8] consider an NTK-based analysis in adversarial training. However, [8] only involves the optimization convergence under adversarial training without generalization analysis. Previous works [1,2] are based on the sole NTK for generalization guarantees. Therefore, we need a new way to build different NTKs to connect clean/robust accuracy.\n\nOur analysis addressed the following technical difficulties: 1) how to build the proof framework under multi-objective training by the well-designed joint of NTKs; 2) how to tackle the coupling relationship among several NTKs and derive the lower bound of the minimum eigenvalue of NTKs. 3) how to properly tackle the robust term $A^\\star(x,W)$  between the input and weight perturbation; (See Lemma 4 for detail). \nAccordingly, our result demonstrates that, under adversarial training, the generalization performance (clean accuracy and robust accuracy) is affected by different NTKs. Concretely, the clean accuracy is determined by one clean NTK and robust NTK; while robust accuracy is determined by robust NTK and its \u201ctwice\u201d perturbation version. Our results demonstrate the effect of different search schemes, perturbation radius, and the balance parameter, which doesn\u2019t exist in previous literature. \n\n---\n\n> **Q6:** [It would be interesting to learn about the correlation results when measured with the existing clean NTK, such as in previous works by [1,2] (which could correspond to beta being 0).]\n\n**A6:** We are thankful to the reviewer for the suggestion. In the first column of Figure 5, we have provided the result for KNAS [10], which relies on the minimum eigenvalue of the clean NTK, and we can see the correlation with respect to clean NTK is lower than robust NTK.  [3, Zhu et al] propose EigenNAS, which also uses the clean NTK but with different estimations for the minimum eigenvalue of NTK. Below we provide the result for EigenNAS.  Following the original version of the paper, we plot the Spearman correlation between different NTK scores and various accuracy metrics. Specifically, we use adversarial data with PGD attacks to construct the robust NTK. We can see that robust NTK still has a higher correlation than clean NTK.\n\n| |Clean | FGSM, 3/255 | PGD, 3/255| FGSM, 8/255| PGD, 8/255|\n|--------|-------|-------|--------|------|--------|\n| EigenNAS, clean| 0.527|  0.546|  0.543|  0.601|  0.598|\n| EigenNAS, PGD, 3/255|0.534|  0.553|  0.550|  0.608|  0.605 |\n| EigenNAS, PGD, 8/255| 0.540|  0.559|  0.556|  0.614|  0.610|\n\n\n---\n\n> **Q7:** [There is a high correlation between clean and robust accuracy. The implications of using just clean NTK values for adversarial robust search and how that might affect the interpretation of the results.]\n\n**A7:** We agree with the reviewer that there exists a partial correlation between clean accuracy and robust accuracy. However, we can still see from the results of Figure 5 (or the response for Q6) that the correlation between clean NTK and robust accuracy is lower than that of the robust NTK. This implies that using clean NTK for robust NAS is worse than using robust NTK under adversarial training. \n\n---\n\n> **Q8:** [In Table 2, the entries in the first column are not center-aligned.]\n\n**A8:**  We are thankful to the reviewer for the attentive reading. We fixed the vertical alignment issue in the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087344631,
                "cdate": 1700087344631,
                "tmdate": 1700087344631,
                "mdate": 1700087344631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v6KVxe78Of",
                "forum": "cdUpf6t6LZ",
                "replyto": "zPRViYFFWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (3 out of 3) to reviewer QErk"
                    },
                    "comment": {
                        "value": "> **Q9:** [The interpretations of Figure 3, specifically parts (b) and (c), pose some difficulties. Enhanced specificity in the explanations accompanying these figures would greatly facilitate understanding.]\n\n**A9:** We have updated the caption in Figure 3 (b) and (c) as follows: (b) Overall, the architecture ranking sorted by robust metric and clean metric correlate well for lower ranking (when the value of the x-axis is larger, i.e., ranking of those bad architectures) but there still exists a difference for higher ranking. This motivates the NAS for robust architecture in terms of robust accuracy instead of clean accuracy. (c): The result reveals a high correlation across various datasets, thereby inspiring the exploration of searching on a smaller dataset to find a robust architecture for larger datasets.\n\n---\n\nIf the reviewer QErk has any remaining concerns, we are happy to clarify further. \n\n---\n\n### Refs\n\n[1] \"Generalization properties of NAS under activation and skip connection search.\" NeurIPS, 2022. \n\n[2] \"Generalization bounds of stochastic gradient descent for wide and deep neural networks.\" NeurIPS, 2019.\n\n[3] https://github.com/MadryLab/robustness.\n\n[4] \"Overfitting in adversarially robust deep learning.\" ICML, 2020.\n\n[5] \"Make some noise: Reliable and efficient single-step adversarial training.\" NeurIPS, 2022.\n\n[6] \"Exploring the loss landscape in neural architecture search.\" Uncertainty in Artificial Intelligence, 2021.\n\n[7] \"Torchattacks: A pytorch repository for adversarial attacks.\" ArXiv, 2020.\n\n[8] \"Convergence of adversarial training in overparametrized neural networks.\" NeurIPS, 2019.\n\n[9] \"Explaining and harnessing adversarial examples.\" ICLR, 2015.\n\n[10] \"KNAS: green neural architecture search.\" ICML, 2021.\n\n[11] \"Theoretically principled trade-off between robustness and accuracy.\" ICML, 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087499028,
                "cdate": 1700087499028,
                "tmdate": 1700087499028,
                "mdate": 1700087499028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dp8Vj7g09u",
                "forum": "cdUpf6t6LZ",
                "replyto": "zPRViYFFWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3918/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer QErk"
                    },
                    "comment": {
                        "value": "We are grateful for the valuable feedback from the reviewer QErk. During the rebuttal, we have:\n\n- Explained the twice robust NTK theoretically and empirically.\n\n- Clarified the theoretical contribution of our work and its contrast against the prior work.\n\n- Reported the correlation results when measured with clean NTK.\n\n- Polished the writing of Table 2, clarified the evaluation metrics, and explained the implications of using clean NTK values for robust NAS.\n\nWe would appreciate it if the reviewer could provide feedback on our previous responses.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508726269,
                "cdate": 1700508726269,
                "tmdate": 1700509036905,
                "mdate": 1700509036905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]