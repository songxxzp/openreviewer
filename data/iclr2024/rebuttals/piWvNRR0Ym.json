[
    {
        "title": "Towards Minimal Targeted Updates of Language Models with Targeted Negative Training"
    },
    {
        "review": {
            "id": "l9NK1UZ2FU",
            "forum": "piWvNRR0Ym",
            "replyto": "piWvNRR0Ym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6282/Reviewer_eQid"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6282/Reviewer_eQid"
            ],
            "content": {
                "summary": {
                    "value": "This work tackles the problem of updating a language model to reduce undesirable behavior (e.g. generating offensive content or hallucinations), while minimally changing generations elsewhere. The authors propose a method, TNT (Targeted Negative Training) which aims to keep the probability distribution of text close to the original except for instances which are intended to be removed. The paper presents some experiments with a T5 model on reducing hallucinations in summarization and in avoiding toxic responses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The topic studied by this work is timely, and progress in this direction would be of interest to many in the community.\n2. The proposed method is novel and a significant contribution to the literature"
                },
                "weaknesses": {
                    "value": "1. The experiments could be greatly strengthened. The authors study a single model T5, at a fairly small scale (220M) parameters. No purely autoregressive model is studied. Only two tasks are examined. This lack of breadth makes it hard for readers to access the generality of the claims in the paper. I believe this paper would benefit greatly from more experiments and ablations.\n2. There are very few comparisons to previous work, which in many cases tackle the same tasks presented in the experiments. As an example, avoiding toxic generations has been studied by [1,2]\n\n[1] Lu, Ximing, et al. \"Quark: Controllable text generation with reinforced unlearning.\" Advances in neural information processing systems 35 (2022): 27591-27609.\n[2] Ilharco, Gabriel, et al. \"Editing models with task arithmetic.\" arXiv preprint arXiv:2212.04089 (2022)."
                },
                "questions": {
                    "value": "I have no questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6282/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633251132,
            "cdate": 1698633251132,
            "tmdate": 1699636688733,
            "mdate": 1699636688733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VDSDRKmTXB",
                "forum": "piWvNRR0Ym",
                "replyto": "l9NK1UZ2FU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eQid"
                    },
                    "comment": {
                        "value": "Thank you for your review. We have responded to your concerns below:\n\n1. [*The experiments could be greatly strengthened. The authors study a single model T5 (220M). No purely autoregressive model is studied. Only two tasks are examined. This lack of breadth makes it hard for readers to access the generality of the claims in the paper. I believe this paper would benefit greatly from more experiments and ablations.*] \n\nThank you for the feedback. First, we would like to emphasize that in order to generate Figure 2, we ran 7 * 9 = 64 finetuning jobs on two very different generation tasks (hallucination in summarization and toxicity in response generation), for 126 jobs in total. Figure 2 shows that TNT consistently outperforms or at least matches baseline methods across all rates of reduction in both tasks, demonstrating that its benefit is broad and robust to different definitions of unwanted. We\u2019ve additionally added a set of ablations comparing methods on external data rather than model generations; TNT still outperforms baseline the results, illustrating that the proposed losses are better even under a different data context, and the main results are better than the ablations, confirming our hypothesis that preferencing more common token conditionals yields a more minimal update. Due to the extensive nature of each experiment on just one model and task, we were not able to complete such experiments for larger models (magnitudes more resource-intensive) or additional tasks (requires appropriate dataset), but we hope the mathematical analysis and updated experiments convinces the reviewer that there is merit in the proposed approach.\n\n\n2. [*There are very few comparisons to previous work, which in many cases tackle the same tasks presented in the experiments, e.g. avoiding toxic generations has been studied by [1,2]*] \n\nWe chose as baselines other finetuning-based methods that take into account token-level information for learning. The works the reviewer mentioned do not fall into this setting, but we have instead included them as background or related work:\n- Quark finetunes an existing language model on its generations by prepending inputs with sequence-level rewards and regularizing with a KL penalty on the token-level conditional distributions. Then, during inference time one conditions on high reward to sample from the model. TNT differs in that it utilizes token-level annotations and directly specifies the output token probabilities rather than conditioning the inputs on the rewards.\n- Task Arithmetic subtracts the weights of pretrained and finetuned models to yield task vectors which are composed together to add or negate behaviors. As these task vectors are a function of vanilla finetuning, e.g. via filtering the dataset of toxic language, this approach, like finetuning, will not yield minimal targeted updates.\n\nDo you have any additional questions or concerns that you would like to share? If not, would you kindly consider raising your score?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592788622,
                "cdate": 1700592788622,
                "tmdate": 1700592788622,
                "mdate": 1700592788622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cl5CI3f5rP",
                "forum": "piWvNRR0Ym",
                "replyto": "VDSDRKmTXB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6282/Reviewer_eQid"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6282/Reviewer_eQid"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my comments. While the number of experiments itself is not very small, there is a lot of redundancy in the experimental settings, which makes it hard to know whether the findings from this paper are broad and general. After this response, my concerns still stand, and I am thus sticking with my original scores. I believe this paper has great potential and would highly encourage authors to improve the breadth of their experiments."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666539164,
                "cdate": 1700666539164,
                "tmdate": 1700666539164,
                "mdate": 1700666539164,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bcv4C9q6Z1",
            "forum": "piWvNRR0Ym",
            "replyto": "piWvNRR0Ym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6282/Reviewer_fDnb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6282/Reviewer_fDnb"
            ],
            "content": {
                "summary": {
                    "value": "Summary:\n\nThe paper introduces Targeted Negative Training (TNT), a method for minimally updating language models to prevent unwanted outputs. Unlike previous techniques, TNT fine-tunes models using negative examples generated by the model itself, with the goal of closely aligning the updated model's distribution to the original while avoiding specific undesired behaviors. The method operates by minimizing reverse KL-divergence, ensuring the updated model does not deviate significantly from its initial training. Experiments demonstrate that TNT effectively maintains original model performance better than other negative training approaches while reducing unwanted behaviors. However, it requires access to the original model and detailed token-level annotations, presenting potential practical challenges. TNT's iterative nature also suggests it could be used to enhance model safety over time through continuous refinement."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Advantages:\n - Iterative Model Updates: TNT allows for iterative updates to language models without needing all negative tokens to be specified upfront. This flexibility is advantageous for practical applications where updates may be continuous and ongoing.\n\n - Maintained Model Performance: The experimental setup indicates that TNT can effectively maintain the original model's performance better than baseline methods while also reducing unwanted behaviors.\n\n - Reproducibility and Accessibility: The experiments are reproducible, with the promise of making the code public and using publicly available datasets, which enhances the credibility and utility of the research."
                },
                "weaknesses": {
                    "value": "Disadvantages:\n - Limited Novelty: The core ideas and methods of TNT may not be as novel as claimed, given the prior existence of the NADO algorithm [NeurIPS 2022, https://arxiv.org/pdf/2205.14219.pdf] framework which appears to address similar goals using related techniques.Further more, NADO has proven its objective to be the **theoretically closed form solution** of the shared targets of TNT/NADO, which weakens the value of the approximated solution (step-level branch-cutting) given by TNT. TNT's flexibility is also less than that of NADO as it requires auxiliary negative annotation, which is a closer setup to that of the FUDGE algorithm [NAACL 2021, https://arxiv.org/abs/2104.05218].\n\n - Oversight in Literature Review: Even if TNT can differ itself from NADO and FUDGE (since the two previous methods study different tasks, yet essentially with similar mathematical setup), the absence of a citation and/or discussion of these existing works suggests a possible gap in the literature review process, which might question the thoroughness of the background research conducted for the paper."
                },
                "questions": {
                    "value": "What is the essential different between TNT and previous constrained decoding algorithms (FUDGE, NADO, Neural Logic, etc.) that aim to maximize/minimize a given (emplicitly through a symbolic process or implicitly through negative samples) sequence boolean function that defines the negativity of samples?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6282/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811646889,
            "cdate": 1698811646889,
            "tmdate": 1699636688587,
            "mdate": 1699636688587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "stOhoEnVod",
                "forum": "piWvNRR0Ym",
                "replyto": "bcv4C9q6Z1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fDnb (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your review. Thank you especially for highlighting a key strength of our approach in its ability to enable iterative model updates: \u201cTNT allows for iterative updates to language models without needing all negative tokens to be specified upfront. This flexibility is advantageous for practical applications where updates may be continuous and ongoing.\u201d\n\nWe first wish to clarify and address two points the reviewer made in the paper summary:\n1. [*The method operates by minimizing reverse KL-divergence, ensuring the updated model does not deviate significantly from its initial training.*] \n\nThe target distributions of interest are the optimal distribution in terms of reverse KL divergence with the original under the constraint that negative tokens are avoided. Then, the suite of TNT algorithms optimize for these target distributions via different combinations of forward and reverse KL divergence (depending on whether the token conditional requires a negative update or not). This is different from \u201cthe method operates by minimizing reverse KL-divergence.\u201d\n\n2. [*It requires access to the original model and detailed token-level annotations, presenting potential practical challenges.*] \n\nA finetuning method like TNT is specifically meant for the maintainers of a given language model (i.e., to iteratively update their model), so we do not foresee a problem in the method requiring access to the original model. We agree that the fact that the training algorithms require the output probabilities of the original model could present some issues around the amount of memory needed during training, but there are strategies to reduce memory consumption by trading off with compute time, e.g. relevant output probabilities distributions could be precomputed. \n\nAs for token-level annotations, we address the practicality of this information in Section 2.1. To summarize, while we agree that in some cases token-level annotations can be more expensive to collect than sequence level ones (e.g. it is easier to identify that a piece of code fails than to find the source of a bug), in many cases the two involve comparable effort, e.g. labeling an overall sequence with \u201chas hallucination\u201d or \u201chas offensive language\u201d generally requires identifying the hallucination or offensive language itself. In such a setting, we expect a method that takes this more fine-grained information into account to be more sample-efficient (a recent example being fine-grained RLHF vs sequence-level RLHF)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592229871,
                "cdate": 1700592229871,
                "tmdate": 1700592229871,
                "mdate": 1700592229871,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qIfWLi37gt",
            "forum": "piWvNRR0Ym",
            "replyto": "piWvNRR0Ym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6282/Reviewer_VP85"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6282/Reviewer_VP85"
            ],
            "content": {
                "summary": {
                    "value": "This work provides a fine-tuning-based algorithm to update a language model to suppress harmful content generation while remaining as close to the original model as possible. Given a set of harmful responses to avoid (defined by a function that takes in a string and produces binary output), the authors define the ideal target model as one that matches the original model conditioned on never producing an undesirable string. The authors optimize a loss function based on this definition and find that the resulting model better satisfies the desired notion of safety."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper's key problem is relevant to practice and is well-specified \n\n2. The paper's solution is similarly elegant and was a natural consequence of the problem specification, providing a tractable solution to the proposed problem. \n\n3. The results demonstrate a solid improvement over reasonable baselines to provide targeted updates to the model."
                },
                "weaknesses": {
                    "value": "1. The authors are missing connections to existing methods. For example, [Korbak et al, 2022](https://arxiv.org/abs/2205.11275) (among others) show that PPO would converge to the same closed-form presented in Eq. 2 when using the positivity/negativity classification as a reward function; this method would also avoid the drawbacks mentioned in the related work for inference time procedures.\n\n2. The paper demonstrates their technique on some relatively easier benchmarks, and it would be much more interesting to try more complicated schemes. There are three ways in which I find them weak\n    - In the specific case where p_neg is defined as the presence of a bad token in the string, there is no need to do any training, and by simply ignoring bad tokens at decoding, one recovers the true optimal solution for both greedy and temperature decoding with zero overhead. Both the toxicity and hallucinations benchmarks provided in this text are dangerously close to \"reject any sentence with a bad (word/entity)\", which makes the application rather uninteresting. It would be cooler to see benchmarks where the reward model may still be automated but captures some global property of the sentence that requires a learning-based technique such as yours to solve.\n    - For the toxicity benchmark, the authors mention that 1.6% of the time, the completion is toxic. I believe a very natural baseline to this problem is performing temperature 1 decoding and regenerating anytime the output is toxic. I get the sense this will very rarely require few regenerations for this specific benchmark, making the learning rather excessive. Having a benchmark that requires a larger change will be a true test of this problem.\n   - For baselines, as mentioned in Weakness 1, I believe PPO is a more natural baseline for learning from a reward model\n\nOverall, I believe the work can better demonstrate the technique."
                },
                "questions": {
                    "value": "1. At the top of page 7, the paper mentions that all experiments were done with greedy decoding. Does that mean fine-tuning was also based on greedy generations? If only 1.6% of model generations were toxic, would the model get any gradient signal for 98.4% of the generations?\n\n2. Why is TNRLL not called TNFR?\n\n3. Just to be clear, does \"token level annotations\" refer to a function that can take a sentence and assess whether it is negative or positive? This was not super clear to me, even though there were a few sentences dedicated to it on page 4. If it is a function, it might be better specified as such."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6282/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815667116,
            "cdate": 1698815667116,
            "tmdate": 1699636688439,
            "mdate": 1699636688439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EQBAuzFheU",
                "forum": "piWvNRR0Ym",
                "replyto": "qIfWLi37gt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VP85 (1/2"
                    },
                    "comment": {
                        "value": "Thank you for your review! Addressing each comment below:\n1. [*The authors are missing connections to existing methods. For example, PPO would converge to the same closed-form presented in Eq. 2 when using the positivity/negativity classification as a reward function; this method would also avoid the drawbacks mentioned in the related work for inference time procedures.*] \n\nThank you for bringing this up. We have added to the related work that, given a positivity/negativity classifier as a reward function, the objective PPO seeks to maximize (equivalent to minimizing reverse KL) has an optimal solution that is a soft version of the result TNT targets: namely, $p^*_{PPO}(x|c) = p_{o}(x|c)\\exp(r(x)/\\beta)$ while $p^*_{TNT}(x|c) = p_o(x|c)\\mathbf{1}[r(x)]$, where the former is approximately equal to the latter when $\\beta$ is small.\n\n2. [*It would be cooler to see benchmarks where the reward model may still be automated but captures some global property of the sentence that requires a learning-based technique such as yours to solve.*] \n\nWe agree that the wordlist setup does not demonstrate the full practical utility of TNT and have replaced this experiment with one where we train a  token-level classifier to identify toxic spans. In this updated experiment, the benefit of TNT over baselines is even more pronounced, with TNT methods overall (and each of TNRF, TNRR, and TNRLL individually) being strictly better than baselines across all ranges of similarity and reduction (see Figures 2(c) and (d)).\n\n3. [(paraphrased for brevity) *For the toxicity benchmark, learning is rather excessive for a base rate of 1.6% (e.g., natural baseline is to regenerate anytime output is toxic). Having a benchmark that requires a larger change will be a true test of this problem.*] \n\nWe agree, and with our new broader definition of toxicity defined by a token-level classifier trained on human annotations, the level of toxicity of the original model is 8%. Moreover, this 8% rate is not constant across inputs; for instance, we see that for examples where the input contains toxicity (based on the same classifier definition), the model generations have toxic language 23.3% of the time. Thus in our updated experiment we have a setting where regeneration would be quite expensive for a certain subset of inputs, making finetuning a more practical alternative.\n\n4. [*I believe PPO is a more natural baseline for learning from a reward model.*] \n\nThanks for the suggestion. We initially did not consider PPO as a baseline since it is an algorithm generally used to optimize sequence-level rewards. [1] recently introduced Fine-Grained RLHF which could be used to consider token-level rewards, but we did not have the chance to implement this baseline given time constraints (It is also worth noting that this baseline is much more complicated than the proposed approach, requiring sampling from the model during training as well as keeping track of four separate models, including a separate value model that is trained simultaneously to the policy model). Our experiments do hint at the opportunity for considering alternatives beyond such a baseline, though. Namely, whereas the objective considered by the PPO algorithm is equivalent to minimizing the reverse KL divergence between the current model $p_\\theta(x|c)$ and the target distribution $p^*(x|c)$, regardless of whether we consider sequence- or token-level rewards, TNT encompasses a suite of objectives which consider minimizing not just the reverse KL between token-level conditional distributions but also other combinations of divergences, and our experiments show that other divergence combinations may be preferable in certain scenarios, even when the algorithm is fixed regardless of objective; in particular, at lower levels of reduction, TNT losses that use forward KL (i.e., TNRF and TNFF) tend to outperform TNRR at maintaining similarity with the original model generations.\n\n5. [*At the top of page 7, the paper mentions that all experiments were done with greedy decoding. Does that mean fine-tuning was also based on greedy generations? If only 1.6% of model generations were toxic, would the model get any gradient signal for 98.4% of the generations?*] \n\nThanks for the question. The fine-tuning was also based on the greedy generations; however, the token-level KL divergence terms are computed using the full token-level output distributions, such that any deviation of any probability mass between the current model and the desired target output distributions will yield gradient signal. We have also confirmed empirically that the model gets gradient signal throughout training (via logging the gradient norm)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591650437,
                "cdate": 1700591650437,
                "tmdate": 1700591650437,
                "mdate": 1700591650437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MZElBBkxTo",
                "forum": "piWvNRR0Ym",
                "replyto": "qVDFTTc0ud",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6282/Reviewer_VP85"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6282/Reviewer_VP85"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the further clarification and experiments during the rebuttal process. My score remains at a 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722093551,
                "cdate": 1700722093551,
                "tmdate": 1700722093551,
                "mdate": 1700722093551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kfphUJHwN9",
            "forum": "piWvNRR0Ym",
            "replyto": "piWvNRR0Ym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6282/Reviewer_DCRf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6282/Reviewer_DCRf"
            ],
            "content": {
                "summary": {
                    "value": "1. The paper tackles the problem of modifying the generative distribution of LLMs to reduce the likelihood of generating undesirable tokens whilst simultaneously not deviating too much from the original distribution.\n2. Concretely, the authors present their approach (dubbed Targeted Negative Training, or TNT), wherein for each prefix and continuation from an already trained model, if the target token is annotated to be undesirable then the approach minimizes a divergence between the original distribution from the already trained model and a modified distribution based on setting the logit of the undesirable token to 0 and renormalizing for the target model. Otherwise the objective minimizes a divergence between the original and the learned model distribution.\n3. The authors explore different instantiations of divergences: (forward KL, forward KL), (reverse KL, reverse KL) and (reverse KL, forward KL) for the (negative token signal, positive token signal), which they dub (D_{n}, D_{p}) respectively. In addition to that, they also explore with (forward KL, maximum likelihood) and (reverse KL, maximum likelihood) for (D_{n}, D_{p}).\n4. The results presented show that the proposed method allows for better tradeoffs between controlling for hallucinations and keeping the model close to the original distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method for leveraging sentence level annotations in order to modify model behaviour is quite interesting. \n2. I quite like the extensiveness of explorations in terms of exploring the different divergences for both the positive and negative signals. From the results, both TNFF and TNRF seem to achieve a pretty good tradeoff between being faithful to the original distribution and controlling for hallucinations."
                },
                "weaknesses": {
                    "value": "1. For the different proposed models, without an equivalent table similar to Table 1, it is hard to understand the effectiveness of the approach. Concretely, similar to the TNFLL and TNRLL rows, it would also be good to have an equivalent row for TNFF, TNRR and TNRF.\n2. From Table 1, for both the TNFLL and TNRLL approaches, the performance is considerably worse compared to the baseline method. For TNFLL, it the hallucination rate is substantially higher, while for TNRLL, the BLEU score is much lower. Given this observation, I am hesitant to believe that the proposed approach is actually substantially than the baseline approach.\n3. In my opinion, intuitively, because this approach minimizes the KL at a prefix level, especially considering the fact that the annotations obtained are from a noisy source, it is possible that this approach would steer the model towards not predicting certain words in certain contexts. Concretely, (based from the example in Figure 5), for the sentence \"In some regions of the country, the sex ratio is still quite concerning\", because the annotations are noisy, the word \"sex\" would be (incorrectly) marked as offensive. Consequently, because of the proposed objective, the model might not be able to produce the token \"sex\" for a similar prefix as \"In some regions of the country, the\", even if it did make sense in the context. I think this is a reasonably big limitation of the approach, and it would have been nice to have some discussion on this in the paper."
                },
                "questions": {
                    "value": "1. Would it be possible to rows for TNFF, TNRR and TNRF in Table 1 ?\n2. Would it be possible to provide some clarification on how Figure 2 was constructed ? Specifically, is the level of hallucination mapped to a different value of \\alpha used (so higher \\alpha -> lower hallucination rates and original distribution fidelity) ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6282/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699475448942,
            "cdate": 1699475448942,
            "tmdate": 1699636688310,
            "mdate": 1699636688310,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DQipYfqBOT",
                "forum": "piWvNRR0Ym",
                "replyto": "kfphUJHwN9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6282/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6282/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DCRf (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review. We address each of your points below:\n1. [*For the different proposed models, without an equivalent table similar to Table 1, it is hard to understand the effectiveness of the approach. Would it be possible to add rows for TNFF, TNRR and TNRF in Table 1?*] \n\nThank you for the suggestion; we have updated Table 1 to include all TNT methods. We\u2019d also like to clarify that Figure 2 (rather than Table 1) is meant to encapsulate the main result, as it considers all methods across all alpha values in one plot; Table 1 on the other hand only looks at a single fixed alpha value and is instead meant to focus on how changing the loss on negative tokens alone can reduce disfluencies. This is the reason why the original Table 1 focused on TNT methods that share the same loss as baselines on non-negative tokens (i.e., TNFLL and TNRLL). We\u2019ve updated the text to draw attention to Figure 2 as the main result.\n\n2. [*From Table 1 TNFLL hallucination rate is substantially higher (than baselines), while TNRLL BLEU score is much lower (than baselines). Given this observation, I am hesitant to believe that the proposed approach is actually substantially better than the baseline approach.*] \n\nFirst, we wish to reiterate that Figure 2 gives the main result, that TNT methods yield a better trade-off of reducing unwanted behavior vs. maintaining similarity to the original. While we intended Table 1 to focus on disfluency results, based on your feedback we realize how the current presentation of Table 1 could be confusing, as we also report similarity and reduction metrics but only for a single fixed value of $\\alpha$, which puts different methods at different points along their similarity vs. reduction curves. \nThis fixed value of $\\alpha$ in resulted in rows in Table 1 that are hard to compare along similarity and reduction metrics, e.g. if one row has more reduction but lower similarity than another, which is better? Figure 2 is meant to address this challenge. To make Figure 2 and Table 1 congruent, we have additionally updated Table 1 to choose the $\\alpha$ that yields the best BLEU score among $\\alpha$ values that achieve a given rate of reduction (75%). See the updated table for toxicity below. \nNow, the methods can be more easily compared across all metrics (similarity, reduction, and disfluency); for instance, one can see that, given a 75% rate of reduction is achieved, **TNRF is strictly better than baselines on all metrics**, all TNT methods except TNFLL yield better similarity results than baselines, and all TNT methods perform better than baselines on the number of total introduced disfluencies (sum of Repeats and Random ??).\n\n\n|                        |    BLEU   |  ROUGE-L   |  Seq Acc   |  Toxicity  | Repeats |  Random ??  |\n|------------------------|-----------|------------|------------|------------|---------|-------------|\n| Original               | 100.0000  | 100.0000   | 100.0000   | 8.1830     | 16      | 4           |\n| NL + LL ($\\alpha=.01$) | 13.6497   | 32.6104    | 2.0661     | 1.8150     | 287     | 136         |\n| UL + LL ($\\alpha=1.0$) | 37.1265   | 59.9806    | 20.4405    | 1.6784     | 23      | 1122        |\n| TNFLL ($\\alpha=1.0$)   | 33.7884   | 57.1009    | 18.1630    | 1.7577     | 36      | **1**       |\n| TNRLL ($\\alpha=0.1$)   | **39.1922**| **61.4776** | **22.9207** | 1.9471     | 23      | **1**       |\n| TNRR ($\\alpha=0.1$) | **55.9532**| **71.5365** | **35.1366** | **1.4493** | 34      | **3**       |\n| TNRF ($\\alpha=1.0$) | **60.2071**| **74.8574** | **39.6167** | **1.0396** | **21**  | **3**       |\n| TNFF ($\\alpha=10.$)| **61.0565**| **74.2388** | **40.0749** | 1.9031     | 33      | **3**       |\n\n3. [*It is possible that this approach would steer the model towards not predicting certain words in certain contexts. I think this is a reasonably big limitation.*] \n\nWe take the reviewer\u2019s comment to mean that if the model is not context-aware when pushing down probability mass over certain tokens, then it is possible for the model to begin avoiding a given token even in contexts where it would be acceptable. Fortunately, this concern is not an issue for TNT because it is context-aware: all negative annotations are a function of the prefix, which means that the negative signal the model sees is of the form \u201cX is bad when preceded by Y\u201d rather than just \u201cX is bad.\u201d As the wordlist example does not showcase this context-aware property of the method since the annotations themselves do not take into account context, we replaced the wordlist experiment with one based on a token-level classifier that labels spans as toxic based on its context."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6282/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591406824,
                "cdate": 1700591406824,
                "tmdate": 1700591406824,
                "mdate": 1700591406824,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]