[
    {
        "title": "3D Object Representation Learning for Robust Classification and Pose estimation"
    },
    {
        "review": {
            "id": "UP26RrceAt",
            "forum": "8XgCH9y1Bs",
            "replyto": "8XgCH9y1Bs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1954/Reviewer_LehM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1954/Reviewer_LehM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for joint object classification and 3D pose estimation from a single image. \nThe core idea is that by doing joint pose estimation and classification, the classification results are more robust in scenarios with challenging occlusion and image corruption.\n\nEach object class's 3D shape is modelled using a dense 3D mesh. Each vertex has a feature descriptor designed to be pose invariant. \nThe calssification network uses a backbone to detect 2D features, and the next computes the similarity of each 2D feature to the viewpoint-invariant features for each of the 3D vertices of an object class mesh, computing scores (these essentially produces dense 2D-3D correspondences). These scores are used to compute overall per-class scores to perform classification. A set of background features are used to represent the background class. In a second step (not needed for classification), the object pose is computed for the selected object using the 2D-3D dense correspondences using a render-and-compare approach starting from multiple initialisations.\n\nAt training time, featrures for 3D vertices and 2D features prameters are compute jointly. Contrastive learning is used to make the 3D features viewpoint invariant (enforce similarity for 3D features corresponding to the same vertex from different viewpoints,  and doing the opposite for 3D features for different vertices or different objects).\n\nThe method is evaluated on both object classification on standard benchmarks (Pascal 3D, a heavily occluded version of it, OOD-CV2 for out of distribution samples), and compared against some baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to understand\n- The results of the method are convincing, and demonstrate one of the paper claim, i.e. that 3D reasoning makes the approach more robust to occlusions and challenging image conditions.\n- The paper is evaluated on standard benchmarks, with reasonable ablation analysis\n- The method has some interesting interpretability properties, for example by looking at vertex activation output it is possible to understand which parts of the object are occluded in the image"
                },
                "weaknesses": {
                    "value": "My main concern with this paper is lack of novelty, in particular with respect to: \"Neural Textured Deformable Meshes for Robust Analysis-by-Syntheses\" by Wang et al. (which is cited in the submission). This paper seems to follow the same procedure, using mostly the same techniques (feature extraction, 3D mesh representation, contrastive divergence to make 3D features more invariant, etc.). There seem to be some differences (e.g. it seems the submission the 2D to 3D matches is done differently, and the submission, unlike Wang et al., allows to do classification without the more computationally expensive step of 3D pose estimation, but I am not certain),  but these differences are not explained in the submission, nor seem significant enough to warrant sufficient novelty. Moreover, Wang et al. achieve very comparable results, and in some cases superior performance (e.g. for L3) on Occluded PASCAL3D+ (Compare Table 1 in Wang et al. to Table 1 in the submission). I would have expected a much more detailed comparison to the most relevant papers (in particular \"Neural Textured Deformable Meshes for Robust Analysis-by-Syntheses\" by Wang et al.), with clear articulation of what the diffrences are, and pros and cons of the proposed method with respect to these alternatives \n\nI also think some claims are not adequately supported by the results, for example \"exceptionally robust classification and pose estimation results\" as claimed in the abstract"
                },
                "questions": {
                    "value": "1) Could you please provide a detailed explanation of the differences with respect to the most similar approaches (all work by Wang et al., in particular Wang et al. 2023)? \n2) Is it possible to compare to Wang et al. 2023 on the corrupted Pascal3d+ dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1954/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698358481993,
            "cdate": 1698358481993,
            "tmdate": 1699636127035,
            "mdate": 1699636127035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HikUU798Wr",
                "forum": "8XgCH9y1Bs",
                "replyto": "UP26RrceAt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1954/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1954/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer LehM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback.  \n\n**My main concern with this paper is lack of novelty, in particular with respect to: \"Neural Textured Deformable Meshes for Robust Analysis-by-Syntheses\" by Wang et al. (which is cited in the submission). This paper seems to follow the same procedure, using mostly the same techniques (feature extraction, 3D mesh representation, contrastive divergence to make 3D features more invariant, etc.). There seem to be some differences (e.g. it seems the submission the 2D to 3D matches is done differently, and the submission, unlike Wang et al., allows to do classification without the more computationally expensive step of 3D pose estimation, but I am not certain), but these differences are not explained in the submission, nor seem significant enough to warrant sufficient novelty. Moreover, Wang et al. achieve very comparable results, and in some cases superior performance (e.g. for L3) on Occluded PASCAL3D+ (Compare Table 1 in Wang et al. to Table 1 in the submission). I would have expected a much more detailed comparison to the most relevant papers (in particular \"Neural Textured Deformable Meshes for Robust Analysis-by-Syntheses\" by Wang et al.), with clear articulation of what the differences are, and pros and cons of the proposed method with respect to these alternatives**\n \nWe are sorry for the confusion with the related paper [G]. We cited the paper for the sake of completeness, and should have better pointed out that it is contemporaneous concurrent work that was **not published and only available on ArXiv** at the time of submission.  We note that according to ICLR reviewer guidelines we should therefore not be required to compare it as a baseline in our experiments (see last answer in the FAQ in [F]). Nevertheless, when comparing our method with [G] the reviewer must have misread the results of the Tables in [G] since our method outperforms [G] significantly at classification and pose estimation in all IID and OOD scenarios. In addition, as the reviewer requested, we contacted the authors and received their code to test on the corrupted-P3D+ dataset. We report the results in Table 4, and can observe that our model outperforms all types of corruption with a large margin, and we are around 20 percent higher overall.\n\nWe further want to emphasize that, our proposed method performs classification efficiently in real-time, which takes on average 0.02 seconds per image, whereas [G] simply runs a brute force search during inference which takes on average 0.5 seconds per image on the Pascal3D+ dataset under the same setup, suffering from a 25 times slower inference in addition to the lower accuracy and robustness, and hence making it practically infeasible.\n\nThe way how we formulate the contrastive loss is also different, which can be illustrated by the loss function in two papers. We use a vMF-based contrastive loss that encourages features to be distinct through a dot product operation whereas [G] uses a Euclidean-distance based loss estimating the distance between feature vectors. \nThe main focus of [G] is different from this paper. Its goal is to obtain deformable (exact) geometry of the object in its forwarding.\nIn summary, these two papers have major differences in both low-level implementation and high-level goals, and performance [G] in terms of both accuracy and efficiency for the classification task.\n\n**I also think some claims are not adequately supported by the results, for example \"exceptionally robust classification and pose estimation results\" as claimed in the abstract**\n\nWe will tone down this paragraph in the abstract. However, we note that our proposed model significantly outperforms the best baselines at image classification in OOD scenarios, e.g. +5.8% over ConvNext on occluded-P3D+, +21% over Swin-T on OOD-CV, and +3.7% over ViT-16 on corrupted-Pascal3D+. We also note that the ranking among the baselines varies significantly across datasets, e.g. Swin-T achieves the best ranking among baselines on OOD-CV but only ranks third on corrupted-Pascal3D+. In contrast, our proposed method consistently outperforms the baselines across all OOD scenarios.\n\n**Table 4:** Classification accuracy results on corrupted-PASCAL3D+ under 12 different types of common corruptions. Our approach outperforms DMNT by a large margin for all corruptions.\n\n| Nuisance|L0|defocus blur|glass blur|motion blur|zoom blur|snow|frost|fog|brightness|contrast | elastic trans. | pixelate | jpeg | mean|\n|-|-|-|-|-|-|--|-|-|-|-|-|-|-|-|\n| DMNT|94.1|87.1|55.8|78.0| 76.2|69.4|79.6|91.3|92.9|89.9|72.5|78.6|92.3|72.7|\n|Ours|**99.5**|**90.5**|**65.7**|**86.4**| **84.2**|**91.2**|**89.5**|**98.4**|**98.4**|**97.1**| **97.2**|**97.1**|**98.4**|**91.3**|\n\n[F] Reviewer guide: https://iclr.cc/Conferences/2024/ReviewerGuide\n\n[G] Angtian Wang, Wufei Ma, Alan Yuille, and Adam Kortylewski. Neural textured deformable meshes for robust analysis-by-synthesis. arXiv preprint arXiv:2306.00118, 2023"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257265212,
                "cdate": 1700257265212,
                "tmdate": 1700257265212,
                "mdate": 1700257265212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n75KuvtEYZ",
                "forum": "8XgCH9y1Bs",
                "replyto": "HikUU798Wr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1954/Reviewer_LehM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1954/Reviewer_LehM"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "The authors's\u00a0rebuttal allowed me to understand\u00a0the contributions of the paper better. The authors point out that the reviewer policies do not require comparison to unpublished concurrent work, which is the case for \"Neural Textured Deformable Meshes for Robust Analysis-by-Synthesis\" - this addresses my main concern on lack of novelty.\u00a0\n\nI have three asks to improve the manuscript:\n1. Make it much clearer that not having to do render-and-compare for classification while still exploiting 3D reasoning is a big contribution because it speeds up inference, and that render-and-compare is optional for the pose estimation step, which is not required for classification.This is explained better in the rebuttal than in the new version of the paper. \n2. Also the comparison with Nemo is clearer in the rebuttal than in the paper, particularly the points on the class-agnostic backbone, that render-and-compare\u00a0is not needed\u00a0for classification, and the increased robustness on OOD. Please make the paper as clear as the rebuttal\n3. Similarly, the rebuttal is much more clear on the differences with respect to \"Neural Textured Deformable Meshes for Robust Analysis-by-Synthesis\". The paper text on this point is too vague."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337492306,
                "cdate": 1700337492306,
                "tmdate": 1700337492306,
                "mdate": 1700337492306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rAc5jSd4br",
            "forum": "8XgCH9y1Bs",
            "replyto": "8XgCH9y1Bs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1954/Reviewer_35dn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1954/Reviewer_35dn"
            ],
            "content": {
                "summary": {
                    "value": "The paper claims that they propose a 3D object representation at the object category-level that can be used for object classification and 3D object pose estimation.\nThey represent each object category as a cubic with attached features in each vertex, which are trained using multi-view posed images.\nThen, the features are used to classify the object category by directly matching the image feature map with each category's 3D features and estimating the pose of the object in the image using render-and-compare."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The proposed method uses trained 3D features of each category of object classification, instead of performing 3D object pose estimation only, which is pretty interesting since the 3D features can further be leveraged.\n\n2) Visualizations are attached to show the interpretability of the 3D features."
                },
                "weaknesses": {
                    "value": "1) More detailed discussions and comparisons with NeMo. NeMo is highly related to the proposed paper, but the discussion is missing in the introduction and related works. As far as I can see, the 3D object representation in the paper is already proposed by NeMo. NeMo already finds that using a cubic for an object class can achieve good performance in 3D pose estimation. They also use contrastive loss for training and render-and-compare strategy for pose estimation as in this paper. I think the difference is that the paper extends this existing 3D representation to training on multiple categories and uses it for classification. Based on this point, I think a huge part of the contributions of this paper belong to NeMo, and the real contributions are limited.\n\n2) For the evaluation of classification, I think the paper should compare with the state-of-the-art 2D object detectors for classification to show the advantages. For example, I think the YOLO can be easily trained using the same training data as the proposed method, i.e., the projected 2D bounding boxes from the 3D cubics."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1954/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1954/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1954/Reviewer_35dn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1954/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674309359,
            "cdate": 1698674309359,
            "tmdate": 1699636126966,
            "mdate": 1699636126966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d9rywVqCxq",
                "forum": "8XgCH9y1Bs",
                "replyto": "rAc5jSd4br",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1954/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1954/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 35dn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback.  \n\n**More detailed discussions and comparisons with NeMo. NeMo is highly related to the proposed paper, but the discussion is missing in the introduction and related works. As far as I can see, the 3D object representation in the paper is already proposed by NeMo. NeMo already finds that using a cubic for an object class can achieve good performance in 3D pose estimation. They also use contrastive loss for training and render-and-compare strategy for pose estimation as in this paper. I think the difference is that the paper extends this existing 3D representation to training on multiple categories and uses it for classification. Based on this point, I think a huge part of the contributions of this paper belong to NeMo, and the real contributions are limited.**\n\nOur method indeed builds on several previous works while substantially extending them.\nCompared to NeMo, we make several key contributions: (1) An architecture with a class-agnostic backbone, while NeMo uses a separate backbone for each object. This enables us to introduce a class-contrastive loss that enables object classification, while NeMo is not capable of classifying images. As illustrated in the t-SNE plots in reviewer-wnbh\u2019s answer. (2) A principled way of speeding up the inference without having to perform render-and-compare, which advances the brute-force testing and optimization of other concurrent methods [G] (see answer to reviewer-lehm for more details). (3) A comprehensive mathematical formulation that derives a vMF-based contrastive training loss that is different from Euclidean-distance based contrastive loss of NeMo. (4) We demonstrate the possibility to exploit individual vertices during inference rather than considering vertices collectively as a mesh in NeMo, opening potentials tackling segmentation or object part detection tasks. Finally, all our advances lead to (5) substantial improvements in terms of OOD robustness over all baselines at image classification, while at the same time performing on par with models that were specifically designed for robust pose estimation, such as NeMo.\n\n**For the evaluation of classification, I think the paper should compare with the state-of-the-art 2D object detectors for classification to show the advantages. For example, I think the YOLO can be easily trained using the same training data as the proposed method, i.e., the projected 2D bounding boxes from the 3D cubics.**\n\nThank you for the suggestion. We trained YOLO using the official repository [D] on the Pascal3D+ data with standard data augmentation in terms of scale, rotation and mirroring data augmentation. From the table below, we can observe that YOLO obtains similar performances for IID data, but suffers even more than the other classification baselines under out-of-distribution shifts in terms of occlusion, whereas our proposed model achieves much stronger robustness. While refining our YOLO results through additional investigation might be possible, we speculate that utilizing the official training code for YOLO might lead to potential overfitting, given its original focus on detection rather than classification. We anticipate that more advanced data augmentation (such as CutOut or CutPaste) techniques could enhance generalization in YOLO's performance. Notably, it is crucial to emphasize that our method, in contrast, does not necessitate data augmentation and demonstrates effective generalization.\n\n**Table 3:** Classification results on P3D+ and occluded-P3D+ comparing Yolo and our approach. \n\n| Nuisance  | L0   | L1                | L2                | L3                | Mean              |\n|-----------|------|-------------------|-------------------|-------------------|-------------------|\n| YOLO      | 99.3 | 62.4              | 44.9              | 25.0              | 44.1              |\n| Ours      | **99.5** | **97.2**        | **88.3**          | **59.2**          | **81.6**          |\n\n[D]: https://docs.ultralytics.com/"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257253188,
                "cdate": 1700257253188,
                "tmdate": 1700257253188,
                "mdate": 1700257253188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mfKBIDlTMJ",
                "forum": "8XgCH9y1Bs",
                "replyto": "d9rywVqCxq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1954/Reviewer_35dn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1954/Reviewer_35dn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications. I acknowledge that the paper has multiple improvements over NeMo that make it class-agnostic and extra improvements. But I think the paper is a little overclaimed that \"it proposes a new representation ...\", which has already been proposed by NeMo and adapted by this paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633861179,
                "cdate": 1700633861179,
                "tmdate": 1700633861179,
                "mdate": 1700633861179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "A67JqoBGsA",
            "forum": "8XgCH9y1Bs",
            "replyto": "8XgCH9y1Bs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1954/Reviewer_Yc1n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1954/Reviewer_Yc1n"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel 3D object representation learning method for robust classification and pose estimation, exploring the establishment of dense correspondences between image pixels and 3D template geometry. The feature of a pixel in a 2D image is mapped to the corresponding vertex in a set of pre-defined 3D template meshes, which are further trained via contrastive learning and associated camera poses for classification. Finally, the poses are estimated by the refinement from the initial pose of the template."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe motivation of this work, which is to learn representation from 3D template geometry, is technically sound and fits well into object representation learning.\n2.\tThe design of the inference pipeline is highly efficient, where image classification can be achieved merely using vertex features.\n3.\tExtensive experiments verify the effectiveness of the proposed approach on 3D object representation learning and classification, and the accurate pose estimations further demonstrate the interpretability."
                },
                "weaknesses": {
                    "value": "1.\tAll typos should be checked and corrected to improve writing quality (e.g., in the first paragraph of the Introduction Section, \"\u2026 gradient-based optimization on a specific training set (Figure ??).\").\n2.\tThere is a lack of an efficiency comparison with existing methods. It seems that the high efficiency in classification is a critical contribution of this work. A comparison of the inference speed and the number of parameters between the proposed framework and other methods can further support this claim."
                },
                "questions": {
                    "value": "Please refer to the weaknesses listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1954/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734872343,
            "cdate": 1698734872343,
            "tmdate": 1699636126894,
            "mdate": 1699636126894,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XuKuQY7YLy",
                "forum": "8XgCH9y1Bs",
                "replyto": "A67JqoBGsA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1954/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1954/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Yc1n"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback.  \n\n**All typos should be checked and corrected to improve writing quality (e.g., in the first paragraph of the Introduction Section, \"\u2026 gradient-based optimization on a specific training set (Figure ??).\").**\n\nWe are sorry for the typos and will revise the manuscript thoroughly.\n\n**There is a lack of an efficiency comparison with existing methods. It seems that the high efficiency in classification is a critical contribution of this work. A comparison of the inference speed and the number of parameters between the proposed framework and other methods can further support this claim.**\n\nIn terms of classification, our method achieves similar real-time performance as the other baselines, processing over 50 images per second. Among the baselines there is a large variation in terms of the number of parameters (Resnet: 84M, Swin-T: 28M, ViT-b-16: 86M), but we do not observe any correlation with OOD robustness. Compared to those, our model contains 83M which is comparable. However, concerning the pose estimation baseline Nemo, we have a much lower parameter count (83M vs 996M) and faster inference speed. We will make sure to emphasize this more in the revised paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257238857,
                "cdate": 1700257238857,
                "tmdate": 1700257238857,
                "mdate": 1700257238857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oen715L0Ef",
                "forum": "8XgCH9y1Bs",
                "replyto": "XuKuQY7YLy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1954/Reviewer_Yc1n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1954/Reviewer_Yc1n"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. They address most of the points about writing quality and efficiency raised in my review, and I will keep my positive rating.\n\nTo improve the manuscript, it would be nice if the authors could explicitly compare the number of parameters, FLOPS, and inference speed with baseline methods in the revised version to stress their efficiency advantage."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556085435,
                "cdate": 1700556085435,
                "tmdate": 1700556085435,
                "mdate": 1700556085435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SCuivuw2PT",
            "forum": "8XgCH9y1Bs",
            "replyto": "8XgCH9y1Bs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1954/Reviewer_wnbh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1954/Reviewer_wnbh"
            ],
            "content": {
                "summary": {
                    "value": "Authors embed 3D geometry into learning a model for image classification.\nImportant part is identification between background and class-object features, this is done by contrastive repr. learning.\n\nThe method uses the fact that objects in the image are constructed from real 3D. The 3D is represented as a mesh of faces and vertices.  The authors learn matching between image features and their corresponding vertex location.\n\nDuring the interference, the mesh is \u201crendered and compared\u201d to minimize the likelihood of features.\n\nVarious classification experiments are performed. In addition, the 3d pose estimate is evaluated too."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Combining 2D and 3D for recognition is a great way to tackle classification. It opens many options for object representation.\n\nThe method looks clear. Although I did not check all the equations, the paper is written in way so it is understandable.\n\nMany experiments (maybe too much) are presented. I especially like Sec 4.4."
                },
                "weaknesses": {
                    "value": "Authors start the paper with the statement: \u201cwe pioneer a framework for 3D object representation..\u201d, the statement is in contrast with various papers that investigate object representation using techniques on how to map image features to 3D models of faces and vertices. For example M1 or M2. \n\nDataset creation - details in questions.\n\nBaselines and 3D data - details in questions.\n\nFor example, missing papers:\nM1: Choy et al. Enriching Object Detection with 2D-3D Registration and Continuous Viewpoint Estimation CVPR 15\nM2: Start et al. Back to the Future: Learning Shape Models from 3D CAD Data\n BMVC 10"
                },
                "questions": {
                    "value": "\\kappa (eq. 3) is fixed to constant, how is the parameter fixed? Is it from validation data, guess, tuning on testing data?\n\nDoes baselines use 3D? (It looks that authors use 3D but baselines don't).\n\nIm puzzled in the dataset creation. Originally, pascal authors claim that 10812 testing images from Pascal3D+ are used to create Ocluded PASCAL3D (from official github page). Authors use 10812 Pascal3D+ images as a validation set. Is the authors validation imageset same as the testset in Ocluded Pascal? If so, then it does look like a bug in dataset creation. This question and the question above are look for me as crucial for deciding between acceptance or rejection.\n\nHowever the conference is called Conf. on Learning Representation. The paper looks to be structured for a computer vision conference. For example, I would be more interested in how the proposed representation (and its modification) affects the result of how the method expresses the objects rather than many classification results that are provided (I'm not saying skip them all) and are better suited for cvpr-like venues. I would like to see some examples where the representation is beneficial to other methods and where it is in contrast worse."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1954/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770887807,
            "cdate": 1698770887807,
            "tmdate": 1699636126793,
            "mdate": 1699636126793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wObRr3fu02",
                "forum": "8XgCH9y1Bs",
                "replyto": "SCuivuw2PT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1954/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1954/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer wnbh (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback.  \n\n**Authors start the paper with the statement: \u201cwe pioneer a framework for 3D object representation..\u201d, the statement is in contrast with various papers that investigate object representation using techniques on how to map image features to 3D models of faces and vertices. For example M1 or M2.**\n\nWe acknowledge the relevance of these papers and their techniques in mapping image features to 3D models. We will revise our claim in a more nuanced manner to better align with the state of the art. Additionally, we will incorporate references to M1 and M2 in our related work section, recognizing their significance in the exploration of object representation techniques.\n\n**$\\kappa$ (eq. 3) is fixed to constant, how is the parameter fixed? Is it from validation data, guess, tuning on testing data?**\n\nOriginally, we did not explore estimating the concentration parameter $\\kappa$ because there is no closed form solution to it. Therefore, we set it to $\\kappa=1$. Estimating these parameters is non-trivial because of the lack of a closed-form solution and potential imbalances among the visibilities of different vertices. Nevertheless, motivated by the comment of Reviewer-wnbh, we now tested the effect of approximating the parameter using the method as proposed in [A]. In the [Figure 1](https://drive.google.com/file/d/1-6IBsM2FGd84p2r54OPj4cSJoDQUBnwv/view?usp=share_link), we can observe that the concentration of the learned features is slightly higher, where the object variability is low (e.g. wheels, bottle body, \u2026), whereas it is lower for objects with very variable appearance or shape (e.g. airplanes, chairs, sofa, \u2026). When integrating the learned concentration parameter into the classification and pose estimation inference we observe almost no effect on the results (see Table 1). This needs to be studied more thoroughly, but our hypothesis is that the learned representation compensates for the mismatch of the cuboid to the object shape and therefore the weighting of the concentration parameter does not have a noticeable effect. \n\n**Table 1**: Classification results on P3D+ and OCC- P3D+ comparing our results by including and excluding the concentration parameters from the classification during inference.\n| Nuisance  | L0   | L1  | L2| L3  | Mean  |\n|-------------------------------|------|-------------------|-------------------|-------------------|-------------------|\n| Ours with learned $\\kappa$    | 99.5 | 97.2    | 88.4              | 59.2              | 81.6              |\n| Ours with $\\kappa=1$| 99.5 | 97.2              | 88.3              | 59.2              | 81.6              |\n\n\n**Do the baselines use 3D? (It looks that authors use 3D but baselines don't).**\n\nTo the best of our knowledge, our method is the first in the literature to use a 3D representation for image classification. Moreover, it is not clear how to provide existing architectures with additional 3D annotation. Based on the feedback of reviewer-wnbh, we conducted an experiment where we extended several of the classification baselines with an additional pose estimation head. In particular, we follow the popular approach of casting the pose estimation problem as bin classification by discretizing the pose space [C]. We train the models with both the image classification and pose estimation loss until convergence. As can be observed from the table below, the additional 3D information does not improve the classification performance. Moreover, this claim is supported by the t-SNE plots shown in [Figure 2](https://drive.google.com/file/d/1QYy-ExWpWtytRur3oK53yxTSEg951wud/view?usp=share_link). Simply adding 3D-pose information via an additional branch does not induce the network to learn any 3D-aware structure in the representation, whereas the 3D-aware structure is directly built-in in our approach, thus leading to large benefits in terms of robustness and interpretability.\n\nIf the reviewer has any alternative baselines to suggest that leverage the advantages of 3D, we are open to incorporating and testing them in our evaluation.\n\n\n**Table 2:** Classification results on P3D+ and occluded-P3D+ comparing resnet baseline performances by training a network per task and a single network for both. We don\u2019t observe any synergy in the unique model trained on both tasks. \n\n| Nuisance | L0    | L1  | L2  | L3 | Mean  | \n|------------------|------|----------|---------|-----|----|\n| Resnet (cls-only)| **99.3** | 93.8  | **77.8**| **45.2** | **72.3** |\n| Resnet (cls&pose)| 99.2 | **93.9** | 77.6  | 45.0 | 72.2  |\n\n[A] Sra, S. (2011). \"A short note on parameter approximation for von Mises-Fisher distributions: And a fast implementation of I_s(x)\". Computational Statistics. 27: 177\u2013190\n\n[B] Xingyi Zhou, Arjun Karpur, Linjie Luo, and Qixing Huang. Starmap for category-agnostic keypoint and viewpoint estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 318\u2013334, 2018."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257205804,
                "cdate": 1700257205804,
                "tmdate": 1700257205804,
                "mdate": 1700257205804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WbQIHAE2dI",
                "forum": "8XgCH9y1Bs",
                "replyto": "1w41PbtOV1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1954/Reviewer_wnbh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1954/Reviewer_wnbh"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank authors for the clarification. Thanks for all sections that are in the rebuttal, I read them all.\n\nMy main concern was dataset creation mistake that has been clarified. Thus, I stay with my rating: \"above the acceptance threshold\".\n\nI would like to comment on section: \"I would like to see some examples where the representation is beneficial to other methods and where it is in contrast worse.\":\n \n   - Are there any cases where your method does not work and has weak points?\n\n  - Figure3: I'm not sure if I understand it correctly. It is written that the left figure ResNet \"has been trained on both classification and pose estimation\". However, the obtained results suggest that the model did not learn any pose (azimuth). \nFig.3: It is related to Tab.2: Resnet (cls-only) vs. Resnet (cls&pose).  Is it true that this can be seen as a \"naive\" test to add pose into classification and it does not work in this way? \n\n  - Fig.2 is nice comparison with clear result.\n\n  - Kappa explanation is clear.\n\nOnce again, thank you for the rebuttal, the rest is clear. Notes here are details. As I said, I stay with rating #6: Above the acceptance threshold."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1954/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470748414,
                "cdate": 1700470748414,
                "tmdate": 1700470748414,
                "mdate": 1700470748414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]