[
    {
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models"
    },
    {
        "review": {
            "id": "AWrtfNsdbV",
            "forum": "x7cuUZxwFS",
            "replyto": "x7cuUZxwFS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5624/Reviewer_PFEM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5624/Reviewer_PFEM"
            ],
            "content": {
                "summary": {
                    "value": "Despite their success, scaling transformer models in depth remains challenging. This work introduces formulas governing signal moments in transformers, offering a unified signal propagation theory. In this paper, the proposed framework aids in addressing issues like vanishing/exploding gradients, rank collapse, and instability from high attention scores. We also propose DeepScaleLM, an initialization and scaling method conserving output/gradient moments, enabling deep model training. The proposed method improve deep narrow Bert's perplexity by 1.0 point and downstream task performance by 2.2 points compared to shallow models across various sizes, even outperforming larger shallow models with half the parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Simple idea and it makes intuitive sense.\n2. Creative combinations of various existing techniques."
                },
                "weaknesses": {
                    "value": "1. Lack of novelty: scaling Residual/Skip-Connection is a well-known trick to stabilize training of deep neural networks, very similar ideas can be found at [1, 2]\n2. Similar signal propagation idea is presented in [3]\n3. The results are not surprising [4] shows exactly the same conclusion with similar model configs. \n4. The idea of preventing rank collapse has been thoroughly explored in [5,6]\n5. Experiments on downstream tasks seem lack of diversity. More downstream tasks with different characteristics should be included.\n6. Results on more modern architectures beyond Bert should be included to present a convincing argument.\n\nOverall, most of the tricks are already well-studied and published, the paper generally feels incremental and results are not surprising. In combination with weak empirical studies on models with trivial sizes, this paper doesn't seem to be significant enough to be presented at the ICLR venue.\n\n[1] Kai, Hu, et al. \"Is normalization indispensable for training deep neural networks?.\" (Neurips 2020)\n[2] Bachlechner, Thomas, et al. \"Rezero is all you need: Fast convergence at large depth.\" Uncertainty in Artificial Intelligence. PMLR, 2021.\n[3] He, Bobby, et al. \"Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation.\" arXiv preprint arXiv:2302.10322 (2023).\n[4] Xue, Fuzhao, et al. \"A Study on Transformer Configuration and Training Objective.\" (ICML 2023).\n[5] Zhai, Shuangfei, et al. \"Stabilizing transformer training by preventing attention entropy collapse.\" International Conference on Machine Learning. PMLR, 2023.\n[6] Zhou, Daquan, et al. \"Deepvit: Towards deeper vision transformer.\" arXiv preprint arXiv:2103.11886 (2021)."
                },
                "questions": {
                    "value": "1. What happens when the same method is applied on decoder only architecture?\n2. Does the shallow network with same parameters run faster or slower in terms of wall time? What is the benefit of using deeper and narrow config beyond marginal improvement in perplexity?\n3. The results of the pretraining experiments seem off, can you please cite credible sources on numbers with similar config pretrained on Pile-CC?\n4. On the parameter counts, do you count the embedding parameter size when reshaping the networks in the experiments, which could potentially be an unfair comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5624/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697645893484,
            "cdate": 1697645893484,
            "tmdate": 1699636581466,
            "mdate": 1699636581466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NI5BysDHNM",
                "forum": "x7cuUZxwFS",
                "replyto": "AWrtfNsdbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PFEM: Part 1/2"
                    },
                    "comment": {
                        "value": "### **For more Finetuning Experiments, modern architectures and new results**:\nPlease see common response.\n\n### **Regarding rank collapse work in [5,6]**:\nWe discussed in section 3.4 and B.4 that [5] ($\\sigma$Reparam) is one of the many works that have reported/studied rank collapse, and that they propose scaling all the linear weight matrices in the model by their spectral norm. Our method theoretically prevents rank collapse at initialization, whereas [5] empirically finds that bounded weights of attention will prevent rank collapse later during training. Similarly, [6] is also one of the papers we mentioned in 3.4 and B.4, and is again a method that targets attention collapse that occurs dynamically during training, and not at initialization.\n\nBoth these methods will not prevent rank collapse at initialization caused by the very structure of the transformer model, in particular increase in correlation caused by both attention and ReLU/GeLU as shown by our formulae. This is clearly demonstrated by the fact that [5] was unable to train a 200L post-LN model with $\\sigma$ Reparam.\n\nBoth these methods are orthogonal to our method, to further stabilize training dynamics of the model - though in none of the training of our models did we observe divergence, even upto 768 layers (which is significantly deeper than the maximum depths studied in these works).\n\n\n### **Regarding scaling in prior works**:\nScaling the residual/skip connection is one of the features we use in DSLM to stabilize the model, and it indeed has a long history, as we discuss in B.3. However, as our ablations of our initialization in Table 10, and comparisons to DSInit (Table 5) shows, it is crucial to account for the exact constants in model variance propagation.\n\nWithout these constants derived using our formulae (Table 1-2, 11-16), the models perform significantly worse.\n\n### **Regarding \u2018similar signal propagation idea presented in [3]\u2019**:\nAs we discuss in our related works (Section B.2 Paragraph 3), [3] (Deep transformers without shortcuts) assumes MLP to be linear in the effect it has on attention. Our analysis does not make any such assumptions about the interactions between various model components, and in particular, we use closed-form expression for the output correlation of GeLU (the nonlinearity used in [3]). In particular, our formulae can be used to show that an MLP block with GeLU will also increase correlation, in the absence of dropout (the same setting as used in [3]). As such, at deeper depths, the method in [3] will still exhibit rank collapse.\n\nFurthermore, their 72 and 108 layer models underperform compared to a 36 layer model with same hidden dimension, in spite of having many times extra parameters - this clearly highlights deficiencies in their modeling of signal propagation, in particular the impact of non-linearities.\n\n### **Regarding similar observations as in [4]**:\n[4] (A Study on Transformer Configuration and Training Objective (Bamboo)) shows that Masked autoencoder objective may help stabilize a vanilla narrow-but-deep transformer model. Their configurations are vanilla transformers without any modifications, similar to our \u201cbaseline\u201d models. The performance of their model falls at 96 layers, and they found 48 layers to be optimal.\n\nAs can be seen in Table 4, we also observe this drop in performance, for both vanilla Pre-LN and Post-LN models for our experiments on 165M models, where deeper models (such as 192 layers) underperform. Using our method however, one can train much deeper models (such as 192 or 384 layer models) which not only converge, but outperform the vanilla transformers of [4]. These differences are even more pronounced for larger models, where vanilla post-LN BERT from [4] did not converge for deeper models.\n\n\n### **Regarding Wall Clock Time and  benefit of using deeper and narrow config**:\nAs we detailed in Section 4.4 and Appendix J, our deeper models have small overheads in compute and wall clock times compared to shallow models. The benefits of using deeper and narrower configs with our method are -\n1. Improved perplexity - allowing 165M model to outperforms the baseline 330M model with twice the params. At equal params, we achieve 1.0 improvements in perplexity.\n2. Large improvements in downstream performance - Our deeper configs outperform original models by 2.2 points on RACE and 1.0 points on MNLI. The 165M model again outperforms original model with twice the params\n3. Improved model quantization - Similar to Unit Scaling [1], our method results in models which lose much less performance, when quantized (via direct casting) to FP8 precision compared to original models, allowing our model to be served with much less GPU VRAM requirements."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504715281,
                "cdate": 1700504715281,
                "tmdate": 1700504715281,
                "mdate": 1700504715281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0rVwBS93fB",
                "forum": "x7cuUZxwFS",
                "replyto": "AWrtfNsdbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PFEM: Part 2/2"
                    },
                    "comment": {
                        "value": "### **Regarding results of the pretraining experiments compared to other works**:\nSee common response to all reviewers regarding model performance.\nRegarding counting the embedding parameter size when reshaping the networks:\nWe do not count the embedding parameter size when reshaping the model for fair comparison. As our thinner-deeper models have smaller hidden dim, their embeddings are much smaller - for example, the 192L-256D model has 25M fewer params than the baseline 12L-1024D model, and still outperforms them.\n\n### **Regarding pre-training performance**:\n\nOriginal BERT model was trained for 128B tokens, and its performance was reported in Table 6 in Devlin et al. Our models are trained for Chinchilla optimal 3B/6B tokens, 40-20x fewer than the original BERT pre-training tokens. Also, the original Devlin et al models are trained on Book Corpus and Wikipedia, which are much more cleaner datasets than Pile-CC, which is derived from Common Crawl.\n\nOur baseline model are trained for 1e-19 flops, and 165M models for 2.5e-18 flops. As can be seen in figure A5 of Chinchilla paper, their experiments predict a loss of approximately 3.2 to 2.8 given this compute budget - which our models clearly outperform.  Our GPT models achieve better performance than this at a loss of $ln(11.6)=2.45$, our 330M BERT achieves a loss of $ln(13.2)=2.58$, and the 165M model achieves $ln(14.2)=2.65$. Furthermore in [1], for a 32-layer 1024D model trained on C4 (another common crawl derived dataset) for 800M tokens, they report a perplexity of 24.7 for vanilla pre-LN transformer. At the same number of tokens, our baseline GPT 24L 1024D model was at 11.8 PPL.\n\nWe would like to reiterate that we used all the original hyper-parameters of BERT/GPT, and our baseline models directly used the original Megatron-LM codebase. Furthermore, we did a sweep of LR to find the best LR. We will release the training scripts to enable direct reproduction/comparison of our training.\n\n[1] He, Bobby, et al. \"Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation.\" The Eleventh International Conference on Learning Representations. 2022.\n\n### **Regarding Downstream performance**:\nWe add the performance of our model on the MNLI task as well (see common response). Figure 5 in Devlin et al. provides the expected MNLI dev accuracy based on the number of pretraining steps. From Figure 5 of BERT paper, this would correspond to around 25k steps and the expected accuracy is under 80% for 110M Bert Base. Our models achieve better accuracy than expected based on this Figure."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504755624,
                "cdate": 1700504755624,
                "tmdate": 1700504755624,
                "mdate": 1700504755624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uuQh41D3Ms",
                "forum": "x7cuUZxwFS",
                "replyto": "AWrtfNsdbV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for Rebuttal Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to thank you for reviewing our paper. With less than 1 day left, we would appreciate if you could provide feedback regarding whether our response has addressed your concerns. Kindly let us know if there are any other details you would like us to clarify!\n\nBest regards,\nThe Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670994222,
                "cdate": 1700670994222,
                "tmdate": 1700670994222,
                "mdate": 1700670994222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Byqmfc1amC",
                "forum": "x7cuUZxwFS",
                "replyto": "uuQh41D3Ms",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Reviewer_PFEM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Reviewer_PFEM"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your responses"
                    },
                    "comment": {
                        "value": "I have read the responses and do not have further questions. I still found the experiment results not convincing and in combination with lack of novelty, I can not recommend acceptance."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690286174,
                "cdate": 1700690286174,
                "tmdate": 1700690286174,
                "mdate": 1700690286174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QkQdKYr1Jq",
            "forum": "x7cuUZxwFS",
            "replyto": "x7cuUZxwFS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5624/Reviewer_wRZy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5624/Reviewer_wRZy"
            ],
            "content": {
                "summary": {
                    "value": "The author introduces a theory to understand unstable issues in deep transformers and suggests a solution called DeepScaleLM. Their experiments show the effectiveness and superior performance of this method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors present a novel theoretical analysis concerning the moments of transformer models.\n3. Their development of an effective, theory-driven approach is both sound and provides valuable insights.\n3. They conducted comprehensive experimental exploration to support their theory."
                },
                "weaknesses": {
                    "value": "While I am not well-versed in the experimental section, I would like to point out certain aspects that I found challenging or unclear during my reading.\n\n1. In Figure 2, there seems to be a discrepancy. The author mentions that the backward gradient variance rises hyperbolically with N, but the depicted curve suggests a decline as N grows. This is somewhat perplexing.\n2. The representation and caption for Figure 5 lack clarity, making it challenging to decipher the conveyed information.\n3. For Table 4 and Figure 7, it would be beneficial if the author could elucidate why the thinnest and deepest transformers utilizing DSLM yield the most optimal results."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5624/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5624/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5624/Reviewer_wRZy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5624/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699565043,
            "cdate": 1698699565043,
            "tmdate": 1699636581371,
            "mdate": 1699636581371,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gobndo0vhi",
                "forum": "x7cuUZxwFS",
                "replyto": "QkQdKYr1Jq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wRZy"
                    },
                    "comment": {
                        "value": "### **Regarding gradient variance growth discrepancy in Figure 2**:\nWe would like to clarify Figure1-4. These figures depict the output (forward) and gradient (backward) variance after the $N^{th}$ layer, for a single model with 192 layers. The layers with higher numbers are near the output, lowest numbers are near the input, with $N$ denoting the current layer. As can be seen from Figure 2, as the gradient is backpropagated to shallower layers, it increases hyperbolically as a function of the current layer number $N$.\n\nWe should have used $n$ in Figure 1-4 to ensure consistency with our derivation in Appendix H. We will add the above clarifications to Figure 1-4. We will release the scripts to reproduce these figures 1-4.\n\n### **Regarding lack of clarity in Figure 5**:\nFigure 5 depicts the input correlation $r^l_x$ (along x axis), vs the output correlation (along y axis) of a transformer layer, for both FFN block (Red line) and Attention Block (Blue line). The grey line is the $y=x$ line, added to show more clearly that the FFN block decreases correlation (compared to its input) after  $r^l_x > 0.65$, and that the Attention block always outputs correlation 0.9. These output correlations are plotted using Formulae for $r^l_{x_{out}}$ from Table 2.\n\nThis figure shows that with dropout, rank collapse (i.e., a situation where $r^l_x \\approx 1$) will not occur in a standard transformer. This result is then empirically verified in Figure 6, where it can be clearly seen that no rank collapse is observed, contrary to the findings of Noci. et. al. 2022, as further discussed in Appendix G. We will explain these figures more clearly in the manuscript.\n\n### **Regarding explanation about why the thin/deep transformers utilizing DSLM yield the most optimal results (Table 4 and Figure 7)**:\nTable 4 and Figure 7 show that standard transformers should be much more deep, at the expense of width - for example for 165M params, 48-192 layers deep, and for 330M params, 96-384 layers deep. Mont\u00fafar et al. (2014); Raghu et al. (2017) show that the complexity of Deep Neural Nets increases polynomially with width and exponentially with depth. Given a fixed parameter budget, there is a tradeoff between having richer representations from more deep layers stacked one after another, versus the representation capacity of model width.\n\nNote that the 768 layer model underperforms compared to the 192 model - after a very large depth, the width becomes too narrow for the model to have enough representation capacity. The thinnest and deepest model (768 layer) does not perform the best - But our experiments show that, with correct initialization and scaling, the optimal depth is much more deeper (and the width narrower) than is usually used in standard models."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504463665,
                "cdate": 1700504463665,
                "tmdate": 1700504463665,
                "mdate": 1700504463665,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6cYsYZ3e1h",
                "forum": "x7cuUZxwFS",
                "replyto": "QkQdKYr1Jq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for Rebuttal Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to thank you for reviewing our paper. With less than 1 day left, we would appreciate if you could provide feedback regarding whether our response has addressed your concerns. Kindly let us know if there are any other details you would like us to clarify!\n\nBest regards,\nThe Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670968894,
                "cdate": 1700670968894,
                "tmdate": 1700670968894,
                "mdate": 1700670968894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "li6hRSTJiQ",
                "forum": "x7cuUZxwFS",
                "replyto": "6cYsYZ3e1h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Reviewer_wRZy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Reviewer_wRZy"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your response. After reading the rebuttal and the other reviews,  I decided to keep my original score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691423029,
                "cdate": 1700691423029,
                "tmdate": 1700691423029,
                "mdate": 1700691423029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IzhOPrwdLT",
            "forum": "x7cuUZxwFS",
            "replyto": "x7cuUZxwFS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5624/Reviewer_eeiy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5624/Reviewer_eeiy"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript studies signal propagation in transformer networks\nto study difficulties in training deep transformer networks.\nFrom the analysis, an initialisation method is proposed to facilitate learning.\nThe theoretical results are verified empirically on language modelling tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- (significance) Enabling the training of deeper architectures typically leads to improved performance on a variety of tasks.\n   At least historically, this kind of result has proven extremely impactful.\n - (clarity) The paper is well written and easy to follow.\n   I especially liked how Table&nbsp;1 provides an overview over signal propagation through the different building blocks.\n - (originality) This is the first work that performs such a thorough signal propagation analysis for transformer models.\n - (quality) The derivations in the appenix provide evidence for the theoretical claims."
                },
                "weaknesses": {
                    "value": "- (originality) A very similar idea for regular networks has been presented in (Arpit et al., 2016).\n   The connections with this work should definitely be discussed.\n - (originality) By moving all related work to the appendix and not citing much in the main text it becomes unclear which concepts are new and which already exist.\n   I did notice the references to the appendix section, but I would argue that a citation puts more emphasis on the fact that something already exists and is not a contribution of this work.\n   Citing one (seminal) paper for each aspect should suffice to counter this issue.\n - (clarity) In Section&nbsp;4.4 an initialisation that renders a model more linear is claimed to be bad for performance.\n   However, there is quite a bit of work where this kind of linearity is considered desirable (e.g. Hardt &amp; Moritz, 2017; Zhang et al., 2019)\n - (clarity) I am not sure if it makes sense to report perplexity in the context of masked language modelling.\n   I am no expert in NLP, but I thought perplexity is only meaningful for autoregressive language models.\n - (quality) The baseline performances seem to be remarkably weak.\n   Table&nbsp;6 in (Delvin et al., 2019) reports perplexities in the range 3-5.\n   Results for GPT models on the pile go below 1.\n   Also, Yang et al. (2019) report results in the range of 70-76% accuracy for BERT on RACE.\n - (significance) The results in this paper can not be directly applied to vision transformers.\n   It could be emphasised a little stronger in the main paper that the analysis focuses on text inputs.\n - (significance) There are too little direct comparisons with competing methods (e.g.&nbsp;Zhang et al., 2019; Noci et al., 2022; Wang et al., 2022a; He et al., 2023).\n   Apart from Table&nbsp;5, these alternative methods seem to be ignored completely.\n   Also, it would be easier to compare if some of the experiments from these papers would have been adopted.\n - (significance) The experiments all seem to use the same architecture.\n   Ideally, an initialisation method works for a variety of architectures.\n   This is never properly tested.\n - (quality) Experiments do not have error bars.\n   Especially for a random initialisation strategy, error bars would be helpful to assess how consistent the improvements are.\n   If error bars would make the experiments prohibitively expensive, it would be nice to include at least one small-scale experiment to provide some insights on the variability of the proposed method.\n\n### Appendix\n\n - (quality) The propagation of correlation between samples was introduced by (Poole et al., 2016), not by (Schoenholz et al., 2017).\n - (quality) It seems like two fundamental signal propagation papers are missing in the related work.\n   The foundations for signal propagation analysis can be found in (Neal, 1996) and a popular reference is (LeCun et al., 1998).\n - (quality) In Section&nbsp;B.2\u00a71, it is claimed that this work considers expectations over inputs, but the expectations are over inputs AND weights (cf.&nbsp;Glorot &amp; Bengio).\n - (clarity) Section&nbsp;B.2\u00a74 states that non-IID inputs are \"also\" accounted for, but I would argue that this is the only case that is accounted for.\n - (clarity) It is unclear what computation is contained in the embedding component.\n   Originally, I suspected this to be only about the token embeddings, but it seems to include other computations as well.\n - (quality) The approximation for the correlation in the embedding layer seems to be missing the Euler constant:\n   $$\\sum_{i=1}^{|V|} p_i^2 = \\frac{\\sum_{i=1}^{|V|} 1 / i^2}{\\Big(\\sum_{i=1}^{|V|} 1 / i\\Big)^2} \\approx \\frac{\\zeta(2)}{(\\ln |V| + \\gamma)^2}.$$\n   Without the constant, the approximation is pretty bad.\n   Also, it could be made clearer in the derivation where this approximation comes from and that the final simplification step assumes large $L$.\n - (originality) It should be more clearly stated for each derivation where it can be found in literature.\n   In its current form, it is hard to distinguish which derivations are really new.\n - (clarity) Instead of using the identity $\\pi - \\arccos(x) = \\frac{\\pi}{2} + \\arcsin(x)$, it would be better to use $\\pi - \\arccos(x) = \\arccos(-x)$ to stay closer to the formulation from (Cho &nbsp; Saul, 2009; Daniely et al., 2016).\n   Similarly, the GELU variance can be further reduced to $$\\frac{\\sigma^2}{2 \\pi} \\bigg(\\arccos\\Bigl(\\frac{-\\sigma^2}{1 + \\sigma^2}\\Bigr) + \\frac{2 \\sigma^2}{(1 + \\sigma^2) \\sqrt{2 \\sigma^2 + 1}} - \\frac{\\sigma^2}{1 + \\sigma^2}\\bigg).$$\n - (clarity) The derivation of LayerNorm could use some more explanation.\n   Also, the dependencies between samples, sample mean and sample variance should be discussed a bit more.\n   Finally, the affine transformation that is typically included after each normalisation layer is not addressed at all.\n - (clarity) It should be more clear what the limitations of the different approximations for the softmax derivation are.\n   A quick numerical check verifies that the approximation breaks down quite quickly for larger variances.\n   Although this is probably not practically relevant, it would be nice to provide some insights when the analysis breaks down.\n - I noticed that the derivation for scaled dot-product attention starts with some rough shortcuts.\n   Also, a completely different approach is used to handle the softmax.\n   This seems suspicious.\n   I do not have time to check the math any further, but the numerical results should provide enough proof that these results, if not correct, are at least useful.\n\n### Minor Comments\n\n - Citations could be polished a bit more (e.g. \"Deep Information Propagation\" has been published at ICLR, \"ReZero is all you need\" has been published at ICML, ...).\n   I also noticed not some references have a link while others do not have an URL.\n - possible typos in Section&nbsp;1\u00a75: \"issues with very deep transformerS\", \n   in Section&nbsp;1\u00a76: \"ensures the moments of outputs and gradients (to) remain fully conserved\"\n\n### References\n\n - Neal, R. M. (1996). \n   Bayesian Learning for Neural Networks.\n - LeCun, Y., Bottou, L., Orr, G. B., & M\u00fcller, K.-R. (1998). \n   Efficient BackProp.\n   Neural Networks: Tricks of the Trade (1st ed., pp. 9\u201350).\n - Cho, Y. &amp; Saul, L. (2009). \n   Kernel methods for deep learning. \n   Advances in neural information processing systems, 22.\n - Arpit, D., Zhou, Y., Kota, B., & Govindaraju, V. (2016). \n   Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks. \n   Proceedings of The 33rd International Conference on Machine Learning, 48.\n - Daniely, A., Frostig, R., &amp; Singer, Y. (2016). \n   Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity. \n   Advances in Neural Information Processing Systems, 29.\n  - Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., &amp; Ganguli, S. (2016). \n   Exponential expressivity in deep neural networks through transient chaos. \n   Advances in Neural Information Processing Systems, 29. \n - Hardt, M., &amp; Ma, T. (2017). \n   Identity Matters in Deep Learning. \n   International Conference on Learning Representations, 5.\n - Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., &amp; Le, Q. V. (2019). \n   Xlnet: Generalized autoregressive pretraining for language understanding. \n   Advances in neural information processing systems, 32."
                },
                "questions": {
                    "value": "1. Please, add citations to make clear which parts of this work are not part of the contribution.\n 2. Please, connect the proposed initialisation to the work from (Arpit et al., 2016).\n 3. How is perplexity computed for masked language modelling and why does it make sense as a metric?\n 4. Do you have any references or other explanation for the seamingly poor baseline results?\n 5. How easy or difficult would it be to apply the analysis to vision transformers or transformers that do not work with language/embeddings?\n 6. How does deepScaleLM compare to other (non-standard) initialisation strategies and deep transformers?\n 7. How does deepScaleLM perform on different architectures (e.g. GPT, long-context transformers, ...)?\n 8. Would it be feasible to include error bars for some of the results?\n 9. How can the numerical results be so good if the approximation for the Embedding layer correlations is significantly off? \n    Is correlation not that important after all?\n 10. Why are the insights from Section&nbsp;C.7 not reused for the analysis in Section&nbsp;C.8?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5624/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699379302151,
            "cdate": 1699379302151,
            "tmdate": 1699636581279,
            "mdate": 1699636581279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cMLKKjIujA",
                "forum": "x7cuUZxwFS",
                "replyto": "IzhOPrwdLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eeiy : Part 1/4"
                    },
                    "comment": {
                        "value": "### **Regarding applying DSLM to Vision Transformers**:\nKindly refer to common response.\n\n### **Regarding using the same architecture for the initialisation method**:\nPlease see common responses for results with decoder-only (GPT) architecture.\n\n### **Regarding comparison with Arpit et al., 2016**:\nNormalization Propagation (Arpit et al., 2016), proposes theoretical value for the batch statistics of mean and variance after ReLU, and uses these to normalize the output after each Relu. Their theoretical formulae for $\\mu_x$, $\\sigma^2_x$, and $\\sigma^2_g$ are the same as in this work. This normalization successfully conserves the mean and variance of the forward propagation of the model as zero mean and unit variance.\n\nHowever, this results in the backward gradient variance increasing by $0.5*\\frac{1}{\\sigma^2_x} = 1.47$, and to counteract this, they scale the inputs before applying relu by $\\sqrt{\\frac{1}{1.47}}$, and this successfully conserves the gradient as unit variance.\n\nIf our understanding of Arpit et al 2016 is correct, this scaling operation of the input results in the mean and the variance no longer being conserved - something seemingly ignored in Arpit et al. In fact, the variance falls to $30\\%$ of initial value after just 4 layers. Applying an analysis similar to Klambauer et al. 2017 shows that Arpit et al.\u2019s method converges to a fixed stable point of approximately $\\mu=-0.34$ and $\\sigma^2_x=0.24$, as we confirmed using numerical simulations. While we skip this proof here for brevity, kindly let us know if you would like us to expand on its proof and simulation results.\n\nFurthermore, Arpit et. al does not handle attention, which we show requires careful consideration of covariances.\n\n### **Regarding reformatting the Related Works section**:\nSince our work uses proper initialization, residual scaling and signal propagation, we believed a detailed discussion of related works covering all these topics was essential. We felt that a short related works section in the main paper would not do justice to the large number of prior works we wanted to discuss. As such, we dedicated 2 pages to the related works, and it was difficult to have this entire section in the main paper due to space constraints. Instead, throughout our main paper, we tried to refer to prior works in-line. Being cognizant of the importance of this section, we made it the first section of our appendix. We will add a summarized related works section in the main paper as suggested. We had missed Poole et al., 2016 and the seminal references (Neal, 1996, & LeCun et al., 1998), thank you for pointing it out. We will add these to our manuscript.\n\n\n### **Regarding \u201clinearity\u201d being bad for performance**:\nHardt & Moritz, 2017 suggests that a network should be able to express the identity transformation, in order to optimize well. We would like to clarify that by a \u201cmore linear\u201d network, we are referring to a network which behaves almost like a single linear projection during training, because the residuals are negligibly small. As a thought experiment, if $\\beta$ is reduced to a very small value (such as 1e-20), and assuming layernorm gain is fixed to 1, the transformer network will return the same output as input - and even training for a long time will not affect the model\u2019s output significantly. As [1] shows, there is an expressivity-trainability tradeoff in training deep networks, and while having lower $\\beta$ will result in networks whose gradients don\u2019t explode/vanish, they will converge slowly/suboptimally.\n\n\n[1] Yang, Ge, and Samuel Schoenholz. \"Mean field residual networks: On the edge of chaos.\" Advances in neural information processing systems 30 (2017).\n\n### **Regarding pre-training performance**:\n\nOriginal BERT model was trained for 128B tokens, and its performance was reported in Table 6 in Devlin et al. Our models are trained for Chinchilla optimal 3B/6B tokens, 40-20x fewer than the original BERT pre-training tokens. Also, the original Devlin et al models are trained on Book Corpus and Wikipedia, which are much more cleaner datasets than Pile-CC, which is derived from Common Crawl.\n\nOur baseline model are trained for 1e-19 flops, and 165M models for 2.5e-18 flops. As can be seen in figure A5 of Chinchilla paper, their experiments predict a loss of approximately 3.2 to 2.8 given this compute budget - which our models clearly outperform.  Our GPT models achieve better performance than this at a loss of $ln(11.6)=2.45$, our 330M BERT achieves a loss of $ln(13.2)=2.58$, and the 165M model achieves $ln(14.2)=2.65$.\n\nWe would like to reiterate that we used all the original hyper-parameters of BERT/GPT, and our baseline models directly used the original Megatron-LM codebase. Furthermore, we did a sweep of LR to find the best LR. We will release the training scripts to enable direct reproduction/comparison of our training."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504838815,
                "cdate": 1700504838815,
                "tmdate": 1700504838815,
                "mdate": 1700504838815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QhEqF43tzA",
                "forum": "x7cuUZxwFS",
                "replyto": "IzhOPrwdLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eeiy : Part 2/4"
                    },
                    "comment": {
                        "value": "### **Regarding Downstream performance**:\nWe add the performance of our model on the MNLI task as well (see common response). Figure 5 in Devlin et al. provides the expected MNLI dev accuracy based on the number of pretraining steps. From Figure 5 of BERT paper, our models would correspond to around 25k steps and the expected accuracy is under 80% for 110M Bert Base. Our models achieve better accuracy than expected based on this Figure.\n\n### **Regarding Perplexity**:\nThe confusion regarding weak performance arose from two different definitions of perplexity. We calculate perplexity as the exponential of the model\u2019s loss, (as provided in Megatron-LM paper[1] and code [link](https://github.com/NVIDIA/Megatron-LM/blob/443ce9f3f98fdc5a53c6b480c6e21b79944d198e/megatron/training.py#L975). Regarding GPT perplexity < 1, we estimate that the reviewer is referring to bits-per-byte as perplexity, whereas we calculate on a token level. In hindsight, we should have clarified this in the main paper. The chinchilla paper (in Figure A5) shows expected loss given our compute limit of 1e-19 flops at approx 2.8, and our GPT models achieve better performance than this.\n\nFurthermore in [2], for a 32-layer 1024D model trained on C4 (another common crawl derived dataset) for 800M tokens, they report a perplexity of 24.7 for vanilla pre-LN transformer. At the same number of tokens, our baseline 24L 1024D model was at 11.8 PPL.\n\n[1] Shoeybi, Mohammad, et al. \"Megatron-lm: Training multi-billion parameter language models using model parallelism.\" arXiv preprint arXiv:1909.08053 (2019).\n\n[2] He, Bobby, et al. \"Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation.\" The Eleventh International Conference on Learning Representations. 2022.\n\n\n### **Regarding consistency of improvements and error bars**:\nIn our initial experiments, we observed very little variation in performance across different runs - we conjecture that the model is trained on a large enough number of tokens for differences in initialization/data seed to not matter. We provide numbers below for 12L-1024D Post-LN and DSLM models from Table 4 below -\n\n| Model | Mean | Standard Error |\n| ------ | ------ |  ------ |\n|Post-LN Baseline |14.33 | 0.14 |\n| DSLM | 15.56 | 0.08  |\n\nAs the variation was so small, and due to compute limitations, we did not run multiple runs for other experiments thereafter. We also reported the best score for Baseline Post-LN, and the worst score for DSLM for the 12L-1024D models Table 4 for a conservative comparison. We will add these details in the paper.\n\n### **Regarding non-IID inputs being the only case that is accounted for in  B.2**:\nOur exact formulae for blocks and components also account for IID cases - as can be verified by our simulations, in which we do cover cases IID inputs with exactly 0 correlation, as noted in $Corr^l_{x_{in}}$ column in Table 18. In the simplified formulae for Table 19, and in DeepScaleLM initialization and model, we simplified our formulae so that they only remain accurate for non-IID inputs. This was because of three considerations:\n1. In NLP domain, most text will inevitably be non-IID due to repeated common words. This was encountered in all our experiments.\n2. Even in Vision, for ViT in particular, there will be correlation among pixel intensities across patch embeddings, as discussed in common response section.\n3. Even if there is exactly 0 correlation in input, the very first attention layer and the first FFN layer in particular, will add correlations to the output, ensuring our simplified formulae hold reasonably accurately.\n\n\n### **Regarding the computation contained in the embedding component**:\nWe will update our paper with clear description and details of the embedding component. Our embedding component is same as that of the original BERT model, except for position embeddings. The embeddings component of BERT consists of 3 look up tables - token embeddings, position embeddings, and segment embeddings. For a given input token, each of these 3 embeddings are added before being passed to the transformer model. We describe each of these below -\n\n- Word Embeddings - A lookup table mapping input word ids, of size (Vocabulary X Hidden_dim). Output denoted by $x_{out_{we}}$ in Section 3.1.\n\n- Position Embeddings -  A lookup table mapping input positions, of size (Sequence_length X Hidden_dim). We use the more commonly used trainable position embeddings (compared to BERT\u2019s original sinusoidal embeddings)\n\n- Segment Embeddings - A lookup table mapping segments (denoting Sentence A or Sentence B for Next Sentence Prediction task of BERT), of size (2 X Hidden_dim). Output denoted by $x_{out_{se}}$ in Section 3.1."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504882705,
                "cdate": 1700504882705,
                "tmdate": 1700504882705,
                "mdate": 1700504882705,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DXLuZzTGNC",
                "forum": "x7cuUZxwFS",
                "replyto": "IzhOPrwdLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eeiy : Part 3/4"
                    },
                    "comment": {
                        "value": "### **Regarding missing the Euler constant in the approximation for the correlation in the embedding layer**:\n\nWe would like to apologize for a minor error here, where we wrote our predicted $r^l_{x_{in}} = 0.247$. In writing this value for the paper, we had mistakenly used Log to the base 10, instead of natural logarithm. Using natural logarithm gives a predicted value of 0.2273 (instead of the 0.247), which is within 3% of the empirical correlation 0.221. Adding the euler\u2019s constant term gives an even closer value of 0.2267, but not significantly different from our simplified version. In our experiments, we had used the natural logarithm value of 0.227.\n\nWe had skipped the Euler constant, as it was relatively small (0.58) compared to $Log_e(|V|)=10.37$ for our vocabulary of size 32000. We will clarify this, as well as mention the simplification from large L.\n\n### **Regarding stating which formulae can be found in literature**:\nWe will more clearly specify which formulae (or equivalent forms) cannot be found in prior literature, and also color-code Tables 1, 11-16 for the same.\n\n### **Regarding more explanation in the derivation of LayerNorm**:\nWe will add further explanations to the LayerNorm derivations. The affine transformation for layernorm are typically initialized with 1 scale and 0 bias, hence do not change any of our derivations and were hence ignored. We will clarify this in our paper.\n\n### **Regarding limitations of different approximations**:\nWe will add a discussion on the effective range of softmax approximation. Our approximations become inaccurate for larger variances, particularly when softmax starts to \u201cdegenerate\u201d to a single dimension 1 and others 0. It is reasonably accurate in more realistic ranges however.\n\n### **Regarding the proof of scaled dot-product attention being different from softmax**:\nWe had initially attempted a much more simpler proof, where one assumed attention scores to be independent of values. This then enabled the use of our previous LogNormal-based softmax derivation to easily derive the forward variances. But the theoretically predicted values strongly disagreed with empirical values from simulations. This is because SHA is -\n$\\mathrm{Dropout}(\\mathrm{SoftMax}(\\frac{\\mathbf{X_{\\text{in}}}\\mathbf{W_Q}\\mathbf{{W_K}^T}\\mathbf{X^T_{\\text{in}}}}{\\sqrt{d_k}}))\\mathbf{X_{\\text{in}}}\\mathbf{W_V}$ , and the $\\mathbf{{W_K}^T}\\mathbf{X^T_{\\text{in}}}$ term cannot be treated independently of the $\\mathbf{X_{\\text{in}}}\\mathbf{W_V}$ term.\n\nA simple verification of this can be checked by simply simulating $(XW)^T*X$, and verifying that the variances of the results do not match that of $L * \\sigma^2(XW)$, but do if the second X is replaced by another random tensor.\n\nThis necessitates an alternate methodology to derive SHA, where the components are treated as a unified whole.\n\n### **Regarding using perplexity for masked language modeling**:\nThe perplexity definition used here is simply exponential of the pre-training test-set loss, and hence a direct measure of model pre-training performance. Perplexity for MLM has been used in other works, such as Roberta [1], Deberta[2], Megatron LM[3], Scaling Laws vs Model Architectures[4], or a perplexity variant in [5], and in [6].\n\n[1] Liu, Yinhan, et al. \"Roberta: A robustly optimized bert pretraining approach.\" arXiv preprint arXiv:1907.11692 (2019).\n\n[2] He, Pengcheng, et al. \"DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION.\" International Conference on Learning Representations. 2020.\n\n[3] Shoeybi, Mohammad, et al. \"Megatron-lm: Training multi-billion parameter language models using model parallelism.\" arXiv preprint arXiv:1909.08053 (2019).\n\n[4] Tay, Yi, et al. \"Scaling laws vs model architectures: How does inductive bias influence scaling?.\" arXiv preprint arXiv:2207.10551 (2022).\n\n[5] Salazar, Julian, et al. \"Masked Language Model Scoring.\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.\n\n[6] Lu, Jinghui, et al. \"What Makes Pre-trained Language Models Better Zero-shot Learners?.\" Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504950571,
                "cdate": 1700504950571,
                "tmdate": 1700504950571,
                "mdate": 1700504950571,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "isT1Xai20A",
                "forum": "x7cuUZxwFS",
                "replyto": "IzhOPrwdLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eeiy : Part 4/4"
                    },
                    "comment": {
                        "value": "### **Regarding accuracy of numerical results**:\nKindly refer to the response regarding 'approximation for the correlation in the embedding layer'. Our correlation estimates were within 3% of the ground truth value. Furthermore, as can be clearly seen from our formulae and from simulations, correlation is indeed very critical to correctly model signal propagation in transformers.\n\n\n### **Regarding comparisons with competing methods**:\n\nWe further discuss the methods the reviewer mentioned:\n\n**Zhang et al 2019**: We discuss DSInit\u2019s signal propagation in 3.3, their scaling in B.3, and compare performance for two deep models in Table 5. Crucially, DSInit\u2019s initialization fails to converge for the 96-layer model despite a more conservative scaling of $O(N^{-2})$, highlighting the impact of carefully considering the constants (from initialization) hidden away in asymptotic formulations.\n\n**Noci et al 2022**: We provide a detailed discussion of the Rank Collapse of Noci. et. al. in Section 3.4,  Appendix G highlighting some issues with their rank collapse. We also discuss some deficiencies in their signal propagation in 3.3 and B.2 - By not assuming constant attention, we arrive at exponential back-propagated gradients to Q,K, which we show can affect model stability, both theoretically and empirically. We also refer to their scaling in 3.4 and B.3. Their scaling corresponds to $\\lambda=1$ and $\\beta=k/N$. As we mentioned in B.3, we used $lambda^2 +\\beta^2 = 1$ instead, as He. et al 2023 showed fully normalized residual connections often result in better performance. Our initial experiments also showed lower scores with $\\lambda=1$.\n\n**Wang et al. 2022a**: We discuss DeepNorm\u2019s signal propagation in 3.3, and compare performance for two deep models in Table 5. As we further discuss in B.3, DeepNorm shows performance improvements on making the model deeper but keeping the hidden dimension constant, with a larger model - whereas our method shows performance improvements on making the model deeper while keeping the parameters constant.\n\n**He et al. 2023**: We discuss the signal propagation theory of He et al in B.2, scaling in B.3. While He et al. considers the MLP to be linear, but by also considering the non-linearity in MLP we can correctly model how the nonlinearity of MLP strongly affects attention via correlation. We arrive at formulae which model the propagation accurately across the entire model, which cannot be done without considering the non-linearity. Their 72 and 108 layer models underperform compared to a 36 layer model with same hidden dimension, in spite of having many times extra parameters - compared to our method where deeper models outperform standard models without extra params, we hence did not benchmark this method. [2] also found He et al to underperform compared to baselines.\n\nWe would like to further discuss 3 more methods for deeper models, which we investigated:\n\n**Admin (Liu et. al. 2020a)**: Admin requires additional \u201cprofiling\u201d passes over the dataset to achieve unit variance for forward propagation, as we discuss in B.1. - while we can do this fully theoretically. We attempted to train our 192L-256D models of Table 5 with Admin, but all our experiments failed to converge, even after decreasing LR by 20x. As Admin paper only trained upto 72-layer models, we did not consider it for inclusion in Table 5.\n\n**SkipInit**: SkipInit (De & Smith, 2020) underperforms compared to baseline Pre-LN models for Language Modelling, as observed by He et. al. 2023. As our method outperforms baseline Pre-LN model, we did not perform comparisons to this method.\n\n**Shaped Attention**:  As per [2], Shaped Attention[1] does not perform as well as baseline models, resulting in significantly worse performance (Figure 2, 3, 5).\n\n[1] Noci, Lorenzo, et al. \"The shaped transformer: Attention models in the infinite depth-and-width limit.\" arXiv preprint arXiv:2306.17759 (2023).\n\n[2] Anonymous, Simplifying Transformer Blocks, 2023. https://openreview.net/forum?id=RtDok9eS3s"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505139159,
                "cdate": 1700505139159,
                "tmdate": 1700505139159,
                "mdate": 1700505139159,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wdnl0VdI8I",
                "forum": "x7cuUZxwFS",
                "replyto": "isT1Xai20A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Reviewer_eeiy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Reviewer_eeiy"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal acknowledgement"
                    },
                    "comment": {
                        "value": "I thank the authors for their elaborate rebuttal and will consider it for my final decision.\nI also like the color-coding idea to make clear which results are novel!\n\nA few questions remain after a quick read-through of the rebuttal:\n - The perplexity computations as detailed in appendix E of the megatron paper that you refer to seems to confirm that it is a metric for autoregressive predictions.\n   In masked language modelling, you do not necessarily have all of the previous tokens if they have been masked.\n  As a result, it is still not clear why perplexity makes sense in a masked language modelling setting. \n  Does any of the listed papers address this discrepancy?\n - The rebuttal emphasises the need for chinchilla scaling laws to compare results from other works.\n   I am aware of these scaling laws, but I have never seen them being used in this way.\n   Is this a common way to compare models? If yes, I would expect at least one reference that reports similar results.\n   Maybe the other reviewers might also be able to clear this for me, but the performance is so much worse than other reported results and I am not entirely convinced (yet) that these scaling laws can be used in this way.\n - Concerning the comparison with competing models: I would have been interested in an empirical comparison, but I understand if this is not feasible."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650764527,
                "cdate": 1700650764527,
                "tmdate": 1700650764527,
                "mdate": 1700650764527,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KsKM7zO3Q8",
            "forum": "x7cuUZxwFS",
            "replyto": "x7cuUZxwFS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5624/Reviewer_vaGo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5624/Reviewer_vaGo"
            ],
            "content": {
                "summary": {
                    "value": "This paper derives a closed form signal propagation formula, i.e., mean, variance, and input-output correlation, for each component in Transformer, including both forward and backward passes. It helps explain gradient vanishing or explosion, rank collapse, and training instability of Transformer. Leveraging these formulas, the paper then proposes a new initialization scheme, DeepScaleLM, that stabilizes the signal propagation across training. Experimental results show that with the proposed initialization scheme, Transformer with 100s of layers can be trained and obtains better results than a shallow counterpart."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper studies a fundamental problem of scaling neural nets in depth. The study follows the direction of signal propagation, which is promising. The derived mean and variance formulas are shown to be matching the empirical simulation. Experimental results are also encouraging, showing depth is indeed helpful for Transformer as well."
                },
                "weaknesses": {
                    "value": "1. The main issue probably comes from the model selection in the experiment section. Although BERT experiments are good, an additional evaluation using an encoder-decoder Transformer or decoder-only Transformer would provide more interesting spikes and make the justification more compelling.\n\n2. The concrete picture of the adjusted model architecture is missing to the reader. The proposed DeepScaleLM scheme scales residual connections, and seems like it adds extra dropout layers. In which component are these adjustments applied? Part of a layer or every component in a layer? Partial layers or every layer?"
                },
                "questions": {
                    "value": "1. Plotting log-scale in Figure 3 does not help the reader understand deeply about the gap between empirical simulation and theoretical prediction. Providing statistics such as min, max, mean, variance of the gap would be better.\n\n2. In Section 2, the paper assumes normal distribution of inputs, weights, and gradients while deriving the formulas in Table 1. To what extent this assumption holds since in Section 1 paragraph 2 the paper mentioned that some of the assumptions in prior works broke down on real world data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5624/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5624/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5624/Reviewer_vaGo"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5624/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699657184732,
            "cdate": 1699657184732,
            "tmdate": 1699657184732,
            "mdate": 1699657184732,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "avAH9W4bIg",
                "forum": "x7cuUZxwFS",
                "replyto": "KsKM7zO3Q8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vaGo"
                    },
                    "comment": {
                        "value": "### **Regarding more experiments with decoder-only Transformer, and new finetuning datasets**:\nKindly refer to the common response section to find new results.\n\n### **Regarding details about adjusted model architecture**:\nWe do not add any new dropout over the original standard transformer architecture of BERT - we show the critical importance of dropout (which are already present in standard transformer) in stabilizing very deep transformers by preventing rank collapse. The only changes we made were\n\n1) Scaling of residual and skip connection, and\n2) Changing the standard deviation of initialization of weights of model params.\n\n### **Regarding clarity and statistics for Figure 3**:\nTo clarify, Figure 3 shows the variance of gradient across different layers for a given model with 192 layers- it shows the gradient flowing backwards decreases exponentially. We will clarify this in the paper. We provide percentage relative error of our predicted gradient variance compared to the empirical values below:\n\n| Mean Error | Median Error | Std Error |\n| ------------------|---------------------|-----------------------------------|\n|    6.8\\% | 5.2\\% | 7.8\\% |\n\nFurthermore,  we also measure the $R^2$ of our predicted gradient as a goodness of fit measure, and we find and $R^2$ of 0.998 - as can be seen, our predictions provide a good fit. We will add these goodness of fit measures to the main paper.\n\n### **Regarding the assumption of normally distributed inputs, weights, and gradients**:\n**Inputs**: As the embeddings are lookup tables of token-ids, and embedding weights are initialized from normal distribution in xavier, the inputs to the transformer are normally distributed. We will state this clearly in the paper. However, while the inputs are normal, due to the repetition of the tokens / segment ids, the inputs are not IID, as many previous works have assumed - our work handles this explicitly.\n\n**Weights**: Weights are initialized from normal distribution in xavier, and are hence normal.\n\n**Gradients**: As the model outputs are gaussian, the softmax of the classification head results in a Log-Normal distribution for probabilities $p$, as shown in C.7. Since the cross-entropy loss is $-log(p)$, this results in the loss (and hence the final gradient being back-propagated) being log(Log-normal distribution), which is normal distribution.\n\nWe further verify this empirically by checking the normality of the backpropagated gradients to the deepest transformer layer, and the gradients match the best-fit normal distribution with an $R^2$ of $0.999$, showing that the gradients are indeed normally distributed. We will add this discussion on normality to the main paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504400854,
                "cdate": 1700504400854,
                "tmdate": 1700556315180,
                "mdate": 1700556315180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n5TKNZNxNG",
                "forum": "x7cuUZxwFS",
                "replyto": "KsKM7zO3Q8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5624/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for Rebuttal Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe would like to thank you for reviewing our paper. With less than 1 day left, we would appreciate if you could provide feedback regarding whether our response has addressed your concerns. Kindly let us know if there are any other details you would like us to clarify!\n\nBest regards,\nThe Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5624/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670946238,
                "cdate": 1700670946238,
                "tmdate": 1700670946238,
                "mdate": 1700670946238,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]