[
    {
        "title": "ADoPD: A Large-Scale Document Page Decomposition Dataset"
    },
    {
        "review": {
            "id": "BK8gMgOlAe",
            "forum": "x1ptaXpOYa",
            "replyto": "x1ptaXpOYa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1962/Reviewer_ZWQe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1962/Reviewer_ZWQe"
            ],
            "content": {
                "summary": {
                    "value": "### Summary:\nThe paper introduces \"ADoPD,\" a large-scale document page decomposition dataset designed for document understanding. This encompasses tasks such as document entity segmentation, text detection, tagging, and captioning. ADoPD is distinct with its novel document taxonomy, meticulously crafted through a data-driven approach that incorporates both large-scale pretrained models and human expertise. By merging outlier detection with a human-in-the-loop method, the dataset achieves a notable diversity. ADoPD offers deeper insights into document structures and significantly elevates techniques in document processing and analysis.\n\n### Major Contributions:\n1. **Systematic Exploration of Document Taxonomy**: The work presents the first systematic exploration of document taxonomy using a blend of machine learning models and a human-in-the-loop approach.\n  \n2. **Largest and Most Comprehensive Database**: The paper has developed the most extensive and comprehensive database for document image segmentation and object detection.\n\n3. **Thorough Dataset Analysis**: A profound analysis of the proposed dataset is conducted, accompanied by detailed experimental comparisons for entity segmentation and text detection tasks.\n\n4. **Quantitative and Qualitative Results**: The value of the dataset is highlighted through both quantitative and qualitative outcomes.\n\n5. **Advancement in Document Analysis**: The authors express the aspiration for ADoPD to serve as a catalyst in propelling research within the domain of document analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### 1. Originality:\n\n- **Novel Dataset**: The introduction of \"ADoPD\" represents a fresh addition to the domain of document processing. Few datasets offer the same breadth in terms of document page decomposition.\n  \n- **Unique Taxonomy Approach**: The blend of data-driven approaches with human expertise in crafting a document taxonomy is a distinctive contribution. This combination brings together the best of automated and manual categorization.\n\n- **Human-in-the-loop Integration**: Incorporating human feedback in outlier detection for dataset diversity is an inventive methodology. This ensures that the dataset remains robust and versatile.\n\n### 2. Quality:\n\n- **Comprehensive Analysis**: The authors have delved deeply into the analysis of their proposed dataset, providing both qualitative and quantitative evaluations. Such rigorous examination is indicative of the dataset's quality and its potential applicability.\n\n- **Comparison with Existing Datasets**: While the main content delves deeper, the authors seem to be aware of the limitations of existing datasets, positioning ADoPD as a superior alternative. This indicates meticulous research and understanding of the current landscape.\n\n### 3. Clarity:\n\n- **Well-Structured Presentation**: The paper appears to be organized in a logical flow, starting from the introduction of the problem, moving to methodology, followed by results and conclusions.\n\n- **Illustrative Figures**: Based on the portions reviewed, figures like the overview of ADoPD annotations aid in visual comprehension, making the content more digestible.\n\n- **Explicit Problem Statements**: The authors have clearly laid out the challenges and questions they aim to address with their dataset, providing clarity of purpose.\n\n### 4. Significance:\n\n- **Filling a Gap**: With the increasing demand for automated document processing techniques, ADoPD addresses a significant gap in the domain, especially with its emphasis on diversity and comprehensive taxonomy.\n\n- **Potential for Future Research**: The introduction of such a dataset can catalyze further research in document understanding, segmentation, and object detection. It can serve as a foundational resource for subsequent works in the field.\n\n- **Broader Application**: Beyond academic research, the advancements proposed in the paper have potential real-world applications in areas like digital archiving, automated content extraction, and more."
                },
                "weaknesses": {
                    "value": "### 1. **Depth of Comparative Analysis**:\n- **Weakness**: From the segments reviewed, while the paper introduces a new dataset, there seems to be a limited in-depth comparison with existing datasets.\n- **Recommendation**: A deeper comparative analysis highlighting the specific advantages of ADoPD over existing datasets would solidify its significance. Quantitative benchmarks against datasets like DocBank, if not already included in the deeper sections, would be beneficial.\n\n### 2. **Methodological Justifications**:\n- **Weakness**: The choice of the \"human-in-the-loop\" approach for outlier detection is novel, but the paper might not sufficiently justify why this method was chosen over others.\n- **Recommendation**: Delve deeper into the advantages and potential limitations of this method, comparing it with purely automated outlier detection techniques."
                },
                "questions": {
                    "value": "**Dataset Composition**:\n   Could you provide a more detailed breakdown of the types and sources of documents included in the ADoPD dataset? Understanding the diversity in terms of document genres, geographical origins, and linguistic variations would offer more insight into its applicability and robustness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1962/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698303092708,
            "cdate": 1698303092708,
            "tmdate": 1699636127628,
            "mdate": 1699636127628,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RjjuTkcUPH",
                "forum": "x1ptaXpOYa",
                "replyto": "BK8gMgOlAe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZWQe"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the positive feedback and valuable suggestions.\n \n## **Q: Depth of Analysis**\n\nThank you for your suggestion. We've addressed paper writing issues by thoroughly polishing our paper. This includes detailed descriptions of data collection, experimental results (models, data, parameters, and evaluation metrics), and a reorganized experiment section emphasizing key conclusions. We've also introduced additional experiments in the **updated PDF**.\n\n## **Q: Comparative analysis**\n \nThank you for your feedback. Following your suggestions, we conducted comparative testing on two document datasets: DocLayNet (Text Detection) and RVL-CDIP (Document Image Classification).\n\n**[1]** *DocLayNet*\n\nWe investigated three settings: (a) ADoPD fine-tuning with zero-shot testing on DocLayNet. (b) ImageNet-based initialization followed by fine-tuning on DocLayNet. (c) ADoPD fine-tuned model initialization followed by fine-tuning on DocLayNet. The results are presented below (also updated in **Table 4 (b)**).\n\n| Method              | Backbone           | Zero-Shot (ADoPD) | Zero-Shot (ADoPD) | Finetune (ImageNet) | Finetune (ImageNet) | Finetune (ADoPD) | Finetune (ADoPD) |\n|---------------------|--------------------|--------------------:|-----------------------:|-------------------:|-------------------:|------------------------:|------------------------:|\n|                     |                    | **mAP**             | **mAR**                | **mAP**           | **mAR**           | **mAP**               | **mAR**               |\n| Faster R-CNN        | ResNet$_{50}$       | 0.9                 | 58.5                   | 43.0              | 55.8              | 44.5                   | 60.4                   |\n|                     | ResNet$_{101}$      | 1.0                 | 56.6                   | 46.0              | 58.5              | 47.0                   | 60.7                   |\n| Deformable-DETR     | ResNet$_{50}$       | 2.2                 | 80.4                   | 74.7              | 87.2              | 75.4                   | 88.9                   |\n|                     | ResNet$_{101}$      | 2.6                 | 79.0                   | 75.4              | 85.9              | 77.2                   | 88.1                   |\n\n\n**[2]** *RVL-CIDP*\n\nWe compared CLIP vision backbones: (a) Laion pretrained backbone + RVL-CDIP fine-tuned classifier. (b) ADoPD +*Doc2Seq* pretrained CLIP backbone fine-tuned on RVL-CDIP (updated in **Figure 6 (b)** and **Table 6**).\n\n| Model                  | Type        | Letter | Form  | Email | Hw    | Ad    | SR    | SP    | SP    | FF    | NA    | Bgt   | Inv   | Prsnt | Qnr   | Rsm   | Memo  | Avg   |\n|------------------------|-------------|--------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| ViT$_{\\text{G-14}}$     | Supervised  | 86.20  | 76.70 | 94.28 | 93.48 | 91.81 | 71.94 | 91.10 | 89.72 | 94.97 | 83.68 | 81.24 | 87.44 | 78.26 | 84.97 | 92.94 | 85.87 | 86.57 |\n| ViT$_{\\text{G-14}}$+ADoPD | Supervised | 90.87  | 84.48 | 96.98 | 95.34 | 93.76 | 82.39 | 93.51 | 93.00 | 95.61 | 89.81 | 89.62 | 92.85 | 84.29 | 90.23 | 96.45 | 92.62 | 91.38 |\n\nSignificant ADoPD-based improvements were observed across all types, which highlight ADoPD's advantages for finetuning or pretraining.\n \n## **Q: Outlier Detection and Human-in-the-loop**\n\nThank you for your suggestion. Below, we answer your question from two perspectives: outlier detection and human-in-the-loop:\n\n**[1]** *Outlier Detection*\n\nThe document presents a challenge for feature extraction, combining both images and text. A potential drawback may be in feature extraction, particularly in the initial stages relying solely on CLIP trained on natural images. While introducing additional data enhances robustness, this remains an area for improvement. Additional outlier detection methods, such as textual features, will be explored in the Appendix.\n\n**[2]** *Human-in-the-loop*\n\nDuring the data selection, we found that sensitive images, such as those depicting violence, might present within the in-distribution. However, recognizing their undesirability, we deemed them as failure cases. Depending solely on outlier detection to identify such outliers might yield unpredictable results. Therefore, We opted for a human-in-the-loop approach for this process.\n\nWe engaged diverse annotators from various countries and genders to check selected data. Providing only basic keywords without explicit sensitive references, annotators were tasked with removing documents they deemed sensitive. Ultimately, we achieved a drop percentage of 1.77%, and manual checks confirmed the presence of sensitive information in these images."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558887350,
                "cdate": 1700558887350,
                "tmdate": 1700595953892,
                "mdate": 1700595953892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uEKmX7HJnS",
            "forum": "x1ptaXpOYa",
            "replyto": "x1ptaXpOYa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1962/Reviewer_BGf2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1962/Reviewer_BGf2"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new annotated database to enhance automatic document comprehension and, more specifically, document page decomposition. The dataset was annotated multimodally to support 4 tasks: document entity segmentation, text detection, tagging, and captioning. The annotation was done in such a way as to combine the use of large pre-trained models and human expertise. Finally, a great effort has been made to maximize the diversity of document images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper presents a very significant and interesting work on data collection, showing a great diversity. Moreover, the use of numerous pre-trained models such as ViT, CLIP, or BLIP and the fusion of all the information predicted by these models within a single prompt is very interesting and rarely presented (and even used?) in research papers.\n* Even if this is not really shown in the results tables, it is clear that this dataset will certainly help research on documents, particularly thanks to the diversity of the images and the multiple annotations."
                },
                "weaknesses": {
                    "value": "* While the use of 5 pre-trained models to pre-label images is quite innovative, it would also be interesting to indicate the cost of applying these different models to an image (and even to millions of images).\n* I found section 4.1 a little unclear. Although efforts have been made to explain the image selection process as fully as possible, I'm not sure that this methodology can be easily reproduced from these explanations.\n* In the results tables, the authors show how the different models compare once they've been trained on ADOPD. It would have been interesting to compare these models with and without fine-tuning on ADOPD to see the real impact of this dataset.\n* It would have been interesting to see results on other document datasets, starting from models pre-trained on ADOPD vs. on ImageNet for example. This would have shown the real gain of pre-training on a wide variety of document images compared to a large quantity of natural scene images.\n* The metrics used to evaluate the caption performance should probably be a bit more explained in the text."
                },
                "questions": {
                    "value": "* This dataset would be very interesting to explore and obviously to use for pre-training. I think it would also be great to see what generalization capabilities a model trained on this dataset would have, once applied to another use case. This is not described in the paper, but do you plan to make this dataset public? If so, how do you plan to distribute it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1962/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741603380,
            "cdate": 1698741603380,
            "tmdate": 1699636127527,
            "mdate": 1699636127527,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KSRWp5fPw8",
                "forum": "x1ptaXpOYa",
                "replyto": "uEKmX7HJnS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BGf2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback and suggestions.\n \n## **Q:  Cost of applying these different models**\n\nGood question! Our method involves multiple pretrained models. The costs (model and annotation)  can be primarily categorized as follows:\n\n**[1]** *Data Filtering*\n\nCosts in this part include GPU and GPT-4 API calling expenses. In data selection, our challenge was identifying valid document images from the 175M Laion web image pool. We employed models like Document Finding Classifier, Watermark Detection, Language Detection, OCR tools, and CLIP zero-shot tagging. Except for CLIP, other models are lightweight. We utilize GPT-4 ($0.03/1k prompt tokens) for all data during collection and *Doc2Seq* & *Doc2Tag*, with costs limited to selected images.\n\n**[2]** *Data Annotation Cost*\n\nIn our model-assisted data annotation approach, model inference and training occur for each batch of data sent to annotators, incurring relatively high GPU costs. However, this cost is still smaller than human labeling expenses. Additionally, periodic model refinement was observed to accelerate annotation efficiency and reduce costs. Refer to **Fig. 12** (*Appendix, Page 17*) for the annotated cost, displaying a decrease with the increase in data.\n\n## **Q: Unclear Section 4.1 and Reproduce**\n \nThe section has been revised in the updated PDF. In addition, **Sec. 3.3** provides detailed information on data collection and annotation processes, while **Sec. 4.1** covers baseline model implementation, hyperparameters, and evaluation metrics. In **Sec. 4.2**, we analyze experimental results, including new experiments in **Tables 4 & 6**. We'll provide models and annotation guidelines for replication. The revised *Appendix* includes detailed data analyses and taxonomy analysis.\n \n## **Q: With or Without Fine-tuning on ADoPD**\n\nGreat question.  in **Table 4 (a)**, we compared models fine-tuned on EntitySeg and fine-tuned on ADoPD Doc2Mask. The zero-shot performance of EntitySeg-tuned models is lower than ADoPD-tuned ones, given EntitySeg lacks document data. Despite excelling in natural image segmentation according to **Fig. 5 (a)**, EntitySeg-trained models are unfit for documents due to significant composition differences, where layout elements outweigh content significance.\n  \n## **Q: Other Document Datasets and Pre-training**\n\nIn line with your suggestion, we conducted two experiments:\n\n**[1]** *DocLayNet*\n\nIn **Table 4**, ADoPD-trained models achieve good recall on the DocLayNet test set, despite a low mAP. When comparing models fine-tuned on DocLayNet with pre-trained weights from ImageNet and ADoPD, the ADoPD-based models exhibit superior performance, emphasizing their generalization ability. Below, we present some results; for more details, please refer to our revised PDF.\n\n| Method              | Backbone           | Zero-Shot (ADoPD) | Zero-Shot (ADoPD) | Finetune (ImageNet) | Finetune (ImageNet) | Finetune (ADoPD) | Finetune (ADoPD) |\n|---------------------|--------------------|--------------------:|-----------------------:|-------------------:|-------------------:|------------------------:|------------------------:|\n|                     |                    | **mAP**             | **mAR**                | **mAP**           | **mAR**           | **mAP**               | **mAR**               |\n| Faster R-CNN        | ResNet$_{50}$       | 0.9                 | 58.5                   | 43.0              | 55.8              | 44.5                   | 60.4                   |\n|                     | ResNet$_{101}$      | 1.0                 | 56.6                   | 46.0              | 58.5              | 47.0                   | 60.7                   |\n| Deformable-DETR     | ResNet$_{50}$       | 2.2                 | 80.4                   | 74.7              | 87.2              | 75.4                   | 88.9                   |\n|                     | ResNet$_{101}$      | 2.6                 | 79.0                   | 75.4              | 85.9              | 77.2                   | 88.1                   |\n\n**[2]** *RVL-CDIP*\n\nWe also conducted tests on RVL-CDIP, demonstrating a significant improvement when pre-training CLIP on ADoPD data and fine-tuning on RVL-CDIP, a dataset richer in data types than DocLayNet (refer to **Table 6** and **Fig. 6 (b)**).\n\n## **Q: Other issues (Caption Evaluation and Data)**\n \nWe've enhanced caption evaluation in our revised paper. In **Sec. 4.2**, we used 5K test data to assess Doc2Seq's effectiveness. GPT-4 captions achieve commendable CIDEr scores, but a notable disparity persists compared to human annotations. Models fine-tuned on *Doc2Seq* match GPT-4 on B@n but exhibit significantly lower CIDEr scores. Additionally, in **Fig. 6 (left)**, human-annotated captions are notably longer than machine-generated ones, influencing the evaluation results.\n\nWe release ADoPD to support foundational models in document understanding, aiming to inspire progress in document analysis."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557897088,
                "cdate": 1700557897088,
                "tmdate": 1700595918097,
                "mdate": 1700595918097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iFZ7FssnAC",
            "forum": "x1ptaXpOYa",
            "replyto": "x1ptaXpOYa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1962/Reviewer_aArm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1962/Reviewer_aArm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new document page decomposition dataset called ADoPD, mainly focusing on document tagging, segmentation, text detection and caption. Combining automatic procedure like machine/deep learning models and human annotator, this paper creates a much large and high-quality document dataset and still maintains low cost. Additionally, this paper conduct analysis of proposed dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper collects a large amount of data (120k), which has high resolution, wide source, and diverse appearance, the high quality data is beneficial for area. It offers segment, caption annotations which are rare in other document datasets.\n2.\tThe \u201chybrid data annotation\u201d is novel and it helps to ensure the dataset is balanced and diverse. This proposed method alleviates the demand for human annotators and still maintains the high tagging quality for this dataset.\n3.\tThis paper offers quantitative evaluation for the diversity of data distribution, further confirmed the effectiveness of the \u201chybrid data annotation\u201d."
                },
                "weaknesses": {
                    "value": "1.\tThe detection and segmentation annotation quality are not well studied, they all come from pretrained model and corrected by human annotators. It\u2019s better to give more detail about the procedure to correct the annotation. Especially there are some other document datasets like DocBank -- the box annotation is extracted from PDF by automatic tools and it should be much more precise than detection model.\n2.\tAlthough the proposed dataset is claimed to be suitable for caption, but the caption annotation is generated by BLIP-2 and there is no human correction. BLIP-2 is mainly trained for universal visual-language task and it might not suitable for document caption."
                },
                "questions": {
                    "value": "1.\tThe caption annotation is just generated by BLIP-2 and there are not further correction or filter. I think this model-annotation might be meaningless for the area.\n2.\tThe section 4.2.3 Doc2Seq claims that 5000 samples are manually annotated, but after that, the paper says \u201cutilized 80,000 samples for training, 20,000 for testing, and another 20,000 for validation\u201c. It\u2019s confusing how the manually annotation are collected and used, and why it has 20,000 validation and test samples \u2013 not consistent with 5,000 manual annotation.\n3.\tWill the dataset be open-source?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1962/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1962/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1962/Reviewer_aArm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1962/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828150681,
            "cdate": 1698828150681,
            "tmdate": 1699636127439,
            "mdate": 1699636127439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "as4XCsxskb",
                "forum": "x1ptaXpOYa",
                "replyto": "iFZ7FssnAC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aArm"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the encouraging words and helpful comments. We address concerns, questions, and comments below in detail.\n\n## **Q: Mask Annotation Quality and Annotation Procedure**\n\nGreat question. We've updated an explanation of our data annotation process in **Sec. 3.3**. Extracting box annotations from PDF is convenient. However, PDF extraction tools (e.g., *PyMuPDF*) may not fully parse structural information for certain PDFs, such as scanned documents in PDF format. ADoPD aims to avoid bias toward purely digital PDFs, as many documents are in image format.\n\nFor text detection, another reason we chose to have annotators label directly (instead of model-assisted) is that we observed that even when the model is trained with *DocLayNet* or *DocBank*, their prediction results introduce noisy errors and are not robust to real-world document images (especially visually-rich documents). This increases the cost, as annotators need to clean lots of redundant error bounding boxes. Therefore, we did not adopt *DocBank*'s method of parsing digital PDFs but instead used human annotation. Meanwhile, we outline the cost per instance of our manual annotation. The cost of box annotation is low compared to mask annotation. \n\n|  | Add Text Bbox | Modify Mask  |   Add Mask  |\n|----------------------------------|-------------- |-----------------------|--------------------------------|\n| Cost | 0.2*$x$ | 0.7*$x$  | $x$ |\n\nWe can see that masks are the most time-consuming and costly part of the entire process. We present quantitative and visual results in **Fig. 12, 13, 14** (*Appendix, Page 17, 18, 19*), demonstrating that as data annotation increases, the cost decreases when fine-tuning the mask prediction models for assistance. Additionally, we will release our data annotation guidelines.\n \n\n## **Q: Captions and Quality**\n\nWe address this question from two perspectives:\n\n**[1]** *Our caption is not just generated by BLIP-2* \n\nWe apologize for any misunderstandings that may have arisen due to the writing issues in the previous article. We would like to highlight that our captions are not solely based on BLIP2. Instead, they result from *Prompt-Guided Context-Aware Captioning* (updated **Sec. 3.2**). In practice, our captions are rewritten by *GPT-4* (**Eq. 1&2**) based on BLIP-2 captions, zero-shot labels (CLIP + Document taxonomy), OCR results, visual tags (using Recognize Anything Model), as well as rules. Additionally, in **Sec. A.1.1** (*Appendix, Page 14*), we provide a detailed explanation of prompt design and conduct human evaluations to validate the effectiveness of our prompt designs.\n \n**[2]** *Prompt-Guided Context-Aware Captioning Benefits Vision-Language Modeling*\n\nWe attempted to collect human captions but faced challenges in controlling their quality due to variations in annotators' understanding and summarization of documents stemming from diverse educational backgrounds. Hence, we only collected a human-annotated test set (5k) for evaluation, presented in **Fig. 6 (left)** and **Table 5**. We also find that GPT-4 rewritten captions can cover multimodal information of documents and summarize effectively with the combination of human-adjusted prompts and heuristic rules. \n \n## **Q: Doc2Seq Data Collection and Splits**\n\nFor *Doc2seq* data, we utilized prompt-guided context-aware captioning on all ADoPD images, resulting in 80k/20k/20k splits. \n\nFor *Doc2seq* model evaluation, we selectively annotated a subset of document images (5k) with human input. **Table 5** employs human-written document captions for evaluation. Generally, BLIP/BLIP2 zero-shot captions exhibit lower quality compared to models refined with Doc2Seq data.\n\nTo further validate the effectiveness of ADoPD captions, we conducted the experiment in **Table 6** (*Please also refer to the response to Reviewer umJE regarding the validity of captions*.). Specifically, we fine-tuned CLIP based on ADoPD captions, then fine-tuned only CLIP's vision backbone on RVL-CDIP. We observed a significant improvement in CLIP's performance through refinement with ADoPD on RVL-CDIP (see **Fig. 6(b)**). This indicates that rewriting document captions is better than using the noisy captions from Laion.  Detailed analysis in **Sec. 4.2** highlights the importance of caption rewriting for handling noisy data.\n\n## **Q\uff1aWill the dataset be open-source?**\n\nYes, the motivation behind creating ADoPD is to accelerate developments in document understanding. ADoPD represents a small step towards establishing a large-scale, high-quality document image dataset. There are still numerous directions to explore, including the analysis of multilingual documents and sensitive data. We are releasing ADoPD to support the development of future foundational models for document understanding."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555027873,
                "cdate": 1700555027873,
                "tmdate": 1700595835669,
                "mdate": 1700595835669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Co8ZnLXZde",
            "forum": "x1ptaXpOYa",
            "replyto": "x1ptaXpOYa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1962/Reviewer_umJE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1962/Reviewer_umJE"
            ],
            "content": {
                "summary": {
                    "value": "To help the community develop techniques that process documents (like letters, forms, emails, news articles, resumes, and research papers), this work introduces a large dataset for several document understanding tasks. They annotate this data for document classification and for a few \"page-decomposition\" tasks, namely, Doc2Mask, Doc2Box, and Doc2Seq.\n\nThe underlying data is labeled and derived from Laion. However, the authors seem to have some concern about the diversity/balance of the types of data in there and/or the quality of the labels. Annotating this data is hard because of its scale. The authors introduce a rich pipeline to discover document taxonomies. The taxonomy developed in this way helps ensure that the final dataset is diverse and balanced. This pipeline is based on a human in the loop with information derived from several foundation models, namely, an LLM that orchestrates information from an image feature extractor, an OCR model, an image captioning model, an image tagging model, and a vision-language model. \n\nThe authors analyze the document taxonomy and find that it contributes \"to a more comprehensive and balanced dataset\". Then, they evaluate several models on their page-decomposition tasks, namely, Doc2Mask, Doc2Box, and Doc2Seq."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This is clearly a very large and well-managed effort that contributes a resource that the community will benefit from. The authors are thoughtful and thorough in working to ensure the quality of the data.\n\n2. The proposed algorithm for hybrid data annotation is very rich and interesting. I'm not aware of any other successful orchestration of this number of models of various modalities, within a powerful human-in-the-loop iterative k-means clustering. There may be much of independent value in this process, beyond the benchmark itself."
                },
                "weaknesses": {
                    "value": "1. The paper is particularly hard to follow! I cannot overstate this: it affects every section of the paper, from the abstract to the evaluation, so I'm not even 100% sure I understand the work. All sections contain a lot of unnecessary commentary (like comments on novelty or general motivations) that do not contribute a lot of value. At the same time, the background and organization are lacking. Please spend that space on background setup or technical details. For instance, half-way through the introduction, it was not really clear what Document Page Decomposition Dataset means (that's a lot of nouns in a sequence).\n\n2. What is the exact contribution in the paper, compared to the data source in Laion? Were the labels of Doc2Mask, Doc2Box, and Doc2Seq just used as-is from existing data or labeled in this work? It's a significant weakness in my opinion that I'm not sure how to answer this.\n\n3. The evaluation repeatedly claims that the authors \"carry out experiments using various baseline models and parameter configurations, confirming the effectiveness of ADOPD for document domain\". How does evaluating the models confirm the effectiveness of the dataset? It's very plausible in some ways, but could you be more explicit? How much of the data was confirmed to be of high quality and what kind of error analysis was conducted, etc., particularly for any labels (other than taxonomy, which is well-analyzed) generated automatically."
                },
                "questions": {
                    "value": "See weakness 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1962/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1962/Reviewer_umJE",
                        "ICLR.cc/2024/Conference/Submission1962/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1962/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698902481160,
            "cdate": 1698902481160,
            "tmdate": 1700701908313,
            "mdate": 1700701908313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zxrpFttIIk",
                "forum": "x1ptaXpOYa",
                "replyto": "Co8ZnLXZde",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1962/Authors",
                    "ICLR.cc/2024/Conference/Submission1962/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1962/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer umJE"
                    },
                    "comment": {
                        "value": "We thank the reviewer for reviewing our paper and providing valuable suggestions.\n\n## **Q: Background Setup and Technical Details**\n\nThank you for pointing out the paper's writing issues. We've thoroughly polished it in the **updated PDF**, with key improvements highlighted below.\n\n**[1]** *Background Setup*\n\nFollow your suggestions, we added a document page decomposition task introduction at **Sec. 1&3**. To enhance clarity, **Fig. 2** was moved to the **Sec. 1** with refined words. In this adjustment, paragraph 4 aligns with **Fig. 2** to facilitate better understanding.\n\n**[2]** *Technical Details*\n\nWe have improved the writing of technical details.\n\n- **Sec. 3.2**: We describe the models used in taxonomy discovery and improve the connection between **Alg. 1** and the text by including **Fig. 4** for better understanding. We add indices for each step corresponding to **Fig. 4**.\n- **Sec. 3.3**: This new section now includes previously missing data collection details. We have also added detailed collection methods (e.g., prompts, costs, etc.) and data details in the *Appendix*.\n- **Sec. 4.1 & 4.2**: We added a detailed description of the baseline models and evaluation. We supplemented **Table 4** (a, b) and **Table 6**, along with **Fig. 6**, and included two additional analyses for generalization and captions.\n \n## **Q: Contributions**\n\nThe major contributions of this paper can be categorized into two main points: the collected dataset and the proposed methods for dataset collection and evaluation.\n\n**[1]** *Data Contributions*\n\nThe main contribution is our extensive document dataset, detailed in **Table 1**. Notably, *Doc2Mask* and *Doc2Box* are human-annotated, while *Doc2Seq* and *Doc2Tag* involve pre-trained models (e.g., CLIP, GPT-4) with human assistance. *Doc2Mask* is the costliest part (as for costs, please refer to the annotation cost response to Reviewer aArm). Unlike natural images with fixed object sets, understanding and identifying areas to annotate in different document images requires prior comprehension.\n\nWe release the ADoPD to support foundational model development in document understanding, fostering advancements in document analysis.\n\n**[2]** *Method Contributions*\n\nAnother contribution is our proposed data collection pipeline, detailed in **Sec. 3.2** and **Sec. 3.3**. Data-driven document taxonomy discovery is a cornerstone, determining the diversity and distribution of our data. To the best of our knowledge, our paper is pioneering in leveraging large-scale pretrained models for taxonomy discovery and data filtering in the document domain.\n\n\n## **Q: Model Evaluation and Data Analysis**\n\nThank you for your suggestions. We answer this question from two aspects:\n \n**[1]** *Data Effectiveness*\n\nADoPD stands out with fine-grained visual entity masks and text bounding boxes. Our experiments on data effectiveness focus on evaluating mainstream models, backbones (**Table 2&3**), and assessing the generalization ability of models trained on ADoPD.\n\nIn updated **Sec. 4.2**, we study the generalization ability with two new experiments: (a) Training models on ADoPD and testing on DocLayNet, results in **Table 4 (b)**. (b) Testing ADoPD fine-tuned CLIP image encoder on RVL-CDIP, results in **Fig. 6 (a)** and **Table 6**. Below, we show the RVL-CDIP results, highlighting the benefits of our data.\n\n| Model                  | Type        | Letter | Form  | Email | Hw    | Ad    | SR    | SP    | SP    | FF    | NA    | Bgt   | Inv   | Prsnt | Qnr   | Rsm   | Memo  | Avg   |\n|------------------------|-------------|--------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| ViT$_{\\text{G-14}}$     | Supervised  | 86.20  | 76.70 | 94.28 | 93.48 | 91.81 | 71.94 | 91.10 | 89.72 | 94.97 | 83.68 | 81.24 | 87.44 | 78.26 | 84.97 | 92.94 | 85.87 | 86.57 |\n| ViT$_{\\text{G-14}}$+ADoPD | Supervised | 90.87  | 84.48 | 96.98 | 95.34 | 93.76 | 82.39 | 93.51 | 93.00 | 95.61 | 89.81 | 89.62 | 92.85 | 84.29 | 90.23 | 96.45 | 92.62 | 91.38 |\n\nFrom above results and **Table 4,6**, we can see that benefiting from the diverse and fine-grained dense annotations in ADoPD, the trained models exhibit good generalization across various document datasets (DocLayNet and RVL-CDIP). \n\n**[2]** *Data Quality*\n\nDuring data annotation, ensuring data diversity and quality is the original intention of ADoPD. For human-annotated data, we ensure that annotators and reviewers are separate to allow a fair confirmation of the quality and reliability of *Doc2Mask* and *Doc2Box* annotations.\n\nAdditionally, annotators from diverse backgrounds and countries were employed to assess the sensitivity of document images. With basic rules provided, they removed documents deemed sensitive. Data distribution details are in **Fig. 9** (*Appendix, Page 16*).\n\nWe will provide detailed data quality statistical results (with human review) along with the annotation guidelines."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552982155,
                "cdate": 1700552982155,
                "tmdate": 1700595438993,
                "mdate": 1700595438993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mmczeNsvzb",
                "forum": "x1ptaXpOYa",
                "replyto": "zxrpFttIIk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1962/Reviewer_umJE"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1962/Authors",
                    "ICLR.cc/2024/Conference/Submission1962/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1962/Reviewer_umJE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. I have increased my score to 6/10."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1962/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701938066,
                "cdate": 1700701938066,
                "tmdate": 1700701938066,
                "mdate": 1700701938066,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]