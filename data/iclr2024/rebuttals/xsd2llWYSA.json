[
    {
        "title": "GLD: Generative Latent Dynamics for Structured Motion Representation and Learning"
    },
    {
        "review": {
            "id": "DTNYiLg5Wj",
            "forum": "xsd2llWYSA",
            "replyto": "xsd2llWYSA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2281/Reviewer_Sghv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2281/Reviewer_Sghv"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents GLD (Generative Latent Dynamics), a novel self-supervised representation and generation method that captures spatial-temporal relationships in periodic or quasi-periodic motions. GLD improves motion learning by incorporating motion dynamics into a parameterized latent space. The method tracks a wide range of motions, including unseen targets, and adapts to potentially risky targets. Furthermore, the paper presents experimental evidence (on the MIT Humanoid robot) showcasing GLD's effectiveness and long-term learning capabilities in open-ended motion learning tasks.  Additionally, the supplementary experiments demonstrate that GLD possesses long-term learning capabilities, which allow learning agents to strategically progress novel target motions while avoiding unlearnable regions.\n\nThe main contributions of this paper are:\n1. GLD (Generative Latent Dynamics) is a new method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. It uses a novel self-supervised, structured representation and generation approach.\n2. GLD has demonstrated its effectiveness in open-ended motion learning tasks. It has long-term learning capabilities that enable learning agents to strategically advance novel target motions while avoiding unlearnable regions.\n3. An online tracking framework powered by GLD has a fallback mechanism. This enables learning agents to dynamically adapt their tracking strategies and automatically identify and respond to potentially risky targets.\n4. Recognition of spatial-temporal structures creates new possibilities for future motion representation and learning algorithms.\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nAfter reviewed the author's rebuttal, I think the authors have addressed most of my concerns, therefore; I have increased my score."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths of the paper:\n1. Originality: The paper proposes GLD, a method that integrates motion dynamics in a parameterized latent space. It combines periodic autoencoders with generative latent dynamics, showcasing ingenuity in motion representation and learning.\n\n2. Quality: The paper showcases the creativity of periodic autoencoders with generative latent dynamics in motion representation and learning. It presents a well-designed experimental setup, comparing GLD's performance with state-of-the-art methods across various motion datasets, providing strong evidence for its effectiveness.\n\n3. Clarity: The paper is well-structured and presents its ideas in a clear manner. It begins with a thorough introduction that sets the context for the proposed method. The paper explains the underlying concepts and algorithms effectively, making it accessible for both experts and non-experts in the field. The experimental results are presented in an organized way, which helps readers understand the performance of GLD in various scenarios.\n\n4. Significance: The proposed method, GLD, has the potential to significantly improve motion representation and learning. It offers an efficient and effective way to generate structured motion patterns, improving the generalization capabilities of learning algorithms. By addressing challenges with raw motion trajectory data, GLD opens up new possibilities for advancements in motion representation and learning.\n\nIn summary, the paper presents a novel and original approach to motion representation and learning with GLD. It demonstrates its quality, clarity, and significance in the field. The paper's strengths lie in its creative combination of existing ideas, application to a new domain, and addressing limitations of prior results, making it a valuable contribution to the research community."
                },
                "weaknesses": {
                    "value": "1. Limited Data Set: The paper heavily relies on a specific dataset for evaluation, which might not be representative of various motion patterns and scenarios. Hence, the proposed method's performance might not be generalizable to other datasets or real-world applications.\n\n2. Controller's Adaptability: Although the motion learning controller is designed to adapt its tracking strategy dynamically, the paper lacks a thorough analysis of the controller's adaptability in handling various motion patterns and unseen targets (for example, multiple intersecting targets). Further study could help establish the controller's robustness and versatility.\n\n3. Limited Adaptability to Other Domains: The paper discusses motion representation and learning in robotics. However, it may not be easily transferable to other domains, such as human motion analysis, due to differences in motion characteristics.\n\n4. Data Quality: The paper fails to address the possible negative effects of poor-quality data on the proposed GLD method's performance in real-world applications. Poor-quality data could potentially degrade the effectiveness and accuracy of the GLD method in real-world applications."
                },
                "questions": {
                    "value": "1. Can GLD be extended to handle non-periodic motions?\n2. How does GLD perform in long-term learning tasks, and can it adapt to and learn new tasks in open environments?\n3. How can the stability and safety of GLD be ensured when applied in real-world scenarios?\n4. How does the GLD perform with noisy motion trajectories? It would be important to investigate how GLD performs in real-world scenarios where motion data may be corrupted or incomplete.\n5. Can GLD be extended to multi-agent motion learning? How does the computation complexity increase as the number of targets increases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2281/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2281/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2281/Reviewer_Sghv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2281/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698593261610,
            "cdate": 1698593261610,
            "tmdate": 1700655511367,
            "mdate": 1700655511367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PnNzXvBEFP",
                "forum": "xsd2llWYSA",
                "replyto": "DTNYiLg5Wj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Sghv (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your time reviewing our work and your valuable feedback. We have improved our paper based on your concerns, as addressed in the following. Please also check the general response, where we updated the paper with the improvements and presented materials.\n\n**Weakness 1**\n\n> Limited Data Set: The paper heavily relies on a specific dataset for evaluation, which might not be representative of various motion patterns and scenarios. Hence, the proposed method's performance might not be generalizable to other datasets or real-world applications.\n\nWe thank the reviewer for raising concerns about the representativeness of our dataset and the generalizability of GLD. We would like to highlight a few features that our dataset possesses.\n- Dataset Diversity: Our dataset encompasses a wide range of human locomotion types, providing a diverse foundation for evaluation. This includes varied motions such as jogging, running, and striding, ensuring a comprehensive motion spectrum is covered\u200b\u200b.\n- Generalization Capability: GLD's ability to generalize is demonstrated through its successful reconstruction and prediction of unseen motions, such as the diagonal run scenario in Section 5.2. This showcases its effectiveness beyond the specific motions present in the training dataset\u200b\u200b.\n- Adaptability in Real-time Tracking: The system's adaptability is further evidenced in Section 5.3 by its proficiency in real-time tracking of diverse user inputs, including motions like 'spinkick,' which are significantly different from the training data\u200b\u200b.\n- Handling Motion Transitions: GLD effectively handles transitions between different motion types, as shown in our experiments with interpolated movements in Section 5.3, highlighting its capacity to maintain coherence in motion sequences\u200b\u200b.\n\n\nAdditionally, we would like to draw attention to the fact that the GLD training does not assume specific embodiments of input sequences. In fact, any periodic or quasi-periodic signal can be effectively encoded by GLD and converted to quasi-constant frequency domain representations. These signals, or reference motions, are not necessarily physically compatible and thus not necessarily achievable by a specific embodied agent. Therefore, the performance of the controller depends on policy learning, rather than data diversity.\n\n\n**Weakness 2**\n\n> Controller's Adaptability: Although the motion learning controller is designed to adapt its tracking strategy dynamically, the paper lacks a thorough analysis of the controller's adaptability in handling various motion patterns and unseen targets (for example, multiple intersecting targets). Further study could help establish the controller's robustness and versatility.\n\nIn response to the reviewer's comment on the controller's adaptability, we acknowledge the importance of this aspect and direct attention to Section 5.3 of our paper, which details experiments showcasing the controller's adaptability across various motion patterns and transitions. For a more dynamic demonstration, we invite reviewers to view videos on our [project website](https://sites.google.com/view/iclr2024-gld/home), which include scenarios of the controller responding to multiple and unseen motion inputs. \n\nWe acknowledge the need for more challenging input to test the limit of the controller's performance. However, as a design parameter, we would also like to highlight the influence of the fallback threshold $\\epsilon_{GLD}$ in the control robustness and versatility. The design of this threshold determines the conservativeness of the tracking. In the extreme case, the fallback mechanism is either always triggered and thus the controller rejects tracking any input target, or the fallback mechanism is always off and thus the controller attempts to track every target motion. We conducted additional experiments on these cases and presented the result in a video titled \u201cFallback Ablation\u201d on our [project website](https://sites.google.com/view/iclr2024-gld/home)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980456660,
                "cdate": 1699980456660,
                "tmdate": 1699980514889,
                "mdate": 1699980514889,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lxeOrcC6JZ",
                "forum": "xsd2llWYSA",
                "replyto": "DTNYiLg5Wj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Sghv (Part 2)"
                    },
                    "comment": {
                        "value": "**Weakness 3**\n\n> Limited Adaptability to Other Domains: The paper discusses motion representation and learning in robotics. However, it may not be easily transferable to other domains, such as human motion analysis, due to differences in motion characteristics.\n\nWe acknowledge that our current work primarily focuses on motion representation and learning in physics-based character and robotic control. However, it's essential to highlight the inherent versatility of the GLD framework. GLD is fundamentally designed to encode any periodic or quasi-periodic signal, transforming these inputs into quasi-constant frequency domain representations. This capability is not restricted to robotic embodiments or specific motion learning tasks.\n\nThe core strength of GLD lies in its ability to provide a concise and meaningful representation of complex, high-dimensional, long-horizon, non-linear signals. While our demonstrations and experiments are rooted in robotics, the underlying principles of GLD make it a valuable tool for a broader range of applications, including human motion analysis. The architecture and approach of GLD do not inherently limit it to robotic systems; rather, its potential applications can extend to any field requiring efficient and accurate representation of periodic or quasi-periodic signals.\n\nLooking forward, we are optimistic about the adaptation and application of GLD in various other domains. We believe that the framework's ability to distill complex motion dynamics into manageable representations will find relevance and utility in fields such as biomechanics, sports science, and even animation, where understanding and replicating human or character motions are crucial. \n\n**Weakness 4**\n\n> Data Quality: The paper fails to address the possible negative effects of poor-quality data on the proposed GLD method's performance in real-world applications. Poor-quality data could potentially degrade the effectiveness and accuracy of the GLD method in real-world applications.\n\nIn response to the reviewer's concern regarding the impact of poor-quality data, our research has indeed taken into account the variable quality of real-world data. As detailed in Section 5, the human locomotion clips used in our study captured in [1] include elements like noise and aperiodicity. GLD addresses these issues by enforcing a quasi-constant parameterization, which effectively enhances the sequence quality through periodic reconstructions.\n\nThe fundamental design of GLD is to encode periodic or quasi-periodic signals into constant frequency domain representations, a process that is resilient to data quality variances. This ensures that GLD's effectiveness remains robust, regardless of the imperfections in the input data. It's important to note that while the quality of the data may not affect the efficacy of GLD itself, it does have implications for how downstream tasks, such as motion learning, might utilize these data.\n\nFurther addressing the issue of data quality, we specifically explore scenarios in Section A.2.11 of the appendix where certain components of the motion data are unlearnable. Our findings, illustrated in Figure S15, show that through adaptive curriculum learning methods, the learning agent can shift its focus towards targets with more potential for improvement, avoiding areas with limited prospects. This adaptability in handling varied data quality is a testament to GLD's utility and effectiveness in practical, real-world applications.\n\n**Question 1**\n\n> Can GLD be extended to handle non-periodic motions?\n\nPotentially yes. Our framework assumes quasi-constant parameterization for motion sequences that are periodic or quasi-periodic in nature, which are common in human movements. \n\nIn cases where motion sequences do not exhibit periodic characteristics, applying a globally constant set of latent parameterizations can lead to underparameterization of the latent embeddings. This, in turn, may result in less accurate reconstructions of the original motion sequences. We have thoroughly addressed this limitation in Section A.3.1 in the appendix of our paper, with an exemplified non-periodic trajectory. For a more detailed exploration of this issue, we refer you to both Section A.3.1 and Figure S16 in our paper. \n\nHowever, we can propose a potential approach for accommodating such motions within the GLD framework. Essentially, any non-periodic motion can be conceptualized as a periodic sequence, where the period is equivalent to the length of the entire motion. By adopting this perspective, GLD can effectively encode non-periodic motions, provided that the trajectory segment window $H$ is sufficiently extended to encompass the full length of the motion. This adaptation allows GLD to process and represent non-periodic motions by essentially treating them as extended periodic sequences, thereby leveraging its inherent strengths in encoding and analyzing motion data."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980588191,
                "cdate": 1699980588191,
                "tmdate": 1700660694110,
                "mdate": 1700660694110,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2yUd5zWDsA",
                "forum": "xsd2llWYSA",
                "replyto": "DTNYiLg5Wj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Sghv (Part 3)"
                    },
                    "comment": {
                        "value": "**Question 2**\n\n> How does GLD perform in long-term learning tasks, and can it adapt to and learn new tasks in open environments?\n\nYes. GLD's adaptability in long-term learning tasks and open environments is demonstrated through its adaptive curriculum learning approach, as detailed in Sections A.2.10 and A.2.11 of our paper's appendix. By initially training on an offline reference dataset, GLD effectively navigates and expands the motion space during training, generating and adapting to new targets. This is facilitated by an online fitted Gaussian Mixture Model sampling strategy based on Absolute Learning Progress (ALPGMM, [2]), which directs the system's focus towards promising and underexplored motion regions, as shown in Figure S15.\n\nThe method's adaptability is further evidenced by its expansion into previously unexplored motion areas, while maintaining expert performance on mastered motions. This guided exploration significantly increases the motion coverage, demonstrating GLD's ability to adapt to and learn in dynamic and open-ended scenarios. We would direct the reviewer to Sections A.2.10 and A.2.11 for detailed discussion.\n\n**Question 3**\n\n> How can the stability and safety of GLD be ensured when applied in real-world scenarios?\n\nThe fallback mechanism in GLD introduced in Section 4.3.2 is an integral safety feature designed to manage situations where the motion learning controller encounters inputs that are significantly different from its training data. These scenarios potentially lead to poor performance or unsafe actions. The primary function of this mechanism is to intuitively filter out risky motion inputs, providing an essential safeguard against unpredictable behavior that could arise from unanticipated motion inputs. \n\nWe have conducted an additional experiment to demonstrate the utility of the fallback mechanism. This experiment specifically showcases scenarios where the mechanism is disabled, highlighting the contrast in the system's behavior and performance. To provide a clear and accessible illustration of these differences, we have included a video titled \u201cFallback Ablation\u201d on our [project website](https://sites.google.com/view/iclr2024-gld/home). This video visually demonstrates the potential risks and performance issues that arise when the fallback mechanism is not employed, thereby underlining its effectiveness and importance.\n\n\n**Question 4**\n\n> How does the GLD perform with noisy motion trajectories? It would be important to investigate how GLD performs in real-world scenarios where motion data may be corrupted or incomplete.\n\nWe would like to direct the reviewer to our previous response to **Weakness 4** in light of GLD\u2019s performance in dealing with poor-quality motion trajectories.\n\n\n**Question 5**\n\n> Can GLD be extended to multi-agent motion learning? How does the computation complexity increase as the number of targets increases?\n\nGLD is primarily a self-supervised representation learning framework, focusing on the efficient encoding of motion trajectories. It does not inherently encompass agent learning aspects. The application of GLD in the context of motion learning is currently integrated with a reinforcement learning controller that is designed for single-agent scenarios. Extending this framework to multi-agent settings is an intriguing concept; however, it falls outside the scope of our current research. As the number of agents increases, the complexity of the learning environment and the interactions between agents would likely lead to a rise in computational demands. This could involve more sophisticated state representation and decision-making processes to effectively handle the dynamics of multi-agent interactions.\n\nIn summary, while GLD's current implementation is not directly geared toward multi-agent learning, we recognize the importance of this area for future research. Exploring the applicability and scalability in multi-agent settings and its computational implications presents a valuable direction for subsequent work in this field.\n\n\n[1] Peng, X.B., Abbeel, P., Levine, S. and Van de Panne, M., 2018. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG), 37(4), pp.1-14.\n\n[2] Portelas, R., Colas, C., Hofmann, K. and Oudeyer, P.Y., 2020, May. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. In Conference on Robot Learning (pp. 835-853). PMLR."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980634877,
                "cdate": 1699980634877,
                "tmdate": 1700660815263,
                "mdate": 1700660815263,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JabvV79vNk",
                "forum": "xsd2llWYSA",
                "replyto": "2yUd5zWDsA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Reviewer_Sghv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Reviewer_Sghv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their comprehensive rebuttal. I believe most of my concerns have been addressed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655140812,
                "cdate": 1700655140812,
                "tmdate": 1700655140812,
                "mdate": 1700655140812,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FzRMsiXwCo",
            "forum": "xsd2llWYSA",
            "replyto": "xsd2llWYSA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2281/Reviewer_itfQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2281/Reviewer_itfQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, authors tackle the problem of motion representation. Inspired by prior work that takes into account periodicity of motion, the proposed method extends the periodic autoencoder to add a generative capability. Based on the insight that some elements do not change during periodic motions, the paper propose GLD, where subsequent latent representation is trained to be predicted, assuming phase can be advanced incrementally. Using the trained network, the method proposes to train to learn policies that aligns the predicted state and the state of the observation. Then during inference, the authors propose to filter out potentially dangerous or difficult states using a fallback mechanism. They compare the difference between the designated state and the actual predicted states, and if the error is significant, they propose to reject the designated state and fall back to the predicted state. The authors conduct various experiments to demonstrate the effectiveness of the actual latent space, as well as the proposed fallback mechanism."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposal to extend the phase autoencoder mechanism to predict the future state, based on the observation that some elements remain consistent throughout certain periodic motion, is very cleverly devised. The observation is used effectively to predict the upcoming $N$ segments, which can be seen from the prediction experiments conducted in Fig. 5. \n\n- The demonstrations in the supplementary material demonstrates that the fallback mechanism is effective at preventing undesired motion, and maintain the status quo as much as possible.\n\n- The authors applied the proposal to various tasks including motion tracking and motion transition. In both tasks, the proposed prediction method is able to interpolate between motions even when the fallback mechanism is triggered."
                },
                "weaknesses": {
                    "value": "- The authors should make better effort to make the paper self-contained in the main manuscript, and not rely excessively on the supplementary material. There is a severe lack of details in the main manuscript, due to the authors moving them to the supplementary material. For example,\n1. One form of the skill sampler should be included in the main section. \n2. $\\mathcal{U}$ in Fig.2 is unclear from the main section.\n3. Section 5.4 seems unnecessary, as all the content is in the supplementary material.\n\n- The figures seem unorganized, as the readers are asked to refer to figures in a random order, including the supplementary material. The colored lines in Figures 5 and 6 is unclear. There should be some sort of explanation of each element in all the figures.\n\n- Despite the comparison of the actual latent space, the reconstruction accuracy seems to be missing. As I presume that the reconstruction accuracy falls as the prediction segment horizon $N$ increases, the authors should discuss the trade-off in comparison to existing methods. \n\n- The effectiveness of the fallback mechanism must be discussed more quantitatively. The authors only discuss the results in Section 5.3, but there is no concrete evidence that the mechanism worked well. There should be some statistics regarding if the motion prediction actually failed when the fallback mechanism was not introduced. Such objective evidence is lacking for the readers to decide whether it is effective or not."
                },
                "questions": {
                    "value": "- What happens when no fallback strategy is employed? How are the resulting motions look with and without the fallback mechanism? Some comparison would be desirable.\n\n- Was there any drawback from introducing the assumption that some elements are generally constant throughout a sequence? Were there actions that did not present these characteristics?\n\n- Also, the action classes indicated by the colors in Fig.5 seems to be missing, making the evaluation of these latent spaces difficult. What do each color respond to?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "As the method is able to accurately capture and predict periodical motions, it can potentially capture differences among individuals undergoing certain actions such as walking. The authors can mention some of the concerns that may arise from learning from personal motion data."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2281/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2281/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2281/Reviewer_itfQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2281/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662164215,
            "cdate": 1698662164215,
            "tmdate": 1700792814583,
            "mdate": 1700792814583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4tm0cS8ElE",
                "forum": "xsd2llWYSA",
                "replyto": "FzRMsiXwCo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer itfQ (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your time reviewing our work and your valuable feedback. We have improved our paper based on your concerns, as addressed in the following. Please also check the general response, where we updated the paper with the improvements and presented materials.\n\n**Weakness 1, Edited**\n\n> The authors should make better effort to make the paper self-contained in the main manuscript, and not rely excessively on the supplementary material. There is a severe lack of details in the main manuscript, due to the authors moving them to the supplementary material.\n> One form of the skill sampler should be included in the main section.\n> $\\mathcal{U}$ in Fig.2 is unclear from the main section.\n> Section 5.4 seems unnecessary, as all the content is in the supplementary material.\n\nThanks for pointing this out. To address the reviewer\u2019s concerns, we modified Section 4.3.1 to give an example of the skill samplers and refer to the appendix for more variants. We also added the notation of the uniform sampling range $\\mathcal{U}$ accordingly.\n\nIn our initial submission, Sec 5.4, together with Suppl. A.2.10 and A.2.11 primarily served as an in-depth exploration of skill sampler design in motion learning. This extended study, though not affecting the GLD pipeline and conclusion, reinforces our argument for the versatility and applicability of the GLD latent parameterization space.\n\nHowever, as suggested by the reviewer, we realized that presenting this abundant but auxiliary exploration in the appendix may be intensive and distracting. Therefore, we adjusted Sec 5.4 accordingly as an extended discussion to direct interested readers to ablation studies in the appendix while keeping the core innovations of our work in the main text. We also restructured the appendix by removing redundant experiments, discussions and figures to further improve readability. We hope the modified structure could highlight our focus and main contribution in the main text.\n\n\n\n**Weakness 2**\n\n> The figures seem unorganized, as the readers are asked to refer to figures in a random order, including the supplementary material. The colored lines in Figures 5 and 6 is unclear. There should be some sort of explanation of each element in all the figures.\n\nWe thank the reviewer for the constructive feedback regarding the references to appendix figures in the main text. We restructured the figure reference and believe the changes will significantly improve the readability and coherence of our paper.\n\nThanks for the feedback on the clarity of the colored lines in Figures 5 and 6. In these figures, the colored curves represent different states corresponding to various quantities, such as linear and angular velocities ($v$ and $\\omega$), and arm and leg joint positions ($q_{arm}$ and $q_{leg}$). Each curve illustrates the evolution of these quantities over time, aligned with the focus of our paper on periodic motion dynamics.\n\nWe acknowledge that a more detailed explanation of each element within the figures would enhance their clarity. However, we faced the challenge of balancing the depth of explanation with the constraints of space and the need to maintain a focus on the core aspects of our research. Our priority was given to present the periodic nature and the dynamics of the motion trajectories, which we felt were central to the paper\u2019s primary concern. However, we are happy to include the color information as suggested in the final version, provided that more space is available in the main text."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980272654,
                "cdate": 1699980272654,
                "tmdate": 1700660864088,
                "mdate": 1700660864088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bEarpJt3lF",
                "forum": "xsd2llWYSA",
                "replyto": "FzRMsiXwCo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer itfQ (Part 3)"
                    },
                    "comment": {
                        "value": "**Question 2**\n\n> Was there any drawback from introducing the assumption that some elements are generally constant throughout a sequence? Were there actions that did not present these characteristics?\n\nYes. This assumption is indeed a pivotal aspect of our method, and we have carefully considered its potential limitations. As correctly noted, our framework assumes quasi-constant parameterization for motion sequences that are periodic or quasi-periodic in nature, which are common in human movements. However, we acknowledge that this approach may not be ideally suited for non-periodic trajectories.\n\nIn cases where motion sequences do not exhibit periodic characteristics, applying a globally constant set of latent parameterizations can lead to underparameterization of the latent embeddings. This, in turn, may result in less accurate reconstructions of the original motion sequences. We have thoroughly addressed this limitation in Section A.3.1 in the appendix of our paper, with an exemplified non-periodic trajectory. For a more detailed exploration of this issue, we refer you to both Section A.3.1 and Figure S16 in our paper. \n\nHowever, we can propose a potential approach for accommodating such motions within the GLD framework. Essentially, any non-periodic motion can be conceptualized as a periodic sequence, where the period is equivalent to the length of the entire motion. By adopting this perspective, GLD can effectively encode non-periodic motions, provided that the trajectory segment window $H$ is sufficiently extended to encompass the full length of the motion. This adaptation allows GLD to process and represent non-periodic motions by essentially treating them as extended periodic sequences, thereby leveraging its inherent strengths in encoding and analyzing motion data.\n\n\n\n**Question 3**\n\n> Also, the action classes indicated by the colors in Fig.5 seems to be missing, making the evaluation of these latent spaces difficult. What do each color respond to?\n\nThe color assignment of the latent offset in Fig 5 (right) was indicated in the caption in our initial submission. Red corresponds to step in place, yellow corresponds to forward run, and green corresponds to forward stride. To improve the readability as suggested by the reviewer, we added a legend at the top. We hope this helps.\n\n**Ethics Concerns**\n\n> As the method is able to accurately capture and predict periodical motions, it can potentially capture differences among individuals undergoing certain actions such as walking. The authors can mention some of the concerns that may arise from learning from personal motion data.\n\nThanks for pointing out potential ethics concerns our work may involve. We added an ethics statement at the end of the main text before references."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980381893,
                "cdate": 1699980381893,
                "tmdate": 1700660632481,
                "mdate": 1700660632481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6UhGaZI8UD",
                "forum": "xsd2llWYSA",
                "replyto": "FzRMsiXwCo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Reviewer_itfQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Reviewer_itfQ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for addressing the concerns"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to thoroughly address the reviews.\nI think most of my concerns are addressed. However, I still feel Fig. 5 needs further improvement. On the left, does the dark black, blue red lines correspond to the axes on the right indicating overall error? I presumed so (from Reviewer RFvT's comments), but still have difficulties following the figure. As this is the central evidence of the proposal's performance on the main manuscript, it would help with additional details on how to properly interpret the graph."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703137767,
                "cdate": 1700703137767,
                "tmdate": 1700703137767,
                "mdate": 1700703137767,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mY57LR21Br",
            "forum": "xsd2llWYSA",
            "replyto": "xsd2llWYSA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2281/Reviewer_RFvT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2281/Reviewer_RFvT"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a latent dynamics model and control policy to track periodic and quasi-periodic functions. The paper extends a PAE network (Starke 2022), which encodes a motion trajectory into a latent embedding used to generate a set of fourier coefficients. These coefficients are used to build a dynamics model assuming constant frequency, amplitude and offset, but time varying phase. The dynamics model predicts a future latent, which is decoded to produce a resultant motion. Control policies are also trained to generate frequency parameters that result in motion sequences that match some desired trajectory. A fallback mechanism compares the desired trajectory to the generated trajectory, and decides whether this is safe to follow (if both are similar), following a more conservative predicted motion if this is deemed unsafe."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The latent dynamics model proposed seems of value to a broad class of periodic/ quasi periodic motions, and nicely extents the periodic autoencoder architecture (PAE - Starke 2022) to the motion generation use case.\n\nThe proposed approach allows for a natural fallback mechanism (detection of infeasible target motions, and fallback to sensible behaviours that lie within the training set.)\n\nThe proposed model produces expressive motions with a more compact trajectory representation than prior work.\n\nThe paper is very extensive, with a number of interesting ablations and visualisations."
                },
                "weaknesses": {
                    "value": "The motion learning/ control policy part of this work needs a clearer problem formulation. It is unclear what the exact goal is here, and a lot is left to the readers to infer. I gather the goal is to learn to generate a series of motions that track a desired motion sequence, using the motion prediction, but it is unclear to me why you need to generate control parameters to do so, instead of just encoding the target motions directly.  A clearer problem description will help avoid confusion like this. \n\nMuch of the interesting work around control policies is in the appendices\n\nMissing related work:\n\nThis work appears closely related to a phase functioned neural network Holden et al. Phase-Functioned Neural Networks for Character Control, which computes network weights using a cyclic function controlled by phase. The proposed approach uses an autoencoder and has clear differences (PFNN does next state prediction), but the core idea is similar and PFNN also considers aspects like motion blending etc. The paper is mentioned in passing in the introduction, but I would recommend some discussion on this in the related work given the similarity in the core idea. \n\nMinor:\n\nThe term Generative Latent Dynamics is rather general, and not particularly descriptive of the proposed approach.\n\nThe paper is extensive, with a number of detailed appendices. This is good, but I found references to appendix figures in the main text body rather distracting.\n\nAssumption 1. I think it is worth pointing out earlier that since the latent space is learned, this can be enforced. \n\n5. Experiments - These experiments are motivated in terms of real world applicability to real robots, but character animation is not robot control, and this statement should be used with caution."
                },
                "questions": {
                    "value": "Figure 5 is confusing, I assume the right y axis is an error metric? Please label axis and caption to indicate these measures.\n\nFig S7 - this is used to motivate fixing frequency amplitude and bias, could you provide more detail on the motion encoded in this example? It seems that this would be motion dependent."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2281/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755208365,
            "cdate": 1698755208365,
            "tmdate": 1699636160761,
            "mdate": 1699636160761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PqyjlXa9dP",
                "forum": "xsd2llWYSA",
                "replyto": "mY57LR21Br",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RFvT (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your time reviewing our work and your valuable feedback. We have improved our paper based on your concerns, as addressed in the following. Please also check the general response, where we updated the paper with the improvements and presented materials.\n\n**Weakness 1**\n\n> The motion learning/ control policy part of this work needs a clearer problem formulation. It is unclear what the exact goal is here, and a lot is left to the readers to infer. I gather the goal is to learn to generate a series of motions that track a desired motion sequence, using the motion prediction, but it is unclear to me why you need to generate control parameters to do so, instead of just encoding the target motions directly. A clearer problem description will help avoid confusion like this.\n\nWe appreciate the reviewer\u2019s feedback on the motion learning formulation. The primary objective of this second phase is to develop an RL control policy that enables a **physics-based** character or robot to effectively track a series of desired motion sequences generated using the predictive capabilities of our GLD model. In the context of physics-based motion control, direct assignment of state values to the agent is not feasible. Unlike kinematic models where states can be explicitly set, physics-based models include system dynamics and thus must adhere to the laws of physics and environmental interactions. \n\nSpecifically, our control policy outputs desired joint positions $q*$ for the robot, which are further converted to actuator torques and then applied to control the robot\u2019s movements within the simulation environment (detailed in Section A.2.1 in the appendix). This process ensures that the robot\u2019s movements adhere to physical constraints and realistically respond to environmental interactions.\n\nTo clarify this aspect, we have revised Section 4.3 of the manuscript to include a more detailed explanation of the problem space in physics-based motion learning to avoid further ambiguity.\n\n\n**Weakness 2, Edited**\n\n> Much of the interesting work around control policies is in the appendices.\n\nThanks for pointing this out. In our initial submission, Sec 5.4, together with Suppl. A.2.10 and A.2.11 primarily served as an in-depth exploration of skill sampler design in motion learning. This extended study, though not affecting the GLD pipeline and conclusion, reinforces our argument for the versatility and applicability of the GLD latent parameterization space.\n\nHowever, as suggested by the reviewer, we realized that presenting this abundant but auxiliary exploration in the appendix may be intensive and distracting. Therefore, we adjusted Sec 5.4 accordingly as an extended discussion to direct interested readers to ablation studies in the appendix while keeping the core innovations of our work in the main text. We also restructured the appendix by removing redundant experiments, discussions and figures to further improve readability. We hope the modified structure could highlight our focus and main contribution in the main text."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980098381,
                "cdate": 1699980098381,
                "tmdate": 1700660361412,
                "mdate": 1700660361412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WozsKcGDdf",
                "forum": "xsd2llWYSA",
                "replyto": "mY57LR21Br",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RFvT (Part 3)"
                    },
                    "comment": {
                        "value": "**Question 1**\n\n> Figure 5 is confusing, I assume the right y axis is an error metric? Please label axis and caption to indicate these measures.\n\nYes, the label caption ($e$) was a bit off in our initial submission. We updated the figure accordingly. Thanks for pointing it out.\n\n**Question 2**\n\n> Fig S7 - this is used to motivate fixing frequency amplitude and bias, could you provide more detail on the motion encoded in this example? It seems that this would be motion dependent.\n\nThis figure, specifically Fig S7(b), demonstrates the latent parameters of the PAE during a representative forward run motion sequence. The motivation behind showcasing this particular motion is to illustrate the quasi-constant nature of the latent frequency, amplitude, and offset across the motion sequence.\n\nIt's important to clarify that while the specific values of these latent parameters are indeed motion-dependent, the key observation of their relative constancy holds across different motions. This means that for various motion types, the phenomenon of near-constant latent parameters is consistently observed, even though the actual values of frequency, amplitude, and offset may vary from one motion type to another.\n\nTo provide a broader perspective on this aspect, we also refer to Figure S16 in our supplementary material. In this figure, we present a transition between two distinct periodic motions, further illustrating how these latent parameters behave across different motion scenarios. This additional example reinforces our argument that while the specifics of the latent parameters are tailored to each unique motion, their quasi-constant characteristic is a general phenomenon observed in the model\u2019s parameterization."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980168425,
                "cdate": 1699980168425,
                "tmdate": 1700660542698,
                "mdate": 1700660542698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jEvlgDK7An",
                "forum": "xsd2llWYSA",
                "replyto": "WozsKcGDdf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Reviewer_RFvT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Reviewer_RFvT"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive rebuttal and detailed response to my questions. Re weakness 2 and potentially removing the skill sampler - if space allows, it may be useful to just scaffold this with a clearer motivation of the value of a controllable latent space/ problem formulation. I think this section is of value to your claims, so feels a shame to lose it to an appendix."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651307856,
                "cdate": 1700651307856,
                "tmdate": 1700651307856,
                "mdate": 1700651307856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XVWw5YmKCz",
                "forum": "xsd2llWYSA",
                "replyto": "mY57LR21Br",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2281/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RFvT (Additional improvements) and thank you"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the concrete suggestion. Based on this, we made further additional improvements to the paper. Please refer to the **Additional improvements** response under **General response** for the update. We sincerely thank you for reviewing these latest changes."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2281/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661150731,
                "cdate": 1700661150731,
                "tmdate": 1700661180186,
                "mdate": 1700661180186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]