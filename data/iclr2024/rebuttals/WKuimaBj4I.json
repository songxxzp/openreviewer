[
    {
        "title": "Learning Optimal Contracts: How to Exploit Small Action Spaces"
    },
    {
        "review": {
            "id": "JpsNkHQZhB",
            "forum": "WKuimaBj4I",
            "replyto": "WKuimaBj4I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5208/Reviewer_NM4o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5208/Reviewer_NM4o"
            ],
            "content": {
                "summary": {
                    "value": "This paper study the sample complexity (and its implication in online learning) of learning the optimal contract in the principal-agent problem. Specifically, they consider the setting where the principal, who does not know the forecast matrix of the agent, offers the agent a contract, and the agent performs the action that maximizes the agent's own utility, and then the principal observes the outcome as a result of the action performed by the agent but not the action itself. The principal may choose different contracts and repeat the above process to figure out the optimal non-negative and bounded contract that maximizes the principal's utility, and the number of repetitions is the sample complexity of this problem.\n\nThe main result of this paper is an algorithm that has $\\tilde{O}(m^n)$ sample complexity, where $m$ is the number of the outcomes, and $n$ is the number of the actions of the agent. In particular, the sample complexity is polynomial in $m$ when $n$ is a constant.\n\nThe key observation underlying this result is that the polytope $\\mathcal{P}_i$ containing exactly all the contracts to which the best response of the agent is the same action $i\\in[n]$, is defined by $m+n$ inequalities ($m+1$ inequalities that force the contract to be non-negative and bounded, and $n - 1$ inequalities that force the action $i$ to be better than other actions for the agent). Hence, $\\mathcal{P}_i$ has at most $\\binom{m+n}{m}=\\binom{m+n}{n}\\le (m+n)^n$ vertices.\n\nThe algorithm starts with an outer approximation $\\mathcal{P}_i'$ of $\\mathcal{P}_i$ ($\\mathcal{P}_i'$ is initially defined by the $m+1$ inequalities that force the contract to be non-negative and bounded), and as long as there is some vertex $v$ of $\\mathcal{P}_i'$ that is not in $\\mathcal{P}_i$ (in other words, the agent's best response to the contract $v$ is some $j\\neq i$), it can easily find an inequality (using the structure of the principal-agent problem) that approximately corresponds to the constraint ``action $i$ is better than action $j$ for the agent'', and then it adds this inequality to $\\mathcal{P}_i'$. After the algorithm iteratively finds $n-1$ such inequalities, $\\mathcal{P}_i'$ will be approximately same as $\\mathcal{P}_i$. Moreover, to find one such constraint for the current iterate $\\mathcal{P}_i'$, the algorithm only needs to evaluate at most all the contracts corresponding to all the vertices of $\\mathcal{P}_i'$, the number of which is bounded $(m+n)^n$ by the key observation above."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The result is original, and the idea is simple and nice.\n* The writing is good overall."
                },
                "weaknesses": {
                    "value": "I think the biggest weakness of this paper is that it studies the setting without prior distribution of the agent's type.\n* First of all, I feel that the sample complexity problem is not well-motivated in this setting -- If I'm the principal, when an agent comes, I won't try to find my optimal contract merely for this agent by signing the agent multiple times with different contracts and observing the outcomes. I would only use this approach, when there is a large candidate pool, and I do not know the prior distribution of the candidates' skills.\n* Moreover, the previous work of [Zhu et al. 2022] studies the setting with prior distribution of the agent's type. Their open question is also posed in that setting, so I think ``solves an open problem by Zhu et al.'' is an overclaim.\n\nOn a separate note, the space of contract considered in this paper is different from that in [Zhu et al. 2022]. In this paper, the contract is bounded in the sense that the sum of the payments for all outcomes is bounded by some number $B$, but in [Zhu et al. 2022], the contract is bounded in the sense that the payment of each outcome is bounded by some number $B$. That is, even in the narrow setting without prior distribution, the problem studied by this paper is not quite the same as the original one."
                },
                "questions": {
                    "value": "Could you address the comments in the weakness section?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5208/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5208/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5208/Reviewer_NM4o"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698454294137,
            "cdate": 1698454294137,
            "tmdate": 1699636518010,
            "mdate": 1699636518010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eI3rSLV7pD",
                "forum": "WKuimaBj4I",
                "replyto": "JpsNkHQZhB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">First of all, I feel that the sample complexity problem is not well-motivated in this setting -- If I'm the principal, when an agent comes, I won't try to find my optimal contract merely for this agent by signing the agent multiple times with different contracts and observing the outcomes. I would only use this approach, when there is a large candidate pool, and I do not know the prior distribution of the candidates' skills.\n \nWe disagree with the Reviewer on this point, as we think that the sample complexity problem that we study in our paper is well motivated. Indeed, we believe that, even in the case where the principal repeatedly interacts with the same agent, if the principal has no prior knowledge about agent's features, then it is reasonable that the principal would try to learn them by repeatedly signing contracts with the agent. Honestly, we are not able to find a better approach to tackle the problem. At the same time, the problem is arguably interesting and of practical relevance even with a single type.\n   \nMoreover, let us also remark that similar learning problems have also been addressed for the (much simpler) Stackelberg problems, where the leader repeatedly interacts with the same (unknown) follower in order to learn an optimal strategy to commit to (Letchford et al. (2009); Peng et al. (2019)). Followers do not have types in these settings as well.\n    \nAnother way of looking at the sample complexity problem studied in our paper is to imagine a scenario in which the principal has access to some ``simulated model'' of the agent that can be used to learn agent's features and in turn an optimal contract to commit to when the actual interaction with the agent takes place. Thus, we believe that studying learning in principal-agent problems in which the agent has a single type is of interest and well motivated.\n\n> Moreover, the previous work of [Zhu et al. 2022] studies the setting with prior distribution of the agent's type. Their open question is also posed in that setting, so I think ``solves an open problem by Zhu et al.'' is an overclaim.\n\nWe agree with the Reviewer. In the final version of the paper, we will make explicit that our work provides an answer to the open question posed by Zhu et al. (2022) in the specific case in which there is a single agent's types.  Nonetheless, the lower bound by Zhu et al. (2022) holds for the setting with a single type, proving that the setting with arbitrary number of actions is intractable even in single-type instances. This suggests that at least some of the main difficulties of the problem are present in instances with a single type, motivating our study. We believe that our result is a first milestone towards answering the open question by Zhu et al. (2022) in more general multi-type settings. Indeed, when seeking for solutions to open problems, it is reasonable to make the first attempts in settings that are more specific than those in which the question was originally posed. \n\n> On a separate note, the space of contract considered in this paper is different from that in [Zhu et al. 2022]. In this paper, the contract is bounded in the sense that the sum of the payments for all outcomes is bounded by some number B, but in [Zhu et al. 2022], the contract is bounded in the sense that the payment of each outcome is bounded by some number B. That is, even in the narrow setting without prior distribution, the problem studied by this paper is not quite the same as the original one.\n\nLet us remark that, in our paper, the problem of learning an approximately-optimal contract is framed for the contract space defined by the hypercube $[0,B]^m$ with $B \\geq 1$, which is strictly more general that the contract space $[0,1]^m$ considered by Zhu et al. (2022). Thus, our algorithm is guaranteed to return a contract in $[0,B]^m$ (see the Find-Contract sub-procedure). To do so, our algorithm defines other sets of contracts. In particular, we exploit the set of contracts whose $1$-norm is bounded. This is a clever trick to reduce the number of hyperplanes defining the polytope of contracts. We remark that the returned contract lies in $[0,B]^m$ as in Zhu et al. (2022)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700053520595,
                "cdate": 1700053520595,
                "tmdate": 1700053520595,
                "mdate": 1700053520595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sYC1rZJKgY",
                "forum": "WKuimaBj4I",
                "replyto": "eI3rSLV7pD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5208/Reviewer_NM4o"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5208/Reviewer_NM4o"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for clarifying the last point about the contract space."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643760052,
                "cdate": 1700643760052,
                "tmdate": 1700643760052,
                "mdate": 1700643760052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C7uj4GF4cB",
            "forum": "WKuimaBj4I",
            "replyto": "WKuimaBj4I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5208/Reviewer_GCN2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5208/Reviewer_GCN2"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the principal-agent problem where a principal seeks to induce an agent to take a beneficial but unobservable action through contracts. They aim to learn an optimal contract by observing outcomes. The paper proposes an algorithm that can learn an approximately optimal contract within a polynomial number of rounds related to the outcome space size. Specifically, they showed an algorithm with a sample complexity result of O(m^n T^4/5) and also converted it to an explore-then-commit online learning algorithm with sublinear regret."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Learning the optimal principal\u2019s strategy in principal-agent problems when the agent\u2019s type is unknown has become an important problem in those kinds of games with sequential movements. Existing works mainly focused on Stackelberg (security) games. This paper introduced meta-actions to group together the agent\u2019s actions associated with \u201csimilar\u201d distributions over outcomes for the specific contract design problem where the agent\u2019s action is unobservable. Then this paper demonstrates how to utilize this idea to learn the approximately optimal contract. The results seem well-executed and the characterization is interesting."
                },
                "weaknesses": {
                    "value": "One major modeling concern I have is the assumption that the agent will honestly best respond to the principal\u2019s queries, especially if they know that the principal is learning to play against him. This problem is particularly an issue in contract design \u2014 if the principal does not know the agent\u2019s utility and wants to learn to play against the agent, then the agent would have strong incentives to manipulate their responses to mislead the principal to learn some non-optimal contracts and would like to do so to benefit himself. Could you elaborate a bit about the validity of the honest agent best response assumption, and some application domains where this assumption holds?\n\nOn the technical side, I feel like the contribution of this paper might be limited given Peng et al., [2019] and Zhu et al., [2022]. \u201cThe core idea of the algorithm is to progressively build two polytopes Ud and Ld\u201d is highly similar to the algorithm proposed by Peng et al. [2019], except that here the agent\u2019s action is not observable, so the authors replace it with some meta action, which is realized by the Action-Oracle and does not seem to be enough contribution for ICLR. As for the relaxed assumption regarding the constant volume, which seems to be highly dependent on the result Lemma 4 by Zhu et al. [2022], which shows the Continuity of the principal\u2019s utility function."
                },
                "questions": {
                    "value": "Could you provide some application domains where your honest-responding agent behavior holds?     \n    \n   \nHow is your Lemma 8 different from Lemma 4 by Zhu et al. [2022]? I feel like your results can be directly implied from their result about the Continuity of the principal\u2019s utility function."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5208/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5208/Reviewer_GCN2",
                        "ICLR.cc/2024/Conference/Submission5208/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777943948,
            "cdate": 1698777943948,
            "tmdate": 1700675270404,
            "mdate": 1700675270404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zxCrGjsDmf",
                "forum": "WKuimaBj4I",
                "replyto": "C7uj4GF4cB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> One major modeling concern I have is the assumption that the agent will honestly best respond to the principal\u2019s queries, especially if they know that the principal is learning to play against him. This problem is particularly an issue in contract design \u2014 if the principal does not know the agent\u2019s utility and wants to learn to play against the agent, then the agent would have strong incentives to manipulate their responses to mislead the principal to learn some non-optimal contracts and would like to do so to benefit himself. Could you elaborate a bit about the validity of the honest agent best response assumption, and some application domains where this assumption holds?\n\nFirst, let us remark that the assumption that the agent plays a best response after observing the principal's commitment is a standard assumption in the literature on principal-agent problems, even in online settings. See, as an example, the works by Ho et al. (2014); Dutting et al. (2019, 2021, 2022); Alon et al. (2021); Guruganesh et al. (2021);  Castiglioni et al. (2022a;b; 2023); Zhu et al. (2022). We are not aware of any work in which the agent does not play a best response, even if the are plenty of works on online settings (see, e.g., Ho et al. (2014), Cohen et al. (2022), Zhu et al. (2022)). Moreover, such an assumption is made also in the related literature on sample complexity in Stackelberg games (see, e.g., Letchford et al. (2009); Peng et al. (2019)).\n    \nWe agree with the Reviewer that a very smart agent can manipulate their responses. However, this would assume an unreasonably strong knowledge of the principal by the agent and a strong assumption on the behavior of the principal. Similar problems have been studied in simpler settings under strong assumptions (see, e.g., Deng et al. (2019)). Indeed, if the agent lacks knowledge about the principal's reward function and misreports their true best response, then the final contract that the principal commits to may yield lower utility to the agent, compared to the case in which they truthfully respond. Consequently, the agent has no reason to misreport their actual best response if they have no knowledge about the principal's reward function.\n\t\nTo summarize, we agree with the Reviewer that it would be interesting and realistic to study settings in which the agent manipulates the principal. However, the current advancement of the research towards this direction is not mature enough and this problem is unexplored even in much simpler settings.\n    \n[Deng, Yuan, Jon Schneider, and Balasubramanian Sivan. ``Strategizing against no-regret learners.\" Advances in neural information processing systems 32 (2019)]\n\n\n> Could you provide some application domains where your honest-responding agent behavior holds?\n\nWe remark that this assumption is common in all works on repeated principal-agent problems (see, e.g., Ho et al. (2014), Cohen et al. (2022), Zhu et al. (2022)). Indeed, the assumption that the agent plays a best response after observing the principal's commitment is also made in several works that study applications of principal-agent problems. For instance, Ho et al. (2014) study their application to crowdsourcing platforms, while Bastani et al. (2016) study a principal-agent problem with an application in healthcare settings.\n\n> How is your Lemma 8 different from Lemma 4 by Zhu et al. [2022]? I feel like your results can be directly implied from their result about the Continuity of the principal\u2019s utility function.\n\n Lemma 4 by Zhu et al. (2022) is related to the Lipschitz continuity of the principal's utility function along some directions. This result is not applicable in our framework. Indeed, our Lemma 8 relates the principal's utility obtained under approximately-incentive-compatible contracts and incentive-compatible ones. Furthermore, this is only a small component of our proof and not the main result. Indeed, the main contribution is to show that the principal's utility obtained by \"undiscovered\" action is not large with respect to those of \"discovered\" actions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700053083768,
                "cdate": 1700053083768,
                "tmdate": 1700053083768,
                "mdate": 1700053083768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aahNwikVaZ",
                "forum": "WKuimaBj4I",
                "replyto": "C7uj4GF4cB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> On the technical side, I feel like the contribution of this paper might be limited given Peng et al., [2019] and Zhu et al., [2022]. \u201cThe core idea of the algorithm is to progressively build two polytopes Ud and Ld\u201d is highly similar to the algorithm proposed by Peng et al. [2019], except that here the agent\u2019s action is not observable, so the authors replace it with some meta action, which is realized by the Action-Oracle and does not seem to be enough contribution for ICLR. ...\n\n  \nIn the following, we summarize the technical and algorithmic challenges arising in our setting with respect to the one studied by Letchford et al. (2009) and Peng et al. (2019). These challenges mainly arise from the fact that, differently from the Stackelberg case in which one can directly observe the agent's best-response action at the end of each round, in our setting the principal only observes an outcome that is stochastically determined (according to an unknown distribution) as an effect of such an action. This introduce two additional difficulties: we cannot compute the exact separating hyperplanes defining best-response regions and we cannot identify the true set of actions. These difficulties would make the problem impossible to solve in Stackelberg games (e.g., it could be the case that some actions are never discovered). However, we can overcome these difficulties in principal-agent problems by using the fact that the principal's utility enjoys some particular structures that are not present in Stackelberg games. As a result, a direct application of the algorithmic approach by Peng et al. (2019) would fail in our setting. While our approach builds on the same idea of building lower and upper bounds for the agent's best response regions, on a technical perspective our approach is quite different and more involved (as evidence, notice that our solution includes several algorithms and pages of proofs while the algorithm of Peng et al. (2019) can be described and analyzed in a few pages).\n   \n   \n   In the following, we summarize the technical challenges in extending the results of Letchford et al. (2009) and Peng et al. (2019):\n   \n    \n- The Action-Oracle procedure implements several checks that allow to associate the observed empirical distributions to existing meta-actions, discover new meta-actions, or merge existing ones. In particular, the Action-Oracle procedure must adapt and redefine the set of meta-actions to ensure that each meta-action consistently includes agent's actions having a similar distribution over outcomes (see Lemma 1) and limit the number of invocations to Try-Cover (see Lemma 2). We observe that Lemma 1 requires a non-trivial technical effort in order to ensure that the Action-Oracle procedure does not merge empirical distributions that form a \u201cchain\u201d growing arbitrarily in terms of infinity-norm.\n    \n- The Try-Cover procedure requires much effort compared to the the procedure employed by Peng et al. (2019). For instance, the Try-Cover algorithm involves several additional checks to ensure that the procedure terminates appropriately when Action-Oracle updates the set of meta-actions, and the meta-action implemented in the vertex of the lower bound belongs to a suitably-defined set of previously-computed hyperplanes. The need for these additional checks arises from the fact that, in our setting with unobservable actions, the approach proposed by Peng et al. (2019) would not terminate within a finite number of steps. As a result, it becomes challenging to prove that: the final collection of lower bounds is a coverage of the entire space of contracts (Lemma 5), the meta-actions at the vertexes of the final lower bounds are effectively approximate best responses (Lemma 6), and the algorithm terminates in a finite number of steps (Lemma 7).\n    \n- The Try-Cover algorithm relies on a suitably-defined procedure to determine the separating hyperplane between two meta actions (see Find-HS in Appendix B). Such a procedure is made effective thanks to Lemma 9 and Lemma 10. These ensure that the intercepts of the computed hyperplanes are close to the differences between the costs of the two corresponding meta-actions, as defined in Definition 4.\n    \n- In our algorithm, the final lower bounds do not coincide with the actual best-response regions. Consequently, the guarantee that an optimal commitment lies within one of the vertices of the final lower bounds, as it is the case in Peng et al. (2019), is not assured. To address this problem, we introduce two non-trivial lemmas (Lemma 6 and 8) showing that the utility of an optimal contract is close to the one returned by the Discover-and-Cover algorithm. These lemmas require non-trivial effort and rely on the specific properties that are guaranteed by both the Action-Oracle and the Try-Cover procedures. See Appendix C."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700053229471,
                "cdate": 1700053229471,
                "tmdate": 1700053229471,
                "mdate": 1700053229471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "biaux3bjJT",
                "forum": "WKuimaBj4I",
                "replyto": "aahNwikVaZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5208/Reviewer_GCN2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5208/Reviewer_GCN2"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the detailed response from the authors. For the issue of what constitutes sufficient contribution I agree, this is in the eye of the beholder and I am willing to concede this point. I raised my score to a marginal accept."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675223736,
                "cdate": 1700675223736,
                "tmdate": 1700675223736,
                "mdate": 1700675223736,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dzgY2MfPS7",
            "forum": "WKuimaBj4I",
            "replyto": "WKuimaBj4I",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5208/Reviewer_Mt7g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5208/Reviewer_Mt7g"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of learning optimal contracts in a principal-agent setting with focus on the setup where the size of the agent's action space is small."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper improves the previous sample complexity bounds for learning optimal contracts in the case when the number of actions is constant."
                },
                "weaknesses": {
                    "value": "- It seems to me that the approach of this paper seems to be mostly based on the work of [Letchford et al., 2009; Peng et al., 2019], but this is not explicitly mentioned in the paper. This is acceptable to me, but I expect the authors to discuss and summarize the challenges of extending the previous approach to the current setting.\n\nsome minor comments:\n\n- Theorem 1 should rather be an observation or proposition than a theorem, since it is a quite obvious fact in learning optimal contracts, even though it might be mentioned explicitly in previous work.\n\n- Definition 1 should rather be an assumption than a definition. I also do not see any benefit of treating this condition as a high probability event. I think you are essentially restricting the learning problem to a subclass of instances where the approximately optimal contract has bounded payment.\n\n- The current notation is not very readable to me, and I would suggest the authors make some efforts on simplifying them. For example, you could use dot product instead of element-wise product over the outcomes spaces in many places."
                },
                "questions": {
                    "value": "- Do we know any lower bounds for the sample complexity of learning optimal contracts when the action size is small? Do you think it is possible to improve the current method beyond the constant action size case (possibly under other assumptions)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817203196,
            "cdate": 1698817203196,
            "tmdate": 1699636517822,
            "mdate": 1699636517822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "htgKQWDoeR",
                "forum": "WKuimaBj4I",
                "replyto": "dzgY2MfPS7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It seems to me that the approach of this paper seems to be mostly based on the work of [Letchford et al., 2009; Peng et al., 2019], but this is not explicitly mentioned in the paper. This is acceptable to me, but I expect the authors to discuss and summarize the challenges of extending the previous approach to the current setting.\n\n\nIn the following, we summarize the technical and the algorithmic challenges arising in our setting with respect to the one studied by Letchford et al. (2009) and Peng et al. (2019). These challenges mainly arise from the fact that, differently from the Stackelberg case in which one can directly observe the agent's best-response action at the end of each round, in our setting the principal only observes an outcome that is stochastically determined (according to an unknown distribution) as an effect of such an action. This introduce two additional difficulties: we cannot compute the exact separating hyperplanes defining best-response regions and we cannot identify the true set of actions. These difficulties would make the problem impossible to solve in Stackelberg games (e.g., it could be the case that some actions are never discovered). However, we can overcome these difficulties in principal-agent problems by using the fact that the principal's utility enjoys some particular structures that are not present in Stackelberg games. As a result, a direct application of the algorithmic approach by Peng et al. (2019) would fail in our setting. While our approach builds on the same idea of building lower and upper bounds for the agent's best-response regions, on a technical perspective our approach is quite different and more involved (as evidence, notice that our solution includes several algorithms and pages of proofs while the algorithm of Peng et al. (2019) can be described and analyzed in a few pages).\n\n\nIn the following, we summarize the technical challenges in extending the results of Letchford et al. (2009) and Peng et al. (2019):\n\n- The Action-Oracle procedure implements several checks that allow to associate the observed empirical distributions to existing meta-actions, discover new meta-actions, or merge existing ones. In particular, the Action-Oracle procedure must adapt and redefine the set of meta-actions to ensure that each meta-action consistently includes agent's actions having a similar distributions over outcomes (see Lemma 1) and also limit the number of invocations of Try-Cover (see Lemma 2). We observe that Lemma 1 requires a non-trivial technical effort in order to ensure that the Action-Oracle procedure does not merge empirical distributions that form a \u201cchain\u201d growing arbitrarily in terms of infinity-norm.\n\n- The Try-Cover procedure requires much more effort compared to the procedure employed by Peng et al. (2019). For instance, the Try-Cover algorithm involves several additional checks to ensure that the procedure terminates appropriately when Action-Oracle updates the set of meta-actions, and the meta-action implemented in the vertex of the lower bound belongs to a suitably-defined set of previously-computed hyperplanes. The need for these additional checks arises from the fact that, in our setting with unobservable actions, the approach proposed by Peng et al. (2019) would not terminate within a finite number of steps. As a result, it becomes challenging to prove that: the final collection of lower bounds is a coverage of the entire space of contracts (Lemma 5), the meta-actions found in the vertexes of the final lower bounds are effectively approximate best responses (Lemma 6), and the algorithm terminates in a finite number of steps (Lemma 7).\n\n- The Try-Cover algorithm relies on a suitably-defined procedure to determine the separating hyperplane between two meta-actions (see Find-HS in Appendix B). Such a procedure is made effective thanks to Lemma 9 and Lemma 10. These ensure that the intercepts of the computed hyperplanes are close to the differences between the costs of the two corresponding meta actions, as defined in Definition 4.\n\n- In our algorithm, the final lower bounds do not coincide with the actual best-response regions. Consequently, the guarantee that an optimal commitment lies within one of the vertices of the final lower bounds, as it is the case in Peng et al. (2019), is not assured. To address this problem, we introduce two non-trivial lemmas (Lemma 6 and 8) showing that the utility of an optimal contract is close to the one returned by the Discover-and-Cover algorithm. These lemmas require non-trivial effort and rely on the specific properties that are guaranteed by both the Action-Oracle and the Try-Cover procedures. See Appendix C."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052522934,
                "cdate": 1700052522934,
                "tmdate": 1700052522934,
                "mdate": 1700052522934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uA9u43PMnk",
                "forum": "WKuimaBj4I",
                "replyto": "dzgY2MfPS7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Theorem 1 should rather be an observation or proposition than a theorem, since it is a quite obvious fact in learning optimal contracts, even though it might be mentioned explicitly in previous work.\n\nWe agree with the Reviewer that the result in Theorem 1 is somehow expected. Nevertheless, to the best of our knowledge, no previous works have formally proved it. Moreover, the proof is not so trivial to be done formally. That said, we agree with the Reviewer that turning Theorem 1 into a proposition is the right choice.\n\n> Definition 1 should rather be an assumption than a definition. I also do not see any benefit of treating this condition as a high probability event. I think you are essentially restricting the learning problem to a subclass of instances where the approximately optimal contract has bounded payment.\n\nWe believe that the Reviewer is not correctly interpreting Definition 1. Indeed, the definition formalizes the problem solved in our paper, which is the one of learning an approximately-optimal bounded contract, adopting a PAC-learning-inspired perspective. We remark that this problem is meaningful in any instance (also those in which the optimal contract sets payments above the bound).\n Moreover, notice that the `with-high-probability' requirement in Definition 1 is necessary in order to meaningfully define the learning task. This is different from what we believe the Reviewer is considering, i.e., assuming to restrict the attention to problem instances where there exists an approximately-optimal contract with bounded payments. As a result, we think that Definition 1 should remain a definition (and not turning into an assumption). Moreover, let us also remark that our problem (Definition 1) is consistent with what has been studied in previous works (see, e.g., the work by Zhu et al. (2022)).\n\n> The current notation is not very readable to me, and I would suggest the authors make some efforts on simplifying them. For example, you could use dot product instead of element-wise product over the outcomes spaces in many places.\n\nWe thank the Reviewer for the observation. We will take it into consideration in the final version of the paper.\n\n> Do we know any lower bounds for the sample complexity of learning optimal contracts when the action size is small? Do you think it is possible to improve the current method beyond the constant action size case (possibly under other assumptions)?\n\nWhen the number of actions is constant, our algorithm provides a bound that is polynomial in the instance size. We think that this is to be considered as satisfactory and tight (meaning that there exists a trivial lower bound polynomial in the instance size). It is known that without additional assumptions the sample complexity is exponential (Zhu et al., 2022).\n Nevertheless, we believe that this lower bound can be circumvented under some assumptions.\n For instance, assuming that the best-response regions either have a finite constant volume or are empty, we conjecture that it is possible to design an algorithm that depends on the smallest of such volumes and do not require an exponential dependence on the number of agent's actions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052705022,
                "cdate": 1700052705022,
                "tmdate": 1700052705022,
                "mdate": 1700052705022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]