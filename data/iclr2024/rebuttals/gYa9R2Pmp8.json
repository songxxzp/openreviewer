[
    {
        "title": "Jailbreaking Language Models at Scale via Persona Modulation"
    },
    {
        "review": {
            "id": "W09XM46mtN",
            "forum": "gYa9R2Pmp8",
            "replyto": "gYa9R2Pmp8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2149/Reviewer_quGs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2149/Reviewer_quGs"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to generate attack prompts for LLMs. The general idea is to have several stages of generation, where an unsafe category is combined with a persona to generate the attack prompts. The paper found that the method can lead to high attack success rates against several LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. the presented method is intuitive and clearly presented.\n2. the method is tested on several different models to show its effectiveness\n3. the method is also simple to implement, and it addresses an important problem."
                },
                "weaknesses": {
                    "value": "1. the biggest weakness is the evaluation method used in the paper. The authors claim that they use an LLM to evaluate the attack success rate through few-shot prompting. However, it's hard to trust such an automatic evaluation method. The authors should also conduct human evaluation for the method."
                },
                "questions": {
                    "value": "Have you conducted human evaluation? It shouldn't be too hard since everything is in English."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418392237,
            "cdate": 1698418392237,
            "tmdate": 1699636147931,
            "mdate": 1699636147931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mb4QwxJTE7",
                "forum": "gYa9R2Pmp8",
                "replyto": "W09XM46mtN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2149/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2149/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We have written a [general response](https://openreview.net/forum?id=gYa9R2Pmp8&noteId=2gC4CNvHds) that can also help clarify our work.\n\n> human evaluation for the method is required\n\n**We conduct human evaluation of our method**, we are sorry if it was not clear enough. We manually labeled 300 random generations and used them as gold-standard to measure the effectiveness of our automated classifier. Please, see Appendix A for all details on the evaluation. Let us know if you have any further questions, we are happy to elaborate more.\n\nIf this was the main reason for rejecting the paper, we would really appreciate if you could revisit your decision. We are happy to discuss any other concerns you have during the rebuttal."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152276614,
                "cdate": 1700152276614,
                "tmdate": 1700152291307,
                "mdate": 1700152291307,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pJ5SkWICAz",
            "forum": "gYa9R2Pmp8",
            "replyto": "gYa9R2Pmp8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2149/Reviewer_2ytU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2149/Reviewer_2ytU"
            ],
            "content": {
                "summary": {
                    "value": "This work explores jailbreaking LLMs by adding particular persona-related system prompt. Particularly, they design an automated (or semi-automated) way to search the system prompt based on the attacking field. The empirical results show that personalized system prompt could effectively stimulate the LLMs to generate harmful outputs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposal to induce an assertive persona within a Large Language Model (LLM) is well-founded. Consequently, this paper merits increased attention concerning the use of system prompts in real-world applications.\n\n2. The empirical results regarding Persona-modulated Human Response (HR) highlight the vulnerabilities associated with modifying system prompts, which currently remain accessible without restrictions."
                },
                "weaknesses": {
                    "value": "1. It is advisable to incorporate additional baseline comparisons within the empirical results section. For instance, it would be valuable to assess the performance when transitioning from a persona-modulated prompt within the system prompt to one within the user prompt.\n\n2. The methodological exposition, particularly in relation to the automated procedures, would benefit from greater elaboration. For instance, it would be helpful to provide specifics on the prompting template utilized for the automatic generation of persona-modulated prompts by GPT-4.\n\n3. The outcomes stemming from the semi-automated red-teaming pipeline warrant quantification. This quantification is essential for a comprehensive evaluation of the trade-off between efficacy and efficiency in the context of the study."
                },
                "questions": {
                    "value": "1. what is the performance of moving the persona-modulated prompt from system prompt to user prompt?\n2. what is the prompting template for GPT4 to automatically generate the persona-modulated prompts\uff1f\n3. What are the quantified results of semi-automated approach. Are there any illustration about the trade-off between its effectiveness and efficiency?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper is about jailbreaking LLMs, which may raise ethics concerns about the safety of LLMs."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698462491737,
            "cdate": 1698462491737,
            "tmdate": 1699636147853,
            "mdate": 1699636147853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LSfwRK3sem",
                "forum": "gYa9R2Pmp8",
                "replyto": "pJ5SkWICAz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2149/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2149/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We have written a [general response](https://openreview.net/forum?id=gYa9R2Pmp8&noteId=2gC4CNvHds) that can also help clarify some of the issues you raised. We cite and address the main concerns in your review below.\n\n> assess performance when transitioning persona-modulated prompt from the system prompt to user prompt.\n\nWe do not directly ablate this, but **Claude 2 and Vicuna take the persona-modulation prompt in the user prompt** since they do not support system prompts. This indirectly serves as evidence that our method does not strongly rely on the type of prompt used. We think re-running the experiments in GPT-4 user prompt is very costly and would provide little value since our main goal is to show this attack works rather than optimizing its performance. Please, if you think otherwise, let us know and we will be happy to discuss or consider further experiments.\n\n> provide specific prompt templates\n\nWe discussed this in the general response. **To prevent misuse, we are sharing prompts and details with trusted researchers** upon request. We included a \u201cbroader impact\u201d paragraph discussing this decision in the updated PDF.\n\n> The outcomes stemming from the semi-automated red-teaming pipeline warrant quantification.\n\nThanks for bringing this up. This is not easy since quantification may strongly depend on the expertise of the human and how long they iterate on an attack. In our internal experiments, the authors could achieve close to 100% harmful completion rates with modest efforts. We illustrate the tradeoffs between the attack modalities in Figure 5 in Appendix B. It includes some estimates of the time required by each attack modality based on our experience. In Appendix E you can find specific harmful generations for prompts that are used as safety examples in the GPT-4 system card. All in all, we believe this section is meant to be a qualitative contribution rather than a quantitative one. We just want to motivate that issues with automations can be easily fixed by having a human in the loop. Please, let us know if there are any specific metrics you think could help improve this point.\n\n> Ethics concern: Potentially harmful insights, methodologies and applications\n\nThanks for bringing this up. This is a well-known dilemma in security research. It is well-established in computer security that properly disclosing vulnerabilities is better than keeping them private. In general, we think this work does not enable any new dangerous behavior and we kept details private to prevent misuse by a wider audience. **We included a new \u201cBroader impact\u201d paragraph** in the updated version of the paper addressing this point."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152186644,
                "cdate": 1700152186644,
                "tmdate": 1700152186644,
                "mdate": 1700152186644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uly53Nr9Um",
                "forum": "gYa9R2Pmp8",
                "replyto": "LSfwRK3sem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2149/Reviewer_2ytU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2149/Reviewer_2ytU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. It would be appreciated if the authors could answer the following questions.\n\nBased on the Persona-modulated Prompt in Table 1, the author uses [System] and [User] to differentiate the persona-modulated Prompt and the question specific prompt. Will the [system] part (persona-modulated prompt) be placed in the system prompt or the user prompt when doing inference. If it is placed in the user prompt, what is the format to combine the persona-modulated prompt and the specific question prompt?\n\nRegarding the responses of system prompt, the author mentioned that Claude 2 and Vicuna take the persona-modulation prompt in the user prompt since they do not support system prompts. To my knowledge, the API-based models like GPT4 and Claude2 do provide interface for system prompt (https://platform.openai.com/docs/api-reference/chat/create) where the user pass values of \u201crole\u201d to determine what type of prompt it is. The default system prompt is important for human aligned chat models. For opensource models like vicuna, they also have official chat templates (check the chat template of vllm https://github.com/vllm-project/vllm) and it is well-known that chat models are sensitive to system prompts. That\u2019s why I raise the questions about where the \u2018Persona-modulated Prompt\u2019 is placed."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218052020,
                "cdate": 1700218052020,
                "tmdate": 1700218052020,
                "mdate": 1700218052020,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V6umIUEwYx",
            "forum": "gYa9R2Pmp8",
            "replyto": "gYa9R2Pmp8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2149/Reviewer_pyxV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2149/Reviewer_pyxV"
            ],
            "content": {
                "summary": {
                    "value": "This proposes to design a persona modulation as a black-box jailbreak to make the LLMs to take on specific personas. Then it can comply with the harmful instructions and generate responses for harmful topics."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper proposes a persona modulation to enable LLMs to follow harmful instructions and generate corresponding responses."
                },
                "weaknesses": {
                    "value": "1. **No novelty:** It seems this paper only designs a persona prompt. I don't see any novelty at all.\n\n2. **Experimental setting has some issues:** \"The authors of the paper manually labeled 300 random completions.\" Only authors of this paper annotated the completions. It might have some bias. Besides, baseline only has the one without prompt, which seems not enough."
                },
                "questions": {
                    "value": "1. What about steering the system to be not only one personas? How about two personas in consistent or contradictory status?\n\n2. It seems only one third of the categories achieved a harmful completion rate over 50%. Does that mean the proposed approach is not generalizable enough?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper proposed a method to tell LLMs to follow harmful instructions, and the corresponding outputs may be harmful!"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789042941,
            "cdate": 1698789042941,
            "tmdate": 1699636147778,
            "mdate": 1699636147778,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kx6bB8VkrA",
                "forum": "gYa9R2Pmp8",
                "replyto": "V6umIUEwYx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2149/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2149/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We have written a [general response](https://openreview.net/forum?id=gYa9R2Pmp8&noteId=2gC4CNvHds) that can also help clarify some of the issues you raised.\n\n> No novelty\n\nWe have clarified our contributions in the updated version of the paper. We paraphrase them here: \n\n* **Finding successful persona-modulation prompts**: Telling the model to behave like a persona is not enough since they refuse by default since past persona-modulation attacks (like DAN) have been patched. Our first contribution is finding ways to enable persona-modulation again.\n* **Automating prompt generation**: This is one of our main contributions. We show that prompt engineering might no longer be required as LLMs become more capable and can craft jailbreaks at scale. We use GPT-4 to generate all persona-modulation prompts, which has not been done before in the literature. This makes attacks very scalable and cheap.\n* **Finding yet another vulnerability in SOTA safeguards**: Raising awareness on limitations of existing safeguards is important to show why models should not be blindly trusted and motivate better safeguards.\n\n> Bias in manual annotations\n\nThanks for bringing this up. Crowdsourcing is known to have many problems [1,2,3] and authors' labels are generally accepted as gold-standard because authors are better trained for the task. Several authors independently labeled several splits to try to address this potential issue.\n\n> What about steering the system to be not only one personas? How about two personas in consistent or contradictory status?\n\nHaving the LLM respond in two personas is something that we tested and found that it hampers longer free flowing conversations limiting the amount of useful information we can get from the model. \n\n> Does that mean the proposed approach is not generalizable enough?\n\nWe have addressed this in our general response. Most of the failure cases are due to artifacts in the automated pipeline that can be solved with human oversight (semi-automated attack). We included a new paragraph in the discussion adressing this issue. Also, in practice, an attacker could improve performance with approaches like  best-of-n-sampling.\n\n-----\n\n[1] Veselovsky, Veniamin, Manoel Horta Ribeiro, and Robert West. \"Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks.\" arXiv preprint arXiv:2306.07899 (2023).\n\n[2] Peng, Andi, et al. \"What you see is what you get? the impact of representation criteria on human bias in hiring.\" Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Vol. 7. No. 1. 2019.\n\n[3] Michael Chmielewski and Sarah C Kucker. 2020. An mturk crisis? shifts in data quality and the impact on study results. Social Psychological and Personality Science, 11(4):464\u2013473."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152085037,
                "cdate": 1700152085037,
                "tmdate": 1700152085037,
                "mdate": 1700152085037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hbu58XOXYY",
                "forum": "gYa9R2Pmp8",
                "replyto": "RAHCmbZNkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2149/Reviewer_pyxV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2149/Reviewer_pyxV"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the author's effort in providing the responses. After carefully reading the author's rebuttal, I prefer to keep my current scores."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622394367,
                "cdate": 1700622394367,
                "tmdate": 1700622394367,
                "mdate": 1700622394367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "laM8n2Runf",
            "forum": "gYa9R2Pmp8",
            "replyto": "gYa9R2Pmp8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2149/Reviewer_DNY8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2149/Reviewer_DNY8"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates persona modulation as a method to jailbreak language models at scale. It shows persona modulation can automate the creation of prompts that steer models to assume harmful personas, increasing harmful completions. This jailbreak transfers between models and is semi-automated to maximize harm while reducing manual effort."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents a novel and thorough study of persona modulation to jailbreak LLMs\n- Evaluating the attack against many different harm categories and several state-of-the-art models demonstrates the approach can generalize to different scenarios\n- The methodology is clearly explained and the experiments are well-designed overall."
                },
                "weaknesses": {
                    "value": "- Although the attack achieves a very high one-off attack rate, the attack pattern remains fixed and obvious, which can potentially be identified by some adversarial classifier. It is unclear whether the proposed attack would still be effective if the target model is fine-tuned on the attack prompt with safe answers provided.\n- Factors that make some personas/prompts more effective than others are not analyzed.\n- Concrete mitigation strategies are not discussed in detail."
                },
                "questions": {
                    "value": "Was any analysis done on what factors make some personas and prompts more effective than others in the automated workflow?\nWhat are some ways model providers could mitigate this attack specifically going forward? If some kind of mitigation is performed, can you estimate how the attack rate will change?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2149/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820330816,
            "cdate": 1698820330816,
            "tmdate": 1699636147697,
            "mdate": 1699636147697,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uYy2RSI6gf",
                "forum": "gYa9R2Pmp8",
                "replyto": "laM8n2Runf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2149/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2149/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We have written a [general response](https://openreview.net/forum?id=gYa9R2Pmp8&noteId=2gC4CNvHds) that can also help clarify some of the issues you raised.\n\n> The attack pattern remains fixed and obvious\n\nOur intuition is that taking on certain personas is a desirable feature that LLM providers want to have in their models to enable functionalities like the OpenAI Marketplace. Thus, defending against it might not be trivial to solve.\n\nAlso note that every single persona-modulation prompt is different from each other and requires personalization (this is why automation is important). Defending against a wide set of prompts is not trivial.\n\nThe fact that \u201cattacks are fixable\u201d is generally true for the entire jailbreaking literature that focuses on finding current vulnerabilities to inform future safety measures. With this work we want to keep showing new failures modes that prove a more general concern: **fixing specific attacks from the literature does not bring safety by default because new attacks (like ours) can be devised.** \n\nFinally, we circumvent SOTA safety measures in GPT-4 and Claude 2 showing this attack vector is currently not easy to detect nor obvious.\n\n> Factors that make some personas/prompts more effective than others are not analyzed.\n\nThanks for bringing this up. **We included a discussion paragraph** on \u201climitations with generating instructions\u201d. Since we only use API access to models that either comply with our harmful instructions or provide very general evasive responses, it is hard to disentangle what exactly makes a prompt more effective than others. We, however, identified that the issues are often intermediate artifacts in the automated steps. That is why including a human in the loop is effective and we use human oversight to get responses for prompts shown to be fixed in the GPT-4 system card (see Appendix E). \n\n> Concrete mitigation strategies are not discussed in detail.\n\nPlease, see our general response. We believe that **fixing specific attack vectors does not provide a robust solution to the underlying inherent vulnerability in LLMs**. Additionally, it is hard to devise new mitigations since we do not know what measures are already implemented by LLM providers. This is also in line with most recent work on jailbreaking LLMs (e.g. [1]).\n\n-----\n\n[1] Wei, A., Haghtalab, N., & Steinhardt, J. (2023). Jailbroken: How does llm safety training fail?. arXiv preprint arXiv:2307.02483."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2149/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151956940,
                "cdate": 1700151956940,
                "tmdate": 1700152005587,
                "mdate": 1700152005587,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]