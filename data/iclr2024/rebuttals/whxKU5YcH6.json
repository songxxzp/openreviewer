[
    {
        "title": "SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection"
    },
    {
        "review": {
            "id": "OUYSJP1GKV",
            "forum": "whxKU5YcH6",
            "replyto": "whxKU5YcH6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8679/Reviewer_3dEG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8679/Reviewer_3dEG"
            ],
            "content": {
                "summary": {
                    "value": "Drawing upon the observation of prevalent substructure differences between in-distribution (ID) and out-of-distribution (OOD) graphs, this paper introduces SGOOD, a graph-level OOD detection framework. SGOOD enhances OOD graph detection by incorporating mpre substructure information into ID graph representations. It achieves this through the creation of super graphs of substructures, the implementation of a two-level graph encoding pipeline, and the utilization of three graph augmentation techniques for graph representation. Extensive experiments demonstrate the effectiveness of SGOOD in graph-level OOD detection tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper presents a well-structured writing.\n2.\tIt incorporates state-of-the-art graph-level OOD detection algorithms in comparative experiments.\n3.\tThe paper explores an intriguing and relatively unexplored research area, emphasizing the importance of graph-level OOD detection."
                },
                "weaknesses": {
                    "value": "1. The motivation to improve graph-level OOD detection by encoding more substructure information into graph representations is unclear.\n2. The notion that encoding more substructure information into graph representations will enhance graph-level OOD detection faces skepticism. In practice, theoretically more powerful GNNs often under-perform their 1-WL equivalent counterparts across various graph datasets [1]. This is due to the fact that, in cases where node attributes can function as supplements to structural information, nearly all graphs can be differentiated by 1-WL equivalent GNNs. Substructures do not exist in isolation, and are accompanied by a lot of attribute information. Furthermore, these concerns are verified by the results presented in Table 7. Specifically, more powerful GNNs like NGNN and GNN-AK+ fail to outperform 1-WL equivalent GNNs SAG, TopK, and DiffPool in the graph-level OOD detection task. \n3. This paper lacks a clear definition of the graph distribution, and it does not explore the factors contributing to the distribution differences between ID and OOD graphs. It places excessive emphasis on the influence of substructures in graph-level OOD detection while neglecting the discussion of node attributes. Two graphs with identical structures but distinct node features may exhibit entirely different distributions.\n4. The paper does not explicitly delineate the specific contributions of the proposed method, SGOOD, to the graph-level OOD detection task. Given the existence of many  theoretically more powerful GNNs, it remains unclear why SGOOD better than those GNNs in the graph-level OOD detection task. SGOOD appears to resemble a new GNN with powerful expressiveness rather than a specialized GNN that can identify OOD graphs.\n5. Author wrote: \"For augmentations, intuitively, if more information about training ID data is preserved, it is easier to distinguish unseen OOD data. The substructure-preserving graph augmentations are designed to achieve this. \" Please provide further explanation for \u201cmore information\u201d. What we need to do is to embed all the information related to the substructure into the graph representation? In [2], authors proposed that encoding the task-agnostic (e.g., graph classification task-agnostic) information into representations can improve the OOD detection task. \n\n[1] Dwivedi  et al. Benchmarking graph neural networks. arXiv, 2020\n\n[2] Winkens et al. Contrastive training for improved out-of-distribution detection. arXiv, 2020."
                },
                "questions": {
                    "value": "1. Table 1 lacks clarity, making it difficult for readers realize the ID and OOD graphs used in statistics, and these statistical findings rely on prior knowledge.\n2. GNNsafe appears to be primarily designed for node-level OOD detection. How can it be implemented at the graph-level?\n3. OCGIN, OCGTL, and GLocalKD are predominantly designed for graph anomaly detection, and their use as comparison algorithms may not be entirely appropriate for graph-level OOD detection.\n4. Figure 1 does not effectively convey how SGOOD is specifically tailored for the graph-level OOD detection task."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8679/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8679/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8679/Reviewer_3dEG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8679/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698124955089,
            "cdate": 1698124955089,
            "tmdate": 1699637087941,
            "mdate": 1699637087941,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UrY0zZ4jFy",
                "forum": "whxKU5YcH6",
                "replyto": "OUYSJP1GKV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3dEG (1/3) and look forward to your reply"
                    },
                    "comment": {
                        "value": "We thank your effort to review our paper and we appreciate your recognition on the strengths of our paper. Please find our detailed responses below.\n\n\n\n**W1:** The motivation to improve graph-level OOD detection by encoding more substructure information into graph representations is unclear.  \n**Q1:** Table 1 lacks clarity, making it difficult for readers realize the ID and OOD graphs used in statistics, and these statistical findings rely on prior knowledge.  \n**Response:**  \nThe motivation of encoding task-agnostic substructures to improve graph-level OOD detection is empirically justified in Table 1 of the paper. The intuition is that substructure differences commonly exist in real-world ID and OOD graphs, and naturally, if the learned representations can encode substructures, which means better differentiation between ID and OOD graphs, the performance of graph-level OOD detection could be improved.\n\nWe agree that Table 1 itself is not self-contained, due to the lack of space in introduction, and we have revised Section 1 in the latest version uploaded. Specifically, please refer to Table 2 in Section 4 for ID and OOD graph statistics. Table 1 shows the percentage of OOD test graphs containing substructures never appeared in ID training graphs in each dataset. These percentage values are high, higher than 44% in 4/6 datasets, indicating that many OOD graphs contain substructures rarely appear in ID graphs. This observation motivates our design to encode substructures to improve graph-level OOD detection. Note that the only purpose of this statistical finding is to motivate our idea, and it is not used in the training, since OOD graphs are only available at test time.  \n\n\n**W2:** The notion that encoding more substructure information into graph representations will enhance graph-level OOD detection faces skepticism. In practice, theoretically more powerful GNNs often under-perform their 1-WL equivalent counterparts across various graph datasets [1]. This is due to the fact that, in cases where node attributes can function as supplements to structural information, nearly all graphs can be differentiated by 1-WL equivalent GNNs. Substructures do not exist in isolation, and are accompanied by a lot of attribute information. Furthermore, these concerns are verified by the results presented in Table 7. Specifically, more powerful GNNs like NGNN and GNN-AK+ fail to outperform 1-WL equivalent GNNs SAG, TopK, and DiffPool in the graph-level OOD detection task.  \n**W4:** The paper does not explicitly delineate the specific contributions of the proposed method, SGOOD, to the graph-level OOD detection task. Given the existence of many theoretically more powerful GNNs, it remains unclear why SGOOD better than those GNNs in the graph-level OOD detection task. SGOOD appears to resemble a new GNN with powerful expressiveness rather than a specialized GNN that can identify OOD graphs.  \n**Response:**   \nTo answer **W2**, we echo your insightful comment in **W5**: \"In [2], authors proposed that encoding the task-agnostic (e.g., graph classification task-agnostic) information into representations can improve the OOD detection task.\" Existing GNNs like NGNN and GNN-AK+ learns task-specific substructures. As mentioned in Section 1, these methods are trained with a focus on classification-related structures. *On the contrary, our method SGOOD preserves graph classification task-agnostic substructures*, so that the generated representations by SGOOD can better distinguish ID and OOD graphs at test time. The input substructures used in SGOOD are not restricted by ID graph labels, and these substructures are extracted by existing methods as a pre-step. This explains the performance improvements by SGOOD over NGNN and GNN-AK+ in Table 7. In order to make the paper complete, the analysis in Section 3.5 just serves as a complementary justification for SGOOD from the perspective of 1&2-WL test.\n\nThen for **W4**, the main contributions of SGOOD are *how to effectively encode task-agnostic substructures into the generated representations* to detect OOD graphs, different from GNNs like NGNN and GNN-AK+ that are to learn task-specific substructures. To achieve this, as shown in Figure 1, we first design a super graph of substructures, which models the relationships between task-agnostic substructures in a graph, and then generate substructure-enhanced graph representations via two-level graph encoding (Section 3.1); we then design substructure-preserving graph augmentations to enrich training data, while keeping task-agnostic substructures themselves intact (Section 3.2); lastly we design proper objective functions in Section 3.3. These techniques in SGOOD are designed with the purpose to encode task-agonistic substructures into representations for better graph-level OOD detection.\n\nWe have revised Section 1 and 3 accordingly in the latest version of our paper that has been uploaded."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8679/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149280612,
                "cdate": 1700149280612,
                "tmdate": 1700532034564,
                "mdate": 1700532034564,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iO8uoQ1AXQ",
                "forum": "whxKU5YcH6",
                "replyto": "OUYSJP1GKV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer 3dEG,\n\nWe thank your constructive comments, which are solvable. In our responses, we have addressed all your comments and improved our paper. This is a reminder to discuss. Your feedback on our responses is important to us. \n\nSummary of changes:\n- (W1, Q1) We have revised Section 1 to make clearer the motivation of encoding substructures for graph-level OOD detection.\n- (W2, W4, W5) We have revised our paper to highlight that our method SGOOD preserves task-agnostic substructures, and thus better distinguish ID and OOD graphs at test time, compared with existing powerful GNNs, and further explained our technical contributions. \n- (W3) We have clarified that our method does not require or leverage assumptions on graph distributions, and thus is not constrained to a specific underlying graph distribution.  Different types of datasets may have different ID and OOD graph distributions, and SGOOD can versatilely perform superior on all datasets.\n- (Q2, Q3) We have explained how and why to compare with baselines GNNSafe, OCGIN, OCGTL, and GLocalKD.\n- (Q4) As suggested, we have improved Figure 1 and its description.\n\nBest,  \nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8679/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580394620,
                "cdate": 1700580394620,
                "tmdate": 1700580394620,
                "mdate": 1700580394620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WpvnaTFbCb",
                "forum": "whxKU5YcH6",
                "replyto": "iO8uoQ1AXQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8679/Reviewer_3dEG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8679/Reviewer_3dEG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my concerns. The response has clarified part of my concerns. However, upon reviewing the revised manuscript, I noted significant modifications to the introduction. The current version proposes encoding task-agnostic substructures in the ID graph to improve OOD graph detection\u2014a concept absent in the initial manuscript. This introduction of new elements has given rise to fresh uncertainties for me.\n\nSpecifically, I question the appropriateness of defining modularity-based substructures as \"task-agnostic.\" The author asserts that these substructures are task-agnostic due to their independence from specific learning tasks, such as graph classification. \nThe assertion may somewhat inaccurate, given that these structures are closely tied to community detection. For instance, graphs within the same class may exhibit highly similar community structures, especially in social networks. The relevance of modularity-based substructures to the graph classification task appears uncertain and contingent upon the specific dataset used."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8679/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627823722,
                "cdate": 1700627823722,
                "tmdate": 1700627823722,
                "mdate": 1700627823722,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9xYfdzzmQi",
                "forum": "whxKU5YcH6",
                "replyto": "OUYSJP1GKV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Responses to Reviewer 3dEG"
                    },
                    "comment": {
                        "value": "Dear reviewer 3dEG, \n\nPlease find our further clarifications below. Thank you and look forward to your reply.\n\n> Significant changes in Introduction \n\n**Response:** We clarify that the change in Introduction is **just to swap and reorganize the content** in the two paragraphs in blue, which is not significant, if you compare it with the original version. In particular, we adopted your suggestions in W1, Q1, and W5, and reorganized the content to make our motivation clearer. **The change does not affect our technical designs.** We just color the paragraphs for your easy reading.\n\n> The highlight of \u201ctask-agnostic\u201d \n\n**Response:** In the original manuscript, we have discussed in Section 1 that \u201cThese methods are trained using ID graphs and classification loss with a focus on *classification-related structures* of ID graphs.\u201d, which indicates that these existing methods are task-specific, echoing your comment in W5, W2. On the other hand, our method is indeed using task-agnostic substructures, as indicated by \u201cour framework SGOOD is *orthogonal* to existing subgraph detection methods\u201d in Section 3.1.  \nMoreover, following your suggestion, we think that it is better to highlight the term \"task-agnostic\" in the revision, compared with the original version. Note that **this change also does not affect our technical designs**.\n\n> The author asserts that these substructures are task-agnostic due to their independence from specific learning tasks, such as graph classification. The assertion may somewhat inaccurate, given that these structures are closely tied to community detection. \n\n**Response:** Note that (i) the substructures are identified in a *preprocessing* step without knowing the classification task, and (ii) in the training stage, the substructures are already fixed as input, and *we do not modify them based on class labels*, and thus they are task-agnostic. (iii) We agree that some hidden correlation may exist between class labels and community structures. Therefore, in **Table 6**, besides modularity-based community structures, we have tried *different substructures* detected by different methods, which all improve the performance, compared with the base method without substructures. This validates that **our method is not limited to a certain type of relationship between substructures and class labels.**"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8679/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631839762,
                "cdate": 1700631839762,
                "tmdate": 1700642714500,
                "mdate": 1700642714500,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sTJM7NDoDr",
            "forum": "whxKU5YcH6",
            "replyto": "whxKU5YcH6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8679/Reviewer_NoSM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8679/Reviewer_NoSM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes novel graph-level OOD detection framework that generates substructure-enhanced representations and uses substructure-preserving graph augmentations for contrastive training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed SGOOD outperforms a number of existing baselines.\n2. The design of substructure-enhanced representation learning and augmentation is interesting.\n3. The paper is well-organized and clear."
                },
                "weaknesses": {
                    "value": "1. The proposed substructure learning on graphs is related to identifying and learning causally invariant substructures, which has been studied in some previous works [1-3].\n2. As for Substructure-Preserving Graph Augmentations, although it perserves substructures, it might change the semantics of graphs.\n\n\n[1] Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs\n[2] RIGNN: A Rationale Perspective for Semi-supervised Open-world Graph Classification\n[3] Debiasing graph neural networks via learning disentangled causal substructure"
                },
                "questions": {
                    "value": "How can the proposed SGOOD ensure semantically meaningful substrctures extracted by predefined methods? Why not using other learning based techniques like hypergraph learning, graph pooling or causal learning to extract substructures?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8679/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698315354550,
            "cdate": 1698315354550,
            "tmdate": 1699637087832,
            "mdate": 1699637087832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AtCJqkHdSd",
                "forum": "whxKU5YcH6",
                "replyto": "sTJM7NDoDr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NoSM and look forward to your reply"
                    },
                    "comment": {
                        "value": "We thank your effort to review our paper, and appreciate your recognition on (i) the superior performance of our method SGOOD, (ii) the interesting design of our techniques, and (iii) the clarity of the paper. Please find our detailed responses below. \n\n**W1:** The proposed substructure learning on graphs is related to identifying and learning causally invariant substructures, which has been studied in some previous works.  \n**Response:**  \nWe clarify that we do not learn causally invariant substructures. The substructures used in SGOOD are *task-agnostic*, while causally invariant substructures in existing studies are learned with specific tasks. (i) As stated in Section 3.1 (last paragraph on Page 3), our method SGOOD is orthogonal to existing subgraph detection methods, and it is not our focus on how to identify substructures.  (ii) As shown in Figure 1, SGOOD is not relevant to learning causally invariant substructures. We use substructures to enhance the generated representations for graph-level OOD detection. (iii) As shown in Table 6, SGOOD can work with different substructures identified by different methods, and improve performance compared with the base method without substructures.\n\n\n**W2:** As for Substructure-Preserving Graph Augmentations, although it preserves substructures, it might change the semantics of graphs.  \n**Response:**  \nWe highlight that (i) for the purpose of enriching the input graph data, augmentation techniques, including ours, will change the semantics of graphs, but (ii) there is no need to worry about this in SGOOD, since in the training of SGOOD for loss $\\mathcal{L}_{CE}$ in Eq. (5), all original input graphs are used *without* semantic changes. Moreover, as shown in Table 4 of the paper, the proposed augmentation techniques indeed bring performance improvements. \n\n\n**Q1:** How can the proposed SGOOD ensure semantically meaningful substructures extracted by predefined methods?  \n**Response:**  \nWe clarify that it is not our focus on how to extract semantically meaningful substructures, as stated in Section 3.1. The substructures are task-agnostic and extracted as a pre-step. Our main goal is how to encode the substructures into effective representations for graph OOD detection, as illustrated in Figure 1. Our design is motivated by the observation in Table 1 that ID and OOD graphs often have different task-agnostic substructures. As validated in Table 6, SGOOD with different substructures by different methods can always improve performance, compared with the base method without substructures. \n\n**Q2:** Why not using other learning based techniques like hypergraph learning, graph pooling or causal learning to extract substructures?  \n**Response:**  \n(i) In Table 7 of the paper, we have compared with graph pooling methods (SAG, TopK, DiffPool), which is outperformed for graph-level OOD detection. (ii) The focus of SGOOD is to learn expressive representations to encode task-agnostic substructures to distinguish OOD graphs, while it is not our focus on how to extract substructures, as stated in Section 3.1, and it is a different topic from our method. We agree that it is possible to further investigate the impact of learning techniques as future work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8679/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147371560,
                "cdate": 1700147371560,
                "tmdate": 1700147371560,
                "mdate": 1700147371560,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xqzJiJQFrn",
                "forum": "whxKU5YcH6",
                "replyto": "sTJM7NDoDr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer NoSM, \n\nThis is a reminder for discussion. We appreciate your recognition on the superior performance and interesting design of our method.   \nYour comments are insightful and solvable. We have further clarified our technical designs and explained experimental results to address all your comments.\n\nBest,  \nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8679/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622554040,
                "cdate": 1700622554040,
                "tmdate": 1700622554040,
                "mdate": 1700622554040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "znmEhFuAK5",
            "forum": "whxKU5YcH6",
            "replyto": "whxKU5YcH6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8679/Reviewer_AhYj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8679/Reviewer_AhYj"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies out-of-distribution detection on graph data, which is an under-explored research area in GNNs. The authors propose to exploit the substructure information that is invariant between in-distribution and out-of-distribution to endow the model with the OOD detection capabilities. To this end, the authors resort to constructing a super graph of substructures, augmentation for graph data and contrastive loss designs. Experiments with comparison with several SOTA models verify the effectiveness of the model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method seems novel and reasonable\n\n2. The paper is well written and clearly presented\n\n3. The experiment results are strong given the comparison with several SOTA methods"
                },
                "weaknesses": {
                    "value": "1. The proposed method seems incremental and redundant\n\n2. Some of the claims are inproperly stated without justification\n\n3. Theoretical contributions are weak"
                },
                "questions": {
                    "value": "1. How is the model sensitive to different substructures as prior information? And how does this impact different tasks and datasets?\n\n2. How are the negative samples for contrastive loss constructed? How is the sensitivity of the model w.r.t. number of negative samples?\n\n3. The authors mentioned that GNNSafe [1], which is the state-of-the-art model for out-of-distribution detection on graphs, cannot be directly compared, can it be stated more clear why GNNSafe is not comparable with the methods in the experiment?\n\n4. The experimental datasets already used are small. How does the model perform on large datasets? What is the computation cost compared with others?\n\n[1] Qitian Wu et al., Energy-based out-of-distribution detection for graph neural networks. International Conference on Learning Representations, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8679/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736580753,
            "cdate": 1698736580753,
            "tmdate": 1699637087715,
            "mdate": 1699637087715,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oXPi7YZbSO",
                "forum": "whxKU5YcH6",
                "replyto": "znmEhFuAK5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AhYj (1/2),  and look forward to your reply"
                    },
                    "comment": {
                        "value": "**Response to Strengths and Weaknesses:** We appreciate your recognition on (1) the novelty and reasonableness of our method, (2) the clarity of our paper, and (3) the strong experimental results compared with existing methods. We clarify that the main focus of this paper is to develop a new method SGOOD that is effective to handle graph-level OOD detection on various real datasets. The analysis in the paper is to justify the effectiveness of our method, while theoretical contribution is not a major focus of this paper. In terms of the first two weakness points, since they are quite general, we focus on addressing your Questions. Please see our responses below. \n\n\n**Q1:** How is the model sensitive to different substructures as prior information? And how does this impact different tasks and datasets?  \n**Response:**  \nAs stated in Section 3.1, our framework SGOOD is orthogonal to existing subgraph detection methods. The substructures used in SGOOD are task-agnostic. As reported in Table 6 of the paper, compared with the base version without substructures (w.o. substructures), SGOOD can always improve OOD detection performance when adopting different substructures identified by different methods (Modularity, Graclus, LP, BRICS). This demonstrates that SGOOD is not sensitive to specific substructures as prior knowledge. Moreover, it is a natural observation that on different datasets, the performance may vary, but as reported in Table 3 of the paper, SGOOD consistently achieves superior performance under various metrics, including AUROC, AUPR, and FRP95, across 8 real datasets.\n\n**Q2:** How are the negative samples for contrastive loss constructed? How is the sensitivity of the model w.r.t. number of negative samples?  \n**Response:**  \nAs stated in Eq. (6) and the paragraph above Eq. (6) in Section 3.3, given a batch with $B$ training graphs, for each training graph $G_i$ with its super graph $\\mathcal{G}_i$, we first choose two augmentations $\\mathcal{T}_0$ and $\\mathcal{T}_1$ among {I, SD, SG, SS} developed in Section 3.2 based on validation, apply augmentations $\\mathcal{T}_0$ and $\\mathcal{T}_1$ over super graph $\\mathcal{G}_i$ to get $\\hat{\\mathcal{G}} _{i,0}$ and $\\hat{\\mathcal{G}} _{i,1}$ respectively, and then transform graph $G_i$ accordingly to construct $\\hat{G} _{i,0}$ and $\\hat{G} _{i,1}$, which are the two augmented samples generated from $G_i$. Following the established convention in graph contrastive learning [1], pairs of augmented graphs originating from the same graph are treated as positive pairs, while pairs generated from different graphs within the batch are considered negative pairs. In such a way, in a $B$-size batch, for every $G_i$, it will have $2B-2$ negative samples, as shown in the denominator of Eq. (6). \nApparently the number of negative samples is related to batch size $B$. We vary $B$ from 16 to 256 to evaluate sensitivity of SGOOD w.r.t. the number of negative samples, and report the results in Table 1 below. Observe that as increasing from 16 to 128, the overall performance increases and then becomes relatively stable, which proves the effectiveness of the augmentation techniques developed in SGOOD and also validates the superior performance of SGOOD when varying batch size and the number of negative samples. We have added this experiment in Appendix C.\n\n**Table 1.** Varying batch size and the number of negative samples (AUROC)  \n|B|ENZYMES|IMDB-M|IMDB-B|BACE|BBBP|DrugOOD|\n|-|-|-|-|-|-|-|\n|16|73.00|77.81|75.44|75.32|59.84|70.50|\n|32|73.91|77.35|78.11|76.99|60.31|71.20|\n|64|74.52|78.13|78.57|80.91|61.61|71.65|\n|128|74.41|78.84|80.42|84.39|61.25|73.15|\n|256|74.41|77.12|79.55|82.43|62.50|71.11|\n\n**Q3:** The authors mentioned that GNNSafe [2], which is the state-of-the-art model for out-of-distribution detection on graphs, cannot be directly compared, can it be stated more clear why GNNSafe is not comparable with the methods in the experiment?  \n**Response:**  \nWe clarify that (i) GNNSafe is compared in experiments, as reported in Table 3, and (ii) the compared GNNSafe is a modified version, since GNNSafe itself is for node-level OOD detection, but not for graph level. Specifically, in Section 3.2 of the GNNSafe paper [2], in a single graph, GNNSafe propagates node energy scores that are obtained by training GNN on *node labels*, which however are not available in our graph-level setting where only *graph-level labels* are available. Therefore, we compared with a modified version of GNNSafe without such node energy score propagation in Table 3 of our paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8679/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148984085,
                "cdate": 1700148984085,
                "tmdate": 1700148984085,
                "mdate": 1700148984085,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pwPstjqRI5",
                "forum": "whxKU5YcH6",
                "replyto": "znmEhFuAK5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8679/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer AhYj,\n\nThis is a reminder for discussion. We appreciate your recognition of the novelty and reasonableness of our method, the strong experimental results, and the clarity of the paper.   \nYour insightful comments are solvable. We have added new experiments and provided clarifications and justifications, to address all your comments.\n\nBest,  \nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8679/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622427428,
                "cdate": 1700622427428,
                "tmdate": 1700622427428,
                "mdate": 1700622427428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]