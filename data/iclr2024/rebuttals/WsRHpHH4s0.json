[
    {
        "title": "Harnessing Overlap in Blockwise Transformers for Near-Infinite Context"
    },
    {
        "review": {
            "id": "EPkKuj04LW",
            "forum": "WsRHpHH4s0",
            "replyto": "WsRHpHH4s0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8983/Reviewer_Z7SR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8983/Reviewer_Z7SR"
            ],
            "content": {
                "summary": {
                    "value": "Using large context lengths poses memory challenges since it is needed to maintain the key value vectors of earlier tokens. This paper proposes to distribute these vectors across multiple nodes allowing linearly scaling the context length with number of nodes. To address the challenge that computing the attention for a token requires access to all previous key value vectors, the authors propose a scheme to move the vectors across the node. Namely, they suggest forming a ring of nodes and passing the vector to the next node in the ring continuously until all vectors pass through all nodes. Moreover, the authors point out that the communication can be overlapped with the computation, thus making the overhead negligible. The paper shows the method can be practically applied through various experiments, namely fine-tuning LLaMA with 500k token context length."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method effectively allows scaling the context length with the number of nodes. The proposal to overlap communication and computation could allow hiding the communication cost. It can also be implemented in a fairly straightforward manner. The authors show Ring Attention's practicality through experiments. It is very nice to see experiments done both on GPUs and TPUs. The paper is written clearly and can be followed reasonably easily."
                },
                "weaknesses": {
                    "value": "While the paper contains experiments showcasing that ring attention can be applied in practice, a comparison with other methods is missing. For example, if applicable, is it better to apply pipeline parallelism or ring attention? Other similar methods also include DeepSpeed Inference [1] which suggests to offload the kv cache to cpu and similarly overlap communication and computation and FlexGen [2]. Additionally, in light of these existing work (some of which are also not referenced), the novelty of the work might be limited.\n\nFurthermore, an ablation study is missing. For example, how important is it to overlap communication and computation?\n\nI would also like to point out that while the method allows scaling the context length by increasing the number of nodes, it does not address challenges of handling long contexts by Transformers (e.g. [3]). As such, I find comparison with Claude and GPT3.5 in Figure 3 a bit misleading. In particular, we do not expect this method to improve the performance of the model on small context lengths (though a small increase is observed possibly due to more fine-tuning). \n\n[1] Aminabadi, Reza Yazdani, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase et al. \"DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale.\" In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-15. IEEE, 2022.\n\n[2] Sheng, Ying, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. \"FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU.\" (2023).\n\n[3] Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. \"Lost in the middle: How language models use long contexts.\" arXiv preprint arXiv:2307.03172 (2023).\n\nSome minor suggestions:\n* Table 1 is not table but a figure. \n* The last paragraph of section 2 is an almost replica of the introduction (this is not a problem on its own. I am merely pointing this out in case the authors would like to make better use of the space)"
                },
                "questions": {
                    "value": "(Please also see the weakness section.)\n\n1. How is ring attention applied while doing auto regressive decoding (in the stage of decoding tokens one by one)?\n\n2. How does the training time scale with number of nodes? For example, what is the rate of slowdown observed when training a model with 2k context length on two nodes in comparison with k context length on one node (expected 4x) and in comparison with using normal attention to train with 2k context length on a single node?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8983/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8983/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8983/Reviewer_Z7SR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8983/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698628732170,
            "cdate": 1698628732170,
            "tmdate": 1699637130726,
            "mdate": 1699637130726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JOfVZczW4g",
                "forum": "WsRHpHH4s0",
                "replyto": "EPkKuj04LW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8983/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8983/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Z7SR"
                    },
                    "comment": {
                        "value": "Dear reviewer Z7SR, \n\nThank you so much for your review and for highlighting the effectiveness of our method, the comprehensiveness of our experiments, and the clear, easily-followable writing of our paper. We appreciate your feedback and suggestions, which we have incorporated into the revised version to improve it.\n\n\n**#1: Relation to DeepSpeed-inference and FlexGen**\n\nThere are some key differences: \n\n1. CPU memory and the CPU-GPU interconnect are much slower than the GPU-GPU interconnect Ring Attention utilizes. Thus, using CPU offloading would not work for large context training and inference. \n\n2. Ring Attention allows training and inference with a context size that scales linearly with device count, while prior CPU offloading is only applicable to inference. Ring Attention does not add communication overheads while prior CPU offloading addes large communication costs. \n\n3. DeepSpeed-inference and FlexGen require storing the entire full sequence on each device, while Ring Attention distributes a long sequence to a ring of devices, and never needs to store full sequence on any device. This allows Ring Attention to process device count times longer sequences. \n\n4. Due to limitations in physical space and CPU memory, offloading to the CPU is insufficient for managing long sequence lengths. The small CPU-accelerator bandwidth necessitates more stringent requirements for hiding communication in CPU offloading, reducing its flexibility and applicability. In contrast, Ring Attention, which is widely applicable on GPU and TPU, requires only a fast interconnect between nearby devices, and keeps all data fast memory only while CPU offloading saves data to slower CPU memory. This minimal requirement enables Ring Attention to be scalable to any number of devices. Unlike CPU offloading that necessitates storing the full sequence on each device, Ring Attention distributes the sequence across multiple devices.\n\n\n**#2: While the paper contains experiments showcasing that ring attention can be applied in practice, a comparison with other methods is missing. For example, if applicable, is it better to apply pipeline parallelism or ring attention?**\n\nOther parallelism methods, like data, pipeline, tensor parallelism, and FSDP, necessitate storing entire sequences on each device, rendering them incapable of processing large context sizes. Pipeline parallelism also leads to device idle time due to non-overlapped communication. Sequence parallelism distributes the sequence across devices but requires extensive communication that cannot be overlapped, making it infeasible for large context sizes.  \n\nWe have also included a comparison with Deep Speed Ulysses, which was published on Arxiv three days prior to the ICLR submission deadline. Deep Speed Ulysses combines sequence parallelism and tensor parallelism for their optimized NVSwitch all-to-all topology. It can reduce the communication cost of sequence parallelism. Importantly, it requires gathering and storing full sequence on each device, while Ring Attention distributes long sequences to a ring of devices and never needs to keep full sequence on any device. Ring Attention outperforms it substantially both in maximum context size and training efficiency. Please refer to our response to reviewer haGA for the full results.\n\nRing Attention is the first work that distributes transformers computation on long sequences across devices and enables fully overlapped communication with computation. Ring Attention is orthogonal and fully compatible with existing large-scale training parallelism methods. \n\nWe demonstrate this by applying Ring Attention to FSDP end-to-end large-scale training of 7B-65B LLM on 32-1024 TPUv4, covering various compute budgets. Ring Attention facilitates the effortless training of very large context sizes without incurring overheads or making approximations. For example, on 1024 TPUv4, Ring Attention enables training with context sizes over millions in tokens, which is 512 times larger than prior state-of-the-art memory efficient / flash attention, without adding communication nor computation overheads."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504567760,
                "cdate": 1700504567760,
                "tmdate": 1700504567760,
                "mdate": 1700504567760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4bWNdZC4OX",
            "forum": "WsRHpHH4s0",
            "replyto": "WsRHpHH4s0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8983/Reviewer_rUcK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8983/Reviewer_rUcK"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a distributed computing strategy for distributing self-attention computation to across multiple devices without introducing latency on host communication. The authors observed that when distributing blockwise computation of self-attention in multiple devices, the order of which block is computed first does not change the output. So, they proposed a ring style communication. Each device will take care of self-attention output of a subset of queries, and it only receives a subset of keys and values from the next device at each time, which reduces the communication cost. Also, by overlapping the computation of queries\u2019 attention to the current subset of keys and values and the communication of the next subset of keys and values, this host communication does not introduce additional latency overhead."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea is simple and effective. \n\n2. The proposed distributed self-attention computation allows bypassing the hardware limitation of single device. \n\n3. The ring style communication allows the communication requirement stays constant when scaling to more devices. \n\n4. The overlapping of computation and communication hides the communication overhead."
                },
                "weaknesses": {
                    "value": "1. There is only one toy experiment comparing the performance of RingAttention with other efficient attention baselines. The proposed method enables full self-attention for longer sequences, so it would be interesting to see the performance potential of full self-attention on the real world datasets. \n\n2. It would be better to include the results on network bandwidth utilizations in Table 3.\n\n3. In causal attention (used in most LLMs), the compute cost for different queries are different. Queries at the begin of sequence require less compute. In the proposed method, since each device takes care of a subset of queries, the compute load on different devices are different. Will this method make some devices underutilized?"
                },
                "questions": {
                    "value": "1. I am wondering if the same idea could be adopted to distribute linear projections in Attn and FFN computations as well"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8983/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647785493,
            "cdate": 1698647785493,
            "tmdate": 1699637130617,
            "mdate": 1699637130617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YS1Sm71oIh",
                "forum": "WsRHpHH4s0",
                "replyto": "4bWNdZC4OX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8983/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8983/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer rUcK"
                    },
                    "comment": {
                        "value": "Dear Reviewer rUcK,\n\n\nThank you so much for the positive review and for highlighting that our work is simple and effective. We appreciate your feedback and suggestions, which we have incorporated into the revised version to improve it.\n\n\n**#1: The proposed method enables full self-attention for longer sequences, so it would be interesting to see the performance potential of full self-attention on the real world datasets.**\n\nThank you for the suggestion. We have conducted additional experiments on more tasks. We applied Ring Attention to train agentic transformers on hundreds of trajectories to achieve better in-context RL. The results in Table 5 show that a large context window significantly improves performance of learning decision making policy from large scale experience. Large context allows agentic transformers to encode many more trajectories at the same time, so the hindsight relabeling is more informative and makes transformers learn a policy more effectively. \n\n\n**#2: It would be better to include the results on network bandwidth utilizations in Table 3.**\n\nThanks for suggesting to report network utilization rate in addition to maximum context sizes in end-to-end large-scale training, which can provide more information about the training. \nWe have included the measurement of network utilization rates in Table 5. We measure the bandwidth utilization using \\texttt{jax.profiler} and, based on the average measurements from 3 runs. The results show that Ring Attention has significantly higher bandwidth utilization rates. This is attributed to more communication from larger context sizes.\n\n\n**#3: Balancing compute load on different devices for causal attention (used in most LLMs)**\n\nYou are right that in causal attention mode, our method will make some devices underutilized. This is a great, insightful suggestion to Ring Attention. By skipping upper triangular blocks computation and balancing the computation load between devices when using causal attention, the compute cost can be reduced and potentially leading to a twofold increase in speed. We are definitely interested in this and planning to research this direction in future work. \n\n\n**#4: Whether the same idea could be adopted to distribute other computations as well.**\n\nWe focus on self-attention in this work, since it contributes the largest activation size in today\u2019s state-of-the-art AI. We believe the idea of Ring Attention can be applied to any all-to-all ops and even mixture of experts or split up a FFN computation or some aggregation. \n\n\nPlease let us know if our response resolves your concerns. We look forward to hearing from you. Thank you so much."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504489647,
                "cdate": 1700504489647,
                "tmdate": 1700504489647,
                "mdate": 1700504489647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OFPtXBT579",
            "forum": "WsRHpHH4s0",
            "replyto": "WsRHpHH4s0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8983/Reviewer_dMuy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8983/Reviewer_dMuy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes ring attention, which is a way of implementing softmax dense attention.  The attention operation is  \ndistributed over multiple GPUs, and computed in parallel along the sequence dimension.  Attention is also computed in a blockwise fashion, without ever instantiating the full attention matrix, which reduces memory requirements. Together, these two techniques allow dense attention to be scaled to very long sequences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well-written, and it cites the appropriate literature.  The technique described in this paper does, in fact, allow transformers to be trained with very long sequences."
                },
                "weaknesses": {
                    "value": "Unfortunately, although the paper is technically sound, it does not really contribute anything new.  The ideas presented\nin this paper will already be obvious to most practitioners.  As a result, I do not feel that it is appropriate for publication at a conference. \n\nThe fact that transformers process all elements of a sequence in parallel, (at least during training) was the whole inspiration for the original 2017 paper that introduced the transformer architecture.  Thus, the idea of distributing the computation in parallel along sequence length is most definitely nothing new.  This paper merely distributes the computation over multiple devices.\n\nA naive implementation of attention instantiates the entire attention matrix, which is too large to fit in memory for long sequences.  The blockwise computation of dense attention avoids such instantiation, and it does involve some subtle implementation details wrt. to the handling of softmax.  However, it is also not a contribution of this paper; the authors cite the appropriate prior work. \n\nThe remaining piece of the puzzle is to overlap the matrix multiplications for each block with the communication overhead of transferring blocks between different hosts.  However, this is also nothing new; tiled matrix multiplication, distributed over multiple GPUs, is implemented by every major machine learning library today (e.g. Jax), and existing implementations overlap matmuls and communication.\n\nThus, the authors merely observe that it is possible to implement long-range attention using a simple combination of well-understood and previously published techniques.  This fact should be obvious to most practitioners, and IMO, does not rise to the level of a conference paper.  I would encourage the authors to publish this work in a workshop or on arXiv."
                },
                "questions": {
                    "value": "The authors claim that ring attention over very long sequences is suitable for both training and inference.  However, the cost of attention in both cases is still O(N^2).  For training, this is less of an issue; all elements of the (long) sequence are processed in parallel, so the sequence length is essentially just a batch dimension, and the number of tokens per training step is not excessive for an LLM.  \n\nHowever, for inference, tokens are processed autoregressively, so very long sequence lengths will lead to very slow generation times, and thus very high latency when serving the model.  The authors do not discuss or even mention this problem.  How does the latency of ring attention over, e.g., 8M tokens during inference compare to a more conventional short-context model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8983/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8983/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8983/Reviewer_dMuy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8983/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782677781,
            "cdate": 1698782677781,
            "tmdate": 1699637130484,
            "mdate": 1699637130484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wYaI5JJTH4",
                "forum": "WsRHpHH4s0",
                "replyto": "OFPtXBT579",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8983/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8983/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer dMuy"
                    },
                    "comment": {
                        "value": "Dear reviewer dMuy,\n\nThank you so much for the review. We believe that there are misunderstandings. We appreciate your feedback and suggestions, which we have incorporated into the revised version to improve it.\n\n\n**#1: This is also nothing new; tiled matrix multiplication, distributed over multiple GPUs, is implemented by every major machine learning library today (e.g. Jax), and existing implementations overlap matmuls and communication.**\n\nWe respectfully disagree with the reviewer\u2019s criticism regarding novelty. To the best of our knowledge, Ring Attention is the first work that proposes distributing blockwise transformers in a ring topology and overlaps communication with computation. It is also the first to enable training transformers on extremely long sequences without incurring extra computation or communication costs. Ring Attention allows the context size to scale linearly with the device count, addressing the longstanding challenge of freeing transformers from memory and context limitations. For example, Ring Attention enables training LLaMa with a context size of more than millions of tokens on 1,024 TPUv4, which is 512 times larger in context size compared to the previous state-of-the-art blockwise transformers and memory-efficient/flash attention. In contrast, today\u2019s state-of-the-art commercial models have much more limited context sizes, such as GPT-4-turbo has only 128K. \n\nWe believe that an effective, novel, and intuitive idea should be encouraged rather than discouraged.\n\nWe would like to point out that attention is not tiled matrix multiplication since it involves softmax, that is why the ML community has published memory efficient attention which computes attention cumulatively to allow long sequence and flash attention which optimizes the speed while allowing long sequence. However, these works require storing full sequences on each device, making it impossible for processing long sequences such as books, videos, and trajectories, among others. \n\nPast published work in the ML community has suggested less scalable solutions, such as sequence parallelism, tensor parallelism, and fully sharded data parallelism. These methods are less scalable because they require storing full sequences on each device (as in tensor parallelism and fully sharded data parallelism) or incur large communication costs that cannot be overlapped (as in sequence parallelism). Ring Attention, however, is an innovative approach that distributes long sequences across a ring of devices and never needs to store the full sequence on any single device. It is the first work that enables linear scaling up of context size with the number of devices for extremely long sequences without incurring extra computation or communication costs.\n\nA second important contribution of our paper is the first set of experiments with transformers trained to this massive context length, showing good performance on several ML tasks. We demonstrate that Ring Attention enables us to train Vicuna chatbot to 512K context size with just 32 GPUs, while prior state-of-the-art memory-efficient/flash attention-based methods can only handle 16K context size. This is a 32-fold increase in context window size compared to the previous long context Vicuna. The model's performance is competitive with GPT-4 and Claude-2 in long-context line retrieval tasks, and it can process sequences over five times longer than those models. We also applied Ring Attention to scale agentic transformers across hundreds of hindsight-relabeled trajectories. This approach led to substantially improved in-context RL performance, achieving state-of-the-art results in in-context RL tasks.\n\nApart from proposing the idea of Ring Attention, this work provides an open-source implementation, which is plug-and-play and easy to use. \n\nIn sum, Ring Attention is the first work proposing the distribution of blockwise transformers in a ring topology and overlaps communication with computation. This approach addresses the long-standing memory challenge of transformers without making approximations, allowing the context size to scale linearly with device count without additional computation or communication costs.\n\n\n**#2: The authors claim that ring attention over very long sequences is suitable for both training and inference. However, the cost of attention in both cases is still O(N^2).**\n\nSince we do not make approximations to attention, the compute cost is indeed still O(N^2). Our aim is to make it possible to fit long sequences into transformers, i.e., addressing the memory challenge. Combining Ring Attention with approaches that reduce compute costs, such as sparse attention, is a promising direction, but our work makes even precise attention efficiently scalable to sequence lengths in the millions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504401405,
                "cdate": 1700504401405,
                "tmdate": 1700504446905,
                "mdate": 1700504446905,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oat63YmtMv",
                "forum": "WsRHpHH4s0",
                "replyto": "wYaI5JJTH4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8983/Reviewer_dMuy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8983/Reviewer_dMuy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.  Unfortunately, I must stand by my original assessment.  Sharding along sequence length is not particularly different from sharding along any other dimension wrt. to doing a matrix multiply.   As I mentioned in my original review, dealing with softmax does require a mathematical trick to avoid instantiating the full attention matrix, but that trick is not a contribution of this paper.  The ring topology of TPUs is used in the same way by existing parallel matrix multiplication libraries.\n\nBTW, I do not mean to disparage the software engineering effort required to implement Ring Attention in any way.   If you were to release it as an open-source library, I believe it would be a valuable contribution to the industry.  I also appreciate your observation that there is not necessarily any need to switch to fancy new attention mechanisms, when conventional attention can be implemented at long range, simply by sharding the computation along a different dimension."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515311761,
                "cdate": 1700515311761,
                "tmdate": 1700515311761,
                "mdate": 1700515311761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zZGbCRRlar",
            "forum": "WsRHpHH4s0",
            "replyto": "WsRHpHH4s0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8983/Reviewer_haGA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8983/Reviewer_haGA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an efficient distributed computation of full attention mechanism of long sequences across the sequence dimension. By leveraging the blockwise computation of full attention, the proposed Ring Attention distributes long sequences across multiple devices while overlapping the communication of key-value blocks with the computation of blockwise attention. Importantly, Ring Attention enables us to store the output of each layer across multiple devices, significantly reduces the memory demand in Transformer for long sequences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The challenge the paper aims to address is well-motivated and the proposed method is well-introduced. The experimental results are strong."
                },
                "weaknesses": {
                    "value": "There are concerns/questions about the proposed Ring Attention:\n\n1. If I understand correctly, the Ring Attention is based on the assumption that the blockwise computation of self-attention on different devices is well-balanced. However, in the causal mode of self-attention, which is widely used in auto-regressive LLMs, this assumption does NOT hold. It is because under the causal mode, only the lower triangular blocks are necessarily to compute, yielding around half of FLOPs of the non-causal mode with full attention. But in Ring Attention, even under causal mode, we still need to compute all the blocks in the full attention matrix. When the sequence is super long, this may require significantly more FLOPs? \n\n2. For the results in Table 3, Ring Attention are not compared with other sequence parallel mechanism, such as Deep Speed Ulysses which leverages all-to-all communication to speed up sequence parallelism."
                },
                "questions": {
                    "value": "NA."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8983/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698967118029,
            "cdate": 1698967118029,
            "tmdate": 1699637130370,
            "mdate": 1699637130370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HIY0tae4Gz",
                "forum": "WsRHpHH4s0",
                "replyto": "zZGbCRRlar",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8983/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8983/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer haGA"
                    },
                    "comment": {
                        "value": "Dear Reviewer haGA,\n\nThank you so much for the positive review and for highlighting that our paper is well-motivated, the proposed method is well-introduced, and for acknowledging our strong experimental results. We appreciate your feedback and suggestions, which we have incorporated into the revised version to improve it.\n\n\n**#1: Comparison with Deep Speed Ulysses.**\n\nWe appreciate the pointer to Deep Speed Ulysses which appeared on Arxiv 3 days before the ICLR deadline. \n\nThe idea of Deep Speed Ulysses involves combining sequence parallelism and tensor parallelism to leverage their optimized all-to-all NVSwitch-based topology. First, it splits along the sequence dimension with sequence parallelism. Then, before computing attention, it aggregates query, key, and value using all-to-all to ensure that each host has the complete sequence; each host processes one attention head for tensor parallelism of attention scores. Finally, it uses all-to-all again to collect results along attention heads and reshards along the sequence dimension. This method helps in reducing the communication cost of sequence parallelism.\n\nImportantly, Deep Speed Ulysses requires gathering and storing the entire sequence on each device, which becomes an issue for large context windows. In contrast, Ring Attention distributes sequences across devices and communicates key-value pairs in a ring that no device ever holds the entire sequence. Therefore, Ring Attention can scale context size linearly with device count, whereas Deep Speed Ulysses cannot.\n\nDeep Speed Ulysses is great for leveraging all-to-all topology but all-to-all is difficult to scale to a large number of devices. For instance, for H100, the maximum number of GPUs supported with nvlink all-to-all interconnect is limited by network switch. In contrast, TPU torus design can maintain a fast interconnect arbitrarily, such as on thousands of TPUs with optical circuit switching. Since Ring Attention requires a minimal ring topology, it works great on both GPU and TPU, and scales particularly well to large TPU pods. For instance, Ring Attention allows a >10M context size for LLaMa LLM with TPU 1024, which is 512 times longer than the prior state-of-the-art in memory efficiency/flash attention.\n\nWe conducted a comparison in end-to-end training of LLaMa (7B, 13B, 34B) using 512x A100 80GB. We compared the maximum context size achieved by Deep Speed Ulysses and Ring Attention, and measured their model flops utilization (MFU). The table below shows the results. Ring Attention not only achieves 64 times longer context size thanks to distributing the sequence across devices but also attains much higher MFU due to overlapping communication in a ring of devices. It\u2019s worth noting that self-attention has lower MFU than feedforward network, and larger context can lower MFU. Even so, Ring Attention outperforms the baseline, showing its effectiveness.\n\n\n| | Context size (x1e3) | | MFU(%) | |\n\n|---------|---------------------|-------|-------|----|\n\n| | Ulysses | RingAttention | Ulysses | RingAttention |\n\n| 7B | 256 | 16384 | 38 | 42 |\n\n| 13B | 128 | 8192 | 40 | 44 |\n\n| 34B | 64 | 4096 | 41 | 45 |\n\n\n\n**#2: Saving compute in casual mode.**\n\nThis is a great, insightful suggestion for Ring Attention. By skipping the computation of upper triangular blocks and balancing the computation load between devices when using causal attention, the compute cost can be reduced, potentially leading to an increase in speed. We are definitely interested in this and plan to research this direction in future work. \n\n\nPlease let us know if our response resolves your concerns. We look forward to hearing from you. Thank you so much."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504374491,
                "cdate": 1700504374491,
                "tmdate": 1700504374491,
                "mdate": 1700504374491,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]