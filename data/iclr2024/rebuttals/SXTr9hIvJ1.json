[
    {
        "title": "Reweighted Solutions for Weighted Low Rank Approximation"
    },
    {
        "review": {
            "id": "9Fc8jUrzpM",
            "forum": "SXTr9hIvJ1",
            "replyto": "SXTr9hIvJ1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7701/Reviewer_ftz8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7701/Reviewer_ftz8"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the weighted low rank approximation problem (WLRA) when the weight matrix is also of a low rank. The authors propose a new relaxed solution to this problem which outputs a matrix that can be well stored. As a corollary, the authors also give nearly optimal communication complexity bounds for another distributed problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The studied problem of WLRA is important.\n- The proposed algorithm is efficient and can be implemented.\n- The writing is clear."
                },
                "weaknesses": {
                    "value": "- The output of the main algorithm is not guaranteed to be of low rank, which is inconsistent with the original goal of WLRA. It may be better to discuss more about this type of output, specifically, why it can replace a low-rank matrix in practice.\n- There is no theoretical guarantee for Algorithm 2, which makes it less interesting."
                },
                "questions": {
                    "value": "- On page 2 before Algorithm 1, the authors claim that the storage space $O((n+d)rk)$ is nearly optimal for $r=O(1)$. Does there a matching lower bound of space for WLRA when $r=O(1)$?\n- Is rank $r$ known in advance? If not, suppose we run Algorithm 1 with $r > rank(W)$, what happens?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Reviewer_ftz8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698150648716,
            "cdate": 1698150648716,
            "tmdate": 1699636938290,
            "mdate": 1699636938290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fft6RTyNBG",
                "forum": "SXTr9hIvJ1",
                "replyto": "9Fc8jUrzpM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "> The output of the main algorithm is not guaranteed to be of low rank, which is inconsistent with the original goal of WLRA. It may be better to discuss more about this type of output, specifically, why it can replace a low-rank matrix in practice.\n\nWe have indeed departed from outputting a low rank matrix for the WLRA problem, and instead opted for a generalized solution. However, this solution can still be stored using few parameters, and can also be applied quickly if we use our practical variation of Algorithm 2. Our extensive experiments show that these relaxed solutions are indeed useful in practice, and demonstrates that it allows for solutions that can be stored and applied efficiently, allowing for state of the art performance for tasks such as neural network compression with a weighted objective.\n\n> There is no theoretical guarantee for Algorithm 2, which makes it less interesting.\n\nWhile we have not been able to analyze Algorithm 2, our experiments demonstrate its effectiveness in practice. We believe this is already compelling evidence for practitioners to adopt our approach.\n\n> On page 2 before Algorithm 1, the authors claim that the storage space $O((n+d)rk)$ is nearly optimal for $r = O(1)$. Does there a matching lower bound of space for WLRA when $r = O(1)$?\n\nWhen $r = O(1)$, the storage space is $O((n+d)k)$. Note that the size of the output requires this many bits of space, since the output is a factorization of the $n\\times d$ matrix into a $n\\times k$ matrix and a $k\\times d$ matrix.\n\n> Is rank $r$ known in advance? If not, suppose we run Algorithm 1 with $r > \\mathrm{rank}(\\mathbf W)$, what happens?\n\nWe assume that $r$ is known in advance. Note that $r$ can be readily computed with access to the weight matrix $\\mathbf W$. As we do in our experiments, one can also approximate $\\mathbf W$ by a low rank matrix via an approximate SVD. We do not prove guarantees when $r > \\mathrm{rank}(\\mathbf W)$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329339694,
                "cdate": 1700329339694,
                "tmdate": 1700329339694,
                "mdate": 1700329339694,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QQtWMAHtVc",
                "forum": "SXTr9hIvJ1",
                "replyto": "fft6RTyNBG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_ftz8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_ftz8"
                ],
                "content": {
                    "title": {
                        "value": "Further question on the lower bound for r = O(1)"
                    },
                    "comment": {
                        "value": "In the reply, the authors claim that the size of the output also requires $O((n+d)k)$ storage space since it is a factorization. However, since $r = O(1)$, I am not sure whether it is possible that the factorization can be done with even fewer bits of space. For instance, is it possible that the factorization of the $n\\times d$ matrix $L$ is sparse? Or is it possible to further factorize $L$ into a $n\\times r$ matrix and a $r\\times d$ matrix? These seem to be possible to further reduce the bit complexity. Hence, I am asking whether there exists a formal theorem/proof for the bit lower bound."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700354855142,
                "cdate": 1700354855142,
                "tmdate": 1700354855142,
                "mdate": 1700354855142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YepK47YuJn",
            "forum": "SXTr9hIvJ1",
            "replyto": "SXTr9hIvJ1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7701/Reviewer_hd3F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7701/Reviewer_hd3F"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new approximate low rank weighted recovery problem, which is very interesting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. New $\\kappa$ approximate framework\n2. Both theoretical and practical algorithms are proposed, which are actually simple to use.\n3. Theoretical guarantees are offered to ensure the quality of the solution in certain cases.\n4. Experiments conducted are convincing."
                },
                "weaknesses": {
                    "value": "1. Writing could be clearer with the different notations, and the overall objective the paper wants to achieve."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Reviewer_hd3F"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698178816603,
            "cdate": 1698178816603,
            "tmdate": 1699636938175,
            "mdate": 1699636938175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eZc0tOjokd",
                "forum": "SXTr9hIvJ1",
                "replyto": "YepK47YuJn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329334294,
                "cdate": 1700329334294,
                "tmdate": 1700329334294,
                "mdate": 1700329334294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zKGqNYvxDV",
                "forum": "SXTr9hIvJ1",
                "replyto": "eZc0tOjokd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_hd3F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_hd3F"
                ],
                "content": {
                    "comment": {
                        "value": "Without saying but I'll keep my original score. I think the authors proposed an interesting framework, and its immediate usefulness is not the most important concern in my eye. This is a paper tailored more towards bringing forward a new way of thinking from a theoretical perspective (though incremental in some ways)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634478426,
                "cdate": 1700634478426,
                "tmdate": 1700634478426,
                "mdate": 1700634478426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "92ZOlhGxuZ",
            "forum": "SXTr9hIvJ1",
            "replyto": "SXTr9hIvJ1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
            ],
            "content": {
                "summary": {
                    "value": "Weighted low rank approximation (WLRA) is an important and fundamental problem in numerical linear algebra, statistics, and machine learning. Specifically, given two matrices $\\mathbf{A}$, $\\mathbf{W} \\in \\mathbb{R}_{\\ge 0}$, and a parameter $k$, our goal is to minimize $|| \\mathbf{W} \\circ (\\mathbf{A} - \\widetilde{\\mathbf{A}})||_F$ subject to $\\mathrm{rank}(\\widetilde{\\mathbf{A}}) \\le k$. This paper considers one of its relaxed versions: solving a matrix $\\widetilde{\\mathbf{A}}$ such that $|| \\mathbf{W} \\circ (\\mathbf{A} - \\widetilde{\\mathbf{A}}) ||_F \\le \\kappa \\cdot \\min _{\\mathrm{rank}(\\mathbf{A}')\\le k} ||\\mathbf{W} \\circ (\\mathbf{A} - \\mathbf{A}')||$, which assumes $\\mathrm{rank}(\\mathbf{W}) = r$ and removes the rank bound for matrix $\\widetilde{\\mathbf{A}}$.   \nFor the above problem, this paper proposes a simple algorithm and proves its correctness. As a corollary, this paper obtains the first relative error guarantee for unsupervised feature selection with a weighted $F$-norm objective. In addition, this paper researches the communication complexity for WLRA and gives the almost matched upper bound and lower bound."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) This paper proposes a simple algorithm for one relaxed WLRA problem and proves its correctness.  \n(2) It extends to unsupervised feature selection with a weighted $F$-norm objective.    \n(3) It explores the communication complexity of the WLRA problem and gives the almost matched upper bound and lower bound.  \n(4) The experimental results indicate the strengths of the proposed algorithm with respect to the approximation loss and running time, compared with the existing methods.  \n(5) This paper is well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "(1) This paper relaxes the classical WLRA problem with two conditions: 1) removing the low-rank requirement for matrix $\\widetilde{\\mathbf{A}}$; 2) assuming the weight matrix $\\mathbf{W}$ is low rank. Given these two conditions, the problem becomes much easier, and the proposed algorithm is kind of trivial. Furthermore, if one discarded the low-rank requirement for matrix $\\widetilde{\\mathbf{A}}$, the relaxed WLRA problem would be kind of insignificant.  \n(2) The unsupervised feature selection with a weighted $F$-norm objective directly follows Theorem 1.2. Also, although the bounds for communication complexity of WLRA problem are almost tight, the results and processes are kind of straightforward. Therefore, the contributions in this paper are quite limited.  \n(3) In Related Work, the following reference is missing. $\\textit{Recovery guarantee of weighted low-rank approximation via alternating minimization}$."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698197763497,
            "cdate": 1698197763497,
            "tmdate": 1699636938051,
            "mdate": 1699636938051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O6NGsDLOSu",
                "forum": "SXTr9hIvJ1",
                "replyto": "92ZOlhGxuZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "> This paper relaxes the classical WLRA problem with two conditions: 1) removing the low-rank requirement for matrix $\\tilde{\\mathbf A}$; 2) assuming the weight matrix is low rank. Given these two conditions, the problem becomes much easier, and the proposed algorithm is kind of trivial. Furthermore, if one discarded the low-rank requirement for matrix $\\tilde{\\mathbf A}$, the relaxed WLRA problem would be kind of insignificant.\n\nPlease note that even if we allow $\\tilde{\\mathbf A}$ to violate the low rank condition, we can still study a non-trivial problem if $\\tilde{\\mathbf A}$ is structured, i.e. can be stored using much fewer bits of memory or can be applied much quicker than $\\mathbf A$. We agree that our proposed algorithm is simple, yet the analysis is nontrivial and our experiments show that our idea leads to great practical benefits over prior work, as demonstrated by our experiments.\n\n> The unsupervised feature selection with a weighted F-norm objective directly follows Theorem 1.2. Also, although the bounds for communication complexity of WLRA problem are almost tight, the results and processes are kind of straightforward. Therefore, the contributions in this paper are quite limited.\n\nThe fact that our Theorem 1.2 immediately gives the unsupervised feature selection result shows that Theorem 1.2 is a powerful result. We ask the reviewer to consider the importance of the problem of unsupervised feature selection with a weighted objective, which is a commonly encountered problem in practical data analysis, and the fact that such results were not possible prior to our result.\n\nOur communication complexity lower bound indeed uses standard tools, but we consider the conceptual message resulting from this theorem to be our more important contribution. Our result shows that when considering the communication complexity of the WLRA problem, the rank of the weight matrix must necessarily factor into the communication complexity. That is, without the low-rank assumption on the weight matrix, the WLRA problem is difficult in the communication complexity model. We believe this perhaps surprising message to be one of our most important contributions, as a priori it is not clear the rank would parameterize the communication complexity. \n\n> In Related Work, the following reference is missing: Recovery guarantee of weighted low-rank approximation via alternating minimization.\n\nWe have included this work in the Related Work section."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329330903,
                "cdate": 1700329330903,
                "tmdate": 1700329330903,
                "mdate": 1700329330903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LlOnEFcWj6",
                "forum": "SXTr9hIvJ1",
                "replyto": "O6NGsDLOSu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. I still have the following concerns.  \n(1) The rank $r$ of the weight matrix $W$ is known in advance and $r = O(1)$. I think this assumption is kind of strong. Given this condition, the space complexity $O( (n+d) rk))$ almost matches the lower bound $\\Omega( (n+d) k)$. This cannot be taken as a contribution since in classical WLRA [Li-Liang-Risteski'16, Song-Ye-Yin-Zhang'23], the output matrix has rank $k$ and thus takes space $O( (n+d) k)$.       \n(2) There is no theoretical guarantee for Algorithm 2. You claimed that its effectiveness is demonstrated by experiments, which cannot convince me since it only works for four datasets.   \n(3) This paper does not analyze the time complexity of the proposed algorithm. For an $n \\times d$ matrix, it takes time $O(ndrk)$ to compute its rank $rk$ approximation by SVD. However, the existing work [Song-Ye-Yin-Zhang'23] for WLRA has time complexity $O(ndk \\log(1/\\epsilon))$. Therefore, this paper requires $r = O(1)$, and in terms of space and time, the proposed algorithm has no advantage compared with the existing work on classical WLRA.   \n(4) For column subset selection (Corollary 1.3), the algorithm returns a set $S$ of size $|S| = O(r k / \\epsilon)$ and a matrix $X \\in \\mathbb{R}^{|S| \\times d}$, which quantity is taken as a column subset selection of matrix $A$? Is it $(W \\circ A)|^S$? If yes, how can $(W \\circ A)|^S$ approximate $A$?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508410874,
                "cdate": 1700508410874,
                "tmdate": 1700508410874,
                "mdate": 1700508410874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BPphXqBIHH",
                "forum": "SXTr9hIvJ1",
                "replyto": "yiJk7DbYwn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your clarification.  \n(1) Your explanation about the differences with the algorithms based on alternating minimization [LLR'16, SYYZ'23] makes sense.  What is $\\kappa$'s value? Is it equal to $1+\\epsilon$?    \n(2) I don't negate your experimental results. My question is $\\textit{could you give the theoretical guarantee of Algorithm 2 but not verification by just running experiments?}$    \n(3) Your explanation about the running time makes sense. It would be better to add the time complexity of the proposed algorithm to the future paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533187520,
                "cdate": 1700533187520,
                "tmdate": 1700533187520,
                "mdate": 1700533187520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gVDo7kezys",
                "forum": "SXTr9hIvJ1",
                "replyto": "5N4NSKKQgz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_GrgZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581536443,
                "cdate": 1700581536443,
                "tmdate": 1700581536443,
                "mdate": 1700581536443,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rNrDShejg9",
            "forum": "SXTr9hIvJ1",
            "replyto": "SXTr9hIvJ1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7701/Reviewer_e1o5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7701/Reviewer_e1o5"
            ],
            "content": {
                "summary": {
                    "value": "In their manuscript, the authors propose an algorithm for the well-known weighted low-rank approximation problem, which is known to be NP-hard, but which has various applications in statistics, signal processing and machine learning. If W is the (non-negative) weight matrix, the authors consider taking the partial SVD of W \\circ A, if A is the matrix to be approximated, and then multiply the entrywise inverse of W to the resulting matrix. They provide approximation guarantees for the algorithm as well as a supposedly more practical variant of the algorithm, Algorithm 2, which is meant to avoid a computational pitfall of the original method. Furthermore, they provide an analysis of the resulting communication complexity, and relate that to a lower bound. Finally, they conduct numerical simulations comparing the algorithm's performance on model compression datasets with other state-of-the-art weighted low-rank approximation algorithms. The authors also shed light on the appropriateness of a low-rank assumption on Fisher information matrices in the context of neural network loss functions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed algorithms is conceptually simple and appears to be new. The approximation guarantee of Theorem 1.2 is reasonable and simple. The experiments suggest that the resulting weighted low-rank approximation is in the range of the state-of-the-art in terms of approximation quality.  The presentation of the results is relatively clear and many relevant papers and methods are cited."
                },
                "weaknesses": {
                    "value": "The main motivation of the algorithm as well as the theoretical results are tailored to the case where the weight matrix W is low-rank. However, the fundamental problem in this setting is that there is no reason why the entrywise inverse matrix W^{\\circ -1} is low-rank, which leads to the necessity of computing a dense matrix in Algorithm 1, as the authors state, making the algorithm rather inpractical in a large-scale setting. With the practical variant of Algorithm 2, only the storage issue of the resulting approximation is mitigated, but not not the fact that W^{\\circ -1} needs to be computed in a dense manner, which requires O(nd) of storage in intermediate calculations.\nWhile the communication complexity discussion of Section 1.1.2 is interesting, it is unclear whether and how the algorithm can be implemented efficiently in a distributed manner. While the authors claim that their result is the first about communication complexity for weighted low-rank approximation problems, it seems that communication complexity for such results was already previously discussed, e.g., in Musco, Cameron, Christopher Musco, and David Woodruff. \"Simple Heuristics Yield Provable Algorithms for Masked Low-Rank Approximation.\"\u00a0_Innovations in Theoretical Computer Science Conference (ITCS 2021)_. 2021."
                },
                "questions": {
                    "value": "- Can you discuss the time complexity of the resulting algorithms and intermediate space complexity of Algorithm 1 and Algorithm 2?\n- Please discuss your result in the context of previous communication complexity results.\n- I did not find the code for the timing experiments in the provided code submission. Furthermore, it would be good if the hyper parameter choices of the reference algorithms are provided in the manuscript, e.g. of adam and em."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7701/Reviewer_e1o5"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7701/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699235649918,
            "cdate": 1699235649918,
            "tmdate": 1699636937873,
            "mdate": 1699636937873,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qRkWjUzSba",
                "forum": "SXTr9hIvJ1",
                "replyto": "rNrDShejg9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "> With the practical variant of Algorithm 2, only the storage issue of the resulting approximation is mitigated, but not not the fact that $\\mathbf W^{\\circ -1}$ needs to be computed in a dense manner, which requires $O(nd)$ of storage in intermediate calculations.\n\nIn Appendix A, we have added a discussion of how a broad family of structured matrices formed by the sum of support-disjoint rank 1 matrices and a sparse matrix can be stored in a way such that $\\mathbf W^{\\circ -1}$ can be applied and stored efficiently, without forming the matrix $\\mathbf W$. This family captures, for example, all of the classes of structured matrices of interest discussed in the prior work of Musco-Musco-Woodruff \"Simple Heuristics Yield Provable Algorithms for Masked Low-Rank Approximation\" including Low-Rank Plus Sparse, Low-Rank Plus Diagonal, Low-Rank Plus Block Diagonal, Monotone Missing Data Pattern, and Low-Rank Plus Banded matrices.\n\n> While the communication complexity discussion of Section 1.1.2 is interesting, it is unclear whether and how the algorithm can be implemented efficiently in a distributed manner.\n\nOur algorithm essentially only relies on the primitive of an SVD, which is known to be efficiently implementable in a distributed setting (see, e.g., Boutsidis-Woodruff-Zhong 2016 \"Optimal principal component analysis in distributed and streaming models\"). Our work shows that this primitive is indeed useful for weighted low rank approximation, especially for structured weight matrices (see Response 1)x, and that this algorithm is nearly optimal for certain distributed formulations of the weighted low rank approximation problem.\n\n> Can you discuss the time complexity of the resulting algorithms and intermediate space complexity of Algorithm 1 and Algorithm 2?\n\nBoth Algorithms 1 and 2 involve computing rank $rk$ approximations to matrices of size $n\\times d$. Exact SVD can be computed in $O(nd^{\\omega-1})$ operations using $O(nd)$ words of space, whereas randomized approximation algorithms can approximate the Frobenius objective up to a $(1+\\varepsilon)$ factor in time and space $O(\\mathsf{nnz}(\\mathbf A)) + \\tilde O(n) \\mathrm{poly}(k/\\varepsilon)$, where $\\mathsf{nnz}(\\mathbf A)$ denotes the number of nonzero entries of $\\mathbf A$ (Clarkson-Woodruff 2013). Thus for dense matrices where $\\mathsf{nnz}(\\mathbf A) = O(nd)$, the intermediate calculations do not affect the space complexity of the algorithm. We note that in the setting of machine learning applications, dense linear algebra is often used due to improved hardware utilization, and thus $\\mathsf{nnz}(\\mathbf A) = nd$ is a common setting.\n\n> Please discuss your result in the context of previous communication complexity results.\n\nThe study of communication complexity of WLRA is, to the best of our knowledge, new in this work. The work by Musco-Musco-Woodruff mentioned by the reviewer indeed discusses communication complexity and WLRA together. However, in that work, communication complexity is used as a tool to analyze the \"complexity\" of the weight matrix $\\mathbf W$, and the analysis results in an offline algorithm for WLRA. That is analogous to our use of the rank of $\\mathbf W$ as a tool to analyze the ``complexity'' of $\\mathbf W$. On the other hand, that work does not address the problem of how a solution to the WLRA problem can be communicated between servers, which is the topic of our work.\n\n> I did not find the code for the timing experiments in the provided code submission. Furthermore, it would be good if the hyper parameter choices of the reference algorithms are provided in the manuscript, e.g. of adam and em.\n\nThe code for the timing experiments was provided in `weighted_lra.py` in the provided code submission. The adam algorithm was run with 100 epochs, with an initial learning rate of 1.0 that is decayed by a factor of 0.7 for every 10 steps. The em algorithm was run for 25 epochs. These details have been included in the manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329345698,
                "cdate": 1700329345698,
                "tmdate": 1700329345698,
                "mdate": 1700329345698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LRbwg4lhMc",
                "forum": "SXTr9hIvJ1",
                "replyto": "rNrDShejg9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_e1o5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Reviewer_e1o5"
                ],
                "content": {
                    "comment": {
                        "value": ">> With the practical variant of Algorithm 2, only the storage issue of the resulting approximation is mitigated, but not not the fact that $\\mathbf W^{\\circ -1}$ needs to be computed in a dense manner, which requires $O(nd)$ of storage in intermediate calculations.\n\n>In Appendix A, we have added a discussion of how a broad family of structured matrices formed by the sum of support-disjoint rank 1 matrices and a sparse matrix can be stored in a way such that $\\mathbf W^{\\circ -1} $ can be applied and stored efficiently, without forming the matrix $\\mathbf{W}$. This family captures, for example, all of the classes of structured matrices of interest discussed in the prior work of Musco-Musco-Woodruff \"Simple Heuristics Yield Provable Algorithms for Masked Low-Rank Approximation\" including Low-Rank Plus Sparse, Low-Rank Plus Diagonal, Low-Rank Plus Block Diagonal, Monotone Missing Data Pattern, and Low-Rank Plus Banded matrices.\n\nThe special cases you are pointing out are problematic. First, I'd like to clarify for the other reviewers that unlike your comment suggests, based on my reading of your updated appendix, the suggested procedure does not apply to all matrices within the families of Low-Rank Plus Sparse matrices etc., but only to specific cases of such matrices. Furthermore, these special cases are all about matrices that have a significant number of zero entries. What does $\\mathbf{W}^{\\circ -1}$ even mean in these cases, as one is essentially dividing by zero?\n\nFurthermore, I'd like to make my original concern more precise: Algorithm 2 will still require the computation of dense matrices as the second step of Algorithm 2 requires the multiplication of (tilde) $\\mathbf W_{inv}^{\\circ -1} $ with the entries of $\\mathbf{A}$, where (tilde)$\\mathbf W_{inv}$ is the rank-$rk$ approximation of $\\mathbf{W}^{\\circ -1}$. Even if $\\mathbf{W}^{\\circ -1}$ were somehow to be computed efficiently, there is no reason why the entrywise inverse (tilde)$\\mathbf W_{inv}^{\\circ -1}$ of (tilde)$\\mathbf W_{inv}$ can be computed without dense matrix processing. My understanding is that (tilde)$\\mathbf W_{inv}$ will not inherit structural properties of $\\mathbf W$ or of $\\mathbf W^{\\circ -1}$.\n\nThank you for addressing my other questions. In any published version of this paper, it would be important to make the distinction between communication complexity notions of the your paper and the Musco-Musco-Woodruff paper. \n\nAlso in the view of the discussion of other reviewers' comments, I maintain my rating for the submission."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714882295,
                "cdate": 1700714882295,
                "tmdate": 1700715332884,
                "mdate": 1700715332884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tAMNYMbgpF",
                "forum": "SXTr9hIvJ1",
                "replyto": "rNrDShejg9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7701/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We believe there are several inaccuracies in the reviewer's response, and hope the reviewer can consider their evaluation in light of our responses below:\n\n(1) It is inaccurate to say that all of the practical weight matrices we handle have a large number of zero entries. A simple (and uninteresting) special case is a rank-1 matrix, which could have an arbitrary density of zeros or ones, or even be all 1s, and so  has no zero entries in that case. Also low-rank-plus diagonal or low rank plus banded is mostly ones, not zeros. Also (and more interestingly) low rank plus block diagonal as well as monotone missing data patterns can have an arbitrary density of zeros and ones depending on the block or prefix sizes. \n\n(2) Note that if $\\mathbf W_{i,j} = 0$, then we can set $\\mathbf W^{\\circ -1}_{i,j}$ to be arbitrary (say, always equal to $1$ for concreteness) since such entries will be zero-ed out when we apply $\\mathbf W$ entrywise, so in particular there is no worry of dividing by zero if that is the concern. This can be seen in the first line of the proof of Theorem 1.2 at the top of page 6. \n\n(3) We cannot prove that Algorithm 2 works for every low rank plus sparse matrix - however, we believe we make a significant theoretical advance by applying it to a large class of popular weight matrices used in practice and giving the first theoretical bounds for such matrices. In Appendix A, for each weight matrix described there, either $\\mathbf W^{\\circ -1}$ already has low rank, or one can apply $\\mathbf W^{\\circ -1}$  quickly as we describe there. Note that if $\\mathbf W = u \\cdot v^T$, then $\\mathbf W^{\\circ -1} = u^{\\circ -1} \\cdot (v^T)^{\\circ -1}$, where zero entries of $u$ or $v$ can be replaced with an arbitrary number (e.g., $1$ for concreteness) for the reason described in (2) above. Thus, $\\mathbf W^{\\circ -1}$ does partially inherit low rank structure from $\\mathbf W$. Also, as noted in Appendix A, this inheritance of low rank structure holds for larger-rank matrices $\\mathbf W$ provided the combinatorial rectangles defined by the supports of the low rank factors are disjoint, which together with our ability to handle sparsity, solves five different previously studied and practical weight patterns - in fact for every motivating weight matrix studied in Musco-Musco-Woodruff we give an efficient algorithm. Besides our theoretical results, which at least to us are surprising since a priori $\\mathbf W^{\\circ -1}$ may not look easy to apply, we give experimental results for an even wider class of weight matrices $\\mathbf W$ by heuristically computing a low rank approximation of $\\mathbf W^{\\circ -1}$. \n\n(4) We will stress that in the Musco-Musco-Woodruff paper, there is no communication or distributed setting at all. The notion of ``communication complexity\" is a combinatorial notion about a matrix, and that is used to describe the rank of their bicriteria approximations. In our submission, however, we describe a distributed setting and use communication complexity to mean the maximum length of a protocol transcript, over all inputs and random choices. \n\nWe are happy to make these points clearer in the writing in case there is any confusion. Please let us know if there are any other questions, and thanks for giving us the opportunity to respond."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7701/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719952596,
                "cdate": 1700719952596,
                "tmdate": 1700721918948,
                "mdate": 1700721918948,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]