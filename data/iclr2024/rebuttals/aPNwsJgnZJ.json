[
    {
        "title": "Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs"
    },
    {
        "review": {
            "id": "rQzJSLOfNk",
            "forum": "aPNwsJgnZJ",
            "replyto": "aPNwsJgnZJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2691/Reviewer_K64K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2691/Reviewer_K64K"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies horizon-free RL in adversarial linear mixture MDPs with full-information feedback. This paper proposes an algorithm that employs a variance-aware weighted least square for the transition kernel and an occupancy measure-based method for the online search of a stochastic policy. The authors show the algorithm achieves a regret with polylogarithmic dependence on $H$. Further, this paper provides a lower bound showing the inevitable polylogarithmic dependence on state number $S$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is the first work that studies near-optimal horizon-free RL algorithms under adversarial reward and linear function approximation. This progress deserves to be known to the community.\n2. The connection between the value function derived from occupancy measure guided policy updating and the other one derived from backward iteration (Lemma 6.1) is new as far as I know, which may inspire other studies for RL problems.\n3. The paper is clearly written and well-organized. The proofs are technical sound though I don't check the proofs."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper may be limited. Most of the analysis follows from that of horizon-free reinforcement learning for linear mixture MDPs with stochastic rewards (Zhou and Gu, 2022). \n2. The occupancy measure-based algorithm is not computationally efficient as the running time has polynomial dependence on the state number $S$ and action number $A$."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2691/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697707318397,
            "cdate": 1697707318397,
            "tmdate": 1699636210511,
            "mdate": 1699636210511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0PkmL1D45V",
                "forum": "aPNwsJgnZJ",
                "replyto": "rQzJSLOfNk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2691/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2691/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your supportive feedback. We will answer your question as follows:\n\n**Q1**: The novelty of this paper may be limited. Most of the analysis follows from that of horizon-free reinforcement learning for linear mixture MDPs with stochastic rewards [1].\n\n**A1**: We agree that the our analysis framework follows [1]. However, compared to [1] and other very related work [2][3], our paper actually introduces several new techniques. First, compared to [2][3], we use occupancy-measure, rather than policy-optimization, to update the policy, making our work the first to apply occupancy-measure to this setting. Second, compared to [1], we adopted random policy, which inevitably introduce a new policy noise term $\\sum_{k=1}^K\\sum_{h=1}^H \\mathbb{E}\\big[Q_{k, h}(s_h^k, a) - Q_{k, h}^{\\pi^k}(s_h^k, a)\\big] - \\big(Q_{k,h}(s_h^k, a_h^k) - Q_{k, h}^{\\pi^k}(s_h^k, a_h^k) \\big)$ with $a\\sim\\pi^k_h(\\cdot|s_h^k)$. This term is bounded by $\\tilde{O}(\\sqrt{KH})$ by using Azuma-Hoeffding's inequality and therefore introduces an unwanted dependency on $H$. To tackle this issue, we introduce a notion of conditional variance of $Q_{k, h+1}^{2^m}(s_{h+1}^k, a)$ into the analysis of Algorithm 3 (HOME) to bound the policy noise in a recursive way and settle the poly$(H)$ dependency down. This is also a new technique that does not appear in previous analysis [1]. Third, we also proposed a novel lower bound showing that horizon-free regret can only be achieved under finite state assumption.\n\n**Q2**: The occupancy measure-based algorithm is not computationally efficient as the running time has polynomial dependence on the state number S and action number A.\n\n**A2**: We agree with the reviewer as we have discussed in Remark E.2. The necessity of computing all the $\\Omega(S)$ entries of occupancy measure is the limitation of our algorithm. However, as we have shown in Section 6, using occupancy measure is the key technique to get rid of the $H$ dependency in online mirror descent regret. We will explore more efficient algorithms in the future work.\n\nReferences:\n\n[1] Zhou, D., & Gu, Q. (2022). Computationally efficient horizon-free reinforcement learning for linear mixture mdps. arXiv preprint arXiv:2205.11507.\n\n[2] He, J., Zhou, D., & Gu, Q. (2022, May). Near-optimal policy optimization algorithms for learning adversarial linear mixture mdps. In International Conference on Artificial Intelligence and Statistics (pp. 4259-4280). PMLR.\n\n[3] Cai, Q., Yang, Z., Jin, C., & Wang, Z. (2020, November). Provably efficient exploration in policy optimization. In International Conference on Machine Learning (pp. 1283-1294). PMLR."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2691/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357819529,
                "cdate": 1700357819529,
                "tmdate": 1700357819529,
                "mdate": 1700357819529,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K9JJNGknVs",
            "forum": "aPNwsJgnZJ",
            "replyto": "aPNwsJgnZJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2691/Reviewer_ZMLZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2691/Reviewer_ZMLZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the first algorithm enjoying horizon free bounds for the adversarial linear mixture MDP. The algorithm is based on a careful combination of policy updates step performed with mirror descent steps in the occupancy measures space and an optimistic policy evaluation phase carried out using weighted ridge regression estimators.\n\nAn interesting finding is also a separation between the adversarial and non adversarial case. Indeed, the authors managed to prove an asymptothic lower bounds which shows that either $\\sqrt{H}$ or $\\log S$ must be paid in the regret bound while a $S$ independent horizon free regret upper bound can be obtained in the non adversarial case."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think that the algorithm is original and well explained in the main text.\n\nThe result which entails a $\\log S$ dependence would not be very satisfactory in the function approximation setting but the author nicely shows that either this dependence or a $\\sqrt{H}$ dependence must be suffered.\n\nI also enjoyed the lower bound construction which considers a tabular deterministic MDP and reduces it to an expert problem.\n\nThe proofs look correct to me."
                },
                "weaknesses": {
                    "value": "There are few clarity omission or missing definition in the submission. Hereafter, I list few of them:\n\n- I think it should be clearer that also homogenous transition dynamics are required for obtaining reward free bounds. Therefore, the Bellman optimality equation at page 3 should not have $h$ in the footnote of the operator $\\mathbb{P}$.\n\n- the value function $\\overline{V_{k,1}}$ is never formally defined in the paper. So it is difficult to understand what it denotes when reading the regret decomposition in equation (6.1).\nIf I understood correctly from the Appendix, each mirror descent iterate $z_k$ induces via the marginals a transition kernel $\\overline{p_{k}}$ and a policy $\\pi_k$. At this point $\\overline{V_{k,1}}$ denotes the initial value of policy $\\pi_k$ in the MDP endowed with reward function $r_k$ and transition dynamics $\\bar{p}_k$. Can the authors confirm that this is correct and if yes add it to their revision ?\n\n- The definition of Regret at page 3 is a bit unclear. Indeed saying that $V^\\star_k$ is the optimal state value function could make the reader believe that $V^\\star_k = \\max_{\\pi} V^{\\pi}_k$, that is the regret we control has the maximum inside the sum. \nHowever, the regret controlled in the paper has a fixed comparator policy which might not be optimal for any of the reward function revealed at each round."
                },
                "questions": {
                    "value": "I think that it is unclear that $I^k_h$ defined in Appendix C.2 is decreasing. After inspecting the proofs I think that what the authors need is that for any fixed $k$ than $I^k_h$ is decreasing with respect to $h$. Is this correct ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2691/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2691/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2691/Reviewer_ZMLZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2691/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698245783410,
            "cdate": 1698245783410,
            "tmdate": 1699636210420,
            "mdate": 1699636210420,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fOoH08VsqQ",
                "forum": "aPNwsJgnZJ",
                "replyto": "K9JJNGknVs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2691/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2691/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive and constructive feedback. We answer your questions as follows:\n\n**Q1**:I think it should be clearer that also homogenous transition dynamics are required for obtaining reward free bounds. Therefore, the Bellman optimality equation at page 3 should not have \u210e in the footnote of the operator P.\n\n**A1**: Thank you for your suggestion and we have fixed it in the reversion.\n\n**Q2**: The value function $\\bar{V}_k,1$ is never formally defined in the paper.\n\n**A2**: We are sorry for the missing formal definition. We have added an equation to formally define $\\bar{V}_k,1$ in the revision.\n\n**Q3**: The definition of Regret at page 3 is a bit unclear.\n\n**A3**: Thank you for your suggestion and we have revised it to make it clearer.\n\n**Q4**: I think that it is unclear that $I_k^h$ defined in Appendix C.2 is decreasing. After inspecting the proofs I think that what the authors need is that for any fixed k  $I_k^h$ is decreasing with respect to $h$. Is this correct ?\n\n**A4**: Yes, we indeed need for any fixed k then $I_k^h$ is decreasing with respect to $h$. We apologize for the confusion and have made it clear in the revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2691/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357703980,
                "cdate": 1700357703980,
                "tmdate": 1700357703980,
                "mdate": 1700357703980,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bZ5AylZVAn",
                "forum": "aPNwsJgnZJ",
                "replyto": "fOoH08VsqQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2691/Reviewer_ZMLZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2691/Reviewer_ZMLZ"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for correcting the issues I mentioned in my review.\n\nI am keeping my positive score and will support the acceptance of your manuscript in the discussion phase.\n\nHave a nice weekend,\n\nBest,\nReviewer"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2691/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381174149,
                "cdate": 1700381174149,
                "tmdate": 1700381174149,
                "mdate": 1700381174149,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gcTyt1r6Js",
            "forum": "aPNwsJgnZJ",
            "replyto": "aPNwsJgnZJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2691/Reviewer_tM5p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2691/Reviewer_tM5p"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the question of whether the favorable polylogarithmic regret seen in reinforcement learning (RL) with respect to the planning horizon can also be extended to adversarial RL scenarios. The authors introduce the first horizon-independent policy search algorithm, designed to cope with challenges arising from exploration and adversarial reward selection over episodes. The algorithm utilizes a variance-uncertainty-aware weighted least square estimator for the transition kernel and an occupancy measure-based approach for online stochastic policy search."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Given my limited expertise in the adversarial RL domain, my evaluation focuses exclusively on the technical soundness and clarity of the paper. The manuscript exhibits a commendable standard of articulation. The framing of the problem, underlying assumptions, and derived outcomes are adequately elucidated. Notably, the inclusion of a proof sketch in Section 6 enhances the paper's comprehensibility, serving as a valuable reference point for those seeking deeper insight into the paper's theoretical foundations."
                },
                "weaknesses": {
                    "value": "The paper makes relatively strong assumptions: linear, finite-state MDPs and full-information feedback. The only novel aspect here is the paper tackles adversarial reward functions rather than fixed or stochastic rewards. But even so, I think that the full-information feedback assumptions greatly alleviate the difficulty of adversarial rewards. To me, the hardness result is more interesting: an unbounded state space will incur regret in $\\Omega(\\sqrt{H})$. Is this result novel in the literature?\n\nI am a bit confused about the assumptions about the reward. Firstly, can the rewards be negative? If so, would assumption 3.1 still make sense? Furthermore, is assumption 3.1 equivalent to the bounded rewards assumption, i.e., if rewards are bounded in $[-R, R]$, we can always scale everything by $1/RH$ to satisfy assumption 3.1."
                },
                "questions": {
                    "value": "Please see the questions in the weaknesses section above. I am happy to increase my score if there are any misunderstandings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2691/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762077452,
            "cdate": 1698762077452,
            "tmdate": 1699636210335,
            "mdate": 1699636210335,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kldHR4CenS",
                "forum": "aPNwsJgnZJ",
                "replyto": "gcTyt1r6Js",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2691/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2691/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your supportive feedback. We will answer your questions as follows.\n\n**Q1**: The paper makes relatively strong assumptions: linear, finite-state MDPs and full-information feedback. The only novel aspect here is the paper tackles adversarial reward functions rather than fixed or stochastic rewards. \n\n**A1**: We agree with the reviewer that the assumptions we made (linear, full-information) are relatively strong. However, even under these relatively strong but standard assumptions [1][2][3], how to achieve horizon-free regret is still an open problem before our work. Notably, our result is the first horizon-free algorithm in this setting. We agree that finite state space is an extra assumption compared to previous works. However, this is a necessary assumption to achieve a horizon-free regret with adversarial reward, as we have shown in the hardness result in Theorem 5.3.\n\n**Q2**: To me, the hardness result is more interesting: an unbounded state space will incur regret in $\\Omega(\\sqrt{H})$. Is this result novel in the literature?\n\n**A2**: Thank you for your interest! To the best of our knowledge, this result is new and we are the first to show that an infinitely large state space is incompatible with horizon-free regret under adversarial rewards. Therefore, we believe this result is novel and a noteworthy contribution.\n\n**Q3**: Can the rewards be negative?\n\n**A3**: Yes, technically, the reward can be negtive. To accommodate this, we can scale up the reward by a factor of 2 and subsequently translate it to the interval $[-1/H, 1/H]$ by subtracting a constant of $1/H$.\n\n**Q4**: Furthermore, is assumption 3.1 equivalent to the bounded rewards assumption, i.e., if rewards are bounded in [\u2212R,R], we can always scale everything by 1/RH to satisfy assumption 3.1.\n\n**A4**: Yes, scaling the bounded rewards in [-R, R] by a constant of $1/RH$ can make it satisfy our assumption. We would like to bring to the reviewer's attention that the dependency on $H$ in the regret bound of RL algorithms has two sources: 1) the length of planning horizon, and 2) scale of the rewards. Here to offset the dependency on $H$ contributed by the scale of the reward, we scale down the reward by a factor $H$ so that the total reward in each episode is bounded by one, as suggested by [4].\n\nReferences:\n\n[1] Zhou, D., & Gu, Q. (2022). Computationally efficient horizon-free reinforcement learning for linear mixture mdps. arXiv preprint arXiv:2205.11507.\n\n[2] Cai, Q., Yang, Z., Jin, C., & Wang, Z. (2020, November). Provably efficient exploration in policy optimization. In International Conference on Machine Learning (pp. 1283-1294). PMLR.\n\n[3] He, J., Zhou, D., & Gu, Q. (2022, May). Near-optimal policy optimization algorithms for learning adversarial linear mixture mdps. In International Conference on Artificial Intelligence and Statistics (pp. 4259-4280). PMLR.\n\n[4] Nan Jiang and Alekh Agarwal. Open problem: The dependence of sample complexity lower bounds on planning horizon. In Conference On Learning Theory, pp. 3395\u20133398. PMLR, 2018."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2691/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357524046,
                "cdate": 1700357524046,
                "tmdate": 1700357524046,
                "mdate": 1700357524046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R0aYw1G4bv",
                "forum": "aPNwsJgnZJ",
                "replyto": "kldHR4CenS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2691/Reviewer_tM5p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2691/Reviewer_tM5p"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response. I would like to keep my original score and recommend acceptance of the paper.\n\nWarmest regards,  \nReviewer."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2691/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656684111,
                "cdate": 1700656684111,
                "tmdate": 1700656684111,
                "mdate": 1700656684111,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GYSZjwDCNP",
            "forum": "aPNwsJgnZJ",
            "replyto": "aPNwsJgnZJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2691/Reviewer_p5dn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2691/Reviewer_p5dn"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the online learning problem of horizon-free and linear mixture Markov Decision Processes (MDPs). \nTo the best of my knowledge, this is the first paper that can achieve theoretical guarantees with adversarial losses, that is, the loss function can change arbitrarily from episode to episode. \nTo achieve this result, the authors propose two main techniques: (1) a variance-uncertainty-aware weighted least square estimator and (2) an occupancy measure-based approach for constructing policies. The first technique is widely use for linear mixture MDPs, while the second one is mainly used for adversarial losses. \nCombining these two techniques to establish valid regret guarantees is quite challenging. \nMore importantly, the final regret bound is of the order $O(d\\sqrt{K})$, which is nearly the optimal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of combining the two techniques is very interesting. It would be great to see such combination to be applied in other (more general) linear MDP settings.  \n2. Though I just skimmed the proof of several lemmas, the results seems to be rigorous proved and mathematically correct."
                },
                "weaknesses": {
                    "value": "This paper does not have any specific weaknesses."
                },
                "questions": {
                    "value": "1.Is it possible to design policy optimization algorithms for this problem setting? \n2.Is it possible to avoid the usage of occupancy measure (which is not quite efficient in real world)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2691/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2691/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2691/Reviewer_p5dn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2691/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809395464,
            "cdate": 1698809395464,
            "tmdate": 1699636210255,
            "mdate": 1699636210255,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PoPpGuzmWX",
                "forum": "aPNwsJgnZJ",
                "replyto": "GYSZjwDCNP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2691/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2691/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive feedback. We answer your questions as follows:\n\n**Q1**: Is it possible to design policy optimization algorithms for this problem setting? \n\n**A1**: We would like to clarify that policy optimization has previously been applied to adversarial linear mixture MDP [1][2]. However, none of these approaches can guarantee a horizon-free regret upper bound. The reason for this limitation has been shown in Section 6. Specifically, the regret for online mirror descent is $\\tilde{O}(H\\bar{L}\\sqrt{K})$, where $\\bar{L}$ is the upper bound of the subgradient. When employing (multiplicative weights update/EXP3 type) policy optimization, due to reward accumulating, the scale of the Q-functions and the correspoding subgradient is $\\bar{L}=O(1)$, which will inevitably introduce a dependency on $H$. Therefore, we resort to occupancy measures to tackle this challenge.\n\n**Q2**: Is it possible to avoid the usage of occupancy measure (which is not quite efficient in real world).\n\n**A2**: To our knowledge, to tackle adversarial reward, existing approaches are based on either policy optimization or occupancy-measure. As we found that policy optimization is difficult to deduce a horizon-free guarantee, we show that a feasible way is to take occupancy measure-based approach. We will explore other approaches rather than occupancy measure in the future work.\n\nReferences:\n\n[1] Cai, Q., Yang, Z., Jin, C., & Wang, Z. (2020, November). Provably efficient exploration in policy optimization. In International Conference on Machine Learning (pp. 1283-1294). PMLR.\n\n[2] He, J., Zhou, D., & Gu, Q. (2022, May). Near-optimal policy optimization algorithms for learning adversarial linear mixture mdps. In International Conference on Artificial Intelligence and Statistics (pp. 4259-4280). PMLR."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2691/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357415045,
                "cdate": 1700357415045,
                "tmdate": 1700357415045,
                "mdate": 1700357415045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]