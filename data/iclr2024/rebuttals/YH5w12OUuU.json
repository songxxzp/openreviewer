[
    {
        "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting"
    },
    {
        "review": {
            "id": "G9AdPXMHXn",
            "forum": "YH5w12OUuU",
            "replyto": "YH5w12OUuU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8670/Reviewer_izvx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8670/Reviewer_izvx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a way to leverage pre-trained language models for time-series forecasting. Their method is based on two key ideas: (1) decomposing time-series into trend, season and residual components can aid time-series forecasting, and (2) prompting large language models based on a shared pool of prompts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Writing: The paper is well-written for the most part. \n2. Interpretability: The authors aim to shed some light on the time-series predictions made by the model. \n3. Modeling time-series and text together: I really liked this key insight of the paper. I think it is under-explored and valuable. \n4. Theory: The authors provide some theoretical insight into the design decisions behind their model."
                },
                "weaknesses": {
                    "value": "Claims:\n1. While achieving strong prediction performance, the previous works on timeseries mostly benefit ... that captures temporal dependencies but overlooks a series of intricate patterns within timeseries data, such as seasonality, trend, and residual.\" -- I do not agree with the authors, multiple recent approaches using deep learning for time-series forecasting have decomposed model inputs into trend and seasonal components. See N-BEATS, N-HITS (stacked MLP based models), AutoFormer, as an example.\n2. Theorem 3.1 -- I do not fully understand the implications of Theorem 3.1 and how that affects the design choices of the authors.\n3. Prompt pool captures seasonal and trend components: The authors provide an example of 3 time-series from 1 dataset to demonstrate that the prompts capture season and trend components, but I am not sure this is sufficient evidence. It would be interesting to look at the distribution of prompts for multiple (or all) time-series in one or more datasets, as time-series are clustered based on their trend and/or seasonality components. I believe this would give a more, dataset level evidence for the authors' claims.\n4. Interpretability: I am not sure how the GAM and SHAP provide interpretability, beyond confirming what is expected from these models, i.e. the residuals do not have any pattern. \n\nExperimentation: \n1. \"Large-scale experiments/benchmarks\": The authors omit several benchmarks, and therefore I would argue that the experiments are not large-scale. For e.g., for long horizon datasets, the authors do not use the Influenza-like Illnesses and Exchange-rate datasets which PatchTST and TimesNet, and other recent studies. Secondly, there are multiple short-horizon benchmarks, like M3 or the M4 datasets, and the much larger and comprehensive Monash Forecasting archive, yet the authors do not confirm their methods on these datasets. \n2. Multiple methods are omitted from the evaluation, for e.g. statistical methods such as Auto-ARIMA, Auto-THETA, Auto-ETS etc., and deep learning methods such as N-HITS and N-BEATS. Also the authors cite PromptCast but do not compare their method to this particular baseline.\n3. The value of prompt pool-- The authors demonstrate in Table 9 that the prompt pool helps model prediction. How would they explain the methods without prompt pooling doing better on some datasets for some forecasting horizons?\n\nClarity:\n1. Insufficient details in model design and experimentation. See Questions.\n\nMinor: \n1. Typos: inclduing, outpemforms ... etc. \n2. References: I would encourage the authors to find references to accepted papers, instead of citing their ArXiv versions."
                },
                "questions": {
                    "value": "1. Theorem 3.1 -- I do not fully understand the implications of Theorem 3.1 and how that affects the design choices of the authors. What do we learn from the theorem?\n2. Model design: The authors mention they use temporal encoding from PatchTST but do not provide any definition or references for it. As far as I am aware, PatchTST does not use temporal encoding. That is, if my understanding of temporal encoding is accurate. Moreover, work prior to PatchTST, either Informer or Autoformer use temporal encoding instead.\n3. Experimentation: How are LLM-based models like BERT, T5, LLaMA used for forecasting? What is the input to these models? What is the output? \n4. Experimentation: What is the context length for the long-horizon forecasting task?\n5. Experimentation: How are 5/10% of the data points used? How do you define a data point in a long-horizon forecasting time-series? Also 5-10% seems like a lot of data, given that most of the time-series have multiple channels, and the model processes them independently."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8670/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614727711,
            "cdate": 1698614727711,
            "tmdate": 1699637086708,
            "mdate": 1699637086708,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "I4gu39uxqe",
            "forum": "YH5w12OUuU",
            "replyto": "YH5w12OUuU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8670/Reviewer_W4kZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8670/Reviewer_W4kZ"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose TEMPO leveraging a pre-trained language model for time-series forecasting tasks. The two main components of the proposed approach: the decomposition of time series into trend, seasonality, and residuals, as well as the prompt learning, effectively increase the forecasting performance. The improvement is significant. Also, the paper demonstrates the ability of the proposed method to be trained with few-shots and to be adapted to unseen datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper is well-written.\n2) The improvement of forecasting performance over benchmark methods is significant and consistent across datasets."
                },
                "weaknesses": {
                    "value": "1) The idea of decomposition of trend, seasonality, and residuals is not that novel and has been used for time-series forecasting.\n2) The theorem 3.1 does not directly prove the point \u201cmore importance in current transformer-based methods as the attention mechanism, in theory, may not disentangle the disorthogonal trend and season signals automatically\".\n3) The result shown in Figure 2 seems to be obvious since the trend is easier to learn and may take a large portion of the data."
                },
                "questions": {
                    "value": "1) In the few-shot learning setting, do you need to finetune the model on other time-series datasets first?\n2) What is the computation complexity of the proposed method, and how it is comparable to other methods?\n3) Based on the ablation study in Table 9, it seems that w/o prompts have less effect on the final performance. Sometimes without prompts, the performance can be even better. Could you explain this?\n4) How the prompts are initialized and trained.\n5) In Table 1, why don\u2019t you include the traffic forecasting results?\n6) How generalizable can the model be for the unseen domain transfer? For example, what if you choose weather and traffic as the training domain then apply the model to EETm1 and EETm2"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8670/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8670/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8670/Reviewer_W4kZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8670/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733261208,
            "cdate": 1698733261208,
            "tmdate": 1699637086525,
            "mdate": 1699637086525,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "tk1gvink0K",
            "forum": "YH5w12OUuU",
            "replyto": "YH5w12OUuU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8670/Reviewer_2NZn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8670/Reviewer_2NZn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new interpretable solution, TEMPO, for time series forecasting using the power of pre-trained generative models. Specifically, this work first addresses that the semantic information inside time series is important, for example, the authors utilize the trend, seasonality, and residual information to build the tokens of the pre-trained generative models. In addition, this paper proposes a prompt pool to memory the historical patterns of different time series. The reasonable design and results indicate that TEMPO paves the path of further exploring the pre-training models\u2019 power for time series problems."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This paper\u2019s writing is clear and easy to follow. For example, the methodology part gives a clear description on how to build the time series input representation and the design of the prompt pool. The experiments on long-term time series forecasting, short-term forecasting and towards foundation model\u2019s training are well organized to prove the model\u2019s power from different aspects.\n2. The proposed solution is well motivated: the motivation of decomposition is supported by both empirically and theoretically and the introduce of retrieval-based prompt selection can help the large pre-trained model handle complex non-stationary time series data with distribution shifts.\n3. Utilizing the pre-trained transformer backbone, TEMPO give a state-of-the-art results on the popular time series research dataset."
                },
                "weaknesses": {
                    "value": "1. The prompt pool\u2019s improvement is limited: the prompt pool is supposed to have more contribution to the accuracy as the intuition is clear and convincing.\n2. The collection of TETS dataset is not clear: a clear but simple discription in the main paper is necessary.\n3. It seems only decoder-based pretrain model is considered in this paper. The encoder based backbone (like Bert) and encoder-decoder based backbone (like T5) is also recommend in this stage."
                },
                "questions": {
                    "value": "1. Do you have any insights on the prompt pool\u2019s marginal improvement, which is somehow against the intuition;\n2. The description of TETS dataset is suggested to include into  the main paper;\n3. Can you provide the reason on why only decoder-based GPT is included in the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8670/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825776377,
            "cdate": 1698825776377,
            "tmdate": 1699637086403,
            "mdate": 1699637086403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]