[
    {
        "title": "Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning"
    },
    {
        "review": {
            "id": "icaExzlRH5",
            "forum": "VIEbRFp6s3",
            "replyto": "VIEbRFp6s3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_YX5P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_YX5P"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces OG-MARL, an expansive repository made for cooperative offline multi-agent reinforcement learning (MARL). Addressing the current lack of standardized datasets and baselines in offline MARL, the authors offer a collection mirroring real-world system complexities, such as heterogeneous agents and non-stationarity. These datasets, classified into types like Good, Medium, Poor, and Replay, undergo thorough quality assurance checks. The authors have made OG-MARL publicly accessible."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses a significant gap in the field of offline multi-agent reinforcement learning.   This initiative targets the lack of standardized datasets and baselines, a challenge often overlooked by many in the field of reinforcement learning.  In terms of quality, the datasets were curated and validated.   The use of diverse real-world system parameters, like heterogeneous agents and non-stationarity, makes the paper better.  This paper is also easy to follow. By providing a public repository, it contributes to the MARL research community."
                },
                "weaknesses": {
                    "value": "1. The categorization of datasets heavily based on the quality of experience may inadvertently introduce biases. A more well-rounded evaluation could be achieved by integrating additional qualitative and quantitative metrics. \n\n2. The results section provides an overview of algorithmic performance but lacks analytical depth. Explain the reasons behind the observed performances, such as the underperformance of vanilla QMIX, could offer more substantial insights. \n\n3. There is a similar work, \"Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning,\" has been previously published in AAMAS. Does this submission introduce novel datasets or environments that extend beyond those covered in the AAMAS paper? Are there any innovative algorithmic approaches, evaluation metrics, or experimental setups that were not addressed in the prior publication? Further, how does the current paper tackle the challenges and limitations identified in the AAMAS publication? \n\n4.  While this paper undeniably provides significant aid to the research community in terms of establishing a baseline database and engineering groundwork for MARL, its depth seems somewhat superficial.  The ideas, though functional, are straightforward by testing different algorithms in different environments and producing new datasets (by using the old method).   Meanwhile, while the authors have laid out certain frameworks and methodologies, there isn't clear documentation on how one might go about implementing novel algorithms or introducing new environments within the given context.  This work is engineering important but has little contribution to the theoretical underpinnings or conceptual advancements in the field."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Non"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5706/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5706/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5706/Reviewer_YX5P"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697910853160,
            "cdate": 1697910853160,
            "tmdate": 1700515904750,
            "mdate": 1700515904750,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8ghn0K18q8",
                "forum": "VIEbRFp6s3",
                "replyto": "icaExzlRH5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their in depth feedback on our work. We appreciate the reviewer's confidence that *\u201c[...]this paper undeniably provides significant aid to the research community[\u2026]\u201d.*\n1. *\u201cThere is a similar work, \u2018Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning,\u2019 has been previously published in AAMAS.\u201d*\n* The paper at AAMAS was an extended abstract and therefore non-archival and has not been published in any proceedings. Our work improves upon the extended abstract in several significant ways. Firstly, several new environments were supported, namely SMACv2, KAZ, MPE, Voltage Control and CityLearn. Secondly, we added a dataset generated from human players. And finally, we added a competitive offline dataset.\n2. *\u201cThe ideas, though functional, are straightforward by testing different algorithms in different environments and producing new datasets (by using the old method) [\u2026] This work is engineering important but has little contribution to the theoretical underpinnings or conceptual advancements in the field.\u201d*\n* We feel the significance of dataset papers to the field can not be overstated. RL Unplugged [1] and D4RL [2] both helped drive progress in the field of single-agent offline RL and enabled many breakthroughs in recent years [3,4]. Moreover, we echo reviewer KxKX\u2019s remark that *\u201cBenchmarking is essential in machine-learning communities as well as multi-agent learning communities.\u201d* Without proper benchmarking contributions it's impossible to get a clear overview of the current state of the field. We believe it is for these reasons that ICLR welcomes \u201cDataset and Benchmark\u201d contributions (see the list of subject areas at https://iclr.cc/Conferences/2024/CallForPapers). \n3. *\u201cThe results section provides an overview of algorithmic performance but lacks analytical depth.\u201d*\n* As with related offline RL dataset publications RL Unplugged [1] and D4RL [2], providing detailed theoretical or empirical evidence for a discussion on why one baseline outperforms another is not the focus of this work. The purpose of including baselines was so that future works can easily compare their algorithms to baselines on our datasets. We agree however that a deeper analytical analysis is a valuable direction for future work.\n4. *\u201c...there isn't clear documentation on how one might go about implementing novel algorithms or introducing new environments within the given context.\u201d*\n* We provided an in depth tutorial on how to add a new environment and record data using OG-MARL in the code available on our website (https://sites.google.com/view/og-marl). Furthermore, ours is the largest collection of offline MARL algorithms openly accessible and implemented under the same framework and therefore arguably one of the best resources for future researchers to use to implement new algorithms.\n5. *\u201cA more well-rounded evaluation could be achieved by integrating additional qualitative and quantitative metrics.\u201d*\n* Determining the boundaries for the different datasets (Good, Medium, Poor) required an in depth qualitative analysis of the various environments and how return related to agent skills. Unfortunately we opted not to include the details in the final writeup because we thought it would not be important to practitioners using our datasets. However, we see now that in fact the details may be valuable and have therefore added them in the appendix and we kindly invite the reviewer to read this analysis.\n\n[1] Gulcehre, Caglar, et al. \u201cRl unplugged: A suite of benchmarks for offline reinforcement learning.\u201d NeurIPS 2020\n\n[2] Fu, Justin, et al. \u201cD4rl: Datasets for deep data-driven reinforcement learning.\u201d arXiv pre-print 2020\n\n[3] Kumar, Aviral, et al. \"Conservative q-learning for offline reinforcement learning.\" NeurIPS 2020\n\n[4] Kostrikov, Ilya, Ashvin Nair, and Sergey Levine. \"Offline Reinforcement Learning with Implicit Q-Learning.\" ICLR. 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222813984,
                "cdate": 1700222813984,
                "tmdate": 1700222813984,
                "mdate": 1700222813984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XQYpfOzKIA",
                "forum": "VIEbRFp6s3",
                "replyto": "8ghn0K18q8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5706/Reviewer_YX5P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5706/Reviewer_YX5P"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for answering my questions and resolving my concerns. I have raised my scores because of the clarifications on the AAMAS paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515862862,
                "cdate": 1700515862862,
                "tmdate": 1700515862862,
                "mdate": 1700515862862,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XTIBKGsNUz",
            "forum": "VIEbRFp6s3",
            "replyto": "VIEbRFp6s3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_4ULc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_4ULc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes datasets for offline multi-agent reinforcement learning including games(real world problems) with both discrete and continuous actions. The paper also provide different types of the datasets: Good, Medium, Poor. Evaluation results of offline multi-agent reinforcement learning baselines are provided."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The dataset for offline multi-agent reinforcement learning is missing, which is important for this community. 2.The paper provides a comprehensive dataset including both games and real world problem, both discrete and continuous. 3.The paper is well written."
                },
                "weaknesses": {
                    "value": "1.The major concern of the paper is the correctness of the implementation of the baselines. OMAR definitely outperforms CQL in many tasks, as reported in \"Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning https://arxiv.org/abs/2307.01472\". However, this is not true in Table D.5. As a dataset and benchmark paper, I think it's crucial to ensure that the results are replicable and the claims made for previous baselines are correct.\n2.The paper does not provide explanations on why an algorithm outperforms another algorithm."
                },
                "questions": {
                    "value": "See the above section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698321141758,
            "cdate": 1698321141758,
            "tmdate": 1699636597316,
            "mdate": 1699636597316,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oABAHGddj0",
                "forum": "VIEbRFp6s3",
                "replyto": "XTIBKGsNUz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On the performance of OMAR"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback.\n\n*\u201cOMAR definitely outperforms CQL in many tasks, as reported in \"Beyond \nConservatism \u2026\u201d*\n\nWe are confident that our implementation is correct as it closely resembles the results reported by Barde et al. (2023) \u201cA Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem\u201d (https://arxiv.org/pdf/2305.17198.pdf). Here the authors evaluate OMAR on two new continuous action settings, namely Reacher (Table 2) and Ant (Table 3). In their results the authors show that the performance of ICQL and OMAR are very similar, with OMAR\u2019s mean performance only marginally superior to ICQL in 7 out of 11 settings and their uncertainty estimates overlapping in all but one scenario. Furthermore, both OMAR and ICQL are outperformed by ITD3+BC and BC in all but one setting, and usually by some margin. These results closely resemble our reported finding, namely that the performance of ICQL and OMAR are very similar but significantly worse than ITD3+BC and BC on all tested continuous action settings.\n\nThe original OMAR work and the work from the reviewer\u2019s cited paper share authors and therefore do not provide an independent verification of OMAR\u2019s performance. To the best of our knowledge, the paper we cite above, represents the only independent study that uses OMAR, and this work corroborates our findings in terms of the performance of OMAR compared to other algorithms such as ICQL, ITD3+BC and BC.\n\nIn addition, we will add the following remark to the appendix to clarify why there might be a discrepancy in performance. *\u201cWe could not make OMAR or ICQL perform well on our tasks. We are unsure if this is because these algorithms perform poorly on our specific tasks, or if our implementations are missing an important detail. But since our results closely resemble the results reported by Barde et al. (2023), an independent work to the original OMAR paper, we decided to include them.\u201d*\n\nWe trust we have provided enough evidence to validate our reported results using   OMAR and request the reviewer to kindly consider improving their score or provide us with their reason for why this is not possible."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221440200,
                "cdate": 1700221440200,
                "tmdate": 1700221440200,
                "mdate": 1700221440200,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6TC8h2r34W",
            "forum": "VIEbRFp6s3",
            "replyto": "VIEbRFp6s3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_DuA2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_DuA2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed off-the-grid MARL (OG-MARL) datasets with baselines for cooperative offline MARL. The datasets provide settings include complex environment dynamics, heterogeneous agents, non-stationarity, many agents, partial observability, suboptimality, sparse rewards and demonstrated coordination. The OG-MARL provides a range of different dataset types and profiles the composition of experiences for each dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper proposed the datasets of offline MARL by extending the idea of single-agent offline RL datasets such as D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020). The datasets provide settings include complex environment dynamics, heterogeneous agents, non-stationarity, many agents, partial observability, suboptimality, sparse rewards and demonstrated coordination.\n- This paper also provided baselines for existing cooperative offline MARL such as Behaviour Cloning (BC), QMIX (Rashid et al., 2018), QMIX with Batch Constrained Q-Learning (Fujimoto et al., 2019), QMIX with Conservative Q-Learning (Kumar et al., 2020) and MAICQ (Yang et al., 2021). The results concluded that on PettingZoo environments, with pixel observations, MAICQ is the current state-of-the-art offline MARL algorithm in discrete action settings.\n- The paper is well-written and mostly has clarity."
                },
                "weaknesses": {
                    "value": "Although this paper includes a novelty about MARL extension from single-agent RL datasets and baselines, other points seem to be ordinary.  More challenging benchmarks and more real-world scenarios, might provide more significance, as described below. \nThere were also some unclear points described below."
                },
                "questions": {
                    "value": "1. P6: The authors said that \u201cWe chose these environments because they have visual (pixel-based) observations of varying sizes; an important dimension along which prior works have failed to evaluate their algorithms\u201d. What are the prior works specifically and why did they fail the evaluation?\n\n2. For human data (KAZ), the detailed description will be described because humans have diversity and usually the property of the human participants (e.g., age and the game experience) should be reported. If possible, comparison with the data from RL algorithms will estimate the property of human data.\n\n3. The paper mentioned about KAZ that \u201cThe players where given no instruction on how to play the game and had to learn through trial and error.\u201d but does it mean the data may include not only \u201clearned\u201d data but also \u201clearning\u201d data? The data acquisition process can be clarified.  \n\n4. More challenging MARL benchmarks such as team sports (e.g., [1] [2]) or more real-world robotics data might provide more significance. \n[1] Kurach et al. Google Research Football: A Novel Reinforcement Learning Environment, AAAI, 2020\n[2] Liu et al. From motor control to team play in simulated humanoid football, Science Robotics, 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698499865571,
            "cdate": 1698499865571,
            "tmdate": 1699636597206,
            "mdate": 1699636597206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XgtTNoGvr0",
                "forum": "VIEbRFp6s3",
                "replyto": "6TC8h2r34W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback on our work.\n1. *\u201cWhat are the prior works specifically and why did they fail the evaluation?\u201d*\n* Due to space constraints we could only include a subset of all the baseline experiments we ran in the main text. All the baselines on the other environments were included in the Appendix. We chose to include the experiments on environments with pixel-based observations in the main text because prior offline MARL [1,2,3] works had not used such environments and in [4] the authors emphasize that benchmarks on pixel-based observations have been lacking in the broader offline RL literature.\n2. *\u201c... does it mean the data may include not only \u201clearned\u201d data but also \u201clearning\u201d data?\u201d*\n* Yes. To make sure the human-generated dataset included sufficiently diverse data we opted to include \u201clearning\u201d data.\n3. *\u201cMore challenging MARL benchmarks such as team sports \u2026 might provide more significance.\u201d*\n* We agree that adding additional environments such as Google Football would be valuable. However, our priority has been to initially support all of the most popular MARL benchmarks including SMAC v1 & v2, MPE, MAMuJoCo and PettingZoo. Having said that, the data recorder provided in OG-MARL is flexible enough for researchers to use on a wide range of currently unsupported environments with minimal effort. Please see the tutorial we provided in the OG-MARL code for how to record data in a new environment.\n\n[1] Jiechuan Jiang, et al. \u201cOffline Decentralized Multi-Agent Reinforcement Learning.\u201d arXiv Pre-Print\n\n[2] Ling Pan, et al. \u201cPlan Better Amid Conservatism: Offline Multi-Agent Reinforcement Learning with Actor Rectification.\u201d NeurIPS \n\n[3] Yang et al. \u201cBelieve What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning.\u201d NeurIPS 2021\n\n[4] Cong Lu, et al. \u201cChallenges and opportunities in offline reinforcement learning from visual observations.\u201d ICML 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221432884,
                "cdate": 1700221432884,
                "tmdate": 1700221432884,
                "mdate": 1700221432884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6J9W0rkYFl",
                "forum": "VIEbRFp6s3",
                "replyto": "XgtTNoGvr0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5706/Reviewer_DuA2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5706/Reviewer_DuA2"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the reply"
                    },
                    "comment": {
                        "value": "Thank you for the reply and clear comment about your third response (and my fourth question).\n\n> 1. \u201cWhat are the prior works specifically and why did they fail the evaluation?\u201d\nAccording to your comment, the prior work did not evaluate them, but we do not know whether they failed the evaluation or not. If so, I recommend modifying the paper according to your response. \n\n> 2. \u201c... does it mean the data may include not only \u201clearned\u201d data but also \u201clearning\u201d data?\u201d\nYou clarified this point, but did not clarify my second question. As I wrote in my third question, the data acquisition process should be clarified."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688181358,
                "cdate": 1700688181358,
                "tmdate": 1700688181358,
                "mdate": 1700688181358,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ic1NIc47kE",
            "forum": "VIEbRFp6s3",
            "replyto": "VIEbRFp6s3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_UqF2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_UqF2"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the OG-MARL openly available offline datasets and baselines for MARL. The datasets cover a range of scenarios, including micromanagement in StarCraft 2, continuous control in MAMuJoCo, diverse environments in PettingZoo, train scheduling in Flatland, and energy management in Voltage Control/CityLearn. The paper tries to address the lack of benchmark datasets and baselines in offline MARL and aims to facilitate research and comparison of MARL algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper addresses the lack of commonly shared benchmark datasets and baselines in the field of offline MARL.\n- the proposed data contains data on a large collection of various MARL environments, including SMAC, MAMuJoCo, PettingZoo, Flatland, and Voltage Control/CityLearn. \n- the authors provide detailed descriptions of the different environments and datasets, including information about the composition of the datasets and visualizations of the behavior policy."
                },
                "weaknesses": {
                    "value": "- it would be beneficial to include performance comparisons with more existing algorithms and baselines on the provided datasets. For instance, federated offline MARL, etc\n- it would be beneficial if the paper could provide a list of the size of the data and the approximate amount of computation resources required for training the baseline.\n- it seems the dataset has fewer scenarios with competitive case, adding more competitive datasets would probably be helpful to make it more general."
                },
                "questions": {
                    "value": "- For different levels of data (good, medium, etc), how do you make sure that the dataset contains a wide variety of experiences and is not biased to a certain type of policy?\n- Based on the C.1 paper, is the size of the dataset sufficiently large for large-scale experiments such as federated offline MARL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5706/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783722551,
            "cdate": 1698783722551,
            "tmdate": 1699636597095,
            "mdate": 1699636597095,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bEzzIa68Jx",
                "forum": "VIEbRFp6s3",
                "replyto": "ic1NIc47kE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback on our work.\n1. *\u201c...it would be beneficial to include performance comparisons with more existing algorithms\u2026\u201d*\n* We kindly ask the reviewer to point us to any published works in offline MARL we may have missed, we will gladly add these to our related work section. We are unfortunately not familiar with federated offline MARL. It is however our hope that OG-MARL becomes a living and growing project where the community uses the framework we provide to add new baselines and datasets in the future.\n2. *\u201c...adding more competitive datasets would probably be helpful\u2026\u201d*\n* The lack of additional competitive multi-agent scenarios is reflective of the fact that the  offline MARL community has primarily  been focused on the cooperative setting given its potential real-world applicability. Competitive offline MARL research has largely been theoretical [1,2,3]. We hope the competitive dataset we provided, the easy-to-use data recorder and our step-by-step tutorial on how to use it, will encourage other researchers to make future contributions of competitive datasets.\n3. *\u201c\u2026 how do you make sure that the dataset contains a wide variety of experiences \u2026\u201d*\n* For each dataset, we used 4 independently trained systems of policies to rollout and record experiences. Additionally, we added a certain amount of random exploration noise to each policy (epsilon-greedy in discrete action environments and clipped Gaussian noise in continuous action environments. We verified that our datasets were sufficiently diverse by inspecting the violin plots of the distribution of episode returns in the datasets and qualitatively inspecting recordings of the trajectories. We will add the details regarding this qualitative analysis to the appendix of the paper.\n4. *\u201c \u2026 is the size of the dataset sufficiently large for large-scale experiments such as federated offline MARL?\u201d*\n* We have demonstrated that the datasets are sufficiently large for the presented baseline algorithms. However, we are not familiar with federated offline MARL and can therefore not say for sure. However, the data recorder provided in OG-MARL can be leveraged by the research community to craft their own datasets at any desired scale.\n\n[1] Cui, Qiwen, et al. \u201cWhen are Offline Two-Player Zero-Sum Markov Games Solvable?\u201d \nNeurIPS 2022.\n\n[2] Cui, Qiwen, et al. \"Provably efficient offline multi-agent reinforcement learning via strategy-wise bonus.\" NeurIPS 2022\n\n[3] Yan, Yuling, et al. \"Model-based reinforcement learning is minimax-optimal for offline zero-sum markov games.\" arXiv preprint."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221425411,
                "cdate": 1700221425411,
                "tmdate": 1700221425411,
                "mdate": 1700221425411,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iwqk8H58o0",
            "forum": "VIEbRFp6s3",
            "replyto": "VIEbRFp6s3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_KxKX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5706/Reviewer_KxKX"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces Off-the-Grid Multi-Agent Reinforcement Learning (OG-MARL), a repository aiming to address the lack of standardized benchmark datasets and baselines in the emerging field of offline multi-agent reinforcement learning (MARL). The motivation is to leverage large datasets from real-world industrial systems, where distributed processes can be recorded during operation. The provided datasets in OG-MARL exhibit characteristics of complex real-world environments, including partial observability, suboptimality, demonstrated coordination, etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Benchmarking is essential in machine-learning communities as well as multi-agent learning communities.\n- This benchmark contains a variety of settings in multi-agent, such as team & individual rewards and homogeneous & heterogeneous agents.\n- This paper is well-written to some extent."
                },
                "weaknesses": {
                    "value": "- An explanation and comprehensive analysis of the baselines tested on the proposed dataset should be provided as well.\n- Is there any measurement of the diversity of the trajectories in the dataset?\n- Please clarify the difference between this work and another recent work [1].\n\n[1] Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning, AAMAS 2023"
                },
                "questions": {
                    "value": "Please refer to the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5706/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5706/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5706/Reviewer_KxKX"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5706/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699736985588,
            "cdate": 1699736985588,
            "tmdate": 1699736985588,
            "mdate": 1699736985588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QNVgDWQ1Vp",
                "forum": "VIEbRFp6s3",
                "replyto": "iwqk8H58o0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5706/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their time and effort in providing feedback on our work. \n1. *\u201cAn explanation and comprehensive analysis of the baselines tested on the proposed dataset should be provided as well.\u201d*\n* As with related offline RL dataset publications RL Unplugged [1] and D4RL [2], providing detailed theoretical or empirical evidence for a discussion on why one baseline outperforms another is not the focus of this work. The purpose of including baselines was so that future works can easily compare their algorithms to baselines on our datasets. We agree however that a deeper analytical analysis is a valuable direction for future work.\n2. *\u201cIs there any measurement of the diversity of the trajectories in the dataset?\u201d*\n* As in prior works, we used the spread of episode returns as a proxy for diversity. However, unlike most prior works which typically only report the standard deviation of episode returns in the dataset, we proposed visualizing the distribution of episode returns using violin plots, shedding significantly more light on the diversity of trajectories in the datasets. Having said that, designing a better metric to measure the diversity of trajectories is an important open problem in the offline RL literature and a good direction for future work. \n3. *\u201cPlease clarify the difference between this work and another recent work\u2026\u201d*\n* The AAMAS version of Off-the-Grid MARL was an extended abstract and therefore non-archival and has not been published in any proceedings. Our work includes several new environments (SMACv2, CityLearn, KAZ, MPE and Voltage Control). Additionally, we added a dataset generated from human players and a competitive MARL dataset.\n\n[1] Gulcehre, Caglar, et al. \u201cRl unplugged: A suite of benchmarks for offline reinforcement learning.\u201d NeurIPS 2020\n\n[2] Fu, Justin, et al. \u201cD4rl: Datasets for deep data-driven reinforcement learning.\u201d arXiv pre-print 2020"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5706/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221414330,
                "cdate": 1700221414330,
                "tmdate": 1700221414330,
                "mdate": 1700221414330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]