[
    {
        "title": "Scalable and Effective Implicit Graph Neural Networks on Large Graphs"
    },
    {
        "review": {
            "id": "SDQVdDMMZw",
            "forum": "QcMdPYBwTu",
            "replyto": "QcMdPYBwTu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5011/Reviewer_m8F2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5011/Reviewer_m8F2"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to address the scalability and efficiency challenges associated with implicit Graph Neural Networks (GNNs), which are designed to capture long-range dependencies in graph-structured data. It first points out that the implicit GNNs are suffering from the computational burden brought by (1) full-batch training and (2) a large number of iterations to solve fixed-point equations, limiting their applicability to large graphs. This work proposes a scalable and effective SEIGNN model that employs mini-batch training with coarse-level nodes to encourage information propagation between subgraphs. Extensive experiments are conducted to justify the power of SEIGNN in terms of both efficiency and efficacy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work is clearly motivated, addressing the scalability issue of IGNN is an important problem.\n\n2. Good performance and comprehensive ablation study.\n\n3. Writing is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "Please refer to ``Questions``."
                },
                "questions": {
                    "value": "1. I agree that subgraph-wise sampling (ClusterGCN) and node-wise sampling (GraphSage) indeed lead to a loss of long-range dependencies. However, how about layer-wise sampling, such as the approaches like LADIES[1] and fastGCN[2]? \n\n2. I want to confirm my understanding. In subgraph sampling, the coarse nodes are excluded from being the target nodes but will be included in the sampled subgraph through the top-k Personalized PageRank (PPR) selection step, is that correct?\n\n3. Given that the purpose of introducing the coarse nodes is to maintain long-term dependencies in mini-batch training, can SEIGNN ensure that coarse nodes are included in each mini-batch? Or is there a way to encourage the sampled subgraph to include more coarse nodes?\n\n\n[1] https://arxiv.org/abs/1911.07323\n[2] https://arxiv.org/abs/1801.10247"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697489419268,
            "cdate": 1697489419268,
            "tmdate": 1699636489717,
            "mdate": 1699636489717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x4irHVEQZq",
                "forum": "QcMdPYBwTu",
                "replyto": "SDQVdDMMZw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer m8F2"
                    },
                    "comment": {
                        "value": "We appreciate reviewer m8F2 for the valuable feedback and suggestions. We would like to clarify the questions as below. \n\n> **Q1:** I agree that subgraph-wise sampling (ClusterGCN) and node-wise sampling (GraphSage) indeed lead to a loss of long-range dependencies. However, how about layer-wise sampling, such as the approaches like LADIES[1] and fastGCN[2]?\n\nFor layer-wise sampling, it would still lose the ability to capture long-range dependencies as node-wise sampling. The major difference between layer-wise and node-wise sampling is that layer-wise sampling fixes the number of overall sampled nodes **in a layer** to a certain number while node-wise sampling fixes the number of sampled neighbors for **each node**. Thus, layer-wise sampling may have even more restricted sampled minibatches and face a similar problem of losing long-range dependencies as node-wise sampling.\n\n> **Q2:** I want to confirm my understanding. In subgraph sampling, the coarse nodes are excluded from being the target nodes but will be included in the sampled subgraph through the top-k Personalized PageRank (PPR) selection step, is that correct?\n\nYes. Coarse nodes will be included in the sampled subgraph through top-k PPR selection.\n\n> **Q3:** Given that the purpose of introducing the coarse nodes is to maintain long-term dependencies in mini-batch training, can SEIGNN ensure that coarse nodes are included in each mini-batch? Or is there a way to encourage the sampled subgraph to include more coarse nodes?\n\nCurrently, SEIGNN does not directly force coarse nodes to be included in each mini batch. However, coarse nodes can be selected as they may have higher Personalized PageRank (PPR) scores. Which auxiliary nodes are selected is determined by PPR scores, indicating the more relevant nodes are more likely to be selected. \n\nSpecifically for coarse nodes, it's also possible to encourage the sampled subgraph to have more coarse nodes. For example, we directly add all coarse nodes that are 1-hop neighbors of the target nodes into the sampled subgraph. Or more extremely, we can even force all coarse nodes included in the mini-batch when global information is very important for a specific dataset. Although it is interesting to explore further, we leave it as future work due to time limitations. \n\n---\nThank you again for your valuable feedback. Hope our response answers your questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555873149,
                "cdate": 1700555873149,
                "tmdate": 1700555873149,
                "mdate": 1700555873149,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Rb8xWTSEU3",
            "forum": "QcMdPYBwTu",
            "replyto": "QcMdPYBwTu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5011/Reviewer_K2Aq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5011/Reviewer_K2Aq"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a scalable and effective implicit graph neural network (GNN) model, called SEIGNN, that can handle large graph data. Implicit GNNs are models that can capture long-range dependencies in graphs by solving a fixed-point equation. However, they are computationally expensive and memory-intensive, due to their use of full-batch training and a large number of iterations. SEIGNN addresses these limitations by using a mini-batch training method with coarse nodes and a stochastic solver. The coarse nodes are added to the mini-batch to incorporate global information from the graph, and the stochastic solver is used to obtain unbiased approximate solutions with fewer iterations. SEIGNN can be integrated with any existing implicit GNN model to improve its performance. The paper evaluates SEIGNN on six datasets and shows that it achieves better accuracy with less training time than existing implicit GNNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper tackles a novel and important problem of scaling up implicit GNNs to large graphs, which has applications in real-world massive graphs.\n* The authors introduce a mini-batch sampling strategy that can preserve the global and long-range information of the graph by using coarse nodes and can achieve reduced training time by using a stochastic solver.\n* Extensive experiments and analysis are conducted to demonstrate the advantages of the proposed framework in mini-batch training."
                },
                "weaknesses": {
                    "value": "* The paper fails to position itself with recent related works. More SOTA papers in mini-batch training strategy should be included for discussion, such as [1-3]. The idea of introducing coarse nodes during the process of mini-batching is relatively straightforward and appears in existing literature such as [3]. Therefore, it hurts the novelty of this work regarding the first contribution without a thorough comparison with the current mini-batching GNN methods that also include coarse nodes. Meanwhile, there is a very relevant and similar work that should be added for comparison [4].\n\n* For the second contribution of accelerated training with the Neumann solver, it is unclear how the authors constrain $n$ to be sufficiently large during experiments. In other words, does the performance of SEIGNN be affected even if the proposed stochastic solver is not an unbiased estimator? Could the authors elaborate more on this? \n\n* The setting of the experiments could be improved to better reflect the contributions. Do the results for all implicit GNNs come from full-batch training on all benchmarks? What would performance be if we incorporated different mini-batching sampling strategies for the baseline implicit GNNs? Meanwhile, more recent baselines of scalable GNN should be added for comparison such as [5-6].\n\n* This manuscript aims to scale implicit GNNs to large graphs, but the benchmarks have nearly become standard for even some GNNs that are not specifically designed for scalability. Therefore, large graphs such as ogbn-papers100M should be evaluated for the scalability aim. Note that even the largest dataset (ogbn-products) used in this manuscript is marked as medium in OGB benchmarks.\n\n[1] VQ-GNN: A Universal Framework to Scale-up Graph Neural Networks using Vector Quantization, NeurIPS 2021\n\n[2] Influence-Based Mini-Batching for Graph Neural Networks, LoG 2022\n\n[3] SMGRL: Scalable Multi-resolution Graph Representation Learning, arXiv 2022\n\n[4] Efficient and Scalable Implicit Graph Neural Networks with Virtual Equilibrium, Big Data 2022\n\n[5] Decoupling the Depth and Scope of Graph Neural Networks, NeurIPS 2021\n\n[6] Sketch-GNN: Scalable Graph Neural Networks with Sublinear Training Complexity, NeurIPS 2022"
                },
                "questions": {
                    "value": "Please kindly refer to the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698314743397,
            "cdate": 1698314743397,
            "tmdate": 1699636489614,
            "mdate": 1699636489614,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1iTGpOIbn1",
                "forum": "QcMdPYBwTu",
                "replyto": "Rb8xWTSEU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K2Aq [1/2]"
                    },
                    "comment": {
                        "value": "[Part1 / 2] We appreciate reviewer K2Aq for the valuable feedback and suggestions. However, we respectfully disagree with some comments and we believe there might be some misunderstandings. We would like to clarify and answer the questions as below. \n\n---\n> **W1:** The paper fails to position itself with recent related works. \n\nFirst, we would like to emphasize again that our submission mainly focuses on scalable **implicit** GNNs instead of scalable traditional/explicit GNNs. Therefore, for scalable traditional/explicit GNNs, we only cover/discuss some well-known sampling-based works such as GraphSAGE, ClusterGCN, GraphSAINT as in USP [1]. \n\nFor VQ-GNN [2], it uses quantization to scale up GNNs, which is a different direction for improving scalability compared with sampling-based works.\n\nFor SMGRL [3], in our humble opinion, it actually does not add coarse nodes for training. It directly trains on different coarse graphs rather than considering both original nodes and coarse graphs for training simultaneously. Additionally SMGRL is for explicit GNNs, which is not our main focus.\n\nFor VEQ [4] which is published in Big Data conference 2022, we agree that it is relevant and we apologize that we did not notice as it is not publicly available on Arxiv (conference proceedings are also not available without purchase). \nComparing VEQ and our work SEIGNN, VEQ utilizes previous equilibrium states as an initialization for speeding up training while our work proposed a new solver to obtain unbiased approximation of equilibrium. For mini-batch training, VEQ has specific designs for update while our work SEIGNN is compatible with different sampling methods as shown in Table 6. Moreover, in terms of accuracy, we observe that SEIGNN is better than VEQ on some datasets such as Flickr, Reddit, Yelp, and ogbn-arxiv. We will include above discussions/comparisons into a later version of our manuscript.\n\n[1] Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States, ICLR 2023.\n\n[2] VQ-GNN: A Universal Framework to Scale-up Graph Neural Networks using Vector Quantization, NeurIPS 2021.\n\n[3] SMGRL: Scalable Multi-resolution Graph Representation Learning, arXiv 2022.\n\n[4] Efficient and Scalable Implicit Graph Neural Networks with Virtual Equilibrium, Big Data 2022.\n\n---\n> **W2:** For the second contribution of accelerated training with the Neumann solver, it is unclear how the authors constrain \nto be sufficiently large during experiments. In other words, does the performance of SEIGNN be affected even if the proposed stochastic solver is not an unbiased estimator? Could the authors elaborate more on this?\n\nThe reviewer's question is how we set $n$ to be sufficiently large in experiments, to ensure it to be unbiased\u00a0and what would happen if it is not unbiased with $n$ not sufficiently large.\n\nWe believe that it is a misunderstanding of our proof in Appendix A.1 and Proposition 1. $n$ in A.1 is a theoretical concept and not a real value usable in experiments. In experiment, we are always using an algorithm with $n = \\infty$. Thus, the proposed stochastic are always ensured to be unbiased (either up to all terms when the assumption holds **OR** up to machine precision when the assumption does not hold).\n\nWe have revised the proof in the appendix (highlighted in blue) and clarified that `n` is a theorectical concept and in our empirical experiments $n = \\infty$.\n\n---\n> **W3:** The setting of the experiments could be improved to better reflect the contributions. Do the results for all implicit GNNs come from full-batch training on all benchmarks? What would performance be if we incorporated different mini-batching sampling strategies for the baseline implicit GNNs? Meanwhile, more recent baselines of scalable GNN should be added for comparison such as [5-6].\n\nFor our experimental results in the submission, USP uses mini-batch training as in their paper and IGNN and MGNNI use full-batch training. \n\nWe incorporated different mini-batch sampling methods for the baseline implicit GNNs. The results on Flickr are following: \n| |SEIGNN| IGNN + GraphSAGE | MGNNI + GraphSAGE | IGNN + ClusterGCN | MGNNI + ClusterGCN | IGNN + full batch | MGNNI + full batch |\n|--|--|--|--|--|--|--|--|\n|Accuracy|0.5575|0.5203|0.5219|0.5259|0.5278|0.5302|0.5352|\n\nWe can see that baseline implicit GNNs with different mini-batch sampling are slightly worse than using full-batch training, and our method SEIGNN outperforms them. \n\nFor more recent baselines of scalable GNNs, We would like to mention again that our goal is not beating all scalable GNNs. Instead, we focus on designing more  scalable, effective, and efficient **implicit** GNNs along with more insights on this topic. We believe that this direction is valuable and our submission may be a valuable step towards scale **implicit** GNNs."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566917616,
                "cdate": 1700566917616,
                "tmdate": 1700566917616,
                "mdate": 1700566917616,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3RkP9Cc8E3",
                "forum": "QcMdPYBwTu",
                "replyto": "Rb8xWTSEU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K2Aq [2/2]"
                    },
                    "comment": {
                        "value": "[Part 2/2]\n> **W4:** This manuscript aims to scale implicit GNNs to large graphs, but the benchmarks have nearly become standard for even some GNNs that are not specifically designed for scalability. Therefore, large graphs such as ogbn-papers100M should be evaluated for the scalability aim. Note that even the largest dataset (ogbn-products) used in this manuscript is marked as medium in OGB benchmarks.\n\nFor choosing datasets in experiments, we mainly follow USP [1] and use ogbn-products as the largest dataset in our experiments. We agree that ogbn-products might be standard for traditional/explicit GNNs. However, it is not easy for implicit GNNs which previously only did full-batch training before USP [1]. \n\nogbn-papers100M is a very large graph with 100M nodes and only a few decoupling-based traditional/explicit GNNs can work on it. Even for explicit GNNs, training efficiency on ogbn-papers100M is also a problem. We agree that the aim to scale up implicit GNNs to handle graphs with 100M nodes is important. However, it requires further improved training efficiency for implicit GNNs, which we will leave for future work.\n\n[1] Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States, ICLR 2023.\n\n\n----\nThank you again for your valuable feedback. Hope our response answers your questions and we are willing to discuss/clarify further if you have any other questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567077053,
                "cdate": 1700567077053,
                "tmdate": 1700677194853,
                "mdate": 1700677194853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XepZAT8BK8",
                "forum": "QcMdPYBwTu",
                "replyto": "Rb8xWTSEU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for the closing Author/Reviewer discussion period"
                    },
                    "comment": {
                        "value": "Dear Reviewer K2Aq,\n\nWe would like to thank you again for your helpful review of our work. As the deadline for discussion draws near, we are eager to hear whether our response has addressed your concerns and any other feedback you may have. We are willing to discuss/clarify further if you have any remaining concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675608712,
                "cdate": 1700675608712,
                "tmdate": 1700675608712,
                "mdate": 1700675608712,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R0mHpl1OKw",
                "forum": "QcMdPYBwTu",
                "replyto": "XepZAT8BK8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Reviewer_K2Aq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Reviewer_K2Aq"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thank you for your response and the additional discussions. \n\n* The comparison with VEQ is important and essential to demonstrate the novelty of SEIGNN. In my opinion, conference proceedings are also not available without purchase is not a reason for not aware of VEQ since most of researchers can access it through their institutions. \n\n* Given scalability is the main focus and contribution of this paper, the evaluation on large-scale graphs with billions of nodes (such as ogbn-papers100M) should be taken, otherwise the industrial application of SEIGNN on practical scenarios is likely impossible. \n\n* It is still valuable to compare with the SOTA scalable GNNs, since it could help understand the gap between implicit GNNs and regular GNNs on large-scale graphs and motivate further researches. \n\nDue to these concerns, I choose to maintain my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725118427,
                "cdate": 1700725118427,
                "tmdate": 1700725118427,
                "mdate": 1700725118427,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jGKKqItFHf",
            "forum": "QcMdPYBwTu",
            "replyto": "QcMdPYBwTu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5011/Reviewer_4DC8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5011/Reviewer_4DC8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a scalable and effective implicit GNN (SEIGNN) with a mini-batch training method. Experiments demonstrate SEIGNN outperforms the state-of-the-art implicit GNNs on various large graphs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. SEIGNN is fast and memory-efficient.\n2. The accuracy of SEIGNN on the large datasets is significantly higher than the state-of-the-art implicit GNNs."
                },
                "weaknesses": {
                    "value": "My major concern is the reproducibility of this paper.\n\n1. Table 4 shows that the improvement of SEIGNN is due to the coarse nodes. However, the authors do not analyze why the coarse nodes improve the prediction performance.\n2. The authors do not provide the codes for reproducibility."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642598550,
            "cdate": 1698642598550,
            "tmdate": 1699636489543,
            "mdate": 1699636489543,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z4UQPvzsdL",
                "forum": "QcMdPYBwTu",
                "replyto": "jGKKqItFHf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4DC8"
                    },
                    "comment": {
                        "value": "We appreciate reviewer 4DC8 for the valuable feedback and suggestions. We would like to clarify the questions as below. \n\nFor the reason why the coarse nodes improve the prediction performance, we have provided some analyses with Figure 3 and the related text descriptions/explanations on Page 9. It shows that coarse nodes can improve performance more for low-degree nodes compared with relatively high-degree nodes. Coarse nodes help facilitate long-range information propagation which helps low-degree nodes to receive sufficient information. \n\nWe agree that more in-depth analyses from different perspectives would be an interesting direction to explore further. For example, the shortest distances between two nodes have been changed by adding coarse nodes and how it may make differences in terms of performance. However, due to time limitations, we leave it for future work. \n\nFor the code implementation, we agree that reproducibility is important for the community and we will definitely make the implementation publicly available upon acceptance of our submission.\n\nWe would like to thank you again for your valuable feedback. Hope our response answers your questions and we are willing to discuss/clarify further if you have any other questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503799298,
                "cdate": 1700503799298,
                "tmdate": 1700503799298,
                "mdate": 1700503799298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yBib9UDh8F",
            "forum": "QcMdPYBwTu",
            "replyto": "QcMdPYBwTu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5011/Reviewer_Jpev"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5011/Reviewer_Jpev"
            ],
            "content": {
                "summary": {
                    "value": "The authors identify 2 problems with mini-batching implicit GNNs:\n\n1) The traditional minibatches formed for GNN training is often done through sampling which ignores long-range dependency nodes.\n\n2) Implicit GNNs take a long time to converge, hurting scalability.\n\nThe authors propose 2 solutions to said problems.\n\n1) The authors propose adding coarse nodes between minibatch subgraphs to facilitate long-range message propagation during training (when combined with standard techniques like GraphSAGE)\n\n2) The authors extend Neumann series, a implicit GNN solver proposed in [1], by making it a unbiased stochastic solver.\n\nThe authors provide experiments and ablations to show their minibatch sampling method is superior to existing works.\n\n[1] Eignn: Efficient infinite-depth graph neural networks"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors provide a compelling argument that their augmentations to mini-batch sampling and implicit GNN solving can improve the scalability of implicit GNNs. \n\n- The authors identified a compelling issue with existing mini-batch approaches.\n\n- The solutions the authors provide are simple, yet intuitive. \n\n- The paper is written clearly and is easy to follow.\n\n- The authors provide timing experiments to empirically verify SEIGNN's scalability.\n\n- The authors try various different minibatch methods with the coarse nodes.\n\n- The method is targeted towards implicit GNNs, which is a subset of all possible GNNs (though the authors demonstrate the method generalizes to other GNNs)"
                },
                "weaknesses": {
                    "value": "To be fully convinced, I have a few more questions regarding the approach:\n\n- [important] On datasets where full-batch training is available, what is the performance trade-off of using SEIGNN over the full implicit GNN and a naive subsampling of the full-batch?\n\n- How does modifying the sizes of the minibatch subgraphs affect the efficacy of SEIGNN's coarse nodes?\n\n- How important is using PPR for the coarse node idea? Could SEIGNN generalize to other importance metrics?\n\n- The method is targeted towards implicit GNNs, which is a subset of all possible GNNs (though the authors demonstrate the method generalizes to other GNNs)"
                },
                "questions": {
                    "value": "In addition to the weaknesses section, I have a few more questions:\n\n- Have you considered adding multiple coarse nodes in between mini-batch subgraphs? What would be the effect of this?\n\n- What would be the effect of removing/keeping the coarse nodes post-training?\n\n- Why is the stochastic solver slightly better in performance than the Neumann solver in Table 4 of the ablation studies? Is the Neumann solver a truncated version here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5011/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5011/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5011/Reviewer_Jpev"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781322860,
            "cdate": 1698781322860,
            "tmdate": 1700675773157,
            "mdate": 1700675773157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eQqRgfXkNc",
                "forum": "QcMdPYBwTu",
                "replyto": "yBib9UDh8F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jpev [1/2]"
                    },
                    "comment": {
                        "value": "[Two parts of response: 1/2] We appreciate reviewer Jpev for the valuable feedback and suggestions. We would like to clarify and answer the questions as below. \n> **W1:** [important] On datasets where full-batch training is available, what is the performance trade-off of using SEIGNN over the full implicit GNN and a naive subsampling of the full-batch?\n\nThanks for the good question. We would like to first point out that the results of baseline implicit GNNs in Table 1 of our submission is using full-batch training. \n\nTo answer the question, we conduct the experiments on Flickr dataset to compare SEIGNN with 1) baseline implicit GNNs with full-batch training, 2) baseline implicit GNNs with random sampling of a full batch, 3) baseline implicit GNNs with GraphSAGE (neighbor sampling). \n\n| | SEIGNN | IGNN + full batch | MGNNI + full batch |IGNN + random sampling | MGNNI + random sampling | IGNN + GraphSAGE | MGNNI + GraphSAGE |\n|:---:|:------:|:--:|:--:|:--:|:--:|:--:|:--:|\n|Accuracy|0.5575|0.5302|0.5352|0.4971|0.5008|0.5203|0.5219|\n\nFrom these results, we can see SEIGNN generally outperforms baseline implicit GNNs with different variants. Comparing full-batch version and mini-batch version of implicit GNNs, full-batch version usually performs better and neighbor sampling version is better than naive random sampling. \n\nBesides, we conduct experiments to show the results of SEIGNN with our designed mini-batch + coarse nodes, SEIGNN with full-batch training, and SEIGNN with sampling methods. \n\n| | SEIGNN (with our mini-batch) | with full-batch | with GraphSAGE sampling | with random sampling |  \n|--|--|--|--|--|\n|Accuracy| 0.5575 |0.5392 | 0.5251 | 0.5037 | \n\nWe can see a similar trend: full-batch is better than naive sampling methods (i.e., random sampling and GraphSAGE sampling). Additionally, SEIGNN with designed mini-batch coarse nodes outperforms the full-batch version and variants with naive samplings.\n\n\n\n> **W2:** How does modifying the sizes of the minibatch subgraphs affect the efficacy of SEIGNN's coarse nodes?\n\nWe conduct the experiments by varying the batch size in a mini-batch subgraph (i.e., the number of target nodes). The results on Flickr are shown below.\n\n| Batch Size | 8192 | 4096 | 2048 | 1024 | 512 | 256 |\n| ---------- | -- | -- | -- | -- | -- | -- | \n| Accuracy   | 0.5564 | 0.5560 | 0.5509 | 0.5425 | 0.5078 | 0.4514 | \n\nWe can see that using a very small batch size would lead to bad performance while using the batch size 8192 and 4096 have good accuracy. \nThe reason of the bad performance of using a small batch size can be that a small batch size makes the information constrained in a small subgraph, which prohibits the information propagation from distant nodes. \n\n\n\n> **W3:** How important is using PPR for the coarse node idea? Could SEIGNN generalize to other importance metrics?\n\nIn our submission, we follow Shadow-GNN [1] to use PPR as an importance metrics to form a subgraph. As pointed out in [1], we can easily extend the approach by using other metrics such as katz index, SimRank [2], and feature similarity, which we leave as future work due to time imitations.\n\n[1] Decoupling the Depth and Scope of Graph Neural Networks. NeurIPS 2021.\n\n[2] Simrank: a measure of structural-context similarity. KDD 2002\n\n\n\n> **Q1:** Have you considered adding multiple coarse nodes in between mini-batch subgraphs? What would be the effect of this?\n\nWe are not very sure about the meaning of \"in between mini-batch subgraphs\". We answer the questions assuming that we are talking about having multiple coarse nodes for each subgraph (multiple coarse nodes represent a subgraph). If not, could you please clarify the question? \n\nWe think that this would be an interesting direction to explore for future work. Intuitively, multiple coarse nodes for a single subgraph can facilitate the information propagation within the subgraph when the subgraph is large. However, in another way, multiple coarse nodes for a single subgraph might be similar to a single coarse node for a subgraph but having more subgraphs/partitions with relatively smaller graph sizes doing graph partitioning. If we add more multiple coarse nodes for a subgraph, it is also interesting to explore the design for how to connect coarse nodes within the same subgraph. Due to the time limitation, we haven't conducted the experiments for this and we leave it for future work. \n\n\n\n> **Q2:** What would be the effect of removing/keeping the coarse nodes post-training?\n\nWe compare these two during the testing phase.\n\nOn Flickr, in testing, accuracy of removing coarse nodes: 0.5575, keep coarse nodes: 0.5551. On Reddit, in testing, removing coarse nodes: 0.9776, keeping coarse nodes: 0.9778.\nWe can see that either removing or keeping coarse nodes would have similar performance. Thus, we believe that either way can be fine for inference/test."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501863965,
                "cdate": 1700501863965,
                "tmdate": 1700501863965,
                "mdate": 1700501863965,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bms9iJJc3m",
                "forum": "QcMdPYBwTu",
                "replyto": "yBib9UDh8F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jpev [2/2]"
                    },
                    "comment": {
                        "value": "> **Q3:** Why is the stochastic solver slightly better in performance than the Neumann solver in Table 4 of the ablation studies? Is the Neumann solver a truncated version here?\n\nAnswer: \nYes. Here, the Neumann solver is a truncated version of the infinite sum (as shown in Eq(7)). The better performance of the stochastic solver compared with the Neumann solver is caused by the unbiased approximation of the equilibrium obtained using the stochastic solver. We theoretically proved that the stochastic solver provides an unbiased approximation in Proposition 1 of our submission. In contrast, using the Neumann solver would incur some approximation errors of the equilibrium in expectation, which leads to slightly worse performance. \n\n\nThank you again for your valuable feedback. Hope our response clarifies your questions and we are willing to discuss/clarify further if you have any other questions. Hope you may consider increasing the rating to support our submission if you are satisfied with our response and think our submission is worth publishing."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502115614,
                "cdate": 1700502115614,
                "tmdate": 1700502115614,
                "mdate": 1700502115614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ganMrreXl9",
                "forum": "QcMdPYBwTu",
                "replyto": "yBib9UDh8F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for the closing Author/Reviewer discussion period"
                    },
                    "comment": {
                        "value": "Dear Reviewer Jpev,\n\nWe would like to thank you again for your helpful review of our work. As the deadline for discussion draws near, we are eager to hear whether our response has addressed your concerns and any other feedback you may have. We are willing to discuss/clarify further if you have any remaining concerns."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675503447,
                "cdate": 1700675503447,
                "tmdate": 1700675503447,
                "mdate": 1700675503447,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUPROtfG9T",
                "forum": "QcMdPYBwTu",
                "replyto": "yBib9UDh8F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5011/Reviewer_Jpev"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5011/Reviewer_Jpev"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for your detailed rebuttal. \n\nI find the method more convincing after \"the naive subsampling experiment\" (W1) and \"the minibatch subgraph size ablation study\" (W2). The other answers seem sound as well. Yes, by \"multiple in-between nodes\", I mean if we expand the many-to-one relationship between subgraph nodes and a single coarse node, either by adding dummy coarse nodes or additional coarse nodes per subgraph.\n\nConditioned on the inclusion of \"the naive subsampling experiment\" and \"the minibatch subgraph size ablation study\", I believe the strengths of the paper outweigh its weaknesses. Thus, I have increased my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675731010,
                "cdate": 1700675731010,
                "tmdate": 1700675747447,
                "mdate": 1700675747447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]