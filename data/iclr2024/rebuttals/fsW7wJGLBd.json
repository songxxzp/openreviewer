[
    {
        "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game"
    },
    {
        "review": {
            "id": "jyfyGTiGp6",
            "forum": "fsW7wJGLBd",
            "replyto": "fsW7wJGLBd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission432/Reviewer_6PUw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission432/Reviewer_6PUw"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a dataset designed to evaluate the robustness of Large Language Models (LLMs). The dataset is collected using a carefully crafted game called Trust Tensor. Through this game, over 100,000 attack prompts and 46,000 defence prompts are gathered. By examining the attack and defence pairs, the study reveals that successful attacks often exhibit a simple structure, highlighting the adversarial vulnerabilities of LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The game Trust Tensor is well-designed and has the potential to serve as a benchmark for evaluating the adversarial robustness of Large Language Models."
                },
                "weaknesses": {
                    "value": "The task of prompt extraction is distinct from the Trust Tensor. In other words, the Trust Tensor is not very suitable for collecting the prompt extraction dataset."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698891896301,
            "cdate": 1698891896301,
            "tmdate": 1699635969796,
            "mdate": 1699635969796,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v5NRqkmwUX",
                "forum": "fsW7wJGLBd",
                "replyto": "jyfyGTiGp6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6PUw"
                    },
                    "comment": {
                        "value": "Thank you for your review. We\u2019re glad to hear you found our game to be well-designed and useful as an adversarial robustness benchmark! We've uploaded a new revision of the paper with changes summarized in the top-level shared response. We'd also like to offer a clarification on prompt extraction:\n\n> The task of prompt extraction is distinct from the Trust Tensor. In other words, the Trust Tensor is not very suitable for collecting the prompt extraction dataset.\n\nYou're right that the game does not explicitly reward players for performing prompt extraction, but many players found that an effective way to achieve \"access granted\" was to first perform prompt extraction, read the password from the prompt, and then enter the password verbatim. See the middle row of Figure 1 for an example of this. Around 27% of successful attacks on other players contain the defender's exact access code (indicating that the attacker performed successful prompt extraction), and the prompt extraction robustness benchmark contains a subset of \u200b\u200b569 strong, human-verified examples of prompt extraction across different defenses. (The number 569 has increased from the original submission, as we expanded and manually reviewed our benchmark subsets for the latest revision.)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214564644,
                "cdate": 1700214564644,
                "tmdate": 1700214564644,
                "mdate": 1700214564644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TlnOTRZE7D",
            "forum": "fsW7wJGLBd",
            "replyto": "fsW7wJGLBd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission432/Reviewer_ZAuw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission432/Reviewer_ZAuw"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a large dataset of human-generated adversarial examples for instruction-following LLMs. The defense and attack strategies are identified by different methods, and some insights on attacks and defense are given. This proposed dataset is also used to evaluate the robustness of LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "[1]\tThe proposed dataset has been released publicly.\n\n[2]\tThe samples in the dataset are high-quality since they are devised manually."
                },
                "weaknesses": {
                    "value": "[1]\tAlthough the size of the dataset is pretty large, the mechanism of this game is monotonous. \n\n[2]\tI am unsure whether the topic of this paper aligns with the theme of this conference."
                },
                "questions": {
                    "value": "[1]\tWhat are the potential applications of this dataset? In other words, how can it help improve the robustness of LLMs?\n\n[2]\tI wonder whether the attack and defense in the dataset are still effective if the prompt of the opening defense is changed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699014501421,
            "cdate": 1699014501421,
            "tmdate": 1699635969707,
            "mdate": 1699635969707,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LiOR6tQDen",
                "forum": "fsW7wJGLBd",
                "replyto": "TlnOTRZE7D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZAuw"
                    },
                    "comment": {
                        "value": "Thank you for your review and suggestions for improvement. We\u2019ll address these points one by one below. We've also uploaded a new revision of the paper with various general improvements (see shared top-level response). We hope we have addressed all your concerns; please consider revising your score if this is the case.\n\n## Weakness 1: Game mechanism\n\n> [1] Although the size of the dataset is pretty large, the mechanism of this game is monotonous.\n\nWe're not sure whether \"monotonous\" in this context means \"not enjoyable\" or \"generates uninteresting data\", so we'll discuss both:\n\n* **Do players like it?** Tensor Trust is a simple game, but players appear to enjoy it. After our initial promotion push, much of our web traffic has been driven by organic sharing on social media (i.e. not due to our own promotion), which continues to drive a few thousand attacks per day. As of November 16th, we have >3,000 registered accounts and >330,000 submitted attacks, as well as >250 players on our Discord, which shows strong engagement.\n* **Is the data interesting?** We chose a simple objective for the game (make the LLM say \"access granted\") so that it would be easy to objectively evaluate attack success (as opposed to, e.g., jailbreaking attacks, which require subjective evaluation). Despite its simplicity, we've found that this objective is sufficient to elicit a rich dataset of attacker behaviors, including prompt extraction as a means to gain additional information (as opposed to just prompt hijacking), and the various classes of attack outlined in Table 1. Overall, we found that the game setup strikes a good balance between making it easy to do objective evaluation of attack/defense strategies and encouraging diverse techniques.\n\n## Weakness 2: Relevance to ICLR\n\n> [2] I am unsure whether the topic of this paper aligns with the theme of this conference.\n\nWe think that ICLR would be a good venue for this paper due to its history of publishing strong benchmark datasets and adversarial robustness work. In the past, ICLR has been a venue for seminal work in this area, such as the [ImageNet-C dataset for robust computer vision](https://openreview.net/forum?id=HJz6tiCqYm) (ICLR'19, now cited 2.6k times), or [this popular vision-based adversarial example defense](https://openreview.net/forum?id=rJzIBfZAb#) (ICLR'18, now cited >10k times). More recently, ICLR 2023 published a variety of papers that consider datasets/benchmarks (examples: [[1]](https://iclr.cc/virtual/2023/poster/11601), [[2]](https://iclr.cc/virtual/2023/poster/11422), [[3]](https://iclr.cc/virtual/2023/poster/11900)) or adversarial robustness (examples: [[1]](https://iclr.cc/virtual/2023/poster/11455), [[2]](https://iclr.cc/virtual/2023/poster/11946), [[3]](https://iclr.cc/virtual/2023/poster/11270)). It also had two workshops on robustness/security/privacy ([[1]](https://iclr.cc/virtual/2023/workshop/12827), [[2]](https://iclr.cc/virtual/2023/workshop/12825)). Both \"societal considerations including fairness, safety, privacy\" and \"datasets and benchmarks\" are listed as relevant topics in [this year's CFP](https://iclr.cc/Conferences/2024/CallForPapers), so we expect this kind of work will continue to be of interest to the broader community at ICLR 2024.\n\n## Question 1: Applications\n\n> [1] What are the potential applications of this dataset? In other words, how can it help improve the robustness of LLMs?\n\nThis is a good point, and we've modified the paper to make this more clear. To summarize, here are some ways in which the data and experiments in the paper can help improve LLM robustness:\n\n* Researchers can use our robustness benchmarks (section 3/section 6) to measure whether newly released LLMs (or new forms of LLM finetuning) improve robustness to prompt hijacking and prompt extraction.\n* Our taxonomy of strategies (section 4) and qualitative analysis in the appendix shows various ways in which existing LLMs fail to be robust to prompt injection, which we believe is useful to target future efforts in fine-tuning LLMs against prompt injection (e.g. the analysis in appendix H shows that system-user message roles have little effect on GPT 3.5 Turbo, which highlights the need for stronger steerability for these types of models). This qualitative analysis may also be useful for prompt engineers and red-teaming efforts.\n* We also hope that our data will be useful for directly training better-defended models (by optimizing them to ignore the adversarial examples in our dataset), as well as for automated red-teaming (e.g. learning multi-step attack strategies from the data). We leave these more complex applications to future work.\n\nWe've  now improved the contribution statement in the introduction to make the first two contributions (evaluation benchmark, interesting qualitative analysis) more clear, and highlighted the potential future work in the third point in the conclusion.\n\n(continued in reply)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214511157,
                "cdate": 1700214511157,
                "tmdate": 1700214511157,
                "mdate": 1700214511157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aehNWMAG7g",
                "forum": "fsW7wJGLBd",
                "replyto": "TlnOTRZE7D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZAuw"
                    },
                    "comment": {
                        "value": "Thank you again Reviewer ZAuw for your thoughtful feedback. We have attempted to address your concerns in our rebuttal and in our latest revision of the paper. We would appreciate it if you could look over our rebuttal and update your score if it has addressed your concerns. We're also happy to address any additional concerns you might have before the end of the discussion period on November 22 (tomorrow)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594603896,
                "cdate": 1700594603896,
                "tmdate": 1700594603896,
                "mdate": 1700594603896,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BKlNveDOhH",
            "forum": "fsW7wJGLBd",
            "replyto": "fsW7wJGLBd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission432/Reviewer_gj2Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission432/Reviewer_gj2Z"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel dataset of over 100,000 prompt injection attacks and 46,000 defense prompts, generated by players in an online game called Tensor Trust. These attacks target Large Language Models (LLMs) and shed light on the vulnerabilities of LLM-based applications to malicious prompt manipulations. The dataset is the largest of its kind, and it reveals that LLMs are susceptible to both prompt extraction and prompt hijacking attacks. The authors use this dataset to create benchmarks to evaluate LLM resistance to such attacks and demonstrate that many models are vulnerable. They also show that these attack strategies generalize to real-world LLM applications beyond the game setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The use of an online game to collect human-generated adversarial examples for instruction-following LLMs is a novel and creative way to understand the weaknesses of these models.\n\n* The creation of benchmarks for evaluating LLM resistance to prompt injection attacks is a valuable contribution, as it provides a standardized way to assess the security of these models. This is of practical importance in the development of secure LLM-based applications."
                },
                "weaknesses": {
                    "value": "* The paper's setting is initially hard to grasp; authors should aim to explain the threat model clearly, using notations and specifying the level of access attackers have.\n\n* The paper should establish a connection to Textual Backdoor attacks, even though these attacks typically require a more significant level of access to LLMs or their pretraining data than the setting the authors are primarily interested in. This additional context would help improve the clear understanding of the threat model in which Tensor Trust operates."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission432/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission432/Reviewer_gj2Z",
                        "ICLR.cc/2024/Conference/Submission432/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699514019029,
            "cdate": 1699514019029,
            "tmdate": 1700578783243,
            "mdate": 1700578783243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3VkaUuXOgS",
                "forum": "fsW7wJGLBd",
                "replyto": "BKlNveDOhH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gj2Z"
                    },
                    "comment": {
                        "value": "Thank you for your review! We're glad that you agree that benchmarking resistance to prompt injection is a valuable contribution and that you found our work novel. We also found your feedback in the weaknesses section particularly helpful, and have updated the paper in response. We've uploaded a new copy of the paper (please see the shared top-level message for details), and we address your specific points in more detail below.\n\n## Weakness 1\n\n> The paper's setting is initially hard to grasp; authors should aim to explain the threat model clearly, using notations and specifying the level of access attackers have.\n\nOur new revision uses more precise notation in the introduction, and additional detail on the attacker and defenders' abilities and goals. The relevant paragraphs now read as follows:\n\n> The Tensor Trust web game simulates a bank. Each player has a balance, which they can increase by either coming up with successful attacks or by creating a defense that rebuffs attacks. In this section, we will explain the mechanics of defending and attacking, as well as implementation details on how we evaluate the attacks and defenses submitted by users.\n>\n> **Notation**  \n> We use $\\mathcal V$ to denote a token vocabulary and $L : \\mathcal V^* \\times \\mathcal V^* \\times \\mathcal V^* \\to \\mathcal{V}^*$ to denote an LLM that takes in three strings and outputs a single response string. $G : \\mathcal V^* \\to \\{T, F\\}$ denotes a *goal predicate* that determines whether a string says  \"access granted\" (achieved using the regex in Appendix B).\n>\n> **Defending**  \n> Each account has a _defense_ which consists of three prompts: an opening defense $d_{\\text{open}}$, an access code $c_{\\text{access}}$, and a closing defense $d_{\\text{close}}$, as shown in Fig. 2. When a user saves a defense, we validate it by sandwiching their access code between the opening and closing defense and feeding it to the LLM $L$. The access code can only be saved if it makes the LLM output \"access granted\". In other words, $G(L(d_{\\text{open}}, c_{\\text{access}}, d_{\\text{close}}))$ must be true.\n>\n> **Attacking**  \n> A player can select any other player's account and submit an attack against it. The text of the first player's attack, $c_{\\text{attack}}$, is sandwiched between the defending player's opening and closing defense ($d_{\\text{open}}$ and $d_{\\text{close}}$), and then fed into the LLM $L$. If the LLM outputs \"access granted\" (i.e., $G(L(d_{\\text{open}}, c_{\\text{attack}}, d_{\\text{close}}))$ is true), the attacker steals a fraction of the defender's money. Otherwise, the defender is granted a small amount of money for rebuffing the attack. The attacker cannot see $d_{\\text{open}}$ or $d_{\\text{close}}$, but can see the LLM's response to their attack. In the game, this is depicted as in Fig. 2.\n\nPlease let us know if we can make this more clear!\n\n## Weakness 2\n\n> The paper should establish a connection to Textual Backdoor attacks, even though these attacks typically require a more significant level of access to LLMs or their pretraining data than the setting the authors are primarily interested in. This additional context would help improve the clear understanding of the threat model in which Tensor Trust operates.\n\nThanks for the suggestion! We added the following paragraph to the related work section (section 7) to connect our work with past work on training-time attacks, including textual backdoor attacks. In this paragraph, [Dai et al.](https://arxiv.org/abs/1905.12457), [Chen et al.](https://arxiv.org/abs/2006.01043), [Qi et al.](https://arxiv.org/abs/2105.12400), and [Wallace et al.](https://arxiv.org/pdf/2010.12563.pdf) belong to this category. If we missed any work that you think would be useful to add to the paper, we would be happy to add them.\n\n> Other past work considers training-time attacks. This might include poisoning a model\u2019s training set with samples that cause it to misclassify certain inputs at test time (Biggio et al., 2012; Dai et al., 2019; Chen et al., 2021; Qi et al., 2021; Wallace et al., 2020), or fine-tuning an LLM to remove safety features (Qi et al., 2023). These papers all assume that the attacker has some degree of control over the training process (e.g. the ability to corrupt a small fraction of the training set). In contrast, we consider only test-time attacks on LLMs that have already been trained.\n\nPlease consider updating your score if these changes have adequately addressed your concerns about the paper, and let us know if there\u2019s any other way that we can improve the writing."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214438122,
                "cdate": 1700214438122,
                "tmdate": 1700214438122,
                "mdate": 1700214438122,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W0Gd8KbiyL",
                "forum": "fsW7wJGLBd",
                "replyto": "BKlNveDOhH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission432/Reviewer_gj2Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission432/Reviewer_gj2Z"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thanks for the modifications. I have increased my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578758647,
                "cdate": 1700578758647,
                "tmdate": 1700578771526,
                "mdate": 1700578771526,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]