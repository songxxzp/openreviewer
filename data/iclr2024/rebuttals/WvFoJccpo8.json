[
    {
        "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"
    },
    {
        "review": {
            "id": "0JpsSERa7Y",
            "forum": "WvFoJccpo8",
            "replyto": "WvFoJccpo8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2414/Reviewer_2aMS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2414/Reviewer_2aMS"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposes QA-LoRA, a LoRA based parameter efficient LLM finetuning scheme with quantization. QA-LoRA extends QLoRA to be able to add low-rank matrices with pre-trained weights in low-bit tensors directly, without the need to PTQ on low-rank matrices. To guarantee that the summation of low-rank matrices and pre-trained weights are still within the same quantization range, the authors relax the requirement of each row being the same into groups, through group-wise quantization. This improves the accuracy and efficiency during inference. During evaluation, the authors experimented with a series of LLaMA and LLaMA2 models with different sizes. Results showed that the proposed models can achieve superior performance than LoRA and QLoRA."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper organization, presentation, and references are good.\n* The proposed method has enough novelty."
                },
                "weaknesses": {
                    "value": "* Parameter offset in experiments: The proposed method incorporates group-wise/sub-channel qunatization, which includes an additional number of parameters for scales. Also, the proposed QA-LoRA reduces the size of low-rank matrices. However, these parameter offsets are not reflected in the results, which could be misleading to the audiences. It would be more informative to add the actual model size (or estimated) in MB/GB for each of the models.\n* In the ablation study, only group size is examined. It would be worthwhile to experiment with the D_int as well, as it is also part of the tradeoff between model size and accuracy. It would be interesting to see what is the lowest D_int in this setup, compared to vanilla LoRA."
                },
                "questions": {
                    "value": "See those in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2414/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697575829823,
            "cdate": 1697575829823,
            "tmdate": 1699636176534,
            "mdate": 1699636176534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x8WePEYbrX",
                "forum": "WvFoJccpo8",
                "replyto": "0JpsSERa7Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2414/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2414/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2aMS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and questions.\n\n>**Q1.** Parameter offset in experiments: The proposed method incorporates group-wise/sub-channel quantization, which includes an additional number of parameters for scales. Also, the proposed QA-LoRA reduces the size of low-rank matrices. However, these parameter offsets are not reflected in the results, which could be misleading to the audiences. It would be more informative to add the actual model size (or estimated) in MB/GB for each of the models.\n\n>**A1.** Thanks for the suggestion. First, we report the sizes of the final models of QLoRA and QA-LoRA in the following table. Please note that there are two ways for post-processing in QLoRA, *i.e.*, the unmerged ($\\tilde{\\mathbf{W}}$ and $s\\cdot\\mathbf{A}\\mathbf{B}$ are stored individually, which saves memory but the inference is slow) and merged ($s\\cdot\\mathbf{A}\\mathbf{B}$ is added to $\\tilde{\\mathbf{W}}$, which is faster in inference but requires large memory because the matrix must be stored in FP16). QA-LoRA enjoys both low memory usage and a fast inference speed. A side note: In 33B and 65B models, setting $L=32$ in QA-LoRA results in slightly larger model sizes compared to QLoRA, but one can set $L=128$ which causes a negligible accuracy drop.\n>Note that the final model size of QA-LoRA is exactly the size of $\\mathbf{W}'$ (or equivalently, $\\tilde{\\mathbf{W}}$) because $s\\cdot\\mathbf{A}\\mathbf{B}$ is merged into $\\tilde{\\mathbf{W}}$ after adaptation. Take the 7B model with $L=32$ as an example. The baseline, the unmerged version of QLoRA, is sized 4.6G, in which $\\tilde{\\mathbf{W}}$ is sized 4.0G and $\\mathbf{A}$ and $\\mathbf{B}$ combined is sized 0.6G. QA-LoRA increases the first amount to 4.3G and eliminates the second amount.\n>**In the revised paper, we have added the following tables to Appendix C.**\n\n|Models|QLoRA (unmerged)|QLoRA (merged)|QA-LoRA ($B=4$, $L=32$)|QA-LoRA ($B=4$, $L=128$)|\n|:-|:-:|:-:|:-:|:-:|\n|**LLaMA-7B**|4.6|13.5|4.3|3.7|\n|**LLaMA-13B**|8.1|24.4|8.1|6.9|\n|**LLaMA-33B**|18.9|55.5|20.0|17.5|\n|**LLaMA-65B**|36.1|122.3|39.0|34.7|\n\nTable. The model size (GB) of QLoRA and QA-LoRA with respect to different options. The QLoRA models used NF4 and FP16 numerics.\n\n>**Q2.** In the ablation study, only group size is examined. It would be worthwhile to experiment with the D_int as well, as it is also part of the tradeoff between model size and accuracy. It would be interesting to see what is the lowest D_int in this setup, compared to vanilla LoRA.\n\n>**A2.** Good suggestion! We follow QLoRA to set $D_\\mathrm{int}=64$. During the rebuttal, we diagnose the performance with respect to $D_\\mathrm{int}$ in the following table. We find that the MMLU accuracy is not largely impacted by the value of $D_\\mathrm{int}$ unless it is too small (the same conclusion as in the QLoRA paper). We agree that discovering the smallest $D_\\mathrm{int}$ value is interesting, and we empirically find that the smallest $D_\\mathrm{int}$ that maintains the stability of QA-LoRA is $4$. **In the revised paper, we have added the table to Appendix B and the analysis to the main article.**\n\n|#Bits|D_int|MMLU (0-shot)|MMLU (5-shot)|#Bits|D_int|MMLU (0-shot)|MMLU (5-shot)|\n|:-|:-|:-:|:-:|:-|:-|:-:|:-:|\n|4|1|37.3|38.1|2|1|26.3|26.7|\n|4|2|38.7|38.2|2|2|26.0|26.6|\n|4|4|39.0|39.3|2|4|26.6|26.7|\n|4|8|39.0|39.6|2|8|25.8|26.8|\n|4|16|39.0|39.5|2|16|25.9|26.7|\n|4|32|39.3|39.3|2|32|25.9|26.5|\n|4|64|39.0|39.2|2|64|25.3|27.2|\n|4|128|39.2|39.3|2|128|25.8|26.8|\n\nTable. The MMLU accuracy (%) of $D_\\mathrm{int}$ measured on INT4 and INT2 quantization upon LLaMA-7B on the Alpaca dataset."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620770397,
                "cdate": 1700620770397,
                "tmdate": 1700620770397,
                "mdate": 1700620770397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6cRoqRWDQO",
            "forum": "WvFoJccpo8",
            "replyto": "WvFoJccpo8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2414/Reviewer_YexA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2414/Reviewer_YexA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a Quantization-Aware Low-Rank Adaptation (QA-LoRA) for efficient fine-tuning of LLM. This work comes improves Q-LORA algorithm by introducing group-wise operators which lift the need for post-training quantization. QA-LoRA implementation is simple and generic. It benefits from a balance between the number of parameters required for adaption and quantization. The experiments show that fine-tuning and inference stages are computationally efficient thanks to the use of INT4. The memory footprint of QA-LoRA is lower than QLoRA. In terms task accuracy, QA-LoRA is better than Q-LoRA with post-training quantization (GPTQ)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This work solves a limitation of previous parameter-efficient tuning of LLMs by eliminating the need for a separate post-training quantization which drops model accuracy\n- QA-LoRA further enhances memory efficiency of SOTA while preserving accuracy\n- The experiments are convincing as they cover a wide range of scenarios"
                },
                "weaknesses": {
                    "value": "QA-LoRA introduce a hyper-parameter (L: Group size). This requires additional optimization and It is unclear if it can be selected without tuning."
                },
                "questions": {
                    "value": "- I wonder if larger model where the need of this technique is crucial, e.g., 30B-60B, could be discussed.\n- Figure 3 legend should have QA-LoRA instead of A-LoRA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2414/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783131916,
            "cdate": 1698783131916,
            "tmdate": 1699636176457,
            "mdate": 1699636176457,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j1qWNcZLdZ",
                "forum": "WvFoJccpo8",
                "replyto": "6cRoqRWDQO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2414/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2414/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YexA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and questions.\n\n>**Q1.** QA-LoRA introduce a hyper-parameter ($L$: group size). This requires additional optimization and it is unclear if it can be selected without tuning.\n\n>**A1.** Thanks for the question. We first analyze the impact of $L$. A smaller $L$ results in (1) a higher degree of freedom (which often leads to higher fine-tuning accuracy), (2) a larger memory to store the quantized weight matrix, $\\tilde{\\mathbf{W}}$, and (3) an increasing risk of over-fitting if $L$ is too small. Therefore, a good strategy is to set $L$ to be a relatively small (but not too small) integer, *e.g.*, $L=32$ as a good practice as shown in Table 5. We cannot guarantee that no hyper-parameter tuning is needed, but there is a clear guideline to do this. For example, one can try $L=16$, if the GPU memory allows, to achieve higher accuracy.\n\n>**Q2.** I wonder if a larger model where the need for this technique is crucial, e.g., 30B-60B, could be discussed.\n\n>**A2.** Nice suggestion! In the original version, we provide experiments on the 33B and 65B models of LLaMA in Table 1, and QA-LoRA achieves improvement over the baseline, QLoRA with GPTQ. Upon these results, we discuss the need for QA-LoRA in large models in the following aspects. (1) QA-LoRA reduces the computational costs of fine-tuning large models, *e.g.*, using QA-LoRA, only 1 and 2 V100 GPUs are needed for fine-tuning the 33B and 65B models. (2) When larger models are used, there can be increasing needs for low-bit (*e.g.*, INT3 and INT2) quantization, especially when the large models are to be deployed to edge devices. QA-LoRA shows significant advantages in such scenarios. **In the revised paper, the above contents are added to Section 4.2 where the experimental results are discussed.**\n\n>**Q3.** Figure 3 legend should have QA-LoRA instead of A-LoRA.\n\n>**A3.** Sorry for our carelessness. **It has been fixed in the revised version.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620730811,
                "cdate": 1700620730811,
                "tmdate": 1700620730811,
                "mdate": 1700620730811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ARK6gYos5J",
            "forum": "WvFoJccpo8",
            "replyto": "WvFoJccpo8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2414/Reviewer_KGp3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2414/Reviewer_KGp3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a modification to the QLoRA method, designed to facilitate the training of large language models with limited computational resources. QLoRA initially quantizes neural network weights to NF4 format and subsequently optimizes LoRA matrix weights in FP16. This process increases inference latency, as everything is converted to FP16 during inference. The proposed alternative method outlined in this paper ensures the appropriate quantization of LoRA weights without necessitating a reversion to FP16 during inference. This enhancement involves just a few lines of code, offering a more efficient solution. While the paper lacks specific results and comprehensive explanations, it presents a promising direction for optimizing large language model training within constrained computational budgets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Addresses a Significant Issue** - QLoRA's potential is realized through its ability to quantize LoRA weights, effectively resolving the disparities observed between fine-tuning and inference in QLoRA.\n\n**Streamlined Implementation** - The authors highlight the method's simplicity, emphasizing that it necessitates a mere two lines of code modification to yield impressive enhancements.\n\n**Thorough Assessment** - The evaluation is meticulous, with the authors examining a spectrum of competitive methods and diverse model architectures to demonstrate the method's advantages comprehensively."
                },
                "weaknesses": {
                    "value": "Reasoning behind the method - Wy should all the c_ij as defined in the paper be equal is not clear \u2014 which is the main motivation for the group-wise quantisation. I would be willing to improve the scores with better explanation on the explanation of the method (See the questions)"
                },
                "questions": {
                    "value": "1. In Algorithm 1, the function `merge_with_quantization` is defined but never used. \n2. What is the degree of freedom of quantisation and adaptation - These seem to be new terms that are added in this paper and not used in the literature. These have to be defined, before claiming that they are increased or managed by the proposed method\n3. Page 6 is just results - Page 6 is just results without much to interpret. These are a bunch of numbers. Please consider presenting this table in a better manner. Can this be presented as a graph for readers? Just numbers are hard to read\n4. In Table 2, you indicate that the number of parameters in the method are lesser than QLoRA \u2014 almost by 2x. Why is this the case? This seems like a unfair comparison to QLoRA. What is the time taken in hours for fine-tuning with similar number of parameters.\n5. Section 3.3 explains the method and the reasoning behind on why the rank degenerates to 1. But the explanation is not comprehensive."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2414/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699260211424,
            "cdate": 1699260211424,
            "tmdate": 1699636176385,
            "mdate": 1699636176385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j9UPI8YgzW",
                "forum": "WvFoJccpo8",
                "replyto": "ARK6gYos5J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2414/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2414/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KGp3 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and questions.\n\n>**Q0.** Reasoning behind the method - Why should all the c_ij as defined in the paper be equal is not clear \u2014 which is the main motivation for the group-wise quantization.\n\n>**A0.** Please refer to our response to **Q5**.\n\n>**Q1.** In Algorithm 1, the function `merge_with_quantization` is defined but never used.\n\n>**A1.** Sorry for misleading. Due to the space limit, we did not add the `main` function in the paper, which calls `qalora_training` (which calls `qalora_forward`) and `merge_with_quantization`. The `merge_with_quantization` function is called *after* the entire QA-LoRA training procedure for merging the LoRA weights $s\\cdot\\mathbf{A}\\mathbf{B}$ into the quantized weight matrix $\\tilde{\\mathbf{W}}$. It is done by updating the $\\beta$ parameter into $\\beta_\\mathrm{new}$. **In the revised paper, we have added an explanation in the main article, after the reference to Algorithm 1.**\n\n>**Q2.** What is the degree of freedom of quantization and adaptation - These seem to be new terms that are added in this paper and not used in the literature. These have to be defined, before claiming that they are increased or managed by the proposed method.\n\n>**A2.** Thanks for the question. Literally, the degree of freedom (DoF) means how many parameters (or pairs of parameters) can be used for quantization or adaptation. We explain our insight as follows and, **to avoid confusion, we replaced all \"the degree of freedom\" with \"the number of parameters\" in the revised paper.**\n>In QLoRA, each column of $\\mathbf{W}$ is quantized using the same pair of $\\alpha$ and $\\beta$ parameters, which we say the DoF (number of parameters) of the *quantization* part is $1$; meanwhile, the matrix $\\mathbf{A}$ is sized $D_\\mathrm{in}\\times D_\\mathrm{int}$, implying that each of the $D_\\mathrm{in}$ entries in the column of $\\tilde{\\mathbf{W}}$ is multiplied by an individual parameter, which we say the DoF (number of parameters) of the *adaptation* part is $D_\\mathrm{in}$. There is clearly an imbalance here, which causes two issues: (1) the over-high quantization error harms the fine-tuning accuracy, and (2) the adaptation weights cannot be merged to the quantized matrix, $\\tilde{\\mathbf{W}}$.\n>This motivates us to (1) increase the number of parameters of the *quantization* part from $1$ to $D_\\mathrm{in}/L$ using group-wise quantization, where $L$ is the group size, and (2) decrease the number of parameters of the *adaptation* part by squeezing the input dimensionality of $\\mathbf{A}$ from $D_\\mathrm{in}$ to $L$. This is why QA-LoRA improves the accuracy and naturally allows the adaptation weights to be merged into the quantized matrix.\n\n>**Q3.** Page 6 is just results - Page 6 is just results without much to interpret. These are a bunch of numbers. Please consider presenting this table in a better manner. Can this be presented as a graph for readers? Just numbers are hard to read.\n\n>**A3.** Thanks for the suggestion. We followed QLoRA and other recent works to report these exact numbers to ease the comparison against existing papers. Actually, the main results of Table 6 (5-shot MMLU on the Alpaca dataset) have been summarized in Figure 1. **In the revised paper, we plot other results (0-shot MMLU and the FLAN v2 dataset) as similar figures and add them to Appendix B.**\n\n=====*to be continued in Part 2*====="
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620566693,
                "cdate": 1700620566693,
                "tmdate": 1700620566693,
                "mdate": 1700620566693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kCgAOjZyS8",
                "forum": "WvFoJccpo8",
                "replyto": "ARK6gYos5J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2414/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2414/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KGp3 (Part 2)"
                    },
                    "comment": {
                        "value": "=====*continuing from Part 1*=====\n\n\n>**Q4.** In Table 2, you indicate that the number of parameters in the method is lesser than QLoRA \u2014 almost by 2x. Why is this the case? This seems like an unfair comparison to QLoRA. What is the time taken in hours for fine-tuning with a similar number of parameters?\n\n>**A4.** The reason behind the fewer amounts of parameters, compared to QLoRA, lies in the reduction of the dimensionality of $\\mathbf{A}$. Compared to LoRA and QLoRA where $\\mathbf{A}$ has $D_\\mathrm{in}\\times D_\\mathrm{int}$ parameters, QA-LoRA reduces the number to $L\\times D_\\mathrm{int}$ where $L$ is the group size and $L\\ll D_\\mathrm{in}$. This reduces the number of parameters in QA-LoRA by around $1/2$ (originally, $\\mathbf{A}$ and $\\mathbf{B}$ have similar numbers of parameters). We would like to stress that this is a good feature of QA-LoRA which allows it to achieve higher fine-tuning accuracy with fewer parameters.\n>Regarding the fine-tuning time, it is *not* largely impacted by the parameters because the amount of LoRA parameters is much smaller than that of the LLM itself (*e.g.*, 89M or 160M *vs.* 7B). To verify this point, we double $D_\\mathrm{int}$ which also doubles the number of parameters, surpassing that of QLoRA, but QA-LoRA is still much faster than QLoRA (see the table below). Essentially, QA-LoRA is faster in training because it uses INT4 computation while QLoRA uses NF4 computation which is not well optimized by CUDA.\n>**In the revised version, we make this point clear by adding the above contents to Section 4.2 (the paragraph starting with \"the efficiency of QA-LoRA\").**\n\n|Method|#Params|Time|\n|:-|:-:|:-:|\n|QLoRA ($D_\\mathrm{int}=64$)|160M|40.0h|\n|QA-LoRA ($D_\\mathrm{int}=64$)|89M|21.5h|\n|QA-LoRA ($D_\\mathrm{int}=128)$|178M|21.8h|\n\nTable. A comparison of the number of parameters and training time.\n\n>**Q5.** Section 3.3 explains the method and the reasoning behind why the rank degenerates to 1. But the explanation is not comprehensive.\n\n>**A5.** First of all, please note that the final step of QA-LoRA is to fuse $\\tilde{\\mathbf{W}}$ (the quantized matrix during training) and $s\\cdot\\mathbf{A}\\mathbf{B}$ (the adaptation weights) into a new matrix $\\mathbf{W}'=\\tilde{\\mathbf{W}}+s\\cdot\\mathbf{A}\\mathbf{B}$. For the efficiency of inference, we hope that $\\mathbf{W}'$ is still in low-bit quantization. This means that all values in $\\tilde{\\mathbf{W}}$ and $\\mathbf{W}'$ must come from an equidistant discrete set (see Footnote 1).\n>As an example, let a column of $\\tilde{\\mathbf{W}}$ contain the numerical values from the set of $\\{0.3,1.5,2.7,3.9\\}$, where we can use INT2 quantization ($\\alpha=1.2$, $\\beta=0.3$). Now, it is added by the corresponding column in $s\\cdot\\mathbf{A}\\mathbf{B}$ (both $\\mathbf{A}$ and $\\mathbf{B}$ contain continuous numerical values), and we require that the summation is still INT2-quantizable. To guarantee this in a continuous optimization procedure, the only known way is to constrain the added values to be constant, which derives that all row vectors of $\\mathbf{A}$ are identical, and hence $\\mathrm{rank}(\\mathbf{A})=1$. Actually, we can choose not to say $\\mathrm{rank}(\\mathbf{A})=1$ but simply say that the flexibility of adaptation is largely constrained because the row vectors of $\\mathbf{A}$ must be identical.\n>We hope that we have made the explanation comprehensive. Of course, we are open to further discussions. Thanks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2414/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620650177,
                "cdate": 1700620650177,
                "tmdate": 1700620666557,
                "mdate": 1700620666557,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]