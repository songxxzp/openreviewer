[
    {
        "title": "Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution"
    },
    {
        "review": {
            "id": "9XdqYd78yP",
            "forum": "am7BPV3Cwo",
            "replyto": "am7BPV3Cwo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5203/Reviewer_b8DW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5203/Reviewer_b8DW"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem out-of-distribution (OOD) detection when original in-distribution (ID) data suffers from class imbalance. The authors first report an experimental observation regarding two major challenges in OOD detection under the studied setting: misidentification of tail-side ID samples as OOD (False positives) and of some OOD samples as head-side ID (False negatives). They then statistically compare the learning objectives in OOD detection with class-balanced vs. -imbalanced ID data, allowing them to attribute these challenges to the auxiliary bias term that does not cancel out in the class-imbalanced scenario. Based on this analysis, they propose two unique avenues to mitigate the influence of this bias term: post-hoc normalization and train-time regularization. Together, they form a cohesive framework for OOD detection with class-imbalanced ID data, dubbed ImOOD. The authors provide substantial empirical evidence for the effectiveness of ImOOD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The statistical analysis performed in the paper is sound and written in a manner that is easy to follow. The authors include a helpful intuitive understanding of the bias term that connects it back to the empirical observations about the behavior of OOD detection models under the class-imbalanced setting.\n- ImOOD is well-motivated and theoretically-grounded on the the above analysis. The general thinking behind the method is sound, and for the most part, it\u2019s described clearly enough to understand and re-implement the design."
                },
                "weaknesses": {
                    "value": "- I believe the major drawback of this paper lies in the limited scope of the studied problem. It appears that the authors assume that the class-imbalance problem only exists on the original ID dataset. I am not too convinced with the practicality of the proposed scenario; it\u2019s hard to imagine in what deployment scenarios only the ID dataset will suffer from this problem, while the OOD dataset(s) used for training/testing is void of it. Wouldn\u2019t it be more natural to assume that the ID training dataset can be refined in advance to make it class-balanced, while the OOD data the detector must be able to identify arise in various forms, and thus is more likely to be class-imbalanced?\n    - What if the auxiliary OOD dataset used for training and/or the target OOD dataset exhibits class-imbalance as well? I think it is important to study various combinations of class-balanced and -imbalanced ID/auxiliary OOD/target OOD datasets. Does the analysis performed in the paper extend to and hold in such settings? Can ImOOD still outperform other baselines?\n- Please correct me if I am wrong, but it appears that the verification of post-hoc normalization (minus the train-time regularization) on larger datasets appears to be missing. Even on CIFAR10-LT, the improvement from post-hoc normalization seems marginal, but this concern could be alleviated, as long as the improvement is consistent across various datasets. If post-hoc normalization is not quite as effective on datasets, it would signify that ImOOD is heavily reliant on the train-time regularization.\n- More details on how the label distribution is learned (during test-time regularization) would be appreciated. Also, do you use the same learned label distribution when performing post-hoc normalization? Or is it used only for training-time regularization? If you discard it after training, one would have to know the label distribution anyways for post-hoc normalization, so do we really need to use the learned distribution in the first place?\n- The empirical validation is limited to one target OOD dataset per InD dataset. Validation on more challenging OOD datasets (e.g., near OOD data, data with spurious correlation) would be helpful to gauge the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5203/Reviewer_b8DW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697990692507,
            "cdate": 1697990692507,
            "tmdate": 1699636517345,
            "mdate": 1699636517345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RWqOmLQDqJ",
                "forum": "am7BPV3Cwo",
                "replyto": "9XdqYd78yP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer b8DW (Part. I)"
                    },
                    "comment": {
                        "value": "We thank the valuable comments, and we answer them in detail as follows.\n\n> Q1: How to deal with the self-imbalanced OOD data?\n\nThe self-imbalanced OOD data is indeed a practical problem. \nAs suggested, we now take CIFAR100 as the imbalanced auxiliary OOD data, and train models with CIFAR10 as ID.\nThen models are evaluated on the remaining 5 subsets of the SCOOD benchmark (CIFAR100 was originally used as test OOD data but is now eliminated).\nAccording to the results reported below, our ImOOD consistently outperforms the advanced PASCL method.\n\n|  Method  |   AUROC\u2191  |    AUPR\u2191  |   FPR95\u2193  |\n|:--------:|:---------:|:---------:|:---------:|\n|   PASCL  |   88.05   |   86.63   |   38.69   |\n| **Ours** | **89.14** | **87.30** | **35.48** |\n\nBesides, we have also noticed a recent work by BalEnergy $^{[1]}$ that discussed this issue and added class-aware loss regularizations on OOD training data.\nAccording to the experiments in Appendix B.3 (due to different experimental settings) of our submitted paper, we equipped our ImOOD with BalEnergy and achieved better OOD detection performance.\nIt indicates our method does not conflict with imbalanced OOD data optimization techniques.\n\n|   ID Data   |   Method  |   AUROC\u2191  |   AUPR\u2191   |   FPR95\u2193  |\n|:-----------:|:---------:|:---------:|:---------:|:---------:|\n|  CIFAR10-LT | BalEnergy |   92.51   |   91.87   |   31.11   |\n|             | **+Ours** | **92.80** | **92.26** | **30.29** |\n| CIFAR100-LT | BalEnergy |   77.55   |   72.95   |   61.54   |\n|             | **+Ours** | **77.85** | **73.96** |   61.82   |\n\n> Q2: About the effectiveness of post-hoc normalization.\n\nThanks for pointing out this. In the manuscript, we only provide the performance gain brought by our post-hoc normalization on CIFAR10-LT benchmark. Here we supplement the results on CIFAR100-LT and ImageNet-LT benchmarks, which shows applying our method leads to consistent improvements on various datasets. The general validation of our post-hoc normalization is verified.\n\n|  Benchmark  |   Method  |   AUROC\u2191  |   AUPR\u2191   |   FPR95\u2193  |\n|:-----------:|:---------:|:---------:|:---------:|:---------:|\n|  CIFAR10-LT |  BinDisc  |   87.37   |   81.05   |   33.66   |\n|             | **+Ours** | **87.64** | **81.61** | **31.31** |\n| CIFAR100-LT |  BinDisc  |   72.09   |   67.63   |   68.70   |\n|             | **+Ours** | **72.33** | **67.96** | **67.54** |\n| ImageNet-LT |  BinDisc  |   70.12   |   69.64   |   78.13   |\n|             | **+Ours** | **70.74** | **70.28** | **76.83** |\n\nHowever, as our experiments suggest, the enhancement by post-hoc normalization is relatively limited, while our training-time regularization seems to bring more significant advances.\nThe main reason may be that the estimates of probability distributions are not well-calibrated, especially for the class prior $P(y) \\coloneqq \\pi_y$ whose numerical instability may severely affect the OOD detection process, as discussed in Appendix 2.3 in the submitted paper.\nWe thus further develop the training-time regularization technique to automatically adjust the estimates, in order to ultimately learn a better OOD detector $g(x)$ close to the ideally balanced $g^{bal}(x)$.\nOnce $g(x)$ is optimized, no estimate is used for detecting OOD samples, and the output of scorer $g(x)$ is more stable and effective in performing OOD detection.\n\nWe will add the experiments and discussions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373908036,
                "cdate": 1700373908036,
                "tmdate": 1700373908036,
                "mdate": 1700373908036,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K6V7PHJnNp",
            "forum": "am7BPV3Cwo",
            "replyto": "am7BPV3Cwo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5203/Reviewer_LaLW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5203/Reviewer_LaLW"
            ],
            "content": {
                "summary": {
                    "value": "When encountering inherent imbalance of in-distribution (ID) data, the paper identified two common challenges faced by different OOD detectors: misidentifying tail class ID samples as OOD, while erroneously predicting OOD samples as head class from ID. To explain this phenomenon, the authors introduce a generalized statistical framework, termed ImOOD, to formulate the OOD detection problem on imbalanced data distribution. Consequently, the theoretical analysis reveals that there exists a class-aware bias item between balanced and imbalanced OOD detection, which contributes to the performance gap."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is written well and is easy to understand.\n2. The studied problem is very important.\n3. The results seem to outperform state-of-the-art."
                },
                "weaknesses": {
                    "value": "1. I am curious about the possibility of this method being extended to distance-based OOD detection methods, such as Mahalanobios distance, etc. How well does the current method compare to the state-of-the-art distance-based methods?\n2. I am curious about how well the current method performs on synthesized outliers (Tao et.al. 2023).\n3. Could you elaborate more on a closely related baseline, Jiang et al. 2023, and discuss the similarities and differences w.r.t your work?"
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698531635723,
            "cdate": 1698531635723,
            "tmdate": 1699636517247,
            "mdate": 1699636517247,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "elMK5Zpu3C",
                "forum": "am7BPV3Cwo",
                "replyto": "K6V7PHJnNp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer LaLW (Part. I)"
                    },
                    "comment": {
                        "value": "We thank the valuable comments, and we answer them in detail as follows.\n\n> Q1: Comparison and extension to distance-based OOD methods.\n\nThanks for the thoughtful suggestion. Due to the time limitation, here we mainly compare our method with Mahalanobios distance $^{[1]}$ (denoted as \"Maha\") on the CIFAR10-LT benchmark. The results show that our post-hoc normalization (denoted as \"Maha + infer\") brings slight improvements (e.g., 0.3% increase on AUROC), while our training-time regularization (denoted as \"Maha + train\") significantly boosts the imbalanced OOD detection (e.g., 1.2% increase on AUROC and 3.4% decrease on FPR95).\n\n|    Method    |   AUROC\u2191  |   AUPR\u2191   |   FPR95\u2193  |\n|:------------:|:---------:|:---------:|:---------:|\n|     Maha     |   88.26   |   87.94   |   42.74   |\n| Maha + infer |   88.58   |   87.98   |   42.47   |\n| Maha + train | **89.43** | **88.31** | **39.37** |\n\nHowever, the final results are still worse than advanced models (with 92%+ on AUROC for example). A possible reason might be that the feature-level distance estimation is basically inaccurate, especially for the tail classes with rare data samples to statistic (e.g., computing the mean and variance). On the other hand, our method takes the imbalance problem into consideration, which helps balance the feature space by using the training-time regularization.\n\n> Q2: Ablation on synthesized OOD training data.\n\nThanks for the insightful suggestion. Here we equip with the novel VOS $^{[2]}$ and NPOS $^{[3]}$ to synthesize virtual OOD data in the feature space for training.\nAccording to the results shown below, synthesized outliers can also improve the OOD detection performance (e.g., 72% v.s. 75% on AUROC), and our method brings further advance regarding the vanilla VOS and NPOS methods (e.g., 75% v.s. 78% on AUROC).\n\n\n| Training OOD |  Method  |   AUROC\u2191  |    AUPR\u2191  |   FPR95\u2193  |\n|:------------:|:--------:|:---------:|:---------:|:---------:|\n|     None     |    MSP   |   72.09   |   70.40   |   68.79   |\n|    VOS-Syn   |    VOS   |   75.44   |   73.86   |   65.04   |\n|              | **Ours** | **78.61** | **76.97** | **64.65** |\n|   NPOS-Syn   |   NPOS   |   74.83   |   74.02   |   69.67   |\n|              | **Ours** | **77.05** | **76.52** |   70.85   |\n\nHowever, compared to the real OOD training data (e.g., the TinyImages80M used in our original setting), the synthesized outliers seem to be unsatisfactory to help train a SOTA OOD detector. It indicates that the density-based VOS and the distance-based NPOS also suffer from inaccurate feature-level statistics on imbalanced data distribution, and are unable to generate qualified virtual outliers.\nThis is more severe for models training from scratch, as NPOS also discussed, and image-level OOD sample synthesis may help alleviate this problem.\n\nWe will add those experiments and discussions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373800299,
                "cdate": 1700373800299,
                "tmdate": 1700373800299,
                "mdate": 1700373800299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JNXcEu2JL5",
            "forum": "am7BPV3Cwo",
            "replyto": "am7BPV3Cwo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5203/Reviewer_r7k6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5203/Reviewer_r7k6"
            ],
            "content": {
                "summary": {
                    "value": "To tackle the OOD detection under class imbalanced setting, the paper proposed the statistically guided framework called ImOOD. Through the statistical analysis, the authors have found that there exists a bias term responsible for the performance gap between balanced and imbalanced OOD detection models. Under the ImOOD framework, by leveraging the bias term, the authors then propose the post-hoc normalization and training time regularization technique to enhance the OOD performance. The experimentation conducted on multiple real-world datasets demonstrates the effectiveness of the proposed post-hoc normalization and training time regularization techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The motivation for proposing the ImOOD framework is clear. The authors have done a great job in terms of empirically demonstrating the limitations present in existing OOD detection techniques under imbalanced data distribution. For example,  Figure 1 demonstrates how OOD samples are incorrectly detected as head class samples and that of the in-distribution tail class samples as OOD samples. \n* The proposed post-hoc normalization and training time regularization are backed by the strong statistical analysis conducted in Section 3.1 along with empirical evidence.\n* The paper is well-written and easy to follow. \n* An extensive ablation study is conducted to showcase the effectiveness of each component in their proposed framework. For example, in Table 4 the authors have shown how the estimation of the class-prior in the training-time regularization helps to improve the performance."
                },
                "weaknesses": {
                    "value": "* The authors have used the auxiliary OOD training dataset and its effect on performance is not very clear. It would be interesting to see the sensitivity of the OOD performance with respect to the selection of different OOD training data. For example, what does the performance look like if we use the Cifar100 as OOD data for the model with Cifar10 as ID data? \n* The authors may need to report the performance of the wide range of OOD detection methods to demonstrate the effectiveness of their proposed post-hoc normalization and training time regularization. For example, the authors may consider the most representative OOD techniques like OpenMAX [1], CGDL [2], OLTR [3], etc.\n* The performance gain using the post hoc normalization seems to be marginal on OE and BinDisc, especially in terms of AUROC and AUPR. Having a more detailed explanation for this would be useful. \n\n**References**\n1. Bendale et al. \u201cTowards open set deep networks\u201d. CVPR2016. \n2. Sun et al. \u201cConditional gaussian distribution learning for open set recognition\u201d. CVPR2020\n3. Liu et al. \u201cLarge-scale long-tailed recognition in an open world.\u201d CVPR2019."
                },
                "questions": {
                    "value": "It would be interesting to see the experimental results mentioned in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5203/Reviewer_r7k6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764002129,
            "cdate": 1698764002129,
            "tmdate": 1699636517122,
            "mdate": 1699636517122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rTUACZynbu",
                "forum": "am7BPV3Cwo",
                "replyto": "JNXcEu2JL5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer r7k6 (Part. I)"
                    },
                    "comment": {
                        "value": "We thank the valuable comments, and we answer them in detail as follows.\n\n> Q1: Ablation on the choice of auxiliary OOD training data.\n\nThanks for the insightful suggestion. In the main experiments reported in our manuscript, we follow PASCL $^{[1]}$ to adopt the TinyImages80M $^{[2]}$ as auxiliary OOD data for training-time regularization, and the models are tested on the SCOOD benchmark for 6 subsets. \nWe now train the models with CIFAR100 as auxiliary OOD data (CIFAR10-LT is ID), and test them on the remaining 5 subsets of the SCOOD benchmark $^{[3]}$ (CIFAR100 was originally used as test OOD data but is now eliminated).\nAccording to the results reported below, our ImOOD consistently outperforms the advanced PASCL method.\n\n|  Method  |   AUROC\u2191  |    AUPR\u2191  |   FPR95\u2193  |\n|:--------:|:---------:|:---------:|:---------:|\n|   PASCL  |   88.05   |   86.63   |   38.69   |\n| **Ours** | **89.14** | **87.30** | **35.48** |\n\nHowever, models trained with CIFAR100 perform slightly worse than models trained with TinyImages80M, indicating that a more generally collected auxiliary OOD dataset also contributes to the final OOD detection performance, which is consistent with common sense.\nWe will add the experiment and discussion.\n\n> Q2: Comparison with other representative OSR/OOD detection methods.\n\nThanks for the thoughtful suggestion. Due to the time limitation, we mainly compare our method with the representative OpenMAX $^{[5]}$ and OLTR $^{[6]}$ on CIFAR100-LT benchmark.\n\nBefore the comparison, we want to carefully identify that the classical Open-Set Recognition (OSR) setting is slightly different from the Out-Of-Distribution Detection (OOD) setting $^{[4]}$. In particular, vanilla OSR methods (like OpenMAX and OLTR) take a proportion (e.g., 50%) of CIFAR100-LT classes as ID (or, base) classes, and the remaining as OOD (or, novel) classes. No auxiliary OOD training data is utilized for optimization. To ensure comparability, we follow the OOD setting, that is, taking all 100 classes from CIFAR100-LT as ID classes, and use the corresponding [codebase](https://github.com/ma-xu/Open-Set-Recognition) to train the models of OpenMAX and OLTR without auxiliary OOD dataset. OOD test set is the SCOOD benchmark.\n\nThe results below show that compared to the MSP baseline, OpenMAX gains 1.4% on AUROC, and OLTR further improves 1.0% on AUROC and 3.5% on AUPR by taking the imbalanced distribution into account. Moreover, with the same model as MSP baseline, Energy shows comparable performance, and our method on post-hoc normalization further brings significant improvement, reaching 66.38% of AUROC and 76.98% of FPR95. Finally, we also report the performance of our method trained with auxiliary OOD data (i.e., TinyImages80M) merely as a reference, to avoid unfair comparison with the above methods without OOD training data.\n\n\n|  Method  |  Aux OOD Data  |    AUROC\u2191   |    AUPR\u2191    |    FPR95\u2193   |\n|:--------:|:--------------:|:-----------:|:-----------:|:-----------:|\n|    MSP   |       No       |    62.17    |    57.99    |    84.14    |\n|  OpenMAX |       No       |    63.58    |    57.91    |    80.40    |\n|   OLTR   |       No       |    64.52    |    60.43    |    82.88    |\n|  Energy  |       No       |    64.87    |    61.11    |    78.89    |\n| **Ours** |       No       |  **66.38**  |  **61.76**  |  **76.98**  |\n| **Ours** |    **_Yes_**   | **_74.14_** | **_68.43_** | **_65.73_** |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373683644,
                "cdate": 1700373683644,
                "tmdate": 1700373683644,
                "mdate": 1700373683644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z4pcmgh6vu",
                "forum": "am7BPV3Cwo",
                "replyto": "rTUACZynbu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5203/Reviewer_r7k6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5203/Reviewer_r7k6"
                ],
                "content": {
                    "title": {
                        "value": "Comment on author response"
                    },
                    "comment": {
                        "value": "I appreciate the thorough rebuttal. The authors have addressed most of my concerns and would like to keep my current score. However, I still feel that it is crucial to carefully choose auxiliary OOD data during the training, taking into account the specific nature of the OOD data that may arise in the testing phase.  As such, the applicability of the proposed technique in the real-world OOD detection task can be rather limited."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589033404,
                "cdate": 1700589033404,
                "tmdate": 1700589033404,
                "mdate": 1700589033404,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "knIDqHFe4E",
            "forum": "am7BPV3Cwo",
            "replyto": "am7BPV3Cwo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5203/Reviewer_qdXa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5203/Reviewer_qdXa"
            ],
            "content": {
                "summary": {
                    "value": "The main contribution of this paper is the introduction of a generalized statistical framework called ImOOD, which addresses the problem of detecting and rejecting unknown out-of-distribution (OOD) samples in the presence of imbalanced data distributions. The paper identifies two common challenges faced by existing OOD detection methods: misclassifying tail class in-distribution (ID) samples as OOD, and incorrectly predicting OOD samples as head class ID samples. Then, the authors propose a general framework that can lead to improved detection performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a generalised statistical framework called ImOOD, which addresses the problem of OOD detection in the presence of imbalanced data distributions. Imbalanced data distributions widely exist in real world, and a set of fundamental works have proposed to tackle this questions to boost OOD detection. It seems that there may still exist many open questions, making the research direction in this paper promising in OOD detection. \n\nThe paper identifies two common challenges faced by existing OOD detection methods, namely misclassifying tail class ID samples as OOD and incorrectly predicting OOD samples as head class ID samples. This identification helps in understanding the limitations of current approaches.\n\nThe proposed method  is driven by the Bayesian analysis, demonstrating the impacts of data imbalance and suggesting a general framework that can handle the distribution shift in OOD detection. The proposed framework, as claimed by the authors, can be used to handle imbalanced issue for a set of different scoring strategies, and the evaluation results further verify the power of their method against imbalance data more or less."
                },
                "weaknesses": {
                    "value": "A direct question is that if we have used learning algorithms that can handle imbalanced data (which is often the case in reality) to train the basic model, do we need to handle imbalanced data for OOD detection thereafter. \n\nThe authors identify two cases that make previous OOD detection methods fail, i.e., misclassifying tail class ID samples as OOD and incorrectly predicting OOD samples as head class ID samples. It is a direct conclusion and seemingly to be important, but could the authors further connect these two cases to previous works that handle imbalanced data in OOD detection and the proposed ImOOD. For example, why previous works, such as [1], will fail to discern these two cases and why ImOOD can overcome previous drawbacks. \n\ng should be defined in a proper position in advance. It seems that g should be the detector built upon f.\n\nMy main concern lies in the inaccurate estimation of beta, consisting of three terms that are all biased from my view. For P(i|X), a direct failure case is that when we have a detector whose g always greater than 0 (e.g., for distance based scoring), then P(i|X) will be always greater than 0.5. Thus, all data points will be taken as ID cases, making the estimation obviously a biased case. P(y|x,i) also suffers from biased estimation, due to the well known calibration failure of deep models. Such an issue, from my view, cannot be ignored in the field of OOD detection, since it is one of the main reason why MSP score is not effective in practice. The estimation of P(y) is also biased, since the training time data distribution (ID + OOD) is different from the test situation, thus n_o used for training cannot cover the real test situation. \n\n\nDetailed discussion about the hyper-parameters setting and the choice of auxiliary OOD data and the evaluation datasets should be discussed in detailed. More methods that handle OOD detection with imbalanced data should be considered here, such as [1]. More ablation studies to test the respective power of P(i|x), P(y), and P(y|x) should be tested.\n\n\n[1] Detecting Out-of-distribution Data through In-distribution Class Prior."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804893996,
            "cdate": 1698804893996,
            "tmdate": 1699636516958,
            "mdate": 1699636516958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gIyBPxk7LH",
                "forum": "am7BPV3Cwo",
                "replyto": "knIDqHFe4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qdXa (Part. I)"
                    },
                    "comment": {
                        "value": "We thank the valuable comments, and we answer them in detail as follows.\n\n> Q1: Can algorithms that merely handle the imbalanced ID classification immediately address the OOD detection issue?\n\nThanks for pointing out the important preliminary problem. Wang et al.$^{[1]}$ had widely discussed this issue and conducted a series of experiments (part of the results on CIFAR10-LT is displayed below). The results indicated that simply combining the ID classification optimization (e.g., LA $^{[2]}$ or $\\tau$-norm $^{[3]}$) with vanilla OOD detection techniques (e.g., OE) cannot address the challenge. We thus have to take imbalanced ID classification and OOD detection into integrated consideration.\n\n|   Method  | ARUOC\u2191 |  AUPR\u2191 | FPR95\u2193 |\n|:---------:|:-----:|:-----:|:-----:|\n|  OE + **None**  | _89.92_ | _87.71_ | 34.80 |\n| OE + $\\tau$-norm | 89.58 | 85.88 | _33.80_ |\n|   OE + LA   | 89.46 | 86.39 | 34.94 |\n|    **Ours**   | **92.73** | **92.31** | **28.27** |\n\nRelevant citations and conclusions will be added to the revised manuscript.\n\n> Q2: Why previous methods failed at handling the imbalance problem but our ImOOD works?\n\nThanks for the thoughtful suggestion. We briefly summarize the main drawbacks of prious representative methods (PASCL $^{[1]}$ and ClassPrior $^{[4]}$):\n\n1. **Our method** concurrently considers the misidentified ID samples from tail-class and OOD data predicted as head-class, as discussed in Section 3.1 in our manuscript.\n2. **PASCL** focuses on improving the separability between tail-class ID samples and OOD data, while **ignores** the misidentification between OOD data and ID head-class, as illustrated in Figure 1 in our manuscript.\n3. **ClassPrior** takes both the two factors into account, and develops a series of manually-designed post-hoc normalization techniques on models only trained with ID data. However, it makes a **strong assumption** that the posterior probability of OOD data equals the prior ID probability, that is, $P(y|x \\in \\mathcal{D}^{out}) = P(y)$ (see its Theorem 3.2). The question is this assumption does not always hold. In our Table 2, we report its \"RP+MSP\" (RePlacing) strategy on models trained with CIFAR10-LT, which leads to negligible improvements (e.g., 0.3% increase of AUROC). We further test its \"RW+Energy\" (ReWeighting) strategy on the same model, as shown in the table below, which even causes performance degradation. **In contrast**, our method does not make any assumption on $P(y|x \\in \\mathcal{D}^{out})$, and shows strong robustness to the practical scenarios (e.g., 4.3% increase of AUROC). **Besides**, our ImOOD provides training-time regularization to further boost the imbalanced OOD detection, where ClassPrior is inapplicable since its assumption $P(y|x \\in \\mathcal{D}^{out}) = P(y)$ does not necessarily hold during training.\n\n|  Post-hoc  | AUROC\u2191 |  AUPR\u2191 | FPR95\u2193 |\n|:----------:|:-----:|:-----:|:-----:|\n|    Energy  | 73.16 | 71.73 | 69.19 |\n| ClassPrior | 72.22 | 70.88 | 70.75 |\n|    **Ours**    | **77.48** | **76.29** | **66.47** |\n\nWe will supplement those discussions to clarify our novelty and contribution."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373418984,
                "cdate": 1700373418984,
                "tmdate": 1700373650704,
                "mdate": 1700373650704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]