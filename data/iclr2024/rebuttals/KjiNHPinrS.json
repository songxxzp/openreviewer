[
    {
        "title": "Timesteps meet Bits: Low-Latency, Accurate, & Energy-Efficient Spiking Neural Networks with ANN-to-SNN Conversion"
    },
    {
        "review": {
            "id": "N7BFRFMlqS",
            "forum": "KjiNHPinrS",
            "replyto": "KjiNHPinrS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission452/Reviewer_JeWJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission452/Reviewer_JeWJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel ANN-SNN framework including using QCFS to get the pretrained ANN and then apply a three-step conversion. This article changes the encoding method of IF neurons to binary and further enhance the representation ability. The paper also proposes a fine-grained regularizer to reduce the spiking activity of the SNN, which further improves the compute efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a novel binary encoded IF neuron and modified the conversion process to fit the binary encoding. The idea of the sparsity regularizer is also interesting.\n\nThe experimental results are good. Within 4 timestep, the network can get comparable results with ANN even on ImageNet dataset."
                },
                "weaknesses": {
                    "value": "> \u201cNote that our neuron model simply postpones the firing and reset mechanism until after the input current accumulation over all the T time steps, and does not change the complexity of the traditional.\u201d \n\nI am not sure what this means. Does it indicate that the neuron here will accumulate the input without firing and reset until the last timestep? Authors should explain why they change the neuron behavior. Also, authors should demonstrate more detailed ablation study on the proposed binary encoding and conversion framework using the normal IF neuron.\n\n\nThe sparsity regularizer is a loss function that constrain the activation values in the ANN. However, as the coefficient parameter lambda increases, the performance of SNN will decrease, so the author should discuss more to prove that the advantage of reducing by  adding constraints is greater than the performance loss. Also, in Figure 4, the spiking activity with lambda=1e-8 is larger than the spiking activity with lambda=0. Does that mean the sparsity regularizer does not work?"
                },
                "questions": {
                    "value": "Please refer to the concerns addressed in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission452/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697570725530,
            "cdate": 1697570725530,
            "tmdate": 1699635971621,
            "mdate": 1699635971621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "CYLcLMM94i",
            "forum": "KjiNHPinrS",
            "replyto": "KjiNHPinrS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission452/Reviewer_yJuh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission452/Reviewer_yJuh"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a Spiking Neural Network (SNN) architecture with low latency and high model accuracy by introducing an approach of shifting input values by 1 bit (or equivalent to multiplying by 2) at each time step. This shifting mechanism takes advantage of the binary nature of spikes in SNNs, resulting in a significant reduction in the spiking length, scaling down the representation to a logarithmic scale."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is written with remarkable clarity, and the experiments are very comprehensive."
                },
                "weaknesses": {
                    "value": "The novelty of this paper may be questionable, as a previously published paper [1] discusses the use of similar shifting technology for converting Artificial Neural Networks (ANN) to SNN. Their work provides a more general theory, demonstrating that the scale of shifting can vary not only to 2 but also to other fractions such as 3, 4, and beyond. Thus, it can be argued that this paper can be considered a specific case or an application of the more comprehensive theory presented in [1]. Alternatively, another published paper by Kim et al. [2] explores a similar idea but approaches it from the opposite direction in shifting.\n\n\n[1] Wang, Z., Gu, X., Goh, R. S. M., Zhou, J. T., & Luo, T. (2022). Efficient spiking neural networks with radix encoding. IEEE Transactions on Neural Networks and Learning Systems.\n\n[2] Kim, J., Kim, H., Huh, S., Lee, J., & Choi, K. (2018). Deep neural networks with weighted spikes. Neurocomputing, 311, 373-386.\n\n\nThe primary distinction between this paper and the previous work [1] is the object of shifting, with this paper proposing to shift the input left, while [1] focuses on shifting the output right. However, it can be argued that this contribution may not be substantial enough to warrant a separate paper submission to a prestigious conference like ICLR, as these two shifting policies are mathematically equivalent and yield nearly identical results.\n\nIn conclusion, while the paper presents an interesting concept for improving SNN performance, its novelty is questionable, given the existence of related work [1] that provides a more general theory and [2] that explores similar ideas from a different perspective. The contribution of changing the direction of shifting alone may not be sufficient to justify a separate publication in ICLR."
                },
                "questions": {
                    "value": "It would be beneficial if the authors could provide a clarification regarding the unique contributions of this paper in relation to the previously mentioned work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission452/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission452/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission452/Reviewer_yJuh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission452/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742478919,
            "cdate": 1698742478919,
            "tmdate": 1700671305482,
            "mdate": 1700671305482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "gyRwm4o7I2",
            "forum": "KjiNHPinrS",
            "replyto": "KjiNHPinrS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission452/Reviewer_bagt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission452/Reviewer_bagt"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach to convert artificial neural networks (ANNs) to spiking neural networks. The method relies on the quantization of activations. It has been shown that the proposed method can improve the accuracy performance of SNNs and reduce their spiking activities."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method improves the accuracy performance of SNNs on ImageNet dataset.\nThe spiking activities have been reduced."
                },
                "weaknesses": {
                    "value": "The proposed method is similar to quantized neural networks. As such, a comparison is required with quantized neural networks in terms of both accuracy and efficiency. \n\nThe type of each vector/matrix (e.g., integer, real, etc) needs to be specified."
                },
                "questions": {
                    "value": "My main question is the difference between this work and bit-serial quantized networks in terms of computations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission452/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801810254,
            "cdate": 1698801810254,
            "tmdate": 1699635971441,
            "mdate": 1699635971441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "zlrGq3MmIp",
            "forum": "KjiNHPinrS",
            "replyto": "KjiNHPinrS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission452/Reviewer_nh19"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission452/Reviewer_nh19"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the source of conversion error in ANN to SNN and proposed an accurate and efficuent conversion method. The error reduction is a step-up over previous methods in identifying some extra mitigation methods that are shown to be effective as experimental results suggests"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper is well organized and mostly easy to follow.\n* Experiments are conducted extensively in numerous dataset and compared against recent methods.\n* Background are discussed throughly.\n* Extra error gaps and mitigation methods, are summurized clearly over previous methods.\n* Experimental results, expecially at two steps, demonstrates clear improvement in speed and accuracy over previous methods."
                },
                "weaknesses": {
                    "value": "* My understanding is that this address some of the error gaps uncovered in the QCFS paper. While the paper has demnstrated faster conversion steps and better accuracy performance, would it be possible to compare and discuss energy against at least QCFS method?"
                },
                "questions": {
                    "value": "Please see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission452/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804625922,
            "cdate": 1698804625922,
            "tmdate": 1699635971348,
            "mdate": 1699635971348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]