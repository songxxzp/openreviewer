[
    {
        "title": "Dynamic Discounted Counterfactual Regret Minimization"
    },
    {
        "review": {
            "id": "6emmtxogRs",
            "forum": "6PbvbLyqT6",
            "replyto": "6PbvbLyqT6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4228/Reviewer_ESsF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4228/Reviewer_ESsF"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the CFR framework in the setting of incomplete information games, and proposes a modification that allows for equilibrium finding using a dynamic, automatically-learned scheme. To do so, the authors formulate CFR\u2019s iteration process as an MDP and give a transformation that frames the (CFR) discount scheme as a policy optimization problem wherein the dynamic discounting scheme is an agent that interacts with the MDP. The primary motivation of this framework is to be able to learn discounting policies that can generalize over different IIGs, rather than having to be carefully designed for specific games. In this direction, the authors design a training algorithm based on evolution strategies that can generalize well, even to games that are not seen in the training process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The core idea of the paper, namely the dynamic policy optimization procedure for learning discount schemes, is conceptually important since it allows generalization to unseen games, circumventing the need for suboptimal fixed discounting schemes. Moreover, the experiments prevented are convincing, showing not just benchmarking against current state of the art but also ablation studies which gives a stronger idea of what aspects of DDCFR makes the most improvement over standard CFR approaches. The DDCFR methodology is also shown to be effective in modifying other CFR variants, which is a useful property to have. Moreover, the paper is well written in the way that motivations are clear, and any potential concerns are discussed in fair detail (for instance, the discussion about the potential additional computational costs)."
                },
                "weaknesses": {
                    "value": "The framework introduced seems to be useful in practice and effectively stitches together several well-established ideas. A minor weakness would be that there isn\u2019t much technical novelty, since all the results and techniques used are taken from prior works. However, this doesn\u2019t diminish the quality of the results greatly to me. Another minor weakness is that currently the explanations in the ablation studies section feel quite vague and handwavy. My suggestion to the authors would be to highlight several key results in the table and provide a more substantiated explanation for each. It would also have been nice to see running time comparisons in the experiments to properly verify the authors\u2019 claims about additional computational costs not causing major increases in running time."
                },
                "questions": {
                    "value": "- How does the DDCFR technique compare to deep methods like Deep CFR when it comes to larger scale games? \n- How does DDCFR empirically scale to games with more players? In principle, it seems to me that it would provide similar benefits compared to current CFR methods in multiagent settings but I\u2019m curious if this is something the authors have explored."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Reviewer_ESsF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4228/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698124081106,
            "cdate": 1698124081106,
            "tmdate": 1699636389981,
            "mdate": 1699636389981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UhyC13jYDs",
                "forum": "6PbvbLyqT6",
                "replyto": "6emmtxogRs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ESsF"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer for the positive recommendation of our work and the insightful feedback. Following are our responses to your concerns.\n\n* **How does the DDCFR technique compare to deep methods like Deep CFR when it comes to larger scale games?**\n\nMany thanks for your constructive comments. DeepCFR finds an approximate Nash equilibrium by combining a *base CFR-type algorithm* with deep neural network function approximation. For instance, the original DeepCFR paper [1] employs LinearCFR as the base CFR algorithm, which is a specific version of DCFR.\n\nBased on these considerations, *there are inherent differences in the underlying principles and objectives of DDCFR and DeepCFR*. Specifically, DDCFR focuses on enhancing the performance of base CFR-type algorithms through dynamic discounting. In contrast, DeepCFR aims to use neural networks to approximate and generalize the behavior of base CFR-type algorithms. Consequently, it is not appropriate to directly compare these two approaches from our perspective. \n\nWe want to thank you again for your insightful comment, which has inspired us greatly. In particular, *since our DDCFR is a strong CFR-type algorithm, it can also serve as a strong base algorithm for DeepCFR.* DeepCFR, in turn, can utilize neural networks to approximate and generalize the behavior of DDCFR, thereby potentially improving its performance compared to using the original LinearCFR base algorithm. Since deep methods are not the focus of our paper, we consider this promising idea as an intriguing avenue for future research.\n\n[1] Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret minimization. In International Conference on Machine Learning, pp. 793\u2013802, 2019.\n\n* **How does DDCFR empirically scale to games with more players?**\n\nThank you very much for your insightful question. Our work focuses on solving two-player zero-sum games, and we have not delved into the application of DDCFR in multiplayer games for the following reasons:\n\nFor two-player zero-sum games, CFR effectively computes strategies that converge to an approximate Nash equilibrium. A Nash equilibrium is useful in two-player zero-sum games since it ensures that the player maximizes its utility against a worst-case opponent, effectively avoiding losses. However, when applied to multiplayer games, CFR loses all theoretical guarantees. It is not guaranteed to converge to a Nash equilibrium, and playing a Nash equilibrium may not be wise in multiplayer games [1].\n\nNevertheless, CFR-type algorithms have the potential to generate strategies that *empirically* perform well in multiplayer games [2,3]. We believe that our DDCFR is a general framework that can enhance CFR's empirical performance in multiplayer games as well. We will explore this exciting direction in our future work.\n\n[1] Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456): 885\u2013890, 2019.\n\n[2] Nicholas Abou Risk and Duane Szafron. Using counterfactual regret minimization to create competitive multiplayer poker agents. In International Conference on Autonomous Agents and Multiagent Systems, pp. 159\u2013166, 2010.\n\n[3] Richard Gibson. Regret minimization in games and the development of champion multiplayer computer poker-playing agents. PhD thesis, University of Alberta, 2014.\n\n* **It would also have been nice to see running time comparisons in the experiments to properly verify the authors' claims about additional computational costs not causing major increases in running time.**\n\nThank you very much for your constructive suggestion. We have added a table in Appendix D, which compares the running time between DCFR and DDCFR. The results show that the additional computation time incurred by DDCFR is negligible compared to the overall iteration time."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150128890,
                "cdate": 1700150128890,
                "tmdate": 1700150128890,
                "mdate": 1700150128890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SNtWN45mqr",
                "forum": "6PbvbLyqT6",
                "replyto": "UhyC13jYDs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Reviewer_ESsF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Reviewer_ESsF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for the response and answers to my questions, they were very helpful. My recommendation for acceptance remains unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461802012,
                "cdate": 1700461802012,
                "tmdate": 1700461802012,
                "mdate": 1700461802012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6cQuk0ko0q",
            "forum": "6PbvbLyqT6",
            "replyto": "6PbvbLyqT6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4228/Reviewer_ffQX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4228/Reviewer_ffQX"
            ],
            "content": {
                "summary": {
                    "value": "The paper offers an interesting extension to counterfactual regret minimization inspired by reinforcement learning for learning the Nash equilibrium in imperfect information games. Specifically, the paper proposes an MDP formulation of adjusting the discount rate in DCFR algorithms, a provable guarantee for the proposed RL approach, empirical tips and tricks for speeding up the convergence of the RL algorithm, and simulation studies on incomplete information games."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem studied by the paper is an interesting one and the approach taken is intriguing.\n2. The paper provides the learned models in the supplemental materials, which make the results easier to verify.\n3. The paper offers a comprehensive overview of the problem setting and discusses well its contributions in context of existing works.\n4. The set of games considered in the experiments is comprehensive."
                },
                "weaknesses": {
                    "value": "1. The paper could be better organized. Admittedly the problem studied here is a challenging one and necessitates heavy notations, as well as a collection of specific terminologies in both AGT and RL. However, some concepts are not used (or at least used explicitly), causing the paper to appear to be overwhelmingly dense at a first read. Assuming that the paper is targeted at audience with a game theory background, perhaps it would make more sense to spend some time describing and motivating the key algorithmic contributions from the RL angle. For instance, it might be useful to defer the mathematically rigorous definitions of *information set reach probability* or *interval history reach probability* to the appendix, and keep in the main body only an intuitive description of these concepts, as the paper's key contribution lies in the MDP-based approach proposed later. This takes up space and takes attention away from more important concepts such as exploitability $e$ and \"realized\" exploitability $E$.\n2. On are related note, there are some overloaded notations that make the paper harder to parse. For instance, the letter $a$ refers both to the actions taken by the agents in the \"Game Theory\" part of the paper and the action taken by the learner in the \"Reinforcement Learning\" part of the paper.\n3. For the experiments, it would be nice if the authors could provide the wall-clock times for DDCFR. DCFR has an added benefit that the algorithm requires little computation complexity at each round (the discount rates are specified beforehand) and it would be a fairer comparison to consider the computational aspect as well. (I am convinced by the arguments in Section 3.4, but adding these results will better validate the claims in Section 3.4 empirically.)\n4. Theorem 1 requires the action space to be discretized, yet this is not stated explicitly (also see the following section for question on boundedness of $\\tau_t$.\n5. It appears that experiments are only run once, yet multiple runs are required to demonstrate that the proposed approach convincingly outperform DCFR, and the figures in the paper are not due to luck alone."
                },
                "questions": {
                    "value": "1. Shouldn't there be (at the very least) some mild assumptions on boundedness of $\\tau$? In the degenerate case, suppose the initial policy always select $\u03c4 \\to \\infty$ and some really poor choice of discount rate. Will the algorithm still converge at the rate provided in Theorem 1?\n2. Purely out of curiosity, why should we directly \"jump\" to RL which, as the paper points out, \"notably sensitive to hyperparameters\". Does it make sense to consider a bandit-based approach for optimizing the discount rates?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Reviewer_ffQX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4228/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698017966,
            "cdate": 1698698017966,
            "tmdate": 1700160381374,
            "mdate": 1700160381374,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HS6f8hRXCI",
                "forum": "6PbvbLyqT6",
                "replyto": "6cQuk0ko0q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ffQX (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the insightful feedback and your efforts in reviewing our paper. Following are our responses to your concerns.\n\n* **The paper could be better organized.**\n\nThank you very much for your valuable suggestion. Section 2 of the current paper contains a significant number of formulas, potentially imposing a heavier reading burden on the audience. Following your advice, we have carefully restructured Section 2. In this revised version, we provide only intuitive descriptions of key concepts, such as counterfactual values and reach probabilities, in Section 2. We have moved the mathematically rigorous definitions of these terms to Appendix A, as well as the related terms relationship $g \\subseteq h$ and player function $\\mathcal{P}(h)$. Furthermore, We have simplified the definition of the range of utilities $\\Delta$. We believe that this updated version will enhance the paper's accessibility for researchers in both the AGT and RL communities.\n\n* **There are some overloaded notations that make the paper harder to parse.**\n\nThank you very much for pointing out the issue with overloaded notation making certain parts of the paper ambiguous. Using unique symbols eliminates confusion and improves readability.\n\nFollowing your suggestion, we have made the following modifications.\n\n1. We use $\\hat{a}$ to denote the action of the RL agent and to distinguish it from the action $a$ in the game tree.\n2. We use $\\hat{R}^G$ to denote the reward function in MDP and to distinguish it from the cumulative regret $R$ in CFR.\n3. We use $\\delta$ to denote the noise standard deviation in ES and to distinguish it from the strategy $\\sigma$ in the game.\n4. We use $N$ to denote the population size in ES and to distinguish it from the state transition $P^G$ in MDP.\n5. We uniformly use $t$ to denote one iteration and $T$ to denote the total number of iterations.\n\n* **For the experiments, it would be nice if the authors could provide the wall-clock times for DDCFR.**\n\nThank you very much for your constructive suggestion. To further support claim in Section 3.4, we have added a table in Appendix D, which compares the wall-clock time between DCFR and DDCFR. The results show that the additional computation time incurred by DDCFR is negligible compared to the overall iteration time."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150034571,
                "cdate": 1700150034571,
                "tmdate": 1700150034571,
                "mdate": 1700150034571,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ckEvqw50BU",
                "forum": "6PbvbLyqT6",
                "replyto": "6cQuk0ko0q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ffQX (2/2)"
                    },
                    "comment": {
                        "value": "* **Theorem 1 requires the action space to be discretized, yet this is not stated explicitly (also see the following section for question on boundedness of $\\tau$)**\n\nThank you very much for your comments. It seems that there may be a slight misunderstanding regarding Theorem 1. Please allow us to provide a more detailed explanation.\n\nFirst of all, Theorem 1 supports a continuous action space, as along as actions are within a certain range. For example, in every iteration $t$, $\\beta\\_t$ can take on any real value between -5 and 0.\nBy the way, our DDCFR also adopts a continuous action space design. The ablation studies in Section 4.3 demonstrate that the continuous action space design outperforms the discrete action space design.\n\nOn the other hand, the algorithm still converges even when $\\tau \\rightarrow \\infty$. As $\\tau\\rightarrow \\infty$, DDCFR never adjusts the hyperparameters and always uses the initial discount rate as you mentioned. In other words, $\\forall t \\le T, \\alpha\\_t=\\alpha\\_1, \\beta\\_t=\\beta\\_1, \\gamma\\_t=\\gamma\\_1$. As long as $\\alpha\\_1, \\beta\\_1, \\gamma\\_1$ are within the range specified by Theorem 1, it still satisfies the requirement of Theorem 1. Therefore, the algorithm still converges at the rate provided in Theorem 1.\n\nIn fact, we can view DCFR as a special case in this setting, where $\\alpha\\_1=1.5, \\beta\\_1=0, \\gamma\\_1=2, \\tau\\_1=\\infty$. This specific configuration meets the requirements of Theorem 1.\n\nIn summary, Theorem 1 indicates that numerous discounting schemes converge in theory as long as the discounting weights in each iteration are within a certain range. The primary contribution of our DDCFR is the discovery of a dynamic and automatically learned scheme.\n\n* **It appears that experiments are only run once, yet multiple runs are required to demonstrate that the proposed approach convincingly outperform DCFR, and the figures in the paper are not due to luck alone.**\n\nThank you very much for your comments. We want to emphasize that there is *NO randomness throughout the iteration process* of DDCFR when using the learned discounting policy on the testing games. Specifically, DDCFR always initializes with the uniform random average strategy, and the policy network consistently produces the same fixed action when inputs are determined. Put simply, each run of DDCFR with the learned model consistently generates the same exploitability curve.\n\nWe use ES to learn the discounting policy, and we select one of the best performing policy on the training games for testing. In response to your concerns, we independently ran the ES training process 5 times and tested the performance of the resulting 5 discounting policies. In Appendix F of the revised paper, we have added figures to show the average performance of DDCFR. The results show that DDCFR is still superior to other baselines. This proves that the superiority of DDCFR is not due to luck.\n\n* **Does it make sense to consider a bandit-based approach for optimizing the discount rates?**\n\nAs far as we know, in a typical bandit scenario, there is usually a single state with multiple available actions. However, in our setting, the agent takes actions across multiple states, ranging from $s\\_1$ to $s\\_T$. The optimal action varies from one state to another. Therefore, based on our understanding, the bandit-based approach is less suitable for our scenario."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150075800,
                "cdate": 1700150075800,
                "tmdate": 1700150075800,
                "mdate": 1700150075800,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b5Lie4H8bU",
                "forum": "6PbvbLyqT6",
                "replyto": "ckEvqw50BU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Reviewer_ffQX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Reviewer_ffQX"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their detailed feedback and timely updates to the manuscript. I have increased my score.\n\nI think it *might* be useful to consider clarifying that $|A|$ does not mean the cardinality of a finite set, which seems to be a more commonly used definition in RL. Same for emphasizing that runs in the experiment section are deterministic as opposed to stochastic, which again is more common in RL. These are really, really minor nitpicks."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700160368264,
                "cdate": 1700160368264,
                "tmdate": 1700160368264,
                "mdate": 1700160368264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bQrzopsIHa",
                "forum": "6PbvbLyqT6",
                "replyto": "6cQuk0ko0q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Dear reviewer, thank you very much for reviewing our response and updating the score. We genuinely appreciate your meticulous reading and insightful comments. \n\n* **I think it might be useful to consider clarifying that $|A|$ does not mean the cardinality of a finite set, which seems to be a more commonly used definition in RL.**\n\nThank you very much for your good suggestion. We have incorporated the clarification in section 2.1 of the revised paper: *With a slight abuse of the cardinality notation $|\\cdot|$, we use $\\vert\\mathcal{A}\\vert$ to denote the maximum number of actions within each information set, i.e., $\\vert\\mathcal{A}\\vert = \\max\\_{I\\in\\mathcal{I}}\\vert \\mathcal{A}(I) \\vert$.*\n\n* **Same for emphasizing that runs in the experiment section are deterministic as opposed to stochastic, which again is more common in RL.**\n\nThank you very much for this suggestion. We have also clarified this in section 4.2 of the revised paper: *It is worth noting that, unlike agent performance testing in RL, which usually requires multiple runs due to the randomness in the environment, the iteration process of DDCFR is deterministic (Appendix F). Therefore, we only need to test it once in each game.*\n\n&nbsp;\n\nThank you once again for your invaluable contributions to our work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700195240189,
                "cdate": 1700195240189,
                "tmdate": 1700196345381,
                "mdate": 1700196345381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9RhNXkn0Lz",
            "forum": "6PbvbLyqT6",
            "replyto": "6PbvbLyqT6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4228/Reviewer_u3Qf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4228/Reviewer_u3Qf"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a dynamic weighting scheme for iterates of CFR, employing an MDP to determine optimal weights based on the algorithm's state. The empirical results support the effectiveness of this approach. Their scheme/framework is modular and can be applied to state-of-the-art CFR variants including DCFR and PCFR$^+$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I reviewed a previous version of this paper.\n\nThe reviewers have adequately addressed the concerns brought up by the reviewers in this version of the paper. This includes applying their framework to PCFR$^+$ and comparing DDCFR directly to PCFR$^+$, as well as comparing to Greedy Weights.  Additionally, they provide a convergence analysis of their algorithm."
                },
                "weaknesses": {
                    "value": "The algorithms' numerical performances are depicted starting at 500 iterations. It would be nice to show the performance of the algorithms in early iterations. \n\nWhile the paper notes that the time spent training the network to be used for the dynamic weight calculation is amortized by the number of times the policy computed is applied in application of the framework to equilibrium computation in different games, it never explicitly states how much time was spent training the network."
                },
                "questions": {
                    "value": "I would suggest using a log-scale on the x-axis and start the x-axis at the first iteration. This allows easy comparison of the algorithms being compared and makes clear both short-term performance and long-term performance. It is confusing that the exploitability starts at small values at the beginning of the graph and then doesn't seem to change much in Kuhn, for example.\n\nCould you note the actual amount of time required to training the policy in Section 3.4? Additionally, I would suggest ordering \"(i)\", \"(ii)\", and \"(iii)\", in the same order you state them in Line 264."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Reviewer_u3Qf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4228/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792246981,
            "cdate": 1698792246981,
            "tmdate": 1699636389805,
            "mdate": 1699636389805,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qyQQEAK3Ia",
                "forum": "6PbvbLyqT6",
                "replyto": "9RhNXkn0Lz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u3Qf"
                    },
                    "comment": {
                        "value": "Thank you very much for your constructive comments and acknowledgment of our efforts. Your previous feedback has been immensely helpful in improving our work's quality. Following are our responses to your concerns.\n\n* **The algorithms' numerical performances are depicted starting at 500 iterations. It would be nice to show the performance of the algorithms in early iterations.**\n\nFollowing your valuable suggestions, we have added figures to include all iterations in Appendix E of the revised paper.\n\n* **Could you note the actual amount of time required to training the policy in Section 3.4?**\n\nThank you for your helpful suggestion. We should have explicitly stated the training time for the policy to provide key details of the overall approach.\n\nSpecifically, we trained the agent for 1000 epochs across 200 CPU cores, which took approximately 24 hours. Per your advice, We have added this information at the training details in the revised paper.\n\n* **I would suggest ordering \"(i)\", \"(ii)\", and \"(iii)\", in the same order you state them in Line 264.**\n\nFollowing your suggestion, we have unified the order in section 3.4 of the revised paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149957308,
                "cdate": 1700149957308,
                "tmdate": 1700151328538,
                "mdate": 1700151328538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Chx8cXaCNy",
                "forum": "6PbvbLyqT6",
                "replyto": "qyQQEAK3Ia",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Reviewer_u3Qf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Reviewer_u3Qf"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for your response, especially for including the figures and noting the training time. I confirm my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490461667,
                "cdate": 1700490461667,
                "tmdate": 1700490461667,
                "mdate": 1700490461667,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t3i5OvkdPM",
            "forum": "6PbvbLyqT6",
            "replyto": "6PbvbLyqT6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4228/Reviewer_f8hB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4228/Reviewer_f8hB"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a Reinforcement Learning framework to manage the decaying parameters of Discounted CFR dynamically. In order to do so, the paper proposes a game-agnostic neural policy that receives as input the iteration and current exploitability and outputs DCFR's parameters and a time window for which those parameters will be used.\nThe proposed algorithm is proven to maintain the same (ignoring constants) regret guarantees than CFR, and experiments validate the approach of training a general policy in simple games and then evaluating it on a suite including much larger games. Ablations are provided, showing the value of each piece of the algorithm presented."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* simple idea. The whole paper is built on an initial observation and a single concept. This makes the paper follow a linear and clear structure\n* clear presentation. The paper is well written and answers almost all the questions one may have on the topic.\n* clear and sound experimentation framework. Experiments answer all the questions.\n* good empirical results. The results are encouraging and worth of publication"
                },
                "weaknesses": {
                    "value": "* the tradeoff between computational power and significance of the results is arguable, While I agree on the amortized costs of training on small games and use the same policy on larger instances, the computation of accurate exploitability is a hard constraint of the technique as presented in the paper. While I think that this is the largest weakness on the scalability of the technique, I do not think that such a constraint should be circumvented in this paper.\n* the comparison with RL algorithms could be explored a bit more. I'm convinced that PPO is a bad choice in the given setting due to long horizons and sparse rewards, but I'm not convinced on the multi-task nature of the RL challenge at hand. In my view, the task is identical and there is just stochasticity induced by the distribution over games during training. I'd expect a multi-task problem to explicitly pass to the policy the task as observation in order to condition the policy. My suggestion is to expand a bit more on the topic in the paper.(e.g. why wouldn't sset gamma=1 solve the long horizon problem?)\n\nMinor suggestions:\n* Is equation 4 wrongy formatted? Especially the parentheses and the $\\ast$ symbols\n* add the default DCFR parameters as a constant line in the plots in Figure 4"
                },
                "questions": {
                    "value": "* in section 3.4, you say that the exploitability computation can be done in parallel. I'm not conviced about this, since to run a CFR computation you need the parameters, which depend on the explotability of the average strategy **after** the latest iteration. Am I missing something? The only parallelizable step is the computation of the gradient in the ES subroutine\n* what happens in the first 500 iterations of DDCFR? Why are those hidden?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4228/Reviewer_f8hB"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4228/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844968803,
            "cdate": 1698844968803,
            "tmdate": 1699636389733,
            "mdate": 1699636389733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "17ETPFjGJq",
                "forum": "6PbvbLyqT6",
                "replyto": "t3i5OvkdPM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer f8hB (1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for your positive recommendation of our work and your insightful comments. Following are our responses to your concerns.\n\n* **Why the exploitability computation can be done in parallel.**\n\nWe sincerely apologize for any confusion arising from our inaccurate description of \"the exploitability calculation and the CFR iteration process are independent\". In fact, what we are trying to convey is that *the exploitability calculation of the average strategy at iteration $t-1$ and the instantaneous regrets calculation at iteration $t$ are independent.* Allow us to provide a more detailed clarification.\n\nOn each iteration $t$, our DDCFR first recursively traverses the game tree using the strategy $\\sigma^t$ to compute the instantaneous regret $r^t$. Then, DDCFR updates the cumulative regrets $R^t$, the average strategy $\\bar{\\sigma}^t$, and the new strategy $\\sigma^{t+1}$ using the weights $(\\alpha\\_t,\\beta\\_t,\\gamma\\_t)$ generated by the discounting policy $\\pi\\_{\\boldsymbol{\\theta}}(s\\_t)$. Specifically, $\\alpha\\_t,\\beta\\_t,\\gamma\\_t=\\pi\\_{\\boldsymbol{\\theta}}(s\\_t)=\\pi\\_{\\boldsymbol{\\theta}}([\\hat{t}, \\hat{E}\\_{t-1}^G])$, where $\\hat{t}$ is the normalization of iteration $t$ and $\\hat{E}\\_{t-1}^G$ is the normalized exploitability of the average strategy $\\bar{\\sigma}^{t-1}$ at iteration $t-1$.\n\nCalculating $\\hat{E}\\_{t-1}^G$ requires traversing the game tree using the average strategy $\\bar{\\sigma}^{t-1}$, which leads to additional time overhead under naive implementation. *However, it's crucial to note that we can calculate $\\hat{E}\\_{t-1}^G$ and compute the instantaneous regret $r^t$ in parallel since these two processes depend on two different strategies, $\\bar{\\sigma}^{t-1}$ and $\\sigma^t$, and each requires traversing the game tree once.* Consequently, there is no additional wall-clock time overhead associated with the calculation of exploitability in our parallel implementation.\n\nThe only time overhead of our DDCFR is the policy inference time, i.e., calculating $\\pi\\_{\\boldsymbol{\\theta}}([\\hat{t}, \\hat{E}\\_{t-1}^G])$. However, this cost is negligible compared to the overall iteration time. Please refer to appendix D for more details.\n\nWe genuinely appreciate your insightful comment, which has prompted us to refine the description in Section 3.4. Additionally, we have corrected the notation for normalized exploitability from $\\hat{E}\\_{t}^G$ to $\\hat{E}\\_{t-1}^G$ in Section 3.1 of the revised paper.\n\n* **The scalability of the proposed technique.**\n\nOur DDCFR algorithm is founded on the state-of-the-art DCFR algorithm. Consequently, the scope of application for our DDCFR is identical to that of DCFR. When scaling up to larger games, we can leverage numerous well-established techniques, such as decomposition [1] and subgame-solving [2,3], to mitigate problem complexity.\n\nTaking inspiration from your valuable feedback, we plan to explore how to integrate the concept of dynamic discounting into decomposition and subgame-solving techniques to address larger-scale imperfect information games in the future.\n\n[1] Neil Burch, Michael Johanson, and Michael Bowling. Solving imperfect information games using decomposition. In AAAI Conference on Artificial Intelligence, pp. 602\u2013608, 2014.\n\n[2] Noam Brown and Tuomas Sandholm. Safe and nested subgame solving for imperfect-information games. In Advances in Neural Information Processing Systems, pp. 689\u2013699, 2017.\n\n[3] Noam Brown, Tuomas Sandholm, and Brandon Amos. Depth-limited solving for imperfect-information games. In Advances in Neural Information Processing Systems, pp. 7674\u20137685, 2018."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149864690,
                "cdate": 1700149864690,
                "tmdate": 1700149864690,
                "mdate": 1700149864690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SIAhn2Nd2y",
                "forum": "6PbvbLyqT6",
                "replyto": "t3i5OvkdPM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer f8hB (2/2)"
                    },
                    "comment": {
                        "value": "* **The comparison with RL algorithms could be explored a bit more.**\n\nThank you very much for the insightful feedback. When formulating CFR's iteration process as an MDP, applying RL to optimize the discounting policy seems to be the most natural and straightforward approach. However, we found that achieving meaningful results using RL (e.g., PPO) is challenging and exhibits significant instability.\n\nIn fact, multitask learning is an important issue in our setting. The training games are diverse in size with different converge speed, balancing the learning across different task in multi-task RL is difficult and requires extensive hyperparameter tuning. By the way, it seems like that explicitly conditioning the policy on game IDs may not generalize well to unseen testing games.\n\nGiven these challenges, our current approach opts for the simple, scalable and robust ES to optimize the policy. Exploring more advanced multitask RL algorithms and automatic hyperparameter tuning techniques to optimize the policy more efficiently is a promising direction for future work.\n\n* **Is equation 4 wrongly formatted.**\n\nThank you very much for your thorough review. In the revised paper, we have made corrections to Equation 4, ensuring that the parentheses and symbols are correctly formatted. We have also diligently reviewed and adjusted the remaining formulas to ensure consistency.\n\n* **Add the default DCFR parameters as a constant line in the plots in Figure 4.**\n\nThank you for your suggestion to enhance Figure 4. We have updated the figure in the revised paper to include a constant line representing the default DCFR parameters, which helps readers more easily interpret the results and compare DDCFR's dynamic discounting scheme with DCFR's fixed scheme.\n\n* **What happens in the first 500 iterations of DDCFR? Why are those hidden?**\n\nDue to the rapid decline in exploitability in the early iterations of different CFR algorithms, their differences are primarily manifested in the later iterations. Consequently, for a better comparison of the algorithms, we focus on the later iterations.\n\nFor the sake of completeness, in Appendix E of the revised paper, we have added figures to include all iterations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149906104,
                "cdate": 1700149906104,
                "tmdate": 1700149906104,
                "mdate": 1700149906104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3ABQIjEuTo",
                "forum": "6PbvbLyqT6",
                "replyto": "SIAhn2Nd2y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4228/Reviewer_f8hB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4228/Reviewer_f8hB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks or your answers, they satisfy my questions"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469977235,
                "cdate": 1700469977235,
                "tmdate": 1700469977235,
                "mdate": 1700469977235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]