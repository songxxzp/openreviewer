[
    {
        "title": "Universal Humanoid Motion Representations for Physics-Based Control"
    },
    {
        "review": {
            "id": "FgMcXcm9Ib",
            "forum": "OrOd8PxOO2",
            "replyto": "OrOd8PxOO2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission461/Reviewer_arMx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission461/Reviewer_arMx"
            ],
            "content": {
                "summary": {
                    "value": "This work provides a method to learn a universal humanoid motion prior for different downstream tasks, the prior is designed to cover a wide range of motions. The learned latent space can be used for long, stable and diverse motion generation, and also for solving tasks with natural motions. The latent is learnt by first training an imitator controller to imitate a wide range of human motions, and then learn the latent space by distilling the learned motion imitator. Results show the learned controller could generate a wide range of human motion, and outperform baselines on downstream tasks by a large margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The work is well-motivated, addressing the problem that motion imitator can hardly be used for downstream tasks (especially when interaction with the environment is required) and learned motion prior can not cover a wide range of human motions.\n2. Method is well demonstrated with details, and most designs are well-explained. Clear ablation study also shows the effectiveness of different components. Additional implementation details and hyper-parameters are provided in the supplementary materials for the community to reproduce the results. \n3. The learned humanoid motion prior showed impressive motion imitating performance over a wide range of human motion, and also showed promising results when applying to diverse downstream tasks, including motion tracking and locomotion over complex terrains, et al.\n4. The supplement videos make the difference between proposed method and baseline more comprehensive."
                },
                "weaknesses": {
                    "value": "1. One of this work\u2019s claims is that previous work has limited coverage of the learned latent space that can not cover the wide spectrum of possible human motion. But in Table 1, there is no comparison with ASE or CALM or Imitate&Repurpose on motion imitation performance. Though ASE, or CALM might be a bit hard to compare, Imitate&Repurpose should be reasonable to compare with. I\u2019m not expecting it to perform better, just for completeness.\n2. Though PHC+ exhibit great performance on motion imitation, it\u2019s comparison with PHC might be a bit unfair, since one of the modification fo PHC+ is \u201cremoving\u201d some hard-negative in the dataset, provide motion imitation result on modified dataset might be beneficial. \n3. Some content is a bit hard to read: In Figure 3, it\u2019s quite hard to see the human motion in the Figure 3(e) row, and the title for each row is really hard to see. (Minor issue)"
                },
                "questions": {
                    "value": "1. Is the proposed method robust against methodology and dynamics changes? It would be interesting to see these results and potentially enable appling the proposed method to humanoid robots."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission461/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698856631884,
            "cdate": 1698856631884,
            "tmdate": 1699635972397,
            "mdate": 1699635972397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hG4s52IUPU",
                "forum": "OrOd8PxOO2",
                "replyto": "FgMcXcm9Ib",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Review arMx"
                    },
                    "comment": {
                        "value": "We thank the reviewer for helpful, constructive, and positive feedback. We are glad that you find our work \"well-motivated\", \"well-demonstrated\", and \"outperform baselines on downstream tasks by a large margin\". Here, we try to address the questions and concerns: \n\n\n**Comparison with Baselines on Imitation**\n\nAs ASE and CALM do not use a motion-tracking objective for training, we cannot compare motion imitation results directly. We show that they do not have good motion skill coverage in the VR controller tracking task, which is a generalization and arguably harder task than full-body motion imitation. The VR controller tracking task only has partial body observations, but requires the policies to produce the same full-body motion as the full-body imitation task. Between PULSE and imitate \\& repurpose, the main difference is how the latent space is trained, where PULSE is through distillation and imitate \\& repurpose is through RL. Imitate \\& repurpose also does not use a Gaussian prior or learnable prior, though both PULSE and Imitate \\& repurpose use the AR1 prior. We report the result of training using RL and no distillation in Table 6 of Appendix C.2. We can see that training without distillation can learn a large amount of motion in AMASS (76\\%), but it cannot scale to the performance of PULSE. We hypothesize that random sampling for variational bottleneck and random sampling for RL hampers the learning process. This ablation can be viewed as imitate \\& repurpose implemented in our framework and humanoid, and we show that it does not scale as well in the imitation task. \n\n**Comparison between PHC and PHC+**\n\nBoth PHC and PHC+ are evaluated on the same set of modified training and testing sequences in Table 1.  After removing sequences with issues, there are 11313 and 138 AMASS motion sequences for training and testing, and we re-run all PHC's evaluations on the modified dataset. \n\n**Plot Content Visibility**\n\nWe have updated the plot for better visibility. Thanks!\n\n**Answer to Questions**\n\n- As sim2real from physics simulation [1, 2, 3, 4] continues work, we believe that a similar methodology should transfer to real humanoids. Although modifications to the humanoid kinematic tree, reward designs, and domain randomization will be required, the methodology should be applicable. PHC has also demonstrated motion imitation on diverse body shapes, which could also be distilled to a shape/dynamics-aware latent space. \n\n\n\n[1] Cheng, Xuxin, Kexin Shi, Ananye Agarwal, and Deepak Pathak. 2023. \u201cExtreme Parkour with Legged Robots.\u201d arXiv [Cs.RO]. arXiv. http://arxiv.org/abs/2309.14341.\n\n[2] Zhuang, Ziwen, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, and Hang Zhao. 2023. \u201cRobot Parkour Learning.\u201d arXiv [Cs.RO]. arXiv. http://arxiv.org/abs/2309.05665.\n\n[3] Fu, Zipeng, Ashish Kumar, Ananye Agarwal, Haozhi Qi, Jitendra Malik, and Deepak Pathak. 2021. \u201cCoupling Vision and Proprioception for Navigation of Legged Robots.\u201d http://arxiv.org/abs/2112.02094.\n\n[4] Radosavovic, Ilija, Tete Xiao, Bike Zhang, Trevor Darrell, Jitendra Malik, and Koushil Sreenath. 2023. \u201cLearning Humanoid Locomotion with Transformers.\u201d arXiv [Cs.RO], March. https://doi.org/10.48550/ARXIV.2303.03381."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465878346,
                "cdate": 1700465878346,
                "tmdate": 1700465878346,
                "mdate": 1700465878346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yG2gNenoij",
                "forum": "OrOd8PxOO2",
                "replyto": "hG4s52IUPU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Reviewer_arMx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Reviewer_arMx"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you authors for addressing the questions.\n\nI'll keep my original evaluation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544041185,
                "cdate": 1700544041185,
                "tmdate": 1700544041185,
                "mdate": 1700544041185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1lN3VwPPfX",
                "forum": "OrOd8PxOO2",
                "replyto": "FgMcXcm9Ib",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your suggestions!"
                    },
                    "comment": {
                        "value": "Thanks again for your questions and discussion. It will help us improve the paper and mention the pure RL-based experiments in the main paper. \n\nWe appreciate the discussion and constructive comments. \n\nBest, \n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617905083,
                "cdate": 1700617905083,
                "tmdate": 1700617938713,
                "mdate": 1700617938713,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ew2rNZc4UN",
            "forum": "OrOd8PxOO2",
            "replyto": "OrOd8PxOO2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission461/Reviewer_UBzx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission461/Reviewer_UBzx"
            ],
            "content": {
                "summary": {
                    "value": "In the context of human motion representation, the paper proposes a method to create a fundamental representation of humanoid motion that can be used for humanoid control, human motion generation or motion tracking. This representation is created via two main elements: an imitation method and a physics-based learned prior.\nThe distillation is made through a VAE-like architecture that learns the prior R and decoder D that are then used for each downstream task to generate to generate the action."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written\n- The presented goal of learning a universal human motion latent representation is very interesting, and the effectiveness on relevant downstream tasks is well presented\n- The curation of the mocap training dataset for PHC (along with other modifications) increases robustness and allows fail states recovery\n- The downstream tasks are relevant and show interesting use cases for the learned representation\n- The ablation study is quite compelling"
                },
                "weaknesses": {
                    "value": "- Quantitative results on the VR-controller tracking task are a little bit disappointing compared to Scratch\n- The references section could be cleaned up and harmonised, notably for publication conferences\n- Although the writing is clear, some typos remain (e.g. Guassian)"
                },
                "questions": {
                    "value": "- In section 4.3, a failure case is described using the prior R but it is unclear to me how do we recover from it apart from restarting the task's policy learning? Does it happen often?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission461/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698900329527,
            "cdate": 1698900329527,
            "tmdate": 1699635972328,
            "mdate": 1699635972328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v6tVmzInmZ",
                "forum": "OrOd8PxOO2",
                "replyto": "Ew2rNZc4UN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Review UBzx"
                    },
                    "comment": {
                        "value": "The authors thank the reviewer for constructive and encouraging feedback. We are glad that you find our method a \"fundamental representation of humanoid motion\", our method's effectiveness \"well presented\", and ablation \"compelling\". To address questions and concerns: \n\n**VR Motion Tracking Results**\n\nWe acknowledge that the quantitative result on VR motion tracking is less performant on joint position errors though better on success rate. Similar performance degradation on joint errors can also be observed in Table 1, where after distillation with the variational latent space, the imitation performance is worse than the teacher's. This is a known trade-off between prior regularization and achieving good reconstruction performance in VAEs, where the reconstruction error usually does not reach 0 during training. As shown in Appendix C.3, if a non-variational latent space is used (e.g. spherical), a better distillation performance can be achieved (100\\% success rate and 28.1 $E\\_\\text{mpjpe}$). However, such a latent space does not provide a variational prior, and random samples from it do not generate coherent motion. Thus, while our latent space has good coverage of the motor skills required to perform AMASS, it is not full coverage. Downstream task such as VR motion tracking is searching in a constrained solution space and can lose performance on precise motion tracking (though gains faster convergence). Future investigation is needed to achieve better motion imitation results while retaining the generative capabilities of the latent space. \n\n\n**Failure Case not Using Prior R**\n\nWe believe that this refers to the cases \"one can also supervise $\\pi\\_{\\text{task}}$'s distribution using prior $\\mathcal{R}$\", which leads to an \"adverse feedback loop\". We apologize for the confusing statement. Here, we are referring to an alternative to using the residual action formulation. In Sec. 4.3,  we propose to let downstream policy produce a residual with respect to the learned prior $\\boldsymbol{a}\\_t = \\mathcal{D}(\\pi\\_{\\text{task}}(\\boldsymbol{z}\\_t | \\boldsymbol{s}^{p}\\_t, \\boldsymbol{s}^{g}\\_t) + \\boldsymbol{\\mu}^{p}\\_t)$. The alternative is to directly output the latent code rather than the residual, and supervise the output distribution with the prior $\\mathcal{R}$. We **do not use** this alternative due to the described issue. The paragraph is included as a justification for the residual action formulation. This failure case only applies to the alternative approach and does not apply to the residual action one. \n\n\nThe typos and the reference section are cleaned up in the revision. Thanks!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465553311,
                "cdate": 1700465553311,
                "tmdate": 1700465553311,
                "mdate": 1700465553311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9uekF0QYjE",
                "forum": "OrOd8PxOO2",
                "replyto": "v6tVmzInmZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Reviewer_UBzx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Reviewer_UBzx"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response that cleared up the misunderstandings that I had reading the paper. My grade stays the same as 8-Accept."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651843924,
                "cdate": 1700651843924,
                "tmdate": 1700651843924,
                "mdate": 1700651843924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HLH7V96PMP",
            "forum": "OrOd8PxOO2",
            "replyto": "OrOd8PxOO2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission461/Reviewer_a8Fw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission461/Reviewer_a8Fw"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a system to train human motion controllers that can imitate large motion datasets and be used efficiently for training tasks such as terrain navigation and vr motion following.\n\nKey contributions:\n\n1. Modification of PHC to learn a better motion controller.\n\n2. A VAE like distillation process to obtain a controller and an action space that can be used efficiently for downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A controller that can imitate the whole AMASS dataset.\n\n2. A wide range of tasks to verify the efficiency of the system."
                },
                "weaknesses": {
                    "value": "1. No comparison with baselines that resemble the proposed method, e.g., Physics-based character controllers using\nconditional VAEs, by Won et al or ControlVAE: Model-based learning of generative controllers for physics-based characters. by Yao et al, which also uses VAEs.\n\n2. The main difference between the proposed system and other systems is that the proposed system is able to scale to the whole AMASS dataset while other systems are mainly doing locomotion or something similar. However, this is not well demonstrated in the downstream tasks, which are mostly just comprised of locomotion tasks in addition to some simple reaching, which the other systems can already do pretty well (maybe less efficiently?)."
                },
                "questions": {
                    "value": "1. It will be nice to showcase some scenarios where the benefit of learning the whole AMASS dataset is useful.\n\n2.  What are the motions that PHC cannot handle?\n\n3. I feel like the motion quality produced in the downstream tasks is suboptimal/unnatural. It will be nice to have a metric to measure the motion quality generated in the downstream task for potential future improvement for future work.\n\n4. For the speed task (and other tasks as well, but speed task is one of the results in the ASE paper, so I will focus on this), looks like the motion quality of ASE is really bad, while the original ASE paper has pretty good motion quality (at least visually), any comment on the discrepancy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission461/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission461/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission461/Reviewer_a8Fw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission461/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698982843543,
            "cdate": 1698982843543,
            "tmdate": 1700601731481,
            "mdate": 1700601731481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T7bZbprM6E",
                "forum": "OrOd8PxOO2",
                "replyto": "HLH7V96PMP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Review a8Fw 1/2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback and constructive comments. To address the questions and concerns:\n\n**Comparison PhysicsVAE and ControlVAE**\n\nWe acknowledge that PhysicsVAE [1] and ControlVAE [2] and ours all strive to learn a compact latent space using a variational bottleneck. While our structure resembles a VAE, we do not use auto-encoding/reconstruction as a learning objective. Our decoder outputs actions $\\boldsymbol{a}\\_t$ instead of the input state $\\boldsymbol{s}\\_t$. As a result, we do not require learning any world model during training. This design allows us to directly distill motor skills from a pretrained imitator that has achieved strong performance on a large-scale (40 hours) motion dataset. \n\nPhysicsVAE[1] has tested on the CMU dataset (a subset of AMASS) to further understand its scalability. They \"observed that the model learned from the CMU dataset does not perform well when compared to other models learned from either the locomotion dataset or the dancing dataset\" (Sec.6, third paragraph, page 11 in PhysicsVAE). The learned latent space from PhysicsVAE also requires an additional adaption layer to solve some of the harder tasks such as terrain traversal, which PULSE does not require. \n\nSimilarly, ControlVAE [2] stated that \"experiment on a large-scale dataset composed of 3.3 hours of motions\" \"converges to a lower reward\", and the \"learned skill embedding can still recover input motion with a larger visual discrepancy, but the performance on the downstream task are significance degrenrated\" (Sec.5.7, last paragraph, page 14 in PhysicsVAE). \n\nCompared to PhysicsVAE and ControlVAE, we learn from a much larger dataset and demonstrate that our latent space can (1) scale to the motion dataset after distillation, (2) perform well for downstream tasks without any additional adaptations. PhysicsVAE and ControlVAE also use different humanoids and simulation environments (Bullet and ODE) from ours (SMPL humanoid and Isaac Gym), and the gap between environment and humanoid choices hinders the reimplementation and fair benchmarking.\n\n**Benefit of scaling to the AMASS dataset**\n\nOur main motivation for scaling to AMASS is to have a latent space that covers a wide range of human motion. Thus, we include the VR controller tracking task, which requires tracking three 6DOF points in the global space. This is a generalized version of the full-body motion tracking task, where only partial observation is given to the humanoid to create full-body motion. This task requires the latent space to contain motor skills to match **arbitrary** user input in a continuous and streaming fashion. A latent space that has only seen small-scale locomotion datasets would not have the motor skills required to perform these free-form motions. Our supplement video (Sec. Motion Tracking Downstream Task) shows that the controller that uses our latent space can handle motions such as squatting, punching, dancing, running, rolling, etc. We also quantitatively demonstrate the tracking performance on the AMASS dataset in Table 2. Notice that the joint errors reported in Table 2 are **full-body** joint errors, and a low value means that the full-body motion generated from three-point input is close to the ground truth. Table 2 and Fig. 5 show that using our latent space, one can efficiently solve this task and achieve a high success rate on training, testing, and real-world data capture. Previously proposed latent spaces (ASE and CALM), when also trained with the AMASS dataset, do not obtain the coverage demonstrated in our latent space. We also note that few prior latent space methods have attempted a motion-tracking task due to the diversity and coverage required for free-form motion tracking. \n\nOn generative tasks, we test simple ones ranging from reach, speed, and strike, but also challenging ones like complex terrain traversal. The terrain traversal task requires the humanoid to handle stairs, slopes, rough surfaces, and obstacles. Prior art  [3]  on this task has relied on GAIL to achieve natural motion. Using our latent space, we no longer require GAIL but can still learn human-like behavior. Note that previous latent space approaches either fail on a stairs-only environment [4] or require an adaptation layer to traverse rough terrains/slopes [1], while our latent space can be used to solve the task without modification, thanks to the wider range of motor skills it can draw upon."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464237902,
                "cdate": 1700464237902,
                "tmdate": 1700464237902,
                "mdate": 1700464237902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fqLDdWScd2",
                "forum": "OrOd8PxOO2",
                "replyto": "HLH7V96PMP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Review a8Fw 2/2"
                    },
                    "comment": {
                        "value": "**Answers to Questions**\n\n- We design the VR motion tracking task to show that our learned latent space can cover a large amount of motor skills. A controller using our latent space is able to achieve good full-body motion tracking performance on the AMASS train and test and also transfers well to real-world data captures using commercial VR headsets. If the latent space has only been trained on a handful of locomotion sequences, it will only have the skills to perform simple actions. Our result on complex terrain traversal also shows that scaling matters, where the humanoid will employ skills like jumping when traveling fast on stairs. \n\n- We will include failure cases for PHC+ from the AMASS test set and other data sources. In general, while PHC can imitate a large number of motion sequences, unseen highly dynamic ones (e.g. badminton jump smash) or noise motion estimated from videos can still cause the humanoid to fall. \n\n\n\n- We agree that motion naturalness can be hard to evaluate and subjective, and we included a large number of video results for qualitative evaluation. The motion quality of a generative method is hard to quantify since there is no ground truth to compare against. Thus, we follow the prior art in the field and provide reward comparisons and video demonstrations. For the VR tracking task, we include pose estimation metrics, which are quantifiable ways to judge the quality of estimated motion. For unnaturalness in motion, we acknowledge that PULSE does not strictly constrain itself to certain types of motion when solving tasks. Since we train on a large-scale motion dataset, certain styles of motion that may be deemed unnatural for the speed task are also available in the latent space. For instance,  AMASS contains motions for \"running fanatically\" or \"limping\", whose motor skills are also learned in the latent space and can be used to solve the speed task. While training from scratch will often result in inhuman behavior, PULSE solves tasks using human-like behavior even when there are no constraints on the strategies used. To further constrain the solution space to natural behavior defined for certain tasks, one can enforce symmetry or incorporate additional regularizations. \n\n\n- The discrepancy on baselines (ASE and CALM) mostly comes from the training data scale. For a fair comparison, all baselines are trained on the AMASS training set, the same as PULSE. The original ASE is trained on a specialized dataset that only contains locomotion or character movement (eg. strike). By construction, a latent space trained on specialized datasets will only include motor skills that are deemed natural for the downstream task. However, datasets like AMASS contain diverse full-body motion that are not specialized. Thus, motor skills for sequences such as \"crouch walking\" or \"running fanatically\" could also be learned in the latent space of ASE. When using these motor skills for downstream tasks, the resultant motion can appear unnatural, though still human-like (in the sense that a human can recreate these motions if desired).\n\n\n[1] Won, Jungdam et al. \u201cPhysics-based character controllers using conditional VAEs.\u201d ACM Transactions on Graphics (TOG) 41 (2022): 1 - 12.\n\n[2] Yao, Heyuan, Zhenhua Song, Baoquan Chen, and Libin Liu. 2022. \u201cControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters.\u201d arXiv [Cs.GR]. arXiv. http://arxiv.org/abs/2210.06063.\n\n[3] Rempe, Davis, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, and Or Litany. 2023. \u201cTrace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion.\u201d arXiv [Cs.CV]. arXiv. http://arxiv.org/abs/2304.01893.\n\n[4] Hasenclever, Leonard, Fabio Pardo, Raia Hadsell, Nicolas Heess, and Josh Merel. 13--18 Jul 2020. \u201cCoMIc: Complementary Task Learning \\& Mimicry for Reusable Skills.\u201d In Proceedings of the 37th International Conference on Machine Learning, edited by Hal Daum\u00e9 Iii and Aarti Singh, 119:4105\u201315. Proceedings of Machine Learning Research. PMLR."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464871848,
                "cdate": 1700464871848,
                "tmdate": 1700465193342,
                "mdate": 1700465193342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eZvsXz6cId",
                "forum": "OrOd8PxOO2",
                "replyto": "HLH7V96PMP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Experiment for showcasing the benefit of scaling to AMASS"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer again for raising the concern over the benefit of scaling to AMASS. Here, we would like to add some quantitative evidence. \n\nWe study the effect of training our framework on a locomotion-only dataset (a subset of AMASS, used in PACER [3]) that contains 275 motion sequences of a total length of 38 minutes. The dataset contains mainly walking, running, and jumping. After distilling, the encoder-decoder imitator can reach an imitation result of: \n\n  | AMASS-Locomotion |                                  |                                  |                                  |                                  |\n| ----------- |----------- |----------- |----------- |----------- |\n  | $Succ \\uparrow$       | $E\\_{g-mpjpe} \\downarrow$        | $E\\_{mpjpe} \\downarrow$        | $E\\_{acc} \\downarrow$        | $E\\_{vel} \\downarrow$       |\n|       100%                           |  25.5                                |     20.1                             |                 2.9                 |          4.3  | \n\nwhich shows that the latent space has successfully learned the training sequences. Random sampling from the latent space also results in human-like walking and running motion. \n\nHowever, when applying this latent space  to the VR controller tracking task, we obtain the following result on the AMASS train and test, as well as the real-world data capture: \n\n | | AMASS-Train |                                  |                                  |                                  |                                  |  AMASS-Test |                                  |                                  |                                  |                                  |\n| ----------- | ----------- |----------- |----------- |----------- |----------- | ----------- |----------- |----------- |----------- |----------- |\n| | $Succ \\uparrow$   | $E\\_{g-mpjpe} \\downarrow$        | $E\\_{mpjpe} \\downarrow$        | $E\\_{acc} \\downarrow$        | $E\\_{vel} \\downarrow$       |  $Succ \\uparrow$       | $E\\_{g-mpjpe} \\downarrow$        | $E\\_{mpjpe} \\downarrow$        | $E\\_{acc} \\downarrow$        | $E\\_{vel} \\downarrow$     |\n| Locomotion-latent |  56.8 % |   111.6 | 75.4 | 4.5 | 8.8 |  15.2 % |   138.2 | 90.1 | **5.7** | **10.9** |  \n| Ours | **99.5 %** |   **57.8** | **51.0** | **3.9** | **7.1** |  **93.4 %** |   **88.6** | **67.7** | 9.1 | 14.9 |  \n\n\n | |Real-World |                                  |                                  |                                  |                                  |                         |\n| -----------| ----------- |----------- |----------- |----------- |----------- |----------- |\n|  | $Succ \\uparrow$       | $E\\_{g-mhpe} \\downarrow$         | $E\\_{acc} \\downarrow$        | $E\\_{vel} \\downarrow$       |\n| Locomotion-latent |5/14 | 117.5 | **5.5** | 9.5 |\n| Ours |**14/14** | **68.4** | 5.8 | **9.0** |\n\nBoth ours and the locomotion-latent model have the same training data and procedure, with the only difference being the training data when forming the latent space. From the result we can see that scaling to AMASS significantly increases the VR controller tracking task's performance on the real-world dataset, showcasing its usability in real-world scenarios. Scaling to a large dataset increases the diversity and coverage of the latent space and improves its applicability to downstream tasks. \n\nWe would love to further discuss and provide explanations if the reviewer has any additional concerns or questions. Thanks!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601585854,
                "cdate": 1700601585854,
                "tmdate": 1700601585854,
                "mdate": 1700601585854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r6XDlVx6Vb",
                "forum": "OrOd8PxOO2",
                "replyto": "eZvsXz6cId",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Reviewer_a8Fw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Reviewer_a8Fw"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "Thanks for clarifying my questions. I raise my score accordingly."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601776475,
                "cdate": 1700601776475,
                "tmdate": 1700601776475,
                "mdate": 1700601776475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dn04ct69VV",
                "forum": "OrOd8PxOO2",
                "replyto": "HLH7V96PMP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your constructive comments!"
                    },
                    "comment": {
                        "value": "Thanks again for your constructive comments and discussion, which will help improve our paper and our presentation on the importance of scaling up to a bigger motion dataset. \n\nWe appreciate the discussion and raising the score. \n\nBest, \n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617795020,
                "cdate": 1700617795020,
                "tmdate": 1700617929886,
                "mdate": 1700617929886,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]