[
    {
        "title": "Denoising Task Routing for Diffusion Models"
    },
    {
        "review": {
            "id": "LpzqvR1hSR",
            "forum": "MY0qlcFcUg",
            "replyto": "MY0qlcFcUg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7379/Reviewer_wtsR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7379/Reviewer_wtsR"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, diffusion training is cast as multi-task learning, where each task corresponds to the denoising task at a specific timestep. The authors present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. Besides, the channel partitioning considers task affinity and task weights in diffusion models. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes a simple add-on strategy for existing diffusion model architectures, which is simple yet effective, without introducing additional parameters, and contributes to accelerating convergence during training.\n2. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method.\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Some advanced routing methods [1, 2] improve the random routing by considering the inter-task relationship. Hence, it is better to discuss and compare the proposed method with them.\n\n2. In Figure 9, the images generated by the baseline (the first row) look very strange and both R-TR and DTR methods alleviate it (the second and third rows). So why the random routing method can work well? In particular, in the fifth case/column, the image generated by R-TR looks better than the one generated by DTR. Why?\n\n[1] Pascal et al. Maximum Roaming Multi-Task Learning. AAAI 2021.\n\n[2] Ding et al. Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. CVPR 2023."
                },
                "questions": {
                    "value": "Please address my concerns in the \"Weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698628578245,
            "cdate": 1698628578245,
            "tmdate": 1699636882757,
            "mdate": 1699636882757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ttp8Bqw2j4",
                "forum": "MY0qlcFcUg",
                "replyto": "LpzqvR1hSR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to wtsR"
                    },
                    "comment": {
                        "value": "Dear Reviewer wtsR,\n\nWe appreciate the valuable feedback that you provide on our work. Your insights have been instrumental in refining our work and we are sincerely grateful for the opportunity to incorporate your constructive comments. In the following, we will thoroughly address each of the concerns raised and make the necessary revisions to improve the overall quality of our paper.\n\n---\n\n### **W1: Comparison with other routing methods**\nThank you for providing us with further methods.\n\n**Max-roaming [1]**: We sincerely appreciate your recommendation. Comparing our method to max-roaming can further support the effectiveness of our method. We applied max-roaming to DiT-B/2 and trained it on the FFHQ dataset. The below table shows the comparative results between DTR and max-roaming.\n\n| Method      | FID      |\n|-------------|----------|\n| DTR         | **7.32** |\n| Max-Roaming | 39.90    |\n\nConsequently, their approach shows suboptimal performance due to its failure to incorporate the diffusion prior to task weight and task affinity. We suggest that this phenomenon is due to the detrimental effects of introducing randomness into the prior. Indeed, as illustrated by Go et al. [3], the application of random loss weighting has been observed to result in lower performance compared to the vanilla scenario.\n\n**ETR-NLP [2]**: utilizes slightly different routing with channel masking in DTR and TR and max-roaming [1]. Instead of task-specific masks in channel mask routing, task-specific layers (convolution layer) are employed for calculating representations in task-specific channels. Indeed, these task-specific layers require additional parameters, and they grow as the number of tasks increases, resulting in a huge cost in diffusion where there are a lot of denoising tasks (e.g. $T$ =1000). Meanwhile, our DTR does not require additional parameters for task routing. Also, their official code has not been released yet. In this regard, we did not conduct a direct comparison to ETR-NLP.\nNevertheless, we tried to implement ETR-NLP in block-wise routing during the discussion phase, however, we failed to converge ETR-NLP despite trying various hyperparameters for training. We suspect that unaddressed elements in the scope of their paper might play a role in this outcome, and we will conduct experiments upon their code release.\n\n---\n### **W2: Quality of generated images**\nIn some cases, DTR seems to perform worse than R-TR, while R-TR seems to outperform the baseline. However, the general trend observed is DTR >= Baseline >= R-TR. It is important to note that all qualitative results presented in the paper are uncurated samples, randomly selected without cherry-picking, which means that they may include instances where our approach encountered challenges or failures.\n\n---\n\n### **References**\n\n[1] Pascal et al. Maximum Roaming Multi-Task Learning. AAAI 2021.\n\n[2] Ding et al. Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. CVPR 2023.\n\n[3] Go et al. Addressing Negative Transfer in Diffusion Models. Neurips 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290200286,
                "cdate": 1700290200286,
                "tmdate": 1700290200286,
                "mdate": 1700290200286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FvdV9DMxx0",
                "forum": "MY0qlcFcUg",
                "replyto": "ttp8Bqw2j4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_wtsR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_wtsR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your response! It well addressed my concerns. Thus I will keep my initial rating towards acceptance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624405404,
                "cdate": 1700624405404,
                "tmdate": 1700624405404,
                "mdate": 1700624405404,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jyJlvZpKo8",
            "forum": "MY0qlcFcUg",
            "replyto": "MY0qlcFcUg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7379/Reviewer_VZ4w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7379/Reviewer_VZ4w"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. The authors incorporate two prior knowledge aspects of diffusion-denoising tasks\u2014task affinity and task weights\u2014into the model architecture design to mitigate the negative transfer phenomenon. The paper provides empirical results on several image generation tasks and a qualitative analysis to validate the effectiveness of DTR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper effectively addresses the negative transfer phenomena by establishing task-specific pathways for multiple denoising tasks. The concept of integrating key prior knowledge in diffusion and task routing is well-presented and could potentially influence future work on architecture design in diffusion models.\n* The implementation, although simple, is effective and yields significant performance gains on multiple benchmarks.\n* The paper is structured well, making it easy to understand and follow."
                },
                "weaknesses": {
                    "value": "* The empirical analysis could be more comprehensive in decoupling the contributions of task weights and task affinity. As I understand, the results in Figure 4 only ablate the significance of the synergy of the two priors. To study the direct contribution of **Task Weights**, it would be helpful to compare `DTR with random routing but task-dedicated allocation channels` with `Random Task Routing (R-TR)`. Similarly, to study the contribution of **Task Affinity**, a comparison between `DTR with task-unified allocation channels but sliding window channels` and `R-TR` would be useful.\n* The paper does not adequately explain the sensitivity of DTR to different masking strategies. The authors should elaborate on why they chose Equation (4) as the masking strategy and discuss potential alternatives."
                },
                "questions": {
                    "value": "* Would DTR achieve better performance by reducing the overlap channels at higher timesteps? Given the authors' assertion in Figure 6 that \"at higher timesteps, the model primarily focuses on learning discriminative features that are relevant to specific timesteps, whereas at lower timesteps, the model tends to exhibit similar behavior across different timesteps,\" it appears that denoising tasks at higher timesteps have less correlation. Would assigning these tasks entirely distinct channels be beneficial?\n* Is there a typographical error in Equation (4)? Should the first $t$ be replaced with $t-1$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7379/Reviewer_VZ4w"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822517465,
            "cdate": 1698822517465,
            "tmdate": 1700728994267,
            "mdate": 1700728994267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bWQmCsIbyP",
                "forum": "MY0qlcFcUg",
                "replyto": "jyJlvZpKo8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VZ4w (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer VZ4w,\n\nThank you for your insightful feedback on our paper. We are grateful for the opportunity to enhance our work through your constructive comments. Below, we will address all raised concerns by the reviewer and revise the paper accordingly.\n\n---\n\n### **W1: Studies for validating 1) task affinity and 2) task weights.**\n\nWe agree that the contribution of both prior 1) task affinity and 2)  task weights should be analyzed in our work. In this regard, we would like to respectfully emphasize that the contributions of the performance for both priors are already validated in Figure 4. \n\n1. *Task Affinity*: the random routing method (R-TR) routes the tasks without specific consideration of task affinity. As detailed in Section 4.2, the expected number of shared channels remains constant for any pairs of distinct denoising tasks. Contrarily, our Denoising Task Routing (DTR) utilizes a sliding window strategy, shifting activated channels by the timestep from 1 to $T$. This approach ensures that tasks at neighboring timesteps share similar channel sets, thereby embedding task affinity into the routing process. For instance, when $\\alpha=1$ in Eq. (4), DTR shifts channels linearly, focusing solely on task affinity without considering task weights, as seen in Figure 2. The performance superiority of DTR ($\\alpha=1$) over R-TR, as shown in Figure 4, serves as an empirical validation of the benefits of incorporating task affinity in DTR.\n2. *Task Weights*: By adjusting $\\alpha$, DTR also accounts for task weights alongside task affinity. When $\\alpha > 1$, a higher number of dedicated channels are allocated to later timesteps, in line with our intention to assign greater weight to these timesteps. We compared the performance of DTR with $\\alpha=1$ and DTR with $\\alpha>1$, finding that the latter generally surpasses the former, as evidenced in Figure 4 and Table 4. This comparison substantiates the impact of considering task weights in our model.\n\nIn summary, our results validate the efficacy of DTR in the context of both task affinity alone and in combination with task weights. We regret to inform you that we are currently unable to fully comprehend your queries regarding 'DTR with random routing but task-dedicated allocation channels with Random Task Routing (R-TR)' and 'DTR with task-unified allocation channels but sliding window channels and R-TR'. We kindly request further clarification on these points to provide a more thorough response. We eagerly anticipate a productive and enlightening discussion on these matters.\n\n---\n\n### **W2-1: Rationale for choosing Equation (4).**\nThank you for pointing this out. As mentioned above in W1, our DTR firstly incorporates task affinity with activated channels as a sliding window according to timesteps, then modulates the shifting ratio of the sliding window across timesteps for consideration of task weights. In this regard, we implemented the idea as Eq. (4), However, we acknowledge that the conceptual explanation of Eq. (4) had been lacking in the manuscript. Through Eq. (4), each mask has $C_{\\beta}$ sequential activated channels, while it is shifted as like a sliding window. In this situation, the available start index of activated channels is in {$0, \\dots ,C \u2212 C\u03b2$}, and we quantized this start index according to timestep as $(\\frac{t}{T})^{\\alpha}$, enabling modulation of shifting ratio of the sliding window.\n\n### **W2-2: Sensitivity of DTR masking strategies.**\nWe would like to respectfully emphasize that we have validated the sensitivity of DTR masking strategies with regard to the hyperparameters $\\alpha$ and $\\beta$ in Figure 4 and Table 4. \n\n### **W2-3: Discussion on potential alternatives.**\nThank you for pointing this out. We agree that discussion on potential alternatives for masking strategies can promote future work by starting from our work. In our work, task-specific masks are not changed and optimized through training procedures. By utilizing well-known methods such as reinforcement learning and evolutionary algorithms, the masks can be more optimized than our DTR masks. Thanks again for pointing this out, we will add this discussion to our manuscripts.\n\n--- \n\n### **Q1-1: Would DTR achieve better performance by reducing the overlap channels at higher timesteps?**\nWe validated that reducing overlapped channels at higher timesteps improves the performance in Figure 4 and Table 4. By increasing $\\alpha$ until adequate level, overlapped channels at higher timestep reduce. As shown in the results, the FID score is improved, achieving better performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288339451,
                "cdate": 1700288339451,
                "tmdate": 1700288339451,
                "mdate": 1700288339451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OLUYWbaCt6",
                "forum": "MY0qlcFcUg",
                "replyto": "gApeTLoysJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_VZ4w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_VZ4w"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "The response for Week 1 thoroughly addresses my concerns, and I appreciate the detailed answers to the other questions, which have enhanced my understanding of the paper. Consequently, I will increase my rating to 8."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729055549,
                "cdate": 1700729055549,
                "tmdate": 1700729055549,
                "mdate": 1700729055549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "67iJkjCxQQ",
            "forum": "MY0qlcFcUg",
            "replyto": "MY0qlcFcUg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Denoising Task Routing (DTR), an add-on strategy for diffusion models that incorporates multi-task learning (MTL). The proposed channel masking strategy effectively boosts performance without introducing any extra parameters. The experiments demonstrate consistent improvement across evaluation protocols."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed routing mask strategy is interesting as it leverages the task similarity between adjacent timesteps. \n\n2. The experiment conducted in this study is comprehensive and demonstrates significant performance improvement."
                },
                "weaknesses": {
                    "value": "1. The idea of considering diffusion models as multi-task learning has previously been proposed by Hang et al. (2023) and Go et al. (2023a). The proposed masking strategy in this work is a simple modification of TR (Strezoski et al., 2019). Its novelty is limited.\n\n2. It lacks an ablation study to evaluate the necessity of the proposed masking strategy. Ding et al. (2023) propose to divide channels into shared channels and task-specific channels. Assigning each time-step cluster (Go et al., 2023a) to the respective task-specific channels can serve as an important baseline."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW",
                        "ICLR.cc/2024/Conference/Submission7379/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698923124240,
            "cdate": 1698923124240,
            "tmdate": 1700723367840,
            "mdate": 1700723367840,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eAAUBByNrP",
                "forum": "MY0qlcFcUg",
                "replyto": "67iJkjCxQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8VoW"
                    },
                    "comment": {
                        "value": "Dear Reviewer 8VoW,\n\nWe are grateful for the insightful feedback you have provided on our work. Your perspectives have played a crucial role in refining our research and we sincerely appreciate the opportunity to incorporate your constructive comments. In the following, we address each of the concerns raised in detail and make the necessary revisions to improve the overall quality of our work.\n\n---\n\n### **W1: Limited novelty**\nThank you for raising this important point, and we sincerely apologize for any confusion caused.\nWhile it is acknowledged that the diffusion model has been established in prior research as a multi-task learning framework for denoising tasks, it is noteworthy that the emphasis in these studies was primarily on optimization, with less consideration given to the architectural aspect. In the field of multi-task learning, it is widely accepted that both optimization and architecture play a pivotal role. We have shown that this architectural approach is the first point (in diffusion) and is also orthogonal to their methodology.\n\nIn addition, Task Routing (TR) differs from our approach in several key respects. First, we have successfully eliminated the need for additional parameters when applying Task Routing (TR) to existing architectures. Second, our research emphasizes the importance of considering inter-task relationships and warns against a naive application that could lead to performance degradation. This underscores a critical message: performance improvements in the diffusion model are achievable by accounting for prior knowledge of denoising tasks.\n\nIn light of these distinctions, we note that our contribution to task routing is significant and remains a valid addition when viewed through the lens of incorporating diffusion as a form of multi-task learning.\n\n---\n\n### **W2: Comparison with other potential baselines.**\n\n**Comparison with ETR-NLP (Ding et al. (2023) [2])**\n\nETR-NLP divides channels into shared and task-specific channels, however, it utilizes slightly different routing with channel masking in DTR and TR and max-roaming [1]. Instead of task-specific masks in channel mask routing, task-specific layers (convolution layer) are employed for calculating representations in task-specific channels. Indeed, these task-specific layers require additional parameters, and they grow as the number of tasks increases, resulting in a huge cost in diffusion where there are a lot of denoising tasks (e.g. $T$=1000). Meanwhile, our DTR does not require additional parameters for task routing. Also, their official code has not been released yet. In this regard, we did not conduct a direct comparison to ETR-NLP.\n\nNevertheless, we tried to implement ETR-NLP in block-wise routing during the discussion phase, however, we failed to converge ETR-NLP despite trying various hyperparameters for training. We suspect that unaddressed elements in the scope of their paper might play a role in this outcome, and we will conduct experiments upon their code release.\n\n**Comparison with the timestep cluster-based task-specific channels (Go et al, 2023a [3])**\n\nThank you for suggesting a valuable recommendation. Comparing our DTR to task-specific channels which are allocated according to timestep clusters can show differences between routing with entire tasks and clustered tasks. We utilized timestep-based clustering [3] with ($k=8$) and assigned channel masks by regarding each cluster as one task and trained DiT on the FFHQ dataset. The below table shows the results, and our DTR dramatically outperforms the timestep cluster-based DTR, showing that routing entire tasks is more beneficial than routing clustered tasks. Thank you for improving our work, and we will add this to the manuscript. \n\n\n| Method                     | FID      |\n|----------------------------|----------|\n| DTR                        | **7.32** |\n| DTR w timestep-cluster [3] | 9.61     |\n\n\n### **References**\n[1] Pascal et al. Maximum Roaming Multi-Task Learning. AAAI 2021.\n\n[2] Ding et al. Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. CVPR 2023.\n\n[3] Go et al. Addressing Negative Transfer in Diffusion Models. Neurips 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287904864,
                "cdate": 1700287904864,
                "tmdate": 1700287904864,
                "mdate": 1700287904864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZUjHprFFBL",
                "forum": "MY0qlcFcUg",
                "replyto": "eAAUBByNrP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your response. There seems to be some misunderstanding in W2. Actually, what I meant is to first cluster the tasks and then assign them to shared channels and task-specific channels as in [2]. For instance, in a scenario with 8 task clusters and 32 channels, we can use 16 channels as shared channels, assign 2 task-specific channels to each task cluster. I wonder if the experiment result (i.e., DTR w timestep-cluster [3] in the table) include shared channels."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468610542,
                "cdate": 1700468610542,
                "tmdate": 1700468610542,
                "mdate": 1700468610542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6qDwzg1Gdt",
                "forum": "MY0qlcFcUg",
                "replyto": "67iJkjCxQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the official comment by Reviewer 8VoW"
                    },
                    "comment": {
                        "value": "Thank you for your clarifications regarding the concerns raised in your review. We appreciate your continued engagement with our work, and we're glad to have the opportunity to address the points you've mentioned. Below are our responses to your comments:\n\n--- \n\n### **1. Misunderstanding in W2**.\nWe appreciate the opportunity to address a potential misunderstanding highlighted by the reviewer in reference [2] regarding our ETR-NLP module. Concerning point [2], we wish to clarify a potential misunderstanding by the reviewer. In their ETR-NLP module referenced in [2], channels are categorized into two types: task-specific and shared. However, contrary to what the reviewer suggests, their task routing is conducted via a task-specific module, not through channel masking. Channel indices of both shared and task-specific channels are commonly shared across all tasks, with the shared channel representations processed by shared modules, while task-specific channel representations go through task-specific modules (such as convolutional layers).\n\nFor instance, in a setup with 8 task clusters and 32 channels, 16 channels could be designated as shared, with the remaining 16 channels serving as task-specific across the 8 clusters. For each cluster, the shared layer processes the 16 shared channels, and concurrently, the corresponding task-specific layer handles the 16 task-specific channels. The outputs from these layers are subsequently concatenated. This example illustrates that the distinction between shared and task-specific channels is more nuanced than our task routing.\n\nTherefore, the example provided by the reviewer does not precisely align with the methodology used in [2]. However, to address your concern, we are currently conducting experiments on your suggested channel masking where half of the channels are designated as shared and the other half are equally divided among task-specific channels for each task. We will share the results once available.\n\nWe hope this clarification provides a clearer understanding of our approach and resolves any misconceptions that may have arisen.\n\n--- \n\n### **2. Whether shared channels are considered in experimental results in our response.**\n\nRegarding DTR w/ timestep-cluster [3] in our response, we initially grouped $T$ denoising tasks into 8 clusters. We then applied the standard DTR settings ($\\alpha=4, \\beta=0.8$) by treating each task group as a single task. Given that $\\beta$ is set to 0.8, which is above 0.5, and the masks are allocated as a sliding window, certain channels function as shared channels, being commonly activated across all tasks.\nWe note that $(2\\beta - 1) \\cdot C$ channels are shared across all tasks ($C$ indicates the total number of channels)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489336894,
                "cdate": 1700489336894,
                "tmdate": 1700491806260,
                "mdate": 1700491806260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ddwWSRSKet",
                "forum": "MY0qlcFcUg",
                "replyto": "67iJkjCxQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the official comment by Reviewer 8VoW - 2"
                    },
                    "comment": {
                        "value": "### **Experimental results regarding W2**\n\nWe sincerely appreciate your patience while waiting for our results. In response to the reviewer's concern in W2, we configured half of the channels as shared across all denoising tasks, while the remaining channels were divided into $k=8$ segments, each activated for specific tasks. We denote this as Explicit Routing with Clustered Denoising Tasks (ERCDT) and illustrate the comparative results in the below table. \n\n| Method  | FID      |\n|---------|----------|\n| Vanilla | 10.99  |\n| ERCDT   |     10.13     |\n| DTR     | **7.32** |\n\n\nAs shown in the results, our DTR dramatically outperforms ERCDT. This implies that such explicitly structured channel allocation may not be as effective as DTR. The primary reason for this disparity appears to be ERCDT's inability to sufficiently capture and reflect the nuanced, proximal relationships among denoising tasks due to its clustering approach. For instance, in ERCDT, the denoising task at $t=1$ is considered nearly equivalent to the task at $t=5$ or $t=100$ within the context of $k=8$ clusters, failing to recognize the higher affinity between tasks at closer time intervals. Furthermore, in the sequence of tasks, while $t=124$ and $t=125$ belong to the same cluster, timesteps $t=125$ and $t=126$ fall into different clusters. This quantized clustering doesn't effectively reflect one timestep difference between them. Consequently, this limitation hinders the performance of ERCDT compared to DTR, supporting the effectiveness of our DTR.\n\nHowever, ERCDT surpasses the performance of vanilla training. This result suggests that task routing incorporating the relationship of denoising tasks by task clustering boosts performance despite of its discreteness. \n\nWe are profoundly grateful for your insightful suggestion. It has contributed significantly to our analysis, and we will ensure to include this valuable perspective in our paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577806135,
                "cdate": 1700577806135,
                "tmdate": 1700578181920,
                "mdate": 1700578181920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QzT9WdtUsX",
                "forum": "MY0qlcFcUg",
                "replyto": "6qDwzg1Gdt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your response. It has addressed all of my concerns. I will raise my score to 8."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723345900,
                "cdate": 1700723345900,
                "tmdate": 1700723345900,
                "mdate": 1700723345900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c5folv6ykB",
                "forum": "MY0qlcFcUg",
                "replyto": "67iJkjCxQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for engaging discussion."
                    },
                    "comment": {
                        "value": "We sincerely appreciate the valuable and insightful comments provided by Reviewer 8VoW, which have been very helpful in improving our work. We are glad to hear that your concerns have been resolved. Thank you for your priceless effort to engage in discussion.\n\nSincerely,\n\nAuthors."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725415189,
                "cdate": 1700725415189,
                "tmdate": 1700725584822,
                "mdate": 1700725584822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]