[
    {
        "title": "D^3: Distributional Dataset Distillation with Latent Priors"
    },
    {
        "review": {
            "id": "tLYr0DPRWs",
            "forum": "mdqOjfQ29b",
            "replyto": "mdqOjfQ29b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4136/Reviewer_3cs3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4136/Reviewer_3cs3"
            ],
            "content": {
                "summary": {
                    "value": "Distributional Data Distillation (D3) is a groundbreaking approach that transforms dataset distillation into a distribution-based problem. Unlike traditional methods that create a finite set of real or synthetic examples, D3 generates a probability distribution and a decoder to approximate the original dataset. Using Deep Latent Variable Models (DLVMs), it combines a trajectory-matching distillation loss with a distributional discrepancy term, resulting in strong alignment between original and distilled data. Across various computer vision datasets, D3 demonstrates effective distillation with minimal performance loss, even excelling with large datasets like ImageNet, surpassing sample-based methods consistently."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. simple and intuitive idea\n2. presentation is smooth and easy to understand"
                },
                "weaknesses": {
                    "value": "1. I highly disagree with the statement claimed in this authors, \"Existing methods face challenges in scaling efficiently beyond toy datasets.\", \"More generally, these methods lack fine-grained control over distillation strength and often struggle to scale beyond smaller datasets like CIFAR-10 and MNIST, experiencing diminished performance when compressing larger or higher-dimensional datasets, such as ImageNet.\"[1, 2] cited in this paper, presented in CVPR 2023 has its main results in ImageNet on its cover.\n\n2. Lack of novelty and significant contributions, the idea is of sampling from a latent distribution has been thoroughly explored since VAE came out. It's unclear what is the significant contribution or additional innovation the authors are intending to propose.\n\n3. Experiment results in tables seem incomplete, it's hard to have holistic picture on how good this method is.\n\n\n[1] Cazenavette, George, et al. \"Generalizing Dataset Distillation via Deep Generative Prior.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[2] Cui, Justin, et al. \"Scaling up dataset distillation to imagenet-1k with constant memory.\" International Conference on Machine Learning. PMLR, 2023.\n[3] Wu, Xindi, Zhiwei Deng, and Olga Russakovsky. \"Multimodal Dataset Distillation for Image-Text Retrieval.\" arXiv preprint arXiv:2308.07545 (2023)."
                },
                "questions": {
                    "value": "1. Is it a typo where in the abstract, it claims to have done experiments on imageNet, but in the actual experiments, it runs ImageNette, which is a 10 class subset and a much easier problem to solve.\n2. Why does ConvNet have better accuracy than more complex and sophisticated networks?\n3. Why are Imagenette and imagewoof results not available for DM in table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697663987619,
            "cdate": 1697663987619,
            "tmdate": 1699636379081,
            "mdate": 1699636379081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BbhqNZRQAx",
                "forum": "mdqOjfQ29b",
                "replyto": "tLYr0DPRWs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 3cs3"
                    },
                    "comment": {
                        "value": "1. **On weakness 1 and question 1**: Besides TESLA [1], to our best knowledge, no other methods have reported results on ImageNet1k. Therefore, we made the claim that scaling to larger dataset poses a challenge to existing data distillation work. We want to thank author for pointing out the typo - As pointed out by the reviewer, we also only showed that our method works well on ImageNette and ImageWoof, which is a simpler problem on ImageNet1K. Compared to TinyImageNet and CIFAR-10,  ImageNette and ImageWoof contains images of higher resolution ($128 \\times 128$ compared to $32 \\times 32$ or $64 \\times 64$). We were able to show that for higher resolution images, our method outperforms baselines on those two subsets compared to other generative-based distillation methods at a much lower compression rate.  \n\n2. **On weakness 2** (our work compared to VAE and other generative models) In general, generative models are not designed to perform the exact data distillation tasks: see meta-response 3\n\n3. **On weakness 3**: As suggested by all three reviewers, we fully agree that a better comparison for us to make is directly against generative-based distillation methods instead of distillation models that directly output images. We included an updated Table 1 (see meta-response). We acknowledge that comparing our method against non-generative distillation methods can be unfair, and we thank reviewers for pointing that out! Our initial intention was to two-fold. Firstly, non-generative methods are well-known and some of them are still at SOTA. Secondly, we derived our training objective (MTT and DM) from those methods, and we wanted to show that by combing two objectives and distilling into a distribution, our method provides improvement either in distillation quality or/and compression rate. \n\n4. **Question 2**: ConvNet has better accuracy because we distilled the data based on ConvNet (for both expert trajectories in MTT and feature extraction in DM). Cross-architecture generalization is a problem faced by many distillation methods. Data distilled based on one architecture will have worse performance on unseen architectures. In our experiment section 4.2 we show that our methods provide a better cross-architecture generalization compared to existing methods. \n\n[1] TESLA: https://arxiv.org/pdf/2211.10586.pdf"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246086852,
                "cdate": 1700246086852,
                "tmdate": 1700246086852,
                "mdate": 1700246086852,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1rDemH8Kks",
                "forum": "mdqOjfQ29b",
                "replyto": "BbhqNZRQAx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4136/Reviewer_3cs3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4136/Reviewer_3cs3"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for your responses. However, I still find it unconvincing, one of the main claim of \"Generalizing Dataset Distillation via Deep Generative Prior\" is that they can handle images (> 128). Other weaknesses are unmitigable without significant changes to the draft and experiment setup."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673047051,
                "cdate": 1700673047051,
                "tmdate": 1700673047051,
                "mdate": 1700673047051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hnA2ouIMdO",
            "forum": "mdqOjfQ29b",
            "replyto": "mdqOjfQ29b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4136/Reviewer_yDY8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4136/Reviewer_yDY8"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Distributional Data Distillation (D3), a novel approach to dataset distillation. Unlike existing methods that condense datasets into smaller versions, D3 focuses on creating a conditional latent distribution $p(z)$ and a decoder $Q_\\mathcal{S}^\\theta (x|z)$. The paper utilizes the resulting data distribution $Q_\\mathcal{S}^\\theta (x) = \\int Q_\\mathcal{S}^\\theta (x|z) p(z) dz$, called Deep Latent Variable Models (DLVMs), and a new training objective, combining trajectory-matching distillation with a distributional discrepancy term like Maximum Mean Discrepancy (MMD). Experimental results across various computer vision datasets, including the challenging ImageNet, demonstrate that D3 effectively condenses datasets with minimal performance loss. Notably, it consistently outperforms traditional sample-based distillation methods, even for large high-resolution datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The proposed method is simple and the description is easy-to-follow.\n\n- This paper proposes improved MMD loss over previous work which only matches mean of feature vectors."
                },
                "weaknesses": {
                    "value": "- The idea of utilizing a generative prior has already been explored in several papers, including HaBa, LinBa, KFS, IT-GAN, and GLaD.\n\n- The comparison to the original literature on dataset distillation is entirely unfair. The proposed method outputs a distribution; therefore, it should be compared to deep generative models. Deep generative models can perform the exact same tasks as the proposed method."
                },
                "questions": {
                    "value": "- In the loss of $\\mathcal{L}_\\texttt{MTT}$, why do we need the KL penalty? How does this regularization effects the performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671473497,
            "cdate": 1698671473497,
            "tmdate": 1699636378986,
            "mdate": 1699636378986,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7idugWTqOD",
                "forum": "mdqOjfQ29b",
                "replyto": "hnA2ouIMdO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to yDY8"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the detailed and valuable comments. We address the specific questions below.\n1. **weakness 1** As suggested by all three reviewers, we fully agree that a better comparison for us to make is directly against generative-based distillation methods instead of distillation models that directly output images. We included an updated Table 1, see meta-response (2). We acknowledge that comparing our method against non-generative distillation methods can be unfair, and we thank reviewers for pointing that out! Our initial intention was to two-fold. Firstly, non-generative methods are well-known and some of them are still at SOTA. Secondly, we derived our training objective (MTT and DM) from those methods, and we wanted to show that by combing two objectives and distilling into a distillation, our method provides improvement either in distillation quality or/and compression rate. \n2. **weakness 2 other generative-based distillation method** How we differ from existing data distillation methods that also uses a generative model:\n    \n    a. Overall, our method is similar to LinBa, HaBa, KFS, IT-GAN, and GLaD in the use of latent code and generative prior. Despite similarities in the overall framework, our model differs from all existing work in explicitly using distributions to represent distilled data. We will provide further details on a per-model basis below. \n    \n    b. HaBa and LinBa: Both LinBa and HaBa distill data into a latent space and generate image samples using a decoder. Our method is similar to LinBa and HaBa in the use of a latent code and decoder structure. We differ from them in two aspects: First, their decoders are not GAN or VAE based. Second, HaBa and LinBa assumes a one-to-one relationship between each latent prior and the synthetic image, while we have a one-to-many relationship between latent prior and generated image. \n    \n    c. KFS: KFS also trains a generative model along with latent codes for data distillation. To generate synthetic images, they rely on using different decoders for a given latent code. Therefore, to represent a dataset, KFS requires multiple latent codes and multiple decoders. Similar to all existing work, the synthetic data generation is deterministic, instead of distributional. Our work presents a dataset such that with one decoder, each latent code represents a distribution, and one can draw samples of synthetic images from the distribution and only need one decoder to generate images. \n    \n    d. IT-GAN: IT-GAN uses pre-trained GAN as the decoder and only learned latent codes. Furthermore, IT-GAN learns one latent code for each image in the original dataset (whole-set learning). To achieve data distillation, IT-GAN simply subsamples a small portion of the IT-GAN generated data as the \u201cdistilled dataset\u201d.  IT-GAN already pointed out that existing GAN such as , is trained to generate images visually similar to the original data but might not be suitable for classification. In their method, they only learn the latent prior for GANs such that pre-trained GAN can be repurposed for data distillation. In our method, we further push the idea and directly train decoder and latent code from scratch. Moreover, we do not require to match a latent code to each training image. In fact, each of our latent code represents a distribution of possible synthetic images. Our decoder is trained to represent information useful for image classification. \n\n    e. GLaD: Similar to IT-GAN, GLaD also uses pre-trained GANs and only learns the latent prior.  GLaD showed that using pre-trained GAN in addition to the MTT method helps MTT perform better on unseen architectures but does not change the performance on seen architectures. We showed in our experiment that our method performs better on seen architectures at the same time generalize well to unseen architectures. \n\n 3. **weakness 2 generative models** In general, generative models are not designed to perform the exact data distillation tasks:  see meta-response (3)\n 4. **question on KL** We conducted an additional ablation study on KL-loss on  TinyImageNet(2 prototype/class at comp. rate 0.07%) and we noticed that by removing KL-Divergence, the overall performance remained roughly unchanged. We are testing out on more experiments and if the conclusion holds, we will remove it from the training objective since it could be redundant. \n |  | Test Acc. (%) |\n| --- | --- |\n| With KL |  24.6(0.2) |\n| Without KL | 25.3(0.4) |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245801868,
                "cdate": 1700245801868,
                "tmdate": 1700245801868,
                "mdate": 1700245801868,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Mjs8EIZqil",
            "forum": "mdqOjfQ29b",
            "replyto": "mdqOjfQ29b",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4136/Reviewer_TU1J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4136/Reviewer_TU1J"
            ],
            "content": {
                "summary": {
                    "value": "Dataset distillation is a technique used to condense large datasets into smaller synthetic versions while maintaining predictive performance. It has applications in various machine learning domains, but existing methods face challenges when scaling beyond small datasets and experience diminishing returns as the distilled dataset size increases. To address these limitations, a novel approach called Distributional Data Distillation (D3) is introduced.\n\nUnlike previous methods that distill datasets into finite sets of real or synthetic examples, D3 frames the data distillation problem as a distributional one. Instead of producing individual examples, D3 generates a probability distribution and a decoder that can approximately regenerate the original dataset. Deep Latent Variable Models (DLVMs) are used to parameterize the condensed data distribution.\n\nD3 introduces a new training objective that combines a trajectory-matching distillation loss with a distributional discrepancy term, such as Maximum Mean Discrepancy. This objective encourages alignment between the original dataset distribution and the distilled distribution.\n\nExperimental results on various computer vision datasets demonstrate that D3 effectively distills datasets with minimal performance degradation. Even for large high-resolution datasets like ImageNet, D3 consistently outperforms sample-based distillation methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This paper challenge the conventional approach of distilling into a finite set of samples, instead\ncasting the problem as a distributional one: finding a synthetic probability distribution which, when\nsampled to produce training data, yields performance comparable to training on the original dataset.\n\n2) To make this optimization problem tractable, this paper parametrize the distribution using Deep Latent\nVariable Models (Kingma & Welling, 2013), and design a loss function that combines a state-of-the\u0002art gradient-matching criterion (Cazenavette et al., 2023) with a distributional loss (e.g., MMD or\nWasserstein distance) \u2014 a natural choice for our distributional framework.\n\n3) This novel distributional dataset distillation perspective is appealing and it could addresses many of the limitations of prior distillation methods."
                },
                "weaknesses": {
                    "value": "1)  The design in LEARNING THE DISTILLED DISTRIBUTION Matching is simply borrowed from [1]. Please clarify the difference.\n\n2) The comparison in Table 1 is confusing.  Is Comp. rate good when this rate is high or low?\n\n3) More comparison with generative-based dataset distillation methods could be added.\n\n[1] Dataset distillation by matching training trajectories."
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4136/Reviewer_TU1J"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747277913,
            "cdate": 1698747277913,
            "tmdate": 1699636378875,
            "mdate": 1699636378875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VjNwLFZYOv",
                "forum": "mdqOjfQ29b",
                "replyto": "Mjs8EIZqil",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TU1J"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the detailed and valuable comments. We address the specific questions below.\n1. **MTT objective** Thanks for the clarification! In this paper, we directly borrowed the training objective from MTT [1]. While our main contribution focused on how the distilled data should be represented as a distribution, we also departed from the \u201csingle\u201d training objective used in existing models. Specifically, we combined MTT and DM objectives together and showed in the ablation study that by combining the two objectives, our method improved the quality of the distilled data compared to each objective used by itself. MTT tends to excel at distilling synthetic data that works well on seen-architecture (same architecture as the experts) but suffers from cross-architecture generalizability.  Adding DM objective improves cross-architecture generalizability of the distilled data without sacrificing seen architecture performance. \n2. **Compression rate** In the context of data distillation, the objective is to compress a dataset into a smaller one without sacrificing the performance of model trained on the dataset. Therefore, compression rate, defined in Section 3.4, is better when it is low. We will incorporate this clarification in the paper. \n3. **Table 1 Comparisons** We fully agree that a better comparison should be against generative-based data distillation methods. Please see meta-response (2) for details. However, we acknowledge that comparing our method against non-generative distillation methods can be unfair, and we thank reviewers for pointing that out! Our initial intention was to two-fold. Firstly, non-generative methods are well-known and some of them are still at SOTA. Secondly, we derived our training objective (MTT and DM) from those methods, and we wanted to show that by combining two objectives and distilling into a distillation, our method provides improvement either in distillation quality or/and in compression rate. \n\n[1] MTT: https://arxiv.org/abs/2203.11932"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244984247,
                "cdate": 1700244984247,
                "tmdate": 1700244984247,
                "mdate": 1700244984247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nis7zme1vF",
                "forum": "mdqOjfQ29b",
                "replyto": "VjNwLFZYOv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4136/Reviewer_TU1J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4136/Reviewer_TU1J"
                ],
                "content": {
                    "title": {
                        "value": "Thank for your response."
                    },
                    "comment": {
                        "value": "Thanks, i have no further questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648687813,
                "cdate": 1700648687813,
                "tmdate": 1700648687813,
                "mdate": 1700648687813,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]