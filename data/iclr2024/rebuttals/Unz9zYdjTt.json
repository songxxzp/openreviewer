[
    {
        "title": "FedNovel: Federated Novel Class Learning"
    },
    {
        "review": {
            "id": "mFpCh8mbdg",
            "forum": "Unz9zYdjTt",
            "replyto": "Unz9zYdjTt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5479/Reviewer_hsUT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5479/Reviewer_hsUT"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores novel class discovery and learning within the framework of federated learning, addressing the challenge of evolving data on local client devices. The study introduces a Global Alignment learning framework aimed at estimating the number of global novel classes and providing guidance for local training. Specifically, this framework initially estimates the number of novel classes by identifying high-density regions within the representation space. Then, it captures all potential correlations between prototypes and training data to alleviate issues related to data heterogeneity. The proposed approach demonstrates advanced performance across a range of benchmark datasets"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to read. \n\n2. It introduces an effective approach for discovering novel classes with Global Alignment learning, specifically targeting the federated learning setting with dynamic change in data distribution.  \n\n3. The authors conducted thorough experiments to validate the effectiveness of their proposed approach. Through empirical evidence, they support their claims and offer insights into the performance and advantages of novel class learning under the constraint of non-iid data, and privacy protection."
                },
                "weaknesses": {
                    "value": "1. The method lacks sufficient elaboration and requires additional effort to comprehend the underlying technique. For instance, it encompasses multiple design elements, yet Figure 1 fails to offer adequate details to facilitate a clear understanding of the approach. And it is difficult to visualize the algorithm. It would be better if the paper provided an algorithm block. (Q1, Q2, Q4)\n\n2. Some of the assertions made in the paper need additional justifications. (Q3)"
                },
                "questions": {
                    "value": "1. What is the training process? \n\n(a) In section 3.2, the known classes depend on Equation (2) to converge to model $m^L$. Then Section 3.3.1 includes a modified PCL to enhance training. Is the modified PCL included during the training for $m^L$ or after the global server achieved $m^L$?\n\n(b) From Figure 1, the local prototypes are only uploaded once to the server, which contradicts my understanding of FL: the local models should communicate with the central server for several rounds until converge. And as the local model keeps updating, the local prototypes should also evolve, how does one-time uploading transmit every local information?\n\n2. Section 3.2 mentions the unlabeled testing datasets \"belongs to a unified novel label space\", does it assume the number of classes of novel samples is fixed? If this number is not fixed, then how to decide the number of clusters? If this number is fixed, how does it apply to the setting where the novel data are continually emerging?\n\n3. Section 3.3.2 chooses neuron weights as prototypes since the data from other clients is unavailable in the FL setting. However, there is still a lot of difference between weights and features. Any justification to support that using weights as prototypes is as effective as data/features?\n\n4. The \"anchor sample\" first appears in Section 3.3.1. How are they selected, or what is their definition? \n\n5. In Section 3.3.2, there is a data memory storing filtered-out data. Are these data filtered out because they are known class data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5479/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5479/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5479/Reviewer_hsUT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633779986,
            "cdate": 1698633779986,
            "tmdate": 1699636559237,
            "mdate": 1699636559237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rKwMQAAZ2L",
                "forum": "Unz9zYdjTt",
                "replyto": "mFpCh8mbdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hsUT Part [1]"
                    },
                    "comment": {
                        "value": "We really appreciate your constructive comments and suggestions. We provide the following response to answer your questions and address your concerns.\n\n\nWeaknesses:\n\n>1. The method lacks sufficient elaboration and requires additional effort to comprehend the underlying technique. \n\nThanks for your suggestion. We have revised Figure 1 in our manuscript to include more information and details, enabling it to more clearly express the method we propose. As for the algorithm flowchart, we have provided the optimization pipeline for the novel class learning stage in the appendix (Section C). In our revision, we present the complete GAL algorithm process, encompassing both the known class learning and novel class learning stages.\n\n>2. Some of the assertions made in the paper need additional justifications.\n\nThank you for your comment. We have answered your questions in detail in the subsequent Question Section. Please refer to the responses provided below.\n\nQuestions:\n\n>1.(a) The known class learning training loss.\n\nWe apologize for the confusion caused. To develop model $m^L$, during the known class learning, we train the model using both the modified Prototype Contrastive Learning (PCL) and cross-entropy loss simultaneously.\n\n>1.(b) The question about the prototype communication.\n\nAs illustrated in Figure 1, **local prototypes are uploaded to the server only once** at the beginning of novel class learning. Upon receiving these local prototypes, the server uses Potential Prototype Merging (PPM) to estimate the number of novel classes and to construct global prototypes. These global prototypes are then distributed to all clients, who use them to initialize the novel class classifier. Then, the novel class classifier is updated through local training and central aggregation. Thus, **the update of the novel class classifier essentially equals the evolution of the global prototypes.** In this case, the entire novel class learning process requires only a single communication of local prototypes at the beginning. The reduction in the number of prototype communications further enhances privacy protection.\n\n>2. Whether the number of novel classes is fixed or not during novel class learning.\n\nIn FedNovel, we assume that **novel class learning can be conducted periodically**. During each novel class learning stage, we only invite clients who are active and available at the time to participate, and we pause the update of their data memories. Therefore, in this case, the number of novel classes in each novel class learning stage remains constant. For the continuously incoming novel data, we can initiate another stage of novel class learning when the novel data memory of the majority of clients is nearly full.\n\n>3. Any justification to support that using weights as prototypes is as effective as data/features?\n\nYour understanding is correct. The weights of a standard linear fully connected layer indeed differ significantly from features. However, under specific conditions, if we ensure that **the bias of the linear fully connected layer are zero**, and train the neural network using the cross-entropy loss, the weights of the linear fully connected layer can be viewed as prototypes [1]. Moreover, the classifier consists solely of a single unbiased linear fully connected layer, which is put right after the feature extractor. Consequently, **the dimensionality of this layer's weights is the same as that of the extracted representations**. With the bias consistently set to zero, the computation of cross-entropy loss is based on the softmax probabilities derived from the inner products between the representations and the weights of that linear layer. Minimizing cross-entropy loss increases the inner product between the sample representation and the corresponding neuron weight (associated with the sample's ground truth label). The inner product between vectors is proportionally related to the cosine similarity between them. Thus, **an increase in the inner product equals an increase in cosine similarity**. Therefore, we can consider the neuron weights of the linear fully connected layer as prototypes based on cosine similarity.\n\n>4. The typo of \"anchor sample\" that appears in Section 3.3.1.\n\nThanks for your question and careful reading. The term 'anchor sample' was a typo. Our original intention was to refer to each regular training sample. We have removed the term 'anchor' in our revision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450673687,
                "cdate": 1700450673687,
                "tmdate": 1700450673687,
                "mdate": 1700450673687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jO8THlIZGB",
                "forum": "Unz9zYdjTt",
                "replyto": "mFpCh8mbdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of Further Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer hsUT,\n\nThe conclusion of the discussion period is closing, and we eagerly await your response. We greatly appreciate your time and effort in reviewing this paper and helping us improve it.\n\nThank you again for the detailed and constructive reviews. We hope our response is able to address your comments related to the training pipeline clarification, questions about the novel class number, justification about neural parameter-based prototypes, and clarification of novel data memory. We take this as a great opportunity to improve our work and shall be grateful for any additional feedback you could give us.\n\nBest Regards,\n\nAuthors of Paper 5479"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577094597,
                "cdate": 1700577094597,
                "tmdate": 1700577094597,
                "mdate": 1700577094597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rhycmagIAx",
                "forum": "Unz9zYdjTt",
                "replyto": "mFpCh8mbdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Phase Ends in Less Than 24 Hours. We Eagerly Await Reviewer hsUT's Feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer hsUT,\n\nAs the rebuttal discussion phase ends in less than 24 hours, we want to express our gratitude for your engagement thus far. We shall kindly remind you that after the 22nd, we are not allowed to respond to your further questions you may have. We really want to check with you whether our response addresses your concerns during the author-reviewer discussion phase. We have diligently addressed every concern and question you raised during the initial review, and our extensive efforts are aimed at enhancing the clarity and quality of our work.\n\nYour feedback is really important to us. We eagerly await any potential updates to your ratings, as they play a critical role in the assessment of our paper. We genuinely hope our responses have resolved your concerns and provided satisfactory explanations. Your thoughtful evaluation greatly aids in our paper's refinement and strength. We sincerely appreciate your dedication and time again.\n\nBest regards,\n\nAuthors of Paper 5479"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661080083,
                "cdate": 1700661080083,
                "tmdate": 1700661080083,
                "mdate": 1700661080083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0hX1tfzLYH",
                "forum": "Unz9zYdjTt",
                "replyto": "mFpCh8mbdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final Reminder before Reviewer-Author Discussion Phase Closure for Reviewer hsUT"
                    },
                    "comment": {
                        "value": "Dear Reviewer hsUT,\n\nThank you again for the initial comments. As the Reviewer-Author Discussion phase is closing in less than 12 hours (Nov. 22nd AoE), we would greatly appreciate any feedback on our rebuttal. We fully understand that you may be busy at this time, but hope that you could kindly have a quick look at our responses and assess whether they have addressed your concerns and warrant an update to the rating. We would also welcome any additional feedback and questions.\n\nBest Regards,\n\nAuthors of Paper 5479"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699927751,
                "cdate": 1700699927751,
                "tmdate": 1700699927751,
                "mdate": 1700699927751,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dzdxp8vKmg",
            "forum": "Unz9zYdjTt",
            "replyto": "Unz9zYdjTt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5479/Reviewer_RSBY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5479/Reviewer_RSBY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a prototype-based class number estimation method for Federated Novel Class Discovery, where the model is required to merge and align novel classes that are discovered and learned by different clients under privacy constraint. Extensive experimental results demonstrate the effectiveness of proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.\tThis paper is well-written, well-organized and easy to follow.\n2.\tThe performance of the proposed method is impressive.\n3.\tAblation studies are comprehensive and demonstrate the effectiveness of proposed method."
                },
                "weaknesses": {
                    "value": "1.\tLacks of crucial reference literatures [A][B]. Thus, Federated New Class Discovery/Learning is not a new research problem. In contribution, authors say,\u201d we are the first to focus on this problem and propose an effective solution\u201d. It might be somewhat overclaiming.\n2.\tInsufficient comparison and discussion. From my understanding, the proposed method is similar to commonly-used semi-supervised learning methods. \n3.\tLimited novelty. From the perspective of NCD methods, prototype-based contrastive learning and low confidence sample rejection have explored in [C]. Why is the proposed method superior to [C]? From the perspective of semi-supervised federated learning methods, the federated prototype learning has been studied in [D][E]. What\u2019s the novelty of the proposed methods compared with [D][E]?\n4.\tFrom my understanding, [A][B] can also be used in federated new class learning. It is better to discuss and compare the proposed method with [A][B].\n5.\tLack of theoretical proof why the estimation method is better than other competitors.\n6.\tThe reasonableness of the experiment setting still needs to be considered. As authors claimed in the paper, they use Dirichlet Distribution to control data heterogeneity. (1) It makes client labeled and unlabeled data highly-unbalanced. However, in [F], the labeled and unlabeled data are kept fixed partitioning. It is better to discuss the relationship between proposed method and data distribution. (2) It might lead clients to have some sharing categories or have non-overlapping categories. How does the proposed method solve both situations?\n\n[A] Towards Unbiased Training in Federated Open-world Semi-supervised Learning. ICML, 2023.\n[B] Federated Generalized Category Discovery, Arxiv, 2023.\n[C] Opencon: Open-world contrastive learning. TMLR, 2022.\t\n[D] Fedproto: Federated prototype learning across heterogeneous clients, AAAI,2022.\n[E]Federated Semi-Supervised Learning with Prototypical Networks, Arxiv,2022.\n[F] Generalized Category Discovery, CVPR, 2022."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722333225,
            "cdate": 1698722333225,
            "tmdate": 1699636559151,
            "mdate": 1699636559151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G2XoIZICjC",
                "forum": "Unz9zYdjTt",
                "replyto": "dzdxp8vKmg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RSBY Part [1]"
                    },
                    "comment": {
                        "value": "We would like to thank you for your constructive comments. We will address your concerns, answer your questions, and clarify your misunderstandings as follows.\n\n>1. Lacks of crucial reference literature.\n\nTo prevent misunderstandings, we need to clarify that Federated OpenSet/OpenWorld Learning (FOL) is related to but distinctly different from, the FedNovel problem we are focusing on. Firstly, **their application scenarios differ**. FOL is typically applied in static federated training, where the simultaneous learning of known and novel classes is achieved within a single training phase. In contrast, FedNovel focuses on dynamic federated training, where the federated model, after having been employed for a period following the training of known classes, is then found to require the learning of novel classes. **For dynamic federated training, the goal extends beyond learning novel classes to effectively mitigate the forgetting of known classes. FOL, which only considers single-phase training, is not applicable in continual dynamic federated learning.**\n\nSecondly, **the training settings differ**. For FOL, given its focus on static federated training, clients' local data is also static, meaning there is no continual influx of streaming data, and **labeled data can be statically retained for training**. Furthermore, static federated training does not allow the participation of new clients. In contrast, new clients in dynamic federated training often have no data at all. Although their streaming data continues to arrive, it also lacks any labels. FedNovel specifically addresses the challenges posed by the continual arrival of streaming data. It assumes that during the novel class learning stage, **there will be no labeled data available, whether for known or novel classes**.\n\nCertainly, we understand your concern. We have already discussed the similarities and differences between FedNovel and FOL in the related work section and **removed the claim of the first work in our revised manuscript**. Furthermore, we have **modified the title of our paper to \"Federated Continual Novel Class Learning\"** to avoid any confusion and misunderstanding.\n\n>2. Insufficient comparison and discussion.\n\nCommonly used semi-supervised learning (SSL) typically assumes that the training data is partially labeled and that **both labeled and unlabeled data belong to the same label space**. The most prevalent method in SSL is progressive learning. This approach initially trains the model on labeled data, then tests it on unlabeled data, selecting high-confidence data to add back into the training set. As training goes on, an increasing amount of unlabeled data is incorporated, gradually improving the model's performance. In contrast, **the novel class learning problem that FedNovel focuses on has no labeled data available, and the novel classes do not overlap with the known classes**. Under such settings, even if a model trained on known classes is tested on novel classes, the confidence becomes meaningless due to the completely different label spaces.\n\nWe understand your concerns. During the rebuttal phase, we conducted comparison experiments between our GAL approach and a federated semi-supervised learning work, SemiFL [F]. The experiment results are provided below, and we can observe that GAL can still substantially outperform SemiFL.\n\n| Dataset |  | CIFAR-100 |  |  |  | Tiny-ImageNet |  |  |  | ImageNet-Subset |  |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|  | known | novel | all |  | known | novel | all |  | known | novel | all |\n| SemiFL | 42.3 | 28.8 | 39.6 |  | 33.2 | 25.5 | 32.5 |  | 37.1  | 27.5 | 35.2 |\n| Our GAL | 72.6 | 45.8 | 67.3 |  | 57.6 | 32.7 | 55.0 |  | 55.8 | 29.7 | 50.5 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450184286,
                "cdate": 1700450184286,
                "tmdate": 1700694398732,
                "mdate": 1700694398732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bd7p9j9OHE",
                "forum": "Unz9zYdjTt",
                "replyto": "dzdxp8vKmg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder of Further Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer RSBY,\n\nThe conclusion of the discussion period is closing, and we eagerly await your response. We greatly appreciate your time and effort in reviewing this paper and helping us improve it.\n\nThank you again for the detailed and constructive reviews. We hope our response is able to address your comments related to the comparison and discussion with more literature, novelty clarification, theoretical explanation of PPM, and issues about experiments. We take this as a great opportunity to improve our work and shall be grateful for any additional feedback you could give us.\n\nBest Regards,\n\nAuthors of Paper 5479"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576796489,
                "cdate": 1700576796489,
                "tmdate": 1700576796489,
                "mdate": 1700576796489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GSDCzGgg6l",
                "forum": "Unz9zYdjTt",
                "replyto": "dzdxp8vKmg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Phase Ends in Less Than 24 Hours. We Eagerly Await Reviewer RSBY's Feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer RSBY,\n\nAs the rebuttal discussion phase ends in less than 24 hours, we want to express our gratitude for your engagement thus far. We shall kindly remind you that after the 22nd, we are not allowed to respond to your further questions you may have. We really want to check with you whether our response addresses your concerns during the author-reviewer discussion phase. We have diligently addressed every concern and question you raised during the initial review, and our extensive efforts are aimed at enhancing the clarity and quality of our work.\n\nYour feedback is really important to us. We eagerly await any potential updates to your ratings, as they play a critical role in the assessment of our paper. We genuinely hope our responses have resolved your concerns and provided satisfactory explanations. Your thoughtful evaluation greatly aids in our paper's refinement and strength. We sincerely appreciate your dedication and time again.\n\nBest regards,\n\nAuthors of Paper 5479"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660966679,
                "cdate": 1700660966679,
                "tmdate": 1700660966679,
                "mdate": 1700660966679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WAyFwfnxnK",
                "forum": "Unz9zYdjTt",
                "replyto": "dzdxp8vKmg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final Reminder before Reviewer-Author Discussion Phase Closure for Reviewer RSBY"
                    },
                    "comment": {
                        "value": "Dear Reviewer RSBY,\n\nThank you again for the initial comments. As the Reviewer-Author Discussion phase is closing in less than 12 hours (Nov. 22nd AoE), we would greatly appreciate any feedback on our rebuttal. We fully understand that you may be busy at this time, but hope that you could kindly have a quick look at our responses and assess whether they have addressed your concerns and warrant an update to the rating. We would also welcome any additional feedback and questions.\n\nBest Regards,\n\nAuthors of Paper 5479"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699861926,
                "cdate": 1700699861926,
                "tmdate": 1700699861926,
                "mdate": 1700699861926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WY9C3CiQiE",
                "forum": "Unz9zYdjTt",
                "replyto": "G2XoIZICjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_RSBY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_RSBY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response and detailed explanation. As pointed out by Reviewer a8Cy, the authors claim that FedNovel belongs to continual learning. However, this might result in a simpler and impractical setting for the following reasons: 1) Only engaging in one-step incremental new category discovery fails to demonstrate the ability for multi-step continuous learning, as seen in DM or iGCD[G]; 2) In comparison to fedossl[A] and Fed-GCD[B], the proposed FedNovel trains the model in two separate steps, using fully labeled and fully unlabeled data respectively, instead of training on mixed labeled and unlabeled data. Consequently, FedNovel contributes to a somewhat simpler and less realistic task.\n\n[A]: \"Towards Unbiased Training in Federated Open-world Semi-supervised Learning,\" ICML, 2023. [B]: \"Federated Generalized Category Discovery,\" Arxiv, 2023. [G]: \"Incremental Generalized Category Discovery,\" ICCV, 2023. [H]: \"Grow and Merge: A Unified Framework for Continuous Categories Discovery,\" NIPS, 2022."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707814327,
                "cdate": 1700707814327,
                "tmdate": 1700707814327,
                "mdate": 1700707814327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7DesLGrrzm",
                "forum": "Unz9zYdjTt",
                "replyto": "dzdxp8vKmg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply! This is our further response, part [2]."
                    },
                    "comment": {
                        "value": "In our response to Reviewer a8Cy, we elaborated on the differences in learning difficulty between label available novel class discovery and continual novel class learning. For label available NCDL, there are generally two approaches. The first one is based on semi-supervised learning (SSL), where the model is trained using labeled known class data while simultaneously selecting high-confidence unlabeled novel class data to participate in training. As training progresses, more novel class data are incorporated into training, enhancing the model's ability to extract representations for novel classes. The model's ability to extract representations for known classes is directly determined by the labeled known class data, **which means that the learning difficulty for known classes is fixed as long as the supervised training is applied.** Under such a scenario, if learning for known and novel classes is regarded as two separate tasks, **the SSL-based NCDL works like multi-task learning, leading to the learning of more task-shared features. Such task-shared features greatly benefit novel class learning.** The second approach to labeled available NCDL typically involves sufficient supervised pre-training using labeled known class data, followed by learning from unlabeled novel class data under the guidance of the labeled known class data. **With this approach, the model, trained on labeled known class data, tends to learn task-private features that are less beneficial to novel class learning compared to the task-shared features.** However, the second approach **still allows for the use of labeled known class data during novel class learning, facilitating cross-task transfer learning.** If there is any forgetting of the knowledge about known classes during novel class learning, it can be alleviated by retraining on the labeled known class data. Our proposed GAL is similar in training setup to the second method, but **differs in that there is no labeled known class data available to maintain the learned knowledge** during novel class learning. Although a significant portion of the knowledge the model has acquired about known classes is task-private, **there still exists some task-shared general knowledge. Without a dedicated design to combat knowledge forgetting, both task-shared and task-private knowledge will gradually fade away.** Once the task-shared knowledge is forgotten to a certain extent, novel class learning becomes an impossible task. Indeed, the difficulty of training for a task largely depends on the supervision information. **For labeled known class data, whether trained simultaneously with unlabeled novel class data or in supervised pre-training before novel class learning, the learning difficulty remains the same. For unlabeled novel class data, which lacks labels for novel classes, the learning difficulty is significantly higher, especially without even any supervision information from known classes.**\n\n**We have removed the claim of being the first work in our revised manuscript, but we still hope to inspire further exploration into the scenario of federated continual novel class learning through our work. It is by addressing these challenges that federated learning can be sustainably applied in the ever-changing real world.**\n\n\n[1] Grow and Merge: A Unified Framework for Continuous Categories Discovery. NIPS, 2022.\n\n[2] Class-incremental Novel Class Discovery. ECCV, 2022"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713716295,
                "cdate": 1700713716295,
                "tmdate": 1700713818221,
                "mdate": 1700713818221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iZJu8mBa8L",
            "forum": "Unz9zYdjTt",
            "replyto": "Unz9zYdjTt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5479/Reviewer_a8Cy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5479/Reviewer_a8Cy"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a novel FL scenario, where the data distribution involves dynamic and continual changes. Instead of naively integrating FL and conventional novel class discovery methods,  the authors propose a Global Alignment Learning (GAL) framework to estimate the number of novel classes and optimize the local training process in a semantic similarity-empowered reweighting manner. Extensive experiments have been conducted to demonstrate the efficiency of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[+] The whole paper is easy to understand, and well-written.\n\n[+] The problem statement is very nice and clean. It also has some applications in practice.\n\n[+] There is enough empirical evidence to support the main claims of the paper."
                },
                "weaknesses": {
                    "value": "[-] It seems that open-world semi-supervised learning [1][2] also considers classifying both seen and unseen classes during the testing phase. Please compare it in related work.\n\n[1] Open-world semisupervised learning. In International Conference on Learning Representations, 2022.\n\n[2] Robust semi-supervised learning when not all classes have labels. In Advances in Neural Information Processing Systems, 2022.\n\n[-] In section 4, the baselines on federated self-supervised learning methods are insufficient. It is suggested to add more FedSSL methods in the experimental part.\n\n[3] Semifl: Semi-supervised federated learning for unlabeled clients with alternate training. Advances in Neural Information Processing Systems, 35:17871\u201317884, 2022."
                },
                "questions": {
                    "value": "It is unclear why the value of $n_{size}$ is set as 2. A more detailed explanation should be added."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746424660,
            "cdate": 1698746424660,
            "tmdate": 1699636559037,
            "mdate": 1699636559037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vgGlz4nz7I",
                "forum": "Unz9zYdjTt",
                "replyto": "iZJu8mBa8L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer a8Cy"
                    },
                    "comment": {
                        "value": "We would like to thank you for your positive and constructive comments on our work. We will address your concerns one by one as follows.\n\n>1. Discuss more related works.\n\nThank you for your suggestion, we have discussed open-world semi-supervised learning (OSSL) in the Related Work of the revised paper. Although the setup of OSSL does share certain similarities with ours, there are still key differences: (1) **The data distribution differs**. In OSSL, the training data is centralized to locate a machine during training. However, in FedNovel, the data is distributed to multiple clients following the non-IID setting, which means the local data distributions of clients are distinct from each other. (2) **The data availability differs**. The data in OSSL are static, which means all labeled and unlabeled data can be accessed at the same time, thus they can be learned within a single training phase. In contrast, the data in FedNovel are dynamic. FedNovel first requires the model to learn only on labeled known classes, then moves to learn on unlabeled novel classes without access to labeled known class data. Therefore, in the novel class learning stage, there is one more challenge in FedNovel than OSSL, which is to preserve good performance on known classes.\n\n\n>2. Experimental results of SemiFL.\n\nThanks for your suggestion. Similar to what we mentioned before, the key difference between SemiFL and FedNovel is also the **data availability**. The data in SemiFL are static while the data in FedNovel are dynamic. SemiFL can train on all labeled and unlabeled data in the same training phase, while FedNovel can only train on labeled classes in the first training phase and unlabeled classes in the following training phase. To further evaluate the effectiveness of our GAL, we also carry out the comparison experiments with SemiFL as below. Based on the results, GAL still achieves better performance than SemiFL.\n\n| Dataset |  | CIFAR-100 |  |  |  | Tiny-ImageNet |  |  |  | ImageNet-Subset |  |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|  | known | novel | all |  | known | novel | all |  | known | novel | all |\n| SemiFL | 42.3 | 28.8 | 39.6 |  | 33.2 | 25.5 | 32.5 |  | 37.1  | 27.5 | 35.2 |\n| Our GAL | 72.6 | 45.8 | 67.3 |  | 57.6 | 32.7 | 55.0 |  | 55.8 | 29.7 | 50.5 |\n\n>3. Why $n_{size}$ is set as 2.\n\nIn our Potential Prototype Merging (PPM), $n_{size}$ is used to control the data scale of high-density regions. As the data in FedNovel are non-IID distributed among clients, **a particular client may have only a few data samples for certain novel classes. Besides, for a particular novel class, it may be possible that only a few clients own this class**. Setting a small $n_{size}$ can help our PPM not ignore the novel classes that correspond to a small number of local prototypes."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449895540,
                "cdate": 1700449895540,
                "tmdate": 1700449895540,
                "mdate": 1700449895540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8Hv0PBpd9u",
                "forum": "Unz9zYdjTt",
                "replyto": "vgGlz4nz7I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_a8Cy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_a8Cy"
                ],
                "content": {
                    "comment": {
                        "value": "I really appreciate the authors' response. Most of my concerns are well addressed. However, according to **Response to Reviewer RSBY**, the authors claimed that the application scenario belongs to continual learning, I'm afraid this setting would be easier than traditional static federated training, where the known and novel classes are simultaneously learned during the training phase."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559292540,
                "cdate": 1700559292540,
                "tmdate": 1700559292540,
                "mdate": 1700559292540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cogUINbv2o",
                "forum": "Unz9zYdjTt",
                "replyto": "iZJu8mBa8L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Do We Address Your Further Question? Further Check with Reviewer a8Cy"
                    },
                    "comment": {
                        "value": "Dear Reviewer a8Cy,\n\nThank you for your initial positive feedback and insightful suggestions. We appreciate your recognition of our efforts to address your concerns. We would like to check with you whether our response addresses your further concern about the learning difficulty of different application scenarios. We really hope our response can address your concern, certainly, we will be glad to answer any further questions.\n\nWe value your input and sincerely hope you consider raising your rating based on the improvements we\u2019re implementing. Your endorsement would greatly enhance the credibility of our work.\n\nThank you once again for your time and valuable feedback.\n\nBest Regards,\n\nAuthors of Paper 5479"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661513913,
                "cdate": 1700661513913,
                "tmdate": 1700661513913,
                "mdate": 1700661513913,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ppceEt6Kje",
                "forum": "Unz9zYdjTt",
                "replyto": "wweGyEpmqO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_a8Cy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_a8Cy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response.  In my opinion, label-available NCDL methods [a] [b] [c] focus on a more practical scenario, where novel classes may appear in the unlabeled data. The proposed methods alleviate the training bias among labeled and unlabeled data caused by the class distribution mismatch. For novel class learning, they should avoid interference in the mixed training process since novel classes may be mistakenly classified into known classes. If the one-step training becomes two-step training, the above challenge may no longer exist.  The main challenge turns to how to avoid catastrophic forgetting and how to ensure consistency of the generated pseudo-labels between different clients. Based on this, I prefer to maintain my rating.\n\n[a] Open-world semisupervised learning. ICLR, 2022.\n\n[b] Robust semi-supervised learning when not all classes have labels. NeurIPS, 2022.\n\n[c] Towards Unbiased Training in Federated Open-world Semi-supervised Learning. ICML, 2023."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733563525,
                "cdate": 1700733563525,
                "tmdate": 1700733563525,
                "mdate": 1700733563525,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PXHw110V4p",
            "forum": "Unz9zYdjTt",
            "replyto": "Unz9zYdjTt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5479/Reviewer_skcH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5479/Reviewer_skcH"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel method to learn novel classes in federated learning with emerging unknown classes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. (Originality) The proposed method is novel by known-class representation learning and adaptive class merging without access to clients' data.\n2. (Clarity) The paper is clear in techniques. Methods are well formulated and motivated. Sufficient details are provided for the experiments.\n3. (Significance) The proposed method is sufficiently evaluated in multiple datasets, models including small-scale sets (like Cifar10, or Cifar100) and large-scale sets (ImageNet). In all of these experiments, the proposed methods outperform the baselines in both known class and novel class evaluations.\n4. (Quality) Extensive experiments evaluate the method in multiple dimensions. Importantly, multiple federated learning is demonstrated to be integrable with the proposed method."
                },
                "weaknesses": {
                    "value": "1. The authors claim their contribution as a federated novel-class learning without compromising privacy. However, it is unclear how the existing federated novel-class learning methods compromise privacy. Importantly, the definition of private information is vague. It seems that the number of novel classes is thought to be private, which however is not necessarily true. Without specification on the privacy definition, there also lacks sufficient justification for how the proposed method will protect privacy. Though I appreciate the empirical results of privacy evaluation, the authors should clarify the meaning of privacy and adjust the claim of privacy."
                },
                "questions": {
                    "value": "* What is the definition of private information in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5479/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5479/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5479/Reviewer_skcH"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698902382091,
            "cdate": 1698902382091,
            "tmdate": 1700690473003,
            "mdate": 1700690473003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O4gJWNW6ub",
                "forum": "Unz9zYdjTt",
                "replyto": "PXHw110V4p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer skcH"
                    },
                    "comment": {
                        "value": "First of all, we really apprepiate your positive and constructive comments on our work. We will address your concerns one by one.\n\n>1. The number of novel classes\n\n**In FedNovel, the number of novel classes is not private information.** This can be verified by works [1] in federated open-world semi-supervised learning, where the server is assumed to know the exact number of unseen classes before training. In contrast, the proposed GAL can estimate the number of novel classes by using our designed Potential Prototype Merging (PPM).\n\n[1] Towards Unbiased Training in Federated Open-world Semi-supervised Learning. ICML, 2023.\n\n>2. Definition of private information\n\n**The private information that needs to be protected in FedNovel is the meaningful semantic information of raw data for every client.** If existing data reconstruction attacks can't recover such semantic information from the prototypes sent by each client, we can say that our GAL does not leak private information. We have carried out experiments of launching data reconstruction attacks on the prototypes in GAL. The detailed results are shown in Appendix B.8, and the reconstructed images shown in Figure 3 look very similar to random noise in human eyes. To further verify that semantic information is not contained in the reconstructed images, we compute the Structural Similarity (SSIM) between reconstructed images and all images from each class. Then we check whether there is a set of classes that always share higher similarity with the reconstructed images and whether this class set is major included in the clusters corresponding to the attacked prototypes. Specifically, we choose the top 50% classes (10 out of 20) according to the average SSIM and set their corresponding dimensions as 1 in a zero vector. We also create another zero vector with the major component classes of a prototype as 1. Then we calculate the Manhattan Distance (MD) between these two vectors. For a better comparison, we also obtain the same vectors for random noise images and calculate the MD between vectors of reconstructed and random noise images. **According to the following results, we can conclude that data reconstruction attacks cannot recover meaningful semantic information, thus our GAL is privacy-preserving.**\n\n|Manhattan Distance|CIFAR-100|Tiny-ImageNet|ImageNet-Subset|\n|:---:|:---:|:---:|:---:|\n|with the vectors of major cluster components|9.7|10.1|10.5|\n|with the vectors of gaussian noise images|10.1|9.9|10.2|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449758047,
                "cdate": 1700449758047,
                "tmdate": 1700449758047,
                "mdate": 1700449758047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "olLAknYAGu",
                "forum": "Unz9zYdjTt",
                "replyto": "O4gJWNW6ub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_skcH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_skcH"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for authors' responses, which addressed all of my concerns."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588857532,
                "cdate": 1700588857532,
                "tmdate": 1700588857532,
                "mdate": 1700588857532,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]