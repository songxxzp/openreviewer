[
    {
        "title": "ContextRef: Evaluating Referenceless Metrics for Image Description Generation"
    },
    {
        "review": {
            "id": "hQyn5QRN3v",
            "forum": "j0ZvKSNZiP",
            "replyto": "j0ZvKSNZiP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission357/Reviewer_T1dS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission357/Reviewer_T1dS"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents ContextRef, a new benchmark designed to assess the alignment of referenceless image description quality metrics with human preferences. The proposed benchmark incorporates human-subject ratings across diverse quality dimensions and ten robustness checks to gauge metric performance under varying conditions. Additionally, the study delves into several metric approaches, revealing that none of them prove successful on ContextRef due to their insensitivity to fundamental changes in examples. Lastly, the paper suggests that careful fine-tuning has the potential to enhance metric performance while acknowledging the ongoing challenge of effectively accounting for context."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to understand.\n2. The introduction of the new benchmark, ContextRef, is indeed a valuable contribution to the field. A crucial feature of ContextRef is the presentation of images and descriptions within a contextual framework, which plays a crucial role in generating appropriate descriptions."
                },
                "weaknesses": {
                    "value": "While the paper highlights the underperformance of existing methods when dealing with context, this finding is unsurprising to me since the tested models were not trained with context."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771281819,
            "cdate": 1698771281819,
            "tmdate": 1699635962834,
            "mdate": 1699635962834,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ePeFqAhrY1",
                "forum": "j0ZvKSNZiP",
                "replyto": "hQyn5QRN3v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the encouraging comments and supportive feedback! Indeed, the relatively poor performance of these models in scoring image descriptions in context did not come as a significant surprise to us \u2013 prior work showing the poor performance of referenceless metrics in this setting (e.g. [1]) was a significant motivation for this work. Our paper establishes a solid empirical basis for this claim, offers new evidence that context is crucial, and points the way to variants of these metrics that are more successful, with new data and new fine-tuning results.\n\nAt the same time, one may find the poor performance of likelihood-based models somewhat surprising because, indeed, one may argue that these models are specifically trained with context. In particular, language models have been consistently shown to extract remarkably much from information simply presented in their context windows; this property of large pretrained language models has been the subject of extensive interest in NLP [2,3].\n\nThus, we\u2019d like to emphasize that, 1) while the observation that these models perform poorly in context is in line with prior work, the universality of this property was likely not something we could have anticipated a priori and 2) we expect there may be additional strategies that leverage pretrained models in novel ways to better incorporate contextual information into existing referenceless metrics, as done in the contextual CLIPScore from [1] \u2013 in fact, this is one of the motivations for creating this benchmark.\n\nFinally, independent of how surprising the poor performance of models for integrating context might be, our goal is that this framework will help to push the development of metrics that are sensitive to image context and therefore lead to improvements in the theoretical approach to the problem and, eventually, the downstream usefulness of those systems.\n\nBased on your comment, we further highlighted the context integration procedures in Section 3 and Appendix H, as well as in the extended related works discussion in Appendix J.\n\n--\n\n[1] \u201cContext matters for image descriptions for accessibility: Challenges for referenceless evaluation metrics,\u201d Kreiss et al. 2022\n\n[2] \u201cAn Explanation of In-context Learning as Implicit Bayesian Inference\u201d Xie et al. 2021\n\n[3] \u201cRethinking the Role of Demonstrations: What Makes In-Context Learning Work?\u201d Min et al. 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241927597,
                "cdate": 1700241927597,
                "tmdate": 1700241965193,
                "mdate": 1700241965193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OlO58hP1gz",
            "forum": "j0ZvKSNZiP",
            "replyto": "j0ZvKSNZiP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission357/Reviewer_hp9w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission357/Reviewer_hp9w"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an evaluation protocol to evaluate referenceless metrics commonly used for the assessment of text image description generators. Authors first build an evaluation dataset as a subset of the WIT dataset comprising 204 image-caption-context-description samples. This dataset is used for a user study, where humans are rating the quality of different image description across multiple axes. Human preference is then used as a reference to evaluate the quality of referenceless metric, alongside measures of metric robustness by applying several transformations to the generated descriptions. Finally, authors investigate whether model-based metrics can be improved by fine-tuning models on human scores and augmentations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses an interesting topic. Referenceless metrics are often used as a means to demonstrate the performance of a new model, with limited insights into how reliable they can be. Providing a solution to understand which metrics are most accurate has the potential to be very useful.  \n\nAuthors have evaluated a large set of model-based evaluation metrics, with the somewhat low correlations suggesting a lack of reliability. Furthermore, the reference dataset and associated human study are a useful resource that could be used to evaluate future metrics."
                },
                "weaknesses": {
                    "value": "The paper presentation could be substantially improved. The related work section is focusing almost solely on the clip model,and fails to provide an accurate review of pre-existing related literature. Authors should review pre-existing metrics, related benchmarks and evaluation strategies. Similarly, section 3 provides descriptions of metrics being analysed in this work, and lacks accurate descriptions to understand the particularities of each method. For example, only encoder architectures are discussed for the Flamingo model, but the actual mechanisms used to compute the metric are not discussed. The strategy used to integrate context information as also poorly explained, and could benefit from providing equations. \n\nCertain claims seem excessive. Authors mention that their results suggests that referenceless metrics can be successfully used based on observed correlation, however somewhat low correlations are observed (.2-.3), suggesting the opposite. In addition, the use of context is highlighted multiple times throughout the paper, yet the impact of integrating this context variable is barely discussed, the only relevant experiment is the use of shuffled contexts in section 5.2. It would have been interesting to discuss context more in depth and how it impacts user evaluation and metric performance. \n\nThe fine-tuning experiment in section 6 doesn\u2019t seem particularly necessary, as metrics become tailored for a specific dataset/user preference, This reduces applicability and generalizability, and seem to contradict the main objective of the paper."
                },
                "questions": {
                    "value": "- Section 4.1 mentions that the dataset is built by collecting caption-alt description pairs for each image, yet in the human study, it is mentioned that 5 descriptions are provided. Where are the additional descriptions obtained from? \n\n- There appears the be a lot of similarities between the data collection and human study protocol proposed in this paper and in Kreiss et al 2022b. Can authors clarify how related these two strategies are?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772588743,
            "cdate": 1698772588743,
            "tmdate": 1699635962749,
            "mdate": 1699635962749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wWDN74Gc0f",
                "forum": "j0ZvKSNZiP",
                "replyto": "OlO58hP1gz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for all of the constructive comments and helpful suggestions. We updated the paper accordingly, and summarize our changes and responses here. We hope that they address the reviewer's concerns; however, if there are specific areas where we can further improve, please don't hesitate to let us know.\n\n*> The related work section is focusing almost solely on the clip model,and fails to provide an accurate review of pre-existing related literature. Authors should review pre-existing metrics, related benchmarks and evaluation strategies.*\n\nWe agree that the paper benefits from discussing those related works in more detail and updated our paper accordingly. Since we had to contend with the space limitations, we opted for discussing much of the related literature in the thematically corresponding sections wherever possible (Section 2 for referenceless metrics and robustness checks, Section 3 for relevant models, Section 4 for evaluation frameworks and annotation guides, and Section 5 for common model-generated errors) and specifically elaborate on pre-existing metrics as well as prior evaluation strategies and benchmarks in a new Appendix section (Appendix I).\n\n*> Section 3 provides descriptions of metrics being analysed in this work, and lacks accurate descriptions to understand the particularities of each method. For example, only encoder architectures are discussed for the Flamingo model, but the actual mechanisms used to compute the metric are not discussed. The strategy used to integrate context information is also poorly explained, and could benefit from providing equations.*\n\nThanks for pointing this out! We\u2019ve added some equations to clarify how context is incorporated, we streamlined the model details provided in Section 3 and added more model details in Appendix H. For the likelihood-based approaches, context integration is simply accomplished by including the context as part of the input text when computing likelihood. There are certainly other ways of incorporating context into these likelihood-based models, but we leave this to future work. We\u2019ve also reproduced the equation from Kreiss et al. (2022) to help clarify the role of context in the tested CLIPScore. \n\n*> Authors mention that their results suggests that referenceless metrics can be successfully used based on observed correlation, however somewhat low correlations are observed (.2-.3), suggesting the opposite.*\n\nWe agree that the previous version of the paper failed to properly contextualize the size of the correlations and we are aiming to address this here. Due to the interannotator variation, the highest correlation that is theoretically achievable on our dataset is actually much lower than 1. Depending on the question, it ranges between 0.58 and 0.66. (We computed this by correlating our dataset with the average rating for each description.) However, this still highly overestimates the realistic upper bound for a correlation. To address this, we created a counterfactual dataset by sampling with replacement from the original dataset and correlated the average ratings of the counterfactual dataset with the original data. This suggests a more realistic human upper bound with our data at between 0.42 and 0.57 depending on the question. These numbers are further supported by prior work correlating different participant populations on description quality tasks. Kreiss et al. (2022) found a correlation of about 0.5 when comparing sighted and BLV user judgments. These prior observations better contextualize the quality of the correlations reported in the paper. We agree that correlations below 0.3 should not be considered strong and we agree with the reviewer that our paper doesn\u2019t rest on the assumption that those correlations are particularly high. In the updated paper, we adjust our language accordingly. However, for post-image ratings on the overall question, 5 out of the 7 metrics achieve a correlation higher than 0.35 and we believe that the estimates of an upper correlational limit suggest that correlations of 0.35 and above are very impressive, given the interannotator variation that is a core characteristic of psychological preference studies. We also added further discussion on the interannotator variation in Appendix H. We integrated the estimated correlation ceiling into Figure 2 in the updated paper to help readers contextualize the size of the correlation.\n\n(We also realized that the y axis breaks in Figure 2 weren\u2019t well chosen to identify the correlation values. We therefore changed y axis breaks in Figure 2 to make the values clearer and added a table with the exact correlation values to the appendix.)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241661551,
                "cdate": 1700241661551,
                "tmdate": 1700241661551,
                "mdate": 1700241661551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wey5tAMzfD",
            "forum": "j0ZvKSNZiP",
            "replyto": "j0ZvKSNZiP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission357/Reviewer_jrgb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission357/Reviewer_jrgb"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a new benchmark to evaluate the alignment of referenceless metrics, like CLIPScore, with human preferences in the context of image description generation. It comprises human ratings and robustness checks, emphasizing the importance of context in image descriptions. The assessment reveals that none of the existing methods fully meet the benchmark, especially in context sensitivity. However, the paper suggests that fine-tuning can enhance metric performance, although ContextRef remains a challenging benchmark due to the complexity of context dependence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "ContextRef represents a significant advancement in the evaluation of referenceless metrics for image description generation. By emphasizing the importance of context, it addresses a critical gap in existing benchmarks.\nThe benchmark includes both human ratings along various quality dimensions and robustness checks. This comprehensive approach allows for a more nuanced understanding of the strengths and weaknesses of referenceless metrics.\nBy focusing on context, ContextRef aligns more closely with real-world scenarios where image descriptions are typically encountered, making the benchmark more relevant and applicable.\nhe paper sheds light on the often-underestimated role of context in image description quality, contributing valuable insights to the field and encouraging future research to consider this aspect more thoroughly."
                },
                "weaknesses": {
                    "value": "This work involves human ratings, which are subjective and can introduce bias. The paper could benefit from a more detailed discussion of how raters were selected and trained to ensure consistency and mitigate potential biases.\n\nOverall, I am not working on this area at all, I would like to check comments from other reviewers for the final rating."
                },
                "questions": {
                    "value": "no."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785157113,
            "cdate": 1698785157113,
            "tmdate": 1699635962672,
            "mdate": 1699635962672,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M9NfO15klj",
                "forum": "j0ZvKSNZiP",
                "replyto": "wey5tAMzfD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful engagement with our paper and for raising the following important point, which we have now addressed in the paper. \n\n*> This work involves human ratings, which are subjective and can introduce bias. The paper could benefit from a more detailed discussion of how raters were selected and trained to ensure consistency and mitigate potential biases.*\n\nTo address this point, we added a subsection with details on the participant population (Appendix A) and we added a detailed limitations section (Appendix I). For convenience, we summarize our response here, as well.\n\nThe goal of the ContextRef framework is to test model-based metric alignment with people\u2019s intuitions for description quality for an accessibility goal. However, as the reviewer rightfully points out, there is interannotator variance, and people\u2019s cultural backgrounds and individual needs necessarily shape their preferences. To get first traction on the core problem itself, we get multiple ratings for each individual description, which reduces the effect of outliers but reflects general preference patterns in the recruited population. This is not to say that outliers aren\u2019t meaningful, and we added a discussion on two core sources of variance.\n\nFirstly, we elaborate on the annotator selection and discuss the implications of it. This includes that recruitment was restricted to US-based adults who are part of the Prolific platform. We agree that this leads to implicit biases in the data that might not generalize to participant populations outside of our recruitment parameters, which is a clear limitation that we now elaborate on in the paper. In the paper, we focus on a pipeline that allows us to get initial traction on the overall problem of using models for image description quality assessment, but this doesn\u2019t yet allow insights on important perspectives such as cultural representation and equity promoted by these models, which need to complement our framework. \n\nSecondly and in addition to cultural variation, there is individual variation, which is an important component of current image description research. One such example is description length. A variety of papers have found conflicting evidence on whether shorter or longer descriptions are generally preferred (Slatin & Rush, 2003; Petrie et al., 2005; Rodr\u00edguez V\u00e1zquez et al., 2014). We therefore chose to set up this study based on a goal-oriented framing (consider the purpose and choose your responses accordingly) as opposed to explicit training and instructions. However, we encourage work that builds on ContextRef to adjust the experimental design according to potentially distinct downstream needs. To incorporate the individual uncertainty into our framework, we correlate metric predictions to individual rating distributions instead of the averaged ratings for each description. \n\nFinally, we would like to emphasize that we chose this experimental method since it was established by prior work with sighted as well as Blind and Low-Vision (BLV) participants, which helps ground our findings in the existing literature.\n\nWe hope that this addresses the reviewer\u2019s concerns but if there are any particular points, we can expand on, please let us know.\n\n\u2013\n\nJohn M Slatin and Sharron Rush. Maximum accessibility: Making your web site more usable for everyone. Addison-Wesley Professional, 2003. \n\nHelen Petrie, Chandra Harrison, and Sundeep Dev. Describing images on the web: a survey of current practice and prospects for the future. Proceedings of Human Computer Interaction International (HCII), 71(2), 2005.\n\nSilvia Rodr\u00edguez V\u00e1zquez, A Bolfing, and P Bouillon. Applying accessibility-oriented controlled language (cl) rules to improve appropriateness of text alternatives for images: an exploratory study. Proceedings of LREC 2014, 2014."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241561769,
                "cdate": 1700241561769,
                "tmdate": 1700241561769,
                "mdate": 1700241561769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]