[
    {
        "title": "Towards Out-of-federation Generalization in Federated Learning"
    },
    {
        "review": {
            "id": "GOnS9xnwxP",
            "forum": "70PPJo3DwI",
            "replyto": "70PPJo3DwI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4505/Reviewer_RF1L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4505/Reviewer_RF1L"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of out-of-federation generalization in the context of federated learning. The goal is to train a model able to generalize well to unseen clients during training. In order to achieve this goal, the paper proposes Topology-aware Federated Learning (TFL), a framework combining two ideas: robust/agnostic federated learning (Deng et al., 2020; Mohri et al., 2019), and federated multi-task learning (Smith et al., 2017; Vanhaesebrouck et al., 2017). \n\nTFL considers an optimization problem over the model parameters and importance of each client, as well as the client topology capturing the clients' relationship. In order to solve this optimization problem, TFL acts iteratively: at each iteration, the  model parameters and importance of each client are optimized for a fixed topology, then the topology is updated for fixed model parameters and importance vector.\n\nThe paper curates two out-of-federation benchmarks using real-world healthcare data, and empirically evaluates the performance of TFL on these and standard benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper curates two out-of-federation benchmarks using real-world healthcare data. Both datasets could be beneficial in future work. \n- The numerical experiments are extensive and covers many aspects of the proposed learning framework."
                },
                "weaknesses": {
                    "value": "- The technical novelty of the paper is limited. The paper simply combines the ideas from  robust/agnostic federated learning (Deng et al., 2020; Mohri et al., 2019), and federated multi-task learning (Smith et al., 2017; Vanhaesebrouck et al., 2017).\n- The notation and the theoretical results are not rigorous: \n    - The definition of $\\Theta$ is not consistent when moving from (2) to (3). In (2), we optimize the parameters of one global model, while in (3), we optimizer the individual parameters of each client. \n    - The function $F$ as defined in (6) ought to be iteration-dependent since $\\mathbf{p}$ has the potential to vary from one iteration to another.\n    - The function $F$ is defined in two different manners in (3) and (6). The function $F$ as defined in (3) should depend on $W$, while the function $F$ defined in (6) should depend on $\\mathbf{p}$.\n    - In light of the previous point, it is unclear what is the statement of Theorem 1. \n    - I am uncertain about how to interpret Theorem 2.\n- Other minor issues: \n    - In the abstract and introduction, the paper conveys an initial emphasis on healthcare data. However, I believe it would be more effective if the paper avoids an exclusive focus on healthcare data, considering that the proposed approach holds broader applicability. Instead, I recommend the authors highlight healthcare as one potential use case rather than positioning it as the sole focus of the paper.\n    - In Figure 2, it is unclear why FedProx need more communication in comparison with FedAvg. \n    - In the opening of Section 2, the paper asserts that the empirical risk is equal to the population risk, a statement generally incorrect. I believe the authors intended to convey that the population risk is approximately equal to the empirical risk."
                },
                "questions": {
                    "value": "I am of the opinion that the current theoretical results are not entirely accurate. Should the authors fail to refute my assertion, I recommend considering the removal of these theoretical results from the paper. However, if the authors are able to either demonstrate the correctness or agree to exclude the theoretical results, I am open to revising my evaluation positively."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Reviewer_RF1L"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770068741,
            "cdate": 1698770068741,
            "tmdate": 1700640402037,
            "mdate": 1700640402037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AYtyzrUUKq",
                "forum": "70PPJo3DwI",
                "replyto": "GOnS9xnwxP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RF1L"
                    },
                    "comment": {
                        "value": "Q1. **Clarification on technical novelty**\n> The technical novelty of the paper is limited. The paper simply combines the ideas from robust/agnostic federated learning (Deng et al., 2020; Mohri et al., 2019), and federated multi-task learning (Smith et al., 2017; Vanhaesebrouck et al., 2017).\n\nOur method is NOT a simple combination of existing ideas; it is significantly different in tackling the Out-Of-Federation (OOF) generalization problem. **First**, Directly applying robust/agnostic FL to tackle OOF generalization may suffer from the over-pessimism problem (GroupDRO, Sagawa et al., 2019; Data geometry, Liu et al., 2022), leading to poor OOF-resiliency (as discussed in Sec. 2). To this end, we propose to minimize the model risks over not only worst-case but also the influential clients, thus building a more balanced model with better OOF robustness (Verified via extensive empirical results). **Second**, how to identify influential clients for the OOF scenario is largely underexplored. Federated multi-task learning does not have a discussion on how to identify influential clients. Our method innovatively employs client centrality as a measure of influence. And we have developed a technique to seamlessly integrate client\u2019s influential information into the robust optimization process.\n\nQ2. **On notation and the theoretical results**\n\nWe have refined the notations in our paper to improve clarity and consistency. The following are the key changes we have implemented: (i) In equation (3), we introduced a dependency on W to function F. Additionally, we adjusted the second term to highlight its role in measuring the similarity between client k and client j. How to calculate this term is deferred to Equation (4). All modifications ensure that equation (3) is consistent with equation (2). (ii) We recognized the necessity for equation (6) to be dependent on p. Consequently, p has been incorporated into function F.\n\nRegarding the theoretical results, we acknowledge that more work is needed to refine them. We will remove it from the appendix. However, we are committed to continuing our exploration of the theoretical analysis to enhance its clarity.\n\nQ3. **Other minor issues**\n> In the abstract and introduction, the paper conveys an initial emphasis on healthcare data. However, I believe it would be more effective if the paper avoids an exclusive focus on healthcare data, considering that the proposed approach holds broader applicability. Instead, I recommend the authors highlight healthcare as one potential use case rather than positioning it as the sole focus of the paper.\n\nThe reviewer\u2019s feedback is constructive and can broaden the audience of this paper. We have adjusted the abstract and introduction correspondingly by highlighting not exclusively focusing on healthcare.\n\n> In Figure 2, it is unclear why FedProx needs more communication in comparison with FedAvg.\n\nFigure 2 shows the wall-clock time and the optimal performance of each method. It's important to note that wall-clock time is affected by both communication and computation costs. In the case of FedProx, there is an increased amount of local computation involved. This additional computation contributes to a higher wall-clock time for FedProx when compared to FedAvg.\n\n> In the opening of Section 2, the paper asserts that the empirical risk is equal to the population risk, a statement generally incorrect. I believe the authors intended to convey that the population risk is approximately equal to the empirical risk.\n\nThanks for the reviewer\u2019s comments. We have adjusted the sentences accordingly in the paper (highlighted in light blue)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638320689,
                "cdate": 1700638320689,
                "tmdate": 1700638395292,
                "mdate": 1700638395292,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rVsilHlBYC",
                "forum": "70PPJo3DwI",
                "replyto": "AYtyzrUUKq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4505/Reviewer_RF1L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4505/Reviewer_RF1L"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their rebuttal. The provided response aligns with my expectations, prompting me to raise my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640492471,
                "cdate": 1700640492471,
                "tmdate": 1700640492471,
                "mdate": 1700640492471,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AEVz8E39rA",
            "forum": "70PPJo3DwI",
            "replyto": "70PPJo3DwI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4505/Reviewer_L1rQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4505/Reviewer_L1rQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed a new optimization framework for solving out-of-federation (OOF) problems in federated learning. In particular, they proposed to alternatively optimize a client graph topology and minimize the overall weighted loss whose weights closely dependent on the graph topology. In this case, the new optimization framework can leverage the influential clients and also the \u201coutliers\u201d. They conducted comprehensive numerical experiments to compare the proposed framework with several existing baselines on both real-world datasets and some FL benchmark datasets. The proposed framework has marginal improvements over existing baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, the paper is clear and easy to understand. The proposed framework seems novel. The authors conducted many empirical experiments to demonstrate the performance of the proposed framework."
                },
                "weaknesses": {
                    "value": "1. As the authors has mentioned, solving the client topology learning problem requires $O(N^2)$, which does not scale well when $N$ is large. Although the authors provide an alternative solution: clustering based method, no experiments has conducted under such scenarios.\n2. In the existing literature, there are a few papers have discussed how to measure the similarity between two clients\u2019 local distribution, for example, using prototype model. How's the method used in this paper compared with those ones?\n\n> Tan, Y., Long, G., Liu, L., Zhou, T., Lu, Q., Jiang, J., & Zhang, C. (2021). FedProto: Federated Prototype Learning across Heterogeneous Clients. AAAI Conference on Artificial Intelligence.\n\n3. I wonder if the proposed framework still works when there is/are an/some adversarial client(s) presented in the FL system. Sometimes the adversarial behavior may occur due to connectivity issues."
                },
                "questions": {
                    "value": "1. Page 4, line 4, \u201cIn the objective function, the first term follows the same spirit of Equation 3\u2026\u201d, a typo? Is this Equation 2?\n2. Figure 4, do clients use the same training algorithm and same training hyperparameters? If the clients uses different algorithm or training hyperparameters, I doubt their model parameters will be similar even if they have similar local data distribution.\n3. In Equation 4, why using cosine similarity and $\\ell_0$ distance? In figure 4, it seems that other differentiable  distances also follow the same trend as cosine similarity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Reviewer_L1rQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775373276,
            "cdate": 1698775373276,
            "tmdate": 1699636426663,
            "mdate": 1699636426663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QxhCQ8Xtja",
                "forum": "70PPJo3DwI",
                "replyto": "AEVz8E39rA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L1rQ"
                    },
                    "comment": {
                        "value": "Q1. **Experiment on TFL with client clustering**\n> As the authors has mentioned, solving the client topology learning problem requires $O(N^2)$, which does not scale well when \n$N$ is large. Although the authors provide an alternative solution: clustering based method, no experiments has conducted under such scenarios.\n\nWe conducted experiments on the eICU dataset to empirically validate the effectiveness of our clustering-based method. The eICU dataset was selected for its large scale compared to all other evaluated datasets. As shown in the following table, our clustering approach significantly reduces computation costs by 69%, with only a small decrease in OOF performance by 0.77%.\n\n|                   |     ROC-AUC    | Wall-clock time (s) |\n|:-----------------:|:------------:|:---------------:|\n|       FedAvg      | 57.18\u00b10.03 |      120.15     |\n|        TFL        | 58.41\u00b10.06 |      437.61     |\n| TFL w/ Clustering | 57.96\u00b10.18 |      133.08     |\n\nQ2. **Compared with prototype-based method (FedProto)**\n> In the existing literature, there are a few papers have discussed how to measure the similarity between two clients\u2019 local distribution, for example, using prototype model. How's the method used in this paper compared with those ones?\n\nThe prototype-based method offers an alternative approach to assessing client similarity. However, there are two key distinctions between this method and ours. First, in contrast to prototype-based methods that are specifically designed for classification. Our method is more general and can easily handle various tasks (classification, regression, and segmentation; we empirically verified all three tasks). Second, even for classification tasks, the prototype-based method needs to learn additional prototypes for each class. Our method, on the other hand, does not require this extra step, making the training process more efficient. We have revised the related work to have a discussion on the prototype method and cite the mentioned paper.\n\nQ3. **Discussion on adversarial client(s)**\n> I wonder if the proposed framework still works when there is/are an/some adversarial client(s) presented in the FL system. Sometimes the adversarial behavior may occur due to connectivity issues.\n\nWe see the potential of using client topology to tackle adversarial clients. In scenarios where clients, due to system or network failures, disobey the protocols and send arbitrary messages (such as shuffled, sign-flipped, or noised parameters), our client topology learning approach becomes particularly useful. These adversarial behaviors typically result in models that show lower similarity to normal models. By leveraging client topology learning, we can identify these adversarial clients as isolated nodes within the topology.\n\nOnce identified, applying centrality measures to these nodes can effectively lower their importance scores. This approach minimizes their impact on the overall model aggregation process. Therefore, we believe our method possesses a degree of robustness against adversarial scenarios in FL. Improving FL\u2019s adversial robustness is an important and interesting problem, we will leave the exploration of TFL in this direction to future work.\n\nQ4. **Minors**\n> Page 4, line 4, \u201cIn the objective function, the first term follows the same spirit of Equation 3\u2026\u201d, a typo? Is this Equation 2?\n\nYes, this is a typo. We have adjusted it correspondingly in the paper.\n\n> Figure 4, do clients use the same training algorithm and same training hyperparameters?\n\nAll the models are trained using the same training algorithm and hyperparameters. We have adjusted the corresponding sentence to make it clear.\n\n> In Equation 4, why using cosine similarity and $l_0$ distance? In figure 4, it seems that other differentiable distances also follow the same trend as cosine similarity.\n\nIn Equation 4, \"sim\" denotes any similarity measure. $l_0$ norm is used to enforce the sparsity of the client topology."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637396361,
                "cdate": 1700637396361,
                "tmdate": 1700637515325,
                "mdate": 1700637515325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kI41D09F40",
            "forum": "70PPJo3DwI",
            "replyto": "70PPJo3DwI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4505/Reviewer_Q4J6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4505/Reviewer_Q4J6"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the OOF generalization problem in federated learning, i.e., whether a trained global model can generalize to new clients that do not participate in FL training. The author propose a method to construct a client similarity graph, and emphasize those \u201cinfluential\u201d clients in the graph. The algorithm is empirically shown to be effective and outperforms a line of federated DG baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow. The algorithm is clear. \n2. The experiments are extensive and verify the superior performance of the proposed algorithm."
                },
                "weaknesses": {
                    "value": "1. A detailed explanation of the motivation of the algorithm will be beneficial. For example, what is the motivation behind emphasizing influential client? Is it because the OOF clients\u2019 distribution are more likely to be similar to these clients? If so, why is that the case? If not, why up-weighting these clients? \n2. The author claim that solving Equation 4 with $l_0$ can be NP-hard. However, it seems to be wrong. In this objective function, the optimization for each $w_{k, l}$ is purely disentangled, since $\\\\|W\\\\|\\_0 = \\sum_{k, l} 1\\\\{w\\_{k, l} > 0\\\\}$. And the solution is just very similar to the proposed method, if $\\|W\\|_0$ is weighted by epsilon. This does not hurt the soundness of the method."
                },
                "questions": {
                    "value": "1. How the wall-clock time is calculated in Figure 2? Usually, wall-clock time is influence by both computation and communication cost, and their weights depend on the bandwidth, delay, device, \u2026 I believe number of communication rounds or total # bits transmitted could be a better metric for communication efficiency. \n\nMinor: page 4 line 4: Equation 3 -> Equation 2"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4505/Reviewer_Q4J6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813819226,
            "cdate": 1698813819226,
            "tmdate": 1699636426539,
            "mdate": 1699636426539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vywPg9O7et",
                "forum": "70PPJo3DwI",
                "replyto": "kI41D09F40",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Q4J6"
                    },
                    "comment": {
                        "value": "Q1. **More detailed explanation**\n> A detailed explanation of the motivation of the algorithm will be beneficial. For example, what is the motivation behind emphasizing influential clients? Is it because the OOF clients\u2019 distribution is more likely to be similar to these clients? If so, why is that the case? If not, why up-weighting these clients?\n\nWe appreciate the reviewer's suggestion and have revised the second paragraph on page 2 to provide a more detailed explanation for emphasizing influential clients. Our rationale is based on the idea that influential clients are the most representative within the federation. Therefore, they are more likely to be similar to unseen OOF clients. Then we use a concrete example from healthcare to support our claim. \n\nQ2. **The sparsity term in Equation 4**\n\nThanks for pointing out. We have adjusted the sentences accordingly in the paper (highlighted in light blue).\n\nQ3. **On wall-clock time**\n> How the wall-clock time is calculated in Figure 2? Usually, wall-clock time is influence by both computation and communication cost, and their weights depend on the bandwidth, delay, device, \u2026 I believe number of communication rounds or total # bits transmitted could be a better metric for communication efficiency.\n\nThe wall-clock time is calculated by running each method individually on the same machine until optimal performance is achieved. We have different opinions on choosing communication round over wall-clock time as the metric. The scalability of FL can also be affected by the computation complexity. So, wall-clock time could be a holistic metric to assess scalability. We have adjusted the corresponding to make this point clear. Additionally, in the experimental section (Figure 7), we provide a comparison of the methods' efficiency in terms of communication rounds. The following table shows the optimal performance alongside the total number of communicated parameters. Our method achieves the best performance with the fewest total communicated parameters.\n\n|            | ROC-AUC | Total # of comm. params |\n|:----------:|:-----:|:-----------------------:|\n|   FedAvg   | 57.18 |          15.66M         |\n|   FedProx  | 57.21 |          15.66M         |\n|    DRFA    | 57.20 |          10.44M         |\n|    FedSR   | 57.25 |          20.88M         |\n| TFL (Ours) | 58.41 |          10.44M         |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636393135,
                "cdate": 1700636393135,
                "tmdate": 1700638639360,
                "mdate": 1700638639360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "83bTIHPlV5",
                "forum": "70PPJo3DwI",
                "replyto": "ny4SpHx4id",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4505/Reviewer_Q4J6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4505/Reviewer_Q4J6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "Thanks for your rebuttal. \n\nRegarding the wall-clock time, I still believe running each method on single machine and collecting the wall-clock time may not be an appropriate way to measure efficiency, especially given the facts that (1) real FL systems involves multiple edge devices, even for cross-silo FL, and more importantly (2) in real FL systems, it is the **communication cost** that dominate. In the FedAvg paper, it is claimed that \"in federated optimization communication costs dominate\". However, recording the wall-clock time on single machine mainly just reflect the computational cost, not the communication cost. Please correct me if you are actually running the experiments with multiple machines. \n\nHowever, I appreciate the additional table regarding the total # of communication parameters. Could you please provide insights on why your TFL requires less parameters to communicate? Thanks!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685602619,
                "cdate": 1700685602619,
                "tmdate": 1700685602619,
                "mdate": 1700685602619,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lnjNigYAE4",
            "forum": "70PPJo3DwI",
            "replyto": "70PPJo3DwI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4505/Reviewer_Nx6V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4505/Reviewer_Nx6V"
            ],
            "content": {
                "summary": {
                    "value": "This work tackles the problem of out-of-generalization in FL where the trained model from conventional FL performs poorly for clients outside of the current federation with different distributions. The work proposes to leverage client topology where the relationships across the clients are learned with a weight matrix as we do for graphs. The relationships are learned with pair-wise similarity with only the last few layers of the clients' models. The authors claim that this is communication efficient. The authors include experimental results on a variety of datasets and compare the performance with baselines such as FedAvg, FedProx, DRFA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The work investigates a relevant problem in FL where clients with local data coming from data distributions different from which the global model is trained on suffer from bad performance.\n\n- The work builds upon previous work on graph centrality to propose client centrality and topology for improving previous DRFL methods.\n\n- The work provides experimental validation on extensive number of datasets and different baselines."
                },
                "weaknesses": {
                    "value": "- A main concern I have is regarding the part where we have to learn the client topology through the similarity measures in eq. (3). This requires training over all of the clients' models (even if it is some of the last layers) which can incur large computation overhead for cross-device FL scenarios where the number of clients can easily range to millions of clients. While the authors argue that models can be freely shared among clients and this can address privacy issues, I am unsure why this can ensure privacy. I think it will do the opposite. \n\n- Another concern I have is regarding eq.(5) where the authors try to figure out the influential clients that represent the distribution of out-of-federation clients, along with the worst distribution clients. Wouldn't this lead to potential bias to the distribution of the influential clients? Let us assume there is a setting where there are mainly three groups of clients with different distributions within the out-of-federation group. Wouldn't this lead to the algorithm biasing the model towards one of the distribution and not performing well for the other two groups?\n\n- Lastly, regarding the experimental results, the authors argue that the method is communication efficient since the achieved targeted performance occurs in an earlier communication round for the proposed method compared to other methods. However, I wonder if this is actually the case for the actually communicated number of parameters (for example in Figure7)?\n\nOverall due to these concerns I am leaning towards rejection, but I look forward to the discussions with other reviewers and authors."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986083790,
            "cdate": 1698986083790,
            "tmdate": 1699636426433,
            "mdate": 1699636426433,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m65E03sHGQ",
                "forum": "70PPJo3DwI",
                "replyto": "lnjNigYAE4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nx6V (1/2)"
                    },
                    "comment": {
                        "value": "Q1. **Computation overhead**\n> A main concern I have is regarding the part where we have to learn the client topology through the similarity measures in eq. (3). This requires training over all of the clients' models (even if it is some of the last layers) which can incur large computation overhead for cross-device FL scenarios where the number of clients can easily range to millions of clients.\n\nWe thank the reviewer for pointing out the concern we already discussed in the method section (Page 5 Discussion). In our discussion, we mention that the computation costs for cross-device FL can be significantly reduced via client clustering. By partitioning the clients into clusters, the total number of \u201cclients\u201d is reduced, allowing for cluster-level client topology learning with reduced computation costs. \n\nWe conducted experiments on the eICU dataset to empirically validate the effectiveness of our clustering-based method. The eICU dataset was selected for its large scale (72 clients) compared to all other evaluated datasets. As shown in the following table, our clustering approach significantly reduces computation costs by 69%, with only a small decrease in OOF performance by 0.77%. For more details please refer to the supplementary Section F.\n\n|                   |     ROC-AUC    | Wall-clock time (s) |\n|:-----------------:|:------------:|:---------------:|\n|       FedAvg      | 57.18\u00b10.03 |      120.15     |\n|        TFL        | 58.41\u00b10.06 |      437.61     |\n| TFL w/ Clustering | 57.96\u00b10.18 |      133.08     |\n\nQ2. **Privacy issue**\n> While the authors argue that models can be freely shared among clients and this can address privacy issues, I am unsure why this can ensure privacy. I think it will do the opposite.\n\n **Why this can ensure privacy**: As discussed in Sec. 3.1, traditional topology learning methods generally require access to the data, violating the privacy constraint. We follow the same spirit of FL to learn client topology from model parameters to protect data privacy. \n\n**Information leakage by model sharing**: Sharing model parameters is common practice in FL (Tian et al., 2020; McMahan et al., 2017). Our primary goal is to ensure basic privacy protection in client topology learning. Further discussions on (unintended) information leakage of sharing models (a fundamental problem for FL (Zhu et al., 2019) are out of the scope of this paper. To prevent any misunderstandings, we have revised the relevant sentences to ensure better clarity.\n\nQ3. **Clarification on eq.(5)**\n> Another concern I have is regarding eq.(5) where the authors try to figure out the influential clients that represent the distribution of out-of-federation clients, along with the worst distribution clients. Wouldn't this lead to potential bias to the distribution of the influential clients? Let us assume there is a setting where there are mainly three groups of clients with different distributions within the out-of-federation group. Wouldn't this lead to the algorithm biasing the model towards one of the distribution and not performing well for the other two groups?\n\nThe reviewer\u2019s understanding of eq.(5) seems incorrect. We would like to clarify a couple of points to address your concerns: **First**, Influential clients are identified prior to the optimization process of eq. (5), not during it. This means we first establish who the influential clients are using the learned client topology. Once identified, eq.(5) leverages the client's influence as prior knowledge to guide the model optimization. This process is iterative \u2013 the models are updated based on eq. (5), and then the influential clients are re-evaluated based on these updated models. **Second**, eq.(5) will not only prioritize influential clients. Instead, it will minimize risks over both worst-case and influential clients. By choosing proper \\tau, eq.(5) will build balanced models by striking a good tradeoff between worst-case and influential clients. Thus, it will not lead to models biased toward influential clients. In Sec. 3.2, we provided a detailed justification of our design choice from the perspective of the uncertain set (Hu et al., 2018; Frogner et al., 2021)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635459840,
                "cdate": 1700635459840,
                "tmdate": 1700635703662,
                "mdate": 1700635703662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V6JJy4gDzA",
                "forum": "70PPJo3DwI",
                "replyto": "lnjNigYAE4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4505/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Nx6V (2/2)"
                    },
                    "comment": {
                        "value": "Q4. **On communication efficiency**\n> Lastly, regarding the experimental results, the authors argue that the method is communication efficient since the achieved targeted performance occurs in an earlier communication round for the proposed method compared to other methods. However, I wonder if this is actually the case for the actually communicated number of parameters (for example in Figure7)?\n\nIn terms of total communicated parameters, our method is also communication efficient. The table below details the optimal performance alongside the total number of communicated parameters. Our method achieves the best performance with the fewest total communicated parameters.\n\n|            | ROC-AUC | Total # of comm. params |\n|:----------:|:-----:|:-----------------------:|\n|   FedAvg   | 57.18 |          15.66M         |\n|   FedProx  | 57.21 |          15.66M         |\n|    DRFA    | 57.20 |          10.44M         |\n|    FedSR   | 57.25 |          20.88M         |\n| TFL (Ours) | 58.41 |          10.44M         |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635605461,
                "cdate": 1700635605461,
                "tmdate": 1700635725574,
                "mdate": 1700635725574,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]