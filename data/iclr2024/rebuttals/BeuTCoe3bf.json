[
    {
        "title": "Subgraph-To-Node Translation for Efficient Representation Learning of Subgraphs"
    },
    {
        "review": {
            "id": "03i94im0u1",
            "forum": "BeuTCoe3bf",
            "replyto": "BeuTCoe3bf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2557/Reviewer_1NPi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2557/Reviewer_1NPi"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at the problem of supervised subgraph classification. To handle the scalability issues with the existing models, the authors propose the Subgraph-To-Node (S2N) translation, an efficient data structuring mechanism for manipulating subgraphs prior to model design. They also explore graph coarsening techniques in this context in a data-scarce setting. The authors prove that the S2N node representations approximate the subgraph representations of the original global graph. Their experiments are designed to show that S2N substantially reduces memory and time costs with little degradation in performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The exposition is quite clear. The proposed solutions are quite straighforward and easy to follow. The experimental protocol is more or less quite detailed and sufficiently tests the proposed models. The idea of using graph coarsening for subgraph classification is novel (to the best of my knowledge) and deserves further study."
                },
                "weaknesses": {
                    "value": "1. There are not many real-world datasets for the supervised subgraph classification. The authors should definitely consider synthetic datasets, e.g. those considered by Alsenter et. al. (2020). It is not clear why the authors have not considered such synthetic datasets.\n\n2. The authors do not clarify if the considered datasets are adequately large so that the GPU speed/memory really forms a bottleneck for learning. \n\n3. I am not sure about handpicking the Configuration Model (CM) as a justification for low computational complexity of S2N. Why has this model been picked: One could definitely study some other random graph models and ask the same questions?\n\n4. When you coarsen a graph, how is the structure of the original subgraphs preserved? It is not clear how to use the coarsened graph to say something about the subgraphs in the original graph. \n\nOverall, the paper is not substantial enough for significant research impact."
                },
                "questions": {
                    "value": "Please comment on the enumerated points in Weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814021770,
            "cdate": 1698814021770,
            "tmdate": 1699636192293,
            "mdate": 1699636192293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XnpZmKxWIX",
                "forum": "BeuTCoe3bf",
                "replyto": "03i94im0u1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to `1NPi` (1)"
                    },
                    "comment": {
                        "value": "Thank you for your review and constructive feedback. We provide answers to your questions below.\n\n## More Experiments on Synthetic Datasets\n> There are not many real-world datasets for the supervised subgraph classification. The authors should definitely consider synthetic datasets, e.g. those considered by Alsenter et. al. (2020). It is not clear why the authors have not considered such synthetic datasets.\n\nThank you for your suggestion. We agree with your comment about synthetic datasets and have conducted experiments on them. The table below summarizes the performance of GCNII+S2N on synthetic datasets.\n\n|     | Density    | Cut-Ratio  | Coreness   | Component   |\n|----------------------|------------|------------|------------|-------------|\n| GLASS        | 93.0 \u00b1 0.9 | 93.5 \u00b1 0.6 | 84.0 \u00b1 0.9 | 100.0 \u00b1 0.0 |\n| SubGNN               | 91.9 \u00b1 1.6 | 62.9 \u00b1 3.9 | 65.9 \u00b1 9.2 | 95.8 \u00b1 9.8  |\n| Sub2Vec              | 45.9 \u00b1 1.2 | 35.4 \u00b1 1.4 | 36.0 \u00b1 1.9 | 65.7 \u00b1 1.7  |\n| GCNII / S2N+0        | 67.2 \u00b1 2.4 | 56.0 \u00b1 0.0 | 57.0 \u00b1 4.9 | 100.0 \u00b1 0.0 |\n| GCNII / S2N+A        | 93.2 \u00b1 2.6 | 56.0 \u00b1 0.0 | 85.7 \u00b1 5.8 | 100.0 \u00b1 0.0 |\n| GCNII / S2N+0 + RWPE | 74.8 \u00b1 3.6 | 85.2 \u00b1 5.1 | 56.1 \u00b1 3.0 | 100.0 \u00b1 0.0 |\n| GCNII / S2N+0 + RWPE | 93.6 \u00b1 2.0 | 89.2 \u00b1 2.6 | 77.4 \u00b1 9.1 | 100.0 \u00b1 0.0 |\n\nS2N+A outperforms the state-of-the-art (GLASS) on Density, Coreness, and Component datasets. S2N+0 shows the same performance as GLASS only in Component. For each dataset, the attributes that affect the subgraph properties (i.e., labels of synthetic datasets) are known (as illustrated in the table below) [1]. Because S2N compresses the global graph structure, it is challenging to learn Cut-Ratio, which requires exact information about the border structure (i.e., global structure). Learning Density and Coreness of subgraphs require their internal structures. Therefore, S2N+0, which does not maintain internal structure, relatively underperforms baselines.\n\n| Density | Cut-Ratio  | Coreness  | Component     |\n|--------------|---------|--------------------|-----------------|\n| Internal structure | Border structure | Internal structure, border structure and position | Internal and external position |\n\nTo address this issue, we can add structural encoding to the input features, particularly Random Walk Positional Encoding (RWPE) [2]. The efficiency of S2N is maintained since the RWPE is computed once before training and only requires the memory complexity of $O(N)$. RWPE allows S2N to significantly improve the performance of Density and CutRatio, but not of Coreness. We interpret that RWPE for subgraphs can encode internal and border structures well but cannot encode border positions. We leave the development of structural encoding for S2N as future work.\n\nAbove all, tasks that depend only on specific attributes barely exist in the real world. We can find evidence from the high performance of S2N on real-world datasets. Our proposed methods provide GNN practitioners with an efficient solution for real-world subgraph-level applications. It is analogous to lossy compression of images (e.g., JPEG) to serve them efficiency-sensitive applications.\n\nWe will add all results and discussion to the revised paper.\n\n[1] Alsentzer, Emily, et al. \"Subgraph neural networks.\" Advances in Neural Information Processing Systems 33 (2020): 8017-8029.\n\n[2] Dwivedi, Vijay Prakash, et al. \"Graph Neural Networks with Learnable Structural and Positional Representations.\" International Conference on Learning Representations. 2022.\n\n## Clarification on Computational & Memory Bottleneck\n> The authors do not clarify if the considered datasets are adequately large so that the GPU speed/memory really forms a bottleneck for learning.\n\nFor the datasets we used, the number of nodes is 17K, 15K, 57K, and the number of edges is 317K, 3.2M, 4.6M (Table 4 in the Appendix). In the table below, we introduce some datasets referred to as large-scale in our GNN research community. The number of edges compared to these datasets is at a similar level; thus, similar scalability is required to model the datasets in this paper using GNNs. Particularly, when using attention models (e.g., GATv2), we cannot load the largest dataset using baselines into a single GPU of 11G VRAM.\n\n| Name          | # Nodes | # Edges | Source |\n|---------------|---------|---------|--------|\n| Penn94        | 42K     | 1.4M    | [1]    |\n| arXiv-year    | 169K    | 1.2M    | [1]    |\n| twitch-gamers | 168K    | 6.8M    | [1]    |\n| ogbl-ddi      | 4K      | 1.3M    | [2]    |\n| ogbl-biokg    | 93K     | 5.4M    | [2]    |\n| ogbn-arxiv    | 169K    | 1.2M    | [2]    |\n\n[1] Lim, Derek, et al. \"Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods.\" Advances in Neural Information Processing Systems 34 (2021): 20887-20902.\n\n[2] Hu, Weihua, et al. \"Open graph benchmark: Datasets for machine learning on graphs.\" Advances in neural information processing systems 33 (2020): 22118-22133."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218913853,
                "cdate": 1700218913853,
                "tmdate": 1700547495437,
                "mdate": 1700547495437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UgRYdSVZNM",
                "forum": "BeuTCoe3bf",
                "replyto": "03i94im0u1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to `1NPi` (2)"
                    },
                    "comment": {
                        "value": "## Justification for the Choice of the Random Graph Model\n> I am not sure about handpicking the Configuration Model (CM) as a justification for low computational complexity of S2N. Why has this model been picked: One could definitely study some other random graph models and ask the same questions?\n\nThe complexity of S2N strongly depends on the distribution of translated edge weights. Thus, we need a random graph model that can analytically calculate the distribution of edge weights (i.e., the number of shared edges in two subgraphs). When using the configuration model (CM), the distribution of S2N's edge weights can be derived from the degree distribution of the global graph. This is possible because CM calculates the probability of edge existence through the degrees of a pair of nodes. Note that CM is frequently used in analytically calculating numerous network measures [1].\n\nWe also emphasize that CM only requires a degree sequence or a distribution. That means CM can also generate graphs generated by other random graph models. For example, when the degree distribution is Poisson distribution, CM generates graphs close to the Erd\u0151s\u2013R\u00e9nyi model. CM can also generate degree distributions with other distributions, for example, power-law distributions. See [2] for more details.\n\nWe will add this justification to the revised paper.\n\n[1] Barab\u00e1si, Albert-L\u00e1szl\u00f3. \"Network science.\" Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 371.1987 (2013): 20120375.\n\n[2] Newman, Mark. Networks. Oxford university press, 2018.\n\n## Answers to Questions\n> When you coarsen a graph, how is the structure of the original subgraphs preserved? It is not clear how to use the coarsened graph to say something about the subgraphs in the original graph.\n\nIt seems that graph coarsening in CoS2N has not been written clearly and in detail. The details of this are as follows, and we will include them in the revised paper.\n\n1. Apply the graph coarsening method to the global graph. The output is a partition of nodes in the global graph. In other words, graph coarsening assigns one super-node to each node in the global graph.\n2. Construct induced subgraphs of the global graph per a set of nodes in the same super-node. We call them virtual subgraphs.\n3. Merge a set of virtual subgraphs and an existing set of real subgraphs. Until this step, the structures of the original subgraph are preserved as is.\n4. Perform S2N translation of this union set of subgraphs."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218965116,
                "cdate": 1700218965116,
                "tmdate": 1700325233493,
                "mdate": 1700325233493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gwBb6jPguI",
                "forum": "BeuTCoe3bf",
                "replyto": "UgRYdSVZNM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Reviewer_1NPi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Reviewer_1NPi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423501518,
                "cdate": 1700423501518,
                "tmdate": 1700423501518,
                "mdate": 1700423501518,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sq6jXX2jVm",
            "forum": "BeuTCoe3bf",
            "replyto": "BeuTCoe3bf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2557/Reviewer_tdbM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2557/Reviewer_tdbM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Subgraph-to-node (S2N), an efficient data structure for subgraph-level prediction. The nodes of the new structure correspond to the subgraph and the edges are the relations among the subgraphs. The results shows both high efficiency and performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method shows high performance compared to the existing data structures and other baselines even with a simple and straightforward method.\n2. The clear figures help in understanding and the presentation of the proposed work."
                },
                "weaknesses": {
                    "value": "1. The graph coarsening process lacks novelty. It is quite straightforward and well-known to treat the subgraphs into a single node and link the nodes that share the nodes in the original graphs.\n2. Can this approach distinguish the two subgraphs that share the same number of nodes, i.e., is the proposed structure reconstructable? For instance, what if the red subgraph is connected to the right side of the blue subgraph in Figure 1? It may generate the same subgraphs and same number of shared nodes.\n3. Lack of details for the selection step of the subgraphs to be mapped into new nodes. How do you select the subgraphs and how do you prove that the selected subgraph is the optimal choice?\n4. Lack of backbone architectures, which are limited to GCN-based. What about other GNN backbone architectures such as GIN? Is the proposed method restricted only to GCN as proved in Section 4.2?"
                },
                "questions": {
                    "value": "1. What is the difference between the existing works and the proposed works on super-nodes? I cannot clearly understand what the node boundaries in super-nodes are unknown is in Section 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2557/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2557/Reviewer_tdbM",
                        "ICLR.cc/2024/Conference/Submission2557/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833451741,
            "cdate": 1698833451741,
            "tmdate": 1700466493772,
            "mdate": 1700466493772,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "twGXIyFzmi",
                "forum": "BeuTCoe3bf",
                "replyto": "sq6jXX2jVm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to `tdbM`"
                    },
                    "comment": {
                        "value": "Thank you for your review and constructive feedback. We address your comment on the lack of backbone architectures in the [general response](https://openreview.net/forum?id=BeuTCoe3bf&noteId=6ld2E20A6l). Here, we provide answers to your individual questions below.\n\n## \u200b\u200bLack of Novelty\n> The graph coarsening process lacks novelty. It is quite straightforward and well-known to treat the subgraphs into a single node and link the nodes that share the nodes in the original graphs.\n\nWe argue that the value of simplicity and the improved performance and efficiency over existing methods reinforce the genuine novelty of our contributions. We would like to offer the following points to counter the critique.\n- **Value of Simplicity**: The simplicity of our proposed approach is one of its key strengths and contributions. By distilling the problem to its core and focusing on a simple yet effective idea, we can provide an efficient and effective solution for subgraph-level tasks.\n- **Novel Perspective and Fundamental Question**: We emphasize that it is not well-known to treat the subgraphs into a single node and link the nodes that share the nodes for subgraph representation learning. How to design an efficient data structure for subgraphs is a novel perspective that not only complements but advances the field's understanding and approaches to the problem.\n- **Outperforming State-of-the-Art**: We note that our proposed S2N translation is on par with or better than the state-of-the-art. Our approach is fast and lightweight while maintaining performance levels, and should not be dismissed as straightforward.\n\nLast but not least, we would like to quote the other reviewer `1NPi`'s evaluation of the novelty: *\"The idea of using graph coarsening for subgraph classification is novel (to the best of my knowledge) and deserves further study.\"*\n\nWe think our strengths have not clearly revealed in the introduction, and based on the above bullets, we will revise them so that they are clearly visible. We would appreciate you reconsidering the novelty of our method.\n\n## Answers to Questions\n> Can this approach distinguish the two subgraphs that share the same number of nodes, i.e., is the proposed structure reconstructable? For instance, what if the red subgraph is connected to the right side of the blue subgraph in Figure 1? It may generate the same subgraphs and same number of shared nodes.\n\nGenerally, S2N can distinguish between two subgraphs with the same number of nodes. This is because S2N's node will have different neighborhoods in the S2N graph depending on neighbor subgraphs in the global graph. The case where S2N cannot distinguish between two subgraphs of the same size is when (1) the global structure around subgraphs is completely identical and (2) features of subgraphs are identical. However, it is difficult to exist due to real-world graphs' rich features and complex dynamics.\n\n> Lack of details for the selection step of the subgraphs to be mapped into new nodes. How do you select the subgraphs and how do you prove that the selected subgraph is the optimal choice?\n\nIn this study, all given subgraphs are mapped to new nodes. In detail, all training subgraphs are translated into nodes in the training stage, and all training and evaluation subgraphs are translated into nodes in the evaluation stage (See Section 5). How to select subgraphs that make up the optimal S2N graph is an interesting research topic but is not the scope of this paper. We leave this as a future study and will mention it in the revised paper.\n\n> What is the difference between the existing works and the proposed works on super-nodes? I cannot clearly understand what the node boundaries in super-nodes are unknown is in Section 2.\n\nA node boundary is the boundary between a subgraph (or super-nodes) and the global graph. When we say node boundary is unknown, the super-node is not given to existing graph coarsening methods. These methods aim to find a set of super-nodes that partitions a given graph according to specified constraints. Unlike these, we view given subgraphs as super-nodes and explore whether performance and efficiency differ in subgraph-level downstream tasks when the super-nodes are mapped to nodes. That is, the purpose and primary conditions of the research are different. We will revise this part clearly by choosing straightforward terminologies."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218806801,
                "cdate": 1700218806801,
                "tmdate": 1700218806801,
                "mdate": 1700218806801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5WusbwOrFL",
                "forum": "BeuTCoe3bf",
                "replyto": "twGXIyFzmi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Reviewer_tdbM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Reviewer_tdbM"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the kind response and I slightly revised the score. However, I still have some remaining concerns.\n\nI first don\u2019t agree that it is not well-known to treat the subgraphs into a single node and link the nodes that share the nodes. For instance, a popular graph generative model, JT-VAE, proposes to decompose the graph into junction tree, where a single node corresponds to the motif and edges link the nodes that share the nodes. It might not be well-known for subgraph representation learning, the concept of mapping subgraphs to a single node and linking the nodes is quite well-known for other graph neural network area such as graph generative models. However, I now understand that the idea of using graph coarsening for subgraph representation learning could be novel as the reviewer 1NPi mentioned.\n\nIn addition, the empirical results using GIN and GAT given by the authors lack the details for the results with conventional data structure: separated and connected. Don\u2019t we have to compare the GIN/GAT + S2N results with GIN/GAT + separated / connected, not GLASS and SubGNN to observe the enhancement of S2N data structure?\n\nThank you for the detailed response again.\n\n[1] Jin, W., Barzilay, R., & Jaakkola, T. (2018, July). Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning (pp. 2323-2332). PMLR."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462004703,
                "cdate": 1700462004703,
                "tmdate": 1700462004703,
                "mdate": 1700462004703,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AvrazunAGt",
                "forum": "BeuTCoe3bf",
                "replyto": "1YkPr9r7Xs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Reviewer_tdbM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Reviewer_tdbM"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for the confusion. I revised my score just right before."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466534794,
                "cdate": 1700466534794,
                "tmdate": 1700466534794,
                "mdate": 1700466534794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vXvfVWdZJV",
            "forum": "BeuTCoe3bf",
            "replyto": "BeuTCoe3bf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2557/Reviewer_AUEj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2557/Reviewer_AUEj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes S2N and CoS2N, two new methods for learning the representation of subgraphs where the subgraphs are given as input to the model as well as the original whole graph. The proposed methods are simple and effective, as evidenced by superior results on four real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Both theoretical analysis and experimental support are provided to show the advantages of the proposed methods.\n2. The model design is quite simple yet the results are impressive, both in terms of effectiveness and efficiency."
                },
                "weaknesses": {
                    "value": "1. There lack of ablation study of certain hyperaprameters and design choices. For example, the authors \"use two well-known GNNs\" but it is unclear why alternative choices are not discussed or used. Given the abundance of GNN models nowadays and the fact that GCN and GCN2 are relatively earlier (before 2021), it is unclear if the adoption of more recent GNN models could yield better results. The authors mention one baseline, SubGNN, uses pre-trained embeddings by GIN, yet there is no explanation of why GIN can or cannot be used for the proposed methods.\nWhat is more important, the number of layers is tuned between 1 and 2 layers (Section A.3), and it is unclear how much performance fluctuates with even more or less (0 layers, i.e. no message passing) layers. Similarly, it is unclear if alternative readout methods and graph coarsening methods are experimented with. Adding such additional experiments certainly require more work and resource, but would further help improve the soundness of the paper. \n2. I suggest the authors provide more descriptions of existing methods, esp. SubGNN and GLASS. For example, if and what GNN models are used. There is some detail in Appendix A, but an additional section that focuses on the architectural comparison of all the methods would further enhance the clarity of the paper.\n3. Writing issues, e.g. lack of citation of GIN."
                },
                "questions": {
                    "value": "1. How is the proposed CoS2N related to DiffPool \"Ying, Zhitao, et al. \"Hierarchical graph representation learning with differentiable pooling.\" Advances in neural information processing systems 31 (2018).\"? At a high level, both of them perform pooling and allow further message passing between the pooled clusters/subgraphs. DiffPool adopts a learnable/differentiable way to pool nodes, whereas the proposed method adopts Variation Edges for coarsening. Of course, the task is different, yet I would like to hear from the authors more about the model-architecture-level comparison. This would help readers better see the novelty of the proposed methods with respect to related work designed for different asks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699167986870,
            "cdate": 1699167986870,
            "tmdate": 1699636192087,
            "mdate": 1699636192087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ApIdZEPUQ1",
                "forum": "BeuTCoe3bf",
                "replyto": "vXvfVWdZJV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and constructive feedback. We addressed questions about the lack of GNN models other than GCN and GCNII in the [general response](https://openreview.net/forum?id=BeuTCoe3bf&noteId=6ld2E20A6l). We provide answers to your individual questions below.\n\n## Ablation Studies and Hyperparameter Analysis\n> What is more important, the number of layers is tuned between 1 and 2 layers (Section A.3), and it is unclear how much performance fluctuates with even more or less (0 layers, i.e. no message passing) layers. Similarly, it is unclear if alternative readout methods and graph coarsening methods are experimented with. Adding such additional experiments certainly require more work and resource, but would further help improve the soundness of the paper.\n\nThank you for your suggestion. Now, we are planning to conduct ablation studies on important hyperparameters, including the number of layers, and readouts. Due to the computational resource constraint, we have yet to finish the experiments, but we will include the results as soon as they come out.\n\n## Detailed Descriptions of Architectural Differences\n> I suggest the authors provide more descriptions of existing methods, esp. SubGNN and GLASS. For example, if and what GNN models are used. There is some detail in Appendix A, but an additional section that focuses on the architectural comparison of all the methods would further enhance the clarity of the paper.\n\nWe agree with the reviewer's comment that a detailed architecture description will help with clarity. We describe the architectural differences between SubGNN, GLASS, and S2N below. This will be included in the Appendix.\n\nSubGNN, GLASS, and S2N improve different parts of the machine learning pipeline to solve subgraph-level tasks. SubGNN designs a whole model, GLASS augments input data through a labeling trick, and S2N uses a new data structure.\n\nSubGNN performs message-passing between subgraphs (or patches). Through this, the properties of internal and border structures for three channels (position, neighborhood, and structure) are learned independently. To learn a total of 6 (2 x 3) properties, SubGNN designs patch samplers, patch representation, and similarity (weights of messages) for each property in an ad hoc manner. To learn internal positions, for example, SubGNN patches nodes inside the subgraph uses its representation as a message, and uses distance-based similarity as weights. By the complex model design, SubGNN requires a lot of computational resources for data pre-processing, model training, and inference.\n\nGLASS uses plain GNNs but labels input nodes as to whether they belong to the subgraph (the label of one) or not (the label of zero). Separate node-level message-passing is performed for each label to distinguish the internal and border structures of the subgraph. GLASS's labeling trick is effective, but hard to handle multiple labels from multiple subgraphs in a batch. Although the authors of GLASS propose a max-zero-one trick to address this issue, small batches are still recommended. In addition, using a large global graph requires significant computational and memory resources.\n\nIn comparison, our proposed S2N uses the new data structure that stores and processes subgraphs efficiently. By compressing the global graph, computational and memory resource requirements are reduced. There are no restrictions on batch learning so we can train S2N graphs in the full batch."
                    },
                    "title": {
                        "value": "Response to `AUEj` (1)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218719626,
                "cdate": 1700218719626,
                "tmdate": 1700739671532,
                "mdate": 1700739671532,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cLa9hBBIQS",
                "forum": "BeuTCoe3bf",
                "replyto": "vXvfVWdZJV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Answers to Questions\n> How is the proposed CoS2N related to DiffPool \"Ying, Zhitao, et al. \"Hierarchical graph representation learning with differentiable pooling.\" Advances in neural information processing systems 31 (2018).\"? At a high level, both of them perform pooling and allow further message passing between the pooled clusters/subgraphs. DiffPool adopts a learnable/differentiable way to pool nodes, whereas the proposed method adopts Variation Edges for coarsening. Of course, the task is different, yet I would like to hear from the authors more about the model-architecture-level comparison. This would help readers better see the novelty of the proposed methods with respect to related work designed for different asks.\n\nThanks for the suggestion on the new perspective. We provide a model-level comparison between CoS2N and DiffPool below. This will be added to the Appendix.\n\nDiffPool learns the hierarchy of a graph to obtain graph-level representations. DiffPool softly assigns each node to a cluster during training by optimizing the downstream task loss. To stabilize the soft clustering assignment, the authors of DiffPool employ link prediction loss and entropy regularization loss. The problem is that the assignment matrix must be maintained in GPU memory, which requires quadratic memory complexity regarding the number of nodes. In other words, we cannot apply DiffPool to large graphs such as global graphs in our use cases.\n\nThis study aims to efficiently perform subgraph representation learning by compressing data to be loaded into GPU memory. Memory-intensive graph coarsening, such as DiffPool's soft clustering assignment, should not be used to keep CoS2N efficient. Instead, we can secure the efficiency of CoS2N by performing graph coarsening before training the model, relying only on the structure of the global graph."
                    },
                    "title": {
                        "value": "Response to `AUEj` (2)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218749628,
                "cdate": 1700218749628,
                "tmdate": 1700638436631,
                "mdate": 1700638436631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wrB1Ucq8q1",
                "forum": "BeuTCoe3bf",
                "replyto": "vXvfVWdZJV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2557/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer AUEj"
                    },
                    "comment": {
                        "value": "Dear Reviewer AUEj.\n\nWe kindly remind you that there is one day left until the discussion period ends. We uploaded [the revised paper](https://openreview.net/pdf?id=BeuTCoe3bf) addressing your concerns. If you have any additional questions after reading our revised paper, we are happy to discuss them.\n\nIn summary, our revision includes:\n1. We added experiments using GATv2 (a more recent GNN) and GIN (Table 6 and Appendix A.8).\n2. We cited the GIN paper.\n3. We added ablation studies regarding the number of layers (including no message-passing) and readout methods (Table 7, 8 and Appendix A.9).\n4. We provided more descriptions of existing methods, SubGNN and GLASS (Appendix A.1.1).\n5. We added the model-architecture-level comparison with DiffPool (Appendix A.1.2).\n\nThank you again for your time and effort in reviewing our paper. We are looking forward to your response."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638933656,
                "cdate": 1700638933656,
                "tmdate": 1700638933656,
                "mdate": 1700638933656,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]