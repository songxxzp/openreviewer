[
    {
        "title": "IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning"
    },
    {
        "review": {
            "id": "ZFo2zT0z5K",
            "forum": "qRbkTbe8JT",
            "replyto": "qRbkTbe8JT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7749/Reviewer_siLj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7749/Reviewer_siLj"
            ],
            "content": {
                "summary": {
                    "value": "The paper develops IMEX-Reg as a new method to tackle catastrophic forgetting in continual learning settings.\u00a0 IMEX-Reg\u00a0 is inspired by the nervous system mechanisms and\u00a0combines contrastive representation learning with consistency regularization. It aligns the classifier with CRL representations in the unit hypersphere. Empirical results are offered\u00a0to demonstrate that IMEX-Reg improves the model generalization and leads to SOTA performance\u00a0compared to the baselines.\u00a0IMEX-Reg is also resilient to adversarial data issues and reduces bias towards recent tasks. The approach is also supported by theoretical justifications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. CL is still an active research area and the proposed approach is a new method for this purpose.\n\n2. The paper reads well and can be followed straightforwardly.\n\n3. Section D in the Appendix is informative and offers insights about the weaknesses and future potentials for the proposed research."
                },
                "weaknesses": {
                    "value": "1. Continual learning in the context of the used baselines is a mature field with many existing works. However, the method does provide SOTA results across the board to warrant a contribution that offers a significant performance boost.\n\n2. Theoretical justifications of the paper are not novel and mostly are reiterating previous results. Doing so is OK but does not offer any new theoretical contributions. \n\n3. Some aspects of the algorithm are not studied extensively."
                },
                "questions": {
                    "value": "1. The connection between the proposed approach and the nervous system is very loose and emphasis on this aspect is overstated. What is the reason behind this emphasis without providing much evidence to support it?\n\n2. It is important to study the effect of \\alph, \\beta, and \\lambda on the performance. How the user should tune them? A study should be offered for this purpose. If the performance is sensitive with respect to the values of these hyperparameters, then it is essential to provide a solution for selecting the optimal values.\n\n3. There are other common settings to study CL using CIFAR100, e.g., using 20 tasks each with 5 classes. I think adding experiments for these settings is also helpful to demonstrate how well the method scales when there are more tasks.\n\n4. Having learning curves in CL is common and allows for studying the dynamic of learning. I think providing them in addition to the tables is helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7749/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7749/Reviewer_siLj",
                        "ICLR.cc/2024/Conference/Submission7749/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7749/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698486129360,
            "cdate": 1698486129360,
            "tmdate": 1700992735635,
            "mdate": 1700992735635,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UJQvgazA67",
                "forum": "qRbkTbe8JT",
                "replyto": "ZFo2zT0z5K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7749/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7749/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Reviewer siLj (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for taking the time to provide valuable insights for our paper. We are glad that the reviewer found the paper easy to follow. Below we address the concerns raised by the reviewer to improve the confidence in our paper.\n\n> Continual learning in the context of the used baselines is a mature field with many existing works. However, the method does provide SOTA results across the board to warrant a contribution that offers a significant performance boost\n\nWe are delighted to find that the reviewer recognizes the performance boost achieved by IMEX-REG across all the continual learning scenarios, especially in low buffer regimes. We further conducted several analyses of our method that emphasize on the robustness in real world scenarios (Section 5) and other characteristics such as stability-plasticity tradeoff, model calibration (Appendix C).\n\n> Theoretical justifications of the paper are not novel and mostly are reiterating previous results. Doing so is OK but does not offer any new theoretical contributions.\n\nWe do not claim that the theoretical insights in our paper are novel. Rather these are provided to support our hypothesis that forms the base for our design choices. For example, Conjecture 1 provides insights that CRL and cross-entropy learning share a common hypothesis class that is essential for the proposed novel regularization strategy. This hypothesis is later validated empirically in the Experimental Results (4.2) section. We believe that providing these theoretical justifications strengthen the confidence of the readers in our hypotheses.\n\n> The connection between the proposed approach and the nervous system is very loose and emphasis on this aspect is overstated. What is the reason behind this emphasis without providing much evidence to support it?\n\nWe beg to differ with the reviewer that we overstate the connection between the proposed approach and the learning in humans. However, we do draw inspiration from learning mechanisms in humans as they are the most efficient and robust continual learners out there. Learning in humans is driven by several inductive biases that encourage learning to prioritize solutions with certain properties [1]. Similarly in Deep Learning several algorithmic or architectural choices like convolutions, deep architecture bring in certain inductive biases into the learning objective [1]. We study the domain of continual learning and find that recent rehearsal based approaches suffer from generalization under low buffer regimes. We employ several implicit and explicit regularization strategies to mitigate this problem. CRL, Multi-task Learning and explicit consistency regularization provide inductive biases to learn better representations from limited buffer data to preserve previously learnt information while adapting to novel knowledge. We attribute the significant performance boost of IMEX-REG in several challenging scenarios to these inductive biases.\n\n> Q: It is important to study the effect of \\alph, \\beta, and \\lambda on the performance. How the user should tune them? A study should be offered for this purpose. If the performance is sensitive with respect to the values of these hyperparameters, then it is essential to provide a solution for selecting the optimal values.\n\nWe perform grid search to find the optimum hyperparameter and Table 6 in Appendix shows the hyperparameters used for our experiments.  We agree with the reviewer that a comparative study on hyperparameter tuning would provide more insights into IMEX-Reg\u2019s performance. Table 7 in the revised manuscript provides hyperparameter tuning experiment results. Due to limited computational capacity, we will include an exhaustive study in the final revision.\n\nIn Table 7 in the revised manuscript, we can see that the model is fairly robust to the choice of hyperparameters and still outperforms the next best baseline across all the hyperparameters searched. However the model shows a higher decrease in performance as $\\lambda$ value increases. Higher the $\\lambda$, higher the restriction on the model to preserve old knowledge, thus limiting its ability to learn new information. In such cases, we can look at the task-wise performance of the model or stability-plasticity trade-off (Appendix C.2) to find the optimum balance between preserving learnt knowledge and adapting to novel information."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377214070,
                "cdate": 1700377214070,
                "tmdate": 1700377214070,
                "mdate": 1700377214070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MdKP3Nmv7p",
            "forum": "qRbkTbe8JT",
            "replyto": "qRbkTbe8JT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7749/Reviewer_bb13"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7749/Reviewer_bb13"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an interesting topic, continual learning, which aims to learn a series of tasks without forgetting. In order to the generalization performance of the memory-based methods, this paper introduces to employ contrastive representation learning (CRL) and consistency regularization. The experiment results show that the proposed approach achieve good results in continual learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The main idea seems interesting.\n2. This paper studies an interesting topic."
                },
                "weaknesses": {
                    "value": "1. The notations are hard to follow. For example, x and y should be bold because they are matrixes.\n2. The parameters of the shared model and classifier are not defined.\n3. Eq.1 is not clear to me. What is the actual network for f and g? Why is h not used in Eq.1?\n4. In the text below Eq.2, you said z = h(f(.)). However, z is not defined in Eq.2. The input and output patterns for the models f, g and h are unclear.  \n5. Why introduce the existing Conjecture 1? Does this theory connect with your actual design?\n6. Why the classifier can create the function spaces?\n7. The proposed approach is based on the existing technology, and the overall novelty is small.\n8. The proposed approach still requires the task information, which can not be used in more realistic continual learning settings such as task-free continual learning.\n9. The methodology section is hard to follow. A lot of notations are not defined clearly and the proposed approach is not novel enough."
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7749/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7749/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7749/Reviewer_bb13"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7749/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769880784,
            "cdate": 1698769880784,
            "tmdate": 1699636945793,
            "mdate": 1699636945793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KjRDUtFuUL",
                "forum": "qRbkTbe8JT",
                "replyto": "MdKP3Nmv7p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7749/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7749/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Reviewer bb13 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking time to review our paper. Please find below our response to the questions raised:\n\n> The parameters of the shared model and classifier are not defined.\n\nSection 3 and Figure 1 both describe the parameters associated with shared model and classifier.  Our CL model \u03a6\u03b8 = {f, g, g\u2032, h} consists of a shared backbone f, a linear classifier g, an MLP classifier projection g\u2032, and a projection head h. The classifier g represents all the classes that belong to all the tasks, and the projection head h captures the embeddings of the \u21132-normalized representation.\n\n> Eq.1 is not clear to me. What is the actual network for f and g? Why is h not used in Eq.1?\n\nSection 4.1 provides information on particulars of f and g. We use Resnet-18 as as a backbone f and an MLP classifier as g. In Equation 1, we keep the information generic as any other backbone or classifier can be used in its place. \n\n> In the text below Eq.2, you said z = h(f(.)). However, z is not defined in Eq.2. The input and output patterns for the models f, g and h are unclear.\n\nWe define z = h(f(.)) in the text below Equation 2. z represents any arbitrary 128-dimensional \u21132-normalized projection from CRL head h. \n\n> Why introduce the existing Conjecture 1? Does this theory connect with your actual design?\n\nWe introduce Conjecture 1 to provide more intuition behind our choice of auxiliary task for multi-task learning in a CL setting. Assuming that the tasks in MTL share a common hypothesis class, sharing representations across tasks primarily benefits tasks with limited training samples. Given semantic preserving augmentations, both CRL and supervised learning capture similar discriminative features. Through Conjecture 1, we hypothesize that both CRL and cross-entropy share a common hypothesis class and sharing representations across these tasks especially benefits CL under low buffer regimes. \n\n> Why the classifier can create the function spaces?\n\nDeep learning classifiers create rich function spaces by leveraging hierarchical representation, non-linear activation functions, and automatic feature learning. The layered structure allows them to capture progressively abstract features, while non-linearities enable modeling complex relationships. Automatic feature learning adapts to data intricacies, and parameter sharing fosters generalization. With end-to-end learning and substantial model capacity, deep neural networks efficiently represent intricate patterns in a data-driven manner, making them adept at handling diverse and high-dimensional datasets."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376805538,
                "cdate": 1700376805538,
                "tmdate": 1700376805538,
                "mdate": 1700376805538,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Bxx2wfSFz",
            "forum": "qRbkTbe8JT",
            "replyto": "qRbkTbe8JT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7749/Reviewer_B5E5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7749/Reviewer_B5E5"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach called IMEX-Reg, tailored for low memory buffer scenarios. In addition to several other techniques such as EMA, the approach utilizes contrastive representation learning and applies regularization to both the classification outputs and projection outputs. The motivation behind the approach stems from the fact that contrastive embeddings lie on the hypersphere, which improves training stability and classification performance. The experiment shows that the proposed method outperforms the baselines under the low memory buffer settings and shows the effectiveness of the proposed techniques in the ablation study."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation is clear and the paper is well-written\n2. It outperforms the baselines"
                },
                "weaknesses": {
                    "value": "1. The contribution of the paper is merely incremental as using contrastive learning and regularization on function space have been extensively studied before [1, 2, 3]. Imposing a regularization on the contrastive projected outputs along with the classification outputs improves the performance is not surprising.\n2. The method introduces many hyper-parameters (e.g., alpha, beta, lambda in Eq.7), but there was no study of how these hyper-parameters affect the model performance.\n3. The authors argue that their method has advantages over existing replay methods. However, rehearsal-free methods such as [3, 4] already significantly outperform the proposed method. For instance, [3] achieves 87.8% accuracy on Seq-CIFAR10 and 47.1% on Seq-TinyImageNet without the need to save any samples.\n\n[1] Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning \\\n[2] co2l- Contrastive Continual Learning \\\n[3] A theoretical study on solving continual learning \\\n[4] Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning"
                },
                "questions": {
                    "value": "1. Based on a paper under review, [5] achieves 74+% in Seq-CIFAR10 with a buffer size of 200 in ResNet-18. The authors may compare their method with [5]\n2. Why is this method robust to natural corruption? An important discussion aligned with robustness to data distribution (or corruption) is covered in [6].\n\nMisc.\nPlease use \\` rather than ' for \\`SGD' and \\`Joint' on page 6\n\n\n[5] Learnability and algorithm for continual learning \\\n[6] A multi-head model for continual learning via out-of-distribution detection"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7749/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809936268,
            "cdate": 1698809936268,
            "tmdate": 1699636945578,
            "mdate": 1699636945578,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MJmtIMuwvR",
                "forum": "qRbkTbe8JT",
                "replyto": "0Bxx2wfSFz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7749/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7749/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the reviewer B5E5 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for insightful feedback and relevant references. Please find below our response to the questions raised:\n\n> The contribution of the paper is merely incremental \n\nWe thank the reviewer for relevant references with respect to contrastive learning in CL. The references [1,2,3,8] use contrastive learning in their learning objective. As we have repeatedly mentioned in the paper, we do not claim any novelty emanating from contrastive learning objective. We are very much aware of the competing literature that employs Exponential Moving Average (EMA) and Contrastive Representation Learning (CRL) in CL. Specifically, CLS-ER [7] employs EMA while Co2L [2] employs CRL. Going one step ahead, OCDNet [6] is the closest prior art that employs both CRL and EMA.  \n\nWe respectfully disagree with the idea that imposing regularization on the contrastive projected outputs implicitly improves classification performance. Our novel contribution mainly lies in leveraging desirable traits of learning in the unit hypersphere through explicit classifier regularization in the function space. To the best of our knowledge, there are no works that leverage traits of learning in the unit hypersphere in CL. To this end, we align the geometric structures within the classifier hypersphere with those of the projection head hypersphere to compensate for weak supervision under low buffer regime.\n\nWe carefully curate the list of baselines based on their learning objectives: contrastive learning methods, EMA based methods and approaches  that use both contrastive learning and EMA. We note that these are highly cited works in CL that are published in reputed conferences. We compare and contrast such methods (CLS-ER, CO2L, OCDNet) in \u2018Related Works\u2019, and in experimental evaluation in Tables 1 and 2. We also compare with OCDNet in terms of task-recency bias, robustness against natural and adversarial corruptions in Section 5. Table 3 highlights the contribution of IMEX-Reg in Seq-CIFAR100 for a buffer size of 200. As can be seen from our experimental results and analysis, IMEX-Reg clearly outperforms these competing methods that entail EMA and CRL. This further emphasizes that our novel loss brings additional benefits not just in terms of performance but also in terms of robustness and less bias toward recent tasks. \n\n> The method introduces many hyper-parameters\n\nAs IMEX-Reg entails multiple learning objectives, it introduces several hyperparameters. Table 6 provides the hyperparameters for used IMEX-Reg to reproduce the results reported in Table 1. We agree with the reviewer that a comparative study on hyperparameter tuning would provide more insights into IMEX-Reg\u2019s performance. Table 7 in the revised manuscript provides  hyperparameter tuning experiment results. Due to limited computational capacity, we  will include an exhaustive study in the final revision. \n\n> Rehearsal-free methods such as [3, 4] already significantly outperform the proposed method\n\nThe method in [3] explores possibilities of integrating the existing parameter isolation based continual learning (CL) method HAT or Sup with the strong OOD detection method CSI. As the reviewer rightly pointed out, these approaches attain accuracy higher than the existing replay-based methods. The improvement in performance can be attributed to several aspects of training: HAT and Sup entail parameter isolation during training. That inherently assumes task boundary information during training. It is a stricter constraint as task boundaries are not always accessible in real world CL scenarios. Secondly, these methods involve two stage training wherein the backbone is trained in the first stage and is kept frozen in the second stage. An additional rotation prediction and averaging is done to train a classifier in the second stage. \n\nAs discussed in Table 2, Generalized Class-IL (GCIL) exposes the CL model to a more challenging and realistic learning scenario by using probabilistic distributions to sample data from the CIFAR100 dataset in each task. The aforementioned methods would fail in such scenarios due to the necessity of accessing task boundary information. As having the features lie on the unit-hypersphere leads to several desirable traits, we would like to re-emphasize that  our novelty lies in leveraging activation correlations in the unit-hypersphere of the CRL to guide the classifier towards generalization. Although the above methods attain higher accuracy, they do not address the problem we have at hand. \n\n> Based on a paper under review, [5] achieves 74+% in Seq-CIFAR10 with a buffer size of 200 in ResNet-18. The authors may compare their method with [5].\n\nWe thank the reviewer for suggestions on the related work. We will include this in the final revision."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375810171,
                "cdate": 1700375810171,
                "tmdate": 1700376475466,
                "mdate": 1700376475466,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]