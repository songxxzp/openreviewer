[
    {
        "title": "Decision Transformer is a Robust Contender for Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "CPUnm3Ytcy",
            "forum": "vpV7fOFQy4",
            "replyto": "vpV7fOFQy4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1370/Reviewer_wytN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1370/Reviewer_wytN"
            ],
            "content": {
                "summary": {
                    "value": "The authors compare three approaches to Offline-RL: CQL (representing Q-learning approaches), BC (IL approaches), and Decision Transformer (Sequence Modeling approaches) under a range of environments and data regimes (e.g. varying the amount of data, quality of data, or whether it was generated by a human or agent).\nBased on the results, they distill concise, actionable advice for practitioners."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think this paper is a useful read for every practitioner in Offline-RL as it concisely evaluates and tests a range of relevant environmental factors and how those should influence the choice of algorithm. \nFairly comparing and evaluating different algorithms against each other is a seemingly simple, but actually not that straightforward thing to do as a lot of choices have to be made well: For example, which algorithms to compare, which environments to use and how to distill the results into concise statements that readers will actually remember.\nAnd I think the authors did a great job here in this regard. \nOf course, there are various additional things that they could have looked at (e.g. continuous vs. discrete actions) or a more modern approach than BC - but I do understand that lines have to be drawn somewhere."
                },
                "weaknesses": {
                    "value": "I think the paper has no important weaknesses w.r.t what it set out to do, namely comparing important algorithmic approaches on a range of relevant environments.\nIn terms of publication, one could argue that there is limited novelty - not just in terms of algorithmic novelty, but also insights generated: None of the insights were very surprising and some were quite obvious (e.g. CQL performs worse when the back-propagation of the reward signal is impeded through sub-optimal or longer trajectories). \n\nI do believe this is a valid concern, but nevertheless regard the paper as very useful for the community, hence my initial rating of 6 (with a tendency towards 8)."
                },
                "questions": {
                    "value": "Please let me know if I mischaracterized or overlooked some of your contributions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698401524156,
            "cdate": 1698401524156,
            "tmdate": 1699636064583,
            "mdate": 1699636064583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W0Ovs85owC",
                "forum": "vpV7fOFQy4",
                "replyto": "CPUnm3Ytcy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are very grateful for the reviewer's feedback. \n\nWe have highlighted how our work allows one to determine when DT would be preferred in offline RL in the common response section. We would like to highlight that many insights that are valuable to the community are not necessarily surprising (for ex. Scaling model size and data). All of our insights are backed up by thorough empirical analysis, providing readers with actionable and reliable insights. We emphasized more on the continuous action space environment primarily due to its importance over discrete action space. Having more experiments required more time, resources which might lead to diminishing returns or not significantly alter our conclusions. We hope you would understand, and want to urge the reviewer to consider increasing the score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243812185,
                "cdate": 1700243812185,
                "tmdate": 1700243812185,
                "mdate": 1700243812185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "INmgNMjBTJ",
                "forum": "vpV7fOFQy4",
                "replyto": "W0Ovs85owC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1370/Reviewer_wytN"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewer_wytN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642415279,
                "cdate": 1700642415279,
                "tmdate": 1700642415279,
                "mdate": 1700642415279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UDty89GC3E",
            "forum": "vpV7fOFQy4",
            "replyto": "vpV7fOFQy4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1370/Reviewer_q2Sb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1370/Reviewer_q2Sb"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates three offline RL algorithms DT, CQL and BC under different settings, to see the effective use case for each algorithm. The experiments involve the settings with sparse-dense rewards, different data qualities, various trajectory lengths, random data injection, different complexity of tasks, stochastic environments and scaling with data and model parameters. The experiments are quite thorough with observed facts summarized, as well as conjectural reasons. It is an interesting paper providing some insights for the offline RL community."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written with small formatting issues. The experiments are quite thorough.\n\nIn comparing DT, CQL and BC, the paper involves at least seven different aspects. The observed facts are quite informative. Apart from summarizing the observed facts, the authors also provide reasonable explanations for most of the results."
                },
                "weaknesses": {
                    "value": "Although explanations are provided for most of the observed facts, most of them are speculative without further evidence. For example, in Sec. 4.1, ``a likely reason \u2026 mean CQL must propagate TD errors \u2026\u2019\u2019. If these conjectures are dived deeper with further evidence and justification, it will make the paper more solid.\n\nAnother major critique is that the algorithms CQL and BC are no longer the SOTA results for these offline RL baselines, and more effective algorithms are proposed with improvement. Examples like TD3BC [1], percentage BC, Diffusion Q-learning [2] for BC type methods,  and implicit Q-learning, implicit diffusion Q-learning [3], for Q-learning type methods, etc. Comparing DT with CQL, BC is a good starting point, but it could also be more interesting to see those more advanced algorithms in these comparisons. For example, it is known that BC does not perform well as long as the dataset contains not only high-return samples, but how does percentage BC perform in settings other than Sec. 4.2? Would it be a robust and reliable method as well in the offline setting?\n\nMinor issues: \n\nPosition of Tab. 2 is problematic.\n\nReference:\n\n[1] Fujimoto, Scott, and Shixiang Shane Gu. \"A minimalist approach to offline reinforcement learning.\" Advances in neural information processing systems 34 (2021): 20132-20145.\n\n[2] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. \"Diffusion policies as an expressive policy class for offline reinforcement learning.\" arXiv preprint arXiv:2208.06193 (2022).\n\n[3] Hansen-Estruch, Philippe, et al. \"Idql: Implicit q-learning as an actor-critic method with diffusion policies.\" arXiv preprint arXiv:2304.10573 (2023)."
                },
                "questions": {
                    "value": "In Fig. 1(a), it shows when the dataset contains more sub-optimal samples, CQL degrades its performance more than DT, but when it uses 100% data the performance increases back to as good as DT. Is there any explanation for this?\n\nIt is interesting to see that DT is more reliable than CQL when adding random data (as Sec. 4.4), while less robust when injecting stochasticity in the environment (as Sec. 4.6). What\u2019s the potential reason for DT to perform less robustly in the latter case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698539834720,
            "cdate": 1698539834720,
            "tmdate": 1699636064509,
            "mdate": 1699636064509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2kwp3tUq1f",
                "forum": "vpV7fOFQy4",
                "replyto": "UDty89GC3E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our gratitude to the reviewer for providing us with thorough feedback.\n\n1. We have addressed the explanation concern in Common Response section\n2. We appreciate the reviewers' suggestions to consider additional methods like IQL and TD3BC etc. We do not have a consensus on which algorithms can be considered SOTA in offline RL literature due to many confounding factors (stochasticity, dense/sparse settings etc.). For the sake of keeping our work relevant and broadly applicable, it is reasonable to consider algorithms which have become mainstream and are widely used. Our choice of algorithms (CQL, DT, BC) was driven by their widespread acceptance, ease of interpretation and compute budget. The simplicity of these algorithms, combined with our extensive understanding of them as a community, allows us to clearly trace performance fluctuations, which is crucial for our study's comprehensive analysis. In this way, our work provides valuable insights that enhance understanding of offline RL agents. We would like to highlight the compute and time concerns from the Common Response section to emphasize why having these agents made the most sense.\n\nDue to these concerns, it was reasonable to leave out the diffusion based approach (it is yet to be widely adopted).\n\nHere are some additional reasons why we did not study other methods.\n- Numerous papers that are pivotal and investigative in nature (Engstrom et al. 2019, Andrychowicz et al. 2021, Ilyas et al. 2019) have showcased results on a small set of algorithms and environments:\nBrandfonbrener et al. 2022 showed empirical results on BC, IQL and DT on 1 toy environment and 3 D4RL environments with standard configuration.\nKumar et al. 2022 showed results on only CQL and BC on 3 D4RL environments with standard configuration.\n- GAIL (Ho et al. 2016) requires online reinforcement learning, diverging from our chosen pure offline RL setting where data remains static. TD3BC adds the BC objective into CQL. BC regularization is not as good as the one from CQL. When we are dealing with low-return data, we know it leans more on the BC objective. It is less interpretable.\n- IQL could potentially bolster the perceived strength of our results. However, there's no consensus on its superiority over CQL. In many scenarios, CQL outperforms IQL (Kostrikov et al. 2021).\n- Trajectory Transformer (Janner et al. 2021) employs planning in the learned model and is model based, unlike the model free methods which are the focus of our study. Additionally GATO  (Reed et al. 2022) is not open sourced, making DT the most appropriate choice in the paradigm.\n- We know that %BC only considers a small subsample of the existing dataset. It throws out potentially useful data. For making a fairer comparison, it made sense for each of the agents to see the same amount of data."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243791602,
                "cdate": 1700243791602,
                "tmdate": 1700243791602,
                "mdate": 1700243791602,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jsh4HyWvv2",
                "forum": "vpV7fOFQy4",
                "replyto": "2kwp3tUq1f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1370/Reviewer_q2Sb"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewer_q2Sb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for response.\n\nI think my questions about in-depth analysis beyond speculative reasons for observations are not addressed. \n\nFor percentage BC, I may disagree that it throws out potentially useful data. For BC approach, the quality of the data could be more essential than the amount of it. So I would expect to see the results of percentage BC from the paper. \n\nGiven above, although I generally like the perspectives taken by the paper, I cannot raise my score for now. The paper provides some insights for the domain but can definitely be enhanced to be a stronger one."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671521942,
                "cdate": 1700671521942,
                "tmdate": 1700671521942,
                "mdate": 1700671521942,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CpDrmWKmK2",
            "forum": "vpV7fOFQy4",
            "replyto": "vpV7fOFQy4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1370/Reviewer_2vu5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1370/Reviewer_2vu5"
            ],
            "content": {
                "summary": {
                    "value": "The paper empirically investigates the performance of three representative offline RL algorithms \u2013 Conservative Q-learning (CQL), Decision Transformer (DT), and Behavior Cloning (BC) \u2013 as several dimensions of the dataset, task, or model are varied, with the aim of determining which algorithm is most appropriate, depending on the setting. The authors provide concrete conclusions and recommendations based on their findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The experiments investigate a number of significant practical considerations in offline RL, such as reward structure, inclusion of suboptimal data, and task complexity.\n* The authors provide concrete conclusions and recommendations based on their findings. A practitioner trying to decide which algorithm to use for their task/dataset would likely benefit from reading this paper.\n* The paper is clear, readable, and presented well.\n* The D4RL-style humanoid dataset may be of independent interest to researchers, and I hope that the authors will release it."
                },
                "weaknesses": {
                    "value": "* I am skeptical of the sparsified version of D4RL because it violates the Markov property of the reward function: the reward now depends on the whole trajectory rather than just the current state/action. And two trajectories may end at the same state but have significantly different total rewards, causing disagreement. I can see why this transformation would break CQL but not DT, and indeed CQL deterioriates much more on sparse D4RL than sparse Robomimic. So I think the results here do not convincingly show that CQL is worse than DT for sparse environments in general; on D4RL, this artificial transformation violates an assumption underlying the CQL algorithm, and on Robomimic, CQL was already worse than DT on Robomimic even when given dense rewards.\n* The experiment on stochasticity does not seem to perfectly answer the question \u201chow good are these algorithms in stochastic environments?\u201d because the data you are training on was collected with deterministic dynamics. So really what you are testing is \u201chow well does a policy trained by these algorithms in a deterministic MDP transfer to a similar stochastic MDP?\u201d. This may also be an interesting question to study, but I think that if you intend to answer the first question, the experiment is not entirely appropriate, and it would be best to repeat it with the training data also collected in a stochastic MDP.\n* Some of the conclusions regarding BC are obvious (e.g. BC suffers more than offline RL when there is suboptimal/noisy data) or already widely known (e.g. BC works better than offline RL on human-generated demonstrations). It could provide more insight to include a hybrid method such as TD3-BC and show how the degree of regularization affects the behavior."
                },
                "questions": {
                    "value": "* Am I misunderstanding something in my complaints about the D4RL sparsification or the stochasticity? Please correct me if so.\n* What is a \u201chigh-quality continuous action space\u201d? (near top of pg. 9) Does this just refer to the degree of determinism?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1370/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1370/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1370/Reviewer_2vu5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736959934,
            "cdate": 1698736959934,
            "tmdate": 1700689315415,
            "mdate": 1700689315415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tRybM2j4Yp",
                "forum": "vpV7fOFQy4",
                "replyto": "CpDrmWKmK2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the efforts the reviewer took to provide feedback. We address all of the concerns of the Reviewer below.\n\n1) **Sparse setting in a Markovian environment**. \nWe agree that the existing D4RL sparse setting might not favor CQL due to non Markovian dynamics (similar to original DT study), we ran experimentation on Maze2D dense and split variants. Additionally we have experiments on Robomimic which also follows Markovian dynamics to back up our claims.\n\nThe following experiment was run on the sparse and dense variants of Maze as provided by the D4RL authors. It is worth noting that the Maze task explicitly requires trajectory stitching. As noted by Brandfonbrenfer et al. 2021, sequence modeling based approach would struggle in trajectory stitching. We observe this empirically here. Below we provide all the results sparse vs dense in addition to the experiments in paper to make comparison easier.\n\n|                     | CQL           | DT (RTG=400)  | BC            |\n|---------------------|---------------|---------------|---------------|\n| Maze 2D Sparse      | $19.86(0.95)$ | $6.7(5.89)$   | $15.39(6.57)$ |\n| Maze 2D Dense       | $49.62(7.79)$ | $27.9(6.85)$  | $38(5.23)$    |\n| D4RL Sparse         | $83.64(0.51)$ | $103.15(0.77)$| $38.74(6.58)$ |\n| D4RL Dense          | $40.78(7.88)$ | $76.6(1)$     | $38.74(6.58)$ |\n| Robomimic Sparse    | $30(6.6)$     | $88.2(1.6)$   | $57.2(6)$     |\n| Robomimic Dense     | $35.2(3.7)$   | $89.6(1.4)$   | $57.2(6)$     |\n-----\nWe will adjust the claim to highlight that DT struggles on tasks requiring trajectory stitching and CQL should be a preferred choice. Due to shortage of time, we did not tune any parameters (incl. Experimenting with RTGs)\n\n2) **Stochastic setting**. \nFor comparison with existing stochastic experiments provided in the paper, we trained all the agents on deterministic data from D4RL, and introduced stochasticity during inference on stochastic (both params kept to 0.25).\n\n|                   | CQL        | DT (RTG=400) | BC       |\n|-------------------|------------|--------------|----------|\n| Maze 2D Sparse    | 21.54(2.2) | 8.5(2.35)    | 4.62(3.91)|\n| Maze 2D Dense     | 48.5(5.64) | 37.79(3.76)  | 33.72(5.4)|\n\nHere also we see the trend where CQL emerges to be a preferred choice. The claims made in the paper remain intact.\n\nOne important concern which was raised is, we did not train agents in a stochastic setting. This is not a common setting in offline RL as the existing datasets are typically obtained in a deterministic environment. To address this concern, we trained a SAC agent to generate our own Maze 2D offline RL data in the same way as we generated humanoid with stochasticity parameters both set to 0.1 (prob=0.1, sigma=0.1) (trained for 2x longer to account for instability). \n\nThe following results are obtained when all three agents were trained on stochastic data followed by inference in stochastic setting (both params set to 0.25)\n\n|               | CQL         | DT (RTG=400) | BC        |\n|---------------|-------------|--------------|-----------|\n| Maze 2D Dense | 30.71(0.04) | 36.31(4.58)  | 32.4(6.88)|\n\nWe observed that DT outperforms CQL and BC in this scenario. This is an interesting finding showing that when all three agents are trained in a stochastic setting, CQL may or may not be the best choice when evaluated in a stochastic setting.\n\n3) While some of the conclusions regarding BC were known to the community (such as it typically thrives in expert data regime), our work provided multitudes of additional insights about BC which are novel (are not widely known or have not been shown empirically with this much thoroughness), we mention here a few points that can be considered novel about BC.\n- The trend shown in Figure 1 and Figure 9 of %BC gives a deep insight about the behavior of DT as more high return and low return data is made more accessible, at a much more granular level not seen previously in other works.\n- BC can be a preferred choice over CQL in sparse environments (especially in cases where rewards are not known)\n- BC shows impressive performance on offline RL tasks, outperforming DT and CQL, when data is collected from human demonstrations. We have observed this extensively and have been observed in previous works.\n- BC stays competitive when the task horizon is increased compared to other agents.\n\nThe main point we want to highlight is all of the claims are backed up by rigorous experimentation, providing researchers with reliable insights.\n\n4) By \u201chigh-quality continuous action space\u201d, we are pointing out that if the offline RL data contains trajectories which are deemed high quality (medium-expert), then the drop in performance of DT in stochastic settings might be comparable to that of CQL as our findings show. We will rephrase this sentence to make it more clear.\n\nWe hope this addresses the concerns raised by Reviewer 2vu5 and humbly request them to reconsider the score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243559747,
                "cdate": 1700243559747,
                "tmdate": 1700243735731,
                "mdate": 1700243735731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pqdQRXSeOj",
                "forum": "vpV7fOFQy4",
                "replyto": "tRybM2j4Yp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1370/Reviewer_2vu5"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewer_2vu5"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for conducting additional experiments in response to my concerns! I would say that they have been addressed, and assuming the paper is edited in accordance with your new results and conclusions, I have raised my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689613939,
                "cdate": 1700689613939,
                "tmdate": 1700689613939,
                "mdate": 1700689613939,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ALDa0umpWr",
            "forum": "vpV7fOFQy4",
            "replyto": "vpV7fOFQy4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1370/Reviewer_pkaE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1370/Reviewer_pkaE"
            ],
            "content": {
                "summary": {
                    "value": "This paper contributes to the field of offline Reinforcement Learning (RL) by conducting a comprehensive research study comparing three popular algorithms: CQL, BC, and DT. It investigates their performance across benchmark datasets like D4RL and ROBOMIMIC, analyzing their behavior under varying data quality, task complexity, and stochasticity.\n\nThe key findings reveal that DT requires more data than CQL to achieve competitive performance, but exhibits higher robustness to suboptimal data. Notably, DT outperforms both CQL and BC in sparse reward and low-quality data settings. Interestingly, DT and BC demonstrate superior performance for tasks with longer horizons or data collected from human demonstrations. Additionally, CQL shines in situations characterized by both high stochasticity and lower data quality.\n\nBeyond comparative analysis, the paper delves deeper into DT, exploring optimal architectural choices and scaling trends on ATARI and D4RL datasets. Notably, it demonstrates that increasing the data volume by fivefold results in a 2.5x average score improvement for DT on ATARI.\n\nOverall, this research offers valuable insights for selecting the most appropriate offline RL algorithm based on the specific characteristics of the task and data conditions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper conducts a thorough empirical analysis on offline RL algorithms, focusing on DT and the comparison to other offline RL algorithms, in particular, CQL, and BC. The paper presents a number of valuable practical insights, which offer great value to the RL community regarding selecting offline RL algorithms in different scenarios.\n\nThe paper is well-organized, especially listing the main practical insights at the end of each section, which makes readers easy to follow and understand."
                },
                "weaknesses": {
                    "value": "1. While the paper offers great empirical analysis, it would be still great to provide some technical/theoretical justifications regarding each empirical result shown in the paper, similar to [1], which will further strengthen the claim in the paper.\n\n2. The empirical experiments are done in Robomimic and D4RL, but the D4RL tasks are only mujoco locomotion tasks. The authors should include experiments on more complex tasks such as Androit, Antmaze and vision-based tasks, e.g. vision-based drawer manipulation tasks in [1] and Atari games. \n\n[1] Kumar, Aviral, Joey Hong, Anikait Singh, and Sergey Levine. \"Should i run offline reinforcement learning or behavioral cloning?.\" In International Conference on Learning Representations. 2021."
                },
                "questions": {
                    "value": "1. Please offer theoretical insights in each of the empirical findings where DT is more robust.\n2. Please perform experiments in more complex tasks such as Androit, Antmaze and vision-based tasks, e.g. vision-based drawer manipulation tasks in [1] and Atari games."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699252391375,
            "cdate": 1699252391375,
            "tmdate": 1699636064321,
            "mdate": 1699636064321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zabaZdqbxZ",
                "forum": "vpV7fOFQy4",
                "replyto": "ALDa0umpWr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1370/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1370/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking time to provide feedback. \n\nWe addressed the theoretical justification concern in the common response section.\n\n2) As requested by the reviewer, we are providing additional results on more complex tasks of D4RL such as Antmaze and Adroit. We have tons of results on Atari in Appendix (please see Section H) (due to lack of space in main text space). We were not able to add vision tasks due to reasons mentioned in the Common Response section.\n\n|                   | CQL        | DT (RTG=400) | BC      |\n|-------------------|------------|--------------|---------|\n| Antmaze-umaze-v1  | 71.33(2.05)| 66.66(5.31)  | 62.6(6.8)|\n| Pen-Human-v0      | 72.52(2.31)| 78.36(4.09)  | 85.68(6.77)|\n\n- Antmaze also requires agents to explicitly stitch trajectories. Based on the empirical findings on this environment, we will highlight the trajectory stitching drawback with DT. \n- Results on Adroit corroborate our finding that BC is a superior choice when the data comes from human demonstrations followed by DT. A trend we have seen previously with Robomimic.\n\nWe hope this addresses all of the concerns of Reviewer pkaE."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243102616,
                "cdate": 1700243102616,
                "tmdate": 1700243577807,
                "mdate": 1700243577807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]