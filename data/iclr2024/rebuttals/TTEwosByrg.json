[
    {
        "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators"
    },
    {
        "review": {
            "id": "fA0ST08qwM",
            "forum": "TTEwosByrg",
            "replyto": "TTEwosByrg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8501/Reviewer_K15E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8501/Reviewer_K15E"
            ],
            "content": {
                "summary": {
                    "value": "The paper conduct cross-check study of multiple open and close sourced LLMs against each other to evaluate the cognitive biases inherent in these models, when used as a comparison evaluator for certain tasks.\nThe paper evaluates on 6 different dimensions: Order, Compassion, Egocentric, Salience, Bandwagon and Attentional. For example, egocentric bias indicates the model prefers itself over a competitor.\nResults according to their proposed benchmarks show all models, include large closed source ones, are biased on a few different dimensions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The evaluations of LLMs are comprehensive and contain many aspects, dimensions, as well as correlation analysis with human.\n- Presentation of the paper is clear and understandable."
                },
                "weaknesses": {
                    "value": "- The indicators of the benchmark is overly complicated, contain many numbers, dimensions, which is very difficult to understand. This make understanding and judgements of models based on this benchmark non-trivial. The proposed benchmark should be aggregated in a way that is simpler to grasp without losing too much information.\n- More analysis with human's own biases on these dimensions are needed.\n- The details are quite unclear. The use of 50 examples is insufficient to evaluate 15 LLMs"
                },
                "questions": {
                    "value": "- How these evaluations help in the improvements of LLMs?\n* the average RBO amongst human annotators (0.478) is actually lower than the average RBO between human and models (0.496), so why the conclusion that model evaluations doesn't align with humans if they seem to be better aligned than human? \n* I see that random is calculated for different biases, however, for better models like ChatGPT, the egocentric bias may be unfair because the generation of ChatGPT is indeed better, or the salience biases, maybe indeed the longer generations have higher quality. Does the authors try to decouple these confounders?\n* for the RBO values, does the author adapt equation (1) to equation (2)? may I know more details on why equation (1) cannot be used, and if using equation (2), could the authors provide more insights on what each ranges of values mean? for instance, we know for cohen's kappa, 0.61-0.80 indicates substantial agreement, and 0.81-1 indicates almost perfect agreement."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8501/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698490985296,
            "cdate": 1698490985296,
            "tmdate": 1699637062028,
            "mdate": 1699637062028,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DNKvmqchKI",
                "forum": "TTEwosByrg",
                "replyto": "fA0ST08qwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer K15E (1/N)"
                    },
                    "comment": {
                        "value": "**(Q1) \u201cThe indicators of the benchmark is overly complicated\u2026\u201d**\n\nWe thank you for your feedback. We presented individualized scores across each dimension in order to be as precise as possible to represent a fair comparison and evaluation of each language model as an evaluator. However, we acknowledge that the current organization of scores across several dimensions may complicate their interpretability. In order to increase the readability of Table 2, instead of breaking down Order and Compassion Fade into \u201cFirst Order\u201d and \u201cLast Order\u201d columns, we will merge them as a \u201cpositional\u201d measure and take the more intense value to simplify each of the benchmark metrics to a single score. Additionally, we will move the current representation down to the Appendix to provide a full breakdown of the bias metrics at the reader\u2019s leisure. \n\n---\n\n**(Q2) \u201cMore analysis with human's own biases on these dimensions are needed\u2026\u201d**\n\nWe thank you for your comments. We underline that our main objective of the pairwise human bias experiment (Section 5.2) is to measure how much humans were also affected by different types of cognitive biases, thus comparing them with LLMs. Due to time constraints and the main scope of this entire paper, however, we acknowledge that we were not able to run additional analysis with the biases in human preference. \n\nWe will put more in-depth analyses of human biases in our future work. Thank you. \n\n---\n\n**(Q3) \u201cThe details are quite unclear. The use of 50 examples is insufficient to evaluate 15 LLMs\u201d**\n\nWe understand that the instruction set of sample size 50 may be small; however, we emphasize that the actual number of total evaluations is very large, as we construct pairwise examples between 50 generations of all 15 models for each LM-as-evaluator (totaling 10,500 samples as outlined in Section 4.2). We limited our evaluation size to 50 Q/A instances due to the large number of choices for considering each bias. We also draw upon this setting from previous works in which [1] selects 80 questions to evaluate pairwise instances for only 6 models, totaling 1,200 pairwise examples, and [2] selects computes pairwise comparisons across 12 models and 40 questions.\n\nFurthermore, we underline that considering our task, setting up a test for the significance of results from a specified sample size is relatively difficult as there is no baseline to compare to other than evaluation results from other models. Thus, we provide results from testing the statistical significance of differences in bias scores among the 15 models below from a 1-way ANOVA test. Our calculated p-value of 0.02 highlights that the difference in bias scores between the 15 models is indeed statistically significant to further support the significance of our results over 50 question-answer instances:\n\n|    | sum_sq | df | F | PR(>F) |\n|:-----------|-----:|----:|-----------:|---------:|                    \nC(model) | 1.658140  | 14.0 | 2.027726 | 0.020939\n\n---\n\n**(Q4) \u201cHow these evaluations help in the improvements of LLMs?\u201d**\n \nWe believe our contribution can be utilized for the future development of models in reducing these identified cognitive biases. For example, our preference dataset can be utilized as subsequent RLHF training that can assign negative signals with evaluations associated with a bias from our benchmark. Although employing this in practice remains out-of-scope for this work, we iterate that the contribution and novelty of our work come from our large-scale study of several models and the introduction of a benchmark composed of six cognitive biases, providing valuable insight (as underlined by Reviewer 1K6b) that has not been previously investigated in depth. However, we underline that this direction remains an important and exciting area to explore in future research."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8501/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712965294,
                "cdate": 1700712965294,
                "tmdate": 1700713502598,
                "mdate": 1700713502598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "flHUgdIW6i",
                "forum": "TTEwosByrg",
                "replyto": "fA0ST08qwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer K15E (2/N)"
                    },
                    "comment": {
                        "value": "**(Q5) \u201c... The inter-annotator agreement (IAA) among the workers is not notably high in both ranking and pairwise tasks. The IAA aligns with the agreement between humans and machines. While the authors' claim that \"LLMs are still not suitable as fair and reliable automatic evaluators\" is not incorrect, it appears that humans may also fall short in this regard.\u201d**  \n\nWe thank you for your feedback and acknowledge that the current IAA score (0.47) between humans is not high compared to agreement amongst humans/machines. \n\nWe realize our current calculation procedure may not precisely capture human or each machine preference tendencies by simply aggregating all of them, which, as a result, produces higher agreement between machine and humans and may not be a fair representation of the IAA. Hence, we provide a correction to our previous calculation for a more precise result by computing the RBO between each individual annotator and machine preferences, obtaining an average RBO of **0.357** from $6 \\times 15$ different RBOs. This way, we present a more complete and accurate representation of the involved parties' preference behaviors and thus their agreements. We see that this correction further supports our claim that there is a misalignment between humans and LLMs, compared to IAA among humans. We will add these supplementary results from this new approach in the final draft. \n\nFor the pairwise task measuring biases in human preferences, we acknowledge that there would be lower IAA among humans, as the study involves 75 annotators for each bias (225 annotators) which we argue still achieves reasonable agreement given the complexity, and scale of agreement to calculate between humans. Additionally, we clarify that the main objective of this human pairwise setup is to measure the impact on human evaluation quality via our bias benchmark rather than measuring agreement. The main takeaway is that humans generally were less impacted by most of the biases than LLMs which continue to support our original claims. We will clarify these intentions in the final version of the manuscript."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8501/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713458466,
                "cdate": 1700713458466,
                "tmdate": 1700713987843,
                "mdate": 1700713987843,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BAieA5mHSL",
                "forum": "TTEwosByrg",
                "replyto": "fA0ST08qwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer K15E (3/N)"
                    },
                    "comment": {
                        "value": "**(Q6) \u201dI see that random is calculated for different biases, however, for better models like ChatGPT, the egocentric bias may be unfair because the generation of ChatGPT is indeed better\u2026\u201d**\n\nWe agree with your statement that biases such as Egocentric and Saliency may be difficult to fairly measure. However, we highlight two important aspects regarding the identification of these biases:\n\n1. If multiple models have a large proportion of evaluations preferring their own responses (as the evaluated pool of pairwise instances is the same for each evaluator), we reason that this may suggest \u201cegocentric\u201d qualities within involved evaluators, regardless of the objective strength of the models. Moreover, we see this effect is especially demonstrated between the more powerful models as well (GPT4 & ChatGPT) that suggest the presence of Egocentric evaluations from their disagreement. \n2. We employ various strategies in order to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \u201chierarchical\u201d rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for Saliency or Egocentric bias. \n\nTo get further insight into decoupling them, we provide additional statistics below examining the proportion of Egocentric samples where the (self-preferred) model\u2019s generation was longer/shorter than the other generation.  \n\nOverall, we view that most models (9/16) exhibit a self-preference for their own generations often when their own generations exhibit longer token length. As above, we see that Salience may be associated with higher quality generations, as we see that the strongest models (GPT4, ChatGPT) often prefer their own responses when their generations are longer. Furthermore, even in smaller models (e.g. Cohere, Koala) prefer their own generations more often when they are longer. However, as we previously emphasized, if multiple models observe a self-preference for their own generations, it is difficult to associate with Salience as there is disagreement that is indicative of an Egocentric bias. \n\n| Model (>175B)   | gpt4 | chatgpt | instructgpt |\n|-----------------|------|---------|-------------|\n| Egocentric | $0.78$ | $0.58$ | $0.28$ | \n| Longer Egocentric  | $0.64$ | $0.75$ | $0.43$ |\n| Shorter Egocentric | $0.36$ | $0.25$ | $0.56$ |\n\n| Model (>40B)     | llamav2 | llama | cohere | falcon |\n|-----------------|---------|-------|--------|--------|\n| Egocentric | $0.06$ | $0.0$ | $0.27$ | $0.05$ | \n| Longer Egocentric  | $0.29$ | $0$ | $0.68$ | $0.6$ |\n| Shorter Egocentric | $0.71$ | $0$ | $0.32$ | $0.4$ |\n\n| Model (>10B)       | alpaca | vicuna | openassist | dollyv2 |\n|-------------------|--------|--------|------------|---------|\n| Egocentric | $0.18$ | $0.27$ | $0.15$ | $0.0$ | \n| Longer Egocentric  | $0.38$ | $0.4$ | $0.71$ | $0$ |\n| Shorter Egocentric | $0.62$ | $0.59$ | $0.29$ | $0$ |\n\n| Model (<10B)       | baize | koala | wizardlm | mpt   | redpajama |\n|-------------------|------|-------|---------|-------|-----------|\n| Egocentric | $0.02$ | $0.48$ | $0.14$ | $0.21$ | $0.04$ | \n| Longer Egocentric  | $0$ | $0.55$ | $0.48$ | $0.54$ | $0.79$ |\n| Shorter Egocentric | $0$ | $0.45$ | $0.53$ | $0.46$ | $0.21$ |\n\nWe will include this additional insight into the Appendix section of the final draft for more context into decoupling the confounding factors."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8501/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713482806,
                "cdate": 1700713482806,
                "tmdate": 1700714054478,
                "mdate": 1700714054478,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "20DE9zU1PR",
            "forum": "TTEwosByrg",
            "replyto": "TTEwosByrg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8501/Reviewer_1K6b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8501/Reviewer_1K6b"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the cognitive biases in LLM-based evaluation on 15 different LLMs. It introduces a cognitive bias benchmark that covers both implicit (order, compassion, egocentric, salience) and induced biases (bandwagon, attentional) and explores the LLMs' performance as well as human bias on these aspects. It also examines the correlations between human and machine preferences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* Benchmarking of cognitive biases in LLM assessment is a very important topic because recent studies have extensively used LLM for judgment and were not aware of the limitations of their capabilities. This paper provides a comprehensive investigation of 6 cognitive biases, including the new benchmark and detailed analysis of 15 popular LLMs.\n* It provides several interesting insights into cognitive biases in LLMs with different scales. For example, larger models prefer the long response more than the small models, and small models favor the last-ordered systems while the large ones favor the first one. It also draws attention to the vulnerability of LLMs on the attack of bandwagon and attentional.\n* This paper also conducts human evaluation. It investigates the correlation between LLMs and human, and discuss the potential"
                },
                "weaknesses": {
                    "value": "* The main weakness is that the number of instructions is small: only 50 question-answering instances. This affects the reliability of conclusions since the data points are limited. It will be better to conduct significant tests and report the p-value for results.\n* The experiments do not consider ties in the pairwise evaluation, which may affect the conclusion. For example, if the responses of two systems are very similar, it is fine to choose any of them.\n* For human evaluation, the average RBO among AMT workers is only 0.478, which means that there are diverse preferences between humans. Therefore, the average RBO 0.496 between human and model preferences does not necessarily indicates the misalign with human because even human cannot achieve high alignment."
                },
                "questions": {
                    "value": "* Examples in Table 1 are difficult to understand. For example, in compassion fade, which model is given and affected by the recognizable names?\n* What does the bias score in Figure 3 mean? Is the higher the better or the lower the better?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8501/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8501/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8501/Reviewer_1K6b"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8501/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717612420,
            "cdate": 1698717612420,
            "tmdate": 1699637061891,
            "mdate": 1699637061891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DlHVV0Cnh3",
                "forum": "TTEwosByrg",
                "replyto": "20DE9zU1PR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer 1K6b (1/N)"
                    },
                    "comment": {
                        "value": "**(Q1) \u201cThe main weakness is that the number of instructions is small\u2026\u201d**\n\nWe understand that the instruction set of sample size 50 may be small; however, we emphasize that the actual number of total evaluations is very large, as we construct pairwise examples between 50 generations of all 15 models for each LM-as-evaluator (totaling 10,500 samples as outlined in Section 4.2). We limited our evaluation size to 50 Q/A instances due to the large number of choices for considering each bias. We also draw upon this setting from previous works in which [1] selects 80 questions to evaluate pairwise instances for only 6 models, totaling 1,200 pairwise examples, and [2] selects computes pairwise comparisons across 12 models and 40 questions.\n\nFurthermore, we underline that considering our task, setting up a test for the significance of results from a specified sample size is relatively difficult as there is no baseline to compare to other than evaluation results from other models. Thus, we provide results from testing the statistical significance of differences in bias scores among the 15 models below from a 1-way ANOVA test. Our calculated p-value of 0.02 highlights that the difference in bias scores between the 15 models is indeed statistically significant to further support the significance of our results over 50 question-answer instances:\n\n|    | sum_sq | df | F | PR(>F) |\n|:-----------|-----:|----:|-----------:|---------:|                    \nC(model) | 1.658140  | 14.0 | 2.027726 | 0.020939\n\n---\n\n**(Q2) \u201cThe experiments do not consider ties in the pairwise evaluation\u201d**\n\nThis is a valid point. Our original experiments did not include the \u201ctie\u201d option in order to avoid the possibility of gray areas where models-as-evaluators may assign a tie preference for all pairwise instances. Thus, in order to retrieve realistic results, we only gave the option for models to choose strictly between the two options presented. \n\nWe present a modified version of the prompt that considers ties in each pairwise preference in the following table. The results show evaluations affected by Order bias with considering ties. For Salience, if a pairwise sample was labeled as \u201cTie,\u201d we do not consider it for length bias. \n\nWe see that the inclusion of the tie option does view a considerable change in the bias benchmarks. Notably, the strongest and smallest models (GPT-4, ChatGPT, Baize, WizardLM) do not exhibit any change. However, we see that the mid-range models (Alpaca, Vicuna) and InstructGPT display a large preference for assigning the tie label ($\\geq 90 \\\\%$) that does not present any valid results, to which we had originally only prompted two options for each LM-as-evaluator to avoid this issue. The only model that demonstrated an improvement from previous bias behavior was Cohere. We will include these supplementary results in the Appendix within the updated manuscript via your suggestion. \n\n```\n### Instruction: Which system's response is more coherent considering the reference and instruction?\nThe instruction is: {{instruction}}\nThe reference is: {{reference}}\n'''\n{model1_name}: {model1_response}\n{model2_name}: {model2_response}\n'''\nPlease response directly in the following format: System _ is better\n\nIf you believe each response is equally sufficient simply respond with: Tie\n\nDo not provide any additional text or explanation:\n### Response:\n```\nDue to limited computation resources and time, we only run additional experiments for two models at each size range (+ all API-based models) and present the results below: \n\n| Models  |  GPT-4 | ChatGPT | InstuctGPT | Cohere | Alpaca | Vicuna | Baize | WizardLM |\n|-------------------|-------|---------|------------|--------|--------|--------|-------|----------| \n| Order | $0.17_F$ | $0.38_F$ | $0.24_L$ | $0.33_F$ | $0.82_L$ | $0.32_F$ | $0.95_L$ | $0.64_L$ |  \n| Order (tie) | $0.15_F$ | $0.43_F$ |  $0.0$ |  $0.08_L$ | $0.0$ | $0.0$ | $0.81_L$ | $0.47_L$ | \n| Tie (%) | $0.01$ | $0.0$ | $0.88$ | $0.33$ | $0.95$ | $0.99$ | $0.0$ | $0.04$ |\n\n| Models  |  GPT-4 | ChatGPT | InstuctGPT | Cohere | Alpaca | Vicuna | Baize | WizardLM |\n|-------------------|-------|---------|------------|--------|--------|--------|-------|----------| \n| Egocentric  | $0.78$ | $0.58$ | $0.28$ | $0.27$ | $0.18$ | $0.27$ | $0.02$ | $0.14$ |\n| Egocentric (tie) | $0.77$ | $0.60$ | $0.04$ | $0.25$ | $0.02$ | $0.0$ | $0.08$ | $0.16$ |\n\n| Models  |  GPT-4 | ChatGPT | InstuctGPT | Cohere | Alpaca | Vicuna | Baize | WizardLM |\n|-------------------|-------|---------|------------|--------|--------|--------|-------|----------| \n| Salience | $0.56$ | $0.63$ | $0.66$ | $0.60$ | $0.47$ | $0.53$ | $0.49$ | $0.53$ | \n| Salience (tie) | $0.55$ | $0.67$ | $0.06$ | $0.35$ | $0.01$ | $0.0$ | $0.50$ | $0.48$ |\n\nFor visual clarity, we only display the bias ratio with the highest proportion and denote with subscript $x_F$ or $x_L$ for first- or last-ordered bias, respectively. No subscript means same preference."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8501/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712512152,
                "cdate": 1700712512152,
                "tmdate": 1700712512152,
                "mdate": 1700712512152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RSUavbq0UA",
            "forum": "TTEwosByrg",
            "replyto": "TTEwosByrg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8501/Reviewer_mWup"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8501/Reviewer_mWup"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators to measure cognitive biases in LLM evaluation outputs. To this end, the authors introduce a cognitive bias benchmark for LLMs as evaluators (COBBLER) and find that LLMs are biased text quality evaluators and misalign with human evaluators."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors' contribution in proposing a cognitive bias benchmark for evaluating the quality and reliability of Language Model Evaluators (LLMs) is highly valuable for the research community.\n\n2. The study effectively analyzes six different biases, presenting interesting findings. Specifically, the observation that most of the models strongly exhibit several biases, coupled with the low agreement between machine and human preferences, sheds light on the differences between automated and human evaluations."
                },
                "weaknesses": {
                    "value": "1. In this work, the authors primarily focus on pairwise evaluation based on the coherence criterion, without considering other evaluation formats, such as single-document evaluation and interactive evaluation. \n\n2. As recommended in prior research (Wu & Aji, 2023), it is important to evaluate machine-generated text from various perspectives rather than depending solely on a single unified measure. It would be better to explore more diverse evaluation settings to ensure a comprehensive assessment of LLM-based evaluators.\n\n3. While the authors provide an overview of cognitive bias existing in different models in Section 5.1, it would be valuable to explore the potential contributions of current generation techniques, like self-consistency, in reducing bias. Including insights on these techniques can enhance the discussion and provide a more holistic understanding of bias mitigation approaches."
                },
                "questions": {
                    "value": "The example provided in Table 1 does not align with the definition of compassion fade bias?\n\nThere seems to be confusion regarding the number of models that human evaluators and model-based evaluators are required to rank in Section 5.2. Clarifying whether it is 15 or 4 models is necessary. Additionally, the statement about human consensus being modest but reasonable while model evaluations not aligning closely with human preferences is contradictory, as the average RBO values are very similar. Further clarification is needed to reconcile this discrepancy.\n\nIt is unclear what distinguishes Table 2 from Figure 4 and whether there are any significant differences in the findings presented in these two representations. Providing clarification on the key disparities and highlighting any noteworthy insights derived from each depiction would enhance the reader's understanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The experiments involve human evaluations and admit the annotation bias."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8501/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744184366,
            "cdate": 1698744184366,
            "tmdate": 1699637061777,
            "mdate": 1699637061777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YojkWpB06X",
                "forum": "TTEwosByrg",
                "replyto": "RSUavbq0UA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer mWup (1/N)"
                    },
                    "comment": {
                        "value": "**(Q1) \u201cIn this work, the authors primarily focus on pairwise evaluation based on the coherence criterion, without considering other evaluation formats.\u201d**\n\nThank you for the suggestion, and we completely agree that different types of evaluations could be used and that might result in completely different outcomes. However, please note that adapting these evaluation formats poses some technical difficulties and computational costs. For instance, not all models can generate or evaluate document-level inputs. Also, the interactive evaluations pose a computational challenge (which we iterate consists of over 630k comparisons) and formats such as explanations or discussion-based preferences are challenging to benchmark.\n\nInstead, our work focuses on comprehensiveness and scalability, so we had to simplify the evaluation setup to be a Q/A and preference-based comparison. Extending our study to other formats of evaluation could be an exciting direction for future work.\n\n---\n\n**(Q2) \u201cIt would be better to explore more diverse evaluation settings to ensure a comprehensive assessment of LLM-based evaluators.\u201d**\n\nWe acknowledge that in our evaluation setting, we ask each evaluator to analyze generation quality along one aspect (coherence) with respect to the reference and that this setup may confine the diversity of assessment of LLM-based evaluators. We highlight that it is difficult to conduct independent, more fine-grained evaluation setups due to the scale of our experiments across all 15 models as evaluators. \n\nWe also conjecture that these cognitive biases still remain regardless of evaluation aspects. To validate our conjecture, we conduct an additional experiment incorporating different dimensions of evaluation criteria into our pairwise evaluation prompt and report their results below. In our modified prompt, we ask each evaluator to judge responses based on their \u201ccoherence, accuracy, factuality, and helpfulness\u201d following [1] and [2]. (See the table and prompt below)\n\nWe find that the proportions of evaluations are still affected by Order bias considering ties. We see that by including diverse perspectives in the evaluation setting, for each model, some values become more pronounced (i.e. Cohere for egocentric) or bias decreases (i.e. Vicuna for egocentric). However, we see that for the majority of models in each of the tested benchmarks, the proportion of biased evaluations stays relatively consistent, which remains our conclusion that models still show a large skewness in bias tendency as evaluators along our benchmark.\n\nWe appreciate your suggestion again and will include the fine-grained evaluation in our final manuscript.\n\u2013\nThe following is the prompt and result of the fine-grained evaluation:\nFor context, the prompt we provide is:\n\n```\n### Instruction: Which system's response is more coherent, accurate, factual, and helpful considering the reference and instruction?\nThe instruction is: {{instruction}}\nThe reference is: {{reference}}\n'''\n{model1_name}: {model1_response}\n{model2_name}: {model2_response}\n'''\nPlease response directly in the following format: System _ is better\nDo not provide any additional text or explanation:\n### Response:\nDue to limited computation resources and time, we only run additionals experiment for two models at each size range (+ all API-based models) and present the results below: \n```\n| Models    | GPT-4 | ChatGPT | InstuctGPT | Cohere | Alpaca | Vicuna | Baize | WizardLM | \n|-------------------|-------|---------|------------|--------|--------|--------|-------|----------| \n| Order (coherent)  | $0.17_F$ | $0.38_F$ | $0.24_L$ | $0.33_F$ | $0.82_L$ | $0.32_F$ | $0.95_L$ | $0.64_L$ | \n| Order (diversity) | $0.14_F$ | $0.45_F$ | $0.22_L$ | $0.23_L$ | $0.76_L$ | $0.52_F$ | $0.83_L$ | $0.68_L$ | \n\n| Models    | GPT-4 | ChatGPT | InstuctGPT | Cohere | Alpaca | Vicuna | Baize | WizardLM | \n|-------------------|-------|---------|------------|--------|--------|--------|-------|----------| \n| Egocentric (coherent) | $0.78$ | $0.58$ | $0.28$ | $0.27$ | $0.18$ | $0.27$ | $0.02$ | $0.14$|\n| Egocentric (diversity) | $0.80$ | $0.54$ | $0.29$ | $0.41$ | $0.18$ | $0.18$ | $0.04$ | $0.09$ |   \n\n| Models    | GPT-4 | ChatGPT | InstuctGPT | Cohere | Alpaca | Vicuna | Baize | WizardLM | \n|-------------------|-------|---------|------------|--------|--------|--------|-------|----------| \n| Salience (coherent) | $0.56$ | $0.63$ | $0.66$ | $0.60$ | $0.47$ | $0.53$ | $0.49$ | $0.53$ | \n| Salience (diversity) | $0.57$ | $0.69$  | $0.70$ | $0.65$ | $0.49$ | $0.59$ | $0.50$ | $0.52$ | \n\nFor visual clarity, we only display the bias ratio with the highest proportion and denote with subscript $x_F$ or $x_L$ for first- or last-ordered bias, respectively."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8501/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711988826,
                "cdate": 1700711988826,
                "tmdate": 1700711988826,
                "mdate": 1700711988826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KXOdO4rNev",
                "forum": "TTEwosByrg",
                "replyto": "RSUavbq0UA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer mWup (2/N)"
                    },
                    "comment": {
                        "value": "**Q3) \u201cIt would be valuable to explore the potential contributions of current generation techniques, like self-consistency, in reducing bias.\u201d**\n\nWe thank the reviewer for the comment and acknowledge that exploring other mitigation techniques for each bias is a crucial area to explore within our bias benchmark. As mentioned in Section 4.2, we do include prompting measures such as self-consistency for every bias evaluation by prompting each pairwise instance twice in both orders (i.e. A first, then B, and B first, then A). We underline that the contribution of our work is proposing a benchmark of cognitive biases along six dimensions, testing the ability of automatic annotators to make quality evaluations without being impacted by other implicit (or induced) artifacts that may exist in the prompt. To this, we do try to incorporate some mitigation methods, such as the self-consistency checks to confirm the presence of a bias for fairness, and design our evaluation prompts according to the three criteria outlined in Section 3. However, exploring other mitigation methods to defend against each cognitive bias remains an invaluable task to investigate but remains out-of-scope for this work.  \n\n---\n\n**(Q4) \u201cThe example provided in Table 1 does not align with the definition of compassion fade bias\u201d**\n \nWe clarify that we partially borrow the definition of compassion fade from psychology, employing only the portion of the phenomena with respect to the influence from recognizable names as opposed to its real meaning defined in Section 3.1, which is difficult to examine through prompting. We highlight that our use case of presenting recognizable names in automatic evaluations is to measure the impact on the quality of said evaluations in comparison to ones with anonymized names. \n\nThus, the example provided in Table 1 is intended to display the differing evaluation behavior between evaluation settings presenting anonymized names and recognizable ones. Specifically, `System Star` is associated with `Model Alpaca`, and `System Square` is associated with `Model Vicuna`, where the selection of preferred responses (bolded) is inconsistent between anonymized names and recognizable names. We will make sure to clarify this detail for Table 1 and in the definition of Compassion Fade in Section 3.1 in the final manuscript. Thank you for your feedback.\n\n---\n\n**(Q5) \u201cThere seems to be confusion regarding the number of models that human evaluators and model-based evaluators are required to rank in Section 5.2. Clarifying whether it is 15 or 4 models is necessary. Additionally, the statement about human consensus being modest but reasonable while model evaluations not aligning closely with human preferences is contradictory, as the average RBO values are very similar.\u201d**\n\nThank you for this comment. For the first question, the number of models ($N$) that human and LLM evaluators ranked (as shown in Sec 5.2) is 15. Appendix C.2 shows only the preliminary results of $N=4$ via a ranked-list evaluation setting versus the pairwise study that was conducted. We will add further clarification to this detail in the final draft.\n\nAdditionally, we acknowledge that the current IAA score (0.47) between humans is not high compared to the agreement amongst humans/machines. \n\nWe realize our current calculation procedure may not precisely capture each human or machine preference tendencies by simply aggregating all of them, which, as a result, produces higher agreement between machines and humans and may not be a fair representation of the IAA. Hence, we provide a correction to our previous calculation for a more precise result by computing the RBO between each individual annotator and machine preferences, obtaining an average RBO of **0.357** from $6 \\times 15$ different RBOs. This way, we present a more complete and accurate representation of the involved parties' preference behaviors and thus their agreements. We see that this correction further supports our claim that there is a misalignment between humans and LLMs, compared to IAA among humans. We will add these supplementary results from this new approach in the final draft."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8501/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712245395,
                "cdate": 1700712245395,
                "tmdate": 1700712794377,
                "mdate": 1700712794377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NrvgVNarMi",
            "forum": "TTEwosByrg",
            "replyto": "TTEwosByrg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8501/Reviewer_tfnE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8501/Reviewer_tfnE"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes cognitive biases of large language models (LLMs) that are used as an evaluator. The authors use 15 models including GPT-4, LLaMA, Alpaca, and Koala, to generate their responses on 50 question-answering instructions. Then the models evaluate the pairwise preference of the responses, which is sent to their meta-evaluation benchmark for cognitive biases. The authors propose six types of biases, such as order bias, egocentric bias, and bandwagon effect. The presence of bias is defined when, depending on the specific feature and modifications of the prompt (e.g., the order of paired responses), the preference of a model shows a significant skew compared to random selection. The authors also compare the models\u2019 preferences against human labels collected by crowdworkers. Through their analysis, the authors claim that most of the examined LLMs exhibit cognitive biases and are not reliable evaluators."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- I appreciate the proposed taxonomy of cognitive biases observable in the behavior of LLM evaluators. A comprehensive measurement of these biases is crucial for fair and reliable LLM evaluation.\n- The paper is well-organized and well-written. The figures offer effective visualizations of the experiment pipeline and results. The literature review adequately covers recent relevant works on (meta-)evaluating LLMs."
                },
                "weaknesses": {
                    "value": "- While one of the potential contributions of this paper is its comprehensive analysis of multiple cognitive biases, I believe that most of the biases discussed have been previously identified (see Sections 3.1 and 3.2). The introduction of compassion fade, egocentric bias, and bandwagon effect may bring novelty, yet I have a few reservations:\n   * Regarding compassion fade, the models might be unfamiliar with the names of other models, as their training corpus likely doesn't include information on recent LLMs. The observed effect might closely resemble that of injecting random names.\n    * Concerning the bandwagon effect, the authors appear to rely on a single sentence: \"85% of people believe that {system} is better,\" without exploring variations in the percentage. It would be insightful to investigate what occurs when statements like \"0% of people believe that {system} is better\" are used. Observing a correlation between the biased tendency and the percentage stated in the injected sentence could provide deeper insights.\n- Some models display low rates of valid responses in the pairwise preference task. Specifically, seven out of the fifteen examined models (LLaMA, DollyV2, Koala, etc.) yield less than 80% valid responses, significantly lowering the cognitive bias scores. This issue hampers precise benchmarking performance estimation and poses a risk of underestimating the cognitive biases in models producing many invalid outputs. Consequently, the claim that \"most LLMs exhibit cognitive biases\" is not sufficiently justified, given that weaker models may exhibit biases more frequently.\n- Despite the careful recruitment of crowdworkers through pilot tasks and training sessions, the inter-annotator agreement (IAA) among the workers is not notably high in both ranking and pairwise tasks. The IAA aligns with the agreement between humans and machines. While the authors' claim that \"LLMs are still not suitable as fair and reliable automatic evaluators\" is not incorrect, it appears that humans may also fall short in this regard.\n\nOverall, I think the experiments yield results that do not provide enough empirical support for the authors' claims."
                },
                "questions": {
                    "value": "- I find it challenging to grasp the assumptions the authors make regarding biases in evaluation. The introduction section alludes to \"unbiased\" evaluation, but it remains unclear whether achieving such an evaluation is realistic, considering that even humans may not be capable of conducting entirely impartial assessments. If the authors' objective is to develop LLMs that are free from human-like biases, I question the necessity of drawing comparisons against human evaluations.\n- This paper introduces a new benchmark, but when evaluating a new model, is it necessary to execute the entire process in this study, including generating a large number of pairwise preferences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8501/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8501/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8501/Reviewer_tfnE"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8501/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820589037,
            "cdate": 1698820589037,
            "tmdate": 1699673226040,
            "mdate": 1699673226040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1uikLFHZha",
                "forum": "TTEwosByrg",
                "replyto": "NrvgVNarMi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer tfnE (1/N)"
                    },
                    "comment": {
                        "value": "We thank the reviewer tfnE for dedicating their time and effort to evaluate our manuscript. In response to your valuable feedback, we carefully addressed and clarified the raised concerns and comments below.\n\n---\n\n**(Q1) \u201cI believe that most of the biases discussed have been previously identified\u2026\u201d**\n\nWe respectfully disagree with this. Although some biases, such as Order or Salience, have previously been identified in other works, we highlight the novelty in our work lies in confirming their presence in using LLMs as automatic evaluators in an extensive study of 6 different biases over 15 models. We believe that our focused, in-depth, and larger-scale exploration of these biases in LLMs as evaluators remains a very important research question to present the reliability and vulnerability of LLM evaluations.\nOverall, we believe the results of our benchmarking procedure provide \u201cseveral interesting insights\u201d (as highlighted by Reviewer 1K6b) that have not been previously explored at this scale and give valuable context to the current state of foundation models as automatic evaluators.\n\n\n---\n\n**(Q2) \u201cThe observed effect might closely resemble that of injecting random names.\u201d**\n\nOur intended evaluation setup for compassion fade was to measure its impact on the quality of model evaluations when presented with recognizable names compared to anonymized ones.  We conjecture that if models were fairly judging based on the quality of outputs rather than placing weight on the name of the response model, we would expect similar results (in terms of order and egocentric bias) to the ones observed in the Order bias experiments. As demonstrated in Table 1, the disparity between Order and Compassion Fade results supports our hypothesis that the presence of recognizable names indeed influences evaluations by each evaluator in contrast to anonymized ones.\n\n---\n\n**(Q3) \u201cObserving a correlation between the biased tendency and the percentage stated in the injected sentence could provide deeper insights.\u201d**\n \nThis is a valid point. In our additional experiment, we show a modified statistic for the biased model: \"0% of people prefer {model}.\u201d If bias tendency were indeed correlated with the statistic, we would expect the evaluator model to have 0 preference for bandwagon response. Due to limited computation resources and time, we ran additionals experiment on the bandwagon test with 0% statistic for two models at each size range (+ all API-based models) and present the results below: \n\n| Models  | GPT-4 | ChatGPT | InstuctGPT | Cohere | Alpaca | Vicuna | Baize | WizardLM | \n|:---------------------:|------:|---------|------------|--------|--------|--------|-------|----------|             \n| Bandwagon ($85\\\\%$)    | $0.0$ | $0.86$  | $0.85$     | $0.82$ | $0.75$ | $0.81$ | $0.82$| $0.76$   |                \n| Bandwagon (0\\\\%)  | $0.0$ | $0.0$   | $0.56$  | $0.0$  | $0.52$ | $0.79$ | $0.32$| $0.27$   |\n\nHere one can see that the preference choices for the bandwagon statistic greatly change (besides GPT4 and Vicuna) which suggests that indeed the biased tendency is correlated with the bandwagon statistic. However, we see that Vicuna, in particular, is not greatly affected by the statistics. This suggests that within the prompt, the model only focuses on the phrase \u201cpeople believe that {model} is better\u201d instead of the statistic. Similarly, this may be the case for Alpaca and InstructGPT as well.\nWe also present the results of the bandwagon test by randomly choosing a percentage between 50% and 85% below to support a correlation between biased tendency and the statistic. Most models show slightly lower bias preference as the statistic can range lower down to 50% that further supports a correlation: \n\n| Models   | GPT-4 | ChatGPT | InstuctGPT | Cohere | Alpaca | Vicuna | Baize | WizardLM | \n|:-------------------:|------:|---------|------------|--------|--------|--------|-------|----------|                            \n| Bandwagon ($85\\\\%$)   | $0.0$ | $0.86$  | $0.85$   | $0.82$ | $0.75$ | $0.81$ | $0.82$| $0.76$   |\n| Bandwagon ($50-85\\\\%$) | $0.06$| $0.70$  | $0.84$  | $0.65$ | $0.68$ | $0.96$ | $0.75$| $0.76$   |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8501/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711588073,
                "cdate": 1700711588073,
                "tmdate": 1700711588073,
                "mdate": 1700711588073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GHREtfPAQt",
                "forum": "TTEwosByrg",
                "replyto": "NrvgVNarMi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8501/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer tfnE (2/N)"
                    },
                    "comment": {
                        "value": "**(Q4) \u201cSome models display low rates of valid responses\u2026 Consequently, the claim that \"most LLMs exhibit cognitive biases\" is not sufficiently justified, given that weaker models may exhibit biases more frequently.\u201d**\n\nWe acknowledge that some models display inferior performance when extracting evaluations from them, which may skew our results for benchmarking their behaviors. \n\nHowever, we underline that uncovering a correlation between valid response rates and bias is not within the scope of our findings. If a model is not strong enough to produce valid outputs, we assume those models are not strong enough to be used for evaluations. Weaker models that were not good enough to produce valid inputs were not taken into consideration in this study. And as we don\u2019t consider invalid responses within the study, we emphasize that we claim that only from models that were strong enough to produce valid outputs, most models exhibit cognitive biases from our benchmark. We will add this additional clarity to the final version of the manuscript. \n\nAdditionally, we understand that some low valid response rates ($< 80\\\\%$) may not appear sufficient in precise benchmarking performance estimation and identifying cognitive biases in their evaluations; we would like to emphasize that we attempt to compensate for this by the scale of our experiments. We still observe that most (13/16) models on each benchmark provide a performance rate of $\\geq 50\\\\%$ that represents at least 2,625 samples evaluated, which we believe still provides a considerable source of evidence of some cognitive biases in the tested LLMs-as-evaluators. \n\n**(Q5) \u201c... The inter-annotator agreement (IAA) among the workers is not notably high in both ranking and pairwise tasks. The IAA aligns with the agreement between humans and machines. While the authors' claim that \"LLMs are still not suitable as fair and reliable automatic evaluators\" is not incorrect, it appears that humans may also fall short in this regard.\u201d**\n  \nWe thank you for your feedback and acknowledge that the current IAA score (0.47) between humans is not high compared to agreement amongst humans/machines. \n\nWe realize our current calculation procedure may not precisely capture human or each machine preference tendencies by simply aggregating all of them, which, as a result, produces higher agreement between machine and humans and may not be a fair representation of the IAA. Hence, we provide a correction to our previous calculation for a more precise result by computing the RBO between each individual annotator and machine preferences, obtaining an average RBO of **0.357** from $6 \\times 15$ different RBOs. This way, we present a more complete and accurate representation of the involved parties' preference behaviors and thus their agreements. We see that this correction further supports our claim that there is a misalignment between humans and LLMs, compared to IAA among humans. We will add these supplementary results from this new approach in the final draft. \n\nFor the pairwise task measuring biases in human preferences, we acknowledge that there would be lower IAA among humans, as the study involves 75 annotators for each bias (225 annotators) which we argue still achieves reasonable agreement given the complexity and scale of agreement to calculate between humans. Additionally, we clarify that the main objective of this human pairwise setup is to measure the impact on human evaluation quality via our bias benchmark rather than measuring agreement. The main takeaway is that humans generally were less impacted by most of the biases than LLMs which continue to support our original claims. We will clarify these intentions in the final version of the manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8501/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711722714,
                "cdate": 1700711722714,
                "tmdate": 1700712770481,
                "mdate": 1700712770481,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]