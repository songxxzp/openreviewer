[
    {
        "title": "Two Heads Are Better Than One: Exploiting Both Sequence and Graph Models in AMR-To-Text Generation"
    },
    {
        "review": {
            "id": "tW4f69Y2Mp",
            "forum": "61DYdiyQqk",
            "replyto": "61DYdiyQqk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7480/Reviewer_Koxu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7480/Reviewer_Koxu"
            ],
            "content": {
                "summary": {
                    "value": "The main contribution of the paper is to show that an ensemble of two established methods, sequence-to-sequence (s2s) and graph-to-sequence (g2s), performs better than either method by itself on two standard abstract meaning representation (AMR) benchmarks: AMR 2.0 and AMR 3.0.  The proposed ensemble method is called DualGen.\n\nThe paper shows results for a number of baselines including GPT-4.  It is interesting that the proposed method is considerably better than that on these benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is basically an incremental improvement over prior work (s2s and g2s).  It isn't surprising to see ensembles do well, but it is nice to see that that is the case. \n\nThe paper shows results for a number of baselines including GPT-4.  It is interesting that the proposed method is considerably better than that on these benchmarks."
                },
                "weaknesses": {
                    "value": "This paper is written for an audience that is already familiar with the literature on AMR.  One would hope that the result (two heads are better than one) might generalize beyond this specific case, but there is little evidence or discussion of that.\n\nI found the paper unnecessarily hard to read.  The lead sentence of the abstract doesn't make it clear that AMR is well-established in the literature, and many of the cited papers have hundreds/thousands of citations.  \n\nIt may be useful to compare the description of the problem and the task in https://aclanthology.org/P18-1150.pdf (one of the key cited papers) with the description in this submission.  This submission does not make it as clear that AMR is an established concept in the literature.  Nor is it as clear what the task is.  The cited paper follows the standard formula where there is a section labeled \"Experiments\" with a subsection on materials (5.1 data).   The submission has a short section (4.1 Datasets) with references to LDC.  This made it clear that this is a standard problem in the literature. But by the time I get to the discussion of the dataset, it should be clear what the task is, and how much work there is on this task.\n\nOne would hope that the incremental improvement would be at least as good as the prior art in describing the problem and motivating it, but I found it easier to do that by reading the references than by reading this submission."
                },
                "questions": {
                    "value": "Can you make this paper more accessible to an audience that is not already familiar with AMR?\n\nThe first part of the title (two heads are better than one) suggests that this result might generalize beyond this specific case (ensembling of s2s and g2s on two AMR benchmarks).  Is this paper limited to this specific case, or should it be of interest to a broader audience that may not be familiar with AMR?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Reviewer_Koxu"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697569124738,
            "cdate": 1697569124738,
            "tmdate": 1699636902458,
            "mdate": 1699636902458,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cl2ZkEzCXS",
                "forum": "61DYdiyQqk",
                "replyto": "tW4f69Y2Mp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Koxu"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful feedback. Please find the answers to some specific questions below.\n\n> Can you make this paper more accessible to an audience that is not already familiar with AMR?\n>\n> I found the paper unnecessarily hard to read. The lead sentence of the abstract doesn't make it clear that AMR is well-established in the literature, and many of the cited papers have hundreds/thousands of citations.\n>\n> This submission does not make it as clear that AMR is an established concept in the literature. Nor is it as clear what the task is.\n>\n> The submission has a short section (4.1 Datasets) with references to LDC. This made it clear that this is a standard problem in the literature. But by the time I get to the discussion of the dataset, it should be clear what the task is, and how much work there is on this task.\n\nThank you for pointing out that our paper is hard to read. In the first submitted version, we stated in the second sentence of the abstract that AMR-to-text is a well-studied task. In the revised version, we have added this point to the lead sentence.\n\nIn the first paragraph, we further clarified the definition of AMR in the revised version, explicitly stating that this is an established concept. Figure 1 illustrates an example of AMR.\n\nIn the second paragraph of the introduction, we introduced the AMR-to-text generation task more clearly in the revised version. We also use Figure 1 to illustrate an example of AMR-to-text generation, as previous studies have done. We then stated that our model DualGen is aimed to do AMR-to-text generation in the rest part of the Introduction more clearly.  \n\nIn Section 2 \"Related Work\", we introduced two types of previous methods on AMR-to-text generation, introducing that AMR-to-text is a widely-studied problem and there are a number of existing works exploring this topic.\n\nIn response to your feedback, we have revised some expressions in the abstract and introduction to enhance clarity and better articulate the established task.\n\n> The first part of the title (two heads are better than one) suggests that this result might generalize beyond this specific case (ensembling of s2s and g2s on two AMR benchmarks). Is this paper limited to this specific case, or should it be of interest to a broader audience that may not be familiar with AMR?\n\nDual encoder-decoder models have been widely used in NLP, as we stated in Section 2 \"Related Works\". Our main contribution is to:\n\n1. Demonstrate that this architecture is also effective for AMR-to-text, since no previous work has employed a dual encoder-decoder model in this task.\n2. Propose an innovative method for \"pretraining\" GNN encoders within the dual encoder-decoder framework, since previous studies showed that the pretraining technique is effective for language generative models and no prior work has explored pretraining a GNN for language tasks.\n\nAMR-to-text is a representative and valuable task for graph-to-text generation, and our method is useful in this setting. Because there is extensive work on dual encoder models consisting of un-pretrained GNN encoders, our method has the potential to be generalized beyond this specific case. We will try to add more experiments to prove that this method is also useful for other tasks later if our paper is accepted."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568562723,
                "cdate": 1700568562723,
                "tmdate": 1700568562723,
                "mdate": 1700568562723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nFwemtuCGG",
            "forum": "61DYdiyQqk",
            "replyto": "61DYdiyQqk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7480/Reviewer_PF3i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7480/Reviewer_PF3i"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new framework, DualGen, for AMR-to-text generation. The paper builds a new dual encoder-decoder model based on BART. Specifically, apart from the original BART sequence encoder, the paper introduces a new transformer-based graph encoder that takes in both node and edge embeddings as input. The graph attention mechanism incorporates edge embeddings into node representations. The paper then conducts experiments on both AMR 2.0 and AMR 3.0 datasets by comparing the proposed method with multiple state-of-the-art baselines. Following previous papers, the paper evaluates the results with BLEU, METEOR, and CHRF++. The paper also performs human evaluations. Additionally, the paper compared the proposed model with GOT-3.5 and GPT-4."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes a new dual-encoder architecture, combining graph representation and linearized input. The paper also proposes a new graph attention mechanism incorporating edge embeddings into the node representations.  \n2. The paper did comprehensive experiments with automatic and human evaluations on two datasets. The proposed methods surpass other state-of-the-art models in both automatic and human evaluations. In addition, the paper analyzes the relationship between graph complexity and model performance. It shows that the proposed method can capture the complexities of the graphs more effectively. The proposed method outperforms GPT4. The paper includes a case study with detailed output for readers to compare. \n3. The paper provides code, GPT4 results, silver data, etc."
                },
                "weaknesses": {
                    "value": "1. The idea is a little bit incremental. The graph encoder is built upon Song et al., 2020. The idea of the dual encoder is also not new. For example, OntG-Bart (Sotudeh & Goharian, 2023) also uses a GNN and BART encoder-decoder framework to generate summarization. The dual encoder has also been applied to the multimodal domain (Li et al., 2021). \n2. It would be better for authors to include an additional ablation study for the proposed method instead of only showing the final model. In this way, readers can have a better understanding of the contribution of each component. The analysis in section 4.5 is superficial and needs additional in-depth analysis. The authors can include additional quantitative analysis by analyzing the types of failures for each model.  \n3. Some parts of the paper are not clear. The paper fails to report the inter-annotator agreement for the human evaluations. The paper also includes typos. Typo in \"Metor\"-> METEOR\n\n\nLi, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., & Hoi, S. C. H. (2021). Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34, 9694-9705. https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf\nSotudeh, S., & Goharian, N. (2023, August). OntG-Bart: Ontology-Infused Clinical Abstractive Summarization. In Proceedings of the ACM Symposium on Document Engineering 2023 (pp. 1-4). https://dl.acm.org/doi/abs/10.1145/3573128.3609346"
                },
                "questions": {
                    "value": "Could the authors elaborate on how each encoder contributes to the overall performance of the model? Would the authors consider conducting an ablation study? Specifically, it would be insightful to demonstrate that the pretrained encoder effectively handles Out-Of-Domain (OOD) test cases, while the graph encoder adeptly captures structural information."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Reviewer_PF3i"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698551740826,
            "cdate": 1698551740826,
            "tmdate": 1700696026082,
            "mdate": 1700696026082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fsNKQWoCkX",
                "forum": "61DYdiyQqk",
                "replyto": "nFwemtuCGG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PF3i (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful feedback. Please find the answers to specific questions below.\n\n> The idea is a little bit incremental. The graph encoder is built upon Song et al., 2020. The idea of the dual encoder is also not new. For example, OntG-Bart (Sotudeh & Goharian, 2023) also uses a GNN and BART encoder-decoder framework to generate summarization. The dual encoder has also been applied to the multimodal domain (Li et al., 2021).\n\nWe agree that dual encoders - one decoder is a common approach in NLP. However, we want to emphasize that our paper goes beyond the conventional use of a dual-encoder framework.\n\nIn NLP, employing dual encoders with different architectures involves combining a pretrained Transformer-like sequence encoder with a non-pretrained encoder (e.g., GNN). This is also the case in OntG-Bart (Sotudeh & Goharian, 2023),  with a pretrained BART encoder and a non-pretrained GAT encoder.\n\nWhile GNNs are frequently employed in NLP dual encoder models, no prior work has focused on pretraining the GNN specifically for language-related tasks.\n\nIn our work, **we introduce a distinctive approach by \"pretraining\" the GNN encoder**. This involves using pretrained parameters from the BART encoder to initialize our specially designed GNN, which is different from the established dual-encoder paradigm. This initialization is possible because we intentionally designed our GNN to resemble the Transformer architecture.\n\nOur work is significant because:\n\n1. GNN is powerful in NLP tasks. However, there is no existing pretrained GNN for language tasks that provides a resource for direct use.\n2. GNNs, unlike PLMs with Transformer architecture, are usually incompatible with being trained on a large corpus\n3. As far as we know, no previous work has concentrated on pretraining the GNN component in dual-encoder models to enhance its effectiveness. This implies that our work has the potential to generalize to all Transformer-based dual encoder models in NLP where one of the encoders is a GNN.\n\nIn response to your feedback, we have revised the Abstract, 1 Introduction, 3.2 Model Architecture, and 5 Conclusion in our paper to explicitly articulate our contribution to \"pretraining\" a GNN for language tasks.\n\nReference:\n\n- Sotudeh, S., & Goharian, N. (2023, August). OntG-Bart: Ontology-Infused Clinical Abstractive Summarization. In Proceedings of the ACM Symposium on Document Engineering 2023 (pp. 1-4). https://dl.acm.org/doi/abs/10.1145/3573128.3609346\n\n> The idea is a little bit incremental. The graph encoder is built upon Song et al., 2020.\n\n**Although we used the idea of vectorized structural information in** **Song et al., 2020, there are essential differences between our encoder and Song's:**\n\n1. We use a different method to generate node embeddings, utilizing Bart vocabulary and using the average value, rather than defining a special in-domain vocabulary for AMR\n2. We use the Bart word embeddings and two additional linear projections for vectorized structural information, rather than using sequences of edges and specially defined tokens\n3. We use a different attention mechanism, which resembles the original Transformer rather than Song et al., 2020.\n\nThese differences ensure that the architecture of DualGen's graph encoder can be initialized with Bart parameters, and the graph encoder can integrate with the other part of the model seamlessly.\n\nReference:\n\n- Song, Linfeng, et al. (2020). Structural Information Preserving for Graph-to-Text Generation. ACL. https://aclanthology.org/2020.acl-main.712/"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568198476,
                "cdate": 1700568198476,
                "tmdate": 1700568198476,
                "mdate": 1700568198476,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0I5qVmkig6",
                "forum": "61DYdiyQqk",
                "replyto": "nFwemtuCGG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PF3i (2/3)"
                    },
                    "comment": {
                        "value": "> Could the authors elaborate on how each encoder contributes to the overall performance of the model? Would the authors consider conducting an ablation study? Specifically, it would be insightful to demonstrate that the pretrained encoder effectively handles Out-Of-Domain (OOD) test cases, while the graph encoder adeptly captures structural information.\n>\n> It would be better for authors to include an additional ablation study for the proposed method instead of only showing the final model. In this way, readers can have a better understanding of the contribution of each component.\n\nIn our first version, we did perform an ablation study. We apologize for not explicitly mentioning that one baseline is equivalent to an ablation study.\n\nThe ablation study involves three models besides DualGen: DualGen without graph encoders, DualGen without sequence encoders, and DualGen with un-pretrained graph encoders. We reported results for DualGen without graph encoders in the first version. We have included the outcomes for the other two variants in the revised version.\n\n1. DualGen without graph encoders\n\nIn Table 2, the baseline \"Ribeiro et al. (2021a)\" involves training BART with only vocabulary modifications, which is the same as DualGen without graph encoders. This model is also used in Section 4.6 where we analyze the impact of graph complexity. We also verified the results by training DualGen without graph encoders using our own code, producing similar results to Ribeiro et al. (2021a). Our omission in clearly stating that \"Ribeiro et al. (2021a)\" is the same as our model without graph encoders was an oversight on our part.\n\n2. DualGen without sequence encoders\n\nThe graph encoder in our model is designed not to take all information, but to focus on edge relations. The performance of DualGen without sequence encoders is worse (BLEU ~23) than the full model (BLEU ~48), aligning with our expectations.\n\n3. DualGen with un-pretrained graph encoders\n\nThe graph encoder of DualGen is large. The AMR train sets are small, which are inadequate to train the GNN from scratch. We have tested DualGen with un-pretrained graph encoders, which means the GNN is trained from scratch while the rest parts use Bart parameters. The result shows that the model learns nothing (BLEU < 2). This is not surprising considering the model size.\n\nIn response to your feedback, we have modified sections 4.3 Compared Models and 4.5 Main Results to enhance clarity regarding Ribeiro et al. (2021a). Considering the limited information provided by the ablation study and the 9-page limit of ICLR, we included the ablation results, evaluated all by our own code rather than using results reported by Ribeiro et al. (2021a), in Appendix A in the revised pdf.\n\nReferences:\n\n- Ribeiro, Leonardo FR, et al. (2020) Investigating pre-trained language models for graph-to-text generation. NLP4ConvAI. https://aclanthology.org/2021.nlp4convai-1.20/"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568340068,
                "cdate": 1700568340068,
                "tmdate": 1700568340068,
                "mdate": 1700568340068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MKNYXvlaqm",
                "forum": "61DYdiyQqk",
                "replyto": "nFwemtuCGG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PF3i (3/3)"
                    },
                    "comment": {
                        "value": "> The analysis in section 4.5 is superficial and needs additional in-depth analysis. The authors can include additional quantitative analysis by analyzing the types of failures for each model.\n\nWe added Section 4.7 \"Model Failure\" to the revised pdf, examining the failures of three models: Guo et al. (2019) (g2s), Ribeiro et al. (2021a) (s2s), and our model. Ribeiro et al. (2021a) shares the same architecture and method as Bart, which is the same as our model without the graph encoders.\n\nWe consider entries with a BLEU score lower than 25 as failed cases. We analyzed the graph size (indicated by edge number and node number), reentrance node number, and graph depth for failed cases of three models. We presented the results in Table 3 in the revised pdf and analyzed the results in Section 4.7 \"Model Failure\".\n\nReferences:\n\n- Guo, Zhijiang, et al. (2019). Densely connected graph convolutional networks for graph-to-sequence learning. TACL. https://aclanthology.org/Q19-1019/\n\n- Ribeiro, Leonardo FR, et al. (2020) Investigating pre-trained language models for graph-to-text generation. NLP4ConvAI. https://aclanthology.org/2021.nlp4convai-1.20/\n\n> Some parts of the paper are not clear. The paper fails to report the inter-annotator agreement for human evaluations. The paper also includes typos.\n\nThank you for pointing out the need to add necessary details for the human evaluation experiment. In response to your feedback, we have included additional information on human evaluations in Appendix B. We also have corrected typos."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568408785,
                "cdate": 1700568408785,
                "tmdate": 1700568408785,
                "mdate": 1700568408785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6EjbiNWXG9",
                "forum": "61DYdiyQqk",
                "replyto": "nFwemtuCGG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7480/Reviewer_PF3i"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7480/Reviewer_PF3i"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response to my questions! The authors have addressed most of my questions. I raised my score to 6. However, the addition of a non-pretrained GNN variant ablation would greatly enhance the authors' claim of novelty."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696014898,
                "cdate": 1700696014898,
                "tmdate": 1700696178668,
                "mdate": 1700696178668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q8wL8E04KT",
                "forum": "61DYdiyQqk",
                "replyto": "jqyoDQEQ5e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7480/Reviewer_PF3i"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7480/Reviewer_PF3i"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your clarification. I think all of my questions have been answered."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725193105,
                "cdate": 1700725193105,
                "tmdate": 1700725193105,
                "mdate": 1700725193105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jL3YbZgfl8",
            "forum": "61DYdiyQqk",
            "replyto": "61DYdiyQqk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7480/Reviewer_FACa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7480/Reviewer_FACa"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors focus on the AMR-to-text generation task.  To leverage the advantages of PLMs and GNNs, the paper proposes a dual encoder-decoder model called DualGen, which integrates a specially designed GNN into a pre-trained sequence-to-sequence model. The paper presents extensive experiments, human evaluations, and a case study, showing that DualGen achieves state-of-the-art performance in AMR-to-text generation tasks and outperforms the GPT-4."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation for this paper is intuitive and the description of the proposed methodology is clear and comprehensible.\n\n2. The proposed method exhibits compatible with any encoder-decoder architecture pre-train models. \n\n3. The authors conduct an exhaustive comparsion, and the experimental results show the proposed method outperforms all previous work."
                },
                "weaknesses": {
                    "value": "1. Actually, the multi-source structure (multiple encoders - one decoder) is a common approach in many NLP tasks. Even this paper does present some enhancements for the AMR-to-text generation task, the novelty of the proposed method appears to be somewhat constrained.\n\n2. A commendable aspect of this paper is the authors' comparison of performance with GPT-4. However, considering that GPT-4 is a general model, the comparison may not be entirely equitable. It would be more appropriate to utilize an open-source large language model (e.g., Llama) for the experiment. This is crucial to verify the effectiveness of the proposed method in the context of Large Language Models."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7480/Reviewer_FACa"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7480/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890130956,
            "cdate": 1698890130956,
            "tmdate": 1699636902171,
            "mdate": 1699636902171,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sKsMLjTHqH",
                "forum": "61DYdiyQqk",
                "replyto": "jL3YbZgfl8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7480/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FACa"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful feedback. Please find the answers to some specific questions below.\n\n> Actually, the multi-source structure (multiple encoders - one decoder) is a common approach in many NLP tasks. Even this paper does present some enhancements for the AMR-to-text generation task, the novelty of the proposed method appears to be somewhat constrained.\n\nWe agree that multiple encoders - one decoder is a common approach in NLP. However, we want to emphasize that our paper goes beyond the conventional use of a multi-encoder framework. \n\nWhile GNNs are frequently employed in NLP multiple encoder models, no prior work has focused on pretraining the GNNs specifically for language-related tasks.\n\nIn our work, **we introduce a distinctive approach by \"pretraining\" the GNN encoder.** We use parameters from BART to initialize the special GNNs in DualGen. This is different from the previous multi-encoder paradigm.\n\nOur work is significant because:\n\n1. GNN is powerful in NLP tasks. However, there is no existing pretrained GNN for language tasks that provides a resource for direct use.\n2. GNNs, unlike PLMs with Transformer architecture, are usually incompatible with being trained on a large corpus\n3. As far as we know, no previous work has concentrated on pretraining the GNN component in multi-encoder models to enhance its effectiveness. This implies that our work has the potential to generalize to all Transformer-based multiple encoder models in NLP where one of the encoders is a GNN.\n\nIn response to your feedback, we have revised the Abstract, 1 Introduction, 3.2 Model Architecture, and 5 Conclusion in our paper to explicitly articulate our contribution to \"pretraining\" a GNN for language tasks.\n\n> A commendable aspect of this paper is the authors' comparison of performance with GPT-4. However, considering that GPT-4 is a general model, the comparison may not be entirely equitable. It would be more appropriate to utilize an open-source large language model (e.g., Llama) for the experiment. This is crucial to verify the effectiveness of the proposed method in the context of Large Language Models.\n\nWe believe GPT-4 outperforms all other unfine-tuned LLMs, so we didn't conduct experiments on other unfine-tuned models. As for fine-tuning open-source LLMs on AMR2text, we believe it's not a fair comparison considering the model size and training cost.\n\nOur reason for comparing DualGen with LLMs is that **using small models for AMR2text is still valuable** when LLMs are easily accessible, because un-finetuned LLMs perform badly, and fine-tuning LLMs is expensive. For better model performances, fine-tuning an LLM with a higher cost can always be an option.\n\nIt's worth noting that the DualGen method can extend to LLMs as long as they are open-source and Transformer-based. We plan to add an experiment to fine-tune a dual-encoder Llama with LoRA if our paper is accepted. We apologize for not being able to complete this during this rebuttal session.\n\nIn response to your feedback, we have revised the part \u201c4.9 Comparison With The Most Powerful PLMs\u201d in our paper to state our opinions more clearly."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7480/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568029763,
                "cdate": 1700568029763,
                "tmdate": 1700568029763,
                "mdate": 1700568029763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]