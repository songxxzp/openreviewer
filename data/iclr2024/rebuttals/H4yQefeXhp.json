[
    {
        "title": "DMV3D: Denoising Multi-view Diffusion Using 3D Large Reconstruction Model"
    },
    {
        "review": {
            "id": "0uAwO5JwiM",
            "forum": "H4yQefeXhp",
            "replyto": "H4yQefeXhp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission753/Reviewer_X5Dv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission753/Reviewer_X5Dv"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes DMV3D, a 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. The reconstruction model incorporates a triplane NeRF representation and, functioning as a denoiser, can denoise noisy multi-view images via 3D NeRF reconstruction and rendering, achieving single-stage 3D generation in the 2D diffusion denoising process. The model is trained on large-scale multi-view image datasets of extremely diverse objects using only image reconstruction losses, without accessing 3D assets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The first contribution of this paper is to scale 3D diffusion generative models to very diverse categories and objects. Previous models such as DiffRF, etc. can only generalize within some shapenet like datasets with no more than 13 categories.\n\n2. The model demonstrates a novel method to conduct multiview diffusion. Instead of build attentions across views like mvdreamer or syncdreamer, they use attention to attend with learnable triplane tokens and with each other, therefore incorporating the 3D spatial prior in the process.\n\n3. The model shows good results of 3d generation, especially high quality geometry, which alwyas fail in SDS or nerf2nerf lines of works."
                },
                "weaknesses": {
                    "value": "1.It seems the model learns from the objaverse and mvimagnet, which contain mostly single objects or separated objects. even the examples in out of domain results, in figure 6, the objects are not complicated as people use in SD-based models."
                },
                "questions": {
                    "value": "1. As mentioned in weakness 1,  I would like to see some results of \"bunny seating on pancake\", this kind of generation. Even it is hard to do text to 3d, since the training set doesn't have compound objects, is it possible to do 2d conditioned 3d generation with this kind of prompt?\n\n2. The author mentioned in the 2d conditioned 3d generation task, they do not add noise to the reference view, however, some of other diffusion models usually also add noise to the reference view and each step, use the gt x0 of the that view and add new noise in ancestral sampling. The logic behind is the model is trained with noise images paired with the corresponding time step embedding, the clean image strategy will shock the model in inference. I wonder, in inference, if this clean ref image strategy can bring benefit over adding noise from x0."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Reviewer_X5Dv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698048121950,
            "cdate": 1698048121950,
            "tmdate": 1700471632900,
            "mdate": 1700471632900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3VWPGZ9hTH",
                "forum": "H4yQefeXhp",
                "replyto": "0uAwO5JwiM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your appreciation of our method\u2019s capability to handle diverse objects, our novelty to use a reconstructor as a multi-view denoiser, and our high-quality results. We address your questions below. \n\n1. **Complex prompts**: our text-to-3D model has limited capability to handle complex prompts due to the limited scale of multi-view data. But as the reviewer points out, we can use Stable Diffusion to generate a single image from a complex prompt, and then use our image-conditioned model to lift it into 3D. We show the results here (https://dmv3d.github.io/rebuttal_sdcond.html). \n\n2. **Noise in reference image**: we tried adding noise to reference images, but found that this is essentially equivalent to unconditional generation, causing the model to less respect the respect image. Hence, we choose to use the noise-free reference image across all timesteps."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469216813,
                "cdate": 1700469216813,
                "tmdate": 1700469216813,
                "mdate": 1700469216813,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n19n88Viae",
                "forum": "H4yQefeXhp",
                "replyto": "3VWPGZ9hTH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_X5Dv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_X5Dv"
                ],
                "content": {
                    "comment": {
                        "value": "I'm in general satisfied with the authors' answers which address all my concerns. I would like to raise my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471694177,
                "cdate": 1700471694177,
                "tmdate": 1700471694177,
                "mdate": 1700471694177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WM20uVC8qi",
                "forum": "H4yQefeXhp",
                "replyto": "0uAwO5JwiM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe sincerely thank the reviewer for your feedback and your appreciation of our methods and results.\nAnd we would like to express our gratitude for raising the score of our paper!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558817622,
                "cdate": 1700558817622,
                "tmdate": 1700558961333,
                "mdate": 1700558961333,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fmP5tKmVP1",
            "forum": "H4yQefeXhp",
            "replyto": "H4yQefeXhp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission753/Reviewer_PYdg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission753/Reviewer_PYdg"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method to generate novel 3D objects based on a diffusion model that encloses a large reconstruction model. To leverage the strong generative power of 2D models while improving the 3D consistency of the generated objects, the diffusion model operates on the domain of multi-view images and internally learns a transformer-based reconstruction model to build a 3D representation, which is later rendered into denoised output images. In order for the model to generalize across different categories, the reconstruction model uses the DINO features to bootstrap the features used for deriving tokens. Results show that by training jointly on the Objaverse dataset and MVImageNet dataset, the model is able to generate diverse shapes, conditioned either on images or texts. The usage of the transformer-based large reconstruction model improves the 3D consistency while maintaining a good generation quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly written and well presented. The notations are clear and the illustrations are informative. It's easy to read and understand most of the technical details and design choices.\n- Though conceptually similar to, e.g., MVDiffusion and RenderDiffusion (or diffusion with forward models), the method elegantly combines the advantages of both works with the help of a generalizable large reconstruction model using transformers, hence lifting the previous constraints within only one single category.\n- The method naturally enables conditioning over images by fixing the diffusion variables, leading to a new scheme for bridging 2D and 3D domains.\n- The generated shapes are of high quality and surpass the baselines by a considerable margin."
                },
                "weaknesses": {
                    "value": "- In contrast to Image-based diffusion models, the runtime efficiency might be compromised since the reconstruction model operates during every iteration of sampling. Investigating whether the reconstruction can be repurposed or distributed over the intermediate denoising phases could be insightful, especially since the current intermediate reconstructed model is discarded (would be great if they could be visualized), leading to potential wastage.\n\n- The textures produced lack sharpness. Exploring the proposed framework's performance on higher-resolution images and 3D triplanes would be intriguing. Additionally, employing a hybrid representation that decouples geometry and textures could yield enhanced results."
                },
                "questions": {
                    "value": "- How the camera viewpoints are sampled during the training process? Would the reconstruction model easily fall into a local minima where the 3D results become trivial by generating planes that are parallel to the image planes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Reviewer_PYdg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633163695,
            "cdate": 1698633163695,
            "tmdate": 1699636002527,
            "mdate": 1699636002527,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ouq8uvSn21",
                "forum": "H4yQefeXhp",
                "replyto": "fmP5tKmVP1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your appreciation of our clear presentation, method\u2019s elegance and high-quality results. We address your concerns and questions below. \n\n1. **Runtime efficiency**: we agree with the reviewer that it might be a good idea to interleave geometry-based multi-view denoising and geometry-free multi-view denoising for best runtime efficiency, as rendering 4 views from a triplane at each denoising step does have a negative influence on the inference speed. We leave this point for future exploration. We also visualize the intermediate reconstructed results during denoising process at this link (https://dmv3d.github.io/rebuttal_x0vis.html).\n\n2. **Blurry texture**: we agree with the reviewer that our generated textures for unseen parts are a bit blurry at its current stage, although we outperform baselines. For future work, it\u2019s interesting to explore increasing image and triplane resolution and decoupling geometry and texture modeling to address this issue. \n\n3. **Viewpoint sampling**: we randomly sample 4 views during training to use as our input views, and another randomly selected 4 novel views as supervision. During inference, we sample 4 structured viewpoints around an object to denoise. \n\n4. **Degenerate solution**: we use novel view supervision to prevent the degenerate solution of generating 4 flat planes to explain the input views (see table 3). The results without novel view supervision can be found at this link (https://dmv3d.github.io/rebuttal_wonovelview.html)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469058912,
                "cdate": 1700469058912,
                "tmdate": 1700469853060,
                "mdate": 1700469853060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z1zsEhLQbC",
                "forum": "H4yQefeXhp",
                "replyto": "fmP5tKmVP1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe kindly request your feedback on whether our response has addressed your concerns. If you have any remaining questions or concerns, we are more than happy to address them. Thank you for your time and consideration!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557983051,
                "cdate": 1700557983051,
                "tmdate": 1700558082200,
                "mdate": 1700558082200,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BJfGVhUBQy",
            "forum": "H4yQefeXhp",
            "replyto": "H4yQefeXhp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission753/Reviewer_nAvZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission753/Reviewer_nAvZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach to 3D generation via a single-stage diffusion model. By denoising multi-view image diffusion, the authors aim to generate realistic 3D assets. Central to this methodology is a large transformer model that processes multi-view noisy images to reconstruct a clean triplane NeRF, subsequently yielding denoised images through neural rendering. The proposed method showcases flexibility, supporting both text- and image-conditioning inputs, and claims rapid 3D generation without requiring per-asset optimization. The approach is evaluated and shown to be superior to previous 3D diffusion models in certain domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(+) The paper showcases impressive results in 3D generation compared to prior methods. \n\n(+) The method's ability to accommodate text- and image-conditioning inputs augments its versatility, making it potentially suitable for diverse applications.\n\n(+) The paper is well-structured and clearly explains both the methodology and foundational design choices."
                },
                "weaknesses": {
                    "value": "Although the paper showcases promising results and a solid methodology; however, its level of novelty is unclear:\n\n- It appears that the proposition combines techniques that have been used before. The 3D diffusion part of the proposal seems to have been influenced by \"Viewset Diffusion (ICCV 2023)\", while the design and training approach of the large-scale transformer model is similar to \"LRM: LARGE RECONSTRUCTION MODEL FOR SINGLE IMAGE TO 3D\", which was also submitted to ICLR 2024.\n- Concerns have been raised about potential overlap with the LRM manuscript, questioning submission singularity.\n\nBesides, a balanced perspective is lacking due to the absence of discussion on the paper's limitations, which could provide valuable insights for potential areas of improvement."
                },
                "questions": {
                    "value": "- Given the inherent training characteristics of diffusion models, how does DMV3D achieve a training timeframe analogous to LRM? It would be helpful if the authors could provide an explanation.\n- It is important to clarify the extent of overlap between this work and the LRM submission in order to understand the distinctiveness of the contributions in this manuscript."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission753/Reviewer_nAvZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846627682,
            "cdate": 1698846627682,
            "tmdate": 1700484738472,
            "mdate": 1700484738472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mlhAyw2iYJ",
                "forum": "H4yQefeXhp",
                "replyto": "BJfGVhUBQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your appreciation of our impressive results, our method\u2019s great versatility and clear presentation. Please see our responses to your concerns below.\n\n1. **DMV3D vs LRM**: We would like to clarify the different contributions of LRM and DMV3D. Please note that, in DMV3D, we acknowledge LRM as prior work and have explicitly cited LRM in section 3.2. Our work makes orthogonal contributions,  focusing on different challenges that are not covered by LRM. In particular, LRM focuses on the task of single-image reconstruction only and introduces the first scalable transformer model trained on massive multi-view data. However, in essence, LRM is a deterministic model that cannot model the inherent uncertainty (multiple plausible solutions exist for unseen parts of the 3D objects) in single-image 3D reconstruction. In contrast, DMV3D aims to build a general probabilistic diffusion model for 3D generation. Our framework is not only applicable to single-image reconstruction but also text-to-3D generation \u2013 a task completely uncovered by LRM.   \n\n    To do so, we propose a novel single-stage multi-view diffusion framework and adopt a transformer model to achieve multi-view denoising. Again, please note that we have cited LRM (by anonymously putting it in the supplemental material) and clearly stated in Sec.3.2 that our transformer architecture is inspired by LRM to avoid any misconception about dual submission. We emphasize that the whole story of our paper, including our entire method section (Sec. 3), does not focus on the original transformer contribution made by LRM. Instead, our focus is mainly on: 1) how to build a multi-view diffusion framework for 3D generation (Sec. 3.1), 2) how to improve/extend the transformer model to support multi-view denoising (Sec. 3.2), and 3) how to enable both text and image conditioning (Sec. 3.3). These aspects are distinct from the scope of LRM. Furthermore, our transformer architecture is also different from the one in LRM in taking noisy multi-view images as input, being conditioned by the diffusion time step, utilizing a new ray-conditioning technique to better handle the diverse camera intrinsics, and supporting text conditioning.\n\n    As a result, even for the same single-image reconstruction problem, DMV3D and LRM address it in very different ways. DMV3D leverages multi-view diffusion denoising, whereas LRM  performs single-view image-to-NeRF translation. The transformer inputs (multi-view noisy images vs a single-view clean image) and the inference processes (DDIM-based multi-step diffusion denoising vs single-step transformer inference) are highly different. Moreover, because of being a probabilistic generative model, DMV3D can generate multiple high-quality 3D assets given the same single image input (as shown in Fig. 1 and our project page), while LRM can only generate one single result. The probabilistic modeling introduced by DMV3D also enhances the realism and reduces the blurriness of generated textures for unseen parts; in contrast, LRM tends to generate blurred results due to mode averaging. \n\n    In sum, different from the deterministic model in LRM, our DMV3D introduces a novel probabilistic model for 3D generation and contributes 1) a novel multi-view diffusion framework 2) the general support for both text and image conditioning and  3) new architecture designs to support multi-view denoising and text/image conditioning. The contributions are made orthogonal to those of LRM and we believe these are valuable contributions to the field towards a general, practical, and high-quality 3D generative model.\n\n2. **Novelty**: As discussed in the related work, we are inspired by the RenderDiffusion work for using a reconstructor as a denosier. Concurrent work, Viewset Diffusion, adopts the same idea from RenderDiffusion, but was only demonstrated to work on limited data lacking diversity. Our novel transformer-based 3D denoiser architecture is different from the reconstructor architecture in viewset diffusion and enables state-of-the-art results for scalable, diverse, and high-quality 3D generation.\n\n3. **Limitations**: one limitation with our text-to-3D model is that we cannot handle complex text prompts as well as 2D diffusion models like stable diffusion, as a result of the limited scale of multi-view data compared with 2D data. Another limitation is that the generated textures at unseen views are still a bit blurry compared with input view. It will be interesting to further boost the quality of texture sharpness in future work. \n\n4. **Training time**: despite our method being a diffusion model, we only sample a single time-step at a time to train our multi-view denoiser. This single-step setting is similar to what LRM does except our input images are noisy; hence our model trains as fast as LRM during training."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468837333,
                "cdate": 1700468837333,
                "tmdate": 1700469690972,
                "mdate": 1700469690972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TODNCCltlj",
                "forum": "H4yQefeXhp",
                "replyto": "mlhAyw2iYJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_nAvZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_nAvZ"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to express my appreciation to the authors for their detailed and thoughtful rebuttal. In the response, the authors have clarified several important points that address the concerns raised in my original review.\n\nFirst and foremost, I want to reaffirm my understanding and appreciation of the contributions made by the paper, particularly in introducing the multi-view denoising framework for 3D generation. The promising results that were presented in the paper are worth noting. I also note that the authors had made appropriate citations to related anonymous submissions, namely LRM and Instant3D.\n\nOne of my primary concerns in the original review was regarding the positioning of LRM in relation to the present paper. The authors now clearly acknowledged LRM as prior work in the rebuttal, which alleviates many of my concerns. However, __treating LRM as a prior work also introduces certain expectations__. For instance, when claiming that the proposed method in DMV3D can potentially achieve better results in unseen area reconstruction compared to LRM, it might be beneficial to support such claims with experimental or visual comparison. While the generative formulation in DMV3D seems theoretically capable of addressing the averaging issue in unseen areas, empirical validation would enhance the paper's credibility.\n\nIn conclusion, I acknowledge that DMV3D offers valuable insights to the research community and addresses many of the concerns raised in the original review, including the overlap with LRM. Given these clarifications and improvements, I am inclined to raise my rating for this submission. I would also like to recommend that the authors consider incorporating the discussion related to LRM into the main paper. Additionally, as the scenario of two closely related submissions within the same conference is relatively uncommon, I would also love to hear the comments from other reviewers."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484713114,
                "cdate": 1700484713114,
                "tmdate": 1700484713114,
                "mdate": 1700484713114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oL0ZLtFk2x",
            "forum": "H4yQefeXhp",
            "replyto": "H4yQefeXhp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission753/Reviewer_3SCV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission753/Reviewer_3SCV"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a 3D generation method that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. The proposed method supports both text- and image-conditioned 3D generation. Experimental results seem promising."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of directly denoising a triplane-based NeRF is interesting. The result of multi-view diffusion is promising."
                },
                "weaknesses": {
                    "value": "1. Does the method use a pre-trained stable diffusion model or train the DDPM from scratch? If from scratch, how is the generalization ability guaranteed?\n\n2. For multi-view diffusion, what is the number of views for training and inference?"
                },
                "questions": {
                    "value": "Please see the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699137686233,
            "cdate": 1699137686233,
            "tmdate": 1699636002396,
            "mdate": 1699636002396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PIcMuOQVTy",
                "forum": "H4yQefeXhp",
                "replyto": "oL0ZLtFk2x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your appreciation of our method and result. We address your questions below.\n\n1. Train from scratch: all of our models are trained from scratch without using pretrained diffusion models. Generalization of our model is backed up by large-scale training on the massive multi-view data including Objaverse and MvImgNet (See Fig. 7). \n\n2. Number of views: we use 4 views during training and inference (see section 4.3)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468698359,
                "cdate": 1700468698359,
                "tmdate": 1700468717564,
                "mdate": 1700468717564,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fp0nHB5uO0",
                "forum": "H4yQefeXhp",
                "replyto": "oL0ZLtFk2x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe kindly request your feedback on whether our response has addressed your concerns. If you have any remaining questions or concerns, we are more than happy to address them. Thank you for your time and consideration!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557958524,
                "cdate": 1700557958524,
                "tmdate": 1700558095008,
                "mdate": 1700558095008,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EBrG8T1V3a",
                "forum": "H4yQefeXhp",
                "replyto": "pnMG7PUbYA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_3SCV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_3SCV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. There are still some concerns compared to some concurrent works.\n1. The technical contribution of this work. Compared to LRM, this paper wraps the triplane-NeRF into a diffusion process. What is the motivation for this choice?\n2. In terms of multi-view consistency, it is also beneficial to compare with SyncDreamer [1], which seems very competitive in consistency. \n\n[1] SyncDreamer: Generating Multiview-consistent Images from a Single-view Image, arXiv, Sep 7."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689314402,
                "cdate": 1700689314402,
                "tmdate": 1700689314402,
                "mdate": 1700689314402,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]