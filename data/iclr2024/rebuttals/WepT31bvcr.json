[
    {
        "title": "On Compositional Generalization in Language Models"
    },
    {
        "review": {
            "id": "JJSRdm4lhR",
            "forum": "WepT31bvcr",
            "replyto": "WepT31bvcr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2521/Reviewer_dEnH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2521/Reviewer_dEnH"
            ],
            "content": {
                "summary": {
                    "value": "The authors create a dataset of boolean expressions, and feed it to LLMs to get the value for the boolean expression, along with the steps used to arrive at that value i.e. the proof. For example, the input expression $(((1 \\land \\neg 0 ) \\lor \\neg 1)\\lor 1)$ should give an output of a set of strings resolving the innermost bracket in each step, finally giving out the final value. In this case, the expected output is $((1 \\lor \\neg 1)\\lor 1)$, $(1\\lor 1)$, 1.  The authors aim to understand LLM's compositional generalization by understanding their performance on such examples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Clear presentation"
                },
                "weaknesses": {
                    "value": "- Originality: The idea is somewhat original, in testing LLMs generalization capacity through boolean expressions.\n\n- Significance: The results have minimal to no impact on current understanding of LLMs. The boolean expressions proposed by authors are not some interesting fragment of logic, and neither do they represent any interesting NLP task. This can be simply seen as booleanized and very restricted version of  Chain of Thought reasoning, which works for a few steps and then does not work for deeper reasoning tasks, which is what the authors conclude."
                },
                "questions": {
                    "value": "-  Have you tried boolean expressions which may have more than one proof?\n- In what way can your results aid designing new methods --- prompts, training or theory--- for LLMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2521/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2521/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2521/Reviewer_dEnH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674697466,
            "cdate": 1698674697466,
            "tmdate": 1699636188672,
            "mdate": 1699636188672,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "21ZZMkz8Xp",
            "forum": "WepT31bvcr",
            "replyto": "WepT31bvcr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2521/Reviewer_PEvF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2521/Reviewer_PEvF"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows that large language models (a) fail to generalize the compositional structure and (b) often fail not only in calculating formulas but in grasping the input structure itself.\nThe aim is to ascertain whether a language model can employ basic elements to construct a logical framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is asking a very interesting and fundamental question about LLMs.\n\nThe methods used are basically sound."
                },
                "weaknesses": {
                    "value": "I am confused about the underlying tasks. The paper addresses 2 totally different tasks:\n\n1.  What can an LLM represent in relation to logic relations\n\n2. What proof properties exist for an LLM.\n\nThe main body of the paper lacks a proper theory for the language model and the notion of compositional reasoning in models.\n\nThe boolean representation you use should be defined at the beginning, and not in sec. 4.1 (Experiments). \nIt seems like this is a more general representation of formulae, like an abstract syntax tree (AST). Here, the tree represents the abstract syntactic structure of text  written in a formal language. Each node of the tree denotes a construct occurring in the text.\n\nI would suspect that using the AST is closest to what an LLM creates. Is this the intention? If so, then this compromises any proof methods.\n\nThe claim \"The advantage of representing boolean algebra as a tree lies in dividing the boolean algebra set according to its complexity, represented by depth.\" depends totally on the representation. The proposed representation makes no guarantees of depth-minimality. A lot of work has been done on \"optimal\" boolean reprsentations, e.g., Binary Decision Diagrams (BDD).\n\nThee is no precise definition of compositional reasoning in models. Is there a target definition of compositional reasoning?\nThe claim on p. 5 \"This phenomenon demonstrates the compositionality of boolean algebra.\" makes no sense without a  precise definition of compositional reasoning.\n\n\n\n\n\n\nProblems also arise in task (2), where you \"examine the failure points of the model\u2019s proofs\".\n\nThere is no definition of \"proof\" or proof method. Hence we cannot understand what you mean by task 2.\n\nThere are several recent papers on LLM and Proofs, e.g., \n\nXiong, J., Shen, J., Yuan, Y., Wang, H., Yin, Y., Liu, Z., ... & Liu, Q. (2023). TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models. arXiv preprint arXiv:2310.10180.\n\nHow does your work compare to these?\n\nThe experiments on task 2 (proofs) do not address this task. Please see the above cited paper for a more rigorous method."
                },
                "questions": {
                    "value": "1. Would the approach fare better if you used as a representation:\n\n     (a) Disjunctive Normal Form (DNF)\n\n    (b) Binary Decision Diagrams (BDD)\n\n2.  Do you intend to look at the learning ability of de Morgan's logic rewrite rules, or more complex theorem proving methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2521/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2521/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2521/Reviewer_PEvF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773319882,
            "cdate": 1698773319882,
            "tmdate": 1699636188597,
            "mdate": 1699636188597,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "E4w5UDA916",
            "forum": "WepT31bvcr",
            "replyto": "WepT31bvcr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2521/Reviewer_iCBt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2521/Reviewer_iCBt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a way to evaluate compositionality in Language Models. \nThe main idea is to test the ability to induce the value of a complex semi-linguistic structure by using the value of the components. The target of the analysis is boolean expressions. LMs are exposed to expressions up to a given level for learning. At inference time, these LMs are queried with expressions at a major level of complexity. This is used to test whether or not LMs learn the compositional behavior."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper clearly demonstrates that LMs are not able to generalize"
                },
                "weaknesses": {
                    "value": "- This is only a negative paper: given the finding, there is no solution.\n\n- The paper does not cover an important related part: Memorization"
                },
                "questions": {
                    "value": "- Is there a way to solve the limitation that you discovered?\n\n- How do you relate your work with the idea that large part of these LLMs are memories: see: Data Contamination https://arxiv.org/abs/2203.08242 and PreCog: https://arxiv.org/abs/2305.04673\n\n- How pre-training of GPT2 and T5-base can be helpful?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773685402,
            "cdate": 1698773685402,
            "tmdate": 1699636188519,
            "mdate": 1699636188519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "BjwioYdxWJ",
            "forum": "WepT31bvcr",
            "replyto": "WepT31bvcr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2521/Reviewer_kQgJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2521/Reviewer_kQgJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper evaluates small language models\u2019 (Roberta, T5, GPT-2) depth generalization ability on Boolean expressions when fine-tuned with shorter depth Boolean expressions. In the main experiments, the training set consists of expressions with depth-1 and 2 circuits, whereas the test evaluations contain Boolean expressions with depth 3 and 4. They find that this model fails to generalize longer depth expressions in both the classification and the generation version of the task. They also have initial experiments with prompting LLMs in the Appendix F4."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1) The paper demonstrates generalization failure of small language models.\n2) The paper is written clearly."
                },
                "weaknesses": {
                    "value": "1) The paper does not contribute to the existing literature on compositional generalization in neural models and language models (see my questions on this and Furrer\u2019s and Kim\u2019s surveys for references); and does not evaluate the advanced techniques developed, for example Drozdov prompting work. The paper can also benefit from using the other length generalization papers. I am attaching some representatives that could have been included:\n\n- Drozdov et al. \u201cCompositional Semantic Parsing with Large Language Models\u201d    \n- Liu et al. \u201cLearning algebraic recombination for compositional generalization\u201d.  \n- Anil et al., \u201cExploring length generalization in large language models\u201d,   \n- Newman et al., \u201cThe EOS decision and length extrapolation\u201d.   \n\nReferences for some other compositional generalization work that I think is not successful at recursive generalization:\n\n- Russin, \u201cCompositional generalization by factorizing alignment and translation\u201d.  \n- Conklin, \u201cMeta-learning to compositionally generalize\u201d.   \n- Csordas, \u201cThe devil is in the detail: Simple tricks improve systematic generalization of transformers\u201d.  \n- Akyurek and Andreas, \u201cLexSym: Compositionality as Lexical Symmetry\u201d.  \n\n\n2) The paper refers to the general category of language models, and the paper states in their contribution *\u201cThis demonstrates that language models struggle to understand and generate compositional structure, which implies that the recent achievements in reasoning are not a result of language models' systematical and structural understanding of tasks.\u201d*. \n\nYet, the paper\u2019s main experiments deal with small and relatively old models on a single fine-tuning setting. Moreover, none of the paper that the phrase \u201crecent achievements in reasoning\u201d  refer use Roberta/T5/GPT-2, they instead use Palm and GPT-3. So, I cannot see how the claims you make derived from these results alone. \n\nSo, I believe important experiments are needed to validate these claims and strengthen the contribution of the paper.\n\n    - How does the increasing size of an LM affect these results? Do we see better generalization with increasing size?   \n    - What about mid-size, newer models trained with updated datasets  (LLama, Pythia series).  \n    - What about LLMs with prompting (you have only baseline results in Appendix F4, I suggest moving them to the main body. There are many methods on prompting nowadays gives significantly better results than few-shot prompting, see Drozdov et al., 2023).  \n    - What about LLMs with fine-tuning (GPT had an API for FT).   \n\n\n3) I think Table1, 2 and 8 could be summarized in a single plot which makes the presentation nice. My initial thoughts:\n- X axis is `n` where n is maximum training depth\n- Y axis include three connected dots per model; first dot is the accuracy at `depth<=n` test examples, second that is accuracy at `depth=n+1` test examples, and the third dot is the accuracy at `depth=n+2`\n- Color can specify the different models\n- Style can specify the pre-trained vs non-pretrained\n\nYou could play with these settings, but the essence is that you can compress many tables nicely into plots that are more informative about the generalization gap w.r.t function of `n`, model and pretraining."
                },
                "questions": {
                    "value": "1) In the end of the intro, Contribution-1 includes many claims/contributions and they switch between *\u201cTransformers \u2026\u201d* and *\u201cLanguage models \u2026\u201d*. What is the reason for this switch? Is this paper about the limitations of Transformers trained from scratch or the limitations of LMs? \n\n2) Contribution-2 includes an argumentation *\u201cThis prevents the model from forming a generalizable representation of compositional inputs.\u201d*. First, I suggest moving arguments to other sections. Second, I am not sure what *\u201cgeneralizable representation of compositional inputs\u201d* refers to and where did you show something about the representations of the model? \n\n3) Overall, can you make your contribution statement more concise and include only the concrete contributions of the paper?\n\n4) How does this paper\u2019s contribution differ from previous surveys on LMs\u2019 compositional generalization ability?   \n\n    a) Furrer et al., \u201cCompositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures\u201d,   \n    b) Emin Orhan, \u201cCompositional generalization in semantic parsing with pretrained transformers\u201d.  \n    c) Najoung Kim,  \u201cUncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models\u201d.  \n\nIs the difference evaluating models on the Boolean dataset used in the paper? If the answer is yes, how it fundamentally differs from COGS (Kim and Linzen, 2020), especially from COGS\u2019s recursive generalization?\n\n5) How does this paper\u2019s dataset and experiments differ from the following paper on generalization of Transformers on propositional logic: Schlegel, et al. *\"Can Transformers Reason in Fragments of Natural Language?.\"*? Is the difference Boolean algebra vs propositional logic?\n\n6) Why do you have a subsection as *\"Propositional logic and First-order logic\"* in the Background. Do you evalate on a propositional Logic dataset that I missed?\n\n**Summary of the Review**\n\nOverall, I think this paper does not add to the contributions of the following papers on LMs and compositionality (Furrer et al.; Orhan; Najoung). I delineated more related work that needs to be distinguished in the review. The paper claims about LMs without running state-of-the art LMs and techniques. Therefore, I suggest authors conduct the suggested experiments and revise the paper accordingly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849280459,
            "cdate": 1698849280459,
            "tmdate": 1699636188436,
            "mdate": 1699636188436,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]