[
    {
        "title": "FedQV: Leveraging Quadratic Voting in Federated Learning"
    },
    {
        "review": {
            "id": "EtFeXrJKaR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7549/Reviewer_E32F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7549/Reviewer_E32F"
            ],
            "forum": "r1IbewSnqq",
            "replyto": "r1IbewSnqq",
            "content": {
                "summary": {
                    "value": "The authors propose a new aggregation scheme for FL instead of regular FedAvg. For this, they draw on quadratic voting proposed in the election literature and design a scheme where the weighting of submitted gradients is determined based on both the similarity to the previous global model and a voting budget. The authors also briefly discuss an optimization/extension that includes a reputation system. Notably, the scheme is compatible with existing privacy-preserving and robust aggregation schemes. The authors successfully evaluate the resistance of their scheme when training for standard image classification tasks under a wide range of targeted as well as untargeted data and model poisoning attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and accessible for non-experts.\n\nThe suggested scheme is compatible with secure and robust aggregation schemes.\n\nThe claims on convergence and truthfulness are theoretically proven.\n\nThe authors evaluate the effectiveness on a wide range of both untargeted and targeted attacks instead of focusing on only a specific type of attack."
                },
                "weaknesses": {
                    "value": "References and appendix are not included in the submission PDF and can only be found in the supplementary material.\n\nIt is unclear why the similarity computation takes place on the client instead of server side. Moreover, it is unclear why the server simply relies on the submitted similarity score s instead of verifying this computation. I assume this is for compatibility with secure aggregation methods where the server does not see gradients in the clear. However, this is not discussed and gives opportunity for trivial attacks where the client submits an extremely harmful w but lies about similarity s. Likewise, it might be worth to discuss what happens if advanced attackers optimize their malicious w to cause maximum damage while still keeping the similarity just below the detection threshold.\n\nIt is furthermore unclear to me how the proposed \"masked\" voting rule is supposed to prevent the client from tracking their voting budget. The propsed rule H simply is a log calculation over the normalised similarity s. Intuitively, the clients can thus at least roughly estimate its budget and act accordingly.\n\nThe paper assumes a very high number of malicious clients in the system, e.g., it discusses cases where \"attackers manage to control the majority of votes, then via poisoning their tyranny will manifest itself as a degradation of the accuracy of the FL model used by the minority\". In the experiments, between 10%-50% is assumed. However, Shejwalkar et al. (SP'22) have shown that in cross-device production systems the corruption is below 0.1%. It remains unclear how FedQV performs in these settings.\n\nThe considered data poisoning attacks are not all state of the art. For example, the authors consider simple static label flipping (Fang et al.) but not more advanced dynamic label flipping attacks (Shejwalkar et al., SP'22).\n\nThe paper states that robust aggregations cannot be combined with secure aggregation. However, works like HyFL (arXiv:2302.09904) implement such aggregation schemes in secure computation."
                },
                "questions": {
                    "value": "- Why does the server not compute and verify the similarity measure?\n- How does the masked voting rule prevent the client from roughly tracking/estimating the remaining budget?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Reviewer_E32F"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7549/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697365191291,
            "cdate": 1697365191291,
            "tmdate": 1699636913132,
            "mdate": 1699636913132,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bfu3o4fQez",
                "forum": "r1IbewSnqq",
                "replyto": "EtFeXrJKaR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7549/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7549/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reviews!"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and insightful feedback on our paper. We reply to the questions raised one by one:\n\n**A1**: The decision to calculate the similarity score on the client side is driven by the need to prevent the server's access to gradients, mitigating privacy risks such as model inversion attacks. It also aligns with compatibility requirements for secure aggregation methods.\n\nWe acknowledge the potential vulnerability that the reviewer pointed out. Hence in our experimental evaluation, we have already evaluated a range of such attack scenarios:\n\n(i) Data poisoning attacks: Attackers submit the similarity score based on their poisoned updates;\n\n(ii) Model poisoning attacks: Attackers submit the true similarity score based on their clean updates and poison their model updates;\n\n(iii) Adaptive attack: Attackers manipulate the similarity score and submit poisoned updates.\n\nThe collective results from these attacks are that FedQV is more robust than FedAVG in all of them, as shown in Table 1.\n\n**A2:** The client's inability to track or estimate the remaining budget is ensured through two key mechanisms. First, the vote calculation is performed exclusively on the server side, as indicated in Algorithm 1 Lines 6 to 8. Clients lack access to information about their budget; only the server possesses this knowledge. Second, even if clients were aware of the voting rule, their lack of knowledge about the normalized similarity score, which depends on others' similarities, prevents them from determining their votes. This design reinforces the privacy and security aspects of the proposed masked voting rule.\n\n\n**A3 (Percentage of corruption):** The reviewer\u2019s point about the percentage of attackers is well-taken. The choice of this percentage is guided by the consideration of potential scenarios involving a larger number of attackers in which the damage to the global model can be very substantial. However, we completely acknowledge the importance of evaluating FedQV also under more realistic lower attack percentages. We have already conducted evaluations with low percentages such as 1%, 5%, and 10%, but placed these results in Appendix Table 6 due to the page limit. This supplementary result illustrates that FedQV continues to perform effectively even with lower attacker percentages. We will make a more clear note on this in the revised version.\n\n\n**A4 (Advanced version of label flipping attack)**: We appreciate the observation regarding the advanced label flipping attacks and acknowledge the existence of more advanced attacks. In our evaluation, we tested FedQV against a total of 9 different poisoning attacks, including 3 state-of-the-art attacks published last year. The rest are classic poisoning attacks, which are included for comparability with existing byzantine-robust defences. While we believe these attacks are sufficient to support our findings, recognizing the importance of the attack mentioned by the reviewer, we are open to incorporating it in the camera-ready version to further enhance the comprehensiveness of our evaluation.\n\n**A5 (HyFL)**: In our paper, we state that robust aggregations are difficult to combine with secure aggregation due to the heavy computation, which is also mentioned In HyFL. In the HyFL framework, a Hierarchical FL approach incorporates a lightweight variant of the Trimmed Mean defence designed for fighting against data-poisoning attacks. In this context, FedQV can be implemented atop the Trimmed Mean variant, serving as an additional layer of defence against data poisoning attacks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7549/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157463777,
                "cdate": 1700157463777,
                "tmdate": 1700157506186,
                "mdate": 1700157506186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0P9CIONams",
            "forum": "r1IbewSnqq",
            "replyto": "r1IbewSnqq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7549/Reviewer_dir9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7549/Reviewer_dir9"
            ],
            "content": {
                "summary": {
                    "value": "Federated learning is vulnerable to poisoning attacks by malicious participants. This gets aggravated when the influence of parties is proportional to the amount of data they have. This is due to the fact that malicious participants that claim to have large datasets have a big impact when they send corrupted updates. \n\nThis work presents FedQV, a technique to improve resilience to byzantine attacks in federated learning by leveraging ideas from different domains. First, the work takes ideas from quadratic voting, a technique for robust aggregation of weighted contributions that reduces the otherwise-large influence of participants that possess big datasets. Second, it proposes a control mechanism that requires users to publish the similarity of their updates with respect to the previous global model. Third, it proposes a truthful mechanism that incentives parties to upload consistent contributions to maximize its own utility. Finally, it provides a reputation based approach which assigns different weights to contributions depending on their past faults. \n\nThe paper theoretically shows convergence in the presence of malicious adversaries as well as improved resilience when compared to regular Federated Averaging. In addition, it shows improved resilience when FedQV is combined with other defense measures taken by the server."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes novel ideas by taking techniques from different domains.\n\nI think that the inclusion of incentive-based approaches to deal with corrupted parties is currently less explored that other techniques and can potentially provide new means to understand which attacks are realistic. Therefore, this is a valuable aspect of the contribution. \n\nOther ideas such as the integration of robust voting techniques and the use client-side measures are original and constructive in the discussion of byzantine resilience. In particular, techniques that are compatible with privacy enhancing technologies such as secure aggregation are valuable, especially given that an important use case of federated learning is privacy preserving machine learning."
                },
                "weaknesses": {
                    "value": "However, the contribution presents major problems which are listed below: \n\n1- The threat model of the paper is not clear. A well defined threat model is crucial when studying defenses against active adversaries. In the current contribution, I do not see what are the adversaries' possible behaviors and what are they willing to risk to harm the model (e.g. are they able to collude?). Therefore, it is not clear in practice in which scenarios would the truthfulness and convergence properties hold. This makes hard to evaluate the real impact of the contribution. \n\nFor example, consider an adversary which controls many parties and each party sends a harmful update in a different round. The adversary is willing to sacrifice parties to harm the model. Would this affect the truthfulness property? \n\n2- Clarity. The contribution is a combination of many ideas taken from different domains. However, the inclusion of each ingredient is not properly justified:\n\n2a- The paper introduces quadratic voting (QV) as a defense to reduce the impact of malicious users that falsely claim to have large datasets. However, the paper does not show how this kind of attack can affect other current Federated Learning defenses in practice and how QV is an effective defense. Moreover, the paper does not evaluate the resilience against any attack in which the adversary falsely claims to have a large dataset. Therefore, I am not sure if the initial motivation of QV is present in practice. \n\n2b- The paper introduces adaptive budgets as a reputation system. The original FedQV (Algorithm 1) already seems to penalize contributions. Therefore, the reputation protocol (Algorithm 2) appears a bit redundant. In the following sections of the paper, it is not clear when FedQV-Alg1 and FedQV-Alg2 are applied. \n\n2c- Compatibility with privacy enhancing primitives. In Section 3.2, the paper claims that the compatibility with privacy enhancing primitives (Secure Aggregation, Fully Homomorphic Encryption, Differential Privacy) is one of its main benefits of FedQV. However, I do not see a clear compatibility. For example, in Secure Aggregation the aggregation is not done by the server. Therefore, it not clear (i) how can the server adjust the weights of the aggregation properly and (ii) how can clients share similarity scores and local dataset size with the server without breaking the privacy guarantees. \n\n2d- Meaning of the truthfulness property: Section 4.2 claims that any possible corrupted contribution diminishes the probability of harm as server penalties reduce the influence of corrupted parties. This gives the impression that being honest is better for local utility than performing an attack. However, this is not what we can see empirically in section 5.  \n\n3- Evaluation of FedQV:  the proposed protocol claims to be easy to integrate with other resilience measures because similarities are computed in the client side. As said, this is an interesting feature, but is particularly vulnerable to client-side manipulations of the similarity scores and dataset sizes. It is true that protocol is evaluated against QV-adaptive, an attack that exploits this vulnerability. However, delegating the control of the defenses to possibly malicious clients is a high risk to take. Therefore, its evaluation of this risk requires a more comprehensive treatment. \n\nFor example, an important aspect of the evaluation which I feel is missed is (as said in point 2a) the resilience against an attack in which the adversary falsely claims to have a large dataset."
                },
                "questions": {
                    "value": "Please elaborate on the weaknesses outlined above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Reviewer_dir9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7549/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684024287,
            "cdate": 1698684024287,
            "tmdate": 1699636912996,
            "mdate": 1699636912996,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7eeFwnPw2k",
                "forum": "r1IbewSnqq",
                "replyto": "0P9CIONams",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7549/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7549/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reviews!"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and insightful feedback on our paper. We reply to the questions raised one by one:\n\n1- The reviewer is correct that the threat model is crucial. In our paper, attackers can collude based on the type of attack. For instance, for example, the QV-adaptive attack in Section 5, where collusion is used to manipulate similarity scores. The results in Table 2 demonstrate that FedQV effectively defends against this collusion-based attack.\n\nThe truthfulness property is considered from each party's perspective, focusing on individual utility. This implies that malicious parties also aim to remain in the system while contributing to model harm. Sacrificing some colluders wouldn't yield better results than our considered scenario because our designed QV-adaptive attack avoids such sacrifices, ensuring more powerful harm without compromising colluders unnecessarily. However, even if the manipulated similarity scores are accepted, the inherent constraints of QV, as shown in Figure 1, limit the influence the attackers can exert through restricted aggregation weights, minimizing their harm to the global model. This aspect will be clarified in the revised version for better understanding.\n\n2. \n\n2a-  We acknowledge the risk of malicious users falsely claiming larger datasets and introduce Quadratic Voting (QV) as a defence, specifically evaluating its impact on FedAvg. Figure 1 illustrates how the attack works when a malicious party claims to possess a larger dataset size. The result shows that having or claiming to have a larger dataset does not succeed in harming the model under FedQV.\nFurthermore, it is crucial to note that many current FL Byzantine-robust defences, such as Krum, trimmed-mean, and Rep, do not rely on the size of the dataset as the sole determinant for aggregation weights. Unlike FedAvg, where the attack in question could be effective, these defences utilize alternative aggregation mechanisms that are not susceptible to the same type of manipulation. \n\n2b- FedQV-Alg2's introduction is not redundant; it strategically assigns uneven budgets, rewarding benign peers and limiting malicious influence. This boosts performance by at least 26%, as shown in Fig. 2c compared to the baseline FedQV. \nIn the sections preceding 5.6, we assess the outcomes using the naive FedQV. Subsequently, in Section 5.6, we conduct evaluations of FedQV with reputation (denoted as FedQV + REP) in Figure 2c. The revised version will offer additional clarity on this distinction.\n\n2c- Regarding the compatibility with privacy-enhancing primitives:\n\n(i) To achieve secure aggregation in FedQV, clients initiate the process by sharing their similarity scores with the server. The server, utilizing the masked voting rule, adjusts the weights and broadcasts them back to the clients. Subsequently, clients upload gradients, weights, and local dataset sizes to the aggregator in a secure aggregation process;\n\n(ii) Firstly, the server remains unaware of the local dataset sizes of individual clients. Secondly, the server only possesses similarity scores, making it impossible to reconstruct the original gradients of clients. This safeguards against potential privacy breaches.\nImplementing the confidential computation on top of QV is part of our ongoing work, and we are actively implementing and refining these privacy-preserving mechanisms.\n\n2d- It's important to clarify that local utility for clients is rooted in enhancing their influence on the model, aiming for more aggregation weights and increased participation in FL training rounds. From this perspective, being honest aligns with their goal. However, as attackers seek to harm the model, they may strategically attempt to maximize their harm while remaining within the system. This behaviour is evident in Section 5. We are happy to provide additional clarification in the revised version to ensure a comprehensive understanding.\n\n3- We acknowledge the potential vulnerability that the reviewer pointed out. Hence in our experimental evaluation, we have already considered a range of attack scenarios:\n- Data poisoning attacks: Attackers submit the similarity score based on their poisoned updates.\n- Model poisoning attacks: Attackers submit the true similarity score based on their clean updates and poison their model updates.\n- Adaptive attack: Attackers manipulate the similarity score and submit poisoned updates.\n\nRegarding the attack where adversaries falsely claim to have a large dataset, this attack can be successful against FedAvg (as mentioned in point 2a). But against FedQV it cannot succeed because, as illustrated in Figure 1, FedQV effectively excludes the malicious party from the aggregation process, thereby improving the accuracy of the resulting global model. This demonstrates FedQV's efficacy as a defence against this attack compared to FedAvg."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7549/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157248216,
                "cdate": 1700157248216,
                "tmdate": 1700166177742,
                "mdate": 1700166177742,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LHSnTwhF4O",
            "forum": "r1IbewSnqq",
            "replyto": "r1IbewSnqq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7549/Reviewer_WYWh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7549/Reviewer_WYWh"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced the quadratic voting scheme into federated learning to replace the traditional 1p1v strategy. The idea of allowing the participation parties to decide their participation weight in each round is interesting but I am not sure how useful it is to prevent Byzatine attacks. The authors also integrate other countermeasures to make the scheme Byzantine-robust like suppressing gradients different from the majority and allocating budget based on reputation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of allowing FL participants to decide how to participate in FL using budget is novel. However, I am not sure how useful it is to prevent against Byzantine attackers since this gives the attacker more attack space to explore. For instance, comparing to limiting each client to participate in at most k rounds, the QV strategy of giving a client budget k is a super set and gives the adversary more freedom to design their attacks."
                },
                "weaknesses": {
                    "value": "In the evaluation section, FedQV is only compared with FedAvg. There are many existing Byzantine-robust FL strategies that should be compared with\n[1] Attack-resistant federated learning with residual-based reweighting. Shuhao Fu, Chulin Xie, Bo Li, and Qifeng Chen.\n[2] Robust aggregation for federated learning. Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui.\n[3] Byzantine stochastic gradient descent. Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li.\n[4] Learning from history for byzantine robust optimization. Sai Praneeth Karimireddy, Lie He, and Martin Jaggi.\n[5] Byzantine-Robust Federated Learning with Optimal Rates and Privacy Guarantee. Banghua Zhu, Lun Wang, Qi Pang, Shuai Wang, Jiantao Jiao, Dawn Song, Michael I.Jordan.\n[6] Byzantine-resilient non-convex stochastic gradient descent. Zeyuan Allen-Zhu, Faeze Ebrahimian, Jerry Li, and Dan Alistarh.\n[7] Byzantine-robust learning on heterogeneous datasets via bucketing. Sai Praneeth Karimireddy, Lie He, and Martin Jaggi.\n[8] Variance reduction is an antidote to byzantines: Better rates, weaker assumptions and communication compression as a cherry on the top. Eduard Gorbunov, Samuel Horv\u00e1th, Peter Richt\u00e1rik, and Gauthier Gidel.\n[9] Secure byzantine-robust distributed learning via clustering. Raj Kiriti Velicheti, Derek Xia, and Oluwasanmi Koyejo.\n[10] Byzantine-resilient high-dimensional sgd with local iterations on heterogeneous data. Deepesh Data and Suhas Diggavi."
                },
                "questions": {
                    "value": "Please compare with other existing Byzantine-robust FL systems."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7549/Reviewer_WYWh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7549/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700778753793,
            "cdate": 1700778753793,
            "tmdate": 1700778753793,
            "mdate": 1700778753793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]