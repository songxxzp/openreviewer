[
    {
        "title": "Can long-context large language models understand long contexts?"
    },
    {
        "review": {
            "id": "nKyEvBmVWy",
            "forum": "rtzUW1FU3H",
            "replyto": "rtzUW1FU3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5230/Reviewer_8ZV2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5230/Reviewer_8ZV2"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new benchmark called LooGLE for long-context LLMs, with inputs longer than 24k tokes. Some/most examples are human-annotated, and some/most were cross-validated by multiple annotators.\nFurther, some/most of the documents in the benchmark were published after 2022, which is expected to be after the knowledge cutoff of LLMs such as GPT-3.5 and GPT-4, forcing them to rely only on their in-context learning abilities rather than their prior knowledge.\n\nSuch benchmarks are always useful and needed in the community, especially following the growing interest in long-context LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors made lots of efforts in collecting and curating the data.\n* Such benchmarks are always useful and needed in the community, especially following the growing interest in long-context LLMs."
                },
                "weaknesses": {
                    "value": "- Following the promises in the abstract about the human-annotation and the cross-annotator validation, I was very disappointed to see that a large part of the benchmark's ground-truth output was generated using GPT 3.5 / 4:\n>We utilize the powerful language processing and understanding capability of GPT-3.5-turbo to help generating short QA pairs from the original text.\n\n>we employ GPT-3.5-turbo to generate factual summaries align with the source segment using with constraints\n\nIf this is indeed the case, this is disappointing, and the benchmark may be biased toward \"questions that are easy for ChatGPT to answer\".\n\n- The text is very unclear in many cases. For example when describing the statistics of the benchmark, the paper says:\n>Extra-long realistic documents. It contains 778 latest gathered and extremely long documents\nwith an average of 16.4k words. There are over 6000 test instances without distribution bias for a\nmore generalized assessment, many of which are exceeding 100k words.\n\nSo are there 778 examples or 6000 examples? If \"many of which exceed 100k words\", how many of them? what's the average? Are these two datasets? If not, why are these numbers reported separately?\n\n- The results in Section 4.3.1 are very confusing and unclear. For example:\n>In Table 3, it can be noticed that LlamaIndex obtains from the perspective of GPT4 evalution. Instead\nof memorizing a shortcut of original input with a limited context window, retrieval-based context\ncompression technique augments the LLM by incorporating external memory, allowing relevant\ninformation to be retrieved using a specific query.\n\nI am not sure what such paragraphs are trying to say. What does it mean that \"LlamaIndex obtains from the perspective of GPT4 evalution\"? What do the authors exactly mean by \"memorizing a shortcut\"? Who is memorizing a shortcut?\n- Measuring \"GPT4 score\" on GPT4's outputs is mostly meaningless. It would be better to just completely remove this column, or use another LLM that is not evaluated.\n- Applicability: the paper does not mention anything about its implementation, its ease of use, its availability. As always with benchmarks, the devil is in the details, and the authors have not included the data itself, which makes it hard to really evaluate its quality.\n- Presentation is poor: for example: \n    - the text in Figure 1 is tiny, not allowing to actually understand the overview of the new benchmark. \nThe entire left part of the figure contains barely any information.\nI would prefer an organized and readable list of tasks and data statistics.\n\n    - The text in Table 1 tiny\n    - The text in Table 2 is tiny. Further, it would be helpful if these statistics would include the max/min instead of category, or a more illustrative figure of the characteristics of the examples, as in Figure 1 in the [SCROLLS paper](https://arxiv.org/pdf/2201.03533.pdf)\n    - The text in Figure 3 is tiny. Further, the colors are very similar, and I cannot distinguish between the different models and cannot understand anything from this figure."
                },
                "questions": {
                    "value": "### Questions\n1. Section 3.3.1 says that \"we directly use the abstract of each paper as the reference for generating summaries\" - so, the ground-truth summaries where **generated**? are the Abstracts **used** in any part of the process other than for evaluation?\n2. Are the authors going to release the test sets, or keep them \"hidden\"?\n3. Are there training/test spits, or is everything \"test\"?\n\n### Comments\n1. The comparison in Table 1 on \"which tasks are included in each benchmark\" shows that LooGLE contains many tasks that other prior benchmarks do not. However, it is a bit unfair, because these prior benchmarks contain tasks that are not contained in LooGLE, but these are not mentioned. For example, Scrolls, mentioned in the first line, does contain QA (mentioned with \"X\") and NLI (not mentioned at all).\n\n### Summary\nI appreciate the authors's efforts, but as much as good benchmarks are needed in the community, unfinished benchmarks can do harm and drive research in the wrong direction.\nI cannot evaluate the benchmark itself since it was not released, but the paper still feels a bit unclear and unfinished, which makes me worry that the benchmark is too.\nThus, I currently vote for rejection, and hope that the authors would polish both the paper and the benchmark and release them when they are in a more polished state."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697552850353,
            "cdate": 1697552850353,
            "tmdate": 1699636521500,
            "mdate": 1699636521500,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xRMcgybqrj",
                "forum": "rtzUW1FU3H",
                "replyto": "nKyEvBmVWy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 8ZV2"
                    },
                    "comment": {
                        "value": "> Q1: Following the promises in the abstract about the human-annotation and the cross-annotator validation, I was very disappointed to see that a large part of the benchmark's ground-truth output was generated using GPT 3.5 / 4. If this is indeed the case, this is disappointing, and the benchmark may be biased toward \"questions that are easy for ChatGPT to answer\".\n\n**A:** We thank the reviewer for pointing out this concern. We indeed have spent a huge effort and high cost to make the evaluation as fair as possible in the following steps: \n- **For short dependency QA**, we have manually reviewed all the short QA pairs and carefully refined the answers to make them clear and concise. The model is forced to extract the answer directly from the original document to generate the initial answer. Then we make refinements by filtering out the non-essential contexts and removing redundant descriptions from the model. \n- **For long dependency QA**, there are over 1100 long dependency QA pairs delicately designed by human annotators, despite the high costs and huge effort. Each document for generating QA pairs underwent a meticulous three-step process(Question & answer, Answer & check, Cross-validation & revise) that involved the assignment of two distinct annotators who are unaware of each other\u2019s identities. The annotation adhered to stringent standards including long dependency, diverse problem types, clear & precise questions and deterministic & objective answers. Participants were prohibited from using large language models and tools like ChatGPT for article reading, data generation, and annotation.\n3) **For summarization**, answers are abstracts extracted from the paper.\n4) **For cloze**, we did not use gpt-3.5 to generate both the question and answer directly, and the answers are extracted by a NER model. \n    \n**This rigorous curation process was undertaken to ensure the high quality of the questions, answers, as well as supporting evidence. This approach aims to achieve questions with a high degree of accuracy, precision, and relevance to the document\u2019s content.**\n\n\n> Q2: The text is very unclear in many cases. For example when describing the statistics of the benchmark, the paper says: Extra-long realistic documents. It contains 778 latest gathered and extremely long documents with an average of 16.4k words. There are over 6000 test instances without distribution bias for a more generalized assessment, many of which are exceeding 100k words. So are there 778 examples or 6000 examples? If \"many of which exceed 100k words\", how many of them? what's the average? Are these two datasets? If not, why are these numbers reported separately?\n\n**A:** Thanks for pointing out the misleading data statistics. For the polished version of the dataset, we have collected 776 original long documents, 2 of which exceed 100k words while 30% of which exceed 20k. The average words of LooGLE is more than 2 times longer than the existing dataset for long context. Please refer to **Figure 4** for the data length distribution. Based on the documents, we further generate over 6000 questions for test and evaluation. Details and numbers of the questions in each task, and their distributions, can be found in Table 2. We have also revised the expressions in the paper to make it precise.\n\n> Q3: The results in Section 4.3.1 are very confusing and unclear. For example: In Table 3, it can be noticed that LlamaIndex obtains from the perspective of GPT4 evalution. Instead of memorizing a shortcut of original input with a limited context window, retrieval-based context compression technique augments the LLM by incorporating external memory, allowing relevant information to be retrieved using a specific query. I am not sure what such paragraphs are trying to say. What does it mean that \"LlamaIndex obtains from the perspective of GPT4 evalution\"? What do the authors exactly mean by \"memorizing a shortcut\"? Who is memorizing a shortcut?\n\n**A:** Thank you for your suggestion. We intend to show that the retrieval-based LlamaIndex with external memory does help short QA to some extent and has a competitive performance compared to the winning GPT models, as inferred from Table 3. LLMs inherently lose much information in long inputs with limited context windows (which we call \"memorizing a shortcut\"). We have rewritten this part and hope that the edited section clarifies these key findings."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112116749,
                "cdate": 1700112116749,
                "tmdate": 1700112116749,
                "mdate": 1700112116749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xBGC5syqw9",
                "forum": "rtzUW1FU3H",
                "replyto": "tlJK0SjPLi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5230/Reviewer_8ZV2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5230/Reviewer_8ZV2"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response.\n\n>The model is forced to ...\n\nHow do you \"force\" the model?\n\n>There have been many recent research studies that have shown that the GPT4 evaluator exhibits high consistency with human evaluation and can serve as a reliable annotator to some extent.\n\nPerhaps, but when GPT-4 evaluates **its own predictions**, I do not think that it is a reliable annotator. \n\nFurther, it might be an OK compromise to use GPT-4 as an evaluator in tasks that are hard to evaluate automatically. However when creating a new benchmark, I expect it to be perfectly accurate and clean. As shown in the authors' table, the agreement between GPT-4 and human evaluation is only around 80%.\n\n>How we use GPT4-eval is also delicately designed to ensure fairness. By giving the question(QA only), groundtruth and predicted outputs, we ask GPT4 to compare and score considering the semantic matching, information completeness, consistency, fluency, and grammar. In this way, GPT4 can focus on the comparisons without bias and tendency for better evaluation.\n\nI don't think that anything that GPT-4 is asked to do can be referred to as \"ensured\". How can the authors claim that this \"*ensures* fairness\" and \"without bias\"?\n\n>our polished dataset has already been released on the HuggingFace Datasets and can be easily downloaded anytime for use\n\nI'm happy to hear that the dataset is publicly available already, but I'm not sure how can I evaluate the dataset's quality without breaking anonymity. \n\n>In the future, we will keep the dataset updated with newer documents and diverse tasks with more examples split for different sets and uses.\n\nI actually think that datasets should better remain completely *frozen* rather than continuously updated."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518191654,
                "cdate": 1700518191654,
                "tmdate": 1700518191654,
                "mdate": 1700518191654,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x9rmigpTLa",
            "forum": "rtzUW1FU3H",
            "replyto": "rtzUW1FU3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5230/Reviewer_oc4r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5230/Reviewer_oc4r"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new benchmark for long context understanding, consisting of 24k tokens in average. Firstly, this has the advantage to be more challenging than former benchmarks that have shorter texts compared with current LLMs' context window length (reaching up to 32k tokens). Secondly, it only contains newly created documents (after 2022) which are thus not present in most LLMs' pretraining data, preventing data leakage and enabling fairer evaluation. Experiments on current state-of-the-art LLMs reveal challenges in long-context understanding."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A curated dataset, thoughtfully designed with great efforts to prevent data leakage and ensure long dependency.\n- Great efforts in assessing current state-of-the-art LLMs' long dependency capabilities such as information retrieval, reading comprehension and reasoning, computation and timeline reordering."
                },
                "weaknesses": {
                    "value": "- see questions.\n- the paper is sprinkled with typos (refer to questions for a few)."
                },
                "questions": {
                    "value": "- Is the benchmark english-only ? If so, this needs to be mentioned.\n- How will the dataset be released ? For instance ZeroSCROLLS only released the inputs and evaluate through a system of leaderboard, will LooGLE be released the same way ? I'm concerned that revealing input/gold outputs pairs would lead to data leakage for future models.\n- Concerning data collection, were the sourced documents (after 2022) subjected to any machine-generated verification ? I am concerned that ChatGPT-like texts might compromise the fairness of the evaluation.\n- Are the open-source models instruction-tuned ? Commercial closed source models like GPT3.5 rely on RLHF or some instruction tuning techniques that enable them to better follow instructions. If the considered open-source baselines are not instruction-tuned, the comparison might be unfair since the prompt used for evaluation is the same and is instruction-based.\n- Is it fair to have GPT4 both as baseline and evaluator ? I am also concerned about the creation of the dataset of short QAs (generated by GPT3.5). Is it fair to evaluate a model that was used to create the dataset ?\n- In section 4.2, you mention human evaluation (3) but I cannot find any human evaluation both in the paper and the appendix.\n- Section 4.3.2 is confusing, the experiments are based on the recent work from Liu et al. 2023, that accessing information in the middle of the document is more challenging for LLMs. If I understood correctly, you suggest concatenating the head and the tail of the inputs and give it to the LLMs as input. This would mean discarding the whole \"middle\" and leaving the beginning and the end. This puts an immediate limitation on information that can be retrieved by the LLMs. Also did you only use the same model (GPT4-32k) but with various context input length or different variants of GPT4 ? What is the last entry in Table 5 ? (GPT4). For long summarization, arxiv abstracts are highly biased towards the beginning of the article so it is expected that increasing context would result in a higher divergence between the generated summary (which will contain more and more details) and the (gold) abstract.\n \n**Typo**\n- page 2: \"Mannually designed both short and long dependency tasks\"\n- page 3: \"COmputation.\"\n- page 7: \"Retrival\"\n- page 8: \"GPT4 evalution\"\n- table 4: \"Performence\"\n- appendix page 3: \"Dispcrepancy\"\n- not really typos but it is uninformative to report results of the order of \"e-300\" since at this point there is nothing to really compare."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5230/Reviewer_oc4r"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723649205,
            "cdate": 1698723649205,
            "tmdate": 1699636521416,
            "mdate": 1699636521416,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3pJt2i5ELl",
                "forum": "rtzUW1FU3H",
                "replyto": "x9rmigpTLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer oc4r"
                    },
                    "comment": {
                        "value": "> Q1: Is the benchmark english-only ? If so, this needs to be mentioned.\n\n**A:** The current version of the dataset is English-only. Thanks for your reminder and we have included this detail in the paper as well.\n\n> Q2: How will the dataset be released ? For instance ZeroSCROLLS only released the inputs and evaluate through a system of leaderboard, will LooGLE be released the same way ? I'm concerned that revealing input/gold outputs pairs would lead to data leakage for future models.\n\n**A:** Thanks for your comments and it's a great suggestion.Currently, our polished dataset has been released in the HuggingFace Datasets as the dev set, with all the golden truths. In the future, we will keep updating the dataset with newer documents and diverse tasks, providing more examples split for different sets and uses.\n\n\n> Q3: Concerning data collection, were the sourced documents (after 2022) subjected to any machine-generated verification ? I am concerned that ChatGPT-like texts might compromise the fairness of the evaluation.\n\n**A:** We thank the reviewer for your useful comments For data collection and selection, the sources of original documents (arXiv papers, Wikipedia articles, Movie and TV scripts) are most officially used and widely acknowledged to ensure their high quality. We will keep the data updated and follow your comments on data collection to guarantee the high quality of the dataset.\n\n\n> Q4: Are the open-source models instruction-tuned ? Commercial closed source models like GPT3.5 rely on RLHF or some instruction tuning techniques that enable them to better follow instructions. If the considered open-source baselines are not instruction-tuned, the comparison might be unfair since the prompt used for evaluation is the same and is instruction-based.\n\n**A:** Thank you for the valuable suggestion and we totally agree with that.\nAmong the four open source models we used in this paper, **ChatGLM2-6B** was trained with human preference alignment and **LLaMA2-7B-32K** was instruction tuned for summarization and long context QA. We have highlighted this information in Section 4.1 in the revised paper. Based on your suggestion, we are working on additional experiments for the instruction tuning versions of the other two models. The results for long dependency QA can be seen below:\n| Models         | Context | Bleu1 | Bleu4 | Rouge1 | Rouge4 | RougeL | Meteor_score | Bert_score | GPT4 evaluator |\n|----------------|---------|-------|-------|--------|--------|--------|--------------|------------|----------------|\n| Long_llama-3b-instruct  | 256k     | 5.64  | 0.49  | 17.30  | 3.76   | 16.29  | 6.53         | 84.26      | 21.64          |\n| RWKV-4-raven-14b | 8k     | 3.88  | 0.22  | 20.39  | 3.20   | 19.20  | 6.41         | 81.46      | 14.32         |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119232452,
                "cdate": 1700119232452,
                "tmdate": 1700273562950,
                "mdate": 1700273562950,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2x2iXnn4hT",
            "forum": "rtzUW1FU3H",
            "replyto": "rtzUW1FU3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5230/Reviewer_SWcJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5230/Reviewer_SWcJ"
            ],
            "content": {
                "summary": {
                    "value": "LLMs have shown impressive performance in various NLP tasks. However, the fixed context window length of the transformer architecture limits their ability to understand extremely long inputs. Existing datasets for evaluating LLMs' long context understanding have limitations such as shorter text lengths, outdated documents, and a focus on short dependency tasks. The paper introduces \"LooGLE\"  to evaluate LLMs' ability to understand long contexts. Upon evaluating 8 state-of-the-art LLMs on LooGLE, the authors found:\n1. Commercial models generally outperform open-sourced models.\n2. LLMs excel at short dependency tasks but struggle with real long dependency tasks.\n3. Retrieval-based techniques significantly improve performance on short QA tasks, but many techniques for extending context window length struggle with long context understanding."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Up-to-date Documents**: LooGLE contains documents published after 2022, ensuring that modern LLMs have not been pretrained on these documents.\n- **Diverse Tasks**: LooGLE includes both short and long dependency tasks, providing a evaluation of 8 LLMs' capabilities."
                },
                "weaknesses": {
                    "value": "**Lack of Experimental Data to Support Some Claims**:\n\n- The paper states that \"by employing scaling techniques like positional interpolation, parallelization, and finetuning on longer texts, open-sourced models have shown improvement in handling longer inputs compared to previous versions.\" However, the article does not provide performance data of previous version models. This omission makes it challenging to ascertain the improvements is brought by modifying position embeddings or instruction-tuning. For example, the comparision between vicuna-2k and vicuna-16k is a better case to validate this claim.\n- The claim that \"GPT4-32k performs better than GPT-8k\" is not consistently supported by the provided metrics. The results between the two models vary across different indicators (automatic metrics v.s. gpt4 score). A more in-depth explanation and analysis are needed to support this claim, including understanding the differences in various metrics. It's unclear why, on Long dependency tasks, the longer window 32k model performs worse than the 8k model. \n\n**Absence of Human Evaluation**: The paper mentions conducting human evaluations, but there's no presentation of the related data. GPT-evals may have some preference in generation length, human evaluation can be a better reference."
                },
                "questions": {
                    "value": "- Details of Llamaindex is not clear: 7B or 13B, chat model or regular model?\n- Why not use the chat version of Llama2, which is considered to be skilled at instruction-following and could be potentially better at downstream tasks.\n- Why LlamaIndex is much better than any other open-sourced models? According to your results in Fig3, retrieval+open-sourced model is better than the long-context version of the same base model, and this conclusion is contradict to the conclusion from other long-context benchmarks (Table1).\n- Writing format: it's better to list url as the footnote. It is supposed to leave a black between the main text and citation brackets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698857654902,
            "cdate": 1698857654902,
            "tmdate": 1699636521324,
            "mdate": 1699636521324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c5eCln4rdJ",
                "forum": "rtzUW1FU3H",
                "replyto": "2x2iXnn4hT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer SWcJ"
                    },
                    "comment": {
                        "value": "> Q1: The paper states that \"by employing scaling techniques like positional interpolation, parallelization, and finetuning on longer texts, open-sourced models have shown improvement in handling longer inputs compared to previous versions.\" However, the article does not provide performance data of previous version models. This omission makes it challenging to ascertain the improvements is brought by modifying position embeddings or instruction-tuning. For example, the comparision between vicuna-2k and vicuna-16k is a better case to validate this claim.\n\n**A:** Thanks the reviewer for your constructive advice. \n- We have to emphasize that the paper only claims \"**open-sourced models have shown improvement in handling longer inputs** by employing the scaling techniques\" instead of gaining performance improvements in downstream tasks. \n- However, we also **agree with your suggestion and indeed explore this by adding additional experiments**. We further compare the varying input lengths of ChatGLM2-6B-32k (32k context window) with ChatGLM2-6B (8k context window) on long dependency QA. We select ChatGLM2 as the testing model since it achieves better performance in our previous experiments among different open-sourced models. Here are some findings:\n    1) The original version model ChatGLM2-6B performs better than ChatGLM2-6B-32k across the automated metrics given the same length of inputs. There is a performance decline for the model when extending the longer context window. It can be attributed to the information loss introduced by the scaling techniques of long context models, which calls for further improvement on open-sourced LLMs.\n    2) Moreover, for ChatGLM2-6B-32k with varying lengths of inputs, we find that the extension of longer inputs has limited impact on performance. It is because the long dependency QAs in our dataset request for the long dependency comprehension and  modeling capability, along with further computation and reasoning. Our dataset propose higher demands on true long context undetstanding, which needs to be desperately resolved and enhanced for further LLMs.\n\n| Models         | Context | Bleu1 | Bleu4 | Rouge1 | Rouge4 | RougeL | Meteor_score | Bert_score | GPT4 evaluator |\n|----------------|---------|-------|-------|--------|--------|--------|--------------|------------|----------------|\n| ChatGLM-6b-32k | 32k     | 5.62  | 0.01  | 11.95  | 1.45   | 10.84  | 5.55         | 87.18      | 20.64          |\n| ChatGLM-6b-32k | 24k     | 7.04  | 0.16  | 13.74  | 2.67   | 12.80  | 6.10         | 87.93      | 20.00          |\n| ChatGLM-6b-32k | 16k     | 6.37  | 0.01  | 13.26  | 1.49   | 12.29  | 5.69         | 87.87      | **21.53**          |\n| ChatGLM-6b-32k | 8k      | 4.98  | 0.11  | 11.56  | 1.68   | 10.79  | 5.26         | 87.90      | 21.37          |\n| ChatGLM-6b     | 8k      | **9.72**  | **0.43**  | **14.03**  | **2.79**   | **13.12**  | **9.53**         | **88.74**      | 20.11          |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111612483,
                "cdate": 1700111612483,
                "tmdate": 1700111612483,
                "mdate": 1700111612483,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G1ePeTVAS8",
                "forum": "rtzUW1FU3H",
                "replyto": "2x2iXnn4hT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer SWcJ"
                    },
                    "comment": {
                        "value": "> Q4: Details of Llamaindex is not clear: 7B or 13B, chat model or regular model?\n\n**A:** We use the official default model text-davinci-003 for LlamaIndex in all the experiments. \n\n\n> Q5: Why not use the chat version of Llama2, which is considered to be skilled at instruction-following and could be potentially better at downstream tasks.\n\n**A:** We thank the reviewers for your useful comments.  To the best of our knowledge, LLaMA2-7B-32K used in this paper is fine-tuned from Meta's original Llama-2 7B model to extend context and also instruction-tuned for summarization and long context QA for better performance. We have highlighted this information in Section 4.1 in the revised paper.  We are happy to extend to new models and will keep updating the leaderboard.\n\n> Q6: Why LlamaIndex is much better than any other open-sourced models? According to your results in Fig3, retrieval+open-sourced model is better than the long-context version of the same base model, and this conclusion is contradict to the conclusion from other long-context benchmarks (Table 1).\n\n**A:** Thank you for the comments. \n- LlamaIndex is a retrieval-based data pipeline for different models. We use text-davinci-003 as default model in our experiment instead of the open-source Meta Llama. \n- From the short QA in Table 3, retrieval-based techniques indeed demonstrate their benefits for short QA. However, they have a limited impact on understanding long context for long dependency tasks, as shown in Table 4.\n- Our conclusion in Table 1 is consistent with that in LongBench. However, we have quite a different experiment settings (designed tasks, evaluation metrics, etc.) to drive to this conclusion.\n\n\n> Q7: Writing format: it's better to list url as the footnote. It is supposed to leave a black between the main text and citation brackets.\n\n**A:** Thanks for your advice and we have already moved all the urls as the footnotes the writing format according to your comments (see Page 4 and 7). Also, we have refined the writing format issue of blank you mentioned."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112006279,
                "cdate": 1700112006279,
                "tmdate": 1700185210648,
                "mdate": 1700185210648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Aye4cVYDeh",
            "forum": "rtzUW1FU3H",
            "replyto": "rtzUW1FU3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5230/Reviewer_gE1R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5230/Reviewer_gE1R"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a new dataset, called LooGLE, which aims at evaluation of LLMs on long context. Their dataset has documents with longer length compared to previous benchmarks and it is more up to date (2022+). The proposed dataset has task with long dependencies and the authors have made sure that for some of the tasks, the answer needs to be collected from multiple segments of the documents. The evaluate both commercial and open-souse models on the new dataset and provide some insights."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper addresses a very important area, i.e., long context evaluation of LLMs\n- Based on the description, the collection method, and evaluation results, the proposed new dataset seems to be of high quality.\n- Authors provide extensive evaluation on different commercial and open-sourced models.\n- The paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The paper presents Human Evaluations as one of the evaluation technique but never present human evaluation results.\n- There are several automatic scores have been presented in Tables 3 through 5 and sometimes. These scores not always in agreement; not all the scores are better for the winning model. I found this confusing especially when some conclusions are drawn in the text."
                },
                "questions": {
                    "value": "- For the LlamaIndex, what retriever and chunk size are used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5230/Reviewer_gE1R"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699235449222,
            "cdate": 1699235449222,
            "tmdate": 1699636521224,
            "mdate": 1699636521224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qYN5yp25rv",
                "forum": "rtzUW1FU3H",
                "replyto": "Aye4cVYDeh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer gE1R"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for your acknowledgment and constructive comments on our work. We would like to involve further discussions in the following sections. \n \n \n > Q1: The paper presents Human Evaluations as one of the evaluation technique but never present human evaluation results.\n\n**A:** We have indeed included human evaluations in 3 parts of the experiments in **Appendix C**: \n1) We manually evaluated the accuracy for 4 long-dependency tasks under 3 settings (without CoT, few-shot CoT, zero-shot CoT) in **Figure 10**.\n2) We provide probable explanations for long QA bad cases to provide insights and directions for model promotion in **Appendix C.1**.\n3) We captured discrepancies in the generated outputs of different models to tackle inherent preferences encountered in long context in **Figure 11**.\n\nWe have also revised the paper to make this part more clearly stated for readers.\n \n\n> Q2: There are several automatic scores have been presented in Tables 3 through 5 and sometimes. These scores not always in agreement; not all the scores are better for the winning model. I found this confusing especially when some conclusions are drawn in the text.\n\n**A:** Thanks for pointing out. We summarize the main performance in the experiments and provide potential explanations  as below: \n- **For summarization evaluated by both automatic metrics and GPT4-eval**, GPT4-8k performs better while longer context window has marginal improvement from Table 4 and Table 5. It is due to the intrinsic features of ArXiv papers with both the introduction and conclusion located at the beginning and in the end respectively. In this way, GPT4-8k with limited window size can still perform well to capture the major sketch of the paper. \n- **For cloze, short & long QA evaluated by exact/partial match and GPT4-eval**,  GPT4-32k performs better semantically with more complete inputs with less information loss and stronger context understanding capability from Table 3, 4, 5. \n- **For QA tasks evaluated by other automatic metrics**, GPT4-32k performs worse in some cases from Table 3. It can be attributed to its tendency to generate longer outputs, faces penalties from these automatic metrics. It is more evident in short dependency QA where GPT4-8k can extract the right answer with limited window.\n\nThe disagreement between metrics lies in the reason that each of them **measures the generated texts from different aspects**. For instance, BLEU and ROUGE evaluate n-gram based similarity but compute precision and recall respectively. METEOR incorporates matching in various aspects, including word stemming and variations, syntax, order, and phrase structure, and computes a semantic matching score. BERTScore leverages pre-trained contextual embeddings and computes the cosine similarity from token level. Exact Match entails precise comparison between the predicted entities and the ground truth entities while Partial Match allows for fuzzy matching. Therefore, we use various automatic metrics commonly to obtain comprehensive performance evaluations.\n\nWe have also revised the corresponding expressions in the paper to make it clear.\n \n> Q3: For the LlamaIndex, what retriever and chunk size are used?\n \n**A:** We use the official default model text-davinci-003 as the retriever and default chunk size (1024) for LlamaIndex."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111316313,
                "cdate": 1700111316313,
                "tmdate": 1700111316313,
                "mdate": 1700111316313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]