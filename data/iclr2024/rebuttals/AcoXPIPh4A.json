[
    {
        "title": "Risk Bounds of Accelerated SGD for Overparameterized Linear Regression"
    },
    {
        "review": {
            "id": "Q6GZx9HZhu",
            "forum": "AcoXPIPh4A",
            "replyto": "AcoXPIPh4A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8252/Reviewer_MrsP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8252/Reviewer_MrsP"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization of ASGD for overparameterized linear regression, which is possibly the simplest setting of learning with overparameterization. The authors establish an instance-dependent excess risk bound for ASGD within each eigen-subspace of the data covariance matrix. The theoretical findings show that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD. Our result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues.  Finally, sufficient experiment verify the effectiveness of the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The theoretical findings are solid.\n2. This paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "The experimental results do not provide strong support for the theoretical findings in this paper."
                },
                "questions": {
                    "value": "1. How is overparameterization reflected in Theorem 4.1?\n\n2. Please provide a more detailed explanation of the challenges encountered in theoretical analysis and how to address them in the submission.\n\n3. Can you provide more instances that satisfy Assumption 3.2?\n\n4. The experiments are not comprehensive enough, for example, the paper finds that the variance error of ASGD is always larger than that of SGD, but there is not enough experimental support for this claim."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801787561,
            "cdate": 1698801787561,
            "tmdate": 1699637026189,
            "mdate": 1699637026189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o8lf8mPBMd",
                "forum": "AcoXPIPh4A",
                "replyto": "Q6GZx9HZhu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8252/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MrsP"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback and suggestions!\n\n**Q1**. The experiments are not comprehensive enough, for example, the paper finds that the variance error of ASGD is always larger than that of SGD, but there is not enough experimental support for this claim.\n\n**A1**. Thank you for your suggestion. We have provided additional experiments to justify that the variance error of ASGD is larger than that of SGD. We have also provided additional experiments that compare both the bias error and the variance error of ASGD and SGD for linear regression instances with different spectra of $\\mathbf{H}$, including $\\lambda_k=k^{-2}$, $\\lambda_k=k\\log (k+1)$ and $\\lambda_k=e^{-k/2}$. In these experiments, we use the same hyperparameters as those used in Section 6. $\\mathbf{w}_0$ is initialized as $\\mathbf{w}_0=10\\cdot\\\\mathbf{e}\\_1$, representing the case where $\\mathbf{w}_0-\\mathbf{w}^*$ is refined mainly to the subspace of large eigenvalues, or $\\mathbf{w}_0=10\\cdot\\mathbf{e}\\_{10}$, representing the case where $\\mathbf{w}_0-\\mathbf{w}^*$ is refined mainly to the subspace of small eigenvalues. The experimental results are shown in Figures 3-5 in Appendix A. From our experiments, we can observe that the variance error of ASGD is larger than that of SGD, and that the bias error of ASGD is smaller when $\\mathbf{w}_0-\\mathbf{w}^*$ is refined mainly to the subspace of small eigenvalues. Please refer to Appendix A in the revision for more details.\n\n---\n\n**Q2**. How is overparameterization reflected in Theorem 4.1?\n\n**A2**. The excess risk bound provided in Theorem 4.1 is not explicitly related to the model dimension $d$. Instead, it depends on $k^*$ which is referred to as the effective dimension. For many data covariance matrix $\\mathbf{H}$, even if $d$ is very large or even goes to infinity (i.e., overparameterization regime), $k^*$ can still be small or finite, and the excess risk bound does not go to infinity. This improves previous results that explicitly depend on the model dimension $d$ (Jain et al. 2018). This is how overparameterization is reflected in Theorem 4.1.\n\n---\n\n**Q3**. Please provide a more detailed explanation of the challenges encountered in theoretical analysis and how to address them in the submission.\n\n**A3**. The most significant challenge is that, different from previous methods analyzing SGD (Zou et al. ,2021; Wu et al. ,2022), the monotonicity of the second moment for the bias error $\\mathbf{B}_t$ is violated in ASGD. Therefore, we need to calculate the explicit expression of $\\mathbf{A}_i^k$, and analyze quantities like those in Lemma K.5, Lemma K.6 and Lemma K.7.\n\nAnother challenge is the identification of the eigenvalue cutoffs $k^\\dagger$, $k^\\ddagger$, $\\hat k$ and $k^*$. This is much more complicated than SGD because $\\lambda_i$ significantly affects the decay rate of $\\mathbf{A}_i^k$. Specifically, the eigenvalues of $\\mathbf{A}_i$ can be complex or real, which also depends on $\\lambda_i$. As a comparison, in the SGD results (Zou et al. ,2021; Wu et al. ,2022), there are only two cutoffs for eigenvalue.\n\nLast but not least, we develop a new choice of parameters (Eq. (4.2)) for ASGD in the overparameterized regime. This choice of parameters is obtained through a fine-grained analysis of the effect of fourth-moment (see details in Appendix F.2), where we manage to avoid the model dimension $d$ in our choice of parameters.\n\n---\n\n**Q4**. Can you provide more instances that satisfy Assumption 3.2?\n\n**A4**. Yes. The first example is: $\\mathbf{H}^{-1/2}\\mathbf{x}$ is $\\sigma^2$-sub-Gaussian, then Assumption 3.2 is satisfied with $\\psi=16\\sigma^4$ (see Lemma A.1 of Zou et al., 2021). Another example is: $\\mathbf{H}^{-1/2}\\mathbf{x}$ is $\\sigma^2$-sub-exponential, then Assumption 3.2 is also satisfied with $\\psi=256\\sigma^4$.\n\n---\n\nJain et al. \"Accelerating stochastic gradient descent for least squares regression.\" In Conference On Learning Theory, pp. 545-604. PMLR, 2018.\n\nZou et al. \"Benign overfitting of constant-stepsize SGD for linear regression.\" In Conference on Learning Theory, pp. 4633-4635. PMLR, 2021.\n\nWu et al. \"Last iterate risk bounds of sgd with decaying stepsize for overparameterized linear regression.\" In International Conference on Machine Learning, pp. 24280-24314. PMLR, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553947786,
                "cdate": 1700553947786,
                "tmdate": 1700554253500,
                "mdate": 1700554253500,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5WbhUGPPxh",
            "forum": "AcoXPIPh4A",
            "replyto": "AcoXPIPh4A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8252/Reviewer_BSyC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8252/Reviewer_BSyC"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyze stochastic gradient descent with momentum in the overparametrized linear regression setting and they study excess risk bounds, showing that the variance of this algorithm is never better than for SGD and that the bias is also larger for the subspace of largest eigenvalues. They show that for the subspace of the the lowest eigenvalues, the bias of the algorithm is smaller than that of SGD"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In the strongly convex regime, the authors get better bias error term than previous work (except for a term, that the authors claim that can be removed but that wasn't removed). They extend the techniques in Jain et al 2018 to the overparametrized setting which allows them to specify what happens in their setting with ASGD\nThe paper is well written."
                },
                "weaknesses": {
                    "value": "It is known from before in many settings that accelerated gradient descent does work very well with noise, and in fact, this is what is corroborated by this work for the setting of overparametrized linear regression. The variance is shown to be greater, the bias is shown to be greater for the subspace of large eigenvalues, which are the most important ones. The claim in the abstract about ASGD outperforming SGD if the initialization minus optimizers lives in the subspace of the small eigenvalues is true but a bit useless, since it is very unlikely this happens. It is a low dimensional subspace and most of the rest of the space is dominated by the large eigenvalues.  It is informative and of value to have all of the details in this setting that are provided in this work, but the results are weak, essentially a negative result that could have maybe been anticipated for this kind of algorithm."
                },
                "questions": {
                    "value": "Can you provide any examples of settings in which you can guarantee you can initialize to be in the good regime that you show for ASGD, i.e. when $w_0 -w^\\ast$ is essentially aligned with the subspace associated to small eigenvalues?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837876569,
            "cdate": 1698837876569,
            "tmdate": 1699637026046,
            "mdate": 1699637026046,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ADmtuf0mPh",
                "forum": "AcoXPIPh4A",
                "replyto": "5WbhUGPPxh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8252/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BSyC"
                    },
                    "comment": {
                        "value": "Thank you for your helpful feedback.\n\n\n**Q1**. The claim in the abstract about ASGD outperforming SGD if the initialization minus optimizers lives in the subspace of the small eigenvalues is true but a bit useless, since it is very unlikely this happens. It is a low dimensional subspace and most of the rest of the space is dominated by the large eigenvalues. \n\n**A1**. We would like to emphasize that this result in actually useful in the sense that (1) even for the simplest possible problem of linear regression, no prior work has demonstrated when ASGD can outperform SGD, and we are the first to give a quantitative condition, which deepens the understanding of ASGD vs. SGD in terms of generalization performance; (2) It is not \u201cunlikely\u201d that $\\mathbf{w}_0-\\mathbf{w}^*$ aligns well with the subspace of small eigenvalues. In practice, one often uses zero initialization or randomly initialization $\\mathbf{w}_0$ that is close to zero. In this case, when the ground truth $\\mathbf{w}^*$ is more aligned with the subspace of small eigenvalues, we will have $\\mathbf{w}_0-\\mathbf{w}^*$ is more aligned well with the subspace of small eigenvalues.\n\n\n---\n\n\n**Q2**. It is informative and of value to have all of the details in this setting that are provided in this work, but the results are weak, essentially a negative result that could have maybe been anticipated for this kind of algorithm.\n\n**A2**. We would like to emphasize that our work not only provides a \u201cnegative\u201d result, but also a \u201cpositive\u201d result, which identifies a class of linear regression instances for which ASGD can outperform SGD (in Theorem 5.1). Moreover, we believe that our result is strong: it is the first result characterizing the excess risk of ASGD in the overparameterized regime. From a technical point of view, our result is also highly nontrivial to obtain (See the key techniques in Section 7).\n\n---\n\n**Q3**. Can you provide any examples of settings in which you can guarantee you can initialize to be in the good regime that you show for ASGD, i.e. when $\\mathbf{w}_0-\\mathbf{w}^*$ is essentially aligned with the subspace associated to small eigenvalues?\n\n**A3**. Yes, we can first consider a $2$-dimensional model where $\\lambda_1=1$ and $\\lambda_2=0.01$, the ground truth weight vector is $\\mathbf{w}^*=(1, 10)^\\top$, and the initialization is $\\mathbf{w}_0=(0, 0)^\\top$. This is exactly the case where $\\mathbf{w}_0-\\mathbf{w}^*$ lies primarily in the subspace of small eigenvalues of the data covariance matrix. This type of ill-posed model is quite common in practice. It is also very easy to extend this example to the high-dimensional settings."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553332374,
                "cdate": 1700553332374,
                "tmdate": 1700553537252,
                "mdate": 1700553537252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9rRj64Aa17",
                "forum": "AcoXPIPh4A",
                "replyto": "5WbhUGPPxh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8252/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer BSyC,\n\nThank you again for your insightful comments. We would like to follow up with you and address any outstanding questions if you still have. In our response, we have provided a representative linear regression instance where $\\mathbf{w}_0-\\mathbf{w}^*$ is mainly refined to the space of large eigenvalues of $\\mathbf{H}$. Please let us know if you have any other suggestions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626120930,
                "cdate": 1700626120930,
                "tmdate": 1700626120930,
                "mdate": 1700626120930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7YckWq3x5h",
                "forum": "AcoXPIPh4A",
                "replyto": "9rRj64Aa17",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8252/Reviewer_BSyC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8252/Reviewer_BSyC"
                ],
                "content": {
                    "comment": {
                        "value": "When I asked for examples of settings I was not referring to this. It is clear that one can set up a synthetic example that satisfies the condition tautologically. A priori I don't know if my solution is going to be aligned with the smallest eigenvalues, so if I were to run SGD or ASGD, I'd go for SGD or I would apply a random rotation and then run SGD. I was asking whether you could give any examples of problems, that maybe because of their nature they have some structure so we know this low-eigenvalue alignement condition is going to be satisfied."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639059339,
                "cdate": 1700639059339,
                "tmdate": 1700639059339,
                "mdate": 1700639059339,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "biiJu51cof",
            "forum": "AcoXPIPh4A",
            "replyto": "AcoXPIPh4A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8252/Reviewer_hi74"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8252/Reviewer_hi74"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization of ASGD for overparameterized linear regression, which is possibly the simplest setting of learning with overparameterization. This paper establishes an instance dependent excess risk bound for ASGD within each eigen-subspace of the data covariance matrix. The analysis shows that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD.\nThe result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues. Additionally, when the analysis is specialized to linear regression in the strongly convex setting, it yields a tighter bound for bias error than the best-known result."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The analysis shows that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD.\nThe result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues. Additionally, when the analysis is specialized to linear regression in the strongly convex setting, it yields a tighter bound for bias error than the best-known result."
                },
                "weaknesses": {
                    "value": "No"
                },
                "questions": {
                    "value": "No"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699150167045,
            "cdate": 1699150167045,
            "tmdate": 1699637025901,
            "mdate": 1699637025901,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZGEVP2W8Xc",
                "forum": "AcoXPIPh4A",
                "replyto": "biiJu51cof",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8252/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hi74"
                    },
                    "comment": {
                        "value": "Thank you very much for your strong support!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553240037,
                "cdate": 1700553240037,
                "tmdate": 1700553240037,
                "mdate": 1700553240037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]