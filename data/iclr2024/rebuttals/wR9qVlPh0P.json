[
    {
        "title": "AutoVP: An Automated Visual Prompting Framework and Benchmark"
    },
    {
        "review": {
            "id": "ivws073cnU",
            "forum": "wR9qVlPh0P",
            "replyto": "wR9qVlPh0P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission509/Reviewer_aYHC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission509/Reviewer_aYHC"
            ],
            "content": {
                "summary": {
                    "value": "This paper mainly focuses on visual prompting method. The authors propose a new framework which covers a wide range of design space for visual prompting including input scaling, pretrained model, output mapping. The authors introduce several difference techniques for these dimension which are shown to be effective in the experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is simple yet effective.\n2. The writing is easy to understand."
                },
                "weaknesses": {
                    "value": "1. The input scaling technique was discussed in EVP [1]. The authors may have some more discussion about the relationship and difference with this work.\n2. The different choices for pretrained model are almost the same as the original ones in VP. It would be better if the author can mention this in the paper.\n3. I doubt if the proposed FullyMap can be viewed as an output mapping method rather than a variant of linear probing. By using this method, the distribution of trainable parameters is totally different from the original VP-style methods like VP, EVP and BlackVIP [2] whose trainable parameters only concentrate on modifying the input space without the output space. It is for sure that such a method can bring significant improvement in the experiments. Even if you do not use the visual prompt but simply finetune the model with the additional last fc layer, I assume the model can have similar performance to the linear probed one.\n4. I recommend the authors adopt other datasets used in previous VP papers like SUN, CLEVR and RESISC, and compare the proposed method with other baselines like BlackVIP.\n\n\n[1] Wu J, Li X, Wei C, et al. Unleashing the power of visual prompting at the pixel level[J]. arXiv preprint arXiv:2212.10556, 2022.\n[2] Oh C, Hwang H, Lee H, et al. BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 24224-24235."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission509/Reviewer_aYHC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697943114363,
            "cdate": 1697943114363,
            "tmdate": 1700720882136,
            "mdate": 1700720882136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CmA0AAL47C",
                "forum": "wR9qVlPh0P",
                "replyto": "ivws073cnU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aYHC (1 of 4)"
                    },
                    "comment": {
                        "value": "Dear Reviewer aYHC,\n\nThank you for your time and effort in reviewing our paper. We appreciate that you enjoyed reading our paper, and recognized our proposed framework is simple yet effective. To your questions, we address our response in the following.\n\n**Q1. More Discussion on EVP**\n\nThank you for your thoughtful reminder! EVP is indeed a very inspirational paper that motivates our work. While we did cite EVP, our discussion of their findings on input scaling was omitted. We referenced their findings in Section 3. It's important to highlight that the input scaling in AutoVP extends beyond determining the size of prompt frames and resizing images accordingly. We integrate a **dynamic optimization method** during the training process, elaborated in Section 3: Input Scaling. This represents the most significant departure from EVP, which merely shrinks the input image and fixes the prompt size. Our design optimizes both prompts and prompt sizes, and adjusts the input image accordingly, maximizing the effectiveness of prompts."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480266959,
                "cdate": 1700480266959,
                "tmdate": 1700480266959,
                "mdate": 1700480266959,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iQmAmFvMK1",
                "forum": "wR9qVlPh0P",
                "replyto": "ivws073cnU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aYHC (2 of 4)"
                    },
                    "comment": {
                        "value": "**Q2. The Choices for Pre-trained Model**\n\nWithin the AutoVP framework, we have opted to utilize state-of-the-art classification models, comprising residual network-based, vision transformer-based, and multi-modal models (such as CLIP), within our pre-trained classifier set. The decision to select similar pre-trained backbones aims to ensure equitable comparisons with previous studies, as highlighted in the fixed model comparison in Tables 2 and 8 of the paper. However, we have also integrated new pre-trained models, such as **Swin-T**. Notably, in Table 9 -- AutoVP with source model selection, Swin-T emerges as the most frequently chosen model, illustrating its significant role that contributes to the overall performance of AutoVP."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480336590,
                "cdate": 1700480336590,
                "tmdate": 1700480336590,
                "mdate": 1700480336590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C5971lb0Gd",
                "forum": "wR9qVlPh0P",
                "replyto": "ivws073cnU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aYHC (3 of 4)"
                    },
                    "comment": {
                        "value": "**Q3. FullyMap and LP**\n\nWhile LP and FullyMap share similarities due to both incorporating a linear head, we conducted a comparison between LP and pure FullyMap (without visual prompts). The results shown in the **table below**, demonstrate that **FullyMap without VP exhibits inferior performance compared to LP** on most datasets. This discrepancy may arise from the inability of the pre-trained model's learned features in the linear head to be interpreted for downstream classes, highlighting why FullyMap is not equivalent to LP.\n\nFor a more detailed exploration of FullyMap, **Appendix B.10** Figure 15 illustrates that FullyMap can be interpreted as a weighted combination of multiple source labels, where some human-readable features may exhibit similarity. For instance, in Figure 15(a), 'bumpy' shows similarities with 'custard apple,' 'disk brake,' and 'pineapple,' while in Figure 15(b), 'scaly' shares similar features with 'boa constrictor,' 'coho,' and 'common iguana.'\n\nWhile Figure 16(b) portrays IterMap as successfully mapping to classes more closely aligned with the target classes than FullyMap (Figure 16(a)), a significant accuracy gap is evident. In Figure 16, FullyMap achieves 69.96% accuracy, whereas IterMap-1 only reaches 40.77%. This highlights that merely combining source labels is insufficient for superior performance; **the weighting within the combination plays a crucial role, precisely what FullyMap achieves**.\n\n|   Dataset  | Linear Probing | Non-VP + FullyMap | FullyMap - LP |\n|:----------:|:--------------:|:-----------------:|:-------------:|\n|    SVHN    |      65.4      |        34.2       |     -31.2     |\n| Flowers102 |      96.9      |        89.1       |      -7.8     |\n|   UCF101   |      83.3      |        80.2       |      -3.1     |\n|    Pets    |      89.2      |        91.3       |      +2.1     |\n|  CIFAR100  |      80.0      |        74.5       |      -5.5     |\n|   EuroSAT  |      95.3      |        73.2       |     -21.1     |\n|     DTD    |      74.6      |        67.7       |      -6.9     |\n|    ISIC    |      71.9      |        42.6       |     -29.3     |\n|    FMoW    |      36.3      |        34.9       |      -1.4     |\n|    GTSRB   |      85.8      |        66.0       |     -19.8     |"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480463067,
                "cdate": 1700480463067,
                "tmdate": 1700481198969,
                "mdate": 1700481198969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vEXXTlWxfA",
                "forum": "wR9qVlPh0P",
                "replyto": "ivws073cnU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aYHC (4 of 4)"
                    },
                    "comment": {
                        "value": "**Q4-1. Other Vision Datasets**\n\nWe appreciate the reviewer's mention of these prominent datasets. We incorporated additional datasets\u2014**SUN397, RESISC, CLEVR, and StanfordCar** \u2014and their results are available in **Appendix B.4 Table 5**.\n\nOur findings demonstrate that AutoVP generally outperforms ILM-VP and CLIP-VP across most datasets. When compared to linear probing (LP), the out-of-distribution (OOD) dataset, CLEVR, demonstrated a significant 15% increase in accuracy. However, the in-distribution (ID) datasets, SUN397 and StanfordCar, show inferior performance than LP. This aligns with our findings discussed in Section 5: Performance Evaluation on ID/OOD.\n\n**Q4-2. Comparison with BlackVIP**\n\nWe thank the reviewer for pointing out BlackVIP, one of the pioneering works in VP. However, we did not compare it with AutoVP because BlackVIP operates in a black-box setting, treating the pre-trained backbone as a black box. In contrast, AutoVP views pre-trained backbones as white-box models. Notably, AutoVP focuses on the concept of 'reprogramming' an online API using publicly released model architecture and weights, even though direct access to the model itself is not possible.\n\nDespite this distinction, we present a comparison across 11 datasets with BlackVIP, and the results are summarized in the table below. Since BlackVIP selects CLIP as the pre-trained backbone, we report our results under the same pre-trained model. Our results demonstrate a **16% outperformance compared to BlackVIP**. This considerable gap might be attributed to the difference in *update strategies*: BlackVIP employs SPSA-GC for black-box models, whereas AutoVP utilizes classic gradient descent. Given the differing objectives of these two studies, direct comparisons may present certain unfairness. This discussion has been added in **Appendix B.14**.\n\n|          | Pets | Cars  | Flowers | Food | SUN   | DTD   | SVHN | EuroSAT | RESISC | CLEVR | UCF  | Avg.  |\n|----------|------|-------|---------|------|-------|-------|------|---------|--------|-------|------|-------|\n| AutoVP   | 88.2 | 61.82 | 90.4    | 82.3 | 65.42 | 62.5  | 92.9 | 96.8    | 88.46  | 82.76 | 73.1 | 80.42 |\n| BlackVIP | 89.7 |  65.6 | 70.6    | 86.6 | 64.7  | 45.2  | 44.3 | 73.1    | 64.5   | 36.8  | 69.1 | 64.56 |"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480745080,
                "cdate": 1700480745080,
                "tmdate": 1700480745080,
                "mdate": 1700480745080,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "61WPsA6gkv",
                "forum": "wR9qVlPh0P",
                "replyto": "ivws073cnU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Reviewer_aYHC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Reviewer_aYHC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the feedback. Most of my questions have been solved except for Q3. It is still not clear how such a technique can be viewed as an improved version of VP since they change totally different layers in a model, one for input and one for the prediction."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632638118,
                "cdate": 1700632638118,
                "tmdate": 1700632638118,
                "mdate": 1700632638118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wbWDhKYEOo",
                "forum": "wR9qVlPh0P",
                "replyto": "ivws073cnU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your positive feedback!"
                    },
                    "comment": {
                        "value": "Dear Reviewer aYHC,\n\nWe are delighted to learn that \"Most of my questions have been solved except for Q3\". We understand your concern when we cast the fully map as a generalized/improved version of label mapping (i.e., soft label mapping with learnable weights). We will modify the wording in the future version to clarify this point. \n\nTo address your remaining concern, we would like to bring to your attention that AutoVP is indeed an improved version of existing VP methods, when we constrain the label mapping to be the same as baseline methods, such as IterMap and FreqMap. For example, in the table below (extracted from Table 2), when setting the same label mapping as IterMap-1, one can see that AutoVP improves the accuracy of ILM-VP. The reason is that the feature of input scaling optimization enabled by AutoVP contributes to improved accuracy. Similarly, when including FreqMap-1 (an existing label mapping method) in AutoVP, we also observe an obvious accuracy boost over ILM-VP, such as the Food101 dataset in the table below.\n\nWe hope these case studies can convince the reviewer that AutoVP is indeed an improved version of VP. We are at the reviewer's disposal to answer any remaining concerns the reviewer may have.\n\n|    Dataset    |   AutoVP Setting   |   AutoVP    | ILM-VP  |\n|:-------------:|:------------------:|:-----------:|:-------:|\n|    CIFAR10    |  IterMap-1, p = 23 |  95.2 \u00b1 0.0 |  94.4   |\n|      ISIC     |  IterMap-1, p = 16 | 74.0 \u00b1 1.0  |   73.3  |\n|    Food101    | FreqMap-1, p = 16  | 82.3 \u00b1 0.1  |   79.1  |"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635308794,
                "cdate": 1700635308794,
                "tmdate": 1700635573577,
                "mdate": 1700635573577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8V81fLBxuK",
                "forum": "wR9qVlPh0P",
                "replyto": "ivws073cnU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up to the Reviewer"
                    },
                    "comment": {
                        "value": "Dear Reviewer aYHC,\n\nAs the discussion period is closing soon, we'd like to follow up and see if the Reviewer has had a chance to consider our response. The additional experimental results on the detection task have been provided in our [Update Response](https://openreview.net/forum?id=wR9qVlPh0P&noteId=vayyWGrn3M). We hope our responses and new results are helpful for finalizing the review rating.\n\nYours Sincerely,\n\nAuthors"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720318288,
                "cdate": 1700720318288,
                "tmdate": 1700721592257,
                "mdate": 1700721592257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k28UIxjE0i",
            "forum": "wR9qVlPh0P",
            "replyto": "wR9qVlPh0P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission509/Reviewer_dWYk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission509/Reviewer_dWYk"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces AutoVP, an end-to-end framework that automatically selects optimal visual prompt (VP) configurations for vision downstream datasets. Tested on 12 datasets, AutoVP demonstrates improvements over the traditional linear probing approach, and on OOD test. The performance enhancements are in line with expectations. The significance of AutoVP lies in its automation of the VP configuration process, offering an extensive study for prompt engineering with gains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. An extensive study for visual prompting on vision model such as ResNeXt, ViT, and CLIP model. \n\n2. AutoVP, by applying a series of established approach, from input scaling, to output label engineering, enables huge gain on the results."
                },
                "weaknesses": {
                    "value": "1. The paper is evaluated on 12 visual recognition tasks, what about other tasks, given that this is a benchmark paper. Say Object Detection, Depth, Segmetnation. \n\n2. Reviewer appreciate this systematic study in applying all methods of VP and improve results. However, those results are expected.  Learn a bit of new knowledge after reading this, the reviewer would expect in general more surprising finding or impressive knowledge."
                },
                "questions": {
                    "value": "No"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698864193924,
            "cdate": 1698864193924,
            "tmdate": 1699635977417,
            "mdate": 1699635977417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tVzNiZ1XI4",
                "forum": "wR9qVlPh0P",
                "replyto": "k28UIxjE0i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dWYk (1 of 2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer dWYk,\n\nThank you for your time and effort in reviewing our paper. To your questions, we address our response in the following.\n\n**Q1. Variety of Vision Tasks**\n\nWe appreciate the reviewer's interest in exploring AutoVP\u2019s performance on other vision tasks. We acknowledge the need for a closer examination of various vision tasks. To address this, we integrated AutoVP into a **segmentation task** and compared its performance with linear probing on both ID and OOD datasets. The results are provided in **Appendix B.6** and table below.\n\nIn our VP segmentation framework illustrated in Figure 12, a FullyMap is incorporated after the pre-trained model to enable pixel-wise classification with a custom class number. In contrast, the linear probing approach modifies the last 2D convolutional layer. Table 6 displays the outcomes.\n\nWe evaluated segmentation performance using two metrics: IoU (Intersection over Union) score and pixel-wise classification accuracy. **AutoVP exhibited superior performance on both metrics in the ISIC dataset**. Additionally, segmentation examples highlighted that predictions align more accurately with the ground truth mask when the prompt space is larger (see Figure 13). However, in the ID dataset (Pets), VP performance was inferior to LP. This aligns with our findings in the classification task, where OOD datasets derived greater benefits from visual prompts. \n\n| Dataset |             LP            |           AutoVP          |\n|:-------:|:-------------------------:|:-------------------------:|\n|   Pets  | IoU : 0.83, Pixel : 90.7% | IoU : 0.77, Pixel : 86.9% |\n|   ISIC  | IoU : 0.64, Pixel : 78.1% | IoU : 0.81, Pixel : 89.5% |\n\nAs for **detection tasks**, we are still working on the experiments. We aim to provide those results before the end of the author-reviewer rebuttal deadline."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479976559,
                "cdate": 1700479976559,
                "tmdate": 1700481088841,
                "mdate": 1700481088841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d0QQe4xxkB",
                "forum": "wR9qVlPh0P",
                "replyto": "k28UIxjE0i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dWYk (2 of 2)"
                    },
                    "comment": {
                        "value": "**Q2. AutoVP Novelty and Insights**\n\nWe understand that as a unified automated visual prompting (VP) framework, the reviewer may feel that it lacks originality because it contains many visual prompting methods as special cases. However, we would like to respectfully point out several new components and designs in our AutoVP that the reviewer may overlook, as well as highlight our major contributions to VP. \n\n**[Our novel designs for VP]** AutoVP introduces novel components that have not been studied in prior arts in VP, including the **automated input scaling**, as well as **weight initialization** when using FullyMap. These additions contribute to the uniqueness of our framework, positioning AutoVP not just as a mere automated tuning tool, but as an advanced and comprehensive framework. It amalgamates a multitude of techniques to discern optimal VP configurations for datasets that exhibit diverse characteristics. More importantly, we also show that these two components are very critical to improving VP performance. For example, the optimal configurations in Table 2 suggest that different datasets prefer distinct image scales, and weight initialization with FullyMap is one of the most frequently selected output mapping methods. In doing so, our work transcends the confines of being solely a tuning tool and stands as a powerful toolset for effectively developing and deploying VP across a range of scenarios.\n\n**[Our contribution in demonstrating the advantage of VP]** It's worth noting that one of our pivotal achievements is the notable enhancement in VP performance beyond the established linear probing baseline, which had not been demonstrated in prior arts  (ILM-VP [5] and CLIP-VP [2]). In particular, our data scalability analysis in Sec. 4.1 shows that AutoVP substantially outperforms LP in the few-shot settings (10% and 1% data usage). We believe this is a significant and exciting finding, because efficient few-shot learning is exactly the motivation for prompting. \n\n**[Our contribution in providing new insights for VP]** Another noteworthy result is **our comprehensive analysis of VP on the OOD/ID dataset**, where we show that using AutoVP can significantly improve accuracy (as showcased in Figure 5(b)). This outcome highlights the capacity of AutoVP to operate with a wider prompt space when dealing with OOD datasets like SVHN and GTSRB, thereby leading to substantial accuracy gains. This aspect underscores VP's inherent adaptability to different dataset characteristics and has not been systematically studied in the existing literature.\n\nIn response to this comment, we highlighted the novelty of AutoVP in the revision within the following sections. In the introduction, we have emphasized the **novel components (automated input scaling and weight initialization)** in our main contributions to provide readers with a clear impression at the outset. Additionally, we delve into a detailed discussion of the contributions made by these new modules in Section 5. Furthermore, we've incorporated the reviewer's suggestions to integrate a **segmentation task** into our revised version, available in Appendix B.6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480155645,
                "cdate": 1700480155645,
                "tmdate": 1700480155645,
                "mdate": 1700480155645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4V2SKX5SCb",
                "forum": "wR9qVlPh0P",
                "replyto": "k28UIxjE0i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up to the Reviewer"
                    },
                    "comment": {
                        "value": "Dear Reviewer dWYk,\n\nAs the discussion period is closing soon, we'd like to follow up and see if the Reviewer has had a chance to consider our response. The additional experimental results on the detection task have been provided in our [Update Response](https://openreview.net/forum?id=wR9qVlPh0P&noteId=vayyWGrn3M). We hope our responses and new results are helpful for finalizing the review rating.\n\nYours Sincerely,\n\nAuthors"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720305849,
                "cdate": 1700720305849,
                "tmdate": 1700721556254,
                "mdate": 1700721556254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5Pif4x4J4X",
            "forum": "wR9qVlPh0P",
            "replyto": "wR9qVlPh0P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission509/Reviewer_PZJW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission509/Reviewer_PZJW"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces AutoVP, a comprehensive framework for automating VP design decisions. This framework covers several aspects, including optimizing the prompts, selecting suitable pre-trained models, and determining the best output mapping strategies. \nThe AutoVP along with a set of 12 image-classification tasks serves as a benchmark for evaluating VP performance. \n\nAutoVP is shown to outperform existing VP methods and achieves a considerable performance boost when compared to the linear-probing baseline. Above all, AutoVP offers a hyperparameter tuning tool for VP and a benchmark to further its advancements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper presents its findings with clear figures and detailed statistical reports, making it easier for readers to grasp the results.\n2. This paper does not just present a tool but embarks on a detailed exploration of optimal configurations under various conditions, aiming at proving how different settings affect performance. It also examines the impact of domain similarity on VP performance."
                },
                "weaknesses": {
                    "value": "1. While VP can potentially be used for a variety of vision tasks, the paper seems to focus primarily on image classification tasks, which may limit its applicability to broader vision problems. Are there any additional results on dense discriminant tasks?\n2. When utilizing CLIP as the pre-trained classifier within the framework, which visual backbone is employed, ViT or ResNet?\n3. About the proposed VP benchmark, why do the authors exclude some widely recognized 2D datasets, such as Caltech101, OxfordPets, StanfordCars, SUN397, EuroSAT, and FGVCAircraft, which are all common-used for 2D image classification task evaluation? What are the criteria for dataset selection?\n4. Table 2 reveals that AutoVP underperforms on **6** datasets out of **12** in the benchmark compared to Linear Probing (LP), e.g., AutoVP is **-6.5%, -10.2%, -12.1%** lower than LP on Flowers102, UCF101, and DTD respectively. The reported average accuracy improvement appears to be significantly influenced by the results from the SVHN dataset, which is **+27.5%**. Could the authors provide additional insights into this discrepancy? Furthermore, similar patterns observed in Tables 6 and 7 suggest that these results may not be consistently solid across varied datasets.\n5. The paper mentions ***std*** for AutoVP, indicating some randomness. It's important to think about how this inconsistency could affect the reliability of AutoVP, especially compared to the more stable method of linear probing. While Table 5 indicates a shorter execution time per run for AutoVP, one might infer that achieving the reported performance could necessitate multiple runs, thereby affecting the efficiency.  This inconsistency and the potential extra time needed make it less practical in certain situations."
                },
                "questions": {
                    "value": "Please see the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699075738412,
            "cdate": 1699075738412,
            "tmdate": 1699635977336,
            "mdate": 1699635977336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vn6ftUk0xN",
                "forum": "wR9qVlPh0P",
                "replyto": "5Pif4x4J4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PZJW (1 of 5)"
                    },
                    "comment": {
                        "value": "Dear Reviewer PZJW,\n\nThank you for your time and effort in reviewing our paper. We appreciate that you enjoyed reading our paper. To your questions, we address our response in the following.\n\n**Q1. Variety of Vision Task**\n\nWe focused on image classification as our work is mainly based on previous VP works that also cope with classification tasks. However, we acknowledge the need for a closer examination of various vision tasks. To address this, we integrated AutoVP into a **segmentation task** and compared its performance with linear probing on both ID and OOD datasets. The results are provided in **Appendix B.6** and table below.\n\nIn our VP segmentation framework illustrated in Figure 12, a FullyMap is incorporated after the pre-trained model to enable pixel-wise classification with a custom class number. In contrast, the linear probing approach modifies the last 2D convolutional layer. Table 6 displays the outcomes.\n\nWe evaluated segmentation performance using two metrics: IoU (Intersection over Union) score and pixel-wise classification accuracy. **AutoVP exhibited superior performance on both metrics in the ISIC dataset**. Additionally, segmentation examples highlighted that predictions align more accurately with the ground truth mask when the prompt space is larger (see Figure 13). However, in the ID dataset (Pets), VP performance was inferior to LP. This aligns with our findings in the classification task, where OOD datasets derived greater benefits from visual prompts. \n\n| Dataset |             LP            |           AutoVP          |\n|:-------:|:-------------------------:|:-------------------------:|\n|   Pets  | IoU : 0.83, Pixel : 90.7% | IoU : 0.77, Pixel : 86.9% |\n|   ISIC  | IoU : 0.64, Pixel : 78.1% | IoU : 0.81, Pixel : 89.5% |\n\n\nAs for **detection tasks**, we are still working on the experiments. We aim to provide those results before the end of the author-reviewer rebuttal deadline."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479446576,
                "cdate": 1700479446576,
                "tmdate": 1700481058281,
                "mdate": 1700481058281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tWaygl76CO",
                "forum": "wR9qVlPh0P",
                "replyto": "5Pif4x4J4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PZJW (2 of 5)"
                    },
                    "comment": {
                        "value": "**Q2. CLIP Backbone**\n\nWe utilized CLIP with the **ViT-B/32** vision encoder backbone. We apologize for omitting this information; we have now included it in Section 3: Pre-trained Classifier."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479507222,
                "cdate": 1700479507222,
                "tmdate": 1700479524614,
                "mdate": 1700479524614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n5LBffHcxD",
                "forum": "wR9qVlPh0P",
                "replyto": "5Pif4x4J4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PZJW (3 of 5)"
                    },
                    "comment": {
                        "value": "**Q3. Other Vision Datasets**\n\nThere are no specific criteria for selecting datasets, and the datasets were chosen mainly from state-of-the-art VP baselines [1]. We appreciate the reviewer's mention of prominent datasets. Among the mentioned datasets, such as OxfordPets and EuroSAT, these were already included in our chosen sets. Moreover, we incorporated additional datasets\u2014**StanfordCars, SUN397, RESISC, and CLEVR**\u2014and their results are available in **Appendix B.4 Table 5**.\n\nOur findings demonstrate that AutoVP generally outperforms ILM-VP and CLIP-VP across most datasets. When compared to linear probing (LP), the out-of-distribution (OOD) dataset, CLEVR, demonstrated a significant 15% increase in accuracy. However, the in-distribution (ID) datasets, SUN397 and StanfordCar, show inferior performance than LP. This aligns with the findings discussed in Section 5: Performance Evaluation on ID/OOD.\n\n[1] Chen et al. Understanding and Improving Visual Prompting: A Label-Mapping Perspective (CVPR 2023)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479629949,
                "cdate": 1700479629949,
                "tmdate": 1700479629949,
                "mdate": 1700479629949,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tGg9pgeUXn",
                "forum": "wR9qVlPh0P",
                "replyto": "5Pif4x4J4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PZJW (4 of 5)"
                    },
                    "comment": {
                        "value": "**Q4. Accuracy Improvement across Datasets**\n\nAutoVP has exhibited noteworthy accuracy improvements on out-of-distribution (OOD) datasets such as SVHN. However, it displayed comparatively lower performance on specific in-distribution (ID) datasets. The considerable variation in performance across datasets is rooted in the extent of dissimilarity between the source and target datasets. This aspect has been discussed in Section 5: Performance Evaluation on ID/OOD."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479689297,
                "cdate": 1700479689297,
                "tmdate": 1700479689297,
                "mdate": 1700479689297,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yiz2T6kulP",
                "forum": "wR9qVlPh0P",
                "replyto": "5Pif4x4J4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PZJW (5 of 5)"
                    },
                    "comment": {
                        "value": "**Q5. Randomness of AutoVP and LP**\n\nWe thank the reviewer for pointing out this concern. However, the fact that we do not report the standard deviations of other baselines does not mean that they are more stable than AutoVP, but that most results are adapted from original papers, where their authors also do not report them in the paper. We apologize for causing the misunderstanding to the reviewer. To address this, we have provided the standard deviation of ISIC and EuroSAT for both AutoVP and linear probing in the table below. The results show that **AutoVP and LP exhibit similar standard deviation**, suggesting the stability of AutoVP.\n\n|         | AutoVP      | LP          |\n|---------|-------------|-------------|\n|    ISIC | 74.0 \u00b1 1.0  | 68.50 \u00b1 1.4 |\n| EuroSAT | 96.8 \u00b1 0.2  | 94.70 \u00b1 0.1 |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479773766,
                "cdate": 1700479773766,
                "tmdate": 1700479773766,
                "mdate": 1700479773766,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5O5VlBA2BH",
                "forum": "wR9qVlPh0P",
                "replyto": "5Pif4x4J4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up to the Reviewer"
                    },
                    "comment": {
                        "value": "Dear Reviewer PZJW,\n\nAs the discussion period is closing soon, we'd like to follow up and see if the Reviewer has had a chance to consider our response. The additional experimental results on the detection task have been provided in our [Update Response](https://openreview.net/forum?id=wR9qVlPh0P&noteId=vayyWGrn3M). We hope our responses and new results are helpful for finalizing the review rating.\n\nYours Sincerely,\n\nAuthors"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720294204,
                "cdate": 1700720294204,
                "tmdate": 1700721569576,
                "mdate": 1700721569576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DgPCbhsFjG",
            "forum": "wR9qVlPh0P",
            "replyto": "wR9qVlPh0P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission509/Reviewer_nUAn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission509/Reviewer_nUAn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes AutoVP, an end-to-end expandable framework for automating VP design choices along with 12 downstream image classification tasks that can serve as a holistic VP-performance benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Clarity and Logic**: The paper is well-structured and presents complex ideas clearly, making it understandable for readers.\n\n**Useful Framework**: AutoVP is introduced as a versatile toolbox that simplifies the development of visual prompts, offering a modular design and comprehensive functionalities.\n\n**Improved Performance**: The models tuned with AutoVP demonstrate a significant performance improvement over previous baselines across various image classification tasks."
                },
                "weaknesses": {
                    "value": "**Limited Novelty**: The framework largely combines existing methods, which might suggest a wrap-up of previous work rather than introducing new concepts, limiting the perceived novelty of the research.\n\n**Potential Overfitting**: AutoVP uses different settings for different datasets, raising the question of whether these are overfitted to specific tasks and what the implications are for a robust, universal setting.\n\n**Insufficient Analysis of Mapping Methods**: There is a lack of detailed comparison and analysis of the mapping methods used in visual prompting, which is necessary to understand their impact and provide more comprehensive insights."
                },
                "questions": {
                    "value": "I would suggest expanding the testing of visual prompting from image classification to other tasks like detection and segmentation. This would help ensure that the AutoVP framework is versatile and not just fine-tuned for specific tasks. Aim to create a benchmark that evaluates how well visual prompting works generally, across various types of visual tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission509/Reviewer_nUAn",
                        "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission509/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699587882353,
            "cdate": 1699587882353,
            "tmdate": 1700808471095,
            "mdate": 1700808471095,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C6i89G0xTI",
                "forum": "wR9qVlPh0P",
                "replyto": "DgPCbhsFjG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nUAn (1 of 4)"
                    },
                    "comment": {
                        "value": "Dear Reviewer nUAn,\n\nThank you for your encouraging review, and for recognizing our paper is well-written, the proposed toolbox is useful and has demonstrated its state-of-the-art efficacy across various image classification tasks. We are thrilled that you enjoyed our paper. To your questions, we address our response in the following.\n \n**Q1. Limited Novelty** \n\nWe understand that as a unified automated visual prompting (VP) framework, the reviewer may feel that it lacks originality because it contains many visual prompting methods as special cases. However, we would like to respectfully point out several new components and designs in our AutoVP that the reviewer may overlook, as well as highlight our major contributions to VP. \n\n**[Our novel designs for VP]** AutoVP introduces novel components that have not been studied in prior arts in VP, including the **automated input scaling**, as well as **weight initialization** when using FullyMap. These additions contribute to the uniqueness of our framework, positioning AutoVP not just as a mere automated tuning tool, but as an advanced and comprehensive framework. It amalgamates a multitude of techniques to discern optimal VP configurations for datasets that exhibit diverse characteristics. More importantly, we also show that these two components are very critical to improving VP performance. For example, the optimal configurations in Table 2 suggest that different datasets prefer distinct image scales, and weight initialization with FullyMap is one of the most frequently selected output mapping methods. In doing so, our work transcends the confines of being solely a tuning tool and stands as a powerful toolset for effectively developing and deploying VP across a range of scenarios.\n\n**[Our contribution in demonstrating the advantage of VP]** It's worth noting that one of our pivotal achievements is the notable enhancement in VP performance beyond the established linear probing baseline, which had not been demonstrated in prior arts  (ILM-VP [5] and CLIP-VP [2]). In particular, our data scalability analysis in Sec. 4.1 shows that AutoVP substantially outperforms LP in the few-shot settings (10% and 1% data usage). We believe this is a significant and exciting finding, because efficient few-shot learning is exactly the motivation for prompting. \n\n**[Our contribution in providing new insights for VP]** Another noteworthy result is **our comprehensive analysis of VP on the OOD/ID dataset**, where we show that using AutoVP can significantly improve accuracy (as showcased in Figure 5(b)). This outcome highlights the capacity of AutoVP to operate with a wider prompt space when dealing with OOD datasets like SVHN and GTSRB, thereby leading to substantial accuracy gains. This aspect underscores VP's inherent adaptability to different dataset characteristics and has not been systematically studied in the existing literature.\n\nIn response to this comment, we highlighted the novelty of AutoVP in the revision within the following sections. In the introduction, we have emphasized the **novel components (automated input scaling and weight initialization)** in our main contributions to provide readers with a clear impression at the outset. Additionally, we delve into a detailed discussion of the contributions made by these new modules in Section 5. Furthermore, we've incorporated the reviewer's suggestions to integrate a **segmentation task** into our revised version, available in **Appendix B.6**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478102894,
                "cdate": 1700478102894,
                "tmdate": 1700481558935,
                "mdate": 1700481558935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3b35nVpHhe",
                "forum": "wR9qVlPh0P",
                "replyto": "DgPCbhsFjG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nUAn (2 of 4)"
                    },
                    "comment": {
                        "value": "**Q2. Potential Overfitting**\n\nWe are not sure what the reviewer\u2019s meaning of \u201coverfitting to specific tasks.\u201d In our experiments, AutoVP is designed to automatically search for the best configuration of different downstream tasks given the **same** set of hyperparameter candidate sets, and its selection would vary according to the given datasets. \n\nWe speculate the reviewer would like to learn more about whether AutoVP would \u201coverestimate\u201d at the initial tuning phase. Indeed, as a hyper-parameter optimization (HPO) framework, the resulting configuration among many combinations tends to be overfitted or overestimated. This phenomenon was first discovered as \u201cthe multiple induction problem\u201d [1], also known as the \u201cwinner\u2019s curse\u201d in statistics. One can search all the combinations and train those models longer to avoid the sub-optimal tuning results. However, this may cost much more computation costs. Some research also suggests that if certain datasets are of good quality (well-balanced, large enough, etc.) then they would not be too sensitive to the hyper-parameter, which means the initial tuning phase is enough, and one can regard the tuning results as the optimal configuration.\n\nIn our paper, we also observe a similar phenomenon: when selecting CLIP as the pre-trained model, AutoVP would overestimate the performance of FreqMap and IterMap. That is, FullyMap tends to perform poorly in the hyper-parameter tuning process, yet may achieve higher test accuracy in full training. To address this issue, we use the weight initialization on FullyMap to fix it, which we have discussed in Section 4.2.\n\n[1] Jensen and Cohen. Multiple comparisons in induction algorithms. (Machine Learning)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478245729,
                "cdate": 1700478245729,
                "tmdate": 1700479176415,
                "mdate": 1700479176415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FsWz8nlZ0N",
                "forum": "wR9qVlPh0P",
                "replyto": "DgPCbhsFjG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nUAn (3 of 4)"
                    },
                    "comment": {
                        "value": "**Q3. Mapping Methods**\n\nThis is a great suggestion! We conducted an output mapping analysis on both FreqMap and FullyMap using the DTD dataset to gain insights into how these mappings have been learned. We have updated these results in **Appendix B.10**. Figure 15 illustrates that FullyMap can be interpreted as a weighted combination of multiple source labels, where some human-readable features may exhibit similarity. For instance, in Figure 15(a), 'bumpy' shows similarities with 'custard apple\u2019, 'disk brake\u2019, and 'pineapple\u2019, while in Figure 15(b), 'scaly' shares similar features with 'boa constrictor\u2019, 'coho\u2019, and 'common iguana\u2019.\n\nFurthermore, when comparing FullyMap and IterMap, a significant accuracy gap is observed: FullyMap reaches 69.96%, while IterMap-1 only reaches 40.77%. However, in Figure 16, FreqMap has mapped to some classes that are indeed very close to the target. For instance, in Figure 16(b), 'braided' maps to 'knot\u2019, 'bubbly' maps to 'bubble\u2019, and 'cobwebbed' maps to 'spider web\u2019. This demonstrates that a mere combination of source labels is insufficient for achieving better performance; **the weighting in the combination plays a crucial role, which is precisely what FullyMap accomplishes**."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478427973,
                "cdate": 1700478427973,
                "tmdate": 1700478427973,
                "mdate": 1700478427973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "prfc037F6a",
                "forum": "wR9qVlPh0P",
                "replyto": "DgPCbhsFjG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers",
                    "ICLR.cc/2024/Conference/Submission509/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nUAn (4 of 4)"
                    },
                    "comment": {
                        "value": "**Q4. Segmentation and Detection**\n\nWe acknowledge the need for a closer examination of various vision tasks. To address this, we integrated AutoVP into a **segmentation task** and compared its performance with linear probing on both ID and OOD datasets. The results are provided in **Appendix B.6** and table below.\n\nIn our VP segmentation framework illustrated in Figure 12, a FullyMap is incorporated after the pre-trained model to enable pixel-wise classification with a custom class number. In contrast, the linear probing approach modifies the last 2D convolutional layer. Table 6 displays the outcomes.\n\nWe evaluated segmentation performance using two metrics: IoU (Intersection over Union) score and pixel-wise classification accuracy. **AutoVP exhibited superior performance on both metrics in the ISIC dataset**. Additionally, segmentation examples highlighted that predictions align more accurately with the ground truth mask when the prompt space is larger (see Figure 13). However, in the ID dataset (Pets), VP performance was inferior to LP. This aligns with our findings in the classification task, where OOD datasets derived greater benefits from visual prompts. \n\n| Dataset |            LP            |          AutoVP          |\n|:-------:|:-------------------------:|:-------------------------:|\n|   Pets  | IoU : 0.83, Pixel : 90.7% | IoU : 0.77, Pixel : 86.9% |\n|   ISIC  | IoU : 0.64, Pixel : 78.1% | IoU : 0.81, Pixel : 89.5% |\n\n\nAs for **detection tasks**, we are still working on the experiments. We aim to provide those results before the end of the author-reviewer rebuttal deadline."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479093065,
                "cdate": 1700479093065,
                "tmdate": 1700481030533,
                "mdate": 1700481030533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hOHLA826aJ",
                "forum": "wR9qVlPh0P",
                "replyto": "DgPCbhsFjG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission509/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up to the Reviewer"
                    },
                    "comment": {
                        "value": "Dear Reviewer nUAn,\n\nAs the discussion period is closing soon, we'd like to follow up and see if the Reviewer has had a chance to consider our response. The additional experimental results on the detection task have been provided in our [Update Response](https://openreview.net/forum?id=wR9qVlPh0P&noteId=vayyWGrn3M). We hope our responses and new results are helpful for finalizing the review rating.\n\nYours Sincerely,\n\nAuthors"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission509/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720230169,
                "cdate": 1700720230169,
                "tmdate": 1700721579803,
                "mdate": 1700721579803,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]