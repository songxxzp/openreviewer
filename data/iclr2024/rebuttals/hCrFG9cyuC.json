[
    {
        "title": "PolyVoice: Language Models for Speech to Speech Translation"
    },
    {
        "review": {
            "id": "CDUV7HNDbi",
            "forum": "hCrFG9cyuC",
            "replyto": "hCrFG9cyuC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7518/Reviewer_2GYW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7518/Reviewer_2GYW"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a language model approach to speech-to-speech translation. The approach uses three models: one to map source semantic units to target semantic units, one to predict the duration of target semantic units, and one to predict target acoustic units given source acoustic units and target semantic units with duration information. The semantic units are derived using HuBERT (Hsu et al., 2021) while the acoustic units are derived using Soundstream (Zeghidour et al., 2021). Unlike VALL-E X (Zhang et al., 2023b), which uses phonetic units, this paper uses semantic units, enabling the approach to be extended to unwritten languages. The key novelty of this work is its use of semantic units instead of phonetic units, and its use of a decoder-only architecture that enables prompting. On the EMIME Chinese-English and CVSS English-Spanish speech-to-speech benchmarks, the proposed approach achieves similar translation results to VALL-E X, but with a large improvement in speech naturalness. When provided with ground-truth target texts, the proposed approach performs definitely worse in translation quality than VALL-E-X, but still better in naturalness. The paper reports a result where the model is used in an unwritten language scenario for English-to-Spanish translation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* Proposes a language model approach for speech-to-speech translation that uses semantic units instead of phonetic units, making it usable for unwritten languages.\n* Makes use of decoder-only architectures which enable effective prompting.\n* Reports ablation studies showing advantages of decoder-only over encoder-decoder architecture when using the same training data.\n* Demonstrates advantages of training the model on data from multiple tasks including ASR and MT.\n* Shows improvements in naturalness over VALL-E X in a zero-shot setting.\n* Performs ablation studies to show the utility of each model component."
                },
                "weaknesses": {
                    "value": "* It would have been preferable to report performance on a low-resource target language in an unwritten scenario. Such a setup might reveal additional challenges which are not present in a high resource language such as Spanish."
                },
                "questions": {
                    "value": "* 4.1.1:  For Chinese->English task, what is the size of the semantic unit inventory for Chinese and English?\n* Can the semantic unit inventory be shared between the source and target sides?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Reviewer_2GYW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638868926,
            "cdate": 1698638868926,
            "tmdate": 1699636907698,
            "mdate": 1699636907698,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2aqKpYXLvx",
                "forum": "hCrFG9cyuC",
                "replyto": "CDUV7HNDbi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2GYW"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your positive review and valuable feedback. We hope our response completely addresses any concerns you may have.\n \n&nbsp;\n\n**Feedback to the weakness:**\n\n- **[About the unwritten language scenario]**\n\nAs the data of an unwritten language is still very limited in literature, we follow the practice in the previous works, e.g.\n\nLee A, Gong H, et al. Textless speech-to-speech translation on real data. In Proc. NAACL 2022,\n\nLee A, Chen P J, et al. Direct speech-to-speech translation with discrete units. In Proc. ACL 2022,\n\nRongjie Huang, Jinglin Liu, et al. TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation. In Proc. ICLR 2023,\n\nwhich use a written language with transcriptions discarded to simulate an unwritten scenario.\n  \n&nbsp;\n\n**Feedback to the questions:**\n\n- **[About the size semantic unit inventory]**\n\nIn our core experiment, we use separated tokenized models for Chinese and English.\nIn this case, their semantic units are not shared.\nThe size of the semantic unit set is 500 for Chinese and 1000 for English, respectively. For more details, please refer to Section 4.1.1.\n \n- **[Can the semantic unit inventory be shared]**\n\nYes, if we use the same tokenized model for both languages.\nIn our ablation study (Section 5.3), we train a shared tokenized model for Chinese and English (mHuBERT_zh_en).\nIn this case, the semantic unit set is shared between the two languages."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734680823,
                "cdate": 1700734680823,
                "tmdate": 1700736298987,
                "mdate": 1700736298987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W6aZ9fi2Pv",
            "forum": "hCrFG9cyuC",
            "replyto": "hCrFG9cyuC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7518/Reviewer_PpHm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7518/Reviewer_PpHm"
            ],
            "content": {
                "summary": {
                    "value": "The proposed speech-to-speech (s2s) translation system consists of three models: translation model, duration model, and unit-to-speech model. The novelty of this work is that all of these models are decoder-only (while some prior work preferred encoder-decoder architectures) and the combination of these three models to do unit-based s2s."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The system design (using three decoder-only models) seems sound and worth investigating, although I'm not 100% on board with motivating it with the raise of GPT - there is more to the success of LLMs than being decoder-only models. Anyways, the main results (Table 2) look solid. The system description seems clear superficially, but there are some core open questions (see weaknesses)."
                },
                "weaknesses": {
                    "value": "I couldn't get a good sense of the training data - specifically how the different prompts from Table 1 are used to synthesize data, and the size of the synthesized dataset: is it the 44M sentences from Table 7 in the appendix, or more because multiple prompts are used? How does the training data compare to the baselines?\n\nMy main concern would be that the ablation studies are not effective for disentangling the many design choices and the many moving parts of the whole architecture. The encoder-decoder vs decoder-only comparison is a good start, but I still don't have a good sense about how well the synthetic data generation works, and how well each of the three models do in isolation. Possible interesting ablations would be removing the duration model, replacing u-xlm with an out-of-the-box asr/mt cascade, removing source speech dependency from u-slm, passing through n-bests between the models, etc. I'm not requesting that all possible ablations should be included, but giving a little bit more color to the paper story would make it much stronger."
                },
                "questions": {
                    "value": "See weaknesses:\n- Could you give more details on how the prompts are used\n- Could you give more details on the synthetic training data\n- Have you considered some of the ablation studies mentioned above?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680407544,
            "cdate": 1698680407544,
            "tmdate": 1699636907545,
            "mdate": 1699636907545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oysYXRwPLv",
                "forum": "hCrFG9cyuC",
                "replyto": "W6aZ9fi2Pv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PpHm"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive and valuable feedback, and we hope our response fully resolves your concern.\n \n- **[How the prompts are used]** \n\nWe apologize for an unclear presentation about how the prompts are used to synthesize data. Here is our explanation and we will modify our paper accordingly.\n\nTable 1 shows the prompt templates for different data types. Indeed, there is an instruction set for each data type to form the prompt.\nHere we list some examples of our synthetic training data:\n\n*Sample1*: Translate Chinese text \" \u987e\u5ba2\uff1a\u6253\u6270\u4e00\u4e0b\uff0c\u662f\u5728\u8fd9\u91cc\u6392\u961f\u5417\uff1f \" to English text: Customer: Excuse me, do I queue here?\n\n*Sample2*:  Chinese text \" \u987e\u5ba2\uff1a\u6253\u6270\u4e00\u4e0b\uff0c\u662f\u5728\u8fd9\u91cc\u6392\u961f\u5417\uff1f \" in English text: Customer: Excuse me, do I queue here?\n\n*Sample3*:  Translate the following sentence, \"\u987e\u5ba2\uff1a\u6253\u6270\u4e00\u4e0b\uff0c\u662f\u5728\u8fd9\u91cc\u6392\u961f\u5417\uff1f\"  from Chinese to English: Customer: Excuse me, do I queue here?\n\n*Sample4*: Translate Chinese unit \"<zh_21><zh_161><zh_155>...<zh_266><zh_199><zh_16>\" to Chinese text: \u4f60\u5e26\u7740\u73b0\u91d1\u79bb\u5f00\uff0c\u51cf\u53bb3%\u5de6\u53f3\u7684\u8d39\u7528\u3002\n\n*Sample5*: Translate English text \"douglas mcgray is going to be our guide you walk through the door, you see the red carpeting, you see someone in a suit. they may be greeting you.\" to English unit:  <293><63><662>...<6><407><334>\n\n*Sample6*: Translate Chinese unit \"<zh_16><zh_37><zh_111>...<zh_266><zh_199><zh_16>\" to English unit: <499><334><226>...<544><991><39>\n\nBy utilizing diverse construction templates or prompts, the training data can be significantly increased. This approach is also a mainstream method in training Large Language Models (LLMs) to enhance the diversity of instructions and improve model robustness. \u201c44M sentences\u201d refers to the statistical count of the original machine translation (MT) parallel data. They are multiplied by different instruction prompts selected from the instruction set.\n \n- **[Training data comparison with the baseline]**\n\nIn our paper, we use VALL-E X as our baseline as it is the closest work to us. Regarding the comparison of training data, please refer to the following table, where WenetST represents the end-to-end speech translation training data, expanded upon WenetSpeech by the authors of VALL-E X.\n\n  \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n  \nSystem     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;       |    **VALL-E X Trans (SpeechUT + VALL-E X )**   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |     **PolyVoice**\n\nASR data  &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;    |  LibriLight (60k hrs), WenetSpeech (10k hrs)   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;  | LibriLight (60k hrs), In-house (60k hrs)\n\nMT data   &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;     |  73M sentences      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                             | 44M sentences\n\nST/S2S data  &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;    |  WenetST (10k hrs), GigaST (10k hrs)      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         |  WenetS2S (10k hrs), GigaS2S (10k hrs)\n\n \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nWe have incorporated an additional in-house ASR dataset, specifically a Chinese ASR dataset, into the training of our U-XLM module. The primary objective behind using this dataset is to enhance the performance of our translation module. As a result of this integration, we have observed a notable improvement of 1-2 points in BLEU scores for the entire system. This increase offsets the performance loss incurred by employing unsupervised discretized units instead of phonemes. Although we do not release this particular ASR dataset, we believe a dataset with similar data volume can reproduce our results."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734239468,
                "cdate": 1700734239468,
                "tmdate": 1700738579221,
                "mdate": 1700738579221,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TNe2R1TCBv",
                "forum": "hCrFG9cyuC",
                "replyto": "W6aZ9fi2Pv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up response to Reviewer PpHm"
                    },
                    "comment": {
                        "value": "- **[More ablation studies]**\n \nThanks for the reviewer's constructive suggestion. Actually, we have already done some of the ablation studies suggested by the reviewer. In this work, we performed these analyses to evaluate the performance of individual modules. \n\nFor the evaluation of the U-XLM module, we fixed the U-SLM and tested the performance of different translation modules, as shown in Table 4, comparing S2UT with the encoder-decoder structure. \n\nFor the evaluation of the U-SLM module, we fixed the U-XLM and tested the performance of different synthesis modules, e.g. the comparison between U2S and Unit-based Vocoder. The results are posted in Table 2. \n\nIn Table 6, we have analyzed the impact of removing the duration module on the performance of speech synthesis. \n\nAs for the suggestion of replacing U-XLM with an out-of-the-box ASR/MT cascade, since the existing ASR->MT cascade systems do not support speech unit prediction, it is not easy to perform end-to-end evaluation of the cascade system with U-SLM as the back-end synthesis module. We will try our best to implement a unit-based cascade system and report the results."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734342027,
                "cdate": 1700734342027,
                "tmdate": 1700736238561,
                "mdate": 1700736238561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1FPezhFW7s",
            "forum": "hCrFG9cyuC",
            "replyto": "hCrFG9cyuC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7518/Reviewer_JZne"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7518/Reviewer_JZne"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a decoder-only based system for S2ST. The system includes three decoder-only LMs, including a translation LM to translate source language semantic units to target language semantic units, a duration LM to predict target language semantic unit durations and extend the sequence, and a speech synthesis LM to predict the target language acoustic units which are converted to waveforms by a unit vocoder. Both semantic units and acoustic units are self-supervised learned, hence this framework can be applied to unwritten languages."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1)\tThe use of decoder-only LMs via different prompting strategies and discrete semantic and acoustic units for all components (translation LM, duration LM, and speech synthesis LM) could benefit S2ST from competitive pre-trained text decoder-only LLMs.\n\n(2)\tEmpirical evaluations show that PolyVoice is comparable to VALL-E X, very slightly better on ASV, worse on ASR-BLEU, and better on naturalness.  Ablation studies show the contribution of the designed duration LM which uses a LM to predict durations of semantic units and extend the sequence. The duration LM significantly helps WER, with a very slight help on ASV and slight help on naturalness."
                },
                "weaknesses": {
                    "value": "(1)\tThe innovations of this work need to be more clearly explained. This work bears strong similarity to VALL-E X. It is important to clarify the difference between the proposed approach and VALL-E X, but the paper did not clearly point out the difference between PolyVoice and VALL-E X to highlight the innovations of the proposed PolyVoice.  Both works concatenate source and target semantic units and the source acoustic units to create the prompt for the LM. For PolyVoice, this prompt is created for the duration LM and the speech synthesis LM, respectively.  The soundstream codec is reimplemented, but the impact of the reimplementation is not clear. \n\n(2)\tSome of the key technical presentations lack clarity. \n\na.\tSection 3.1, when describing Unit-based cross-lingual language model (U-XLM) , the paper shows the prompt for encoder-decoder architecture.  The paper should also clarify the prompt for decoder-only LM. \n\nb.\tSection 3.1, when describing training, Table 1 shows how ASR, MT, and TTS supervised data are used for training. The paper also mentioned that \u201cThis approach also enables the direct utilization of unlabeled text and speech data. \u201c, yet how to use unlabeled text and speech data for training is not explained here. Instead, based on Section 4.1.1, it seems that one approach is to apply in-house MT and TTS systems on ASR data to create pseudo S2S data.  This part needs to be clarified.\n\n(3)\tMore complete and also deeper discussions are desired for empirical validations:\n\na.\tTable 2, the paper compares to cascaded (ASR-MT-TTS) and VALL-E X. It is not clear whether all well-established competitive baselines are included in this comparison. \n\nb.\tTable 2, the ASV metric evaluates the capability of preserving the voice of the source speaker. However, without ground truth target information, PolyVoice achieves 0.38 and 0.38 for hyp vs.src and hyp vs. tgt, while VALL-E X achieves 0.37 and 0.37 respectively. These are very small gains, yet the paper did not discuss this point.\n\nc.\tSection 3.1, the paper mentioned that CoT could be applied for source-to-target unit translation, yet prior works (Peng et al., 2023, Towards making the most of chatGPT for machine translation) find that when CoT is applied, the model tends to conduct word-by-word translation, hence degrading the translation performance. Table 2 shows the impact of applying CoT that it improves ASR-BLEU. But this result is not analyzed nor discussed.\n\nd.\tSection 4.3.2, the evaluation of PolyVoice for unwritten language is quite brief. It is only evaluated for the target language treated as an unwritten language.  It would be useful to also extend the evaluation to source or both languages as unwritten language. \n\ne.\tSection 5.1, again, the discussions are very brief.  More analyses and insights are expected to explain the better ASR-BLEU from decoder-only over encoder-decoder. \n\nf.\tSection 5.2, for the other tasks (ASR, ST, MT, TTS) in Table 5, since no baseline results are provided, it is not clear how these performances compare to baseline results on this dataset from prior works.\n\ng.\tSection 5.3, when using a mHuBERT model trained with more parameters and more data, the WER decreased which is explained, but ASV and naturalness got degraded.  Insights are expected to explain these results.\n\nh. The proposed system uses many in-house data and in-house systems. The amount of data and model sizes need to be clearly compared as well when comparing to baselines."
                },
                "questions": {
                    "value": "Please check comments and concerns listed under Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Reviewer_JZne"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698998048229,
            "cdate": 1698998048229,
            "tmdate": 1700688871804,
            "mdate": 1700688871804,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oKWtV8RETs",
                "forum": "hCrFG9cyuC",
                "replyto": "1FPezhFW7s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JZne"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for his constructive and valuable feedback. We assure you that our response aims to address your concern comprehensively and effectively.\n \n- **[The difference between our proposed approach and VALL-E X]**\n\nWe appreciate the reviewer's comments highlighting similarities between aspects of our work and VALL-E X. The core of our pipeline consists of a speech-to-unit translation (S2UT) component followed by a unit-to-speech (U2S) synthesis component. The unit-to-speech language model (U-SLM) in our U2T module shares conceptual similarities with VALL-E X, while other aspects of our overall model design differ. In response, we clarify key differences in motivation and approach:\n\n1. Our work is primarily motivated by exploring fully language model-based architectures for speech-to-speech translation, as a way to extend recent progress in large language models to this task. To our knowledge, we are the first to propose and evaluate such a fully decoder-only pipeline, demonstrating feasibility and encouraging results.\n\n2.  A key difference is that VALL-E X operates on phonemes, while we aim for a universal framework using discrete units that can be learned from unlabeled speech. This provides greater flexibility for low-resource and unwritten languages. While our current unit extraction quality limits gains over VALL-E X, our approach enables potential optimization with improved units and direct application to any language.\n\n3. There are also significant differences in the modeling approach between VALL-E X and PolyVoice for speech-to-speech translation tasks. In VALL-E X, a traditional pipeline system is used, which relies on external ASR (Automatic Speech Recognition) and ST (Speech-to-Text) modules to recognize phonemes in the source language and target language. On the other hand, PolyVoice employs the U-XLM module, which enables end-to-end speech-to-speech translation. This means that the translation task is performed directly without relying on separate ASR or ST modules. The U-XLM module can directly convert speech units in the source language into synthesized speech units in the target language, providing a seamless translation experience.\n\nOverall, we believe our work makes meaningful contributions in exploring language model-based S2ST, despite some similarities to VALL-E X in the speech synthesis component.\n \n- **[Re-implementation of soundstream codec]**\n\nThe main reason we have built our own codec model is that Google has not open-sourced their soundstream model. The only publicly available audio codec model was Encodec released by Meta, but its performance is worse than we anticipated. \nWe trained our own version of soundstream in order to obtain a better audio codec. And the performance of codecs was compared using a subjective metric very similar to  MUSHRA.\n \n- **[Key technical presentations]**\n\n*a. Prompt for model training.*\n\nTo clarify, the prompts shown in Table 1 are used to format the training data for our decoder-only language model, not an encoder-decoder architecture. Specifically, we construct varied training sequences using different prompts to cover diverse data types like ASR, MT, and S2ST. This allows us to train our language model in a decoder-only manner by modeling the training sequences in an auto-regressive way. \n\nWe do not actually utilize any encoder-decoder architectures in our proposed framework. We compare with encoder-decoder models in Section 5.1, where for those baselines we construct the training data in the same way as previous work\n(https://github.com/facebookresearch/fairseq/blob/main/examples/speech_to_speech/docs/direct_s2st_discrete_units.md).\n\n*b. Data utilization.*\n\nLike text-based language models, our framework allows unlabeled speech and text data to be directly leveraged for training via language modeling objectives. In our current implementation, we construct training data by formatting labeled ASR, MT, and pseudo-S2ST data with specific prompts, as detailed in Table 1. Simultaneously, parallel training data is also trained separately in monolingual form, and these type of training samples do not require construction through prompts.\n\nTo clarify, our proposed framework inherently supports training on unlabeled speech and text, similar to standard language model pretraining. We believe that scaling up the training data by incorporating large scales of unlabeled speech and text would enable more powerful models."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725424442,
                "cdate": 1700725424442,
                "tmdate": 1700736132075,
                "mdate": 1700736132075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "18NZUbfcPs",
                "forum": "hCrFG9cyuC",
                "replyto": "1FPezhFW7s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up response to Reviewer JZne"
                    },
                    "comment": {
                        "value": "- **[More complete and deeper discussion]**\n\nThank you for reminding us that more complete and deeper discussions are desired for our work. Your comments help us to improve the paper and the following are our responses.\n\n*a. Comparison with other well-established baselines.*\n\nWe recognize that VALL-E X is the most relevant prior work to compare against, given that they have demonstrated strong performance on speech translation tasks. Since our experiments focus on Chinese-English S2ST, where there are limited existing results available, we benchmark our model mainly against the performance of VALL-E X as the current baseline.\n \n*b. Explanation of the ASV metric.*\n\nWe acknowledge the reviewer's observation that our framework achieves only slight gains over VALL-E X in voice cloning quality. This is expected given the similar implementations for the speech synthesis component. However, by modeling on semantic units rather than phonemes, our model retains more acoustic information that can lead to higher naturalness and better synthesis effects. While not yet reflected in voice cloning metrics, our experiments provide a path to improve acoustic modeling by using unit representations.\n \n*c. Discussion on CoT decoding strategy.*\n\nBased on our understanding, the paper titled  \"Towards Making the Most of ChatGPT for Machine Translation\" primarily focuses on text-to-text translation. Therefore, their conclusions may not directly apply to speech-to-speech translation scenarios. However, in our specific case, we have found that the Chain of Thought (CoT) technique proves valuable when generating intermediate results of both the source and target text during unit-to-unit translation. Our findings align with a recent study conducted by Google, titled \"AudioPaLM: A Large Language Model That Can Speak and Listen\".\n\n*d. Evaluation of unwritten language.*\n\nThanks for the valuable advice to extend the evaluation to encompass unwritten languages, either as the source language or as both languages. We will incorporate the updated experimental results in the upcoming version. To clarify, the primary objective of experimenting on unwritten language scenarios in this paper is to demonstrate the feasibility of extending PolyVoice to unwritten languages.\n \n*e. Better ASR-BLEU from decoder-only over encoder-decoder.*\n\nThe comparison between the decoder-only and encoder-decoder models was conducted under the same training data and similar parameters. We hypothesize that the higher ASR-BLEU score achieved by the decoder-only model over the encoder-decoder model can be attributed to two factors. Firstly, the decode-only framework demonstrates superior capability in fitting large-scale training data. Secondly, the output generated by the decoder-only framework exhibits higher fluency, which positively impacts the evaluation of ASR-BLEU.\n \n*f. Performance of other tasks in Table 5.*\n\nGiven the involvement of multi-task learning (MTL), the primary objective of Table 5 is to demonstrate the overall S2ST gain achieved through MTL. Beating other systems in individual tasks is not our objective. We apologize for any misleading and we will improve our paper presentation. Still, as suggested by the reviewer, we would like to complement Table5 with the following table:\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nTask       &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;           | **ASR**\u2193 |  **ST**\u2191  &nbsp; |   **MT** \u2191 &nbsp; |  **TTS**\u2193\n\nPolyVoice (w/ MTL)   | 4.46 &nbsp;   | 30.8 |  33.81  |  6.99\n\nWhisper[1]   &nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;        | 2.28 &nbsp;  |  18.2 |    -   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  |    -\n\nYourTTS[2]        &nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;          |  -   &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;      |    - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   |      -  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;  |  3.03\n\nNLLB[3]         &nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;               |  -    &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    |    - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   |    25.2  &nbsp;&nbsp; |    -\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nThe evaluation mentioned above was performed using state-of-the-art open-sourced models, all evaluated on the same test set. As expected, Whisper, trained on substantially larger datasets, outperforms our model in terms of ASR performance. However, our model demonstrates superior results in both ST (Speech-to-Text) and MT (Machine Translation), showcasing the translation quality it offers.\nIt is worth noting that our translation model's TTS (Text-to-Speech) results are not as satisfactory as YourTTS. This disparity can be attributed to the utilization of a weaker unit vocoder in this abliation studies."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725976889,
                "cdate": 1700725976889,
                "tmdate": 1700737662097,
                "mdate": 1700737662097,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pHVHlnBKOX",
                "forum": "hCrFG9cyuC",
                "replyto": "1FPezhFW7s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up response to Reviewer JZne"
                    },
                    "comment": {
                        "value": "*g.  Explanation of mHuBERT's performance.*\n\nOur explanation is that a  larger HuBERT trained with more data is stronger in extracting semantic  information. The units extracted from this model contain more semantic information and as a result less prosody and speaker information. Thus the ASV and naturalness degraded.\n \n*h. Comparison of the amount of data and model sizes.*\n\nRegarding the model size, the parameters of U-SLM are consistent with Valle-X. And the training data for U-SLM and Valle-X are essentially consistent. Therefore, the comparison of the synthesis module is absolutely fair. \n\nIn the speech-to-speech translation task, Valle-X utilizes SpeechUT[4] as the ASR and ST model, which predicts source phoneme sequences for recognition, and target phoneme sequences for translation, respectively. SpeechUT follows an encoder-decoder architecture, whereas our U-XLM is a decoder-only structure. A direct comparison of parameters of these definitely different frameworks may not provide an intuitive understanding. \n\nRegarding the amount of training data, please refer to the following table, where WenetST represents the end-to-end speech translation training data, expanded upon WenetSpeech by the authors of VALL-E X.\n\n \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n  \nSystem     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;       |    **VALL-E X Trans (SpeechUT + VALL-E X )**   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |     **PolyVoice**\n\nASR data  &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;    |  LibriLight (60k hrs), WenetSpeech (10k hrs)   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;  | LibriLight (60k hrs), In-house (60k hrs)\n\nMT data   &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;     |  73M sentences      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                             | 44M sentences\n\nST/S2S data  &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;    |  WenetST (10k hrs), GigaST (10k hrs)      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         |  WenetS2S (10k hrs), GigaS2S (10k hrs)\n\n \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nWe have incorporated an additional in-house ASR dataset, specifically a Chinese ASR dataset, into the training of our U-XLM module. The primary objective behind using this dataset is to enhance the performance of our translation module. As a result of this integration, we have observed a notable improvement of 1-2 points in BLEU scores for the entire system. This increase offsets the performance loss incurred by employing unsupervised discretized units instead of phonemes. Although we do not release this particular ASR dataset, we believe a dataset with similar data volume can reproduce our results.\n\n&nbsp;\n\n[1] Radford A, Kim J W, Xu T, et al. Robust speech recognition via large-scale weak supervision[C]//International Conference on Machine Learning. PMLR, 2023: 28492-28518.\n\n[2] Casanova E, Weber J, Shulby C D, et al. YourTTS: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone[C]//International Conference on Machine Learning. PMLR, 2022: 2709-2720.\n\n[3] Costa-juss\u00e0 M R, Cross J, \u00c7elebi O, et al. No Language Left Behind: Scaling Human-Centered Machine Translation[J]. arXiv e-prints, 2022: arXiv: 2207.04672.\n\n[4] Zhang Z, Zhou L, Ao J, et al. Speechut: Bridging speech and text with hidden-unit for encoder-decoder based speech-text pre-training[J]. arXiv preprint arXiv:2210.03730, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726182905,
                "cdate": 1700726182905,
                "tmdate": 1700738635913,
                "mdate": 1700738635913,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p3J9mnaPuF",
            "forum": "hCrFG9cyuC",
            "replyto": "hCrFG9cyuC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7518/Reviewer_Jg4v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7518/Reviewer_Jg4v"
            ],
            "content": {
                "summary": {
                    "value": "Polyvoice is a framework for building speech to speech translation system with language modeling (or decoder only) approach as an alternative to the sequence-to-sequence (or encoder-decoder) architecture. The authors show that this is indeed possible given a combination of such LMs, i.e., translation LM, duration LM and speech synthesis LM. Each of the 3 models use unsupervised semantic and acoustic units.\n- Translation LM -  Uses source semantic units derived from HuBERT to predict target semantic units\n- Duration LM - User source and target semantic units with source duration to predict target duration units\n- Speech Synthesis LM - Uses source and target semantic units with source acoustic units to predict target acoustic units\n\nAuthors show competitive performance on EMIME (Chinese $\\rightarrow$ English) and CVSS (English $\\rightarrow$ Spanish) compared to methods proposed in VALL-E X. They also compare their work to current SoTA seq2seq approach (Lee et al.) and show zero-shot performance on dev-clean set of Librispeech.\n\nOverall, the paper's main contribution is demonstrating that a decoder-only architecture is sufficient towards building a speech-to-speech translation system with unsupervised semantic and acoustic units."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed framework is the novel in its approach towards speech-to-speech translation where it uses decoder-only models.\n- Decoder only framework simplifies the model architecture and hence makes the implementation of the translation system straightforward.\n- The proposed method is based on unsupervised semantic and acoustic units making it possible to build systems of unwritten languages.\n- Performance on the datasets shown is quite competitive and the ablation studies further highlight the importance of the 3 component models of the framework."
                },
                "weaknesses": {
                    "value": "- The duration and speech synthesis models depend on the translation model. Hence the training of two models depend on one upstream model which can make experimentation slow. At least, the duration model can be attempted to be folded in the translation model as shown in the paper Text-Free Prosody-Aware Generative Spoken Language Modeling (Kharitonov et. al.).\n- Since the authors use CVSS it would be desirable to show the performance on other language pairs from the dataset to make the evaluation more robust.\n- The paper stated that they have used \"in-house ASR datasets\". It's not clear how much contribution from these in-house datasets make the method effective. This is makes the reproducibility of the paper very difficult if these \"in-house datasets\" are not released."
                },
                "questions": {
                    "value": "- HuBERT is trained on English only corpus. Did you just apply the model to discretize Chinese speech, or had to adapt it?\n- Why was the specific language pairs that have been evaluated chosen?\n- You have not cited On Generative Spoken Language Modeling from Raw Audio (Lakhotia et. al.) and Text-Free Prosody-Aware Generative Spoken Language Modeling (Kharitonov et. al.) which showed that unsupervised discrete units can be used for speech synthesis and that duration is indeed crucial in improving the prosodic characteristics of the produced speech.\n- Spelling error: \"marked\" instead of \"maked\" in section 4.1.1"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7518/Reviewer_Jg4v"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699017556839,
            "cdate": 1699017556839,
            "tmdate": 1699636907266,
            "mdate": 1699636907266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SuSvmX8AnA",
                "forum": "hCrFG9cyuC",
                "replyto": "p3J9mnaPuF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jg4v"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your positive review and valuable feedback. We hope our response completely addresses any concerns you may have.\n\n\n**Feedback to the weakness**:\n\n- **[Slow training time]**\n\nThe translation front-end model and the speech synthesis back-end model can be trained separately. During the training, the speech of the target language will be converted into discrete units, which on the one hand serve as the training target of the translation module and on the other hand serve as the input of the speech synthesis module. Thus, these two models can be trained in parallel and do not result in slow training time.\n\n- **[Folding the duration model]**\n\nThank you for pointing this out. We may make an attempt on this in the future.\n\n- **[Performance on other language pairs]**\n\nWe didn\u2019t run experiments on other language pairs as our tokenizer didn\u2019t support other languages. We will consider extending our work into other language pairs in the future.\n\n- **[Using in-house data]**\n\nWe have incorporated an additional in-house ASR dataset, specifically a Chinese ASR dataset, into the training of our U-XLM module. The primary objective behind using this dataset is to enhance the performance of our translation module. As a result of this integration, we have observed a notable improvement of 1-2 points in BLEU scores for the entire system. This increase offsets the performance loss incurred by employing unsupervised discretized units instead of phonemes. Although we do not release this particular ASR dataset, we believe a dataset with similar data volume can reproduce our results.\n\n\n**Feedback to the questions**:\n\n- **[HuBERT]**\n\nIn this paper, we employed three variations of HuBERT. In the main and analysis experiments (Table 2-5), separate training was conducted for the Chinese and English versions of HuBERT, as described in detail in Section 4.1.1. In the analysis experiment (Table 6), we trained a bilingual HuBERT model that combines both Chinese and English data, as explained in Section 5.3.\n\n- **[Why the language pairs are chosen]**\n\nChinese->English pair is chosen as they are both high-resource languages. And it is easier to compare our results with other systems (VALL-E X) with this language pair.\n\nEnglish->Spanish pair is chosen as we want to use Spanish to simulate an unwritten language scenario where the amount of available data is relatively limited. Additionally, we directly utilized an open-source multilingual HuBERT model (\nhttps://github.com/facebookresearch/fairseq/blob/main/examples/speech_to_speech/docs/textless_s2st_real_data.md , en-es-fr), which also supports the Spanish language.\n\n- **[Citation and typos]**\n\nThank you very much for pointing these out. We will cite the mentioned paper accordingly and fix the spelling error in the revised version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725172284,
                "cdate": 1700725172284,
                "tmdate": 1700736077680,
                "mdate": 1700736077680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]