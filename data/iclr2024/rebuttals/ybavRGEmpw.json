[
    {
        "title": "Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences"
    },
    {
        "review": {
            "id": "vvjqoPBnWA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission426/Reviewer_8rju"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission426/Reviewer_8rju"
            ],
            "forum": "ybavRGEmpw",
            "replyto": "ybavRGEmpw",
            "content": {
                "summary": {
                    "value": "This paper defines a new divergence considering both the information divergence and optimal transport and then investigates adversarial robustness in terms of the defined divergence from a distributionally robust optimization perspective. Based on the analysis, the authors propose the ARMOR\\_D algorithm and then obtain SOTA performance on MNIST and Malware compared with FGSM, PGD, TRADES, and MART."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper generalizes the adversarial training framework and OT-DRO by defining a new divergence considering both the information divergence and optimal transport.\n- The paper provides a theoretical analysis of the proposed framework."
                },
                "weaknesses": {
                    "value": "- The empirical evaluation is not sufficient, which is not enough to show the effectiveness of the proposed method.\n  - The baselines are weak. The latest baseline used in the paper is proposed in 2020. However, it is 2023 now, there are so many good methods proposed these years. So I think the authors should compare their methods with the latest SOTA method.\n  - There are only **two small** datasets. Firstly, I think the authors should conduct experiments on more datasets to indicate consistent improvement. Secondly, the experiments should be conducted on larger datasets, MNIST is a very easy task, the authors should conduct experiments on CIFAR10, CIFAR100, and ImageNet.\n  - The backbone is small, which is not consistent with the word \"deep learning\" in the topic of this paper. Larger neural networks should be used (together with larger datasets).\n  - Stronger attack methods should be used. The paper only reports performance under FGSM and PGD attacks, which are not strong enough. Stronger attacks such as CW and AutoAttack should be used.\n- There are some other works that investigate adversarial robustness in terms of distributionally robust optimization [1-2], the discussions of related works are mission. Furthermore, given the existence of these papers, I think this paper is not novel enough.\n- The theoretical analyses only show that the proposed framework is more general, but do not show why the more general framework behaves better. In general, a more specific framework may contain additional knowledge about the task and lead to better performance, so the fact that a more general framework leads to better performance is strange to me. I think it needs to be carefully explained.\n\n\n### references\n\n[1] Certifying Some Distributional Robustness with Principled Adversarial Training.\n\n[2] A Distributional Robustness Perspective on Adversarial Training with the $\\infty$-Wasserstein Distance."
                },
                "questions": {
                    "value": "- See the weaknesses.\n- Why adding natural examples to the training can improve adversarial robustness? Does this still hold for larger datasets such as the CIFAR and the ImageNet?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission426/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission426/Reviewer_8rju"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697383202881,
            "cdate": 1697383202881,
            "tmdate": 1699635969134,
            "mdate": 1699635969134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8GzsQFJwDF",
                "forum": "ybavRGEmpw",
                "replyto": "vvjqoPBnWA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\"The baselines are weak. The latest baseline used in the paper is proposed in 2020. However, it is 2023 now, there are so many good methods proposed these years. So I think the authors should compare their methods with the latest SOTA method. \"\n            \n- In the revised manuscript, we reviewed, cited, and compared our method with two additional new methods, one of which was pointed by you: Regniez et al. 2022 (A Distributional Robustness Perspective on Adversarial Training with the $\\infty$-Wasserstein Distance). We also found a more recent work by Bui et al., 2022 (A Unified Wasserstein Distributional Robustness Framework for Adversarial Training) published in ICLR22 that we used to compare our performance on the much stronger Auto-attack and PGD-200. In addition, we modified  the MNIST example to use the standard PGD-AT inner maximizer. This focuses the preliminary MNIST example on    highlighting the unique feature of our method: the  principled addition of adversarial sample re-weighting (based on an f-divergence cost) on top of adversarially modified samples. We find it significantly improves the performance under AutoAttack compared to the baseline as well as to the recent method Bui et al. 2022, which is another method that can be used to augment any empirical risk minimization problem.  After highlighting the effect of re-weighting alone, we move on to the higher-dimensional and more challenging malware example where we also experiment with modifying the OT cost on top of the adversarial re-weighting.   \n\n     \n\"There are only two small datasets. Firstly, I think the authors should conduct experiments on more datasets to indicate consistent improvement. Secondly, the experiments should be conducted on larger datasets, MNIST is a very easy task, the authors should conduct experiments on CIFAR10, CIFAR100, and ImageNet.\"\n            \n- For the discrete case, our dataset is high-dimensional (22,761 features), which is higher than CIFAR10 (3,072). To converge the new experiments on time, we dedicated our limited  resources to benchmarking against Auto-attack.\n\n\"The backbone is small, which is not consistent with the word \"deep learning\" in the topic of this paper. Larger neural networks should be used (together with larger datasets).\"\n            \n- To not emphasize on the word ``deep,\" we removed the word from the title as well as the name of the proposed method.\n\n\"Stronger attack methods should be used. The paper only reports performance under FGSM and PGD attacks, which are not strong enough. Stronger attacks such as CW and AutoAttack should be used.\"\n           \n- We added evaluations on Auto-attack as a comprehensive and strong attack that is an ensemble of four newer attacks based on your comment.\n\n\"There are some other works that investigate adversarial robustness in terms of distributionally robust optimization [1-2], the discussions of related works are mission. Furthermore, given the existence of these papers, I think this paper is not novel enough.      The theoretical analyses only show that the proposed framework is more general, but do not show why the more general framework behaves better. In general, a more specific framework may contain additional knowledge about the task and lead to better performance, so the fact that a more general framework leads to better performance is strange to me. I think it needs to be carefully explained.\"\n\n- We added Section 2.1, which explains the fundamental difference between our method and prior distributionally robust optimization based methods.  Specifically, we show that in addition to adversarially transporting samples (as in prior methods) our approach dynamically re-weights samples via f-divergence to focus the optimization towards improving performance on the more troublesome samples; see Eq. 16-17.  Conceptually, this is the key  innovation of our method, and is not present in prior DRO-based approaches.  \n\n\"Why adding natural examples to the training can improve adversarial robustness? Does this still hold for larger datasets such as the CIFAR and the ImageNet?\"\n\n- The sample re-weighting component of our method focuses the optimizer towards the most troublesome adversarial samples.  When using alpha-divergences, some adversarial samples in each minibatch can even be completely ignored if their loss is small enough.  However ignoring natural samples can be a problem, especially at the beginning of training, before the model has learned anything.  We find that combining the natural loss, which incorporates all samples in each minibatch, with the adversarial loss, which focusing on the most difficult samples,  generally provides the best performance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677298600,
                "cdate": 1700677298600,
                "tmdate": 1700677298600,
                "mdate": 1700677298600,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dPV2byY2Jd",
            "forum": "ybavRGEmpw",
            "replyto": "ybavRGEmpw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission426/Reviewer_sgR7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission426/Reviewer_sgR7"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a novel method for adversarially robust training of deep learning models. Essentially, authors propose to use the extension of the common Adversarial Training where the internal maximization process should be conducted on top of both 1) divergence term between the \"close\" adversarial distribution and the original empirical one, and 2) optimal transport between these two divergences."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "First of all, a lot of seeming correctly theoretical background and proofs are provided (especially in the Appendix A), which make the work standing out of a common adversarial robustness methods.\nAdditionally, authors provided a rigorous experimentation analysis of different variants of their method for two modalities (images and malware detection) and prove the superiority of their approach.\nFinally, which is very nice, their method improves even no-attack setting, which is very important when we don't have adversaries - the system still should work reliable."
                },
                "weaknesses": {
                    "value": "Although the paper is very well written with a lot of theoretical details, there are still some (I hope minor) weaknesses:\n- the work used a super small and toy dataset for images - MNIST - the scale and reliability on results obtained working with it are unlikely used anywhere in reality. Moreover, the conclusion obtained can be misleading. The golden standard is to use ImageNet as a must for the image datasets (probably the same is for the Malware detection dataset, I have just not worked with it)\n- when comparing the results shown in Tables 1 and 2, we can see that the best OT-regularized divergences methods in terms of accuracy are very different for image recognition and malware detection problem. What is the root cause behind it? No any written hypothesis or discussion. I guess (and see the item above) it is somehow related to MNIST dataset and that the results there are like 98-99% which makes all the improvement marginal and not very generalizable \n- starting the Page 2, the cost function $c(x,y)$ is introduced, but all the theorems and results are only valid if $c(x,y)$ is non-negative, but it is not mentioned in the main text of the paper (only in Appendix)"
                },
                "questions": {
                    "value": "Here are a couple of questions\n- what was the original motivation to combine OT and DRO? E.g., some clear bad cases for OT are not in the common DRO and vice versa etc.\n- what was the reasoning behind choosing a probability-related term for the cost function as a $g_{\\delta}(z) = z/(1-z/\\delta)$? To mimic sigmoid?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission426/Reviewer_sgR7",
                        "ICLR.cc/2024/Conference/Submission426/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698534795729,
            "cdate": 1698534795729,
            "tmdate": 1700709952034,
            "mdate": 1700709952034,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kR57mBFrdt",
                "forum": "ybavRGEmpw",
                "replyto": "dPV2byY2Jd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\"The work used a super small and toy dataset for images - MNIST - the scale and reliability on results obtained working with it are unlikely used anywhere in reality. Moreover, the conclusion obtained can be misleading. The golden standard is to use ImageNet as a must for the image datasets (probably the same is for the Malware detection dataset, I have just not worked with it)\"\n\n- We agree that for the continuous case, testing on higher-dimensional datasets can be beneficial. To converge the new experiments on time, we dedicated our limited  resources to benchmarking against Auto-attack. We also note that our malware dataset, also used to demonstrate the effectiveness of the model in discrete cases, has 22,761-dimensional feature vectors, which is higher than CIFAR10 (3,072 features) and on par with ImageNet (23,296 features).\n\n\"when comparing the results shown in Tables 1 and 2, we can see that the best OT-regularized divergences methods in terms of accuracy are very different for image recognition and malware detection problem. What is the root cause behind it? No any written hypothesis or discussion. I guess (and see the item above) it is somehow related to MNIST dataset and that the results there are like 98-99\\% which makes all the improvement marginal and not very generalizable\"\n\n- In our new experiments, we are comparing against much stronger attacks with increased epsilon ball, including many more PGD steps (PGD-200) and the  much stronger  AutoAttack. Accordingly, the new results are not in the 98-99 neighborhood any more.  In addition to including much stronger attacks,  we also modified the MNIST example to use the standard PGD-AT inner maximizer. This focuses the preliminary MNIST example on    highlighting the unique feature of our method: the  principled addition of adversarial sample re-weighting (based on an f-divergence cost) on top of adversarially modified samples. We find it significantly improves the performance under AutoAttack compared to the baseline as well as to the recent method Bui et al. 2022 (published in ICLR22) which is another method that can be used to augment any empirical risk minimization problem.  After highlighting the effect of re-weighting alone, we move on to the higher-dimensional and more challenging malware example where we also experiment with modifying the OT cost on top of the adversarial re-weighting.   \n\n\"starting the Page 2, the cost function is introduced, but all the theorems and results are only valid if is non-negative, but it is not mentioned in the main text of the paper (only in Appendix)\"\n\n- We added a line stating the assumptions on $c$ after equation 4 in the text.\n\n\"what was the original motivation to combine OT and DRO? E.g., some clear bad cases for OT are not in the common DRO and vice versa etc.\"\n\n- OT allows samples to be perturbed (transported) into adversarial samples.  The $f$-divergence component of the DRO framework allows samples to be re-weighted. The motivation was that the re-weighting will shift weight towards the more troublesome samples, thereby making the optimizer focus on the more troublesome samples.  We included a more detailed discussion of this intuition in the new Section 2.1.\n\n\"what was the reasoning behind choosing a probability-related term for the cost function as a $g_\\delta(z)=z/(1-z/\\delta)$? To mimic sigmoid?\"\n\n- In essence, we wanted a function that is smooth, increasing, has a vertical asymptote, and is easy to compute (similar to the behavior of the inverse of sigmoid). The output of the classifier network can be viewed as a probability vector, so it seemed plausible that giving the adversarial sample/label pair this same structure could improve the regularizing effects of a DRO-based method. The structure of the OT cost was chosen to include a vertical asymptote in $g_\\delta$, to ensure that the optimization results in an adverarial probability vector that still predicted the original class label; the difference is only that the certain label is relaxed to a (less certain) probability vector.  We added some clarification of this point to the paragraph above eq. 25."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676876527,
                "cdate": 1700676876527,
                "tmdate": 1700676876527,
                "mdate": 1700676876527,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "usMrveHC7i",
                "forum": "ybavRGEmpw",
                "replyto": "kR57mBFrdt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission426/Reviewer_sgR7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission426/Reviewer_sgR7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for answering and addressing my comments. Increasing the grade."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709904505,
                "cdate": 1700709904505,
                "tmdate": 1700709904505,
                "mdate": 1700709904505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SDLZVUw0aq",
            "forum": "ybavRGEmpw",
            "replyto": "ybavRGEmpw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission426/Reviewer_4TT9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission426/Reviewer_4TT9"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the novel approaches to enhancing the adversarial robustness of deep learning models. This paper enhances adversarial robustness by maximizing the expected loss over a neighborhood of distributions for distributionally robust optimization. For constructing adversarial samples, the proposed method allows samples to be both transported and re-weighted, according to the OT cost and the information divergence. The authors demonstrate the effectiveness of the proposed method on malware detection and image recognition applications and find that it outperforms existing methods at enhancing the robustness against adversarial attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work proposes a novel class of divergences for comparing probability distributions called optimal-transport-regularized divergences and uses them to construct distribution neighborhoods for use in distributionally robust optimization.\n2. This paper proves a number of new properties of the OT-regularized divergences and demonstrates the effectiveness of the OT-regularized divergence method as a novel approach to enhancing adversarial robustness in deep learning leading to substantial performance gains."
                },
                "weaknesses": {
                    "value": "1. For the classification problem on the continuous data, this paper tests the ARMORD adversarial robustness methods on MNIST digit classification, where the dimension of image in the MNIST is 28*28. Note that the computation of Optimal Transport (OT) is closely related to the dimension of data, thus it would be interesting to see the performance of the proposed approach and its counterparts on the image data with larger dimension such as CIFAR-10 with 3*32*32 or the tiny-ImageNet with 3*64*64. \n2. In the comparison of the performance for enhancing the robustness on the MNIST dataset, only the gradient-based attacks are used to evaluate the adversarial robustness. It would be more sufficient for the adversarial robustness of proposed Optimal-Transport-Regularized Divergences if there are evaluations of the optimization-based attack such as CW and the stronger comprehensive attack AutoAttack. \n3. Note that the proposed OT-regularized divergences may need more computational cost than the conventional divergence, thus it would be more meaningful for the practical use to provide the comparison of the computational costs between the proposed OT-regularized divergences based adversarial training and the counterparts."
                },
                "questions": {
                    "value": "This paper proposes the optimal-transport-regularized divergences to enhance the adversarial robustness of deep learning models. Yet, it still needs more details and evidences to further explain the effectiveness of the proposed approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission426/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission426/Reviewer_4TT9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745461718,
            "cdate": 1698745461718,
            "tmdate": 1699635968940,
            "mdate": 1699635968940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HBkNDlAOlN",
                "forum": "ybavRGEmpw",
                "replyto": "SDLZVUw0aq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\"For the classification problem on the continuous data, this paper tests the ARMORD adversarial robustness methods on MNIST digit classification, where the dimension of image in the MNIST is 2828. Note that the computation of Optimal Transport (OT) is closely related to the dimension of data, thus it would be interesting to see the performance of the proposed approach and its counterparts on the image data with larger dimension such as CIFAR-10 with 33232 or the tiny-ImageNet with 364*64.\"\n\n- We agree that similar to our high-dimensional discrete dataset, the continuous case can also benefit from testing on higher-dimensional datasets. To converge the new experiments on time, we dedicated our limited  resources to benchmarking against Auto-attack. Regarding the computational cost of OT, it is only the OT cost function that occurs in the inner maximizer, which is relatively easy to compute. Our method doesn't require the computation of the optimal coupling, which would be much more difficult.  This is clarified in our derivation, Eq.6-9.  We also want to point out that our method can still use simple OT cost function (Eq. 21) that leads to the same PGD-AT inner maximizer as Madry et al. 2018, while retaining the novel adversarial re-weighting  provided by the f-divergence component of our method;  proposing a unified framework to incorporate these two mechanisms in a consistent and principled manner is the primary innovation of our method which, to our knowledge, has not been studied in the adversarial robustness literature.  The unique features are further discussed in the new Section 2.1. To complement this discussion, we modified the MNIST example in Section 3.1 to use the standard PGD-AT inner maximizer, thus focusing this preliminary example on highlighting the unique feature of our method: the combination of adversarial sample re-weighting (based on an f-divergence cost).  This is an important test, before we move on to the higher-dimensional malware example where we also experiment with modifying the OT cost on top of the adversarial re-weighting.   \n     \n\"In the comparison of the performance for enhancing the robustness on the MNIST dataset, only the gradient-based attacks are used to evaluate the adversarial robustness. It would be more sufficient for the adversarial robustness of proposed Optimal-Transport-Regularized Divergences if there are evaluations of the optimization-based attack such as CW and the stronger comprehensive attack AutoAttack.\"\n\n- Thanks for directing us to Auto-attack as a comprehensive attack incorporating a set of four strong attacks. We conducted a new set of experiments to evaluate our performance against auto-attack and compare with recent adversarial robustness methods. see Table 1.\n    \n\"Note that the proposed OT-regularized divergences may need more computational cost than the conventional divergence, thus it would be more meaningful for the practical use to provide the comparison of the computational costs between the proposed OT-regularized divergences based adversarial training and the counterparts.\"\n\n- The additional computation cost is relatively minimal. In the inner maximizer there is an additional term involving the OT cost function, though one can even choose that be a simple hard constraint as in Eq. 21 which requires no additional derivatives to be computed in the inner maximizer.  In the outer minimizer, the f-divergence component of the method is seen in the function $f^*$, which is tantamount to  adding one additional activation function layer (1D).  And then there are the two additional real parameters $\\lambda,\\rho$ that are appended to the NN parameters. Overall these are minimal additions, considering the number and size of the layers. We also note that our method does not require the computation of the optimal coupling (a much more costly task), rather it simply uses the optimal transport cost function in the inner maximizer.\n\n\"This paper proposes the optimal-transport-regularized divergences to enhance the adversarial robustness of deep learning models. Yet, it still needs more details and evidences to further explain the effectiveness of the proposed approach.\"\n\n- We added Section 2.1, which explains the fundamental difference between our method and prior distributionally robust optimization based methods.  Specifically, we show that in addition to adversarially transporting samples (as in prior methods) our approach dynamically re-weights samples via f-divergence to focus the optimization towards improving performance on the more troublesome samples; see Eq. 16-17.  Conceptually, this is the key  innovation of our method, and is not present in prior DRO-based approaches."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676673863,
                "cdate": 1700676673863,
                "tmdate": 1700677371762,
                "mdate": 1700677371762,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n6ZGHPTrle",
            "forum": "ybavRGEmpw",
            "replyto": "ybavRGEmpw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission426/Reviewer_2esQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission426/Reviewer_2esQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed adversarial training with optimal-transport-regularized divergence. It is a framework solving distributional robustness optimization problem where the distributional divergence is regularized by optimal transport. The authors comprehensively study the properties of this problem and demonstrate the effectiveness of the derived algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a framework that is generally applicable and mathematically elegant. The properties of the OT-regularized divergence are nice and the derived algorithm is nice. The paper is well-written and easy to read."
                },
                "weaknesses": {
                    "value": "The theoretical part is nice, my major concern is the empirical part.\n\n1. The datasets studied in the empirical study are very small and are generally considered as toy examples. Experiments on a larger dataset will make the empirical results more convincing.\n\n2. The robustness evaluation is more comprehensive. For example, PGD and FGSM cannot comprehensively and reliably evaluate the robustness of a model as indicated in [A]. The authors should use auto-attack, which is an ensemble of four different attacks, to comprehensively evaluate the robustness and to make the results more convincing.\n\n[A] \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks\". Francesco Croce, Matthias Hein.\nICML 2020."
                },
                "questions": {
                    "value": "The paper is generally well-written and interesting. My questions are the two points in the weakness part. The manuscript will be better with the empirical issues addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699274120703,
            "cdate": 1699274120703,
            "tmdate": 1699635968851,
            "mdate": 1699635968851,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cJuncJ1nlR",
                "forum": "ybavRGEmpw",
                "replyto": "n6ZGHPTrle",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\"The datasets studied in the empirical study are very small and are generally considered as toy examples. Experiments on a larger dataset will make the empirical results more convincing.\"\n\n- We agree that for the continuous case, testing on higher-dimensional datasets can be beneficial. To converge the new experiments on time, we dedicated our limited  resources to benchmarking against Auto-attack. We also note that our malware dataset, also used to demonstrate the effectiveness of the model in discrete cases, has 22,761-dimensional feature vectors which is higher than CIFAR10 (3,072 features) and on par with ImageNet (23,296 features).\n    \n\"The robustness evaluation is more comprehensive. For example, PGD and FGSM cannot comprehensively and reliably evaluate the robustness of a model as indicated in [A]. The authors should use auto-attack, which is an ensemble of four different attacks, to comprehensively evaluate the robustness and to make the results more convincing.\"\n\n- Thanks for your suggestion regarding adding Autoattack. We added Auto-attack to our experiments, along with changing to the more difficult PGD200 (instead of PGD40). We have also changed the variant of our method used there to more directly highlight the contribution of our method's main innovation (i.e., incorporating adversarial sample re-weighting via f-divergence).  Specifically, we changed the MNIST experiment to use the same PGD-AT inner maximizer as Madry et al., 2018, thereby focusing the example on highlighting the unique feature of our method: the combination of adversarial sample re-weighting (based on an f-divergence cost) with the more commonly used adversarially transported samples. By using the  PGD-AT when constructing adversarial samples, this example now  isolates the contribution of our novel adversarial re-weighting to the performance, providing an important preliminary test example, before we move on to the higher-dimensional malware example, where we also experiment with modifying the OT cost on top of the adversarial re-weighting.     Modifying the MNIST example in this way also serves to complement the discussion in the newly added Section 2.1 (portions of which used to be in an appendix), where we show precisely how our method dynamically re-weights samples to focus the optimizer towards the more troublesome adversarial examples; see especially Eq. 16-17."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676349699,
                "cdate": 1700676349699,
                "tmdate": 1700676450035,
                "mdate": 1700676450035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]