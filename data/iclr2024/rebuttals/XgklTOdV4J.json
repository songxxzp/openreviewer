[
    {
        "title": "DualAug: Exploiting Additional Heavy Augmentation with OOD Data Rejection"
    },
    {
        "review": {
            "id": "K9WX0voe8t",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3460/Reviewer_c5kH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3460/Reviewer_c5kH"
            ],
            "forum": "XgklTOdV4J",
            "replyto": "XgklTOdV4J",
            "content": {
                "summary": {
                    "value": "This study reveals a noteworthy observation: a substantial increase in the application of data augmentation transformations for classification tasks leads to a precipitous decline in performance as shown in Figure 1. Building upon this revelation, the authors introduce an Out-of-Distribution (OOD) discarding techniques that not only safeguards performance from the unusual samples but also enhances it. Notably, this novel method harnesses two distinct branches of augmentation, namely the basic augmentation branch and the heavy augmentation branch. The acceptability of a sample is gauged based on the softmax values obtained from each branch's outcome. The authors demonstrate that this innovative approach can be seamlessly integrated into existing AutoAugmentation methodologies, resulting in performance enhancement. Furthermore, they substantiate the efficacy and superiority of their proposed technique through self-supervised performance evaluation which strongly utilize augmentation in their training procedure.\n\nI will subsequently provide a summary of strengths and concerns from my perspective, along with any questions I have."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n1. This paper presents a compelling insight into the AutoAugmenttion methods. It highlights a critical aspect - the profound influence of the number of augmentations on performance. Previous AutoAugmentation approaches have predominantly relied on manually selecting the number of transformations, underscoring the importance of examining this factor.\n\n2. Building on this discovery, the proposed method stands out for its elegant simplicity and remarkable effectiveness. It employs a straightforward scoring mechanism, computed through a simple feed-forward process on the samples. In contrast, previous AutoAugmentation methods have incurred substantial computational costs in the quest for optimal augmentations. This novel approach streamline the process, utilising just one additional branch in the feed-forward stage while achieving impressive results.\n\n3. The paper demonstrates consistent performance improvements across various scenarios. The authors showcase enhanced performance on divers datasets. Despite the extensive history of AutoAugmentation techniques, these improvements are noteworthy, as they consistently enhance results across different use cases."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n1. While this paper demonstrates performance improvements, its applicability is primarily limited to classification tasks, including contrastive learning. As previously mentioned in the strengths section, given the extensive history of AutoAugmentation research, the room for improvement in these specific domains appears limited. Therefore, it is essential to explore the impact of this research on other datasets. For instance, leveraging additional datasets like those used in the AutoAugmentation [1] paper (Flowers, Caltech, Pets, Aircraft, Cars) could provide further insights. Moreover, the proposed method, relying on softmax scores for out-of-distribution sample detection, may require refinement when applied to other tasks, such as object detection (as DADA [2] did).\n\n\n2. Despite the paper's assertion that the proposed algorithm incurs minimal computational costs by utilizing an additional branch for feed forwarding to obtain heavy and basic augmentation datasets, the authors do not provide an analysis of the algorithm's additional computational expenditure. For instance, it would be valuable to compare this algorithm's cost-effectiveness with that of existing methods like RA, which do not incur extra costs. Such an analysis could strengthen the argument for the proposed algorithm's efficacy.\n\n\n3. It appears that the model in this study was trained on datasets twice as large as those used in previous algorithms, given that it utilizes data from two branches. This discrepancy could introduce an element of unfairness when comparing the proposed algorithm with its predecessors. To ensure a fair comparison, one potential approach could involve restricting the number of samples the model encounters during the training process.\n\n4. Minor Weakness:\n(Lack of reference to related work) The paper does not reference two relevant works: DADA [2], which focuses on Data Augmentation using Differentiability, and CUDA [3], which provides an analysis of the number of augmentation operations in both class imbalance and balanced tasks.\n\n[1] AutoAugment: Learning Augmentation Strategies from Data, CVPR 2019  \n[2] DADA: Differentiable Automatic Data Augmentation, ECCV 2020  \n[3] CUDA: Curriculum of Data Augmentation for Long-tailed Recognition, ICLR 2023"
                },
                "questions": {
                    "value": "Here are some brief questions regarding this paper:\n(1) Is it possible to explore alternative metrics for the OOD score, such as the Mahalanobis distance [4]? While the Softmax-based approach is effective, there may be even better scoring methods worth considering.  \n\n\n(2) Could the authors extend their analysis to encompass other tasks, such as object detection?\n\n\n(3) Is there potential for the proposed method to be integrated with mixed sample data augmentation techniques like MixUp [5], CutMix [6], or similar approaches?\n\n[4] A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, NeurIPS 2018  \n[5] mixup: Beyond Empirical Risk Minimization, ICLR 2017  \n[6] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, ICCV 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no ethics concerns of this paper.\n\n\n-------------------------\nAfter reviewing the authors' response, I have chosen to increase my rating from 5 to 6. I appreciate the thoroughness of your responses."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3460/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3460/Reviewer_c5kH",
                        "ICLR.cc/2024/Conference/Submission3460/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3460/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697134140687,
            "cdate": 1697134140687,
            "tmdate": 1700700777218,
            "mdate": 1700700777218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eK7bECuRVg",
                "forum": "XgklTOdV4J",
                "replyto": "K9WX0voe8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c5kH (1/7)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and insightful reviews. The following is our response.\n\n**Weakness1: More dataset results of DualAug **\n\nWe present more results on Flowers, Caltech, Pets, Aircraft, Cars as below tables. The experiment uses the inception-v4 model. DualAug demonstrates a significant improvement over AutoAugment in almost all FGVC datasets except for Pets.\n| Method | Flowers  | Caltech | Pets | Aircraft | Cars | Avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| AA | 87.55% | 81.76% | **84.52%** | 85.63% | 93.51% | 86.59% |\n| AA+DualAug | **88.92%** | **84.85%** | 83.70% | **86.74%** | **94.01%** | **87.64%** |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670563596,
                "cdate": 1700670563596,
                "tmdate": 1700675495177,
                "mdate": 1700675495177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RTSW4oBV9y",
                "forum": "XgklTOdV4J",
                "replyto": "K9WX0voe8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c5kH (2/7)"
                    },
                    "comment": {
                        "value": "**Weakness 1 & Question 2: Other tasks of DualAug**\n\nDualAug can also be extended to other tasks, such as object detection. We can analyze its effectiveness in two phases: the pre-training phase and the finetuning phase.\nIn the pre-training phase, we have observed that DADA has been used to improve the ImageNet pre-training backbone for object detection. Our pre-training backbone can also be leveraged for object detection and other related tasks. When we expand the results of Table 5 to the object detection task, we find that DualAug leads to improvements in performance.\n\n| Method | Classification(Acc. %) | VOC 07+12 Detection(AP) |\n| --- | --- | --- |\n| Simsiam | 68.20% | 51.67 |\n| Simsiam+DualAug | **68.67%** | **52.02** |\n\nIn the finetuning phase, when DualAug is applied to a specific task, its softmax score can also be utilized for OOD detection. Following [1], we conduct an experiment to demonstrate the performance of DualAug in object detection.\n\n| Method | VOC 07 Detection(AP) |\n| :---: | :---: |\n| AA[1] | 56.30 |\n| AA[1]+DualAug | **56.73** |\n\nExpanding DualAug to encompass a wider range of tasks is an exciting aspect of future work. We believe it will lead to further exploration and advancement in the field.\n\n[1] Learning Data Augmentation Strategies for Object Detection ECCV 2020"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675609359,
                "cdate": 1700675609359,
                "tmdate": 1700675609359,
                "mdate": 1700675609359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RS3gNUcLEs",
                "forum": "XgklTOdV4J",
                "replyto": "K9WX0voe8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c5kH (3/7)"
                    },
                    "comment": {
                        "value": "**Weakness 2: Additional computational expenditure.**\n\nWe conduct experiments to ensure that the training costs are aligned. We increase RA's training time to ensure a fair comparison, calling it RA*. The results in the table below clearly demonstrate that RA+DualAug outperforms the RA* and RA. The experiment is conducted using WRN-28-2 on CIFAR-100. \n\nIt is important to note that the policy search cost for RA, AA, and other automated augmentation methods is quite expensive, despite not incurring extra costs in training (as reported in the table below and DeepAA's Table 4). Therefore, the increase in computational cost is slight and reasonable when compared to the significant performance improvement that DualAug provides.\n\n| Method | Accuracy(%) | Search Time(h) | Train Time(h) |\n| --- | :---: | :---: | :---: |\n| RA | 77.94\u00b1.15 | 25 | 1 |\n| RA* | 78.11\u00b1.13 | 25 | 1.5 |\n| RA+DualAug | **78.46\u00b1.12** | 0 | 1.5 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675690068,
                "cdate": 1700675690068,
                "tmdate": 1700675893829,
                "mdate": 1700675893829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yfrJlvYPeI",
                "forum": "XgklTOdV4J",
                "replyto": "K9WX0voe8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c5kH (4/7)"
                    },
                    "comment": {
                        "value": "**Weakness 3: Unfair comparison with previous methods about dataset size.**\n\nIt appears there may be a misunderstanding. We clarify that DualAug is using the **same large** dataset as the previous method. As shown in Figure 3 and Equation (8) of the main paper, the mix of two branch is the same size as the basic augment branch, ensuring a fair comparison."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675729891,
                "cdate": 1700675729891,
                "tmdate": 1700680407933,
                "mdate": 1700680407933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Nv8L9UanI",
                "forum": "XgklTOdV4J",
                "replyto": "K9WX0voe8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c5kH (5/7)"
                    },
                    "comment": {
                        "value": "**Weakness 4: Lack of reference to related work**\n\nWe apologize for the negligence. We have now added the DADA and CUDA to references in the revision."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675760453,
                "cdate": 1700675760453,
                "tmdate": 1700675760453,
                "mdate": 1700675760453,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XwDrgbYi8a",
                "forum": "XgklTOdV4J",
                "replyto": "K9WX0voe8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c5kH (6/7)"
                    },
                    "comment": {
                        "value": "**Question 1: Explore alternative metrics for the OOD score.**\n\nFrom a theoretical perspective, it appears that refining the metrics for the OOD score could improve the performance of DualAug. However, when integrating it into DualAug, the trade-off between computational expenses of OOD score calculation and the gain of performance must be carefully weighed within the context of the specific task.  \n\nConsidering Mahalanobis distance in [1] is quite complex. We attempt to use energy-base model (EBS)[2] as an alternative OOD Score. EBM may have a slight edge over softmax. The experiment is conducted using WRN-40-2 on CIFAR-100. \n\n| Method | Accuracy(%) |\n| --- | :---: |\n| AA+DualAug(SoftMax) | 79.83\u00b1.19 |\n| AA+DualAug(EBM) | 79.95\u00b1.21 |\n\n[1] A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, NeurIPS 2018\n\n[2] Energy-based Out-of-distribution Detection, NeurIPS 2020"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675845871,
                "cdate": 1700675845871,
                "tmdate": 1700675845871,
                "mdate": 1700675845871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YNDOuPSmbP",
                "forum": "XgklTOdV4J",
                "replyto": "K9WX0voe8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c5kH (7/7)"
                    },
                    "comment": {
                        "value": "**Question 3: Integrate DualAug with mixed sample data augmentation techniques**\n\nWe use Mixup as an example to analyze the integration of DualAug with mixed-base augmentation. The experiment is conducted using ResNet-18 on CIFAR-10. Upon observation, it appears that mixup tends to select $\\alpha<=1$ in $Beta(\u03b1, \u03b1)$  which may not provide an enough amplitude of augmentation. To address this, we can simply set $\\alpha>1$ and utilize DualAug to filter OOD data in mixup data. The table below demonstrates that we achieve some improvement compared to standard mixup. \n\n| Method | Accuracy(%) |\n| --- | :---: |\n| Mixup($\\alpha=1.0$) | 95.64\u00b1.11 |\n| Mixup($\\alpha=10.0$) | 94.67\u00b1.14 |\n| Mixup($\\alpha=10.0$)+DualAug | **95.78\u00b1.09** |\n\nWe would like to extend our sincere appreciation once again for your valuable reviews, which have greatly improved the quality of our manuscript. We look forward to your continued discussions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675883049,
                "cdate": 1700675883049,
                "tmdate": 1700676528775,
                "mdate": 1700676528775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SMBrMsfuD0",
                "forum": "XgklTOdV4J",
                "replyto": "K9WX0voe8t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your positive feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer c5kH,\n\nGreatly appreciate your feedback confirming that our response met your concerns. Have a wonderful day ahead!\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739311965,
                "cdate": 1700739311965,
                "tmdate": 1700739311965,
                "mdate": 1700739311965,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hNIaEf2jXF",
            "forum": "XgklTOdV4J",
            "replyto": "XgklTOdV4J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3460/Reviewer_hEeP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3460/Reviewer_hEeP"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach to data augmentation in deep learning, aimed at improving model performance while addressing the issue of overfitting. The authors emphasize the trade-off between data diversity and the potential degradation of data quality that can occur with heavy data augmentation. They introduce a two-branch data augmentation framework called DualAug, which aims to keep augmented data within the desired distribution. The framework consists of a basic data augmentation branch and a heavy augmentation branch, with a mechanism for detecting and filtering out-of-distribution (OOD) data.\nThe contributions of the paper are clearly outlined, including the identification of informative augmented data even with heavy augmentation, the importance of filtering OOD data, and the introduction of DualAug as a practical solution. The experimental results on image classification benchmarks demonstrate the effectiveness of DualAug in improving various data augmentation methods.\nThe related work section provides a comprehensive overview of automated data augmentation and out-of-distribution detection, highlighting the unique aspects of DualAug in comparison to existing approaches. The paper also extends its investigation to semi-supervised learning and contrastive self-supervised learning, showing that DualAug can enhance the performance of these methods. The experimental results are presented clearly and support the paper's claims.\nIn conclusion, this paper introduces a decent contribution to the field of data augmentation, offering a well-motivated solution to the challenges of heavy augmentation and OOD data. The paper is well-written and presents its findings effectively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has the following strengths:\n* DualAug Framework: One of the primary strengths of this work is the introduction of the DualAug framework. Unlike many previous data augmentation methods that often strike a balance between data diversity and the preservation of semantic context, DualAug explicitly focuses on the problem of out-of-distribution (OOD) data caused by heavy augmentation. This framework features two branches: a basic data augmentation branch and a heavy augmentation branch, each tailored to their specific needs. This innovative approach is a fresh take on addressing the challenges associated with heavy data augmentation.\n* OOD Detection Integration: The incorporation of out-of-distribution (OOD) detection within the data augmentation process is a new contribution. While previous works in data augmentation have primarily focused on generating diverse training data, this paper recognizes the importance of detecting and mitigating OOD data, which can adversely impact model performance. This integration sets the work apart from previous methods that often overlook OOD data issues.\n* Integration with Existing Methods: The paper not only introduces a new approach but also demonstrates how DualAug can be integrated with existing data augmentation methods. This approach allows researchers and practitioners to benefit from the proposed framework without needing to reinvent their entire data augmentation pipelines.\n* Avoidance of Additional Models: Unlike some existing methods that rely on additional models or complex optimization strategies, DualAug focuses on the simplicity of implementation. It effectively addresses unexpected augmented data without introducing extra complexity. This simplicity is an attractive feature for practitioners who seek efficient and straightforward solutions."
                },
                "weaknesses": {
                    "value": "There are some potential weaknesses in the work:\n* Threshold Selection for OOD Detection: The paper utilizes the 3\u03c3 rule of thumb to set the threshold for filtering out OOD data. While this is a straightforward approach, it may not be the most optimal one in practice. The choice of threshold values in OOD detection can be crucial, and it's unclear whether the 3\u03c3 rule is universally suitable for different datasets and models. A more robust method for threshold selection would enhance the reliability of the approach.\n* Marginal improvement over existing Methods: One aspect of the paper that requires attention is the relatively marginal improvement in performance demonstrated by the DualAug framework, especially when compared to previous works in data augmentation. While the paper presents itself as an effective approach to addressing the challenges of data augmentation, the magnitude of the performance gains achieved is not particularly substantial."
                },
                "questions": {
                    "value": "The idea is simple and clear which I like. However, the effectiveness is under question because the gains are very marginal. Hence making me question the value of this work to the research community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3460/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3460/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3460/Reviewer_hEeP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3460/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737756968,
            "cdate": 1698737756968,
            "tmdate": 1700701898286,
            "mdate": 1700701898286,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AFiR7t1aSY",
                "forum": "XgklTOdV4J",
                "replyto": "hNIaEf2jXF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hEep (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and insightful reviews. The following is our response.\n\n**Weakness 1: Threshold Selection for OOD Detection**\n\nIn almost all tasks, datasets, and models, we have observed that the 3-sigma rule **consistently yields improvements**, and we do not deliberately adjust  \u03bb. \n\nIn addition, it is a common practice to set a threshold for identifying ID/OOD samples manually in the field of OOD detection, as noted in [1]. The 3-sigma rule offers an adaptive method to dynamically adjust this threshold.\n\n[1] A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks ICLR 2017"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670055797,
                "cdate": 1700670055797,
                "tmdate": 1700680295402,
                "mdate": 1700680295402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RmwJe0jeyE",
                "forum": "XgklTOdV4J",
                "replyto": "hNIaEf2jXF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hEep (2/2)"
                    },
                    "comment": {
                        "value": "** Weakness 2 & Question: Marginal improvement over existing Methods **\n\nThe table below illustrates the performance of the main data augmentation methods on CIFAR-100, WRN-28-10 in recent years. The improvement achieved by DualAug is reasonable, compared to the previous methods.\n| Method | Accuracy(%) | $\\Delta$ |\n| :---: | :---: | :---: |\n| AutoAugment (2018) | 82.90 | - |\n| RandAugment (2019) | 83.30 | 0.40 |\n| TrivialAugment (2021) | 83.54 | 0.24 |\n| DeepAA (2022) | 84.02 | 0.48 |\n| DeepAA+DualAug (2023) | **84.39** | 0.37 |\n\n\nMoreover, DualAug's **stable performance gain** is quite noticeable as observed by the reviewer. DualAug provides consistent improvement in different datasets, various baseline data augmentations (AA, RA, and DeepAA), and tasks (classification, semi-supervised learning, and self-supervised learning). Our extra experiment on more datasets in Response to Reviewer c5kH (1/7) also proves that DualAug is competitive.\n\nThanks once more for the valuable feedback and insightful review you provided. I anticipate engaging in further discussions with you."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670491559,
                "cdate": 1700670491559,
                "tmdate": 1700680039518,
                "mdate": 1700680039518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GiyMzaUfLV",
                "forum": "XgklTOdV4J",
                "replyto": "hNIaEf2jXF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Reviewer_hEeP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Reviewer_hEeP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you authors for their response. The rebuttal seems to be answer my concerns and I would like to adjust my score by one point."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701880280,
                "cdate": 1700701880280,
                "tmdate": 1700701880280,
                "mdate": 1700701880280,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UZAslt9b7m",
            "forum": "XgklTOdV4J",
            "replyto": "XgklTOdV4J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3460/Reviewer_WegG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3460/Reviewer_WegG"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach to improve on any data augmentation method based on pre-defined transformations as those proposed in Autoaugment (Cubuk et al. 2019). The idea is to apply two different kind of transformations to the same image. The first (basic) is the original of a given method such as Autoaugment or Randaugment or Deepaugment. The second (heavy) is the basic combined with an extra one, which correspond to more M, the number of applied transformations (authors tested also more magnitudes and more types of transformations but M seems to be the most important, see table 8).\nFinally an out of distribution detector based on the scores of the classification model will choose whether to apply the basic transformation or the heavy. This approach seems to provide improved results in most of the cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is simple, can be applied to different augmentation methods and does not require extra components.\n- The evaluation is performed on the most important and common datasets for image classification such as CIFAR and ImageNet.\n- The method seems to improve also FixMatch a semi-supervised approach based on data augmentation and SiamSiam a self-supervised learning approach.\n- Ablation studies help to characterise the method."
                },
                "weaknesses": {
                    "value": "- The evaluation seems missing some important evaluations on similar approaches: \na) it is missing a comparison with [1], which seems very similar in aim and has also results on the same datasets. From table 2 in [1], results are actually a bit better in [1], which is a paper form 2020.\nb) the proposed method is quite similar to TeachAugment, as reported by the authors, but the comparison is performed only on ImageNet (table 2) and without the same training. The improvement could be due to a better training pipeline. The authors should train TeachAugment on their pipeline, making sure that there are no differences in the training other than the method.\n- The method seems to have quite some hyper-parameters such as \\labda, M, warm-up phase, which makes it more complex for a real deployment.\n- There is no clear comparison with other methods in terms of computation cost. For instance, in my understanding this approach has a higher computational complexity as Teachaugment, as it requires to evaluate both basic and heavy transformation for each sample.\n- The improvement provided by the approach, although stable on different datasets, seems marginal.\n\n\n[1] Wei, Longhui, et al. \"Circumventing outliers of autoaugment with knowledge distillation.\" European Conference on Computer Vision. Cham: Springer International Publishing, 2020."
                },
                "questions": {
                    "value": "- I would like to see a fair comparison with [1] and TeachAugment on the commonly used datasets.\n- Could you compare the computational cost of the proposed approach and competing methods?\n- How often are mean and std of the basic distribution estimated?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3460/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812652384,
            "cdate": 1698812652384,
            "tmdate": 1699636298561,
            "mdate": 1699636298561,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UZQDMSQpHv",
                "forum": "XgklTOdV4J",
                "replyto": "UZAslt9b7m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WegG (1/5)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and insightful reviews. The following is our response.\n\n**Weakness 1 & Question 1: Comparison and Discussion with TeachAugment and KDforAA[1].**\n\n**KDforAA[1]**: We present a fair comparison between DualAug and KDforAA[1], as shown in the table below. The experiments are based on AutoAugment and use the WRN-28-10 model. To ensure fairness, the settings and Memory/Time cost are aligned with those of KDforAA[1]. DualAug demonstrates better performance compared to KDforAA[1].\n| Dataset  | KDforAA* | AA + DualAug |\n| :---: | :---: | :---: |\n| CIFAR-10 | 97.6 | **97.63\u00b1.09** |\n| CIFAR-100 | 83.8 | **83.94\u00b1.26** |\n\n\\* : Reported in KDforAA[1].\n\n[1] Wei, Longhui, et al. \"Circumventing outliers of autoaugment with knowledge distillation.\" ECCV, 2020.\n\n**TeachAugment**: We provide a fair comparison between TeachAugment and DualAug in the same training pipeline, as shown in the table below. The experiments use the WRN-28-10 model. To ensure fairness, we use the EMA model to filter OOD data in Adv. Aug+DualAug, as done in TeachAugment.\n\n| Dataset | TeachAug | Adv. Aug+DualAug | DeepAA+DualAug |\n| :---: | :---: | :---: | :---: |\n| CIFAR-10 | 97.25%\u00b1.11% | 97.35%\u00b1.09% | **97.52%\u00b1.18%** |\n| CIFAR-100 | 83.08%\u00b1.39% | 83.02%\u00b1.34% | **84.54%\u00b1.23%** |\n\nTeachAug and DualAug present comparable performance in the fair setting, using the Adv. Aug as the baseline. Furthermore, compared to TeachAug, we are able to improve upon a more widely automated augmentation method(DeepAA+DualAug), resulting in better performance in the same training pipeline. It is also worth noting that DualAug exhibits better performance in comparedison to TeachAug in self-supervised learning task, as shown in Table 5 (68.67% vs. 68.20%)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668978192,
                "cdate": 1700668978192,
                "tmdate": 1700674729708,
                "mdate": 1700674729708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UscjxL7LLW",
                "forum": "XgklTOdV4J",
                "replyto": "UZAslt9b7m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WegG (2/5)"
                    },
                    "comment": {
                        "value": "**Weakness 2: Hyper-parameters of DualAug** \n\nThe hyper-parameters of DualAug have shown **consistent performance** across a wide range of experiments. This includes \u03bb for the 3\u03c3 rule, the upper limit of M, and the warm-up phase. In Sections 4.1 Supervised Learning, 4.2 Semi-Supervised Learning, and 4.3 Contrastive Self-Supervised  Learning, these parameters are **consistently** set at 1.0, 10, and 20%, with the exception of ImageNet classification. Additionally, Figure 5 illustrates their consistent performance across different settings."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669296130,
                "cdate": 1700669296130,
                "tmdate": 1700680175608,
                "mdate": 1700680175608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "crslVJfiX1",
                "forum": "XgklTOdV4J",
                "replyto": "UZAslt9b7m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WegG (3/5)"
                    },
                    "comment": {
                        "value": "**Weakness 3 & Question 2: Computational cost of the proposed approach and other methods.**\n\nWe present a comparison of computation costs in the table below. The experiment is carried out on CIFAR-100 using a single 2080Ti GPU with WRN-28-10 model. As shown, when considering memory cost, DualAug uses the same memory cost as AA, while TeachAug and KDforAA require additional memory. When it comes to time cost, DualAug uses the least additional time cost. All in all, considering all factors, DualAug is a reasonable choice for achieving better performance in comparison to KDforAA and TeachAug.  \n\n| Method | Time Cost | Memory Cost  |\n| :---: | :---: | :---: |\n| AA | 5.6h | 4453MiB |\n| AA+DualAug | 8.6h | 4453MiB$^1$  |\n| KDforAA[1] | 12.9h$^2$ | 5804MiB |\n| TeachAug | 9.5h | 8714MiB |\n\n1.  In our implementation, DualAug computes the scores of two branches one after another, thus it does not incur any additional memory cost.\n2.  Containing the time of training the teacher model"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669645801,
                "cdate": 1700669645801,
                "tmdate": 1700674863499,
                "mdate": 1700674863499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fHQpiSwWZ0",
                "forum": "XgklTOdV4J",
                "replyto": "UZAslt9b7m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness 4: Improvement of DualAug**\n\nThe table below illustrates the performance of the main data augmentation methods on CIFAR-100, WRN-28-10 in recent years. The improvement achieved by DualAug is reasonable, compared to the previous method.\n\n| Method | Accuracy(%) | $\\Delta$ |\n| :---: | :---: | :---: |\n| AutoAugment (2018) | 82.90 | - |\n| RandAugment (2019) | 83.30 | 0.40 |\n| TrivialAugment (2021) | 83.54 | 0.24 |\n| DeepAA (2022) | 84.02 | 0.48 |\n| DeepAA+DualAug (2023) | **84.39** | 0.37 |\n\nMoreover, as observed by the reviewer, DualAug's **\"stable on different datasets\"** is quite noticeable. In addition to different datasets, it is also stable across various baseline data augmentations (AA, RA, and DeepAA) and tasks (classification, semi-supervised learning, and self-supervised learning). Our extra experiment on more dataset in Response to Reviewer c5kH (1/7) also proves that DualAug is competitive."
                    },
                    "title": {
                        "value": "Response to Reviewer WegG (4/5)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669789409,
                "cdate": 1700669789409,
                "tmdate": 1700727034387,
                "mdate": 1700727034387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zNr1jnwguP",
                "forum": "XgklTOdV4J",
                "replyto": "UZAslt9b7m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WegG (5/5)"
                    },
                    "comment": {
                        "value": "**Question 3: How often are mean and std of the basic distribution estimated?**\n\nThe mean and std of the basic distribution are estimated in every iteration. In Appendix A, Algorithm 1 details the process for estimating the mean and std of the basic distribution.\n\nWe would like to express our gratitude once again for taking the time to review our manuscript. We hope that my response has addressed your concerns and that you will reconsider the value of my manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669822015,
                "cdate": 1700669822015,
                "tmdate": 1700669971231,
                "mdate": 1700669971231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LI4FiqkZ2v",
                "forum": "XgklTOdV4J",
                "replyto": "UZAslt9b7m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3460/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Anqi Xiao"
                    },
                    "comment": {
                        "value": "Dear Anqi Xiao,\n\n&nbsp;  \n\nThanks for your interest of our work and the suggestion of testing with TA (Wide).  \n\nTA (Wide) uses the same set of transformations as in TA (RA) but with a wider range of strength. Indeed, it is possible that using such a larger augmentation space could produce more OOD data. Following your suggestion, we will report the combination results where TA (Wide) is adopted as the heavy augmentation branch and TA (RA) is as the basic augmentation branch, though probably after rebuttal (as the experiment is still ongoing).  \n\nMoreover, we would like to note that, in our DualAug, we build the heavy augmentation branch by adopting additional transformation operations on the basis of the basic augmentation (e.g., AA, RA, and DeepAA), aiming to exploit the diversity of augmentations possible. Thus, though slightly different from what was suggested, an experiment on TA (Wide) in this setting has been done recently, in which we compared TA (Wide) and TA (Wide) + DualAug on CIFAR-10 using WRN-28-10. The results are shown in the table below, and the statistic is calculated from three independent runs. It can be seen that our DualAug still leads to performance gains.  \n\n| Method | Accuracy(%) |\n| --- | --- |\n| TA(Wide) | 84.16\u00b1.11 |\n| TA(Wide)+DualAug | **84.48\u00b1.13** |  \n\n&nbsp;  \n\nBest regards,  \nAuthors."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3460/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726820056,
                "cdate": 1700726820056,
                "tmdate": 1700739198273,
                "mdate": 1700739198273,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]