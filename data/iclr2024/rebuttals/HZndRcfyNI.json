[
    {
        "title": "Principled Architecture-aware Scaling of Hyperparameters"
    },
    {
        "review": {
            "id": "DVMBTMLoSJ",
            "forum": "HZndRcfyNI",
            "replyto": "HZndRcfyNI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3979/Reviewer_X2JZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3979/Reviewer_X2JZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides valuable insights into the dynamic relationship among network architecture, initialization methods, and learning rates. It combines theoretical advancements with compelling empirical evidence, offering a precise characterization of how network architecture influences the interdependence of initialization strategies and the determination of optimal learning rates. The empirical evaluation is carried out on the NAS-Bench dataset, and the findings hold relevance not only for MLPs but also for CNNs with complex architectural designs. As a result, the implications of this research extend beyond the confines of deep learning theory, sparking meaningful reflections within the practical neural network design and NAS."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper delivers a compelling contribution that encompasses both theoretical and empirical dimensions. The clarity of its exposition, coupled with effective visual aids, greatly enhances comprehension of the concepts presented.\n\nIn terms of theoretical advancements, the authors expand upon the maximal-update (\u03bcP) scaling strategy initially introduced by Yang et al. in 2022. The authors delve into an investigation of how hyperparameters depend on various aspects of network architecture, including depth, connectivity patterns such as residual skips and long-range connections, kernel sizes, layer types, and convolution extensions. \n\nThe authors establish a link between learning rate scaling and layer depth for the first time, demonstrating its effectiveness over approaches that do not account for depth. Section E2 shows that the method also extends to GeLU neurons.\n\nThe authors showcase that the proposed scaled learning rates and initializations bring improvements in achievable accuracies across a broad spectrum of neural network architectures within the NAS-Bench search space. This outcome bears significant implications for the practical design of neural networks and the field of NAS, particularly considering the widespread adoption of NAS-Bench as a guiding benchmark.\n\nThe introduction of scaled learning rates and initializations yields a noteworthy consequence by narrowing the performance gap between \"good\" and \"bad\" architectures within NAS-Bench (Figure 4). They prompts questions about the ranking of architectures within NAS-Bench, prompting a critical reassessment of progress within the NAS field."
                },
                "weaknesses": {
                    "value": "While this paper, on the whole, demonstrates a commendable effort, I encountered two noteworthy concerns while reading the experimental section:\n\n-\tThe rankings of architectures may be influenced by various factors during training, including random seeds. It raises the question of how the authors determined that their revised learning rates and initializations would have the most significant impact on their observation of updated rankings.\n-\tIt's recognized that NAS-Bench typically selects learning rates closer to those utilized by \"good architectures\" that are in practical use, reflecting an intentional bias towards favoring such architectures over \"bad architectures.\" However, in Section 4.3, I could not discern evidence that the new learning rate/initialization approach improves upon the performance of the top-performing architectures, while it appears to only boost many of the \"bad architectures.\" Given this, it raises the question of why this new learning rate/initialization strategy is relevant for practical NAS if it cannot enhance the performance of the best-found architectures."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3979/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632000829,
            "cdate": 1698632000829,
            "tmdate": 1699636359552,
            "mdate": 1699636359552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BCPEo7ei5G",
                "forum": "HZndRcfyNI",
                "replyto": "DVMBTMLoSJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3979/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review!"
                    },
                    "comment": {
                        "value": "We truly thank reviewer X2JZ's time and effort in reviewing our paper!\n\n> **Q1**: The rankings of architectures may be influenced by various factors during training, including random seeds. It raises the question of how the authors determined that their revised learning rates and initializations would have the most significant impact on their observation of updated rankings.\n\nThank you for the question! We agree that numerous factors can influence architecture rankings.\n\nTo delve deeper, we analyze how the random seed affects these rankings. Please kindly refer to Appendix E.4 in our updated paper.\n\nSpecifically, NAS-Bench-201 reports network performance trained with three different random seeds (seeds = [777, 888, 999]). On CIFAR-100, we created a plot similar to the middle column of Figure 4, but it also displays pairwise ranking correlations among those three random seeds. This plot is included in Appendix E.4. In summary, although we observe different ranking correlations between seeds, they are consistently higher (i.e., correlations are more consistent) than those produced by our method.\nThis confirms that changes in network rankings by our architecture-aware hyperparameters are *meaningful*, which can train networks to better performance.\n\n> **Q2**: I could not discern evidence that the new learning rate/initialization approach improves upon the performance of the top-performing architectures, while it appears to only boost many of the \"bad architectures.\n\nThank you! We would like to point out that in the left column of Figure 4, on both CIFAR-100 and ImageNet16-120, the improvement in the high-accuracy regime is either better than or as good as that in the low-accuracy regime."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096984254,
                "cdate": 1700096984254,
                "tmdate": 1700097031924,
                "mdate": 1700097031924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6Tx3JxxiS6",
            "forum": "HZndRcfyNI",
            "replyto": "HZndRcfyNI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3979/Reviewer_nJvL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3979/Reviewer_nJvL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for optimizing hyperparameters in deep neural networks that takes into account the impact of neural architectures on hyperparameters. The authors characterize the dependence of initializations and maximal learning rates on the network architecture, including depth, width, conv kernel size, and connectivity patterns. They generalize their initialization and learning rates across MLPs and CNNs with sophisticated graph topologies. The authors verify their principles with comprehensive experiments and demonstrate that network rankings can be easily changed by better training networks in benchmarks with their architecture-aware learning rates and initialization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe paper begins with a compelling motivation, shedding light on prevalent trends in the current literature landscape. It questions the evaluation of architectural choices and the often overlooked dependency on hyperparameters.\n-\tThe paper provides thorough and well-supported proofs for its derivations related to initialization and maximal learning rates. It introduces a straightforward adjustment to weight initialization by scaling with the layers' in-degree, leading to notable performance enhancements downstream.\n-\tWhile the derivation draws substantial inspiration from previous works, it seems technically correct (and original to certain extent), and diligently cites these sources. Although maybe not the most groundbreaking, gaining a deeper understanding of how to determine initial learning rates for specific architectures holds significant importance. \n-\tThe paper critiques the evaluation methodology employed in NAS benchmarks, uncovering significant shortcomings in the process in their experiments. The evaluation of NAS encompasses a wide range of comprehensive datasets, adding to the paper's robustness and relevance. There is a strong likelihood that the rules derived from this work could prove relevant and valuable for practitioners.\n-\tThe paper exhibits excellent writing and organization, ensuring a smooth reading experience."
                },
                "weaknesses": {
                    "value": "A few questions seem to hinder my understanding of this paper\u2019s contributions, particularly with regard to the root causes behind the perceived limitations of the \u03bcP method. \n\nFirst, the paper argue that their proposed method outperforms the \u03bcP initialization and scaling method introduced by Yang et al. in 2022, often achieving significantly better results. Noting that \u03bcP results were not included in the main paper and were instead instead deferred to a much later section E.3. \n\nFrom a theoretical standpoint, Yang et al. (2022) primarily derived their scaling strategy for multi-layer perceptron (MLP) architectures in their main text, with additional derivations provided in Appendix L. On the empirical side, Yang et al. (2022) also presented experimental results on hyperparameter transfer for ResNet and Transformer architectures.\n\nThe present paper introduces a novel Directed Acyclic Graph (DAG) tool to derive \"architecture-aware\" hyperparameters. However, a fundamental question remains: What are the specific failure modes of the \u03bcP when it encounters intricate network topologies? Is the limitation attributed to a violation of Desiderata L.1, or are there other contributing factors at play?\n\nAdditionally, further clarification is needed regarding the assumptions made and the empirical comparisons drawn, especially across various architectural configurations. For example, it was not clearly mentioned until late, that the authors\u2019 theory cannot apply to normalization layers."
                },
                "questions": {
                    "value": "Same as Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3979/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633086723,
            "cdate": 1698633086723,
            "tmdate": 1699636359418,
            "mdate": 1699636359418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S8qKvw5ufZ",
                "forum": "HZndRcfyNI",
                "replyto": "6Tx3JxxiS6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3979/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review!"
                    },
                    "comment": {
                        "value": "We truly thank reviewer nJvL's time and effort in reviewing our paper!\n\n> **Q1**: What are the specific failure modes of the \u03bcP when it encounters intricate network topologies? Is the limitation attributed to a violation of Desiderata L.1, or are there other contributing factors at play?\n\nThanks for your question! In the second paragraph of Section 2.2, we specifically discussed two core differences between $\\mu$P and our method (and they are also reasons for $\\mu$P\u2019s pitfall in our scenario): 1) the non-trivial dependence of the learning rate on network depth, and 2) the complicated graph topologies of architectures. We will merge Figure 8 in Appendix E.3 into Figure 4.\n\n> **Q2**: Additionally, further clarification is needed regarding the assumptions made and the empirical comparisons drawn, especially across various architectural configurations. For example, it was not clearly mentioned until late, that the authors\u2019 theory cannot apply to normalization layers.\n\nThank you for pointing that out! In the conclusion section, we have clarified some of our limitations, including the existence of normalization layers. We will include additional discussions in our camera-ready version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096487246,
                "cdate": 1700096487246,
                "tmdate": 1700096487246,
                "mdate": 1700096487246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ip5sliTNuy",
            "forum": "HZndRcfyNI",
            "replyto": "HZndRcfyNI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3979/Reviewer_drLU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3979/Reviewer_drLU"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to establish principles for initializing and selecting learning rates in neural network architectures characterized by directed acyclic graphs (DAGs), which can be highly irregular in structure. The authors propose an initialization method to maintain pre-activation variance during forward propagation and derive architecture-specific learning rates using a maximal update prescription. Experimental validation demonstrates their effectiveness and potential for improving neural architecture search benchmarks by enhancing network training and rankings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe paper builds on previous research in hyperparameter and architecture search, aiming to establish a principled connection between weight initialization and learning rate choices with both MLP and CNN architectures.\n-\tIt introduces an architecture-aware modified fan-in initialization scheme that preserves information flow through various graph topologies.\n-\tThe paper analytically derives formulas for scaling learning rates based on architecture topology, specifically using the maximal update (\u03bcP) heuristic.\n-\tExperimental results demonstrate the superior performance of the proposed methods and highlight their potential to reshape network rankings in standard NAS benchmarks.\n-\tThese findings suggest that implementing the proposed principles may lead to improved evaluations of NAS algorithms."
                },
                "weaknesses": {
                    "value": "-\tThe clarity of the main body could be improved a lot, via providing a concise summary of main derivations, potentially by reducing the repetitive criticism of architecture search.\n-\tThe experimental section raises several concerns. The strategy of finding a base maximal learning rate for one epoch may not be meaningful for practical training cycles, where learning rates follow complex schedules. The criticism of NAS for using \"the same hyperparameters\" overlooks the intricate learning rate strategies commonly employed.\n-\tThe empirical results are also not entirely convincing. Figure 2 exhibits a weak correlation with questionable linearity and Figure 3 has a limited range of learning rates. The proposed improvements in Figure 4 seem to be primarily in lower accuracy regimes, raising doubts about absolute improvement on the top-performer architectures (which are of the most interest)\n-\tThe discussion of prior art is limited, particularly regarding weight initialization and learning rates, and it's unclear how the proposed method advances over existing insights in the literature."
                },
                "questions": {
                    "value": "See previous section of weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3979/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634681126,
            "cdate": 1698634681126,
            "tmdate": 1699636359324,
            "mdate": 1699636359324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R9174BhIgh",
                "forum": "HZndRcfyNI",
                "replyto": "Ip5sliTNuy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3979/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review!"
                    },
                    "comment": {
                        "value": "We truly thank reviewer drLU's time and effort in reviewing our paper!\n\n> **Q1**: The clarity of the main body could be improved a lot, via providing a concise summary of main derivations, potentially by reducing the repetitive criticism of architecture search.\n\nWe have summarized our main derivations at the beginning of Section 3. We will further reduce the emphasis on the architecture search part and will update our draft accordingly in our camera ready.\n\n> **Q2**: The strategy of finding a base maximal learning rate for one epoch may not be meaningful for practical training cycles, where learning rates follow complex schedules.\n\nIn Section 4.3, we strictly followed the cosine learning rate schedule used in NAS-Bench-201 to train different networks. Additionally, we discuss the practical implications of finding the base maximal learning rate for one epoch at the bottom of page 7.\n\n> **Q3**: Figure 2 exhibits a weak correlation with questionable linearity and Figure 3 has a limited range of learning rates. The proposed improvements in Figure 4 seem to be primarily in lower accuracy regimes.\n\nWe respectfully disagree that a 0.838 correlation in Figure 2 is weak. Figure 3 spans an even wider range of learning rates than Figure 2. In the left column of Figure 4, for both CIFAR-100 and ImageNet16-120, the improvement in the high-accuracy regime is as good as, or better than, that in the low-accuracy regime.\n\n> **Q4**: The discussion of prior art is limited, particularly regarding weight initialization and learning rates, and it's unclear how the proposed method advances over existing insights in the literature.\n\nIn the second paragraph of Section 2.2, we provide a detailed discussion of closely related works.\n\nOur work advances over existing insights: Previous studies have observed and developed heuristics about the relationship between learning rates and network structures. However, our core advantage over prior work lies in precisely characterizing the non-trivial dependence of learning rates on network depth, DAG topologies, and convolutional kernel size. This goes beyond previous heuristics and has practical implications.\n\nWhile our underlying principle may seem to echo patterns from earlier studies, such as using smaller learning rates for larger models, we provide the exact scale of this dependence (-3/2 for depth, and the summation of depths across the network\u2019s graph structure). As confirmed by reviewers nJvL and X2JZ, our work offers a deeper understanding of how to determine initial learning rates for specific architectures and establishes a link between learning rate scaling and layer depth for the first time."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700095704527,
                "cdate": 1700095704527,
                "tmdate": 1700095704527,
                "mdate": 1700095704527,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mhEZWbl0Km",
            "forum": "HZndRcfyNI",
            "replyto": "HZndRcfyNI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3979/Reviewer_BP74"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3979/Reviewer_BP74"
            ],
            "content": {
                "summary": {
                    "value": "This work provides an extension of $\\mu$transfer to arbitrary network structures and an application of the resulting method to neural architecture search (NAS), specifically in order to make NAS benchmarks more comparable by ensuring all competing networks are given a better chance to train. The major contributions:\n\n* An improved way to set initializations and layer specific learning rates to stabilise training over network scales\n* Experimental results demonstrating that NAS benchmarks are flawed: making the training process more stable massively changes the results\n\nWeights are initialized to be normally distributed with variance:\n$$\n\\sigma^2 = \\frac{C^{(l', l)}}{n} = \\frac{2}{d_{in}^{(l')}}\n$$\nfor an $n \\times n$ weight matrix with $d_{in}^{(l')}$ in-degree (the number of connections to the layer from the previous layer).\nAlso, the final layer is scaled by $\\propto \\frac{1}{n^2}$.\n\nScale the layerwise learning rates according to\n$$\n\\eta^{*} \\simeq c \\left( \\sum_{p=1}^{P} L_p^3 \\right)^{-1/2}\n$$\nwhere $c$ is a layer-independent constant and $L_{p}$ is the number of relu layers on path $p$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Neural Architecture Search needs a way to verify that the networks being searched over are actually being trained sufficiently. The goal of this paper to standardise that process around a framework that should allow much more reliable results in this area of study.\n\nI have not checked all the derivations but the theory appears to be correct. I am confident that I could verify the results if I had more time.\n\nThe contributions as presented are met:\n\n* The initialization is provided, justified and works in the experiments\n* The extension to $\\mu$P is introduced clearly and also works in experiments\n* Experiments demonstrate a key failing of NAS benchmarks and present a solution\n\nThe two limitations in the literature that this paper addresses are:\n\n1. $\\mu$P is not defined for arbitrary DAGs, only feedforward networks\n2. Neural Architecture Search benchmarks are useless if we can't trust that the networks have been trained well\n\nThe presentation is good, explaining the key points of the $\\mu$P paper in a brief way, better than the original paper.\nThe authors use an idiosyncratic system of emphasis, using both underlines and bold fonts on key points. This actually works quite well and I found the points being emphasised generally did earn more attention. The authors also use \u00a7 in place of \"Section\", which I guess saved space, and works just as well."
                },
                "weaknesses": {
                    "value": "The initialization, learning rate tuning and layer specific learning rates are introduced to maintain the update scale at $O(1)$ but the update scale during training doesn't appear to have been measured. It would be nice to see empirical verification that the method is working as intended. Although, I understand that the results in 4.1 and 4.2 both indicate that it is.\n\nGiven the equations above it is not easy for the reader to replicate the exact initialization and learning rate scaling required. Some pseudocode or a reference implementation would help a lot. It's been a problem for $\\mu$P adoption as well, that practitioners find that there were ambiguities in the description that make it hard to implement in practice. For example, I don't know where the $c$ parameter is set and I'm not fully confident how to count the number of paths to produce $L_p$.\n\nIt is stated \"the final layer weights are initialized with variance $1/n^2$ instead of $1/n$\" on page 5, and I know this is from $\\mu$P but it is never stated in this paper why.\n\nMinor presentation issues:\n\n1. \"most designs of principles\" in abstract doesn't make sense\n2. Why is the MSE loss introduced in equation 6? The experiments are mostly classification results using a cross-entropy loss\n3. Equation 8 contains $L_p$ but $L_p$ is only defined in the Appendix, it should be defined near the equation\n4. Section 3.5 \"speed-up\" -> \"speeds up\""
                },
                "questions": {
                    "value": "For each network architecture sampled in the NAS-Bench experiments, the network learning rate is first tuned at small scale. Was the learning rate also tuned on the networks being trained without this hyperparameter transfer method? This may already be in the experiments section or the Appendices and I may have missed it.\n\nWhy are the maximal LRs found by experiment in Section 4.1 and 4.2 not the same as those computed by the theory? Is there any way they could be brought closer together?\n\nHow was the experiment in Sections 4.1 and 4.2 performed? I guess you find the optimal learning rate at a small scale on a set of networks, then scale the network up using the method described in the paper to scale the initalization and learning rates, then perform a grid search to find the optimal learning rates to compare against?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3979/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3979/Reviewer_BP74",
                        "ICLR.cc/2024/Conference/Submission3979/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3979/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795081901,
            "cdate": 1698795081901,
            "tmdate": 1700599359646,
            "mdate": 1700599359646,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fcg6e7VwXX",
                "forum": "HZndRcfyNI",
                "replyto": "mhEZWbl0Km",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3979/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3979/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review!"
                    },
                    "comment": {
                        "value": "We truly thank reviewer BP74's time and effort in reviewing our paper!\n\n> **Q1**: Update scale during training.\n\nThanks for the question! We further conduct experiments to verify the O(1) update scale for pre-activations.\n\nSpecifically, using our settings in Section 4.1 (MLP with topology scaling), we measure the change in a network\u2019s output $z^{(L+1)}$ (Equation 7) before and after the first gradient descent step (averaged over 10 minibatches, on CIFAR-10), and compare the norm of this change across different MLP architectures. Based on our analysis, the baseline (standard initialization plus a fixed learning rate) cannot preserve this O(1) update scale when scaling up networks and will also give a large variance across different architectures.\n\nExperiments show that $\\mathbb{E}\\left[\\left(\\Delta z_{i}^{(L+1)}\\right)^2\\right]$ for the baseline is 5.59 (std = 4.48), while our topology-aware initialization and learning rate yield a norm of 1.70 (std = 1.01). This result indicates that the success of our hyperparameter scaling is based on the preservation of the O(1) update scale across models.\n\n> **Q2**: Reference implementation ($C$ and $L_p$).\n\n* $C$ is initially set as 2 for ReLU in standard initialization, and is further updated to be architecture-aware (see Eq. 4) in our paper.\n* Calculating $L_p$ is straightforward: after representing the architecture as a computational graph, we derive its affinity matrix. Similar to Fig. 1, in this matrix, each element indicates the type of layer ($W$) used to connect two features ($x$ and $z^{(1)}, z^{(2)}, \\cdots, z^{(L+1)}$); \u201c0\u201d indicates no connections (layers) between hidden features, \u201c1\u201d indicates a skip connection, and \u201c2\u201d indicates other parameterized layers (with ReLU activation). This affinity matrix reveals: 1) the number of paths (consecutive layers) from the input to the output (i.e., $P$); 2) the number of parameterized layers on each path (i.e., $L_p$).\nWe will ensure comprehensive documentation in the README.md file in our released code.\n\n> **Q3**: The final layer weights are initialized with variance 1/n^2 instead of 1/n.\n\nWe adhere to the variance of $1/n^2$ for the output layer, primarily to align with the desiderata outlined in Section J.2.1 of the referenced work ($\\mu$P). This approach is chosen to preserve the O(1) size of pre-activations. It also accommodates the observation that \"the input layer is updated much more slowly than the output layer\" as mentioned in their Section 5.\n\n> **Q4**: Minor presentation issues.\n\nThanks for these suggestions!\n* We derive our principle using the MSE loss, mainly for its clarity in analyzing the changes of the model\u2019s output $z^{(L+1)}$ (Eq. 21).\n* $L_p$ is defined in Sec. 3.3 (after Eq. 7) and is also depicted in Fig. 1.\n\n> **Q5**: Was the learning rate also tuned on the networks being trained without this hyperparameter transfer method?\n\nThe learning rate used in the baseline (x-axis in Fig. 4 left column) was not tuned. It was originally pre-determined in NAS-Bench-201.\n\n> **Q6**: Why are the maximal LRs found by experiment in Section 4.1 and 4.2 not the same as those computed by the theory?\n\nMaximal learning rates (LRs) found through experiments are not exactly the same as those predicted by our theory, mainly due to the more complicated training dynamics that are not fully captured by our analysis.\nBring them closer together requires detailed analysis of the gradient descent beyond the first step.\nDespite this, our work demonstrates that analyzing only the first gradient descent step can still yield highly accurate learning rate predictions, with correlations exceeding 0.8.\n\n> **Q7**: How was the experiment in Sections 4.1 and 4.2 performed?\n\nYes, our experiment pipeline is exactly the same as those three steps we stated at the beginning of Section 4."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700095288747,
                "cdate": 1700095288747,
                "tmdate": 1700095288747,
                "mdate": 1700095288747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5U0GSqmiJH",
                "forum": "HZndRcfyNI",
                "replyto": "Fcg6e7VwXX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3979/Reviewer_BP74"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3979/Reviewer_BP74"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: The results you summarise here are promising, if these were presented in a table I would appreciate a comparison to $\\mu$P heuristics. Otherwise, yes it appears that the update scale is being preserved. It may also be worthwhile using a very large batch size for this measurement to minimize SGD noise.\n\nQ2: I should probably have reviewed the code attached when I was reviewing this paper when making statements about example code that would be useful, I apologise.\n\nQ3: This was an issue with presentation but it is minor.\n\nQ4: I see where $L_p$ is defined now before the Appendix, on the line starting **Path Depth**.\n\nMy concerns in the remaining questions are addressed, I will update my review."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3979/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599340154,
                "cdate": 1700599340154,
                "tmdate": 1700599340154,
                "mdate": 1700599340154,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]