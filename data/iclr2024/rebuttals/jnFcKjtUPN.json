[
    {
        "title": "COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL"
    },
    {
        "review": {
            "id": "QcRRImB7Dn",
            "forum": "jnFcKjtUPN",
            "replyto": "jnFcKjtUPN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1948/Reviewer_D4cs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1948/Reviewer_D4cs"
            ],
            "content": {
                "summary": {
                    "value": "This work presents COPlanner, a model-based reinforcement learning algorithm that combines optimistic exploration of high-reward model uncertain regions with pessimistic model learning to avoid biased updates in information-sparse regions. The approach leverages an MPC-style action selection and is evaluated on both proprioceptive as well as visual control tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tIncreasing learning efficiency of model-based reinforcement learning agents is an important research direction, particularly in light of hardware deployment under costly data generation\n-\tCombining optimistic exploration of uncertain high-reward behaviors with conservative model rollouts to improve information quality of samples is a very promising research direction\n-\tMPC-style action selection provides several benefits over direct policy evaluation\n- The approach is evaluated against several baseline agents on OpenAI Gym as well as DeepMind Control Suite environments, with both proprioceptive and visual tasks for the latter"
                },
                "weaknesses": {
                    "value": "-\tGenerally, the paper would benefit from a stronger selection of established baselines. E.g., for proprioceptive DMC model-free MPO/DMPO/D4PG or model-based DreamerV3, for visual DMC DrQ-v2 as a model-free baseline\n-\tThe visual control experiments should be run for more than 1M steps as agents have not converged on the Acrobot/Finger/Quadruped tasks. Comparing with Dreamer-v1 Figure 10, agents would be expected to converge and solve the tasks at approximately 2M steps.\n-\tThe Dreamer-v2 scores in Figure 9 appear to be significantly lower than the ones provided by the original authors on GitHub \u2013 which Dreamer implementation was used to obtain these scores?\n-\tThe task selection for proprioceptive DMC consists of relatively easy tasks and some of the \u201cMuJoCo\u201d/Gym tasks have not been run to convergence\n-\tThe hyperparameter study in Appendix D.4 is interesting, but should be extended to more environments to observe clearer trends\n-\tSome highly related work is missing. E.g., the optimistic exploration of uncertain future returns for visual control has previously been investigated in [1], also building on the Dreamer-v2 agent, while optimistic finite-horizon exploration under nominal reward functions was studied in [2]. Additionally, work such as POLO [3] would be relevant.\n\n[1] T. Seyde, et al. \"Learning to plan optimistically: Uncertainty-guided deep exploration via latent model ensembles,\" CoRL 2021.\n\n[2] P. Ball, et al. \"Ready policy one: World building through active learning,\" ICML 2020.\n\n[3] K Lowrey, et al.  \"Plan online, learn offline: Efficient learning and exploration via model-based control,\" ICLR 2019.\n\n\nMinor:\n\n-\tThe DMC tasks are MuJoCo-based, so \u201cMuJoCo\u201d task description should be replaced by \u201cGym\u201d"
                },
                "questions": {
                    "value": "-\tAction selection by optimizing over a 5-step rollout without terminal value function is surprising. Do you have intuition for why this short-term reasoning is sufficient? \n-\tThe original Dreamer-v3 paper also evaluated on proprioceptive DMC environments - why not use Dreamer-v3 as a proprioceptive baseline as well?\n-\tThere are several task-specific parameter choices for proprioceptive control. How impactful are these? Ideally a single set of hyperparameters would be used throughout."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1948/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1948/Reviewer_D4cs",
                        "ICLR.cc/2024/Conference/Submission1948/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698103540230,
            "cdate": 1698103540230,
            "tmdate": 1700519789651,
            "mdate": 1700519789651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1gnENjwgs5",
                "forum": "jnFcKjtUPN",
                "replyto": "QcRRImB7Dn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D4cs Part 1"
                    },
                    "comment": {
                        "value": "We thank Reviewer D4cs for the insightful feedback. We are encouraged that Reviewer D4cs finds our idea is important and promising, and experiments are fruitful. We now address all of Reviewer D4cs's concerns and questions below:\n\n> Weakness 1: Generally, the paper would benefit from a stronger selection of established baselines. E.g., for proprioceptive DMC model-free MPO/DMPO/D4PG or model-based DreamerV3, for visual DMC DrQ-v2 as a model-free baseline.\n\nThank you for your suggestion. Following your advice, we add a comparison with D4PG in the proprioceptive control task in Figure 4, and a comparison with DrQv2 in the visual control task in Figure 5. \n\nAdditionally, due to space limitations, we include a comparison with DreamerV3 in six medium-difficulty proprioceptive control tasks in Appendix D.1. For specific experimental details and results, please refer to General Response and the updated paper.\n\n> Weakness 2: The visual control experiments should be run for more than 1M steps as agents have not converged on the Acrobot/Finger/Quadruped tasks. Comparing with Dreamer-v1 Figure 10, agents would be expected to converge and solve the tasks at approximately 2M steps.\n\nWe would like to offer our apologies here. Due to differences in the counting methods for Counter and Environment Step in the JAX code of DreamerV3, there is a mistake in the previous version of our paper. The original version of Figure 5 represents the performance of all methods at 500k environment steps. We have run these methods up to 1M steps and have reported the new results in both Figure 5 and Table 5 in the revised manuscript. The new results show that nearly all methods have converged at 1M steps, and DreamerV3 significantly outperforms the results reported in the DreamerV1 paper. We apologize again for this mistake.\n\n> Weakness 3: The Dreamer-v2 scores in Figure 9 appear to be significantly lower than the ones provided by the original authors on GitHub \u2013 which Dreamer implementation was used to obtain these scores?\n\nWe use one of the most popular PyTorch implementations of DreamerV2 available on GitHub, which has the second-highest number of stars: https://github.com/jsikyoon/dreamer-torch. Since it is implemented in PyTorch, there may be differences in the results compared to the TensorFlow version of the original source code. Therefore, we only include these experiments in the Appendix as a reference and do not feature them in the main paper. In the main paper, for the DreamerV3 experiments, we use the original JAX code released with the DreamerV3 paper.\n\n> Weakness 4: The task selection for proprioceptive DMC consists of relatively easy tasks and some of the \u201cMuJoCo\u201d/Gym tasks have not been run to convergence\n\nThe reason we chose these tasks from proprioceptive DMC is that, based on our extensive testing, MBPO-style methods do not work well in more complex DMC tasks. Therefore, we only report the tasks where methods based on MBPO are effective. \n\nAs an addition, in the updated version of our paper, we include a comparison with DreamerV3 in six medium-difficulty proprioceptive control DMCtasks to demonstrate that our method can improve the performance and sample efficiency over the baseline in more complex tasks. Furthermore, in the updated paper, we run the Ant and Humanoid environments from OpenAI Gym up to 200k steps, where all methods have now converged. For specific experimental results, please refer to the updated paper.\n\n> Weakness 5: The hyperparameter study in Appendix D.4 is interesting, but should be extended to more environments to observe clearer trends\n\nIn the updated version of our paper, we add two sets of hyperparameter studies, one using COPlanner-MBPO on the Cheetah-run task and the other using COPlanner-DreamerV3 on the Hopper-hop task. The trends we observed are largely consistent with our previous experimental results. For specific experimental results, please refer to **Appendix D.4, Figure 11** in the updated paper.\n\n> Weakness 6: Some highly related work is missing.\n\nThank you for your reminder. We have added references to these papers in the related works section of our updated paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700249027721,
                "cdate": 1700249027721,
                "tmdate": 1700249048107,
                "mdate": 1700249048107,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AY4k09hInE",
                "forum": "jnFcKjtUPN",
                "replyto": "QcRRImB7Dn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Does our response address your concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer D4cs,\n\nAs the stage of the review discussion is ending soon, we would like to kindly ask you to review our revised paper as well as our response and consider making adjustments to the scores. We believe we have addressed all concerns. Please let us know if there are any other questions. We would appreciate the opportunity to engage further if needed."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447606672,
                "cdate": 1700447606672,
                "tmdate": 1700447706053,
                "mdate": 1700447706053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NQxCyoP2E6",
                "forum": "jnFcKjtUPN",
                "replyto": "AY4k09hInE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1948/Reviewer_D4cs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1948/Reviewer_D4cs"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your replies and updated experiments. I still believe that extension to more complex tasks would benefit the potential impact of the paper. Regarding integration of additional related work: RP1 (Ball et al.) and LOVE (Seyde et al.) explore uncertain rewards and not purely uncertain dynamics. There are some important differences between the works listed on page 6 [Lowrey, ..., Hu] which would be worth expanding on as well as relating to the presented work. Overall, the updated experiments look significantly stronger and I'm leaning towards accept."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519746180,
                "cdate": 1700519746180,
                "tmdate": 1700519746180,
                "mdate": 1700519746180,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PzCzd6Rk99",
            "forum": "jnFcKjtUPN",
            "replyto": "jnFcKjtUPN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1948/Reviewer_NmVv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1948/Reviewer_NmVv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model-based reinforcement learning framework COPlanner to mitigate the model errors from the model rollouts and environment exploration. This framework includes three components, and the most crucial component is the Planner to predict future trajectories based on selected actions and their corresponding uncertainties, and the uncertainties work as penalties during model rollouts and bonuses during environment exploration."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is easy to understand.\n2. The idea of using the variance over predicted states of the ensemble members to approximate the model uncertainty either penalty or reward seems interesting to me.\n3. The proposed framework outperforms baselines in almost all experiments."
                },
                "weaknesses": {
                    "value": "1. Lack of comparisons to some framework.  In section 3.2, the paper mentions some previous methods to estimate uncertainty samples after generations or to decrease model error by re-weighting or discarding samples with high uncertainty. This should require a comparison to demonstrate using variance through model ensemble.\n2. Lack of visualization of experiments. The results are all basically tables, line plots."
                },
                "questions": {
                    "value": "1. It's still not quite clearly to me how Conservative rate and Optimistic rate are selected. What's the intuition here about why they are always different than each other in every experiment settings? Are they selected according to Action candidate number, and Planning horizon as well?\n2. Following the weakness, experiments for comparison and more visualizations are necessary."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1948/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1948/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1948/Reviewer_NmVv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817876808,
            "cdate": 1698817876808,
            "tmdate": 1699636126289,
            "mdate": 1699636126289,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ILwTV8McTM",
                "forum": "jnFcKjtUPN",
                "replyto": "PzCzd6Rk99",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NmVv"
                    },
                    "comment": {
                        "value": "We thank Reviewer NmVv for the insightful feedback. We are encouraged that Reviewer NmVv finds our paper easy to follow, the idea interesting, and experiment results strong. We now address all of Reviewer NmVv's concerns and questions below:\n\n> Weakness 1: Lack of comparisons to some framework. In section 3.2, the paper mentions some previous methods to estimate uncertainty samples after generations or to decrease model error by re-weighting or discarding samples with high uncertainty. This should require a comparison to demonstrate using variance through model ensemble.\n\nIn the updated version of our paper, we include experimental comparisons with M2AC [1]. M2AC falls under the category of methods that discard samples with high uncertainty. For the category of methods that re-weight samples with high uncertainty, one of our comparison baselines, MEEE, belongs to this category. The additional experimental results also demonstrate the strong performance of our method. For specific experimental details and results, please refer to Section 5.1 and Figure 4 in the updated version of the paper.\n\n\n> Weakness 2: Lack of visualization of experiments. The results are all basically tables, line plots.\n\n\nIn the updated version of our paper, we include experiments on test time trajectory visualization. We visualize the trajectories of policies learned by COPlanner-DreamerV3 and Dreamer V3 in the test environment after convergence of policy learning in **Appendix D.8**. From Figures 14 to 17, it can be seen that COPlanner-DreamerV3 learned more rational and stable behaviors compared to DreamerV3.\n\n> Question 1: It's still not quite clearly to me how Conservative rate and Optimistic rate are selected. What's the intuition here about why they are always different than each other in every experiment settings? Are they selected according to Action candidate number, and Planning horizon as well?\n\nAs discussed in Appendix D.4 of our paper, we conduct a detailed hyperparameter study to discuss how parameters should be chosen and then selected a set of parameters that perform relatively well across all tasks. Regarding why these two parameters are different, we believe that these two are essentially completely unrelated parameters, one controlling exploration and the other controlling exploitation (reducing model error). In proprioceptive control tasks, since the model directly predicts the next state, it is more prone to model compounding errors, thus requiring a larger Conservative rate. In contrast, in visual control tasks, the dynamics model predicts in the latent space, which to some extent mitigates the impact of model compounding errors. Therefore, the Conservative rate can be set relatively lower, allowing the model rollout to more likely choose high-reward, model-generated samples for training the policy. The choice of these two parameters is independent of Action candidate number and Planning horizon.\n\nReference:\n\n[1]. \"Trust the Model When It Is Confident: Masked Model-based Actor-Critic.\" Pan et. al. NeurIPS 2020.\n\n\n-----\nThank you again for your effort in reviewing our paper! We are happy to answer any further questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248889095,
                "cdate": 1700248889095,
                "tmdate": 1700248889095,
                "mdate": 1700248889095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JbPmw6g5Sh",
                "forum": "jnFcKjtUPN",
                "replyto": "ILwTV8McTM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1948/Reviewer_NmVv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1948/Reviewer_NmVv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications! I choose to maintain my current score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597979102,
                "cdate": 1700597979102,
                "tmdate": 1700597979102,
                "mdate": 1700597979102,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QIMVLrY9td",
            "forum": "jnFcKjtUPN",
            "replyto": "jnFcKjtUPN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1948/Reviewer_EHTZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1948/Reviewer_EHTZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model based reinforcement learning method that target for solving the challenge of inaccurately learned dynamic model problem through a combination of conservative model rollouts offline and optimistic exploration with the environment online. To estimate the dynamic model uncertainty, the authors utilize the model disagreement method which learns an ensemble of dynamic models.  This estimate uncertainty are utilized in two folds, it can serve as a penalty term during rollouts and as an incentive when interating with the environment online. The authors conduct experiments in environments like MuJoCo and DeepMind Control and demonstrate the proposed method achieves better sample efficiency and performance than other model-based RL baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is easy to follow and the proposed approach is also simple and flexible to use on existing model based RL methods. The authors have done extensive experiments and ablations to show their strengths in sample efficiency and model performance. I think the part of the success of this paper attributes to the good uncertainty estimation."
                },
                "weaknesses": {
                    "value": "The good uncertainty estimation comes with the cost of extra computation, in Appendix D.6, there is an above 20\\% increase for MBPO variant and around 40\\% increase for the DreamerV3 variant. The test time comparison should also be discussed."
                },
                "questions": {
                    "value": "Where do the authors see the main factor for the extra computation cost?  Is it the uncertainty calculation for all action candidates through the ensemble of dynamic models?\n\nWhat options is available for trading off computation cost for uncertainty estimation performance?\n\nIn Figure 7, while the proposed method reached lower loss, this does not imply that the method achieved better exploration, which the authors claim in related work section, how can this be demonstrated?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1948/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1948/Reviewer_EHTZ",
                        "ICLR.cc/2024/Conference/Submission1948/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824698058,
            "cdate": 1698824698058,
            "tmdate": 1700515632422,
            "mdate": 1700515632422,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lCJuJFrlm5",
                "forum": "jnFcKjtUPN",
                "replyto": "QIMVLrY9td",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1948/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EHTZ"
                    },
                    "comment": {
                        "value": "We thank Reviewer EHTZ for the insightful feedback. We are encouraged that Reviewer EHTZ finds our method flexible, our experiments fruitful and results strong. We now address all of Reviewer EHTZ's concerns and questions below:\n\n> Weakness 1: The good uncertainty estimation comes with the cost of extra computation, in Appendix D.6, there is an above 20% increase for MBPO variant and around 40% increase for the DreamerV3 variant. The test time comparison should also be discussed.\n\nAt test time, COPlanner, MBPO, and DreamerV3 are indistinguishable; all use the trained policy to directly interact with the environment, with the input being the observation and the output being the action. The UP-MPC is only used during the policy training phase, that is, interacting with the real environment to obtain real samples and using model rollouts to generate more imaginary samples. Therefore, the increased computational cost only exists during the training phase. During the test time, COPlanner does not incur any extra computational cost.\n\n> Question 1: Where do the authors see the main factor for the extra computation cost? Is it the uncertainty calculation for all action candidates through the ensemble of dynamic models?\n\nThe extra computation cost mainly comes from two parts: \n1. To estimate the model uncertainty of each model-generated sample, we train a dynamics model ensemble. \n2. During the UP-MPC process, we need to perform 5-step planning for each action candidate, which also brings additional time consumption.\n\n> Question 2: What options is available for trading off computation cost for uncertainty estimation performance?\n\nUsing alternative methods for estimating uncertainty in place of a model ensemble can improve computational efficiency. In Figure 12 of Appendix D.5, we compare methods that use different intrinsic rewards to guide exploration. The experiments show that although these methods can speed up training, their performance is not as good as the COPlanner using a model ensemble. Additionally, a potentially effective approach is to use a Bayesian dynamics model instead of a model ensemble for estimating model uncertainty. We plan to further explore and experiment with this approach in future work.\n\n> Question 3: In Figure 7, while the proposed method reached lower loss, this does not imply that the method achieved better exploration, which the authors claim in related work section, how can this be demonstrated?\n\nAs mentioned in point five of our General Response, we add the evaluation of the sample diversity in the real sample buffer obtained by COPlanner-DreamerV3 and DreamerV3 in **Appendix D.7**. The experimental results shown in Figure 13 demonstrate that the sample diversity in the real sample buffer obtained by COPlanner-DreamerV3 is significantly better than that of DreamerV3, proving that COPlanner-DreamerV3 achieves better exploration of the real environment. For specific experimental details, please refer to Appendix D.7 in the updated paper.\n\n\n-----\nThank you again for your effort in reviewing our paper! We are happy to answer any further questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248852206,
                "cdate": 1700248852206,
                "tmdate": 1700248852206,
                "mdate": 1700248852206,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jBvCeJWYxL",
                "forum": "jnFcKjtUPN",
                "replyto": "lCJuJFrlm5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1948/Reviewer_EHTZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1948/Reviewer_EHTZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517481702,
                "cdate": 1700517481702,
                "tmdate": 1700517481702,
                "mdate": 1700517481702,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]