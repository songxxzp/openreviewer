[
    {
        "title": "Byzantine Robustness and Partial Participation Can Be Achieved Simultaneously: Just Clip Gradient Differences"
    },
    {
        "review": {
            "id": "kNtMk1AmcW",
            "forum": "qB0IV2DpeS",
            "replyto": "qB0IV2DpeS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2050/Reviewer_mpAd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2050/Reviewer_mpAd"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors develop a novel method called Byz-VR-MARINA-PP, which can allow partial participation and have Byzantine robustness simultaneously. The convergence results of Byz-VR-MARINA-PP for non-convex objectives and objectives that satisfy PL conditions are provided."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "(1) The paper is generally well-written and not hard to understand. \n\n(2) A rigorous theoretical analysis is provided in this paper. Although I did not check the details, the proof seems to be correct."
                },
                "weaknesses": {
                    "value": "However, there are also some weaknesses, as listed below.\n\n(1) The main differences between Byz-VR-MARINA-PP and Byz-VR-MARINA are that Byz-VR-MARINA-PP adopts gradient clipping and thus allows partial participation (PP). There are typically two benefits of PP, i.e., tolerating inactive clients and accelerating training processes. However, Byz-VR-MARINA-PP seems to perform poorly in either of the two aspects.\n\n(1a) As presented in Algorithm 1, in Byz-VR-MARINA-PP, the clients are sampled by the server before each round. Therefore, if a selected client becomes inactive, the whole training process will be blocked. In other words, Byz-VR-MARINA-PP cannot tolerate inactive clients.\n\n(1b) All clients will participate in the $k$-th training round if $c_k=1$. That is to say, all clients will participate in the training per $1/p$ rounds in expectation. I understand that $p$ is typically small. However, it will also greatly limit the acceleration effect of PP since in federated learning (especially in cross-device federated learning), the fraction of selected clients in each round is usually small.\n\nGiven the reasons above, could the authors specify what benefits the partial participation mechanism can bring?\n\n(2) The computation of full gradients is time-consuming. Moreover, it is unknown whether the gradient clipping is empirically compatible with the PAGE estimator. I strongly suggest the authors empirically test the performance of the proposed method on some federated learning benchmarks such as LEAF [1].\n\n[1] Caldas, Sebastian, et al. \"Leaf: A benchmark for federated settings.\" arXiv preprint arXiv:1812.01097 (2018)."
                },
                "questions": {
                    "value": "Please see my comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698490013984,
            "cdate": 1698490013984,
            "tmdate": 1699636136760,
            "mdate": 1699636136760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9mdsOMS1Un",
                "forum": "qB0IV2DpeS",
                "replyto": "kNtMk1AmcW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mpAd [Part 1/2]"
                    },
                    "comment": {
                        "value": ">**The main differences between Byz-VR-MARINA-PP and Byz-VR-MARINA are that Byz-VR-MARINA-PP adopts gradient clipping and thus allows partial participation (PP). There are typically two benefits of PP, i.e., tolerating inactive clients and accelerating training processes. However, Byz-VR-MARINA-PP seems to perform poorly in either of the two aspects.**\n\nTolerating inactive clients and accelerating the training process are indeed among the main benefits of methods supporting partial participation. However, it is important to distinguish two different situations: the scenario with client sampling and the setup where partial participation appears due to the unavailability of a fraction of clients during particular communication rounds. Our work studies the first case, i.e., we assume in our analysis that all clients are available at any time. Therefore, we do not study the problem of tolerating inactive clients in theory in *all generality*. Next, one can model the inactiveness of the clients as follows: each client is equally likely unavailable during each particular round with some probability (1-q).  This is a common assumption in the literature on PP. Then, our analysis and results will still hold if we multiply $p_G$ and $p_{{\\mathcal G}_C^k}$ by $q$. Finally, in some cases, as our experiments show, Byz-VR-MARINA-PP works even faster than Byz-VR-MARINA, implying that partial participation can accelerate the training for our method.\n\n>**As presented in Algorithm 1, in Byz-VR-MARINA-PP, the clients are sampled by the server before each round. Therefore, if a selected client becomes inactive, the whole training process will be blocked. In other words, Byz-VR-MARINA-PP cannot tolerate inactive clients.**\n\nThis is not a problem of our approach but rather an inevitable drawback of all *synchronized* Byzantine-robust methods: if implemented naively, even in the case of full participation, synchronized methods can be blocked by one Byzantine worker, who can just wait as much as possible without sending anything to the server. A simple way of handling this situation in practice is just to use some timeout for communication round. In this case, no peer can block the method. Therefore, if all sampled clients are inactive, then after a timeout, the server can set $g^{k+1} = g^k$ and run the next round as if the current round never happened.\n\n>**All clients will participate in the $k$-th training round if $c_k = 1$. That is to say, all clients will participate in the training per $1/p$ rounds in expectation. I understand that $p$ is typically small. However, it will also greatly limit the acceleration effect of PP since in federated learning (especially in cross-device federated learning), the fraction of selected clients in each round is usually small.**\n\nWe agree with the reviewer that the usage of the full participation with (even small) probability $p$ is something undesirable for the method with PP. The main reason why Byz-VR-MARINA-PP requires full participation during a small fraction of randomly chosen rounds is in the estimator construction: it is inspired by and based on the Geom-SARAH/PAGE estimator that also uses a full gradient computation with some probability to reset the variance-reduced estimator and control the variance. We apply a similar mechanism to control the variance coming from the sampling of clients. Avoiding full participation is an important direction for future research that we plan to work on. One possible way is to adopt the technique from the recent work [1], where the authors develop a method improving upon MARINA-PP. The key idea in their paper is to use the momentum-based variance reduction for controlling the variance coming from compression and partial participation. This helps to avoid full participation as well as uncompressed vector communication.\n\nWe also want to emphasize that the main novelty of this paper is in the usage of gradient clipping to handle the Byzantine workers in the case of partial participation. Clipping was used to construct robust aggregation by Karimireddy et al. (2021), but it was never used in the way we apply it. We believe our work is an important step towards building more efficient Byzantine-robust methods supporting partial participation. Moreover, in our experiments, we still observe the acceleration of the training due to partial participation.\n\n>**Given the reasons above, could the authors specify what benefits the partial participation mechanism can bring?**\n\nSumming up our responses above, we highlight the following benefits of Byz-VR-MARINA-PP: the ability to tolerate inactive clients when all regular workers can be inactive with equal probabilities and acceleration of the training."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593113203,
                "cdate": 1700593113203,
                "tmdate": 1700593113203,
                "mdate": 1700593113203,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fzk6Akdd2r",
                "forum": "qB0IV2DpeS",
                "replyto": "kNtMk1AmcW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mpAd [Part 2/2]"
                    },
                    "comment": {
                        "value": ">**The computation of full gradients is time-consuming. Moreover, it is unknown whether the gradient clipping is empirically compatible with the PAGE estimator. I strongly suggest the authors empirically test the performance of the proposed method on some federated learning benchmarks such as LEAF.**\n\nWe have conducted preliminary experiments with the proposed method. We kindly refer to our general response and the revised manuscript.\n\n---\nReferences:\n\n[1] Tyurin, A., & Richt\u00e1rik, P. (2022). A computation and communication efficient method for distributed nonconvex problems in the partial participation setting. arXiv preprint arXiv:2205.15580."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593139454,
                "cdate": 1700593139454,
                "tmdate": 1700593139454,
                "mdate": 1700593139454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vo9UQH3MyN",
                "forum": "qB0IV2DpeS",
                "replyto": "kNtMk1AmcW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2050/Reviewer_mpAd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2050/Reviewer_mpAd"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed response, which has addressed some of my concerns. However, there are still some remaining concerns, which I list below.\n\n1. About partial participation (PP). I thank the author for explaining the setting in their work, where all clients are available at any time. There are typically two settings in FL: cross-device FL and cross-silo FL (Kairouz et al., 2021). In cross-device FL, clients are very likely to be unavailable. In cross-silo FL, the participants are typically companies or organizations, and most participants attend each round (i.e., PP is not needed in this case) (Kairouz et al., 2021). Could the authors provide some real-world applications where all clients are available at any time and PP is required?\n\n2. About the experiment. I appreciate that the authors have added some numerical results. However, my concerns have not been fully addressed for the following reasons: (a) The task is quite easy. (b) The ability to handle heterogeneous cases is a main strength of the proposed method, but only homogeneous cases are considered in the experiment.\n\n3. About the acceleration. Could the authors further explain the benefit of acceleration of Byz-VR-MARINA-PP. Does it mean the same precision can be achieved with less computation cost or less wall-clock time? \n\n4. (minor) There is a typo on page 2: 'heterogenous' -> 'heterogeneous'."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647641965,
                "cdate": 1700647641965,
                "tmdate": 1700647641965,
                "mdate": 1700647641965,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dw6ENW7p0g",
            "forum": "qB0IV2DpeS",
            "replyto": "qB0IV2DpeS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2050/Reviewer_Axn5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2050/Reviewer_Axn5"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of partial participation in Byzantine robust algorithms for distributed learning. The authors introduce gradient clipping to limit the influence of Byzantine workers in rounds where they form a majority in the set of selected participants. They prove convergence rates, for a general algorithm featuring variance reduction and communication compression, and claim to match state-of-the-art theoretical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem of partial participation is not well understood in Byzantine robust machine learning, and this paper makes a promising step towards solving it by introducing gradient clipping. Also, the use of the latter method is novel in this context.\n\n2. The technical content and proofs are sound."
                },
                "weaknesses": {
                    "value": "My main concerns revolve around practicality, clarity, related work review, and assumptions.\n\n### A. Practicality: \n* A.1. A major weakness of the paper is the absence of experimental results. I would expect at least experiments on simple tasks, given that the only addition in the proposed algorithm (compared to previous works) is gradient clipping, which is simple to implement.\n\n* A.2. An important weakness in the theoretical analysis is the choice of the clipping parameter. For example, in Theorem 3.1, the clipping parameter $\\lambda_{k}$ depends on the maximum local smoothness constant, the computation of which can be highly impractical.\n\n* A.3. For the variance reduction method employed to have a gradient oracle cost comparable to SGD, $p$ needs to be in the order of $\\frac{1}{m}$ where $m$ is the number of samples per worker. However, my concern is that the excess (non-vanishing) term in (6) of the main theorem would increase proportionally to $m$, which is untight following the existing lower bounds, e.g. Karimireddy et al. (2022).\n\n### B. Clarity:\nThere are many clarity-affecting issues in the paper, which make the submission seem rushed:\n * Several quantities are undefined before they appear: $S_k$ and $g^k$ in the second paragraph of Section 2, $G^k_C$ in the first equation of Section 3\n* How is $g^k$ initialized in Algorithm 2? Does arbitrary initialization work in theory?\n* $n \\choose k$ is incorrectly denoted in the second paragraph of Section 3, and correctly denoted elsewhere.\n* The last sentence in Section 2 seems to be in conflict with Algorithm 1. In the latter, clipping is also performed at the worker level with probability $1-p$.\n\n### C. Related work:\n\nC.1. An important piece of related work is missing from the paper. Data & Diggavi (2021) have tackled the problem of partial participation (and local steps) in Byzantine robust distributed learning. It is essential to include a comparison with their work.\n\nReference: Deepesh Data and Suhas Diggavi. Byzantine-resilient high-dimensional SGD with local iterations on heterogeneous data. ICML 2021.\n\nC.2. Some claims regarding related work, in the paragraph following Definition 1, are inaccurate: a standard aggregator (coordinate-wise trimmed mean) satisfies Definition 1.1 because it satisfies an even stronger robustness criterion as shown by Allouah et al. (2023). Please include this in the paragraph. Moreover, using Bucketing (Karimireddy et al., 2021) is known to amplify the Byzantine fraction, and this may be problematic when considering partial participation.\n\n### D. Assumptions:\n\nSome assumptions are poorly justified: there is no justification for why \"popular [...] robust aggregation rules presented in the literature\" verify Assumption 1. A formal, even simple, justification is important because the assumption seems necessary for the convergence theory."
                },
                "questions": {
                    "value": "I am willing to raise my score if the authors address the weaknesses above. In particular:\n\n1. How do you set the clipping parameters in practice, when you cannot compute smoothness constants? (see A.2)\n\n2. How do your results compare to the work of Data & Diggavi (2021)? (see C.1)\n\n3. If we constrain the oracle cost to be of the same order as SGD, is the excess term in the convergence upper bound tight? (see A.3)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2050/Reviewer_Axn5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765085279,
            "cdate": 1698765085279,
            "tmdate": 1699636136637,
            "mdate": 1699636136637,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XVbMH1UnfX",
                "forum": "qB0IV2DpeS",
                "replyto": "dw6ENW7p0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Axn5 [Part 1/2]"
                    },
                    "comment": {
                        "value": ">**Numerical experiments.**\n\nFollowing the reviewers\u2019 requests, we have conducted preliminary experiments with the proposed method. We kindly refer to our general response and the revised manuscript.\n\n>**The choice of the clipping level.**\n\nWe agree with the reviewer that the smoothness constants can be hard to estimate, and our work leaves an important open problem of how to avoid such a dependence in theory for future research. However, as our experiments show, Byz-VR-MARINA-PP does not significantly depend on parameter $\\alpha$, if $\\lambda_{k+1} = \\alpha\\|\\| x^{k+1} - x^k \\|\\|$.\n\n>**Non-vanishing term in (6).**\n\nThis term is indeed not tight, as well as in the existing result for Byz-VR-MARINA. However, even the case of $\\zeta = 0$ is important for Byzantine-robust literature, especially for collaborative learning applications when the data is open. We also have more general results under Assumption 9 in Appendix B, allowing a special type of heterogeneity that leads to the convergence to any predefined accuracy after a sufficiently large number of steps.\n\nOne possible way of improving this term is to apply our clipping idea to the Byzantine-robust version of DASHA-PP-PAGE from [1]. This algorithmic change should lead to the improvement of the non-vanishing term in (6), which was observed in a very recent preprint [2] in the context of full participation.\n\nWe also want to emphasize that the main novelty of this paper is in the usage of gradient clipping to handle the Byzantine workers in the case of partial participation. Clipping was used to construct robust aggregation by Karimireddy et al. (2021), but it was never used in the way we apply it. We believe our work is an important step towards building more efficient Byzantine-robust methods supporting partial participation. Moreover, in our experiments, we still observe the acceleration of the training due to partial participation.\n\n>**Several quantities are undefined before they appear.**\n\nWe thank the reviewer for spotting these minor issues. We have fixed them in the revised version.\n\n>**Initialization of $g^k$.**\n\nFrom the theoretical analysis perspective, one can take any $g^0$ and just follow the pseudocode given in Algorithm 1. In practice, one can take $g^0 = 0$, $c_0 = 1$, and then follow Algorithm 1 from $k = 1$.\n\n>**Binomial coefficient.**\n\nWe thank the reviewer for spotting this minor issue. We have fixed it in the revised version.\n\n>**The last sentence in Section 2.**\n\nWe wanted to say that the clipping operation can be applied on the server side since the server needs to check that all norms of the received vectors are smaller than $\\lambda_{k+1}$. We have made this sentence clearer."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592891099,
                "cdate": 1700592891099,
                "tmdate": 1700593007419,
                "mdate": 1700593007419,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ASbIKJ4sSw",
                "forum": "qB0IV2DpeS",
                "replyto": "dw6ENW7p0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Axn5 [Part 2/2]"
                    },
                    "comment": {
                        "value": ">**Comparison to Data & Diggavi (2021)**\n\nWe thank the reviewer for providing the reference. We were not aware of this paper during the work on our submission. We have added a detailed comparison to the paper and also provided it below.\n\nThe main difference between Data & Diggavi (2021) and our work is that Data & Diggavi (2021) assume that the number of participating clients $C$ at each round is such that $B \\leq \\epsilon C$, where $B$ is the overall number of Byzantine workers and $\\epsilon \\leq \\frac{1}{3} - \\epsilon\u2019$ for some parameter $\\epsilon\u2019 > 0$ that will be explained later. That is, the results from Data & Diggavi (2021) do not hold when $C$ is smaller than $3B$, and, in particular, their algorithm cannot tolerate the situation when the server samples only Byzantine workers at some particular communication round. We also notice that when $C \\geq 4B$, then existing methods such as Byz-VR-MARINA or Client Momentum (Karimireddy et al., 2021, 2022) can be applied without any changes to get a provable convergence. In contrast, our method converges in more challenging scenarios, e.g., Byz-VR-MARINA-PP provably converges even when $C = 1$ and, in particular, the situation when the server samples only Byzantine workers at some rounds does not break the convergence of our method.\n\nNext, Data & Diggavi (2021) derive the upper bounds for the expected squared distance to the solution (in the strongly convex case) and the averaged expected squared norm of the gradient (in the non-convex case), where the expectation is taken w.r.t. the sampling of stochastic gradients only and the bounds itself hold with probability at least $1 - \\frac{K}{H}\\exp\\left( - \\frac{\\epsilon\u2019^2(1 - \\epsilon)C}{16}\\right)$, where $H$ is the number of local steps. For simplicity, consider the best-case scenario: $H = 1$. Then, the lower bound for this probability becomes negative when either $C$ is not large enough or when $K$ is large or when $\\epsilon$ is close to $\\frac{1}{3}$, e.g., for $K = 10^6, \\epsilon = \\epsilon\u2019 = \\frac{1}{6}, C = 5000$ the above lower bound is smaller than $-720$, meaning that in this case the result does not guarantee convergence. In contrast, our results have classical convergence criteria, where the expectations are taken w.r.t. the all randomness.\n\nFinally, the bounds from Data & Diggavi (2021) have non-reduceable terms even for homogeneous data case: these terms are proportional to $\\frac{\\sigma^2}{b}$, where $\\sigma^2$ is the upper bound for the variance of the stochastic estimator on regular clients and $b$ is the batchsize. In contrast, our results have only decreasing terms in the upper bounds when the data is homogeneous.\n\n>**Coordinate-wise trimmed mean satisfies Definition 1.1.**\n\nWe have added this remark to the paper.\n\n>**Bucketing (Karimireddy et al., 2021) is known to amplify the Byzantine fraction, and this may be problematic when considering partial participation.**\n\nIndeed, Bucketing amplifies the fraction of Byzantine workers, and the generalization of our results to different types of robust aggregation rules and formalisms (e.g., the ones presented by Allouah et al. (2023)) is a very promising research direction. However, we want to highlight here that our results for Byz-VR-MARINA-PP are valid for the same range of $\\delta$ (fraction of Byzantine workers) as the results for Byz-VR-MARINA (without partial participation). From this perspective, the usage of Bucketing is not problematic for our method.\n\n>**Justification of Assumption 1.**\n\nAggregation rules such as Krum and geometric median (GM) satisfy Assumption 1. Indeed, since Krum returns one of the input points, the inequality from Assumption 1 holds. Next, since the geometric median belongs to a convex hull of the inputs, Assumption 1 holds as well. Since Bucketing is applied to the averages computed for each bucket and the norm is a convex function, the composition of Bucketing with Krum/GM satisfies Assumption 1.\n\nHowever, coordinate-wise aggregators such as coordinate-wise median (CM) or coordinate-wise trimmed mean (CTM) do not necessarily meet Assumption 1 from the original version of our submission. To fix this issue, we modify Assumption 1 by introducing a multiplicative factor $F_{\\mathcal{A}}$ in the right-hand side of the inequality. For CM (and Bucketing $\\circ$ CM) and CTM $F_{\\mathcal{A}} = \\sqrt{d}$ and for Bucketing $\\circ$ Krum/GM $F_{\\mathcal{A}} = 1$. We have applied all the necessary changes to the paper.\n\n---\nReferences:\n\n[1] Tyurin, A., & Richt\u00e1rik, P. (2022). A computation and communication efficient method for distributed nonconvex problems in the partial participation setting. arXiv preprint arXiv:2205.15580.\n\n[2] Rammal, A., Gruntkowska, K., Fedin, N., Gorbunov, E., & Richt\u00e1rik, P. (2023). Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates. arXiv preprint arXiv:2310.09804."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592934737,
                "cdate": 1700592934737,
                "tmdate": 1700593030048,
                "mdate": 1700593030048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "syo0R3lmcN",
                "forum": "qB0IV2DpeS",
                "replyto": "ASbIKJ4sSw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2050/Reviewer_Axn5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2050/Reviewer_Axn5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and modifications. It is good that the change of Assumption 1 did not break the main results.\n\nMy main concern (regarding the lack of experiments) has not been adequately addressed. I think that the experiments are insufficient. First, the claim of the robustness of the clipping parameter tuning is not adequately backed; the task is too easy, and I would expect an additional task (at least MNIST). Also, the authors consider only one attack currently, while there are many others (see Karimireddy et al. 2022, Allouah et al. 2023 and other works) that could potentially damage clipping more than methods not using clipping. Finally, experimenting with data heterogeneity (which is supported theoretically) is rather important, even if the authors meant to show the importance of clipping in the easiest case, as clipping may be worsening the effects of heterogeneity.\n\nAlso, after looking at the other reviews, I became concerned with the fact that full participation is needed for the algorithm with some probability. It seems that claiming partial participation can be misleading here, since it usually refers to partial participation in each round, like FedAvg. Is it possible to remove this full participation requirement?\n\nI was also confused by the acceleration observation (middle of Figure 1). Was this theoretically claimed or observed by the authors? If not, how do you explain it?\n\nRegarding your response on the non-vanishing term in (6): if the bound is untight when variance reduction is reasonably costly ($p \\sim \\tfrac{1}{m}$), is it reasonable to qualify a similar method's convergence result by \"SOTA\" (last sentence of Section 1.1)?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641370146,
                "cdate": 1700641370146,
                "tmdate": 1700641370146,
                "mdate": 1700641370146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S5f2kl6ftn",
                "forum": "qB0IV2DpeS",
                "replyto": "dw6ENW7p0g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the comment by Reviewer Axn5"
                    },
                    "comment": {
                        "value": "We thank Reviewer Axn5 for engaging in the discussion with us.\n\n## Full participation at some rounds\n\nThe need for full participation with some (small) probability is indeed a limitation of our method, **though this limitation is not very strong (it is not a limitation in the setup with client sampling) and is needed mostly to make the analysis easier**. The main reason why Byz-VR-MARINA-PP requires full participation during a small fraction of randomly chosen rounds is in the estimator construction: it is inspired by and based on the Geom-SARAH/PAGE estimator that also uses a full gradient computation with some probability to reset the variance-reduced estimator and control the variance. We apply a similar mechanism to control the variance coming from the sampling of clients. Avoiding full participation is an important direction for future research that we plan to work on. One possible way is to adopt the technique from the recent work [1], where the authors develop a method improving upon MARINA-PP. The key idea in their paper is to use the momentum-based variance reduction for controlling the variance coming from compression and partial participation. This helps to avoid full participation as well as uncompressed vector communication.\n\nWe also want to emphasize that the main novelty of this paper is in the usage of gradient clipping to handle the Byzantine workers in the case of partial participation. Clipping was used to construct robust aggregation by Karimireddy et al. (2021), but it was never used in the way we apply it. Moreover, a noticeable part of the literature on partial participation of clients considers the client sampling model. The majority of samplings considered in the literature including the only existing work on Byzantine-robustness and partial participation (e.g., see the references on page 3) are sampling from the whole set of clients such that the probability of sampling each client is strictly positive. **This means that all of these works implicitly require the availability of all clients at any time.** We agree that this assumption does not always hold in practice, but we point out that this assumption is often used in papers focusing on theory. **We believe our work is an important step towards building more efficient Byzantine-robust methods supporting partial participation.** Our work opens a prominent direction for future research: one can apply clipping to the methods from [1] and to ClientMomentum by Karimireddy et al. (2021) to achieve Byzantine-robustness with partial participation.\n\nMoreover, in our experiments, despite all the drawbacks and limitations of the proposed method, we observe a noticeable acceleration of the training due to partial participation for the proposed method.\n\n## Numerical experiments\n\nWe plan to add additional evaluation on different tasks but given the time constraints, we managed to finish the experiments with logistic regression only. The main contributions of our paper are purely theoretical and algorithmic. In particular, we obtained the first rigorous results with provable convergence even when Byzantines form majority at some rounds and we the key idea that we proposed and that was never considered before is to use clipping in the context of Byzantine-robust learning with partial participation.\n\nWe are currently running extra experiments with different attacks to illustrate that Byz-VR-MARINA-PP efficiently tolerates all kinds of Byzantine attacks. We also want to emphasize that **other methods cannot withstand the considered attack** since when Byzantines form the majority, they can arbitrarily damage the convergence.\n\n## Acceleration observation (Figure 1)\n\nThe current analysis does not provide benefits of partial participation -- this is a very standard situation for the analysis of methods with client sampling in the worst case. However, when workers have similar or even identical data, the optimal number of participating clients can be smaller since the estimator becomes quite tight and increasing the batchsize does not lead to noticeable improvements.\n\n## Heterogeneous settings\n\nThe case of heterogeneous data is very important, but we want to emphasize that under heterogeneity, any predefined accuracy of the solution cannot be achieved due to the lower bound from (Karimireddy et al., 2022).  Due to these reasons, homogeneous setup is still quite popular in Byzantine-robust learning.\n\n## SOTA results\n\nWhen data is homogeneous, the results are SOTA. The neighborhood factor can be improved when the data is heterogeneous -- and we explained how in our rebuttal. We will modify this claim to make it more accurate.\n\n---\nReferences:\n\n[1] Tyurin, A., & Richt\u00e1rik, P. (2022). A computation and communication efficient method for distributed nonconvex problems in the partial participation setting. arXiv preprint arXiv:2205.15580."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719008002,
                "cdate": 1700719008002,
                "tmdate": 1700720190373,
                "mdate": 1700720190373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o3duRvDpmV",
            "forum": "qB0IV2DpeS",
            "replyto": "qB0IV2DpeS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2050/Reviewer_cyK5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2050/Reviewer_cyK5"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a robust distributed algorithm against Byzantine attacks that allows partial client participation. While previously proposed methods require the participation of all clients to compute the aggregation rule and have a convergence guarantee, the proposed algorithm allows partial participation using gradient clipping and therefore limits the impact of the Byzantine clients, even if they form a majority in the set of subsampled clients at a given round. The authors provide a convergence guarantee for the proposed algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clear and easy to follow.\n- To the best of my knowledge, this is the first paper to allow partial participation for a robust distributed algorithm against Byzantine attacks."
                },
                "weaknesses": {
                    "value": "- Given that the main motivation for this paper is to allow partial participation because it is more natural in practice, as the authors point out, I would have expected to see some practical experiments to see how gradient clipping actually allow partial participation (and thus the sampling of a majority of Byzantine clients in some rounds) while maintaining good performance. It seems to me that even if clipping can control the impact of Byzantine clients, rounds where they are in the majority will still penalize learning. Do the authors have any insights or perhaps experiment results on the performance of the proposed algorithm?"
                },
                "questions": {
                    "value": "In the algorithm, it is said that the clipping levels $\\lambda_k$ are given as inputs, how is it possible since they depend on the value of $x^{k+1}$ and $x^k$ ?\n\nHow are the gradients clipped at the first iteration since $\\lambda_0$ is not defined?\n\nCan the authors explain why full participation Is needed in some rounds? Would it be possible to avoid full participation and use only partial participation in each round?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2050/Reviewer_cyK5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797365506,
            "cdate": 1698797365506,
            "tmdate": 1699636136518,
            "mdate": 1699636136518,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kHNQsS4vEt",
                "forum": "qB0IV2DpeS",
                "replyto": "o3duRvDpmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cyK5"
                    },
                    "comment": {
                        "value": ">**Numerical experiments.**\n\nFollowing the reviewers\u2019 requests, we have conducted preliminary experiments with the proposed method. We kindly refer to our general response and the revised manuscript.\n\n>**In the algorithm, it is said that the clipping levels $\\lambda_k$ are given as inputs, how is it possible since they depend on the value of $x^{k+1}$ and $x^k$?**\n\nWe thank the reviewer for the comment. To avoid any confusion, we reformulated the algorithm\u2019s description: we need coefficients $\\alpha_k$ as inputs and define $\\lambda_{k+1} = \\alpha_{k+1} \\|\\| x^{k+1} - x^k \\|\\|$. In Theorem 3.1, we use $\\alpha_k = 2\\max_{i \\in \\mathcal{G}} L_i$ and, in Theorem 3.2, $h_k$ is chosen as $\\alpha_k = D_Q \\max_{i,j} L_{i,j}$. Although theoretical values of coefficients $\\alpha_k$ depend on the smoothness constants, the algorithm\u2019s behavior is quite robust to the choice of $\\alpha_k$ as our experiments show.\n\n>**How are the gradients clipped at the first iteration since $\\lambda_0$ is not defined?**\n\nWe notice that during the first step, the sampled workers first do a step and get $x^{1}$ (line 7), and only then apply clipping (if $c_k = 0$) in line 8 using $\\lambda_1 \\sim \\|\\| x^1 - x^0 \\|\\|$. From the theoretical analysis perspective, one can take any $g^0$ and just follow the pseudocode given in Algorithm 1. In practice, one can take $g^0 = 0$, $c_0 = 1$, and then follow Algorithm 1 from $k = 1$.\n\n>**Can the authors explain why full participation Is needed in some rounds? Would it be possible to avoid full participation and use only partial participation in each round?**\n\nWe thank the reviewer for an excellent question. The main reason why Byz-VR-MARINA-PP requires full participation during a small fraction of randomly chosen rounds is in the estimator construction: it is inspired by and based on the Geom-SARAH/PAGE estimator that also uses a full gradient computation with some probability to reset the variance-reduced estimator and control the variance. We apply a similar mechanism to control the variance coming from the sampling of clients. Avoiding full participation is an important direction for future research that we plan to work on. One possible way is to adopt the technique from the recent work [1], where the authors develop a method improving upon MARINA-PP. The key idea in their paper is to use the momentum-based variance reduction for controlling the variance coming from compression and partial participation. This helps to avoid full participation as well as uncompressed vector communication.\n\nWe also want to emphasize that the main novelty of this paper is in the usage of gradient clipping to handle the Byzantine workers in the case of partial participation. Clipping was used to construct robust aggregation by Karimireddy et al. (2021), but it was never used in the way we apply it. We believe our work is an important step towards building more efficient Byzantine-robust methods supporting partial participation. Moreover, in our experiments, we still observe the acceleration of the training due to partial participation.\n\n---\n\nReferences:\n\n[1] Tyurin, A., & Richt\u00e1rik, P. (2022). A computation and communication efficient method for distributed nonconvex problems in the partial participation setting. arXiv preprint arXiv:2205.15580."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592652717,
                "cdate": 1700592652717,
                "tmdate": 1700592671236,
                "mdate": 1700592671236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]