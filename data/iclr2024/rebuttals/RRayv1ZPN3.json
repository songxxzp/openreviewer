[
    {
        "title": "TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models"
    },
    {
        "review": {
            "id": "bfwqtu8g30",
            "forum": "RRayv1ZPN3",
            "replyto": "RRayv1ZPN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_Lua3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_Lua3"
            ],
            "content": {
                "summary": {
                    "value": "The work presents Task-specific Adapters for Imitation Learning (TAIL), a framework for fine-tuning control policies in continual learning setting by borrowing ideas from large language model space. The agent is trained on multiple tasks, one task at the time, and the objective is to learn the new tasks while retaining the performance in old tasks. TAIL algorithms update only a small set of parameters, inspired by parameter efficient training from language model space. The results show that TAIL combined with LoRA outperforms all tested alternatives, especially full finetuning (tuning the whole network) or other methods that fine-tune the whole network."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall nicely wrapped work. While not the most novel solution (see weaknesses), I feel the amount of results and experiments conducted here merit acceptance. I believe these results will be useful for many readers, and the instructions on how to fine-tune these models will support future work.\n\n### Originality\n\nAuthors bring insights from language space to control space in the form of efficient finetuning algorithms, and test those algorithms in a new setting (continual learning).\n\n### Quality\n\nExperiments are throughout with enough ablations and baselines. The methods are evaluated from different angles (performance increase in new task, retainibility in previous tasks, number of parameters / computational requirements.)\n\nAccurate description of the experiment setup provided in the appendix (e.g., hardware, driver versions).\n\n### Clarity\n\nThe manuscript is written well and is clear to read.\n\n### Significance\n\nThe insights are also helpful for simple fine-tuning of models for different tasks: results show that LoRA is also very fast to adapt in general (better than full finetuning).\n\nThe manuscript also provides a list of ablations on what settings are recommended for finetuning, which are useful for people to follow.\n\nGiven the popularity of transformer models and their adaptation in different domains, I see this work being useful for many readers. Focus on experiments that can be ran on lesser hardware (e.g., single GPU, less space) also allows more people to experiment with these features."
                },
                "weaknesses": {
                    "value": "- Using a separate set of weights (\"adapters\") per task feels bit of cheating when comparing the model against the baselines. This alone ensures that you keep high performance in the previous tasks. While the argument for this is valid (with TAIL, you only need a handful of parameters vs. full network), I'd prefer a more apples-to-apples comparison.\n- Limited to only one environment. While I understand data for right setup is hard to come by, it is hard to tell if the results generalize outside this tested environment and data. Showing results in an another type of environment would solidify these insights.\n- (Minor) Proposed framework, despite having its own name, is not especially novel or specific: it is an umbrella term for existing solutions applied on the continual learning setting. This is evident from figures, where the same method (TAIL) appears multiple times, and fundamentally, all these variants are significantly different from each other. I'd perhaps call different instiations of this setup \"TAIL-LoRA\", instead of \"TAIL (LoRA)\", to signify it is LoRA applied in TAIL's fashion."
                },
                "questions": {
                    "value": "1) Page 7, \"Training, Adaptation and Evaluation\" paragraph: Manuscript says \"This limited data setup...\". How do you consider this a \"limited data\" setup? Can you give context/examples what would not be \"data limited\" setup?\n\n2) Page 7, last paragraph. The description of what parts were finetuned/update/adapted and when is somewhat unclear and involved. How did you come up with this setup? Could you clarify this paragraph or provide a figure to help with understand what was updated in which stage?\n\n## Comments\n- Define acornyms only once and consistently use the acornym ever since (e.g., TAIL is defined multiple times).\n\n\n## Rebuttal update 20th Nov\n\nI have read authors' answers to my questions, and I have kept my original review score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Reviewer_Lua3",
                        "ICLR.cc/2024/Conference/Submission4419/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698079040533,
            "cdate": 1698079040533,
            "tmdate": 1700484599831,
            "mdate": 1700484599831,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wkrkCXYB0n",
                "forum": "RRayv1ZPN3",
                "replyto": "bfwqtu8g30",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Lua3"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and positive review! We are glad you find the paper well-written and the experiments and results comprehensive. We respond to individual concerns and questions below. We also included new experiments in the revised version of the paper to provide an even more comprehensive view of our paper.\n\n> **Only one environment?**\n\nWe have added **new experiments** in Franka-Kitchen, using the same pre-trained model and following the experiment setup presented in the RoboAdapter and R3M paper. The detailed results are shown in Appendix C.7, where TAIL-LoRA performs the best, achieving an 80% success rate while RoboAdapter achieves 65%. The results indicate that our findings and claims from the extensive experiments on the large-scale LIBERO datasets can be generalized well on other small-scale domains, such as Franka-Kitchen.\n\nWe would also like to emphasize that the LIBERO dataset is a large-scale, comprehensive benchmark for evaluating continual imitation learning. It comprises over 100 different tasks in various environments, including the Kitchen, Living Room, and Study Room. This dataset aims to evaluate the diverse aspects of continual learning algorithms, including spatial relationship reasoning, object recognition, and understanding of task goals. Therefore, the main results and findings of our work should be generalizable to similar robotic task setups.\n\n> **Per-task weights for TAIL is unfair comparison?**\n\nWe clarify that we compare against Frozen Pre-trained Features (FPF) which also learns a new set of per-task weights. TAIL greatly outperforms this baseline in all adaptation environments (Fig 5), indicating that TAIL\u2019s adapter integration is important for forward transfer even against baselines that learn per-task suite new weights.\n\nTo further illustrate this, we also add a **new experiment** comparing against PackNet [1], which prunes weights before re-training the pruned params (as new params) for new task suites. Results demonstrate that TAIL outperforms this baseline by up to **3x** in specific task suites. See the updated Fig 5 for further details.\n\n[1]: PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning. Arun Mallya and Svetlana Lazebnik.\n\n> **\u201cHow do you consider this a \u2018limited data\u2019 setup? Can you give context/examples what would not be \u2018data limited\u2019 setup?\u201d**\n\nThanks for flagging this. When we talk about \"limited data,\" we mean that only a few demonstrations are available for each task. This is a realistic assumption, considering the expense involved in collecting demonstrations. We have reworded Section 5.2 to make this clearer.\n\n> **Page 7, last paragraph: Clarify description of what parts were finetuned/update/adapted.**\n\nThank you for the suggestion and we apologize for the confusion. Since we only have pretrained CLIP weights for the vision and text encoders, all other parameters (including the GPT-2 model) must be trained in the pretraining stage. After that, during the adaptation stage for the subsequent task suites, we add adapters for the CLIP vision and text encoders and the temporal GPT2 decoder while keeping their transformer backbone parameters frozen. Since the fusion module and policy head are lightweight, we also tune them fully for each adaptation stage and store their parameters together with corresponding adapters per task suite.\n\nIn short, the CLIP spatial and instruction encoders are frozen across all stages. The temporal GPT2 decoder weights are trained in the pretraining stage (Kitchen) but frozen in the adaptation stages. The fusion module and the policy head are fully tuned with adapters per task suite. We have revised the last paragraph of Section 5.2 to address the confusion.\n\n> **Minor comments: change names, acronyms**\n\nThank you for these suggestions. We will address this in the final revision.\n\nPlease let us know if you have any more questions!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272677261,
                "cdate": 1700272677261,
                "tmdate": 1700272677261,
                "mdate": 1700272677261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SZDWXSZviO",
                "forum": "RRayv1ZPN3",
                "replyto": "wkrkCXYB0n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_Lua3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_Lua3"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors. Thank you for your extensive replies and additional results! These are very insightful, and your answers satisfy my questions. I have decided to keep the score at original 8, as the next score is 10. While good paper, and I reckon people working on same benchmarks will find many details here useful, I do not feel it is \"highlight\" quality for ICLR, given the somewhat limited novelty of the method."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484727245,
                "cdate": 1700484727245,
                "tmdate": 1700484727245,
                "mdate": 1700484727245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xkyoXQVOea",
            "forum": "RRayv1ZPN3",
            "replyto": "RRayv1ZPN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_JmE1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_JmE1"
            ],
            "content": {
                "summary": {
                    "value": "The authors applied parameter-efficient fine-tuning methods, including bottleneck adapters, P-tuning, and low-rank adaptation, to the continual imitation learning task. TAIL is designed to adjust large pre-trained models for new tasks even with limited demonstration data, with GPT2 as the backbone and CLIP-based modules as encoders. It avoids issues like catastrophic forgetting by activating the corresponding adapter for that task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper rigorously tests TAIL with prevalent PEFT techniques on the imitating continual imitation learning task.\n\n* TAIL\u2019s ability to achieve superior post-adaptation performance using only 1% of the trainable parameters of full fine-tuning highlights the framework\u2019s efficiency, making it a potentially valuable tool for resource-constrained settings."
                },
                "weaknesses": {
                    "value": "* The framework's capability to circumvent catastrophic forgetting is primarily attributed to its unique configuration that allows for the selection of adapters tailored to individual tasks. Its efficiency largely draws from existing PEFT methodologies, which limits the novelty and distinctiveness of the presented framework.\n\n* Drawing a comparison between TAIL and the fine-tuning baseline methods cited in the paper is unfair. This is because TAIL is designed to learn dedicated adapters for each distinct task. Thus, the claims regarding its efficacy become less compelling."
                },
                "questions": {
                    "value": "* Could you show the results when setting the rank for LORA to 32, 256, 512, and 768 (the dimensionality of the embeddings and hidden states of GPT2)? The ablation studies regarding the number of parameters should be added.\n\n* Could you provide some insights on whether combining the Lora and P-tuning can improve the performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Reviewer_JmE1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698212938104,
            "cdate": 1698212938104,
            "tmdate": 1700635408127,
            "mdate": 1700635408127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q7qxVNFyT5",
                "forum": "RRayv1ZPN3",
                "replyto": "xkyoXQVOea",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JmE1"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments! We appreciate that you find the paper performs rigorous tests and our TAIL framework could be a valuable tool for resource-constrained settings. We have addressed all your comments below. We hope you consider increasing your score after seeing our detailed responses.\n\n> **Novelty: largely tests existing PEFT methodologies**\n\nOur focus is not on proposing new adapter strategies but rather on proposing a general framework (TAIL) for efficient adaptation in a continual imitation learning setting and testing various adaptation strategies for this setting. No prior work considers this adaptation setting with large pre-trained decision-making models, and prior work like RoboAdapter [1] only considers a single adapter strategy.\n\nOur study contrasts multiple adapter strategies in this setting, including the RoboAdapter strategy, and we find great performance differences among the techniques (Fig 4), with RoboAdapter performing on average the worst among all techniques in LIBERO. Our new experiments demonstrate that RoboAdapter performs worse even in Franka Kitchen, a domain from the paper.\n\nPlease see the dedicated \u201cTo summarize our contributions\u2026\u201d paragraph in the Response to all reviewers for further details about our contributions.\n\n[1]: Lossless Adaptation of Pretrained Vision Models for Robotic Manipulation (RoboAdapter). Sharma et al, 2023.\n\n> **Comparison between TAIL and fine-tuning baselines unfair b/c TAIL learns new adapters?**\n\nWe respectfully disagree that the comparison is unfair. FPF (frozen pre-trained features), like TAIL, also learns dedicated additional parameters per task (in the output head) yet performs much worse. As another comparison, the Experience Replay (ER) method maintains a data buffer for **all previous data**, which takes much more storage space than TAIL\u2019s adapter weights. Moreover, except FPF, all other baselines (FFT, EWC, ER) are standard techniques for continual learning that tune the full parameters for each new task. In contrast, TAIL only tunes a small portion, yielding better forward transfer performance than them. Hence, TAIL doesn\u2019t have an unfair advantage in our continual learning setting.\n\nTo further illustrate this, we have also added a **new baseline**, PackNet [1], which prunes the network and then re-trains pruned params for the new task. Results demonstrate that TAIL is superior in adaptation performance for almost all task suites, up to a factor of ****3x**.** Please see the updated Fig. 5 for more details.\n\n[1]: PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning. Arun Mallya and Svetlana Lazebnik.\n\n> **Can you show LoRA dimensionality ablations?**\n\nWe agree that this is an important ablation. Thus, we have performed **new experiments** ablating LoRA ranks from 2 to 768, demonstrating success rates increase to a rank of 32 and then slightly decrease as rank increases to 768. This validates our hypothesis that fine-tuning *more* parameters on limited data can lead to overfitting. These experiments and discussions have been added to Appendix C.4 and Fig. 9.\n\n> **Can you combine LoRA and P-tuning for better performance?**\n\nThank you for this excellent suggestion! We\u2019ve now included a **new experiment** in which we test all combinations of LoRA, bottleneck layers, and p-tuning. The results demonstrate that all combinations that include LoRA perform similarly, while all combinations without LoRA perform far worse, indicating the necessity of LoRA. See Appendix C.6 for more info.\n\nPlease let us know if you have any more questions!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272593364,
                "cdate": 1700272593364,
                "tmdate": 1700272593364,
                "mdate": 1700272593364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aAdCT2nFIU",
                "forum": "RRayv1ZPN3",
                "replyto": "xkyoXQVOea",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_JmE1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_JmE1"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comprehensive additional experimental results, which have effectively addressed my previous concerns about the ablations. Hence, I've decided to raise my scores. However, I still believe that the comparison is not entirely fair for a couple of reasons. 1) Even though ER uses all the previous data, the lack of a MoE setting may also lead to worse performance. 2) The different number of trainable parameters remains a concern. Additionally, the ablation studies related to LoRA's rank suggest that increased parameters do not necessarily equate to improved performance."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635277447,
                "cdate": 1700635277447,
                "tmdate": 1700635328561,
                "mdate": 1700635328561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T1r2sloHcQ",
            "forum": "RRayv1ZPN3",
            "replyto": "RRayv1ZPN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_FCxQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_FCxQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the adaptation problem in continual imitation learning and proposes task-specific adapters when fine-tuning the imitation policy to new tasks. Specifically, the task-specific adapter is an add-on to the imitation policy and can be updated for each task without changing the pretrained weights in the backbone policy. As shown by the paper, this adapter can be implemented by three ways: parallel, i.e., Low-Rank Adaptation (LoRA) in this paper, sequential, i.e., Bottleneck Adapter, & prefix token, i.e., prompt-tuning. The paper evaluates the proposed adapter in LIBERO robotic manipulation continual learning benchmark and shows that LoRA adapter performs the best across all tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Clarity \nThe structure and presentation of the paper is of very good quality. The flow of the paper is excellent with a good and appropriate structure. The underlying motivation of studying the task-specific adapters is well justified."
                },
                "weaknesses": {
                    "value": "### Quality & Contribution\nOne of the main weaknesses of this paper is the lack of comprehensive empirical analysis over multiple different benchmarks, especially those used by previous baseline methods. Particularly, **the conclusions and empirical insights were drawn only from a single dataset, with no results from any other public datasets**. As the main contribution, this paper shows that the LoRA-type integration of the adapter in continual imitation learning performs the best. The paper thus concludes that they \u201care contrary to many results from the vision and language model literature which show that full fine-tuning works better\u201d. However, these results are drawn based on LIBERO robotic manipulation continual learning benchmark ONLY. They have not been verified by any other datasets. It is thus unclear if and how the results and conclusions can be generalized to other robot control tasks, e.g., Franka-Kitchen. \n\nFurthermore, it is unclear if the baseline methods have been well tuned for the LIBERO benchmark. For example, the RoboAdapter method may need to tune the adapter locations and different pre-trained representations for its optimal performance. The paper, however, seems to take the default configs, even though those configs were originally proposed for other robot tasks. To have a fair comparison with RoboAdapter, the paper should consider some of datasets used by RoboAdapter paper, i.e., Metaworld, Franka-Kitchen, and RGB-Stacking task suites, replicate the reported performance of RoboAdapter and then demonstrates on at least one of those datasets that the TAIL performs better than RoboAdapter. \n\n### Originality & Significance\nThe originality of this paper can be limited and incremental. \n\n**In terms of technique**, the adapter idea has been well studied by the RoboAdapter paper, which introduces the adapter layer (Sequential Integration in this paper) to imitation learning and considers its application in robot controls. Though the paper proposes two other adapter mechanisms, LoRA & Prefix prompt tuning, both of them are taken directly from other papers, without any substantial modifications or creative combinations to the continual imitation learning or robot control tasks. \n\n**In terms of the empirical results**, they may not offer any in-depth understanding of using adapters in continual imitation learning and the significance can be limited. The paper mentions that TAIL with LoRA \u201cavoiding catastrophic forgetting and preserving adaptation plasticity\u201d. Avoiding catastrophic forgetting can be evident since the adapter weights are task-specific and not shared across different tasks. But the empirical results shed little light on the adaptation plasticity. It is unclear if and how the adaptation plasticity is preserved given that the empirical results focus only on the success rate, as in Figure 4."
                },
                "questions": {
                    "value": "1. In appendix, \u201call methods share similar amount of parameters\u201d. I\u2019m not quite sure how to interpret this. Why all methods need to have a similar number of parameters? Even though the adapter integration mechanisms are different? Could this constraint on the number of parameters be biased towards to LoRA adapter? (since \u201cfiltering often requires a larger bottleneck size compared to that of LoRA, leading to more parameters\u201d)\n\n2. In the experiment Table 1, \u201cThe BWT for TAIL methods are all 0 (no catastrophic forgetting)\u201d. Is it self-evident? since the TAIL methods produce an ensemble of models (fixed pretrained weights + task-specific adapter weights), each of which aims to solve a dedicated task and is updated independently to each other. Then why do we need to use BWT as a performance indicator. \n\n3. The colours for Prefix Token & Frozen cannot be easily distinguished in Figure 2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698533762136,
            "cdate": 1698533762136,
            "tmdate": 1699636416006,
            "mdate": 1699636416006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z05fkiPimU",
                "forum": "RRayv1ZPN3",
                "replyto": "T1r2sloHcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FCxQ (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review. We are happy you found the presentation very good and our motivation excellent. We have addressed all your comments below. We hope you consider increasing your score after seeing our detailed responses and new experiments.\n\n> **Draw conclusions from more environments, e.g, Franka Kitchen from RoboAdapter?**\n\nThank you for the suggestion; we have now added **new experiments** in Franka-Kitchen, using the same pre-trained model and following the experiment setup presented in the RoboAdapter and R3M [1] paper. The detailed results are shown in Appendix C.7, where our TAIL-LoRA performs the best, achieving an 80% success rate while RoboAdapter achieves 65%. The results indicate that our findings and claims from the extensive experiments on the large-scale LIBERO datasets can be generalized well on other small-scale domains, such as Franka-Kitchen.\n\nWe would also like to emphasize that the LIBERO dataset is a large-scale, comprehensive benchmark for evaluating continual imitation learning. It comprises over 100 different tasks in various environments, including the Kitchen, Living Room, and Study Room. This dataset aims to evaluate the diverse aspects of continual learning algorithms, including spatial relationship reasoning, object recognition, and understanding of task goals. Therefore, the main results and findings of our work should be generalizable to similar robotic task setups.\n\nWe hope this addresses your quality & contribution concerns.\n\n[1] Nair, Suraj, et al. \"R3m: A universal visual representation for robot manipulation.\" *arXiv preprint arXiv:2203.12601* (2022).\n\n> **Originality: adapters studied in other works?**\n\nOur focus is not on proposing new adapter strategies but on proposing a general framework (TAIL) for efficient adaptation in a continual imitation learning setting and testing various adaptation strategies for this setting. No prior work considers this adaptation setting with large pre-trained decision-making models, and prior work like RoboAdapter [1] only considers a *single* adapter strategy.\n\nOur study contrasts multiple adapter strategies in this setting, including the RoboAdapter strategy, and we find great performance differences among the techniques (Fig 4), with RoboAdapter performing on average the worst among all techniques in LIBERO. Our new experiments demonstrate that RoboAdapter performs worse even in Franka Kitchen, a domain from the paper.\n\nPlease see the dedicated \u201cTo summarize our contributions\u2026\u201d paragraph in the Response to all reviewers for further details about our contributions.\n\n[1]: Lossless Adaptation of Pretrained Vision Models for Robotic Manipulation (RoboAdapter). Sharma et al, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272517373,
                "cdate": 1700272517373,
                "tmdate": 1700272517373,
                "mdate": 1700272517373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BPvh8BDNPD",
                "forum": "RRayv1ZPN3",
                "replyto": "T1r2sloHcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FCxQ (Part 2/2)"
                    },
                    "comment": {
                        "value": "> **Significance: results don\u2019t focus on adaptation plasticity?**\n\nWe clarify that we **do** have adaptation plasticity experiments in Table 2 in the main paper and Appendix Table 6. Upon re-visiting previously seen task suites, full fine-tuning suffers large drops in performance despite re-training on the same data (Table 2)\n\nMeanwhile, because TAIL allows us to simply store **~7.8MB** of weights per task suite for later reuse, we can reuse the weights when returning to previously seen tasks. As a comparison, storing the entire Kitchen datasets (for the ER method) and the entire model per task suite takes **28GB* and *660MB*, respectively. Furthermore, adding TAIL w/ LoRA **after full fine-tuning** is worse than LoRA without full fine-tuning on all tasks (Table 6), indicating that full fine-tuning leads to adaptation plasticity loss.\n\nWe have clarified in Section 5.3 and Appendix C of the revised paper that these experiments are studying **adaptation plasticity** specifically. We hope we have addressed both of your main concerns about originality & significance!\n\n> **Why do all methods share similar # params? Does this favor LoRA?**\n\nWe kept the parameter count the same to study how effective adaptation techniques were under the same memory constraint. However, per your request, we have performed a **new experiment** ablating the bottleneck layer size for the sequential Bottleneck Adapter method, demonstrating that LoRA still outperforms in rank scaling \u2014 achieving nearly double the performance at identical rank in some instances. Notably, the sequential Bottleneck Adapters with full rank size, i.e., 768, still under-perform LoRA with rank size 8, showing advantages of the parallel integration style in the continual learning setup. We have added this to Appendix C.4 of the revised paper.\n\n> **Why study Backwards Transfer (BWT) in TAIL methods? Isn\u2019t it self-evident?**\n\nBWT is a relevant and important metric for all non-TAIL methods we compare; it helps analyze catastrophic forgetting among our baselines. In our setting, BWT transfer is poor even among methods made explicitly to resolve this issue (Experience Replay, Elastic Weight Consolidation), indicating the need for TAIL (see Table 1).\n\n> **Fig 2: colours hard to distinguish**\n\nThank you, we have updated the colors.\n\nPlease let us know if you have any more questions!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272548887,
                "cdate": 1700272548887,
                "tmdate": 1700272548887,
                "mdate": 1700272548887,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ti9fVr2nnf",
                "forum": "RRayv1ZPN3",
                "replyto": "BPvh8BDNPD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_FCxQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_FCxQ"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal feedback"
                    },
                    "comment": {
                        "value": "Thank you for the response. Since this paper focuses on the empirical analysis of the task-specific adapters in continual learning, a bit more comprehensive analysis on many different benchmarks would be expected (which would strengthen the conclusion as well). Also, a detailed description of the baseline setups may be required. I appreciate the value of this work and also the authors' extra efforts in providing the empirical results in FrankaKitchen. But I think the work in its current version may be below the acceptance threshold."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607347083,
                "cdate": 1700607347083,
                "tmdate": 1700607347083,
                "mdate": 1700607347083,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "A8ABrL7GFB",
            "forum": "RRayv1ZPN3",
            "replyto": "RRayv1ZPN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_86yf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_86yf"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the continual imitation learning setting where a stream of tasks arrive one at a time. Existing methods are susceptible to catastrophic forgetting or loss of model plasticity as the number of tasks increase. Consequently, the paper proposes task-specific adapters for imitation learning (TAIL), a method that uses additional per-task tuneable parameters and synthesizes them to the pretrained base model's parameters. The paper demonstrates through experiments that TAIL introduces little computational overhead and mitigates catastrophic forgetting and model plasticity problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is easy to follow generally.\n- The method is very intuitive and simple---the fact that the overhead is low for a new task while mitigating catastrophic forgetting (e.g. better than experience replay) is appealing.\n- The empirical analysis demonstrates that using TAIL with low-rank adaptation (LoRA) perform better than existing baselines convincingly in LIBERO, interestingly that it performs that well with rank $r = 8$."
                },
                "weaknesses": {
                    "value": "**Comments**\n- It appears that the number of tasks can easily explode over time since each task instruction (and initial-state distribution) corresponds to a new task. A natural extension will be to mitigate the amount of adapters based on the similarity of task instructions, as hinted in appendix.\n- Since the problem setting indicates that a task definition is not w.r.t. the state-action space, it will be more convincing if there are experiments conducted on cross-embodiment (e.g. different arms or same arm with different inertial properties.)\n- The evaluation metric is unclear---in particular under BWT, the equation seems to be different from what is described---should the *best FWT model* be the best $F_i$ for task $i$ after seeing $k$ tasks?\n- For the 10 validation episodes, I believe it has been shown multiple times that validation error does not necessarily correspond to success rates [1, 2, 3]---a model may achieve high validation error while still achieving high success rate.\n- Regarding success rates, what are the baseline performances? That is, what is the success rate of the expert demonstrations? Do we expect the policy that trains purely using the task-specific data to perform better than the models in Figures 5 and 6? How would a random policy perform?\n\n**References**\n[1]: Hussenot, L., Andrychowicz, M., Vincent, D., Dadashi, R., Raichuk, A., Ramos, S., ... & Pietquin, O. (2021, July). Hyperparameter selection for imitation learning. In International Conference on Machine Learning (pp. 4511-4522). PMLR.  \n[2]: Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., ... & Mart\u00edn-Mart\u00edn, R. (2021). What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298.  \n[3]: Ablett, T., Chan, B., & Kelly, J. (2023). Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks. IEEE Robotics and Automation Letters."
                },
                "questions": {
                    "value": "- Under subsection **Training, Adaptation, and Evaluation**, what exactly is a validation scene?\n- Does ER retrain parameters from scratch, or does it continue training from the current set of parameters? Clearly if it is the latter the model will experience plasticity loss similar to the reinforcement learning setting [1].\n- It appears that some tasks are repeated based on appendix. I am wondering if there is any result regarding the per-task success rate. Do we understand whether the parameters are well-adapted to specific tasks, or are they similar in performance?\n\n**Possible typos**\n- On page 2, subsection PEFT, line 5: \"It is\" instead of \"it's\".\n\n**References**\n[1]: Nikishin, E., Schwarzer, M., D\u2019Oro, P., Bacon, P. L., & Courville, A. (2022, June). The primacy bias in deep reinforcement learning. In International conference on machine learning (pp. 16828-16847). PMLR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Reviewer_86yf"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698559926621,
            "cdate": 1698559926621,
            "tmdate": 1699636415912,
            "mdate": 1699636415912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ui4iFEvorr",
                "forum": "RRayv1ZPN3",
                "replyto": "A8ABrL7GFB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 86yf"
                    },
                    "comment": {
                        "value": "Thank you for the insightful review. We appreciate that you find the paper easy to follow, intuitive, and that TAIL performs well. We have addressed all your comments below. We hope that you consider increasing your score after seeing our detailed responses and new experiments.\n\n> **Cross-embodiment tasks?**\n\nThis is a great suggestion. Following your suggestion, we have added **new experiments** to use the model pre-trained on LIBERO data to adapt to the Franka kitchen tasks. The detailed results are shown in Appendix C.7, where TAIL-LoRA performs the best, achieving an 80% average success rate while the best baseline achieves 67%. The results indicate that our findings and claims from the extensive experiments on the large-scale LIBERO datasets can be generalized well on other small-scale domains, such as Franka-Kitchen.\n\n> **\u201cDo we expect the policy that trains purely using the task-specific data to perform better than the models in Figures 5 and 6? How would a random policy perform?\u201d**\n\nThank you for the suggestion regarding the task-specific agent and random agents. First, using a random policy without any fine-tuning always results in a 0 success rate, since our tasks are relatively long and generally require 200-400 steps. However, per request, we added **new ablation studies** for the task-specific agent in Appendix C.5 of the revised paper.\n\nIn summary, if we only train the model with corresponding task data, it does perform better than sequential full fine-tuning in the later stages. However, our TAIL-LoRA still outperforms this task-specific model. In addition, maintaining an entire copy of the model for each adaptation task consumes much more storage space than our approach, and as shown in our results in Table 2 and Appendix C.3, training a large model on a narrow domain with limited data could reduce the model\u2019s plasticity in adapting to new tasks. Therefore, these results further strengthen the advantage of our approach in terms of both resource efficiency and performance.\n\n> **Tasks explode over time b/c new adapter for each task?**\n\nWe use a single adapter for each task *suite,* which is of at least 8 tasks, corresponding to just 5 adapters for the 5 task suites (totaling 40 different tasks) in LIBERO in Fig. 5. We have clarified this in Section 5.2 of the revised paper. While we agree with your suggestion about using adapters based on the similarity of task instructions as it would be an exciting direction, it is beyond the scope of this work and we leave it for future research direction.\n\n> **Unclear evaluation metric**\n\nThanks for flagging this; there is indeed a typo for the BWT metric calculation. The BWT metric for the k-th task is computed by: \n$B_k = \\frac{1}{k-1} \\sum_{i=1}^{k-1} (S_{i} - F_{i})$, \nwhere $S_{i}$ values are from the best FWT model at task k, and $F_{i}$ values are from the best FWT model in the corresponding i-th task. We have revised this in Section 5.2.\n\n> **Validation error doesn\u2019t correspond to success rate?**\n\nTo clarify, we do not use the validation error to perform model selection. Success rates are simply computed either at the end of adaptation fine-tuning (Table 1) or displayed as a curve over training epochs (Fig 5) (see \u201ctraining, adaptation, and evaluation\u201d in Section 5.2). As mentioned by your referred paper Appendix G [1], validation loss is in fact a poor measure of policy performance. The purpose of displaying the validation errors is to show that full fine-tuning has a higher risk to overfit to the small data.\n\n[1]: Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., ... & Mart\u00edn-Mart\u00edn, R. (2021). What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298.\n\n> **\u201cRegarding success rates, what are the baseline performances? That is, what is the success rate of the expert demonstrations?\u201d**\n\nThe success rate of expert demonstrations is 100%; we have clarified this in Section 5.2.\n\n> **Some tasks are repeated in evaluation?**\n\nAlthough specific tasks may share similar descriptions, they are not identical due to environment variations, including goals, object arrangements, or object layouts (see Fig 3 for concrete examples). Consequently, these tasks are distinct. We have updated Appendix Section D with this discussion.\n\n> **What\u2019s a validation scene?**\n\nEach task comes with 50 demonstrations with different scene setups. We use 40 for training and 10 for validation. The validation scenes are the scenes from which the validation trajectories were created. We have clarified this in Section 5.2 of the revised paper.\n\n> **Does ER retrain from scratch or continue fine-tuning?**\n\nExperience replay (ER) continues fine-tuning\u2014we do not retrain from the original pre-trained model as it would require storing the entire model per task suite and is extremely storage inefficient for the continual learning setting.\n\nPlease let us know if you have any more questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272358559,
                "cdate": 1700272358559,
                "tmdate": 1700272358559,
                "mdate": 1700272358559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z63e4MdBzT",
                "forum": "RRayv1ZPN3",
                "replyto": "ui4iFEvorr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_86yf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_86yf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing all the points. I agree with reviewer Lua3 that this will be very useful work to benchmark against.\nHowever, my understanding is also that this is under supervised imitation learning setting, thus there remains a question about these methods under (inverse) reinforcement learning. With this point along with the discussed limitations, I will keep my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530620569,
                "cdate": 1700530620569,
                "tmdate": 1700530620569,
                "mdate": 1700530620569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uJLRg6lDAF",
                "forum": "RRayv1ZPN3",
                "replyto": "y1kEMizFS7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_86yf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Reviewer_86yf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the discussions, I believe the paper is fairly clear based on my current understanding."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621985157,
                "cdate": 1700621985157,
                "tmdate": 1700621985157,
                "mdate": 1700621985157,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r5LGGuWxKW",
            "forum": "RRayv1ZPN3",
            "replyto": "RRayv1ZPN3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_q7Ls"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4419/Reviewer_q7Ls"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new architecture named TAIL for imitation learning, which effectively adapts large pretrained models to new tasks with limited demonstration data. The main contribution of this method is to effectively incorporate lightweight adapter modules into pretrained models using various integration techniques, including parallel integration (with LoRA weights), sequential integration (Bottleneck Adapter), and prefix token integration. \n\nThe paper provides a comprehensive comparison of these adaptation techniques on the LIBERO robotic manipulation continual learning benchmark. The results indicate that TAIL, particularly when utilizing LoRA integration, outperforms the compared methods in terms of both forward and backward transfer.\n\nOverall, while this paper may not introduce strong technical novelty, it offers a good empirical study of lightweight continual learning techniques for imitation learning of robot control tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The paper includes sufficient experiments on the LIBERO benchmark, which can effectively demonstrate the advantages of employing TAIL in conjunction with the LoRA integration technique.\n3. The proposed TAIL-based methods greatly outperform the conventional fine-tuning method in both forward transfer results and adaptation efficiency."
                },
                "weaknesses": {
                    "value": "1. My primary concern is the technical novelty of this paper, although I acknowledge the significance of benchmarking and conducting an extensive empirical investigation of existing parameter-efficient fine-tuning techniques. The proposed TAIL framework, which incorporates lightweight adapters and task-specific heads, shares similarities with existing methods in continual learning and multi-task learning, as demonstrated in the paper from Rebuffi et al. (2017) titled 'Efficient Parametrization of Multi-Domain Deep Neural Networks.' Although TAIL introduces a new multi-modal, Transformer-based architecture, the core idea bears a strong resemblance to prior work.\n2. In Figure 4, the authors primarily compare various design choices within the TAIL framework when presenting forward adaptation results. It would be valuable if the authors could extend this analysis to include a comparison of TAIL's best performance with the state of the art in lifelong imitation learning. Furthermore, in Table 1, the comparison of TAIL's forward transfer performance with previous continual learning methods may not be fair enough. This is because EWC and ER primarily address catastrophic forgetting and may not contain task-specific model parameters. \n3. In Figure 5, a comparison between TAIL and EWC/ER may not be entirely equitable, considering that these prior methods do not assume known task IDs or retain task-specific model parameters. I would like to suggest the authors include a comparison with multi-task learning methods for a more comprehensive evaluation."
                },
                "questions": {
                    "value": "1. Is a separate adapter $\\omega_k$ trained for each task suite? My understanding is that when re-evaluating previous task suites $j$, the previously trained adapter $\\omega_j$ is reloaded instead of testing the previous task on the adapter $\\omega_k$ trained for the current task $k$. \n2. In Figure 4, it would be insightful to determine whether combining all integration techniques, including LoRA, Bottleneck Adapter, and Prefix Token, would yield further performance improvements."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4419/Reviewer_q7Ls"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629044444,
            "cdate": 1698629044444,
            "tmdate": 1700786566780,
            "mdate": 1700786566780,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QismkTy3T2",
                "forum": "RRayv1ZPN3",
                "replyto": "r5LGGuWxKW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer q7Ls"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review. We appreciate that you found our paper well-written and the experiments sound! We have addressed all your comments below. We hope you consider increasing your score after seeing our detailed responses and new experiments.\n\n> **What is the technical contribution?**\n\nPlease see our answer to this above in the **Response to all reviewers** that addressed this question: \u201cTo summarize our contributions\u2026\u201d\n\n> **\u201cThe proposed TAIL framework, which incorporates lightweight adapters and task-specific heads, shares similarities with existing methods in continual learning and multi-task learning, as demonstrated in the paper from Rebuffi et al. (2017)\u201d**\n\nThank you for this pointer. While both papers have similar motivations, there are major differences. In particular, Rebuffi et al does not (1) study continual imitation learning, (2) investigate decision-making settings, (3) or investigate transformer-based models.\n\n(1) and (2) are important for deploying decision-making agents in realistic settings (e.g., robots) and (3) is important given the impact that transformers have had on large-scale pre-training.\n\nWe\u2019ve now discussed Rebuffi et al. in our updated related works section.\n\n> **Compare TAIL against SOTA in continual imitation learning?**\n\nThat is a good suggestion. We have now added a comparison against PackNet [1], a SOTA method as shown in the original LIBERO paper, which prunes parameters to be then re-learned for every new task. The detailed results are updated in Fig. 5 of the revised paper. TAIL with LoRA outperforms PackNet in forward imitation in every task suite, for example, **2 times** in the Goal task suite and **3 times** in the Living Room task suite. Furthermore, PackNet still suffers from severe catastrophic forgetting problems after long continual adaptation stages while TAIL does not.\n\n[1]: PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning. Arun Mallya and Svetlana Lazebnik.\n\n> **\u201cIn Figure 5, a comparison between TAIL and EWC/ER may not be entirely equitable, considering that these prior methods do not assume known task IDs or retain task-specific model parameters\u201d**\n\nWe clarify that all methods can access task ID: the language descriptions as shown in Fig 1. Thus, task-specific model parameters are also contained in EWC/ER in the language encoder and the entire base model is fine-tuned in EWC/ER so that they can fairly perform forward transfer. We have clarified this in Section 5.2 of the revised paper.\n\n> **Try combining LoRA, bottleneck, and p-tuning?**\n\nThank you for this excellent suggestion! We\u2019ve now included **new experiments** in Appendix C.6\u00a0 of where we test all combinations of LoRA, bottleneck layers, and p-tuning. All combinations including LoRA perform similarly (75% success rate) and significantly better than other combinations (41-65%), indicating that LoRA is necessary for good adaptation performance.\n\n> **Separate adapter for each task suite?**\n\nYes, we include a separate adapter for each task suite. We have clarified this in Section 5.2.\n\nPlease let us know if you have any more questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271978169,
                "cdate": 1700271978169,
                "tmdate": 1700271978169,
                "mdate": 1700271978169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]