[
    {
        "title": "What's in a Prior? Learned Proximal Networks for Inverse Problems"
    },
    {
        "review": {
            "id": "eLHLWWhTNI",
            "forum": "kNPcOaqC5r",
            "replyto": "kNPcOaqC5r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2577/Reviewer_XDuj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2577/Reviewer_XDuj"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a way to learn a denoising map, called LPN, which is exactly the proximal operator of a scalar function which should approximate the true log image prior. They make use of the characterization of nonconvex proximity operators as gradient of convex functions from (Gribonva & Nikolova, 2020) and parametrize their denoiser as the gradient of (an ICNN.+ a quadratic term). They also suggest a specific loss, called \"proximal matching loss\" such that, when the denoiser is trained to denoise Gaussian noise with this loss, it should approximate the prox of the true log image prior. They also propose a convergence proof of the PnP-PGD algorithm with plugged LPN denoiser and experiment on different datasets and inverse problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally well presented and well written. \n- The idea to parameterize a denoiser as the gradient of an ICNNs is new in the PnP literature. \n- The most interesting contribution of the paper is for me the proximal matching loss. \n- The verification of the nonconvexity of the learned prior is also interesting."
                },
                "weaknesses": {
                    "value": "Here are several potential issues that I spotted while reviewing the paper.\n\nMajor weaknesses : \n- The end the proof from Theorem 4.1 is I think not true, and the presented result not valid. Indeed, the definition of subdifferential subdiferential for convergence of nonconvex PGD is not the usual subdifferential but the limiting subdifferential. With this notion, the first equivalence from C.48 is not true when $\\phi_\\theta$ is nonconvex, and only the implication holds. Therefore, the algorithm can not converge to a fixed-point as presented, but only to a critical point of $h + \\eta^{-1}\\phi_\\theta$.\n- The targeted objective function $h + \\eta^{-1}\\phi_\\theta$ contains the stepsize (which is then a regularization parameter). This is uncommon in optimization and not desirable, as a change in the stepsize affects the objective function and the obtained result. Moreover, the stepsize (and thus the regularization parameter) must be bounded by $1/L$, is this limiting in practice and does it impact the performance ?\n- The algorithm analyzed for convergence is just the classical Proximal Gradient Descent (PGD) (or Forward-Backward). \nWhy do you use the convergence analysis from (Bot et. al, 2016) which is specific for an accelerated version of this scheme. You could use directly the (most commonly used) convergence result from (Attouch et. al, 2013) of PGD in the nonconvex setting.\n- \"Definable\" is an important notion that should be defined. Moreover, the stability properties of Definable functions ( by sum, composition, inverse, derivative) are extensively used without referencing the proof of these results. Same comment for the fact that the exponential is definable. \n- You experiment with PnP-ADMM when the presented theoretical result is with PGD. If this is correct, this is a major issue. Indeed, the theoretical convergence results of ADMM in the convex setting are not the same as the ones for PGD. \n\nMinor weaknesses : \n- Lemma C.2 :  I think that $f_\\theta$ is only invertible on $Im(f_\\theta)$ and $\\phi_\\theta$ is only differentiable on $Im(f_\\theta)$. If this is really true on the whole space, can you explain why ? \n- Section 1 : \"for almost any inverse problem, a proximal step for the regularization function is always present\" I do not understand this sentence and what you mean by \"present\".\n- Section 1 : In (Romano et. al, 2017), there is no prox involved. \n- Section 2 : Contrary to what you seem to explain, PnP is really not limited to ADMM !! \n- Section 3 : Parameterizing a network as the gradient of an ICNN as already been proposed in the literature, in particular in the Optimal Transport community. These works should be cited as well. \n- Section 3 : You explain that the chose parameterization is more general and universal than (Hurault et. al, 2022). I am very doubtful about this statement, given the fact that the ICNN parameterization is very (and I think way more) constraining. A way to support your affirmation would be to compare the performance with both denoisers for denoising and PnP restoration. \n- In most PnP algorithm, the Gaussian noise parameter $\\sigma$ on which the denoiser is trained acts as a regularization parameter. Here, you do not mention this parameter, and you do not explain how it is chosen. \n\nThese weaknesses explain my low score, and I am ready to raise my score if the authors answer these limits."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2577/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2577/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2577/Reviewer_XDuj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2577/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698581953493,
            "cdate": 1698581953493,
            "tmdate": 1699636194854,
            "mdate": 1699636194854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nmyhDa7Ifz",
                "forum": "kNPcOaqC5r",
                "replyto": "eLHLWWhTNI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2577/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2577/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their careful review of our work. We have addressed all of your questions and comments, and please don't hesitate to let us know if there are any pending matters.\n\n1. > The end the proof from Theorem 4.1 is I think not true... \n\nThank you for pointing this out. We have corrected the proof. The argument that the algorithm converges to a fixed-point still holds, due to the continuity of LPN. Specifically, since both $f_\\theta$ and $\\nabla h$ are continuous, the mapping from $x_k$ to $x_{k+1}$, $x_{k+1} = f_\\theta(x_k - \\eta \\nabla h(x_k))$ is continuous. Then, given that the iterative algorithm converges, and that the mapping from $x_k$ to $x_{k+1}$ is continuous, the algorithm must converge to a fixed point. Please see the last paragraph of the proof of Theorem 4.1 and Lemma C.2 for the updated proof.\n\n2. > The targeted objective function h + eta^-1 phi contains the step size ... \n\nThank you for pointing this out. Indeed, this 1/eta weighing of the regularizer might seem strange, but we would like to argue that in our setting it is reasonable and may even be useful. \n\nFirst, we would like to comment on the cause of $1/\\eta$ weighing. For an objective $f(x) + g(x)$, the proximal gradient descent (PGD) algorithm applies ${prox}_{\\eta g}(x-\\eta \\nabla f(x))$\n\nat each iteration. Note that the proximal operator $prox_{\\eta g}$ needs to be scaled appropriately by $\\eta$. \nIf we follow this strictly, the objective will not change with eta. However, in our case, the proximal operator is fixed at $prox_{\\phi}$, and we did not scale it accordingly by $\\eta$, which requires evaluation of $prox_{\\eta \\phi}$. Thus our calculation has a subtle difference from classic PGD, creating the $1/\\eta$ weighing in the objective.\n\nSecond, this weighing may be useful, as it provides a way to change the regularization weight \twithout retraining LPN. Indeed, eta can be treated as a knob to control the regularization weight: smaller eta means higher weight on the regularization. However, as the reviewer points out, eta being too large would cause the algorithm to diverge (note that in practice $\\eta > 1/L$ does not necessarily mean the algorithm will diverge, i.e. the actual upper bound may be larger than $1/L$), so this tuning is not full-spectrum. We do not see how this necessarily impacts the performance, as there exist other ways to tune the regularization weight, e.g., via Gaussian noise level at training.\n\n3. > The algorithm analyzed for convergence is just the classical Proximal Gradient Descent (PGD) (or Forward-Backward) ... \n\nWe used the result from Bot et al. (2016) because it includes coercivity of the objective as a hypothesis, whereas the main result in Attouch et al. (2013), i.e. Theorem 5.1, writes the result instead in the form \u201cif the iterate sequence is bounded, then it converges\u201d. Although it is straightforward to argue that coercivity of the objective implies iterate boundedness, we assumed that for a ML venue, it would be easier for readers to parse the proof if we appealed to a result with hypotheses that match our setting. In the main body of the submission, we made a note of the precedence of Attouch et al. (2013) via a citation in the \u201cConvergence Guarantees in Plug-and-Play Frameworks\u201d paragraph at the top of page 6 in the main body. If you want us to change the proof to appeal to Attouch et al. (2013) and argue boundedness of the iterate sequence using coercivity instead of appealing to Bot et al. (2016), we can do so.\n\n4. > \"Definable\" is an important notion that should be defined. Moreover, the stability properties of Definable functions ... \n\nWe have added a new section to the appendices, Section C.4.1, which contains what we believe is a far more complete and readable overview of the relevant aspects of the KL property that we need to prove our convergence results. The submission originally referenced Attouch et al. 2010 for these facts, which seems to us to be a standard and sufficient reference here (allowing one to avoid tedious verifications), but the revision has more precise references to proofs and spells out all details in the verifications. On this note, we would like to note that you are asking for what seems to us to be a very high standard of rigor for a machine learning conference submission, and we hope you will acknowledge our contribution in meeting it. Please let us know if you have further feedback in this connection."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533979906,
                "cdate": 1700533979906,
                "tmdate": 1700533979906,
                "mdate": 1700533979906,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2tPJ3w9dRt",
            "forum": "kNPcOaqC5r",
            "replyto": "kNPcOaqC5r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2577/Reviewer_hkxu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2577/Reviewer_hkxu"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a framework for constructing learned proximal networks (LPN) that offer precise proximal operators for a data-driven regularizer. It also demonstrates a novel training strategy called proximal matching, which ensures that the resulting regularizer accurately captures the log-prior of the actual data distribution. Experiments test the effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper forms a class of neural networks to guarantee to parameterize proximal operators. The idea is novel.\n2.\tSome theoretical results are provided.\n3.\tExperiments show the effectiveness."
                },
                "weaknesses": {
                    "value": "1.\tThe paper is not well organized, making it hard to follow.\n2.\tThe experiments are a little weak. See the questions below."
                },
                "questions": {
                    "value": "My main concerns are the experimental details.\n1.\tIt is better to give some details on the training of the learned proximal networks. For example, how to learn the non-negative weights?\n2.\tHow to set the \\alpha in \\phi_\\theta in the training?\n3.\tHow to set \\eta in Theorem 4.1 in the training?\n4.\tThe methods compared in this paper are relatively old. It is better to compare the recent methods. Then we could find out the superiority of the proposed method.\n5.\tThe used datasets are very small. Deep learning can only be effective when there is a substantial amount of data.\n6.\tIt is best to verify the effectiveness of the proposed method on common real tasks, e.g., real denoising or deblurring.\n7.\tCompared with other methods, how is the training efficiency? It is better to verify it by experiment.\n8.\tAt the end of the Abstract, \"demonstrating\" should be \"Demonstrating\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2577/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661465755,
            "cdate": 1698661465755,
            "tmdate": 1699636194771,
            "mdate": 1699636194771,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JqzWxZp9Cl",
                "forum": "kNPcOaqC5r",
                "replyto": "2tPJ3w9dRt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2577/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2577/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their questions and comments.\n\n1. > It is better to give some details on the training of the learned proximal networks. For example, how to learn the non-negative weights? \n\nThank you for the question. We have provided extensive training details, such as learning rate, Gaussian noise level, schedule of proximal matching $\\gamma$, and batch size in the Appendix E. Sorry for missing the details about how to enforce non-negative weights. During training, we apply weight clipping, i.e., projecting negative weights to 0, at each iteration to ensure the weights are nonnegative. Additionally, in the experiments on CelebA and Mayo-CT, we enforce the weights to be nonnegative at initialization by initializing the weights according to a Gaussian distribution and then taking the exponential. We observed that this initialization helped the training converge faster. We have added these details to Appendix E. In addition, we will also release our code publicly for reproductivity.\n\nIn Appendix E.1, Details of Laplacian Experiment:\n* To enforce nonnegative weights, weight clipping is applied, projecting the negative weights to zero at each training iteration.\n\nIn Appendix E.3, Details of CelebA Experiment: \n* We observed that initializing the respective weights to be nonnegative, by initializing them according to a Gaussian distribution and then taking the exponential, helped the training converge faster. Therefore, we applied such initialization in the experiments on CelebA and Mayo-CT. The same weight clipping as in Appendix E.1 is applied to ensure the weights stay nonnegative throughout training.\n\n2. > How to set the \\alpha in \\phi_\\theta in the training? \n\nThis is a user-defined parameter. We discussed its impact and trade-off in Section 3, the \u201cRecovering the prior from its proximal\u201d paragraph, and footnote 4. The values we used for each experiment are included in Appendix E.\n\n3. > How to set \\eta in Theorem 4.1 in the training?\n\nThe $\\eta$ is the step size of gradient descent while using a trained LPN for solving inverse problems in the PnP framework. It is not a parameter during training. Sorry about the confusion. As noted in Theorem 4.1, $\\eta$ should be between 0 and $1 / \\|A^TA\\|$ to guarantee convergence of the PnP iterates with LPN. In practice, $\\eta$ is a hyperparameter that can be tuned by the user.\n\n4. > The methods compared in this paper are relatively old. It is better to compare the recent methods. Then we could find out the superiority of the proposed method.\n\nWe thank the reviewer for this suggestion. Indeed, we have now added further comparison with a newer, highly relevant and state-of-the-art method (which was also suggested by another reviewer) : \n\n* Hurault, Samuel, Arthur Leclaire, and Nicolas Papadakis. \"Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization.\" ICML 2022. \n\nNote, however, that no other method exists that approximates the solution of a MAP problem based on learned networks that provide exact proximal operators. Our LPN is the first to provide such a guarantee.\nWe refer the reviewers to Figure 5 and Table 1 in the revised manuscript for the new results. The results from LPN are comparable and on-par with those from GS networks, while our method provides the added advantage of providing an exact proximal operator \u2013 which GS networks might not (we explain in detail why this is the case in our response to Reviewer 4, comment 11)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533351208,
                "cdate": 1700533351208,
                "tmdate": 1700533351208,
                "mdate": 1700533351208,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NtU1HXboLs",
            "forum": "kNPcOaqC5r",
            "replyto": "kNPcOaqC5r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2577/Reviewer_8FVJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2577/Reviewer_8FVJ"
            ],
            "content": {
                "summary": {
                    "value": "Inverse problems $y = A(x) + v$ are commonly formulated using regularized least squares:\n$$\\min_x \\frac{1}{2} \\|\\| y - A(x) \\|\\| + \\phi(x)$$\n\nwhere $\\phi$ is an appropriate regularizer. Solving this minimization problem often involves the use of proximal gradient methods. This process necessitates the selection of a regularizer, $\\phi$, and the knowledge of its proximal map. A natural approach, leading to Maximum A Posteriori (MAP) estimation, is to choose $\\phi(x) = -\\log p(x)$.\n\nThis paper presents a method for learning the proximal map of $\\phi(x) = -\\log p(x)$ from data, using only samples from $p(x)$. The authors then apply this proposed approach, in conjunction with proximal gradient methods, to various standard inverse problem tasks, demonstrating favorable performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The use of proximal and descent methods in conjunction with deep neural networks to address inverse problems is a dynamic and exciting field of study. Numerous papers have explored these approaches, attempting to approximate Maximum A Posteriori (MAP) estimation in various ways or training regularizers in a supervised manner (which is not always feasible and not even correct). To the best of my knowledge, this paper stands out as the first to present a principled approach for training proximal maps of the log probability and effectively approximating MAP estimation.\n\nAdditionally, the numerical results presented in this paper are well-executed and show promise."
                },
                "weaknesses": {
                    "value": "- A straightforward solution to obtain the proximal map of log p(x) is to initially train an energy-based model, E, and then compute the proximal map of og E. The authors should either compare this approach with their proposed method or provide clarification on why this is not considered viable or advisable. For instance, one might anticipate encountering similar challenges as those faced when training energy models when training Learned Proximal Networks (LPNs).\n\n- In a similar vein, a natural point of comparison for the proposed approach would involve using other state-of-the-art unsupervised methods that rely on generative models as priors. While the paper does make a comparison with the less recent adversarial regularizer, it would be valuable to assess its performance against other more recent unsupervised methods.\n\n--- \nAfter rebuttal: The authors have addressed the concerns raised in the reviews and I have increased my score."
                },
                "questions": {
                    "value": "- Equation (3.2) should it be \\psi_\\thet(x,\\alpha) ? \n\n- Page 5, \"demillustrates\" should be corrected to \"illustrates.\"\n\n- Page 5, the terminology \"proximal operator of target distribution\" is frequently used but remains undefined. This terminology appears to be uncommon in the field and should be clarified or explained for the readers' benefit.\n\n- It would be insightful to determine whether the log-likelihood computed using LPN matches the log-likelihood computed using other generative models.\n\n- \"Hidden\" in the appendix is that the LPN is first trained on the l1-loss, then it is trained with the proximal matching loss. This should be stated clearly in the main body of the paper. Why this is needed? How bad is LPN trained just with the proximal matching loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2577/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2577/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2577/Reviewer_8FVJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2577/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692259989,
            "cdate": 1698692259989,
            "tmdate": 1700585474081,
            "mdate": 1700585474081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4X1rhNDROE",
                "forum": "kNPcOaqC5r",
                "replyto": "NtU1HXboLs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2577/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2577/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your questions and comments.\n\n1. > A straightforward solution to obtain the proximal map of log p(x) is to initially train an energy-based model, E, and then compute the proximal map of og E. The authors should either compare this approach with their proposed method or provide clarification on why this is not considered viable or advisable. For instance, one might anticipate encountering similar challenges as those faced when training energy models when training Learned Proximal Networks (LPNs).\n\nThanks for bringing up this very relevant comment. There are two main reasons why the above suggested by the reviewer is not straightforward: First, training an energy-based generative model E is a statistically complex task, particularly for distributions supported in high-dimensional spaces. While it is true that good approximate models can be obtained, computing the proximal operator will then boil down to the explicit optimization of a non-convex problem, precluding us from any precise guarantees. Our approach circumvents both of these by computing the exact proximal directly (even if the prior is non-convex), i.e., by learning to compute the maximum of a posterior, which is a significantly easier task than computing the complete posterior and then finding its maximum (which is the approach entailed by the energy model).\n\n2. > In a similar vein, a natural point of comparison for the proposed approach would involve using other state-of-the-art unsupervised methods that rely on generative models as priors. While the paper does make a comparison with the less recent adversarial regularizer, it would be valuable to assess its performance against other more recent unsupervised methods.\n\nThank you for the suggestion. In our revised manuscript, we have added a new and more recent comparison method (Hurault et al. 2022), based on the Gradient Step (GS) denoiser.\n\n* Hurault, Samuel, Arthur Leclaire, and Nicolas Papadakis. \"Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization.\" ICML 2022.\n\nWe chose this method because it is closely related to ours, for the following reasons. First, this method also concerns the topic of solving inverse problems via a variational setup and seeking the Maximum a Posteriori estimate with a data-driven prior. Second, this method also attempts to train a denoiser as a proximal mapping and study convergence guarantees when using the trained denoiser within the PnP framework. Lastly, this method was also suggested by another reviewer as a state-of-the-art method that is comparable to our proposed methodology. Therefore we believe it is a very relevant comparison.\n\nWe refer the reviewers to Figure 5 and Table 1 in the revised manuscript for the new results. Our results are comparable and on-par with those from the GS networks, while our method provides the added advantage of providing an exact proximal operator \u2013 which GS networks might not (we explain in detail why this is the case in our response to Reviewer 4, comment 11). \n\n**Questions**:\n\n* Equation (3.2) should it be \\psi_\\thet(x,\\alpha) ?\n\nRight. Thank you for pointing this out and we have corrected it.\n\n\n* Page 5, \"demillustrates\" should be corrected to \"illustrates.\"\n\nFixed, thanks!\n\n* Page 5, the terminology \"proximal operator of target distribution\" is frequently used but remains undefined. This terminology appears to be uncommon in the field and should be clarified or explained for the readers' benefit.\n\nThank you for the suggestion. It means the proximal operator of the log-prior of the data distribution we aim to learn, i.e. prox_{-log p_x}. We have rephrased it in the paper:\n\nPage 5: \"Figure 2 illustrates the limitations of these distance metrics for learning the true proximal operator of the log-prior of the underlying data distribution (a Laplacian, in this example).\"\n\nMoreover, to avoid any further confusion, we have explicitly defined \u201clog-prior\u201d the first time it appears in the paper: \"In this paper, the \u201clog-prior\u201d of a data distribution px means its negative log-likelihood, \u2212log px.\"\n\n* It would be insightful to determine whether the log-likelihood computed using LPN matches the log-likelihood computed using other generative models.\n\nWe agree with this comment. However, we believe it is out of the scope of this work, which focuses on solving inverse problems with data-driven priors. Note that in this work, we did not attempt to use LPN as a generative model (e.g., using LPN to generate human faces from scratch), or compare it with any generative models. However, we do believe LPN has the potential to be adapted into generative models and we are very interested in exploring this connection in future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533142033,
                "cdate": 1700533142033,
                "tmdate": 1700533142033,
                "mdate": 1700533142033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4bx6Jwf2pV",
                "forum": "kNPcOaqC5r",
                "replyto": "XzwmLfrdJA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2577/Reviewer_8FVJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2577/Reviewer_8FVJ"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n \nThank you for the clarifications and for addressing my concerns. I will raise my score to reflect the improvements."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585359647,
                "cdate": 1700585359647,
                "tmdate": 1700585359647,
                "mdate": 1700585359647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kjws32K3br",
            "forum": "kNPcOaqC5r",
            "replyto": "kNPcOaqC5r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2577/Reviewer_fi4Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2577/Reviewer_fi4Y"
            ],
            "content": {
                "summary": {
                    "value": "Plug-and-play methods are a framework used for solving inverse problems in image processing, and rely on proximal operators. It's been shown that the proximal operator for the linear inverse problem case is equivalent to a MAP denoiser. In the literature PnP methods almost always rely on MMSE denoiser due to difficulty of learning and designing MAP estimators. In this work, the authors suggest a way to learn a MAP denoiser which they then use in an ADMM algorithm to solve inverse problems. They show the algorithm converges to fixed points of the prior for a particular design of neural networks which guarantees strongly convex mapping. Finally they test the algorithm empirically on toy Laplacian distribution as well as MNIST, CelebA and Mayo-CT datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work addresses a prevalent problem at the core of PnP methods, which is the use of MMSE denoisers instead of MAP denoisers, despite the fact that the convergence results for PnP holds for MAP denoisers. They offer a novel way to learn a MAP denoiser and provide convergence results."
                },
                "weaknesses": {
                    "value": "- The main contribution of the work is to propose a way to learn a MAP denoiser through a proximal loss under equation 3.4. Optimizing for this loss entails that the prior distribution is assumed to be a mixture of Gaussians (or Diracs when $\\gamma$ tends to zero) around the training samples. Why is this a good prior? It seems to me that is a too simplistic prior and in the limit of $\\gamma \\to 0$ a discontinuous prior. \n- Although most of the paper was quite clear I found it unclear whether the final implementation enforced convexity on the prior or not. Under section 2 and section 3, a case was made for convexity: a class of learned proximal network was suggested to ensure convexity, and later under (3.1) another regularization was introduced to ensure strong convexity for recovering the prior. I think these sections do not flow well and the thread of logic is easily lost by the reader. The clarity can be improved. Also on a related note, if convexity is assumed, why is it a proper assumption? It most probably would be a too simplistic of an assumption for image priors. \n- The details of the algorithm can be more elaborated in the main text. Specially, since the algorithm is very similar to the basic score-based diffusion generative algorithms for solving inverse problems, the differences between the two should be pointed out. \n- I find it surprising and contradictory that the PSNR values are higher for this methods compared to other methods that use MMSE denoisers. It is expected that for solving inverse problems using a MAP denoiser result in sharper images with lower PSNR (=higher MSE) while using MMSE denoiser result in higher PSNR (=lower MSE) and more blurry results. This is obvious since, as also stated by the authors, the MMSE denoiser pushed the image towards the mean of the posterior while MAP denoiser pushes the image towards the mode of the posterior. Obviously the mean of the posterior is equivalent to the MSE minimizer, so it should result in lower MSE. \n- Additionally, I found it confusing that the qualitative results for CelebA dataset seem pretty much the same across different methods, and not very good in general. The results look pretty blurry for the proposed method which is surprising given that the method relies on a MAP denoiser. \n- Finally, in recent years, there has been some score based diffusion models algorithms which use priors in denoisers to solve linear inverse problems, which result in significantly better performance. Considering the disadvantage in performance, what are the advantages of this methods? It seems necessary to include a comparison to those models."
                },
                "questions": {
                    "value": "Please see the questions raised under the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2577/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699250234970,
            "cdate": 1699250234970,
            "tmdate": 1699636194565,
            "mdate": 1699636194565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cmjwFkcRxq",
                "forum": "kNPcOaqC5r",
                "replyto": "Kjws32K3br",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2577/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2577/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your questions and comments.\n\n1.  > The main contribution of the work is to propose a way to learn a MAP denoiser through a proximal loss under equation 3.4. Optimizing for this loss entails that the prior distribution is assumed to be a mixture of Gaussians (or Diracs when gamma tends to zero) around the training samples. Why is this a good prior? It seems to me that is a too simplistic prior and in the limit of gamma -> 0 a discontinuous prior.\n\nNot quite: the proximal matching loss (Eq. 3.4) does not require the prior distribution to be a mixture of Gaussians or Diracs. There is almost no assumption on the prior to be learned, except the most basic regularity conditions, e.g., for the prior to be continuous in Thm 3.1 (and discrete in Thm B.1). Note that in the paper, immediately after Eq. 3.4, we do mention that the loss \\ell_\\gamma can be thought of as an \\gamma-approximation to a Dirac, but this is just for intuition, and no conditions are imposed on the actual priors.\n\n2. > Although most of the paper was quite clear I found it unclear whether the final implementation enforced convexity on the prior or not. Under section 2 and section 3, a case was made for convexity: a class of learned proximal network was suggested to ensure convexity, and later under (3.1) another regularization was introduced to ensure strong convexity for recovering the prior. I think these sections do not flow well and the thread of logic is easily lost by the reader. The clarity can be improved. Also on a related note, if convexity is assumed, why is it a proper assumption? It most probably would be a too simplistic of an assumption for image priors.\n\nWe believe we see the confusion of the reviewer: We do not enforce convexity on the prior at any point. The review is correct in that convex priors are not well suited for natural images, but note that our method can learn nonconvex priors, as has been demonstrated in the MNIST example (Fig. 3b), and we view this as a main advantage of our method. We will further clarify on this point - thank you.\nIn our presentation, while the function $\\psi$ is convex, note that it is not the prior - the prior is denoted by $\\phi$. Convexity on $\\psi$ is enforced to ensure the LPN, which is defined as the gradient of $\\psi$, is an exact proximal operator, as required by Proposition 2, which uses the fact that proximal operators of nonconvex priors are equivalent to gradients of convex functions. Please see Figure 1 for a clear illustration of this relation.\n\n3. >The details of the algorithm can be more elaborated in the main text. Specially, since the algorithm is very similar to the basic score-based diffusion generative algorithms for solving inverse problems, the differences between the two should be pointed out.\n\nWe thank the reviewer for this question. While our methodology is used to solve inverse problems, PnP in general, and our LPN in particular, are significantly different from score-based diffusion models. First: conditional diffusion models do not minimize a variational problem like we do in this paper (as in Eq. 2.1), but instead provide samples from the posterior distribution. Second: score-based sampling arises as the result of inverting a diffusion process which requires access to an MMSE denoiser, whereas we are only concerned with networks that compute a MAP estimate for a learned prior. Third, the reviewer should note that our LPN provides a (MAP) solution in a single call (or forward pass) for the denoising task, whereas sampling from the posterior as with diffusion models requires very expensive computation. \nFor all these reasons, comparing with diffusion based models is out of the scope of this work. However, based on your suggestion, we have included a detailed discussion of these points in our revised version of the paper (please see Appendix A, \u201cComparison to Diffusion Models\u201d paragraph). If the reviewer has specific works in mind that we should reference and comment on, we would be happy to do so.\nIn terms of algorithm details, we have included the pseudo-code in Algorithms 1 and 4, and details of hyperparameters in Section E. We will also release our code publicly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532627585,
                "cdate": 1700532627585,
                "tmdate": 1700532627585,
                "mdate": 1700532627585,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HCVQQy50XF",
                "forum": "kNPcOaqC5r",
                "replyto": "SiWXpMNYOw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2577/Reviewer_fi4Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2577/Reviewer_fi4Y"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses. Since I think most of the comments (except for #3 ) do not resolve the questions, I keep my initial score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604781191,
                "cdate": 1700604781191,
                "tmdate": 1700604781191,
                "mdate": 1700604781191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]