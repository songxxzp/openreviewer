[
    {
        "title": "Distribution Calibration For Few-Shot Learning by Bayesian Relation Inference"
    },
    {
        "review": {
            "id": "HOLho7uiFq",
            "forum": "8oZf2SlXEY",
            "replyto": "8oZf2SlXEY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6645/Reviewer_czxy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6645/Reviewer_czxy"
            ],
            "content": {
                "summary": {
                    "value": "This paper aimed to solve few-shot learning by generating feature embeddings for the minority classes based on their relations with the majority classes. The author proposed a method based on the so-called \"Bayesian relation inference\" and tested the proposed method on a dermatological image dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Modeling the relationship between classes is a nice approach to few-shot learning\n- It is nice to evaluate the proposed method on real data."
                },
                "weaknesses": {
                    "value": "- Due to the unclear mathematical notation, I was unable to fully understand the proposed method. For example:\n  - What is the definition of the relation graph? What are the nodes and edges?\n  - What is the range of the mean in Eq. (1)?\n  - What is a \"splicing\" of two node embeddings?\n  - What are $n$ and $\\lambda$?\n  - The domains and codomains of $L$ were not clearly stated.\n- I do not have any knowledge of the so-called \"Bayesian relation inference\" and cannot find any references.\n- Figure 1 is dense but not so informative.\n- There is no theoretical guarantee or analysis of the proposed method."
                },
                "questions": {
                    "value": "Please clarify the notation and the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767421815,
            "cdate": 1698767421815,
            "tmdate": 1699636759106,
            "mdate": 1699636759106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kmec0yU5jj",
                "forum": "8oZf2SlXEY",
                "replyto": "HOLho7uiFq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Main Responses to Reviewer czxy (1/3)"
                    },
                    "comment": {
                        "value": "First of all, we would like to thank the reviewer for their encouragement to our work. In this work, we propose a calibration distribution method based on Bayesian relation reference method for few-shot classification task. The highlight of the proposed model is that it **achieves better performance** compared to the SOTA few-shot learning algorithms, while visual analysis proves that the proposed model **has a certain degree of interpretability**, which is useful especially for medical scenarios such as dermatological classification.\n\nThanks very much for pointing out the weaknesses of the article. It helps us to better improve the deficiencies in our paper. We answer the questions raised by the reviewer below:\n\n- ***Q1:*** *What is the definition of the relation graph? What are the nodes and edges?*\n\n  ***A1:*** Thank you for raising this issue. In each relation graph in this paper( Summary graph $\\boldsymbol{M}$ & Gaussian relation graph $\\boldsymbol{A}$ et.al.), **nodes are either base or target class embeddings, and the edges represent the relation intensity between target class and each base class.** Specifically, target class node embedding $N\\_{t} \\\\in \\mathbb{R}^{\\rm{N}\\_{f}}$ is the feature generated by the backbone network( In our paper, we introduce pre-trained Resnet 18 as the backbone network) from a single target image. For the $i$-th base class node embedding $N_{b_{i}} \\in \\mathbb{R}^{\\rm{N}_{f}}$, we first generate the features of all the images in the $i$-th base class through the backbone network, after which we take the average of the features as the base class node embedding.\n\n- ***Q2:*** *What is the range of the mean in Eq. (1)?*\n\n  ***A2:*** Thank you for pointing out this problem which we overlooked. **We rewrite Eq. (1) as follow to make the expression clearer**:\n\n  > \u200b $$\\begin{equation}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\boldsymbol{N\\_{b\\_{i}}} = mean(\\\\sum\\_{X\\_{Y=y\\_{i}}}\\boldsymbol{f}\\_{\\operatorname{Res18}} (X\\_{Y=y\\_{i}}))                             \\tag{1}\\end{equation}$$\n\n  Since the output of $\\boldsymbol{f}_{\\operatorname{Res18}} (\\cdot)$ is bounded to(0, 1), **the range of the mean in Eq. (1) is bounded to(0, 1).**\n\n- ***Q3:*** *What is a \"splicing\" of two node embeddings?*\n\n  ***A3:*** Thanks for your question. \"Splicing\" in our paper represents a simple splice of two embeddings. For example, $\\boldsymbol{N_{t}} \\in \\mathbb{R}^{\\rm{N_{f}}}$ and $\\boldsymbol{N_{b_{i}}} \\in \\mathbb{R}^{\\rm{N_{f}}}$ are two tensors of the same dimension and $\\left[\\boldsymbol{N_{t}}, \\boldsymbol{N_{b_{i}}}\\right] \\in \\mathbb{R}^{2\\rm{N_{f}}}$ represents the splicing of them. We have the first $\\rm{N_{f}}$ features in $\\left[\\boldsymbol{N_{t}}, \\boldsymbol{N_{b_{i}}}\\right]$ are equal to $\\boldsymbol{N_{t}}$ and the last $\\rm{N_{f}}$ features in $\\left[\\boldsymbol{N_{t}}, \\boldsymbol{N_{b_{i}}}\\right]$ are equal to $\\boldsymbol{N_{b_{i}}}$.\n\n- ***Q4:*** *What are $n$ and $\\lambda$?*\n\n  ***A4:*** Thanks for raising this important issue.  **We assume that the summary graph of the coupling is a sampling of a Bernoulli distribution $\\mathcal{B}(\\,n, \\lambda\\,)$ with $n\\rightarrow \\infty$ and $\\lambda \\rightarrow 0$** in Section 3.1, page 4. In the subsequent use of this assumption we did not explicitly state it, causing inconvenience in understanding them. We modify the expression in Section 3.1 and Section 3.3 to improve the readability of the paper. The modifications to Section 3.1 and above for equation (16) in Section 3.3 are as follows, respectively:\n\n  > **In section 3.1:**\n  >\n  > Hence we assume that the summary graph of the coupling is a sampling of a Bernoulli distribution $\\mathcal{B}(n, \\lambda)$ with $n\\rightarrow \\infty$ and $\\lambda \\rightarrow 0$.\n  > \n  > \n\n  >\n  > **In Section 3.3:** \n  > Since each variable in $\\mathbf{S}$ is affected by $\\tilde{\\mathbf{A}}$ in Eq.(6), we have $s\\_{i} \\mid \\tilde{\\alpha}\\_{i} \\sim \\mathcal{N}\\left(\\tilde{\\alpha}\\_{i} * \\mu\\_{i}, \\tilde{\\alpha}\\_{i} * \\sigma\\_{i}^{2}\\right)$. Besides, each element in Gaussian graph is conditioned on the Binomial variable for the same edge of the summary graph. Hence The $\\mathrm{KL}$ term can be further written as:\n  >\n  > \u200b$$\\begin{aligned}  &\\sum_{\\mathrm{i}}^{\\rm{N_{b}}}\\Bigg \\\\{ \\\\mathrm{KL}\\bigg(\\mathcal{B}(n, \\lambda_{\\mathrm{i}}) \\| \\mathcal{B}\\left(n, \\lambda_{\\;\\mathrm{i}}^{(0)}\\right)\\bigg)    \\\\\\\\ \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad +\\mathbb{E}\\_{\\mathrm{\\tilde{\\alpha}}\\_{\\mathrm{i}}}\\bigg[\\mathrm{KL}\\bigg( \\mathcal{N}&\\left(\\tilde{\\alpha}\\_{\\mathrm{i}} * \\mu\\_{i},\\  \\tilde{\\alpha}\\_{\\mathrm{i}} * \\sigma\\_{\\mathrm{i}}^{2}\\right)) \\| \\mathcal{N}\\Big(\\tilde{\\alpha}\\_{\\mathrm{i}} * \\mu\\_{i}^{(0)}, \\tilde{\\alpha}\\_{\\mathrm{i}} * \\sigma\\_{\\;\\mathrm{i}}^{(0)^{\\,2}}\\Big)\\bigg)\\bigg]\\Bigg\\\\}\n   \\end{aligned}$$"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699894219766,
                "cdate": 1699894219766,
                "tmdate": 1699894219766,
                "mdate": 1699894219766,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FujnrjYTKH",
            "forum": "8oZf2SlXEY",
            "replyto": "8oZf2SlXEY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6645/Reviewer_MVEs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6645/Reviewer_MVEs"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the Bayesian graph relation inference for a few-shot learning, addressing the problem of distance base methodologies lacking in redundant information about the relationships between classes. The ultimate goal is to makes a multi relation graphs for few-shot learning. They used conventional and well-known variational inferences, also shows the success in the skin-cancer datasets in the prediction accuracy and interpretation in some sense. Overall, the contribution is incremental, and experiments are limited to skin-cancer datasets. Obviously, the proposed algorithm can have advantages in a few shot and imbalanced datasets cooperated with Bayesian approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The use of variation inference is relatively simple cooperated with simple graphs. The use of multi view graphs can be attractive approach such as the idea of many filters or multi-heads. The interpretabilty plays an important role to validate the proposed algorithm. Experiments follows the standard routine in machine learning, compared to base-line algorithms. Baes-line algorithms seem appropriate in the scope of distance-based few-shot learning. The algorithm provides gains in the performance and interpretability."
                },
                "weaknesses": {
                    "value": "The main weakness is limited experiments. The task of skin-cancer classification is famous and important. However, there are other datasets concerning imageNet, Food-101 and so on. If required, we can generate the imbalanced datasets from the original dataset. More generalization and advantages are widely explored in the various fields, and if possible, the conventional algorithms for few-show learning can be added to baseline algorithms (cannot be fair, but it can provide some implications). The presentation is not kind for the reader, especially not familiar with few-shot learning and mathematical formulation of variation inference (maybe there are many omissions in the math expression)."
                },
                "questions": {
                    "value": "Q1: Please clarify the notation $ \\mathcal{B}(n, \\tilde{\\lambda}_i)$ in the equation (16) since there is no definition. \n\nQ2: In equation (15), I cannot understand the $\\\\sum_{i=1}^M$ since there is not explicit $i$ in arguments of $p$ and $q.$ Please clarify this issue.\n\nQ3: Can you clarify the combination strategy of multi-view graphs? Are the strategies different between training and test steps? Can you have other strategies for the combination of multi-vie graphs?\n\n\nQ4: Considered graphs are limited since there is no connections between baseline classes. What\u2019s your arguments about this problem."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807394240,
            "cdate": 1698807394240,
            "tmdate": 1699636758981,
            "mdate": 1699636758981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8fyTQTPWzL",
                "forum": "8oZf2SlXEY",
                "replyto": "FujnrjYTKH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Main Responses to Reviewer MVEs  (1/3)"
                    },
                    "comment": {
                        "value": "First of all, we would like to thank the reviewer for their encouragement to our work. In this work, we propose a calibration distribution method based on Bayesian relation reference method for few-shot classification task. The highlight of the proposed model is that it **achieves better performance** compared to the SOTA few-shot learning algorithms, while visual analysis proves that the proposed model **has a certain degree of interpretability**, which is useful especially for medical scenarios such as dermatological classification.\n\nThanks very much for pointing out the weaknesses of the article. It helps us to better improve the deficiencies in our paper. We summarize the weaknesses and the improvements based on these weaknesses as follows:\n\n- ***W1:*** *The model can be experimented on datasets other than the dermatology dataset.* \n\n  ***I1:*** Thank you for your constructive comment. The proposed Bayesian distribution calibration model is a generic model which can achieve satisfying performance on various datasets. We chose the Dermnet dataset for our experiments due to the fact that the model has a certain degree of interpretability, which is more important for medical scenarios. To further validate the performance of the model, we will experiment the proposed model on other datasets. Due to time as well as arithmetic issues, **we choose the MiniImagenet dataset for the additional experiments**, and the experiment result will be published in the supplementary material, thank you!\n\n- ***W2:*** *If required, we can generate the imbalanced datasets from the original dataset. More generalization and advantages are widely explored in the various fields, and if possible, the conventional algorithms for few-show learning can be added to baseline algorithms.*\n\n  ***I2:*** Thank you for your suggestions for experiment. Adding the conventional algorithms for few-show learning to baseline algorithms is a fascinating experiment that can provide some implications. **We will compare the performance of the proposed Bayesian distributional calibration method with the conventional Resnet18 model on Dermnet dataset,** and the experiment result will be published in the supplementary material soon, thank you!\n\n- ***W3:*** *The presentation is not kind for the reader, especially not familiar with few-shot learning and mathematical formulation of variation inference.*\n\n  **I3:** Thank you for pointing this out, which is an oversight in our writing. We made extensive revisions to the Method section (Section 3 *Bayesian Relational Inference* on page 3~7) of the manuscript to allow the reader to understand the model details more intuitively. Specifically, we made the following revisions to the Method section:\n\n  - For some variables, **we indicate their dimension**. The description of the variable is modified to be similar to the following form:\n\n    > $\\\\boldsymbol{N_{t}} \\\\in \\\\mathbb{R}^{\\\\rm{N_{f}}}$ represents the target node embedding, $\\\\boldsymbol{N_{b}}\\\\in \\\\mathbb{R}^{\\\\rm{N_{b}} \\\\times \\\\rm{N_{f}}}$ represents the base node embeddings\n\n  - **We rewrite the assumptions related to the Bernoulli distribution as well as the computational part of the paper**, and the physical meanings represented by variables are more precise. The revised presentation is similar to the follow form:\n\n    > $\\\\mathrm{L}\\_{\\\\mathrm{mean}}(\\\\cdot)$ and $\\mathrm{L}\\_{\\mathrm{std}}(\\cdot)$ are implemented by neural networks for estimating the mean and standard deviation of the Gaussian distribution $\\mathcal{N}\\left(\\mu_{i},\\ \\sigma_{\\mathrm{i}}^{2}\\right)$ which can calculate the approximation of the Bernoulli distribution $\\mathcal{B}(n, \\lambda_{\\mathrm{i}})$ with $n\\rightarrow \\infty$ and $\\lambda_{\\mathrm{i}} \\rightarrow 0$\n\n  - **We rewrite the ill-expressed formulas.** For example:\n\n    > \u200b   $$\\\\begin {equation} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\\\widetilde{\\\\alpha}\\_{\\mathrm{i}}= \\sqrt{(m\\_{\\mathrm{i}} \\times (1.0 - m\\_{\\mathrm{i}}))} \\times \\varepsilon\\_{\\mathrm{i}}+m\\_{\\mathrm{i}}      \\tag{6} \\end{equation}$$\n\n  - For the theorems used in the paper we proof them in the **supplementary material A.3.**"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890226427,
                "cdate": 1699890226427,
                "tmdate": 1699895013242,
                "mdate": 1699895013242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bABNu5QuK0",
                "forum": "8oZf2SlXEY",
                "replyto": "FujnrjYTKH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Main Responses to Reviewer MVEs (2/3)"
                    },
                    "comment": {
                        "value": "Thanks very much for the suggestions and questions raised by the reviewer. We answer the questions raised by the reviewer below:\n\n- ***Q1:** Please clarify the notation $\\mathcal{B}(n, \\tilde{\\lambda}_{\\mathrm{i}})$ in the equation (16) since there is no definition.*\n\n  **A1:** Thanks for pointing out this issue. We did not clarify this  Bernoulli distribution clearly in the paper previously. This is an assumption for summary graph mentioned in Section 3.1, where we assume that **each edge of the summary graph is a sampling of a Bernoulli distribution $\\mathcal{B}(n, \\lambda)$ with $n\\rightarrow \\infty$ and $\\lambda \\rightarrow 0$.** We **modify the expression in Section 3.1** to explicitly state the assumptions associated with that Bernoulli distribution. Besides, **we explicitly state the basis for conversion to these two KL terms above equation (16) in Section 3.3.** The modifications to Section 3.1 and above for equation (16) in Section 3.3 are as follows, respectively:\n\n  > **In section 3.1:**\n  >\n  > Hence we assume that the summary graph of the coupling is a sampling of a Bernoulli distribution $\\mathcal{B}(n, \\lambda)$ with $n\\rightarrow \\infty$ and $\\lambda \\rightarrow 0$.\n  > \n  > \n\n  >\n  > **In Section 3.3:** \n  > Since each variable in $\\mathbf{S}$ is affected by $\\tilde{\\mathbf{A}}$ in Eq.(6), we have $s\\_{i} \\mid \\tilde{\\alpha}\\_{i} \\sim \\mathcal{N}\\left(\\tilde{\\alpha}\\_{i} * \\mu\\_{i}, \\tilde{\\alpha}\\_{i} * \\sigma\\_{i}^{2}\\right)$. Besides, each element in Gaussian graph is conditioned on the Binomial variable for the same edge of the summary graph. Hence The $\\mathrm{KL}$ term can be further written as:\n  >\n  > \u200b$$\\begin{aligned}  &\\sum_{\\mathrm{i}}^{\\rm{N_{b}}}\\Bigg \\\\{ \\\\mathrm{KL}\\bigg(\\mathcal{B}(n, \\lambda_{\\mathrm{i}}) \\| \\mathcal{B}\\left(n, \\lambda_{\\mathrm{i}}^{(0)}\\right)\\bigg)    \\\\\\\\ \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad +\\mathbb{E}\\_{\\mathrm{\\tilde{\\alpha}}\\_{\\mathrm{i}}}\\bigg[\\mathrm{KL}\\bigg( \\mathcal{N}&\\left(\\tilde{\\alpha}\\_{\\mathrm{i}} * \\mu\\_{i},\\  \\tilde{\\alpha}\\_{\\mathrm{i}} * \\sigma\\_{\\mathrm{i}}^{2}\\right)) \\| \\mathcal{N}\\Big(\\tilde{\\alpha}\\_{\\mathrm{i}} * \\mu\\_{i}^{(0)}, \\tilde{\\alpha}\\_{\\mathrm{i}} * \\sigma\\_{\\mathrm{i}}^{(0)^{2}}\\Big)\\bigg)\\bigg]\\Bigg\\\\}\n   \\end{aligned}$$\n\n- ***Q2:*** *In equation (15), I cannot understand the $\\sum_{i=1}^{M}$ since there is not explicit $i$ in arguments of $p$ and $q$. Please clarify this issue.*\n\n  **A2:** Thank you for raising this issue. $M$ is the number of Gaussian relation graphs generated in one batch. We add to our paper the note on the meaning of $M$. Equation (15) is now as follow:\n\n  > \u200b$$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\sum\\_{i=1}^{M}\\bigg(\\mathrm{KL}\\left(q\\left(\\tilde{\\mathbf{A}}\\_{i}, \\mathbf{S}\\_{i} \\mid \\mathbf{N}\\_{\\mathrm{t}}, \\mathbf{N}\\_{\\mathrm{1}: \\mathrm{n\\_{b}}}\\right) \\|  p\\left(\\tilde{\\mathbf{A}}\\_{i}, \\mathbf{S}\\_{i} \\mid \\mathbf{N}\\_{\\mathrm{t}}, \\mathbf{N}\\_{\\mathrm{1}: \\mathrm{n\\_{b}}}\\right)\\right)\\bigg)$$"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893367522,
                "cdate": 1699893367522,
                "tmdate": 1699895090381,
                "mdate": 1699895090381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TCIaxwtwMh",
                "forum": "8oZf2SlXEY",
                "replyto": "FujnrjYTKH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiment Result of Conventional Model on Dermnet Dataset"
                    },
                    "comment": {
                        "value": "In few-shot learning algorithms, training a model using conventional algorithms can be difficult due to the large number of categories and the fact that the data for most of the categories is scarce. We divide the test set of the Dermnet dataset in a ratio of 8:2 into a new training and test set. Then we generate a conventional algorithm model by replacing the few-shot classification part of the Bayesian relational inference model( Multi-view Gaussian graph generation component and logistic regression classifier) with a linear classification head. We freeze the Bayesian relation inference module and fine-tune the classification head on the new training set and test it on the new test. We compare the accuracy of this algorithm with that of the few-shot learning algorithms on the 5-way 1-shot task. The experiment result is shown as follow:\n\n\n\n| Method                          | Acc(%)    |\n| ------------------------------- | --------- |\n|   MAML                            | 44.05     |\n|   PN                              | 43.76     |\n|   MN                              | 44.23     |\n| DC                              | 48.99     |\n| tSF                             | 49.38     |\n| GAP                             | 48.92     |\n| *BDC + Conventional Algorithms* | *43.50*   |\n| **BDC (few-shot)**              | **50.59** |\n\nSpecifically, we adopt a three-layer artificial neural network as the linear classification head of the conventional algorithm, trained for 1500 epochs using the Adam optimizer with the learning rate of 0.0008. The result shows that the conventional algorithm's accuracy is **similar to that of the early few-shot algorithms on the 5-way 1-shot task**. It is worth noting that traing the conventional algorithm is **time-consuming** and overall performs less well than the few-shot algorithms."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406638625,
                "cdate": 1700406638625,
                "tmdate": 1700406638625,
                "mdate": 1700406638625,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]