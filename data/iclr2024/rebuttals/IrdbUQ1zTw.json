[
    {
        "title": "Reinforcement Learning-based Layer-wise Aggregation for Personalized Federated Learning"
    },
    {
        "review": {
            "id": "NGfTwK7ECI",
            "forum": "IrdbUQ1zTw",
            "replyto": "IrdbUQ1zTw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4696/Reviewer_M4kH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4696/Reviewer_M4kH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a reinforcement learning-based layerwise aggregation method (pFedRLLA) that applies different mechanisms for different neural network layers. pFedRLLA leverages different aggregation methods for different layers of neural networks, allowing to learn a better representation while ensuring model personalization. The authors conduct extensive experiments to validate the proposed methods which reach state-of-the-art performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper investigates the personalized federated learning problem which is a hot and important topic. Utilizing reinforcement strategy in PFL is also interesting.\n\n- The paper is well-organized and easy to understand. The figures are clear for illustrations.\n\n- The authors conduct extensive experiments to validate the proposed methods which reach state-of-the-art performance."
                },
                "weaknesses": {
                    "value": "- The authors adopt $\\beta$ as the hyper parameters on different rewards. However, how to properly choose $\\beta$? In my opinion, it should be discussed in the paper.\n\n- Some similar paper should be discussed in the main paper, e.g., [1]\n\n- Some information is missing in the main paper, e.g. the name of Alg.2 is missing.\n\nRef:\n[1] Zhu, Suxia, Tianyu Liu, and Guanglu Sun. \"Layer-Wise Personalized Federated Learning with Hypernetwork.\" Neural Processing Letters (2023): 1-15."
                },
                "questions": {
                    "value": "Please refer to the weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4696/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4696/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4696/Reviewer_M4kH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698239279752,
            "cdate": 1698239279752,
            "tmdate": 1699636451337,
            "mdate": 1699636451337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OpliVcYs1P",
                "forum": "IrdbUQ1zTw",
                "replyto": "NGfTwK7ECI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M4kH"
                    },
                    "comment": {
                        "value": "# Answer for Q1\nThank you very much for pointing this out: it is our negligence that we had not explained the tuning of $\\beta_i$ in the paper. This was carried through experimentation, and we have adopted default parameters {1,1,2} (listed before (1a) on page 6 in the revised paper) for all experiments. \nAdditionally,  we have included Table 5 in Appendix F.1 (page 14) to show the ablation study for choosing the weights for the three reward components. We also include here for your convenience: \n\n|var.||$\\beta_1$|||$\\beta_2$|||$\\beta_3$||base|\n|-|-|-|-|-|-|-|-|-|-|-|\n|$\\beta_1$-$\\beta_2$-$\\beta_3$|0-1-2|5-1-2|10-1-2|1-0-2|1-5-2|1-10-2|1-1-0|1-1-5|1-1-10|1-1-2|\n|acc|69.22|70.95|70.85|69.28|69.38|69.34|69.54|71.14|70.95|70.84|\n\nThis is the process of pre-training the DRL agent using different hyperparameter combinations (detailed in Appendix D). The experimental setting is to make `CIFAR-10 s=5`.\n\nFrom this, it can be seen that if any coefficient for the three reward components $r_1, r_2, r_3$, equals 0, this leads to the lowest accuracy; this corroborates using all three. In addition, when varying a single weight, further gains over default choices (base) can be obtained for a specific setting (`CIFAR-10 s=5` was used here) in the case of $\\beta_1,\\beta_3$. For fair reporting of results, we adopt a common setting {1,1,2} in all experiments (for all datasets and heterogeneity levels). \n\n# Answer for Q2\nThank you very much for suggesting this reference. We have listed and discussed this paper in the last paragraph of section 2 (page 3). We also include here for ease of your inspection: \n\npFedLHN [1] employs layer-wise modules with parameter-sharing mechanisms in the hypernetwork to create personalized models for each client. The hypernetwork generates these personalized models by utilizing layer embeddings uploaded by the clients themselves and does not rely on learning from other clients. Moreover, uploading these layer embeddings to the server introduces additional communication.\n\n**Ref**:\n[1] Zhu, Suxia, Tianyu Liu, and Guanglu Sun. \"Layer-Wise Personalized Federated Learning with Hypernetwork.\" Neural Processing Letters (2023): 1-15.\n\n# Answer for Q3\nThank you for your useful feedback on our paper. We have named Alg. 2 as \"Layer-wise aggregation\", and the default parameters for $\\beta_i$ are now listed in the main paper (on page 6, before (1a)). The details for pre-training the DRL agent have been included in Appendix D (page 13). \n\nWe sincerely appreciate the time you took to review our paper and provide us with helpful suggestions!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573889703,
                "cdate": 1700573889703,
                "tmdate": 1700573889703,
                "mdate": 1700573889703,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mzDiH3RGTl",
                "forum": "IrdbUQ1zTw",
                "replyto": "OpliVcYs1P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4696/Reviewer_M4kH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4696/Reviewer_M4kH"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. It effectively addresses my concerns. I will maintain my positive rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630852052,
                "cdate": 1700630852052,
                "tmdate": 1700630852052,
                "mdate": 1700630852052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5A8SWHh8yS",
            "forum": "IrdbUQ1zTw",
            "replyto": "IrdbUQ1zTw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4696/Reviewer_qRiv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4696/Reviewer_qRiv"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a personalized federated learning method based on reinforcement learning, which aggregates the local models in a layer-wise manner. In the method, the body part of the model  (i.e., shared knowledge) is aggregated according to the local data size. Also, the head part of the model (i.e., personalized knowledge) is aggregated by using the personalized aggregation weights that are generated by deep reinforcement learning."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed algorithm uses multi-head autoencoder for dimension reduction instead of PCA, which makes it more practical. The source code of the paper is provided."
                },
                "weaknesses": {
                    "value": "For personalized federated learning, plenty of works have been studied using a variety of learning methods such as knowledge distillation, meta learning, transfer learning, etc., and including reinforcement learning. Considering this, the concept of the paper is not sound and novel, and its contribution seems marginal compared with the conventional federated learning methods with reinforcement learning and multi-armed bandits."
                },
                "questions": {
                    "value": "1. The main concepts of the paper such as layer-wise aggregation, the use of reinforcement learning for model weighting, and dimension reduction have been already considered in federated learning. Please clarify the contribution of the paper compared with the related works, currently, it seems that the contribution is marginal.\n\n2. In a similar context, it is not clear why reinforcement learning is suitable for personalization of federated learning. Learning another deep neural network for DRL agent may incur significant costs. Please justify the rationale of using DRL agent for personalized federated learning.\n\n3. Typically, a lot of experiences is required to learn a DRL policy. In federated learning, is there an enough number of rounds that the DRL policy to be converged? Or, should the DRL policy be trained in advance?\n\n4. How is the multi-head autoencoder trained when using the proposed algorithm? Is it should be trained in advance? This should be clarified in the paper.\n\n5. If the DRL policy or autoencoder should be trained in advance, it should be clarified how the dataset for training them can be obtained before applying federated learning.\n\n6. For a limited number of rounds, it may be doubted that considering accuracy-related rewards (i.e., $r_1$ and $r_2$) will be effective or not. Providing the ablation study of $r_1$, $r_2$, $r_3$ would be helpful to understand the effect of the weights according to the reward structure.\n\n7. It seems that the reward in Eq. (1d) is structured so that the weights closer to the similarity vector have the smaller reward. Hence, it encourages  the weight of the similar client become smaller. But, in my understanding, the concept of the proposed algorithm should encourage the weight of the similar client become larger for personalization.\n\n8. It seems that the weights $p_k$'s from the DDPG should be constrained as to be 1 in sum. How is this realized in the algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642265818,
            "cdate": 1698642265818,
            "tmdate": 1699636451256,
            "mdate": 1699636451256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NbOnrKILX8",
                "forum": "IrdbUQ1zTw",
                "replyto": "5A8SWHh8yS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qRiv (1/2)"
                    },
                    "comment": {
                        "value": "# Answer for Q1\nThank you for this valuable suggestion. You are right that the novelty of the proposed algorithm does not lie on the tools invoked, but rather on the succinct design of a new PFL framework with emphasis on scalability at the server end and no overhead on the user end. \n\nIn specific, this is accomplished by a) adopting a single DRL agent (for all participants), b) using a compound reward that balances both cross-user similarities and local accuracy improvement, and c) obtaining model embeddings with a single MHAE block to alleviate costs and boost the performance of the DRL weight inference. In addition, we have demonstrated through numerous experiments that a simple and efficient layer-wise approach (body and head) is effective in this framework.  \n\nIn our introduction section, we have provided a clearer explanation after the contributions (page 2), of the novelty of our work. We would like to point out that these choices significantly differ from existing methods described in the literature (i.e., rather than simply combining choices from various personalized algorithms, we have utilized standard tools to develop a novel solution for PFL), as elaborated next:\n\na. **Reinforcement Learning**: first, we adopt *a single* DRL agent (for all participants), enabling higher scalability. Second, we use a compound reward that balances accuracy and model similarities. The design of rewards is explicit, which allows a faster convergence. Third, our DRL task is formulated as a single-step task, which not only provides a more pronounced reward signal but also simplifies the collection of training data.\n\nb. **Dimension reduction**: we employ a multi-head autoencoder (MHAE) (i.e., that MHAE tries to get relationships between different layers of the same network). Besides, adopting a single MHAE block alleviates costs and boosts the performance of DRL weight inference.\n\nc. **Layer-wise aggregation**: previous works [3, 4, 5] only considered similarity of user models. In comparison, we considered not only similarity but also the data volume based on the simple observation [1,2] that the body aims to learn domain-specific features while the head task-specific features. Doing this allows us to get a better feature extractor (improve model generalization and capture data diversity and complexity) and a more customized classifier (give us a faster convergence speed) efficiently.\n\n**Refs**:\n\n[1] Decoupling representation and classifier for long-tailed recognition.\n\n[2] The Devil is the Classifier: Investigating long tail relation classification with decoupling\nanalysis.\n\n[3] Layer-Wise Personalized Federated Learning with Hypernetwork.\n\n[4] Personalized Federated Learning with Feature Alignment and Classifier Collaboration.\n\n[5] Layer-wised Model Aggregation for Personalized Federated Learning.\n\n# Answer for Q2\nThank you for raising an important point about the use of DRL in the proposed algorithm. In the revised paper we have listed all the details in Appendix D (page 13).\n\nOur DRL agent is pre-trained. Then it is further finetuned simultaneously with the federated process (and the same holds true for the auto-encoder). During each iteration of the fine-tuning process, we will sample 32 data instances from the replay buffer to finetune the DRL agent every 10 rounds. For the MHAE, we conduct fine-tuning every 10 rounds. We list the details of pre-train in the appendix D (page 13) and fine-tuning in the appendix E (page 14).\n\nIn our method, the additional overhead on the server side mainly comes from the DRL agent and MHAE (other operations like updating storage and computing weights based on data volumes have minimum overhead). We demonstrate the costs in the following setting: `CIFAR-10 s=5, 100 users, global_round=500, local_epoch=5, batch_size=64`. In this experiment, we calculated the average time by dividing the total time by 500, which encompasses the local training time (2.37s), DRL time (0.16s), and MHAE time (0.94s). The latter two times are composed of inference time and fine-tuning time. A detailed analysis is presented below:\n\n- The total cost at the server is just 46% of the cost of local training: 85% for MHAE (70% for inference and 30% for fine-tuning), 15% for DRL agent (12% for inference and 3% for fine-tuning).\n\nThe above analysis shows that compared with local training time, the computation costs caused by the DRL agent are minimal. The MHAE incurs a significant cost, but it is not a core component. We can use other efficient dimension-reduction methods. In addition, in a non-simulated environment, fine-tuning of the DRL agent and MHAE can be parallel-processed, i.e., when the server is idle (selected clients do local training, and the server is waiting for updated models)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572785118,
                "cdate": 1700572785118,
                "tmdate": 1700572785118,
                "mdate": 1700572785118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mWn1neQhLe",
                "forum": "IrdbUQ1zTw",
                "replyto": "5A8SWHh8yS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qRiv (2/2)"
                    },
                    "comment": {
                        "value": "# Answer for Q3 & Q4 & Q5\nThank you very much for these questions. In all our experiments the DRL agent and the MHAE are pre-trained and fixed for all experiments (**answer for Q3 and Q4**). This was carried in the following setting: For each dataset, we pre-train a DRL agent, which can be used in different heterogeneous scenarios of the corresponding dataset. It is important to note that the pre-training setting differs entirely from the experimental settings employed in our paper, which has been discussed in detail in the revised paper in Appendix D (first paragraph, on page 13). When the pre-trained model is in use, we fine-tune it simultaneously with the federated process, as explained in the detailed response to Q2.\n\n**Answer for Q5**, due to space reasons, we do not show the detailed process of pre-training here. We have discussed this in detail in Appendix D (page 13).\n\nAs our DRL approach is solely associated with the model employed (the embedding of model parameters used as the state), it is independent of the task itself. therefore, we further study a scenario where pre-training is carried on a different dataset than the one tested. We have discussed this in detail in Appendix F.2 (page 15).\n\n||CIFAR-100||Omniglot|MNIST*|\n|-|-|-|-|-|\n||$s$=5|$\\alpha$=0.1|-|-|\n|new|$\\underline{74.53}$|53.01|$\\underline{47.74}$|$\\underline{73.90}$|\n|previous|**76.06**|**53.79**|**48.43**|**74.58**|\n|best baseline|73.62|53.21|45.36|73.88|\n\n# Answer for Q6\nThank you for your question. You are right that the choices for the reward are worthy of a detailed discussion, which we include in the updated paper in Appendix F.1 (page 14). We also include here for your convenience: \n\n|var.||$\\beta_1$|||$\\beta_2$|||$\\beta_3$||base|\n|-|-|-|-|-|-|-|-|-|-|-|\n|$\\beta_1$-$\\beta_2$-$\\beta_3$|0-1-2|5-1-2|10-1-2|1-0-2|1-5-2|1-10-2|1-1-0|1-1-5|1-1-10|1-1-2|\n|acc|69.22|70.95|70.85|69.28|69.38|69.34|69.54|71.14|70.95|70.84|\n\nThis is the process of pre-training the DRL agent using different hyperparameter combinations (detailed in Appendix D). The experimental setting is to make `CIFAR-10 s=5`.\n\nFrom this, it can be seen that if any coefficient for the three reward components $r_1, r_2, r_3$, equals 0, this leads to the lowest accuracy; this corroborates using all three. In addition, when varying a single weight, further gains over default choices (base) can be obtained for a specific setting (`CIFAR-10 s=5` was used here) in the case of $\\beta_1,\\beta_3$. For fair reporting of results, we adopt a common setting {1,1,2} in all experiments (for all datasets and heterogeneity levels). \n\n# Answer for Q7\nThank you for your attentiveness. You are absolutely right: there was a typo in (1d) (- sign missing), for which we apologize and have corrected and explained before (2) in the revised paper (on page 6).\n\n# Answer for Q8\nThank you for pointing this out, which we should have explained in the  revised paper (on page 6, paragraph 2). We apply a softmax activation function to the final layer of our network, which normalizes the outputs to add up to 1 as follows: $softmax(p_{k}) = \\frac{exp(p_k)}{\\sum exp(p_k)}$"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573283812,
                "cdate": 1700573283812,
                "tmdate": 1700622686501,
                "mdate": 1700622686501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ERepWz2qhx",
                "forum": "IrdbUQ1zTw",
                "replyto": "mWn1neQhLe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4696/Reviewer_qRiv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4696/Reviewer_qRiv"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the great effort the authors have made to respond and make revisions. I believe the paper is quite improved, but the most significant concern is still not addressed.\n\nI strongly disagree that the training cost for the DRL agent and MHAE is negligible because they are pretrained. Typically, a pretrained model implies a model that can be used for different datasets/tasks with only simple fine-tuning. However, what the authors did in the experiments can be seen as a preparation stage of the proposed approach, in which the DRL agent and MHAE are trained before applying the proposed approach, rather than building a pretrained model. (Note that if someone tries to apply the proposed approach to a new real-world application, typically, he/she cannot train the DRL agent and MHAE as the authors did because there is no such known scenario of the application). In Appendix F.2, the results of using the DRL agent pretrained with a different dataset are presented, but it does not seem to be as effective. Furthermore, the different architectures of neural networks require different MHAEs for the proposed approach. This makes it difficult to use a pretrained MHAE in a variety of applications. \n\nIn summary, I still have serious concerns about the cost of the proposed approach, and this still significantly degrades the contributions of the work. Therefore, I maintain my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582287403,
                "cdate": 1700582287403,
                "tmdate": 1700582287403,
                "mdate": 1700582287403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ARaJ8J8FKp",
            "forum": "IrdbUQ1zTw",
            "replyto": "IrdbUQ1zTw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4696/Reviewer_54Ry"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4696/Reviewer_54Ry"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces pFedRLLA, a reinforcement learning-based layer-wise aggregation method designed to tackle the challenge of statistical heterogeneity in personalized federated learning. The approach applies different aggregation strategies to different layers of neural networks. The feature extracting layers (body) use weights proportional to the data sizes of the clients, while the heads employ a deep reinforcement learning (DRL) agent to generate personalized aggregation weights. The DRL agent considers compound rewards that take into account both the improvement of validation accuracy and the similarity between clients. Besides, to reduce the state space, a multi-head auto-encoder is utilized to obtain low-dimensional embeddings of user models.\n\nExperimental results on benchmark datasets with varying levels of data heterogeneity demonstrate the effectiveness of the proposed method. It outperforms the baselines in terms of accuracy and convergence speed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper presents a novel approach that utilizes reinforcement learning (specifically, DDPG) to tackle the task of learning aggregation weights in the classifier heads. This approach seems to offer a fresh perspective.\n\n2. The proposed method surpasses existing approaches in addressing data heterogeneity in personalized federated learning. The results demonstrate higher accuracy achieved in less time."
                },
                "weaknesses": {
                    "value": "1.The authors claim that the main novelty lies in the layer-wise design in section 2 (related work). However, the idea of separating the body and head for aggregation is not entirely novel, as seen in approaches like pFedLA, which even achieves a finer-grained layer-wise aggregation using the power of hypernetworks. In comparison, this paper only roughly separates the body and head, and cannot be considered truly layer-wise like pFedLA. Additionally, the idea of separating aggregation for the body and head has also been proposed in FedPAC, with a more detailed mathematical logic. Besides, the weighted averaging based on data volume for body aggregation is trivial and straightforward.\n\n2. Although the experimental results demonstrate the superior performance of the pFedRLLA framework, there is a lack of theoretical analysis explaining why RL-based methods work better. It seems that the paper provides more intuitive reasoning.\n\n3. In terms of clarity, the paper occasionally fails to clearly differentiate between crucial and supplementary information. For instance, the descriptions accompanying the figures and tables, such as Figure 1 and Table 2, are excessively long, while the corresponding text in the main body appears relatively weaker. The steps mentioned below Figure 1 are relatively concise, but the figure description provides excessive detail, also including redundant content."
                },
                "questions": {
                    "value": "1. In the reward design, the hyperparameter \"beta\" is not further explained, and it appears that in the code it is simply set as {1, 1, -2}. Regarding the weight allocation for r1, r2, and r3 (the compacts of validation accuracy and similarity), it is unclear if there is a more in-depth consideration. By the way, it would be better to add a negative sign to r3 to maintain consistency with r1 and r2.\n\n2. Will there be any theoretical analysis provided to explain why RL-based methods outperform existing pFed methods? Besides DDPG, have you attempted other RL methods, and why is DDPG the preferred choice?\n\n3. Regarding clarity, there is room for improvement about the issues raised in Weakness3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4696/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4696/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4696/Reviewer_54Ry"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847726559,
            "cdate": 1698847726559,
            "tmdate": 1699636451163,
            "mdate": 1699636451163,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5lxHgP6uIY",
                "forum": "IrdbUQ1zTw",
                "replyto": "ARaJ8J8FKp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4696/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Answer for Weakness1\nThank you very much for these useful comments. We have re-written the related parts in the introduction (page 2) to explain the novelty in our approach which does not lie on the used tools per se, but rather on the design of the framework (targeting scalability on the server end and no additional burden to the users). To that end, through extensive experiments, we have tried to demonstrate that the simplest possible layer-wise approach (body & head) is efficient and suffices to yield superior accuracy over numerous baselines. \n\nRegarding layer-wise aggregation, pFedLA [3] only considered the similarity of user models. In contrast, we considered not only similarity but also the data volume based on the simple observation [1,2] that the body aims to learn domain-specific features while the head task-specific features. This allows us to get a better feature extractor (improve model generalization and capture data diversity and complexity) and a more customized classifier (a faster convergence speed is attained). In FedPAC [4], the computation and transmission of feature centroids introduce additional computational and communication costs, both on the server and client sides. Moreover, its applicability is limited to scenarios involving label distribution shift. In contrast, our algorithm imposes no additional computational or communication burdens on the client side.\n\n**Refs**:\n\n[1] Decoupling representation and classifier for long-tailed recognition.\n\n[2] The Devil is the Classifier: Investigating long tail relation classification with decoupling analysis.\n\n[3] Personalized Federated Learning with Feature Alignment and Classifier Collaboration.\n\n[4] Layer-wised Model Aggregation for Personalized Federated Learning.\n\n# Answer for Q1 \nThank you very much for your question. First, you are absolutely right: there was a typo in (1d) (- sign missing), for which we apologize and have corrected and explained before (2) in the revised paper.\n\nIt was also our negligence that we had not explained the tuning of $\\beta_i$ in the paper. This was carried through experimentation, and we have adopted default parameters  {1,1,2} (listed before (1a) on page 6 in the revised paper) for all experiments. \nAdditionally,  we have included Table 5 in Appendix F.1 (page 14) to show the ablation study for choosing the weights for the three reward components. We also include here for your convenience: \n\n|var.||$\\beta_1$|||$\\beta_2$|||$\\beta_3$||base|\n|-|-|-|-|-|-|-|-|-|-|-|\n|$\\beta_1$-$\\beta_2$-$\\beta_3$|0-1-2|5-1-2|10-1-2|1-0-2|1-5-2|1-10-2|1-1-0|1-1-5|1-1-10|1-1-2|\n|acc|69.22|70.95|70.85|69.28|69.38|69.34|69.54|71.14|70.95|70.84|\n\nThis is the process of pre-training the DRL agent using different hyperparameter combinations (detailed in Appendix D). The experimental setting is to make `CIFAR-10 s=5`.\n\nFrom this, it can be seen that if any coefficient for the three reward components $r_1, r_2, r_3$, equals 0, this leads to the lowest accuracy; this corroborates using all three. In addition, when varying a single weight, further gains over default choices (base) can be obtained for a specific setting (`CIFAR-10 s=5` was used here) in the case of $\\beta_1,\\beta_3$. For fair reporting of results, we adopt a common setting {1,1,2} in all experiments (for all datasets and heterogeneity levels). \n\n# Answer for Q1 & Weakness2\n**Answer 2.1**: Unfortunately, our proposed method relies solely on data-driven techniques for which it is especially challenging to provide theoretical guarantees (in the absence of a commonly adopted model). Instead, we have resorted to extensive experimentation over heterogeneous settings to corroborate the efficacy and robustness of the proposed solution.  This line of research has proliferated in the FL literature [1, 2, 3].\n\n**Answer 2.2**: You are absolutely right that other RL methods (e.g., PPO, TRPO) can be used in the proposed framework. We opted for DDPG (the reason is clarified in the first paragraph of section 3.2.1) and have not tested with any other alternatives. We appreciate the reviewer's understanding that this would not be feasible in the rebuttal period, however, we explicate the choice in the revised paper in Appendix C (last paragraph, on page 13).\n\n**Refs**:\n\n[1] FedFormer: Contextual Federation with Attention in Reinforcement Learning.\n\n[2] Layer-Wise Personalized Federated Learning with Hypernetwork.\n\n[3] Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints.\n\n# Answer for Q3 and Weakness 3\nThank you for your suggestion. We fully agree that the descriptions were excessively long (in an attempt to summarize the content in one place for easier inspection). In the revised paper, we have shortened the caption of Figure 1 (page 4) by 40% by removing the unnecessary content. Regarding Table 2 (page 8), we also shortened the description by 50%."
                    },
                    "title": {
                        "value": "Response to Reviewer 54Ry"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570915690,
                "cdate": 1700570915690,
                "tmdate": 1700574156722,
                "mdate": 1700574156722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]