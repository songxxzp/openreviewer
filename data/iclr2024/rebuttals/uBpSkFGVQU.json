[
    {
        "title": "Depth-Guided Self-Supervised Learning: Seeing the World in 3D"
    },
    {
        "review": {
            "id": "cxTLNEPyCU",
            "forum": "uBpSkFGVQU",
            "replyto": "uBpSkFGVQU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6455/Reviewer_AVEH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6455/Reviewer_AVEH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new self-supervised representation learning approach (SSL) incorporating novel view synthesis data augmentation and estimated depth maps as input. DPT and AdaMPI are used for depth and novel view synthesis estimation. By incorporating depths and novel views during training the authors found their method more accurate when learning from few data and more robust to noisy test inputs. The authors claim this is the first method to use estimated depth as inputs for self-supervised representation learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Biologically inspired and a corresponding well-written introduction.\n\n2. Good correlation with biological elements in the visual system.\n\n3. Interesting reasoning on why novel views should improve SSL: As the mutual information between depth and input images is high, the effect of depth is negligible on an infinite dataset. However, novel 3D views introduce new information.\n\n4. Simple and clear method, seems reproducible.\n\n5. Clear ablation studies."
                },
                "weaknesses": {
                    "value": "1. Figure 1 lacks details. For instance, it is unclear what \"2D  augmentation\" is being performed in PixDepth. It would also be good to visually represent your SSL objective.\n\n3. Depth is dropped from the input to encourage the network to not over-rely on it. However, depth is used to compute the error metrics in the results table. In this case, the comparison could be considered unfair, as the previous methods do not have depth as input. As depth is obtained from a supervised network, this method is not a pure SSL method.\n\n3. Most improvements come from adding the depth channel, which I could not consider an important contribution.\n\n4. What is the reason behind the statement of improvements of SwAV in Table 1 when no results for SwAV are provided?\n\n5. For the results on imagenet-100 no improvement is achieved, it is actually the opposite. This is not clearly reflected in the text in a tricky way. Why are there no results with the combination method (depth + 3D)?\n\n6. I am afraid that the added robustness to corruptions in ImageNet-C and ImageNet-3DCC comes from the robustness of the depth estimation network (trained with depth GTs over a considerable amount of data).\n\n2. Some minor typos: \"a approach\", \"an conceptually\","
                },
                "questions": {
                    "value": "1. Don't you think this is too casual language \"we take seriously two insights\" ?\n\n2. Clear metrics comparing against SOTA would be helpful in the introduction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697779590482,
            "cdate": 1697779590482,
            "tmdate": 1699636721589,
            "mdate": 1699636721589,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "riAqdC0m7j",
                "forum": "uBpSkFGVQU",
                "replyto": "cxTLNEPyCU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AVEH"
                    },
                    "comment": {
                        "value": "> Figure 1 lacks details. For instance, it is unclear what \"2D augmentation\" is being performed in PixDepth. It would also be good to visually represent your SSL objective.\n\nWe thank the reviewer for these suggestions. 2D Augmentation refers to the standard set of augmentations applied on the 2D image. For instance, random crop, flip, color jitter etc. We don\u2019t show the SSL objective here explicitly because we are using several standard methods for SSL and the SSL algorithm itself is not the focus of our work. Our focus is on extending input with a depth map and using augmentations that correspond to different (inferred) 3D views.\n\n> Depth is dropped from the input to encourage the network to not over-rely on it. However, depth is used to compute the error metrics in the results table. In this case, the comparison could be considered unfair, as the previous methods do not have depth as input. As depth is obtained from a supervised network, this method is not a pure SSL method.\n\nWe thank the reviewer for this interesting question. We did run an ablation to analyze the performance of the model when depth is not available during inference. An excerpt from the paper (Section 4.5) follows:\n\n\n\u201cTable 6 reports these results on ImageNette dataset with BYOL. Interestingly, we find that even with the absence of depth information, the\naccuracy of the model is higher than the baseline BYOL. This indicates that the model has implicitly learned some depth signal and captured better representations. It can also be seen that the performance on IN-3DCC is 1.5% higher than BYOL.\u201d\n\n> Most improvements come from adding the depth channel, which I could not consider an important contribution.\n\nWe note that 3D Views and the ensemble of Depth + 3D Views also improve performance over state-of-the-art baseline SSL methods, allowing us to argue for the broader claim that depth signals are valuable early in the vision pipeline. Perhaps the reviewer considered the contribution of a depth channel as unimportant because it is such a minor change to the architecture. We agree it is a minor architectural tweak, but wish to emphasize that it is a significant conceptual change: very little work in the field considers computations to extract depth from 2D images to precede object recognition.\n\n\n> What is the reason behind the statement of improvements of SwAV in Table 1 when no results for SwAV are provided?\n\nWe do not show results with SwAV + 3D views because the multi-crop setting used in SwAV makes it non-trivial to be adapted to 3D Views. We have mentioned this in the paper but we will update the paper to clarify. \n\n> For the results on imagenet-100 no improvement is achieved, it is actually the opposite. This is not clearly reflected in the text in a tricky way. Why are there no results with the combination method (depth + 3D)?\n\nWe are not sure if we understand the question correctly. We report results with depth + 3D in Table 2 on ImageNet-100. We do not report results Depth + 3D on ImageNet-1k due to compute constraints. Apologies, we should have mentioned this fact in the article text.\n\n> I am afraid that the added robustness to corruptions in ImageNet-C and ImageNet-3DCC comes from the robustness of the depth estimation network (trained with depth GTs over a considerable amount of data).\n\n\nWe begin by emphasizing that the depth-estimation network need not be used during inference. As Section 4.5 (Table 6) indicates, when depth is unavailable at inference, the proposed method is still better than the corresponding baseline SSL method. \nRegarding the reviewer\u2019s point that the depth-estimation network confers some robustness benefit to the learned embeddings, we certainly agree, although the benefit is indirect through the SSL method. Our investigation was focused on the empirical question of whether such an indirect benefit would indeed be obtained."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534387737,
                "cdate": 1700534387737,
                "tmdate": 1700534387737,
                "mdate": 1700534387737,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0sPuYhSiov",
                "forum": "uBpSkFGVQU",
                "replyto": "riAqdC0m7j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6455/Reviewer_AVEH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6455/Reviewer_AVEH"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your answers"
                    },
                    "comment": {
                        "value": "1. Even though the augmentations are clear in the text, the two images in PixDepth look almost the same. Representing a more perceptually significant transformation in the image would be better. I still think that for better conveying your idea, it would be good to represent your SSL objective (or objectives) pictorically.\n\n2. Can you point out where is the table that contains the baseline BYOL, which is 1.5% less accurate than your model with no disp input during the test?\n\n3. Thanks for your answer. As the authors mentioned, this is a straightforward architectural change. In terms of making depth an additional input, I think it would be necessary to explore further ways in which depth could be used as input. For example, surface normals and 3d point clouds can be extracted from the depth map.\n\n4. Thanks for the answer.\n\n5. Thanks for your answer. What would be necessary to make the network with depth inputs dropped be as good as the network with depth inputs during testing?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647508435,
                "cdate": 1700647508435,
                "tmdate": 1700647508435,
                "mdate": 1700647508435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hzZR6ZYPND",
            "forum": "uBpSkFGVQU",
            "replyto": "uBpSkFGVQU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6455/Reviewer_BurA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6455/Reviewer_BurA"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to incorporate depth signals into the self-supervised learning (SSL) framework. Specifically, two baselines are provided: the first baseline directly concatenates RGB and depth signals as the input of SSL, and the second baseline augments novel view generated according to the depth signal for SSL."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work investigate the influence of including depth signals into the SSL framework.\n2. The experiments show that with the introduction of depth signals, the existing SOTA SSL methods yield a better performance."
                },
                "weaknesses": {
                    "value": "1. Using depth signals as augmentation is not new in SSL. Previous works, e.g., DepthContrast, have explored it thouroughly.\n2. The proposed method lacks generalizbility. Though it can be adopted to any SSL frameworks, the adopted depth estimation model is supervised trained on several datasets. The performance of depth estimation can not gurantee in scenarios that have a huge domain gap compared to the trained datasets. \n3. Also, due to the utilization of the supervised depth-estimation model, it is questionable to claim the proposed method as a SSL framework.\n4. Experiments are all conducted on the subset of Imagenet or the modification of Imagenet. Results on more datasets are expected."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6455/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6455/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6455/Reviewer_BurA"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698190879299,
            "cdate": 1698190879299,
            "tmdate": 1699636721468,
            "mdate": 1699636721468,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iT73zdfN3o",
                "forum": "uBpSkFGVQU",
                "replyto": "hzZR6ZYPND",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BurA"
                    },
                    "comment": {
                        "value": "> Using depth signals as augmentation is not new in SSL. Previous works, e.g., DepthContrast, have explored it thouroughly.\n\nWe thank the reviewer for pointing us to DepthContrast [1]. As we understand it, DepthContrast requires ground truth depth information in the form of point-cloud data and relies on format-specific encoders (like PointNet++ for point clouds and UNet for Voxels). In contrast, our method takes a standard 2D RGB image input and assumes only that depth maps can be estimated from a separate module, allowing our method to be  seamlessly integrated into standard SSL pipelines. More importantly, the motivation behind the DepthContrast and our method is very different.  \n\nWe have discussed comparisons with other related work in the Appendix A.\n\n> The proposed method lacks generalizbility. Though it can be adopted to any SSL frameworks, the adopted depth estimation model is supervised trained on several datasets. The performance of depth estimation can not gurantee in scenarios that have a huge domain gap compared to the trained datasets. \n\nDepth-Prediction Transformer (DPT) [2] is robust to various natural image datasets as shown in the paper via zero-shot cross dataset transfer. It is due to these reasons that various works in 3D reconstruction [3, 4] and view synthesis [5, 6] use DPT to estimate depth maps.\n\n> Also, due to the utilization of the supervised depth-estimation model, it is questionable to claim the proposed method as a SSL framework.\n\nWhile the reviewer is technically correct, we urge the reviewer to appreciate our underlying premise. We argue that biological agents obtain depth information for free by continuous-time interaction with their environments, and so should a complete, continual learning AI agent. But rather than developing a complex multi-module AI system, we start with the premise that depth maps are available and then ask how they can benefit self-supervised learning. The means by which depth is obtained is not central to our investigation; however, the fact that depth is obtained via extraction from a 2D image (as the human brain does) is key. This work provides insights into and benefits of how to effectively utilize this depth information.\n\n> Experiments are all conducted on the subset of Imagenet or the modification of Imagenet. Results on more datasets are expected.\n\nWe have reported results on the ImageNet-1k dataset (Table 3) consisting of 1.2 million real-world images. It is common in the SSL literature to report the pretraining results only on ImageNet-1k [7, 8, 9, 10]. We also emphasize that SSL in general is computationally expensive.\n\n\n[1] Zhang, Zaiwei, et al. \"Self-supervised pretraining of 3d features on any point-cloud.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n\n[2] Ranftl, Ren\u00e9, Alexey Bochkovskiy, and Vladlen Koltun. \"Vision transformers for dense prediction.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[3] Wu, Chao-Yuan, et al. \"Multiview compressive coding for 3D reconstruction.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[4] Liu, Ruoshi, et al. \"Zero-1-to-3: Zero-shot one image to 3d object.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[5] Han, Yuxuan, Ruicheng Wang, and Jiaolong Yang. \"Single-view view synthesis in the wild with learned adaptive multiplane images.\" ACM SIGGRAPH 2022 Conference Proceedings. 2022.\n\n[6] Jiang, Yutao, et al. \"Diffuse3D: Wide-Angle 3D Photography via Bilateral Diffusion.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[7] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[8] Yeh, Chun-Hsiao, et al. \"Decoupled contrastive learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[9] Shi, Yuge, et al. \"Adversarial masking for self-supervised learning.\" International Conference on Machine Learning. PMLR, 2022.\n\n[10] Dwibedi, Debidatta, et al. \"With a little help from my friends: Nearest-neighbor contrastive learning of visual representations.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534124169,
                "cdate": 1700534124169,
                "tmdate": 1700534124169,
                "mdate": 1700534124169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CprTPZRX88",
            "forum": "uBpSkFGVQU",
            "replyto": "uBpSkFGVQU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6455/Reviewer_YemW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6455/Reviewer_YemW"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a new representation learning method that utilizes an estimated depth map to learn a geometry-aware representation.\nAs mentioned in the abstract, the goal of SSL is to learn useful representation for \"downstream tasks.\"\nHowever, the scale of the conducted experiments is insufficient to claim the effectiveness of the proposed method. \n1) The proposed method is evaluated only in small-scale datasets (e.g., ImageNet-100, ImageNet-1k).\n2) The proposed method is evaluated only in small-scale models (e.g., ResNet-18, 50). \n3) The proposed method is evaluated only in a classification task.\n\nThe proposed method must need to show its effectiveness and scalability in large-scale datasets, various backbone models (e.g., CNN, Transformer variant models), and diverse downstream tasks (e.g., 2D/3D detection, 2D/3D segmentation, 3D reconstruction, 3D view generation, etc)"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper provides a new representation learning method that utilizes an estimated depth map to learn a geometry-aware representation.\nTo train an RGB-D backbone network, the method generates 3D views with an image and estimated depth map and utilizes them with the previous SSL method."
                },
                "weaknesses": {
                    "value": "[Quality & Significance]\nAs mentioned in the abstract, the goal of SSL is to learn useful representation for \"downstream tasks.\"\nHowever, the scale of the conducted experiments is insufficient to claim the effectiveness of the proposed method. \n1) The proposed method is evaluated only in small-scale datasets (e.g., ImageNet-100, ImageNet-1k).\n2) The proposed method is evaluated only in small-scale models (e.g., ResNet-18, 50). \n3) The proposed method is evaluated only in a classification task.\n\nThe proposed method must need to show its effectiveness and scalability in large-scale datasets, various backbone models (e.g., CNN, Transformer variant models), and diverse downstream tasks (e.g., 2D/3D detection, 2D/3D segmentation, 3D reconstruction, 3D view generation, etc)\n\n[Clarity]\nI recommend the authors to narrow down the scope of the proposed method from general SSL to a specific SSL method.\nThe current backbone tasks RGB-Depth image as inputs, so targeting downstream tasks for RGB-D inputs (e.g., RGB-D segmentation, 3D recon, 3D view synthesis, human pose estimation) is a more reasonable choice to claim the effectiveness of the proposed method.\nThe current claim is too general and insufficient to support the claim."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6455/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757451099,
            "cdate": 1698757451099,
            "tmdate": 1699636721332,
            "mdate": 1699636721332,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cx6trALl5E",
                "forum": "uBpSkFGVQU",
                "replyto": "CprTPZRX88",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6455/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6455/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YemW"
                    },
                    "comment": {
                        "value": "> The proposed method is evaluated only in small-scale datasets (e.g., ImageNet-100, ImageNet-1k). The proposed method is evaluated only in small-scale models (e.g., ResNet-18, 50). \n\nWe have reported results on ImageNet-1k which consists of 1.2 million real-world images. It is a bit shocking to see the reviewer classify this dataset as a small-scale dataset. Even in recent SSL papers [1, 2], Imagenet-1k is the only dataset used for evaluation with a single backbone (ResNet-50)\n\n> [Clarity] I recommend the authors to narrow down the scope of the proposed method from general SSL to a specific SSL method. The current backbone tasks RGB-Depth image as inputs, so targeting downstream tasks for RGB-D inputs (e.g., RGB-D segmentation, 3D recon, 3D view synthesis, human pose estimation) is a more reasonable choice to claim the effectiveness of the proposed method. The current claim is too general and insufficient to support the claim.\n\nWe respectfully disagree with the reviewer here. As we pointed out to other reviewers, the goal of SSL is to learn representations that are useful for various downstream tasks. It is common in SSL literature to primarily report linear classification accuracy. In our paper, we focused more on understanding the various components of our method (including depth channel dropout, quality of 3d views) rather than evaluating on a range of downstream tasks. \n\n[1] Bardes, Adrien, Jean Ponce, and Yann LeCun. \"Vicregl: Self-supervised learning of local visual features.\" Advances in Neural Information Processing Systems 35 (2022): 8799-8810.\n[2] Yeh, Chun-Hsiao, et al. \"Decoupled contrastive learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6455/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534505816,
                "cdate": 1700534505816,
                "tmdate": 1700534505816,
                "mdate": 1700534505816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]