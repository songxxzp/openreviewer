[
    {
        "title": "PeFLL: Personalized Federated Learning by Learning to Learn"
    },
    {
        "review": {
            "id": "zKZoDlblG6",
            "forum": "MrYiwlDRQO",
            "replyto": "MrYiwlDRQO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5247/Reviewer_tVNw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5247/Reviewer_tVNw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a learning-to-learn framework for personalized FL, where the goal is to learn a global hypernetwork that outputs each clients' personalized model. A shared embedding network is also trained, which produces each client's descriptor based on its local dataset. During inference, any client (even clients that did not participate during training) can send its descriptor to the server and receive the personalized model. This enables each client to obtain the model without further finetuning at test-time. Convergence analysis as well as generalization bound analysis are provided. The effectiveness of proposed idea is also confirmed via experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of taking advantage of learning-to-learn and hypernetworks in personalized FL is interesting.\n\n2. The proposed idea has several advantages. Specifically, any client can obtain its personalized model by conducting only forward propagations.\n\n3. Experimental results show significant performance advantage of the proposed method.\n\n4. Theoretical results are also provided, further strengthening the paper."
                },
                "weaknesses": {
                    "value": "1. All experiments are conducted using LeNet-style models, which are relatively outdated. I would like to see the performance on ResNet-style models.\n\n2. In Table 2, the performance of \"Local\" is missing. Moreover, during experiments, what is the number of local updates for the baselines like Per-FedAvg and pFedMe. How do their performance improve as the number of local updates increases, and can they achieve similar performance compared to the proposed PeFLL?"
                },
                "questions": {
                    "value": "See weakness bove."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Reviewer_tVNw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698553697433,
            "cdate": 1698553697433,
            "tmdate": 1699636523923,
            "mdate": 1699636523923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OgkKTuK5XL",
                "forum": "MrYiwlDRQO",
                "replyto": "zKZoDlblG6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review, we answer your specific concerns below.\n\n> All experiments are conducted using LeNet-style models, which are relatively outdated. I would like to see the performance on ResNet-style models.\n\nPlease see our reply to the first question of reviewer [Couf](https://openreview.net/forum?id=MrYiwlDRQO&noteId=zjBDwmhek) about the architecture and the size of the network.\n\n\n> In Table 2, the performance of \"Local\" is missing. \n\nWe can include this. It is the same procedure as in Table 1 operating on the same kind of data, so the values for local in Table 2 will not differ substantially from those in Table 1.\n\n\n> Moreover, during experiments, what is the number of local updates for the baselines like Per-FedAvg and pFedMe. How do their performance improve as the number of local updates increases, and can they achieve similar performance compared to the proposed PeFLL?\n\nWe assume by \u2018local updates\u2019 here you mean the number update steps used to personalize/finetune to a test client to obtain the results quoted in Table 2. The personalization on each test client was simply run locally until convergence, which does not usually require many steps due to the small amount of data on each client. The reported result is based on the best validation step. Therefore, we do not see that the results for Per-FedAvg/pFedMe could improve by even more steps."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700181729071,
                "cdate": 1700181729071,
                "tmdate": 1700181729071,
                "mdate": 1700181729071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rm2wAmpNvu",
                "forum": "MrYiwlDRQO",
                "replyto": "OgkKTuK5XL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_tVNw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_tVNw"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the response. However, I do not agree with the authors that experiments with the current model is enough. Although FL can be applied to low-power devices, it could be also applied to other applications like autonomous vehicles, hospitals where clients have enough powers, and in these cases, one might use larger models. Conducting additional experiments on ResNet does not provide any negatives to the paper, but actually will validate the applicability of the proposed method. I still feel that only using LeNet-style model is not enough."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279271083,
                "cdate": 1700279271083,
                "tmdate": 1700279271083,
                "mdate": 1700279271083,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZWzNgJ6OPx",
            "forum": "MrYiwlDRQO",
            "replyto": "MrYiwlDRQO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a federated training algorithm which allows clients to receive a personalized model at test-time, based on their individual local dataset. This is achieved by predicting personalized network parameters based on an individual client's descriptor. This descriptor (a vector embedding), is generated by passing the local dataset through a set network. The embedding computation is done locally, while the (expensive) hypernetwork is run on the server. \nThe training algorithm involves reversing the forward-pass and computing partial gradients at the server and the individual clients.\n\n-- post rebuttal --\nThe authors have addressed my concerns and I do recommend acceptance.\nWhile some question with respect to the method's generalizability to new tasks & models remain, I believe it to be a worthwhile method to experiment with for a new personalization use-case."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper addresses a highly relevant problem in an innovative way. The proposed test-time generation of personalized models through a server-side hypernetwork is original and well-described. The paper reads very well, I enjoyed following the authors' red line through the paper."
                },
                "weaknesses": {
                    "value": "The weaknesses of this paper revolve around the experimental evaluation, specifically the base-lines that the authors consider given their choice of the experimental setting. The evaluation pipeline raises som questions, detailed below. \nThe comparison to baselines is somewhat unfair, due to a lack of measuring communication overhead.\nFinally, i believe the discussion of related works should include the topic of Split-Learning, a version of FL where parts of the forward-pass are performed across different devices (e.g. client and server). PeFFL is an example of split-learning under that definition."
                },
                "questions": {
                    "value": "- The authors only consider the label-skew setting of object-classification computer vision problems C10/C100 and Femnist. Specifically for C10/C100, the authors assign two labels per client. The provided code (mask_absent is True by default) suggest that their method consequently reduces to a binary classification problem at test time. Can you confirm this choice of evaluation is consistent with all baselines? Evaluation in line 374 & 375 of train.py concerns the test-clients. Following into line 20, evaluation is done. The default args embed_split=True suggest that you are using the training-set to compute embeddings, as well as (lines 58-76) using it to test for presence of classes on that client. It seems that for all (train & test) clients, you assume the presence of a labelled training set, which allows for masking your logits. Lines 24-38 suggest you're treating training & test clients identically. I would expect a \"test-client\" to not have any labelled data available, meaning that the logit-masking should not be allowed. Can you clarify your evaluation procedure for me? Do \"test-clients\" use a different training-set for embedding computation? I would expect those to be identical. \nFor the training-clients, I would be curious to understand if you use the labelled training data for embedding computation, or throw those away for the validation-set in Table 1a. You claim that the client descriptor network is not susceptible to overfitting. I would ask you to show evidence for that claim. \n- How does PeFLL fair in non-label skew settings? E.g. I would recommend the Shakespeare dataset (https://arxiv.org/abs/1812.01097) \n- In order to fairly compare PeFFL against baselines, I believe it is necessary to introduce the communication overhead that is introduced by the different methods. You argue that \"It has low latency and communication costs\", which should be clarified to be true only at test-time (and even then is higher than alternatives that function offline, such as FedAvg). During training, PeFFL is constrained to \"FedSGD\", i.e. a client cannot amortize communication with additional computation. Instead, forward-backward pass requires communication of various elements: Embedding, client-specific model and the gradients corresponding to both. I would like to see two analyses:\n1.) GB of communication on the x-axis and test-client accuracy at the y-axis. This allows to draw iso-lines and understand the impact of the high-frequency communication of PeFFL.\n2.) Introduction of another baseline, in which a larger, more flexible model is trained through FedAdam (https://arxiv.org/abs/2003.00295) with properly tuned hyper-parameters. The hypothesis I would like to see tested is if a more flexible model is able to generalize across all train & test clients without requiring personalization. For a given communication budget, should I choose PeFFL or can I fall back to a larger FedAvg-trained model?\n- Another interesting experiment (though not required), would be to see a combination of classical FedAvg and PeFFL. Consider applying PeFFL only to the last layer(s) of a model, while training the rest through standard FedAvg. For object classification, it is usually the last layers that require personalization, while the initial layers can share jointly relevant features. Such a setup would allow the embedding network to be a separate head of the FedAvg-trained network. \n\nShould the authors address my questions, I will consider raising my score"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698782136,
            "cdate": 1698698782136,
            "tmdate": 1700709362919,
            "mdate": 1700709362919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EtbWiOzW4d",
                "forum": "MrYiwlDRQO",
                "replyto": "ZWzNgJ6OPx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed feedback, we address your concerns below.\n\n> I believe the discussion of related works should include the topic of Split-Learning\n\nThank you for bringing this to our attention. We agree that it is both relevant and highly interesting. We will add this topic to our related work section and also include citations to [1,2]. Please let us know if you have other suggested related work. However, we would like to emphasize that we are not doing split learning exactly as described because we are not communicating gradients, but rather model differences after multiple updates (exactly as FedAvg does). We discuss this in more detail below. \n\n[1] Otkrist Gupta, Ramesh Raskar. Distributed learning of deep neural network over multiple agents, Journal of Network and Computer Applications, 2018\n\n[2] Chandra Thapa, M.A.P. Chamikara, Seyit Camtepe, Lichao Sun. SplitFed: When Federated Learning Meets Split Learning, AAAI, 2022\n\n\n> The authors only consider the label-skew setting of object-classification computer vision problems C10/C100 and FEMNIST. Specifically for C10/C100, the authors assign two labels per client. The provided code (mask_absent is True by default) suggests that their method consequently reduces to a binary classification problem at test time. Can you confirm this choice of evaluation is consistent with all baselines? \n\nFirst, what you describe is the setting for CIFAR10. For CIFAR100, each client actually has 10 classes, and for FEMNIST they can have all classes. \nSecond, for all baselines the clients implicitly or explicitly use information about their subset of relevant classes. For example, kNN-Per performs kNN, so it can never predict a class not present in the client\u2019s labeled training set. All other personalized methods train or finetune a classifier locally on a labeled training set, even for the test clients, so classes not present in their training data are quickly suppressed. \n\n\n> It seems that for all (train & test) clients, you assume the presence of a labeled training set. [...] I would expect a \"test-client\" to not have any labelled data available. \n\nIndeed, in our setting \u201ctest clients\u201d differ from \u201ctrain clients\u201d only by the fact that they appear later, after the server is done with training. Maybe a better phrasing would be \u201cclients seen during the server\u2019s training phase\u201d and \u201cclients seen after the server\u2019s training phase is finished\u201d. \n\nIn the supervised setting we study, labels are certainly needed during the training phase, and therefore it makes sense to assume that clients occurring later also have them. Indeed, none of the baselines to which we compare would be able to personalize to a test client that has no labeled data. \n\nHowever, we find the suggestion of learning personalized models for clients without labels for their data also quite exciting. Specifically, all baselines we report, except plain FedAvg, would fail to produce any classifiers for new clients in that setting. PeFLL, however, could work simply by using an embedding network that operates on the input data without labels. We are now running experiments in this setting (and without masking). We will report it as soon as it finishes, and we thank the reviewer for the suggestion.\n\n> Can you clarify your evaluation procedure for me? Do \"test-clients\" use a different training-set for embedding computation? I would expect those to be identical. For the training-clients, I would be curious to understand if you use the labelled training data for embedding computation, or throw those away for the validation-set in Table 1a. \n\nTrain and test clients behave identically in that they receive the embedding network, compute their client descriptor from a batch of their data, send the descriptor back to the server and receive a personalized model. The only difference is that \u201ctrain clients\u201d subsequently participate in the server\u2019s training step by performing gradient computations and sending parameter updates to the server.\n\n\n\n> You claim that the client descriptor network is not susceptible to overfitting. I would ask you to show evidence for that claim.\n\nPlease see the paragraph in the experiment section **Analysis of Client Descriptors** as we believe this answers your question. In this experiment we explicitly test if the client descriptor (embedding) network is able to embed the test clients into descriptor space in a way that is consistent with the embeddings of the train clients. As the experiment shows (Figure 3), the embedding network does indeed learn to do this. If the embedding network simply overfit to the train clients then the test client descriptors would be meaningless."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700181878812,
                "cdate": 1700181878812,
                "tmdate": 1700182198704,
                "mdate": 1700182198704,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c72cZI5SFp",
                "forum": "MrYiwlDRQO",
                "replyto": "ZWzNgJ6OPx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> How does PeFLL fare in non-label skew settings? E.g. I would recommend the Shakespeare dataset\n\nNote that FEMNIST is not a purely a label skew dataset, as different clients correspond to different writers. Thank you for suggesting the Shakespeare dataset, we will try to get experiments running and finished before the end of the discussion period. \n\n\n> During training, PeFFL is constrained to \"FedSGD\", i.e. a client cannot amortize communication with additional computation. Instead, forward-backward pass requires communication of various elements: Embedding, client-specific model and the gradients corresponding to both.\n\nThis is not correct and we believe this could be leading to a **fundamental** misunderstanding of the communication costs of our method. We are **not** constrained to FedSGD. During training each client runs k local updates to their personalized model (line 10 of Algorithm 2). In our experiments k=50, while FedSGD would be k=1. The term that gets sent to the server to be \u2018backpropped\u2019 through the network is the difference between the initial personalized model and the final one **after** these 50 steps, (line 11 of Algorithm 2). We also refer you to our convergence results in Theorem 3.1, which shows that the number of local steps has the same effect as it has in FedAvg, and **proves that our procedure has equivalent convergence to FedAvg with k local steps per round** and not FedSGD. Since our convergence bound is in the same order of FedAvg, the number of global steps required for the convergence is the same as FedAvg, and we do not require more communication rounds than FedAvg. \n\n\n> Introduction of another baseline, in which a larger, more flexible model is trained through FedAdam (https://arxiv.org/abs/2003.00295) with properly tuned hyper-parameters. The hypothesis I would like to see tested is if a more flexible model is able to generalize across all train & test clients without requiring personalization. For a given communication budget, should I choose PeFFL or can I fall back to a larger FedAvg-trained model?\n\nAs we mentioned for the previous question, we have the same number of communication rounds as FedAvg for convergence, just in each round we send two (small) networks instead of one. Also, our optimization is in fact already done using Adam. \n\nRegarding a larger model: we do agree that a large enough global model might also achieve high accuracy in the setting of our experiments. Note, however, that such a baseline would not be a fair comparison in terms of the computational load on client devices (which often have very limited compute and memory)\nIn light of this fact and given our reply to your prior concern about the PeFLL\u2019s communication cost, do you still think such a baseline is necessary?\n\n\n> Another interesting experiment (though not required), would be to see a combination of classical FedAvg and PeFFL. Consider applying PeFFL only to the last layer(s) of a model, while training the rest through standard FedAvg. For object classification, it is usually the last layers that require personalization, while the initial layers can share jointly relevant features. Such a setup would allow the embedding network to be a separate head of the FedAvg-trained network.\n\nWe agree that this could be an interesting experiment, in particular as some early layers of the network could double as the embedding network. PeFFL would allow for such a setting without major modifications. Thank you for the suggestion!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182024910,
                "cdate": 1700182024910,
                "tmdate": 1700182148277,
                "mdate": 1700182148277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qnupLnNKUB",
                "forum": "MrYiwlDRQO",
                "replyto": "ZWzNgJ6OPx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Results of training without using labels for embedding:"
                    },
                    "comment": {
                        "value": "We have now finished running the results for the experiments where the test clients do not possess labeled data. Specifically what we do is: the PeFLL embedding network is trained on the training clients using only the image data and no labels (embed_y = False in the code). The test clients then obtain their personalized model using only their unlabeled data and evaluate this model. Of course since they do not have labeled data there is no class masking (mask_absent = False in the code). We report below the results on the test clients. Note that the only baseline that is capable of addressing this setting is FedAvg, since all personalized FL methods require labeled data to be available at the test clients in order to obtain a personalized model.\n\n\n|              |     |      CIFAR10      |            |    |      CIFAR100       |            | FEMNIST    |\n|--------------|:------------:|:------------:|:------------:|:------------:|:-------------:|:------------:|:------------:|\n| #trn clients | 90         | 450        | 900        | 90         | 450         | 900        | 3237       |\n| FedAvg       | 48.6 \u00b1 0.3 | 49.6 \u00b1 1.0 | 51.7 \u00b1 0.7 | 17.1 \u00b1 0.9 |  19.3 \u00b1 2.0 | 20.5 \u00b1 1.0 | 81.9 \u00b1 0.4 |\n| PeFLL        | 84.5 \u00b1 0.6 | 83.6 \u00b1 1.0 | 81.5 \u00b1 0.7 | 26.3 \u00b1 0.4 | 28.2 \u00b1 0.6  | 30.3 \u00b1 0.6 | 91.4 \u00b1 0.1 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339205318,
                "cdate": 1700339205318,
                "tmdate": 1700339205318,
                "mdate": 1700339205318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "287aQntcS2",
                "forum": "MrYiwlDRQO",
                "replyto": "qnupLnNKUB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications, additional work remains"
                    },
                    "comment": {
                        "value": "I thank the authors for their clarifying responses to my questions.\n\nIf I understand correctly, your additional results were run without label-masking and without including the labels during the training and evaluation of the embedding network. \nWith respect to label-masking, I am still lacking an explanation if this strategy is used during evaluation of all baselines. E.g. during evaluation of the FedAvg baseline. For the additional experiments (where you claim no label masking) you seem to have included the same numbers as in the main paper. Please clarify. \n\nI don't belive there is a need to rename or re-define train vs. test clients. The only necessary clarification concerns weather test-clients are assumed to have labels available, which you have answered to be the case.\n\nI believe indeed that the unsupervised personalization angle to be a strong argument for your paper and would encourage to lean into it more. \n\nThe answer to your question regarding the use of training or evaluation data for the embedding computation is still not clear to me. For a training client, i.e. Table 1a, you assume the existence of a training set, which is used during training, as well as the existance of a test-set on that same client. For the computation of the client-embedding at evaluation-time, which dataset are you drawing a mini-batch from? The training-set (which was used to train the embedding network) or the test-set (which has not been \"seen\" by the training of the embedding network)?\n\nThanks for pointing out the \"Analysis of Client Descriptors\"; Indeed, it answeres my question about the overfitting claim - sorry I didn't associate the two. \n\nIndeed your method goes beyond FedSGD for training of the $\\theta_i$ per client. I was referring to $\\delta \\nu_v^{(i)}$ which is the result of a single forward-backward pass. Fundamentally, my question about the empirical comparisson wrt. communication budget has thus not been answered yet. At each communication round, the communication costs per client for FedAvg are $2*|\\theta|$, i.e. twice the model size. For PeFLL, the communication costs are: $2*|\\nu_v| + 2*|v| + 2*|\\theta|$; These correspond to lines 3, 7, 9, 11, 13, 15 of your algorithm. So the question remains: If I spent the additional communication budget of PeFLL on a larger model for FedAvg, would that change the story? Equivalently, for a given communication budget, i.e. if I stopped training PeFLL earlier than a FedAvg training run with the same model $\\theta$ in both experiments, which model would be better? Assuming you have retained logs for your existing training runs, it should be easy to create learning curves with GB at the x-axis and test-accuracy at the y-axis. \n\nYou are asking if experiments with a larger model are still necessary. I'd argue that the plot I asked for would be enough for me. Given the other reviewer's ask for a larger model, it might be a a good idea regardless. \n\nFor the mean-time, I will raise my score slightly, but I would need the following for further increase:\n- Learning-curve with Accuracy vs. GB and discussion of those\n- Shakespeare (or other dataset) results\n- Clarification for label-masking across all baselines. \n- Discussion of your additional results: Please take apart the effect of label-masking from the effect of adding labels to the embedding network. I am especially curious to understand why C100 degraded significantly: Is the embedding network relying on labels to perform well here? Is the label-masking relevant for C100 to work well in your setup?\n- An updated version of the paper, where these results, comments and clarifications have been added. \n\nAgain, thanks for the interesting work - I appreciate the ideas and contributions. The execution & evaluation is what I am critiquing here."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592653604,
                "cdate": 1700592653604,
                "tmdate": 1700592653604,
                "mdate": 1700592653604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "isK8LnrFvm",
                "forum": "MrYiwlDRQO",
                "replyto": "XOzeFzDJDn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for all the work"
                    },
                    "comment": {
                        "value": "I appreciate the additional results!\nMy asks were:\n- Learning-curve with Accuracy vs. GB and discussion of those\n- Shakespeare (or other dataset) results\n- Clarification for label-masking across all baselines.\n- Discussion of your additional results: Please take apart the effect of label-masking from the effect of adding labels to the embedding network. I am especially curious to understand why C100 degraded significantly: Is the embedding network relying on labels to perform well here? Is the label-masking relevant for C100 to work well in your setup?\n- An updated version of the paper, where these results, comments and clarifications have been added.\nAs far as I can see, you delivered on all of those. \nThe learning-curves are clearly showing two things:\n- Your method outperforms FedAvg strongly. Please add a larger model FedAvg baseline for the final version of the paper, i.e. increase your LeNet model to reasonable width such that you have twice the amount of parameters. \n- Your method seems to not generalize equally well to the LSTM & Shakespeare as it works for C10/C100. For the final version, I'd recommend looking into why that is.\n\nConsidering your C100 results with Shakespeare, I believe that your method might not be as easily generalizable, i.e. I cannot expect it to \"just work\" and provide clear benefits when applying to a real-world scenario or any new scenario that goes beyond small-scale object classification. However I believe it to still be a valuable tool to experiment with and try out for a personalization scenario. \n\nI have updated my rating accordingly!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709284826,
                "cdate": 1700709284826,
                "tmdate": 1700709284826,
                "mdate": 1700709284826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N9UWwi6mkl",
                "forum": "MrYiwlDRQO",
                "replyto": "bj0xzP8km4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_Fw5z"
                ],
                "content": {
                    "title": {
                        "value": "Some more points."
                    },
                    "comment": {
                        "value": "I agree that more modern networks would be interesting, however I believe that also for a more modern network and a given harsh computational resources constraint, a FedAvg global model will underperform compared to a personalized model through this method. Even though evidence for generalization across tasks & modalities is still somewhat lacking, I believe it a worthwhile method to include in ones personalization tool-box. \nAnother recent hypernetwork could be found here: https://hyperdreambooth.github.io/\nWhile not predicting too many parameters (30k), the hypernetwork itself is a modern transformer architecture which scales quadratically with the number of network parameters."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709848520,
                "cdate": 1700709848520,
                "tmdate": 1700709848520,
                "mdate": 1700709848520,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zjBDwmhekR",
            "forum": "MrYiwlDRQO",
            "replyto": "MrYiwlDRQO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5247/Reviewer_Couf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5247/Reviewer_Couf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a learning-to-learn approach to personalized federated learning. A hypernetwork residing on the server outputs the parameters of the personalized models given \u201cdescriptors\u201d of the clients. The descriptor is generated by an embedding model that is shared by the server to the clients so that they can use it on their private data. This method avoids some issues in personalized federated learning by construction, such as the need to fine-tune, how to generalize to unseen clients, clients having little data, system/data/model heterogeneity, etc. Convergence proofs and generalization bounds are given for their method PeFLL. Experiments show that this model is indeed effective by a large margin, especially at generalizing to unseen clients."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and the approach is interesting.\n- Heterogeneity issues can be alleviated with a learning-to-learn approach.\n- Generalizing to unseen clients makes sense with the embedding model. We intuitively assume that client similarity is dictated by the similarity of their data. Thus, clients with similar embeddings should have similar personalized models.\n- Algorithms are well detailed and the code is shared in the supplementary materials for reproducibility.\n- Experiments show significant improvements on FEMNIST and CIFAR-10/100, especially on clients unseen during training.\n- The benefits of client embedding network is justified experimentally."
                },
                "weaknesses": {
                    "value": "- I think that the experiments are not sufficient to conclude that PeFLL is superior. You have to demonstrate its improvement on architectures more modern than LeNet as well. It would also be great to report performance on federated datasets other than FEMNIST (e.g. other LEAF datasets).\n- The computational efficiency of the model is questionable given that the hypernetwork is at least a 100 times bigger than the client's model, and this is only the case for these particular experiments. It might require a wider net to generate larger personalized models. In other words, LeNet on MNIST and CIFAR-10/100 might only need 100 \"ranks\" to perform well, whereas on more complicated models and datasets, a wider net would be needed for good performance, which makes scalability an issue. That is why I think experiments covering these settings are needed to conclude the practical benefits of PeFLL."
                },
                "questions": {
                    "value": "- Can you share some details about training time, memory consumption, and computational efficiency of PeFLL vs. regular models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5247/Reviewer_Couf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5247/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746125863,
            "cdate": 1698746125863,
            "tmdate": 1699636523708,
            "mdate": 1699636523708,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nDBUJPWQTL",
                "forum": "MrYiwlDRQO",
                "replyto": "zjBDwmhekR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review, we address your comments below. \n\n> I think that the experiments are not sufficient to conclude that PeFLL is superior. You have to demonstrate its improvement on architectures more modern than LeNet as well.\n\nNote that the model we are using is not the classic \u201cLeNet-5\u201d with tanh-activation etc. that you might have in mind. It is a modern ConvNet inspired by the LeNet architecture but bigger and with ReLU activations. We use this architecture to allow a fair comparison of PeFLL to prior work on personalized FL, such as the baselines in [1,2,3,4].\n\nMoreover, we do not agree that showing superior performance on small models is irrelevant. In Federated Learning it is very common in practice to use small models due to the fact that we need to keep communication costs low and the client devices themselves are often too limited in terms of compute to be able to train huge models. For instance Google\u2019s Gboard model, which they use in production, is a single layer LSTM, see [5,6].\n\n> The computational efficiency of the model is questionable given that the hypernetwork is at least a 100 times bigger than the client's model, and this is only the case for these particular experiments. It might require a wider net to generate larger personalized models. In other words, LeNet on MNIST and CIFAR-10/100 might only need 100 \"ranks\" to perform well, whereas on more complicated models and datasets, a wider net would be needed for good performance, which makes scalability an issue.\n\nIf we understand you correctly, you are asking generally about the feasibility of using hypernetworks to generate large models. This is absolutely the case: there are a number of papers that use hypernetworks to generate very large models. For instance, [7] uses a hypernetwork to generate the parameters of a ResNet-50: \u201cwe can predict all 24 million parameters of ResNet-50 in less than a second either on a GPU or CPU\u201d and [8] also generates ResNets using a hypernetwork. Note that -by design- in our method the hypernetwork is located on the server (as opposed to on client devices), so we are not so limited in terms of computational resources to run the hypernetwork.\n\n[1] A. Shamsian, A. Navon, E. Fetaya, and G. Chechik. Personalized federated learning using hypernetworks. ICML, 2021.\n\n[2] C. T. Dinh, N. H. Tran, and T. D. Nguyen. Personalized federated learning with moreau envelopes. NeurIPS, 2020.\n\n[3] L. Collins, H. Hassani, A. Mokhtari, and S. Shakkottai. Exploiting shared representations for personalized federated learning. ICML, 2021.\n\n[4] A. Fallah, A. Mokhtari, and A. E. Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In NeurIPS, 2020.\n\n[5] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C.  Kiddon, and D. Ramage. Federated learning for mobile keyboard prediction. arXiv:1811.03604, 2018.\n\n[6] X. Zheng, Y. Zhang, G. Andrew, C. A. Choquette-Choo, P. Kairouz, H. B. McMahan, J. Rosenstock, and Y. Zhang. \"Federated Learning of Gboard Language Models with Differential Privacy.\" arXiv:2305.18465, 2023.\n\n[7] B. Knyazev, M. Drozdzal, G. W. Taylor, and R. Soriano, A.. Parameter prediction for unseen deep architectures. NeurIPS, 2021.\n\n[8]  D. Ha, A. M. Dai, and Q. V. Le. Hypernetworks. ICLR, 2017."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183223906,
                "cdate": 1700183223906,
                "tmdate": 1700183223906,
                "mdate": 1700183223906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Qa94JzUhd",
                "forum": "MrYiwlDRQO",
                "replyto": "zjBDwmhekR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> That is why I think experiments covering these settings are needed to conclude the practical benefits of PeFLL.\n\nWe disagree that PeFLL should run with bigger target networks to be practical. As discussed above, small models are the default for personalized FL. For example, if in each round we had to send a ResNet50 with 24 million parameters to the clients, clients would suffer from large data transfer volumes already after a few rounds of participation. \n\n\n> Can you share some details about training time, memory consumption, and computational efficiency of PeFLL vs. regular models?\n\nCan you clarify this question? Are you referring to the overall runtime, memory and compute of the method in our simulations? Our method runs in around 7 hours, while baselines took between 5 and 10 hours to run. The overall memory consumption is approximately 1 GB. However, we do not think that these statistics are very reflective of what will happen in practice in FL given that runtime is most greatly influenced by how much compute a client device has, scheduling overhead, stragglers etc. During training the amount of compute that is run on each client is roughly the same for all methods, 50 local updates to the personalized model. PeFLL additionally has a single forward and backward pass on the (small) embedding network during training. When running on test clients PeFLL has much lower compute than all other personalized FL methods since it only requires a forward pass on the (small) embedding network while other methods need to run finetuning locally. PeFLL also has additional compute that happens on the server, but this is minimal and consists of a single forward and backward pass through the hypernetwork for each global training round. This is negligible given the large compute power of the server, and the fact that it is only a single pass."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183262218,
                "cdate": 1700183262218,
                "tmdate": 1700183262218,
                "mdate": 1700183262218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bj0xzP8km4",
                "forum": "MrYiwlDRQO",
                "replyto": "zjBDwmhekR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_Couf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5247/Reviewer_Couf"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors' Comments by Reviewer"
                    },
                    "comment": {
                        "value": "Thank you for addressing my comments.\n\nNote that I said \u201cmore modern\u201d and not \u201clarger\u201d. Small models are indeed very relevant in FL. I totally agree with that, even though improvement in communication efficiency (either in terms of connection speed or algorithmic improvements) can allow the use for reasonably bigger models. However, I was actually more interested in seeing performance comparison on \u201cmodern\u201d architectures regardless of size, particularly ones employing residual connections, attention, etc. This might be more relevant for cross-silo FL as models can in fact be a little big.\n\nThanks for sharing some details about training.I understand from you that bigger models can be generated efficiently by hyper-networks, which is good. It would be interesting then to see how that holds up in your experiments. The server could indeed have great computational capabilities, but at least you should demonstrate that scalability would not be an issue (e.g. by showing that compute and/or memory scales linearly with model size, which is doable based on what you said)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5247/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496921334,
                "cdate": 1700496921334,
                "tmdate": 1700497083784,
                "mdate": 1700497083784,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]