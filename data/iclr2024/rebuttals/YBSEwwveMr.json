[
    {
        "title": "Score-Based Multimodal Autoencoders"
    },
    {
        "review": {
            "id": "MBxYSf4opo",
            "forum": "YBSEwwveMr",
            "replyto": "YBSEwwveMr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8436/Reviewer_L126"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8436/Reviewer_L126"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new multimodal VAE model consisting of unimodal VAEs and a score-based model that models the joint distribution of unimodal latent variables. They also introduce an energy-based coherence guidance model, which helps to alleviate the problem when the predicted modalities are not aligned with the observed modalities. Experiments are performed on PolyMNIST and CelebAMask datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Clearly explained expected properties of a multimodal generative model.\n- Proposed method scales with the number of modalities which is important for multimodal generation.\n- Extensive related work."
                },
                "weaknesses": {
                    "value": "- Missing connection and comparison of the methods in the related work section to the proposed method. In what way are SBM-* improving over these methods? I also find that such discussion is missing in the experimental results.\n- No ablation studies on the components of the proposed method. This would also help gaining a more intuitive understanding of the proposed method, which is lacking in the current version.\n- Limited experimental evaluation using only image data. Both PolyMNIST and CelebAMask are image datasets. It would be beneficial to add experiments on text audio modalities as well. \n- I find the novelty of the method a bit limiting. Given that the experimental results do not report any standard deviations makes it hard to judge the weight of the contribution."
                },
                "questions": {
                    "value": "- It has been shown by several works that FID does not provide an adequate evaluation of a generative model (see for example work by Sajjadi et al Assessing Generative Models via Precision and Recall). Are you sure that none of the considered models experience a mode collapse? It would be better to present these results with newer metrics like Improved Precision and Recall [1] or Delaunay Component Analysis [2] below). With the latter, you could also gain more insights into each generated modality by analysing the geometry of their representations. \n\n[1] Kynk\u00e4\u00e4nniemi et al, Improved Precision and Recall Metric for Assessing Generative Models, NeurIPS 2019\n\n[2] Poklukar et al, Delaunay Component Analysis for Evaluation of Data Representations, ICLR 2022\n\n- When calculating conditional accuracy, do you randomise the observed modalities? For example, in Fig 5, how many times did you repeat this experiment? I believe that without std it is hard to draw conclusions. \n\n- I do not see the benefits of keeping both SBM-VAE and SBM-RAE in the experiments. First, it is not clear what version is better and why. The authors do not discuss this at all. Second, it hinders the readability of the results.\n\n- I do not fully understand the unconditional coherence evaluation. Do you sample z_1:M, then generate all x_1:M modalities and evaluate their agreement ? In Fig 6 what are \u201dsimilar modalities\u201d? Please add some more explanation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740388920,
            "cdate": 1698740388920,
            "tmdate": 1699637052303,
            "mdate": 1699637052303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ODOCNEy87w",
                "forum": "YBSEwwveMr",
                "replyto": "MBxYSf4opo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for your comments.\n\n*\u201cMissing connection and comparison of the methods in the related work section to the proposed method. In what way are SBM improving over these methods?...\u201c*\n\nMultimodal VAEs (MoPoE, MVAE, MMVAE, MMVAE+, MVTCAE) learn a joint representation (newer approaches like MMVAE+ added modality-specific representation as well) and forcing the decoder to use this joint representation reduces the generation quality of each modality while helping to improve the coherence (we have seen a similar effect with fine-tuning our decoder to increase coherence see Section A.4.2). Our work, on the other hand, does not learn a joint representation but learns a joint distribution over individual representation, this way the decoder and individual learned representation remains intact and decoder uses only modality-specific representation. In addition, our method exhibits superior coherence in unconditional generation. When no modality has been observed, previous work fails to recover the joint representation. In contrast, our approach samples from a valid joint distribution, resulting in enhanced coherence.\n\n*\u201cNo ablation studies on the components of the proposed method.\u201d*\n\nThe main components of our approach are the score-based model and coherence-guidance EBM. We already have included the results with and without EBM and we have studied the scalability of the score-based model in the presence of a different number of modalities (when trained with 2, 3, all the way up to 10 modalities shown in Figure 8 - (a,b,c)). \nIn the revised version, we also added Table 4 to the paper to explain the effects of beta on the score-based model.\nAdditionally, we added another section to explore the effect of decoder fine-tuning to improve the coherence. \nIf you can think of other ablation studies that are missing from our paper, we are willing to include them.   \n\n*\u201cLimited experimental evaluation using only image data. Both PolyMNIST and CelebAMask are image datasets. It would be beneficial to add experiments on text audio modalities as well\u201d*\n\nWe chose PolyMnist for its simplicity in performing experiments on multiple modalities. Studying the behavior of multimodal VAEs in the presence of different numbers of missing modalities requires a tractable dataset and problem setting. \nWe also added CelebA-MASK-HQ with the attributes as an additional non-image modality. In addition to that, we have added audio-image modality from the MHD dataset [1] (please see the revised version).\n\n*\u201cI find the novelty of the method a bit limiting. Given that the experimental results do not report any standard deviations makes it hard to judge the weight of the contribution.\u201d*\n\nIn our graphs, we plot the standard deviation as a shade in the figures (more visible when zoomed). We report the mean over three-experiment runs on our figures and tables. We have edited the table to include the standard deviation.\n\n\n*\u201cIt has been shown by several works that FID does not provide an adequate evaluation of a generative model\u2026\u201d*\n\nDespite the extensive research for introducing alternative metrics for evaluating generative quality for images, FID has been extensively used in different generative model evaluations [2,3,4] (to name a few), including in all of the baselines in Multimodal VAEs, mostly to achieve comparison consistency.  \n\n*\u201cIt would be better to present these results with newer metrics like Improved Precision and Recall [1] or Delaunay Component Analysis [2] below)...\u201d*\n\nThank you for your suggestions, however, it is worth noting that our method is not learning a joint representation for data modalities and we rely on the learned representation from individual VAEs for each modality. We are claiming that learning the correlation among latent representations of individual modalities is enough to achieve coherence.  \n\n*\u201c Are you sure that none of the considered models experience a mode collapse? \u201c*\n\nWe have added section A.5 to investigate the possibility of mode collapse. We unconditionally generate all the modalities 10000 times and count the present labels using a pretrained classifier. All methods cover all modes (different digits) for each modality, however, MVTCAE has less balanced coverage compared to our approach and MMVAE+ . \n\n*\u201cWhen calculating conditional accuracy, do you randomize the observed modalities? For example, in Fig 5, how many times did you repeat this experiment? I believe that without std it is hard to draw conclusions.\u201d*\n\nWe have reported the standard deviation as shades under the main graph (we ran each  3 times). We use the whole test dataset to calculate the results shown, so yes, multiple data points are given to generate the results. We also revised the tables to include standard deviation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553884732,
                "cdate": 1700553884732,
                "tmdate": 1700554773926,
                "mdate": 1700554773926,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "85AY03fBWP",
            "forum": "YBSEwwveMr",
            "replyto": "YBSEwwveMr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8436/Reviewer_7BDS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8436/Reviewer_7BDS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel multimodal VAE model which addresses the problem of sample quality deterioration as the number of modalities increases, which the previous models are suffering from. The paper proposed to model separate encoder-decoder pipelines and independent posterior distributions for latent variable corresponding to separate modality to improve the generation quality. The author used a Score-based model (SBM) approach to model a joint prior distribution of latent variables for different modalities to achieve coherence among the generated modalities. The authors then performed benchmark on the modified PolyMnist and CelebAMask-HQ dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The author provides a good summary of existing work in the introduction and provided a clear explanation of the motivation of their approach.\n\nThe author was able to provide evidence of improved generative quality in terms of FID scores and qualitative analysis on the two datasets.\n\nThe paper provides interesting, and sound use case of SBM to learn a joint latent prior distribution for inference and provide cases for conditional inference with sets of observed modality and unconditional inference when no modality is observed, along with an EBM based guidance to enhance coherence for sampling."
                },
                "weaknesses": {
                    "value": "Concerns on the benchmark datasets are too simple, because these datasets have very fixed pattern characteristics, and the modality types are also relatively fixed. Therefore, it might not be sufficient to prove the stability and quality of the proposed method in the case of multi-modal and missing modes.\n\nDetails about the specific network structure in the appendix are not clear and dimensions of the latent variables are not clearly stated also.\n\nThe evaluation metric for accuracy is not clearly defined.\n\nIt is not clear if the generative quality is truly decreasing as the number of modalities increases. This relation is not clear in other models (as the generative quality though less optimal than the SBM approach, their generative quality is largely consistent along the number of modalities).\n\nThe coherence of the SBM based model does not show a clear improvement or sometimes performed not as well as previous model (especially in experiment on CelebA-Mask-HQ dataset) in terms of coherence.\n\nThe author used a modified PolyMnist dataset but not clearly defined how the new modalities are generated."
                },
                "questions": {
                    "value": "Please define the accuracy metric. Or is it a term used interchangeably with metric coherence?\n\nPlease specify the dimensions of the latent variables, are those 1D vectors or 2D matrices? How is Unet applied in this case?\n\nThe reviewer acknowledges that the generative quality in terms of FID and qualitative analysis shows an improvement, however the coherence is not improving compared to other works. It would be great if the authors could justify why better generative quality is worth sacrificing for less optimal performance for coherence in the context of multimodality learning.\n\nIt would be great if the author could provide more on how the modified PolyMnist dataset is generated consider this is a new version of the original dataset."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8436/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8436/Reviewer_7BDS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782836812,
            "cdate": 1698782836812,
            "tmdate": 1700939415883,
            "mdate": 1700939415883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1VIKBKO7MJ",
                "forum": "YBSEwwveMr",
                "replyto": "85AY03fBWP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nThank you for your comments.\n\n*\u201cConcerns on the benchmark datasets are too simple, because these datasets have very fixed pattern characteristics, and the modality types are also relatively fixed.\u201d*\n\nExtended PolyMnist enables us to study our approach and baselines in the presence of different numbers of missing modalities. The number of required experiments for such a study is prohibitive for using a more complex benchmark. \nTo showcase the performance of our approach on a high-dimensional setting we have used the high-dimensional CelebA-Mask-HQ dataset which is far more difficult and also includes attribute modality.\nIn addition, to address the reviewer's concern regarding the type of modalities, we have added audio-image modality from the MHD dataset by [5] (please see Section A.8)\n\n*\u201cDetails about the specific network structure in the appendix are not clear and dimensions of the latent variables are not clearly stated also.\u201d*\n\nWe have stated the dimensions of the latent variables in Appendix A.2. We have used a latent size of 64 for our models and MMVAE+ and we increased the latent dimension of the other baselines to 64 \u2217 n where n is the number of modalities.\nFor example, for 10 modalities, we used a latent size of 64 for unimodal VAEs of SBMs and MMVAE+ (because it uses modality-specific latent representation), but for the other baselines, we used a latent size of 640 dimensions to have a fair comparison.\n\n*\u201cThe evaluation metric for accuracy is not clearly defined.\u201d*\n\nThe coherence is modality-specific. For extended PolyMNIST, when there is no observed modality we have reported the number of agreeing modalities as coherence. When there is an observed modality, the coherence reduces to the agreement of the predicted modality with the observed modality, which is accuracy. For example, for each image in the observed modality, we checked if the label of the predicted target modality agrees with the label of the observed modality, and reported the test-set average. To obtain the label of predicted modality we have used a pre-train classifier. \nFor CelebA-Mask-HQ, we used the test set average of the F1 score between the predicted attributes (or mask pixel) and true attributes (or mask pixel) in the test set given the observed modality (e.g. image).\n \n\n*\u201cIt is not clear if the generative quality is truly decreasing as the number of modalities increases.\u201d*\n\nGenerative quality decreases (FID increases) slightly for Product of Expert (PoE) models which use additional information as the number of observed modalities increase but remains fairly flat for mixture models which sample modalities from observed ones. \n\n*\u201cThe coherence of the SBM based model does not show a clear improvement or sometimes performed not as well as previous model (especially in experiment on CelebA-Mask-HQ dataset) in terms of coherence.\u201d*\n\nThe main reason is that the SBM-AE learns to generate samples from the missing modalities and for structured prediction tasks such as mask prediction or attribute prediction, the samples may be different slightly, therefore our approach has been penalized by the used F1 score metrics. On the other hand, using joint representation results in average outputs which works in the favor of the other multi-modal VAEs.\n\n*\u201cThe reviewer acknowledges that the generative quality in terms of FID and qualitative analysis shows an improvement\u2026.\u201d*\n\nYes, that\u2019s correct, but as [4] also states, the generation quality of multimodal VAEs makes them unusable in real-world applications. So, the main contribution was on the quality while having comparable coherence to previous methods. Our models also outperform previous works in terms of unconditional coherence and quality.\n\n*\u201cIt would be great if the author could provide more on how the modified PolyMnist dataset is generated consider this is a new version of the original dataset.\u201d*\n\nThe new modalities are generated the same way as PolyMnist by changing the background of the Mnist dataset as stated in Appendix A.2. We have attached the backgrounds used, the dataset, and the code that was used to generate them in the zip file.\n\n\n*\u201cHow is Unet applied in this case?\u201d*\n\n Unet was used by resizing 64 dimensions of PolyMnist to 8x8 and 256 dimensions of CelebA-Mask-HQ to 16x16 as mentioned in Appendix A.2 and A.5 respectively. \n\n\n[1]Thomas M. Sutter, Imant Daunhawer, and Julia E. Vogt. Generalized multimodal ELBO. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= 5Y21V0RDBV"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553489358,
                "cdate": 1700553489358,
                "tmdate": 1700554921645,
                "mdate": 1700554921645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DnZjA6ZtHy",
                "forum": "YBSEwwveMr",
                "replyto": "1VIKBKO7MJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8436/Reviewer_7BDS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8436/Reviewer_7BDS"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the response. The reviewer appreciate the your response to our concerns and found the clarifications was able to address the confusion and the confusion of the model architecture, metrics and dimensions of latent variables. However, there are still a few issues remain.\n\n1. For the reviewer's original concern on \"It is not clear if the generative quality is truly decreasing as the number of modalities increases. ...\" The reviewer understand the author pointed out certain model shows such trend, but consider the subtly of the increase universally across the models and the limited testing on just two datasets it would not be rigorous enough to make such claim. But the review do acknowledge the SBM-XAE models enhances the generative quality and thank you for the reference one the importance of the quality factor. \n\n2. It would be great if the author could further elaborate on why F1 metric is not in favor of the SBM-XAE. Also, consider the use case of  the conditional denoising SBM, why does the author have to use separate VAEs for separate modalities. Why not add a VAE that jointly encode and decode the modalities? Following the author's rebuttal argument, is that going to further enhance the coherence performance?\n\n3. The reviewer appreciate the new experiment on the MHD dataset. We notice there is a big increase in the relative performance for the coherence metric relative to other models. Could the author elaborate why the model suddenly performs much better than other SOTA models?\n\nThe reviewer again thank the author for their response to our concerns."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709385718,
                "cdate": 1700709385718,
                "tmdate": 1700709385718,
                "mdate": 1700709385718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jRaGoaDqDb",
            "forum": "YBSEwwveMr",
            "replyto": "YBSEwwveMr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8436/Reviewer_NJdA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8436/Reviewer_NJdA"
            ],
            "content": {
                "summary": {
                    "value": "To improve over existing multimodal VAEs, the paper proposes to train unimodal VAEs independently, in order to achieve high generative quality, and a score-based model to learn a joint latent space across modalities and achieve semantic coherence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper deals with the relevant problem of tackling the current limitations of multimodal VAEs. \n- Interesting and encouraging results are shown to back up the effectiveness of the proposed approach. In particular, it is encouraging that the performance of the proposed approach can benefit from the presence of more modalities in terms of performance, and does not suffer from the limitations uncovered in recent work [1]. \n- The comparisons reflect the state-of-the-art in the field, with recent approaches included.\n\n\n[1] Daunhawer I, Sutter TM, Chin-Cheong K, Palumbo E, and Vogt JE. On the limitations of multimodal VAEs. In International Conference on Learning Representations, 2022."
                },
                "weaknesses": {
                    "value": "- To me it is unclear why the score-matching model would lead to a coherent shared latent space, and I would appreciate if authors would clarify that. While in the first step of training we have a prior $p(z_{1:M})=\\prod \\mathcal{N}(0, \\sigma I)$, what are the modelling assumptions on the parametric prior $p_{\\theta}(z_{1:M})$ in the second step?\n- The comparison with MVTCAE [2] from e.g. Figures 4 and 5 is quite important and should be further commented, with maybe more insights (eg. different $\\beta$ values, average performance across modalities for conditional generation, see below) for at least two reasons. First as it does not sub-sample modalities during training, MVTCAE is not subject to the limitations for generative quality uncovered in [1]. Second, MVTCAE and the proposed model are really similar in performance, and from the results it is unclear which one performs better overall (at least in the first experimental setting). \n- Why reporting only the results on the last modality in e.g. Figure 4? (Even though also results on the first modality are available, in the Appendix).This is only a partial insight on model performance, and results should be averaged across modalities for conditional generation (not modalities used for inference, modalities used for generation) to show performance is consistently good. To back up this point, results on the third modality (Figure 3) indicate that MVTCAE has poor generative quality. However, FIDs in Figure 4 for the last modality indicate otherwise. Hence, it seems that in evaluating model performance, one might want to control for the effect of choice of modality.  \n- Did the authors do an ablation for different values of $\\beta$ for the compared models in e.g. Extended PolyMNIST? In the Appendix it is stated that the $\\beta$ was chosen using the validation set. How exactly was it chosen? Should it be by looking at the ELBO value on the validation set, one should be careful as likelihood values prove often not to be representative when it comes to performance of multimodal VAEs. For instance, with models that subsample modalities, high likelihoods for conditional reconstruction across modalities can be obtained by producing average-looking samples (since the modality-specific information about the sample to be reconstructed cannot be inferred). I think it would be important to report results for the compared models across $\\beta$ values instead of reporting results only for a single value. To back up this point, the conditional generation results from Figure 3 for e.g. MVTCAE seem rather different from what reported original work [2] on PolyMNIST, in which the authors use a much higher $\\beta$ value ($\\beta=2.5$ I think).  \n- I found many typos and imprecisions in the manuscript, that has margin for improvement in writing quality. \n\n[1] Daunhawer I, Sutter TM, Chin-Cheong K, Palumbo E, and Vogt JE. On the limitations of multimodal VAEs. In International Conference on Learning Representations, 2022. [2] Hwang HJ, Kim GH, Hong S, and Kim KE. Multi-view representation learning via total correlation objective. In Advances in Neural Information Processing Systems, 2021."
                },
                "questions": {
                    "value": "- I would change \"alternative\" to \"novel\" in the Abstract. Not really clear what the approach is \"alternative\" to otherwise. \n- In the Introduction, I can suggest to make the difference between \"Scalability\" and \"Conditional modality gain\" clearer. As I understand it, scalability refers to the fact that a multimodal VAE trained on a given number of modalities should perform at least as well as the same model trained on fewer modalities. On the other hand, conditional modality gain refers to test-time conditional generation performance improving when more modalities are given for inference.  I would suggest to make a clearer distinction in the text between the two concepts. \n\nFor questions and suggestions to the authors, see also \"Weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8436/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8436/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8436/Reviewer_NJdA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845406397,
            "cdate": 1698845406397,
            "tmdate": 1699637051997,
            "mdate": 1699637051997,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4NbxFfXHj1",
                "forum": "YBSEwwveMr",
                "replyto": "jRaGoaDqDb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nThank you for your comments.\n\n*\u201cTo me it is unclear why the score-matching model would lead to a coherent shared latent space, and I would appreciate if authors would clarify that\u2026\u201d*\n\nOur approach leverages the fact that SBM implicitly learns the underlying data distribution by capturing the correlation among their input variables, which are here the latent representation learned by uni-variate VAEs. SBMs offer tractable joint or marginal sampling, which has been used to sample the latent representation of a missing modality. Therefore, our approach does not require learning an explicit joint representation which is common in the prior multi-modal VAEs.\n\n *\u201cwhat are the modelling assumptions on the parametric prior in the second step?\u201d*\n\nIn the second step we learn the parametric prior which has been parameterized by a UNET.\nWe don\u2019t enforce any specific assumption for training the SBM.\n\n*\u201cdfferent $\\beta$ values, average performance across modalities for conditional generation...\u201d*\n\nWe have added Table 3 in the paper listing the average performance (average of each modality given the rest) of each model as well as the performance for individual modalities in Figure 13 and Figure 14.  MVTCAE results in better accuracy while having a higher FID score due to existing joint representation. When none of the modalities has been observed (unconditional generation), our approach is superior to all of the prior multimodal VAEs including MVTCAE. This is a very important property in scenarios like generating a piece of coherent music consisting of multiple instruments (with each instrument as a modality).\n\n*\u201cWhy reporting only the results on the last modality in e.g. Figure 4?\u201d *\n\nWe have added Table 3, Figure 13, and Figure 14 in the revised version.\n\n*\u201cDid the authors do an ablation for different values of beta for the compared models in e.g. Extended PolyMNIST\u201d*\n\nPlease see the revised section of A.4.2\n\n*\u201cIn the Appendix it is stated that the model was chosen using the validation set. How exactly was it chosen\u201d*\n\nModels were selected using a combination of the task metrics (fid and coherence in both conditional and unconditional settings) to address the existing tradeoff. Note that MVTCAE has a tradeoff between conditional and unconditional performance with respect to different beta. For example, MVTCAE performs better conditionally with lower beta but worse unconditionally. The experiments can be replicated using the zip file we have attached.\n\n*\u201cI would change \"alternative\" to \"novel\" in the Abstract. Not really clear what the approach is \"alternative\" to otherwise.\u201d*\n\nWe employed the term 'alternative' to emphasize the distinctiveness of our method compared to conventional Multimodal VAE techniques, which typically rely on joint representations.\n\n*\"Scalability\" and \"Conditional modality gain\":*\n\nThank you for pointing that out. We refer to Scalability as computational efficiency when adding modalities while performing comparably to a model trained with a lower number of modalities. We have edited that line to make it more clear."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553162221,
                "cdate": 1700553162221,
                "tmdate": 1700555010102,
                "mdate": 1700555010102,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mXi8HZ29IP",
            "forum": "YBSEwwveMr",
            "replyto": "YBSEwwveMr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8436/Reviewer_qLtQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8436/Reviewer_qLtQ"
            ],
            "content": {
                "summary": {
                    "value": "In this study, the researchers introduce a multimodal modeling framework that utilizes distinct encoder-decoder pairs for each type of data input. They devise a method to learn a shared prior for the latent variables that represent different modalities, in order to understand the interconnections between them. Each modality's training is optimized through the maximization of its Evidence Lower Bound (ELBO). The latent variables for each modality are then independently generated using the unimodal encoders. These variables are jointly modeled using a denoising score-matching technique. Additionally, the latent variables are trained to be consistent with each other through a noise-contrastive approach, and the gradient of the noise-contrastive model is used to adjust the scores for latent variables that are not observed.\n\nUsing their proposed method, the authors demonstrate that the Frechet Inception Distance (FID) score of the created outputs remains stable, even as the number of modalities grows. Moreover, the congruence between the modalities, measured by how accurately the class label of the anticipated modality can be predicted, is enhanced when the model includes a mechanism for coherence guidance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper is well-written and easy to read.\n\n2) The primary novelty of the paper stems from its simplicity. While the previous papers have been focussed on modeling all the modalities together, this paper decomposes the problem into two independent aspects\n - Learn latent variables for each modality individually\n - Learn a joint distribution over the latent variables.\nThis is reminiscent of Dall-E [1] where a separate discrete VAE is learned for images followed by a mapping from the text to the latent variables. In [2], a separate VAE is learned for each modality while simultaneously minimizing the KL divergence between the latent variables. However, none of these approaches can be extended directly to more than 2 modalities. \n\nThis paper is most reminiscent of the MMVAE+ paper that learns unimodal as well as cross-modal features. \n\n3) The results in this paper (particularly, the idea of learning latent variables independently) can be significant for the multimodal community.\n\n\n[1] Ramesh, Aditya, et al. \"Zero-shot text-to-image generation.\" International Conference on Machine Learning. PMLR, 2021.\n[2] Pandey, Gaurav, and Ambedkar Dukkipati. \"Variational methods for conditional multimodal deep learning.\" 2017 international joint conference on neural networks (IJCNN). IEEE, 2017."
                },
                "weaknesses": {
                    "value": "1) The proposed approach assumes that all the modalities are present during training. It can't be used for training with missing modalities, unlike other joint learning-based approaches. However, in my opinion, this is not  a major concern\n2) Since the latent variables for each modality are learned independently, they can be highly misaligned. Perhaps, the authors must consider incorporating information from coherence-guided EBM while training the latent variables.\n3) Equation (5) doesn't make much sense since z_u is present in the LHS while also getting marginalized in RHS."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8436/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699256600141,
            "cdate": 1699256600141,
            "tmdate": 1699637051848,
            "mdate": 1699637051848,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0qrxh75BSk",
                "forum": "YBSEwwveMr",
                "replyto": "mXi8HZ29IP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8436/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8436/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nThank you for your comments.\n\n*\u201cThe proposed approach assumes that all the modalities are present during training\u2026\u201d*\n\nYes, as highlighted in our submission, our current model operates under the assumption that all modalities are present. For future work, we plan to adapt our model to train effectively even when certain modalities are missing. This may be accomplished by modifying the score matching objective to mask missing modalities or by imputing the missing modality using sampling from the so-far trained model.\n\n\n*\u201cSince the latent variables for each modality are learned independently, they can be highly misaligned\u2026\u201d*\n\nWe acknowledge that the latent representation learned by our model may not be aligned. Indeed, incorporating an alignment constraint could potentially simplify the training process for the score model, and we plan to explore this possibility in future research. Nonetheless, a strong and robust score model captures the correlation among the latent representations of various modalities. This is particularly important for our SBM-AE framework, as it is designed to work with any pre-trained VAE. Moreover, relying on coherence guidance from an EBM might not be advantageous during the training phase. This is because, at this stage, all modalities are available and inherently coherent, rendering the EBM somewhat superfluous for the SBM. However, during inference time,  in scenarios where multiple modalities are missing, the EBM becomes vital for the SBM. It aids in more effectively navigating the joint score landscape, thereby enhancing the model's performance in predicting more coherent output.\n\n*\u201cEquation (5) doesn't make much sense\u201d*\n\nThank you for pointing out the error. We have updated the section."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552846350,
                "cdate": 1700552846350,
                "tmdate": 1700552846350,
                "mdate": 1700552846350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WJDF6l7GHk",
                "forum": "YBSEwwveMr",
                "replyto": "0qrxh75BSk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8436/Reviewer_qLtQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8436/Reviewer_qLtQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks. I have read the rebuttal. Based on my understanding of this work, I would keep my rating"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8436/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730099597,
                "cdate": 1700730099597,
                "tmdate": 1700730099597,
                "mdate": 1700730099597,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]