[
    {
        "title": "Uncovering Causal Variables in Transformers Using Circuit Probing"
    },
    {
        "review": {
            "id": "ha4x1KH2Nu",
            "forum": "sZq3lDDETp",
            "replyto": "sZq3lDDETp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_Ae4c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_Ae4c"
            ],
            "content": {
                "summary": {
                    "value": "The paper propose a new \"circuit probing\" method to detect some properties for the complicated network. the method utilizes a binary masking model to detect certain pattern, without the need to decode the intermediate representation, or know the causal pattern beforehand."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. paper is well written and easy to follow.\n2. experiment looks good"
                },
                "weaknesses": {
                    "value": "1. please add formula to the methodology part. that part is the most important but the description is vague.\n    (for the training, what's the input looks like? how does the iteration from T_0 to T_N look like? , etc)"
                },
                "questions": {
                    "value": "1. For language model, all the experiment is on the syntactic level, is there anyway we can show the \"circuit probing\" can detect some semantic level pattern?\n2. seems that we still need to know the pattern we want to detect and train the model before probing. I'm wondering what's the use case of this kind of probing? Is there a way to help us probe some unknown pattern that is commonly shown up in the dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Reviewer_Ae4c"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698247399361,
            "cdate": 1698247399361,
            "tmdate": 1699637086484,
            "mdate": 1699637086484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "xd6XIylk3R",
            "forum": "sZq3lDDETp",
            "replyto": "sZq3lDDETp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_TVnz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_TVnz"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a method to interpret the causal variable for the prediction of the model. The proposed method uses a weight mask to learn the subset of whole parameters as an interpretation. Experiments on both synthetic and real-world models could illustrate that the proposed method could generate some reasonable explanation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The studied problem is important for building a transparent model.\n2. The proposed method is easy to implement."
                },
                "weaknesses": {
                    "value": "1. The learned mask by the proposed method is still \"correlation\" rather than \"causal\". What is the relationship between the proposed method with Pearl's causal ladder? A causal model should allow the model to intervene.\n2. The proposed method is quite tricky. What is the difference with the Lottery Ticket Hypothesis. \n3. The experiments are not extensive enough. The paper should conduct more experiments on advanced LLM methods."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Reviewer_TVnz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643293870,
            "cdate": 1698643293870,
            "tmdate": 1699637086381,
            "mdate": 1699637086381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "tNJdg6MBJN",
            "forum": "sZq3lDDETp",
            "replyto": "sZq3lDDETp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_UXVb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_UXVb"
            ],
            "content": {
                "summary": {
                    "value": "Here is a summary of the key points from the paper:\n\nMain Contribution: \nThe paper proposes a new analysis technique called \"circuit probing\" to uncover intermediate causal variables in Transformers. Circuit probing introduces a trainable binary mask over model weights to find low-level circuits that compute hypothesized high-level variables.\n\nWhat is Circuit Probing:\n- Circuit probing optimizes a binary mask over model weights to uncover a circuit that computes a hypothesized intermediate variable. \n- It tests if the variable is represented in the model and causally implicated in model behavior.\n- The method looks for model components whose outputs are partitioned according to the variable.\n\nNovelty:\n- More faithful to model's computation than probing or causal abstraction analysis. \n- Does not require full causal graph specification like causal abstraction analysis.\n- Can uncover development of circuits during training unlike other methods.\n\nExperiments:\n- Simple arithmetic tasks to characterize model algorithms and modularity. Compared to probing and causal abstraction analysis.\n- Analyzed circuit formation during training on arithmetic task. Compared to linear probing. \n- Analyzed syntactic phenomena (subject-verb agreement, reflexive anaphora) in GPT-2 small and medium. \n\nPerformance:\n- Converged with other methods on simple tasks but more superior at assessing modularity. \n- More faithful to circuit development during training than linear probing.\n- Successfully uncovered circuits for syntactic phenomena in GPT-2 while baselines had little impact.\n\nConclusion:\n- Circuit probing is effective for gaining insights into algorithms, structure, and training dynamics of Transformers. \n- It is more faithful to the model than probing and does not require full causal graphs.\n- Can be applied to analyze real-world models like GPT-2.\n\nIn summary, the paper proposes circuit probing as a novel technique for interpretability that can uncover intermediate causal variables in Transformers without needing to specify a full causal graph. Through experiments on arithmetic tasks and GPT-2, it demonstrates the effectiveness of circuit probing compared to probing and causal abstraction techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Proposes a novel analysis technique that addresses limitations of existing methods like probing and causal abstraction analysis. It does not require specifying a full causal graph and is more faithful to the model's internal computations.\n\n- Demonstrates the utility of circuit probing through a diverse set of experiments - on simple arithmetic tasks, training dynamics, and real-world models like GPT-2. The experiments comprehensively evaluate different aspects of the method.\n\n- Provides both quantitative evaluations through metrics like nearest neighbor classification accuracy as well as qualitative evaluations through targeted ablation studies. This helps establish that the identified circuits are causally implicated in model behavior.\n\n- Compares circuit probing extensively to other analysis techniques like probing, counterfactual embeddings and causal abstraction analysis. This contextualizes the relative strengths of circuit probing.\n\n- Makes code and detailed experimental setup available to facilitate reproducibility.\n\n- Theoretically grounded in the idea of finding model components that exhibit equivalence class partitioning based on hypothesized variables.\n\n- Opens up many future research directions like using circuit probing to assess social biases, model safety, compositionality of circuits, etc.\n\nOverall, the solid theoretical motivation, extensive empirical validation, thorough comparison to existing methods, and potential for future work make this a strong paper on an important topic in interpretability research. The novelty of circuit probing and its demonstrated effectiveness on real-world models like GPT-2 make it a valuable contribution."
                },
                "weaknesses": {
                    "value": "- The method is currently limited to analyzing individual circuits in isolation and cannot characterize how multiple circuits compose to produce model behavior. This limits the types of counterfactual analyses that are possible.\n\n- Most experiments are on relatively small toy models and simple arithmetic tasks. More evaluation on large state-of-the-art models trained on complex real-world tasks would be useful.\n\n- Only two syntactic phenomena were evaluated for language models. Testing on a broader set of linguistic capabilities would strengthen the results. \n\n- The comparison to causal abstraction techniques is limited since the full causal graphs required by those methods are rarely available for complex real-world tasks.\n\n- Ablation studies treat the model as a black box. A more fine-grained understanding of how the ablations affect model representations and computations is lacking.\n\n- Quantification of how faithfully the identified circuits match human-specified intermediate variables is missing.\n\n- Theoretical analysis of why circuit probing reveals more faithful circuits compared to probing is not provided.\n\n- Hyperparameter selection for circuit probing is not extensively analyzed. The impact of hyperparameters on results is unclear.\n\n- The paper reiterates some known limitations of probing methods but does not provide novel insights into why probing fails.\n\nOverall, while the paper makes a strong case for circuit probing, more rigorous evaluation on complex real-world tasks, tighter integration of theory, and thorough hyperparameter analysis could make the results even more compelling. Nonetheless, the limitations do not undermine the meaningful contributions made."
                },
                "questions": {
                    "value": "1. How well does circuit probing scale to large state-of-the-art models trained on complex real-world tasks? The current experiments are on relatively small models and simple arithmetic tasks.\n\n2. Can we characterize theoretically why circuit probing surfaces more faithful intermediate circuits compared to standard probing techniques? \n\n3. How sensitive are the discovered circuits to hyperparameters like optimization loss functions, regularization, learning rate schedules, etc.? \n\n4. How quantitatively aligned are the circuits found by probing to human-provided definitions of the intermediate variables? \n\n5. How do the identified circuits compositionally interact within a model to produce predictions? Understanding circuit compositionality could enable more powerful interventions and counterfactuals."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698703858491,
            "cdate": 1698703858491,
            "tmdate": 1699637086248,
            "mdate": 1699637086248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "RkovQsPeIf",
            "forum": "sZq3lDDETp",
            "replyto": "sZq3lDDETp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_LGtg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_LGtg"
            ],
            "content": {
                "summary": {
                    "value": "This paper is addressing the challenge of DNNs interpretability. Specially the authors are introducing a new analysis technique \u2013 circuit probing \u2013 that automatically uncovers low-level circuits that compute hypothesized intermediate variables. The authors aims at capturing causal relationships within the DNNs inner behavior."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ Very interesting research question"
                },
                "weaknesses": {
                    "value": "- Unclear how this could scale\n- Unclear on the soundness of the approach \n- No real contribution, exploitable by other teams \n- No comparison with other causal approaches \n- Experimentation limited: scale, dataset, state-of-the-art comparison\n- No real lessons learnt - difficult to see how does that will help the community"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785427005,
            "cdate": 1698785427005,
            "tmdate": 1699637086114,
            "mdate": 1699637086114,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "UL97OiKjuo",
            "forum": "sZq3lDDETp",
            "replyto": "sZq3lDDETp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_pfUb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8667/Reviewer_pfUb"
            ],
            "content": {
                "summary": {
                    "value": "It is difficult to understand the algorithms implemented by large language models to solve various tasks. Therefore, this paper studies circuit probing, where they find a subnetwork in the model that computes some hypothesized intermediate variable. To test whether this variable is causal, they then ablate the circuit and examine the resulting accuracy drop compared to ablating a random subnetwork. They find that (1) multitask models trained on simple arithmetic tasks can reuse circuits, (2) circuit probing, when applied to the correct intermediate variable, can track the progress of grokking on a simple arithmetic task, and (3) for subject-verb and reflexive anaphora agreement, one can probe models for a subject number circuit."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The authors present an intriguing and creative set of experiments, with interesting results on circuit modularity and tracking the progress of grokking.\n\n(2) The circuit probing method in the paper seems well-thought-out and seems to have many advantages over existing probing methods.\n\n(3) The paper is well organized and easy to follow."
                },
                "weaknesses": {
                    "value": "The method and experiments in the paper seem very cool and have potential. However, overall, I think the claims of causality are not totally sound, and the claims of improved probing faithfulness should be supported by a wider range of experiments and comparisons to existing work.\n\n(1) From what I understand, the only supporting evidence for \"causality\" in the paper is that ablating the found circuit produces a larger accuracy drop than ablating a random circuit. To me, this is a far cry from other works in causal probing which actually perform interventions (which the authors acknowledge in the limitations section). I don't think you can use the word \"causal\" given this small amount of evidence - for example, the found circuit could include parameters important to the task even if the hypothesized intermediate variable is not causal in the sense of interventions.\n\nOne possible remedy could involve fine-tuning the found circuit. For example, in a model which implements (a^2 + b^2), what if you fine-tuned the a^2 circuit into a 3a^2 circuit? Would the final model end up computing (3a^2 + b^2) instead?\n\n(2) At this point, there are a wide range of related probing methods in existing papers, including counterfactual embeddings (https://aclanthology.org/2021.findings-acl.76.pdf), amnesic probing (https://arxiv.org/abs/2006.00995), and subnetwork probing (https://arxiv.org/abs/2104.03514). To differentiate circuit probing from these methods, I think the paper needs a more detailed discussion of the advantages and differences of their method, as well as experiments to support this discussion -- the paper has a few such experiments but would benefit from a more systematic comparison between methods.\n\n(3) The most convincing and interesting results in this paper (grokking, modularity) involve very simple arithmetic tasks, while the LLM probing experiments are much weaker. I love the arithmetic task experiments, but given that probing is most useful for opaque models and opaque tasks, the paper would benefit from more convincing LLM experiments."
                },
                "questions": {
                    "value": "(1) Are the models trained with dropout? Intuitively it seems to me that models trained with dropout should have redundant circuits, such that ablating one of the circuits does not affect accuracy, given that dropout ablates random subnetworks during training.\n\n(2) Related to point (1) in the weaknesses section, in what sense are you using the word \"causal\"? What is the concrete causal claim being made and how do the experiments support that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8667/Reviewer_pfUb"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792096238,
            "cdate": 1698792096238,
            "tmdate": 1699637085984,
            "mdate": 1699637085984,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]