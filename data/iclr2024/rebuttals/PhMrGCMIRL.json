[
    {
        "title": "Fusing Models with Complementary Expertise"
    },
    {
        "review": {
            "id": "etPvlJ7HI0",
            "forum": "PhMrGCMIRL",
            "replyto": "PhMrGCMIRL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8"
            ],
            "content": {
                "summary": {
                    "value": "Acquiring models that generalize across multiple tasks and domains has long been a challenge for the machine learning community. This paper presents the idea of ***Fusion of Experts (FoE)***, aiming to combine the strengths of multiple models with complementary expertise to push their collective generalization capabilities. The main contribution of the paper is to formulate the FoE problem as an instance of supervised learning, which is applicable to both discriminative and generative use cases. In addition, an extended ***FrugalFoE*** has been proposed to allow efficient expert fusion while only evaluating a subset of experts at test time.\nExtensive experimental evaluations on a wide range of tasks demonstrate that the proposed fusion method significantly improves the performance of individual experts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation is sound, and the paper is well written.\n- The proposed method casts the FoE problem of fusing outputs of models with complementary expertise as a supervised learning problem. It can be applied to both discriminative and generative use cases.\n- The extended Frugal Fusion of Experts (FrugalFoE) allows to efficiently perform expert fusion by only evaluating a subset of experts at test time. \n- The proposed fusion method greatly improves the performance of individual experts on a wide range of tasks, while also reducing the number of expert evaluations at test time."
                },
                "weaknesses": {
                    "value": "The primary concern for me is that the proposed fusion method relies on a validation set containing data samples from all $K$ domains, and potentially, the distribution of the validation data is very similar to that of the test data. I can certainly understand that traditional approaches of combining expert predictions may be ineffective, and they mostly use heuristic schemes such as averaging models' outputs or using the most confident model. However, they do not assume that there is additional validation data to access. If there is available validation data, can we train a parameterized fuser for traditional methods? Therefore, this is not fair in a sense, or can I understand that this is a setup for a new fusion task?\n\nOther concerns are as follows.\n- In the experimental section, the authors observed that \"using a single expert was almost as good as using all experts\" on different types of tasks. Could this possibly be the result of an illogical experimental setup? It looks like the knowledge between multiple experts is not complementary but redundant.\n- In Table 3, what is the difference between CNN DM Expert (higher part) and CNN DM Expert only (lower part)? Why is there such a wide performance gap between the two?"
                },
                "questions": {
                    "value": "- Please take a look at **Weaknesses**.\n- Are \"Fusion of Experts\" and \"Mixture of Experts\" two different concepts, and what is the essential difference between them?\n- How to ensure that the knowledge of different experts is complementary?\n- How well does the proposed method perform with test data from outside the $K$ domains?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No potential ethical issues found."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4271/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8",
                        "ICLR.cc/2024/Conference/Submission4271/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698381213099,
            "cdate": 1698381213099,
            "tmdate": 1700637247510,
            "mdate": 1700637247510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LcIHlagYtb",
                "forum": "PhMrGCMIRL",
                "replyto": "etPvlJ7HI0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to reviewer w4J8 (Part-1)"
                    },
                    "comment": {
                        "value": "**Major concern**: *the fusion based method relies on a validation set (+ additional assumption that the validation set should be similar to the test set).*  \n**Response**: Please kindly refer to our general response.  \n\\\n\\\n**Concern**: *If there is available validation data, can we train a parameterized fuser for traditional methods? Therefore, this is not fair in a sense, or can I understand that this is a setup for a new fusion task?*  \n\n**Response**: We kindly request that the reviewer provides further clarification on their concern, as we are not entirely clear on the issue.  \n\nWhen comparing our proposed FoE method to traditional methods, the key difference lies in the approach. Traditional ensembles aggregate experts' outputs through methods like simple averaging or majority voting. Thus, not like FoE, it is test sample agnostic. The MoE method, on the other hand, requires joint training of experts with the gating mechanism. In contrast, FoE allows the use of pre-developed expert models, which are then fused for enhanced performance. This is achieved by training a lightweight fuser model, which is responsible for selecting the most suitable expert for each test sample.  \n\\\n\\\n**Concern**:  *Using a single expert was almost as good as using all experts: the result of an illogical experimental setup? It looks like the knowledge between multiple experts is not complementary but redundant.*  \n\n**Response**: We would like to argue that the experts are indeed complementary, especially when considering Tables 2 and 3, where each expert performs well on only one task, and there is an expert performing well on each task. The term \"redundant\" might refer to our MMLU experiment, as discussed in the paper. In that experiment, we explored the concept of \"weak experts,\" where none of the LLMs are true experts in any of the MMLU categories. Consequently, it is very likely that their knowledge contains redundancy. However, our experiment shows that even in such a scenario (i.e., beyond our initial problem setting), FoE can still deliver a decent performance boost.  \n\\\n\\\n**Concern**: *Some more explanations about the results in Table 3.*  \n\n**Response**: \"CNN DM Expert'' refers to the accuracy of the CNN DM expert on each sentiment analysis task, serving as a baseline to indicate individual expert performance. \"CNN DM Only\" assesses the fuser's performance when only the embedding feature of the CNN DM expert is used for training. This demonstrates that FoE is inherently frugal on the summarization task, as relying on a single expert's embedding is sufficient for accurate expert selection. And that explains why there is a large performance gap between those results. We modified Table 3 to make this less confusing, thanks for pointing it out!  \n\\\n\\\n**Concern**: *Are \"Fusion of Experts\" and \"Mixture of Experts\" two different concepts, and what is the essential difference between them?*  \n\n**Response**: Let's discuss this under three conditions:  \n\nFirst, when all experts are trained jointly with the gating network in MoE, it results in a new model architecture. Training MoE, such as a SwitchTransformer, can be computationally and resource-intensive. In contrast, FoE focuses on utilizing any pre-trained experts and fusing them, requiring only a small fuser model to be trained. Thus, the development cost of FoE can be much lower than MoE in this scenario.  \n\nSecond, when all experts are pre-trained, and one aims to build a MoE using these experts with a gating network that directly processes the data (similar to what the reviewer 2UmK asked about), the gating network must be complex, such as a convolutional neural network or ViT for images, or a BERT/RoBERTa model for sequences of tokens. Compared to such scenarios, FoE requires a much simpler fuser architecture, needing nothing more complex than an MLP or kNN model.  \n \nThird, when all experts are pre-trained, and one wants to build a MoE with the gating network using experts' outputs or intermediate results, FoE is conceptually identical to MoE in this scenario, however we are unaware of prior work considering MoE in such a setting, nor extending it to the frugal setting as in our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699981404378,
                "cdate": 1699981404378,
                "tmdate": 1699982618394,
                "mdate": 1699982618394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0zdsosy89s",
                "forum": "PhMrGCMIRL",
                "replyto": "GvIa7NbtK3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for response"
                    },
                    "comment": {
                        "value": "I thank the authors for their thorough and patient responses, and their explanations addressed most of my concerns. After carefully reading the comments of other reviewers and the newly updated paper, I still have some questions.\n- To me, the only difference between FoE with one expert only and that expert individually (*e.g.*, **FoE w/ CNN DM Expert features only** vs. **CNN DM Expert** in Table 3) is that FoE additionally trains a *fuser* or *projector* (MLP) on top of the features provided by the expert (**CNN DM Expert**) using validation data. Do I understand correctly?\n- If my understanding of the above question is valid, does it mean that we can achieve a decent performance on test data from all domains by simply hooking up a *projector* (MLP) after any expert's feature layer and fine-tuning it with validation data?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548212150,
                "cdate": 1700548212150,
                "tmdate": 1700548212150,
                "mdate": 1700548212150,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0DiKLE8Ibx",
                "forum": "PhMrGCMIRL",
                "replyto": "COz5fgd3CF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8"
                ],
                "content": {
                    "title": {
                        "value": "One more question"
                    },
                    "comment": {
                        "value": "I appreciate the clear explanation, I now better understand the role that the MLP fuser plays. Therefore, the **FoE w/ CNN DM Expert features only** just implies that only **CNN DM Expert's** features are used for training the fuser, and for testing, it is still necessary to send all the experts' features to the fuser to decide which expert's features to use, is that right?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626404113,
                "cdate": 1700626404113,
                "tmdate": 1700626404113,
                "mdate": 1700626404113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1HOAqSYlvB",
                "forum": "PhMrGCMIRL",
                "replyto": "5BXACsbFil",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_w4J8"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the reply"
                    },
                    "comment": {
                        "value": "Thanks so much to the authors for the careful explanation. All my concerns have been addressed, and I am willing to raise my score to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637222783,
                "cdate": 1700637222783,
                "tmdate": 1700637222783,
                "mdate": 1700637222783,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LSPRnPu81q",
            "forum": "PhMrGCMIRL",
            "replyto": "PhMrGCMIRL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4271/Reviewer_HyG2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4271/Reviewer_HyG2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a practical approach to fuse outputs of a set of models that are experts for complementary tasks. Two approaches are proposed:\n* FoE-classification, where a fuser is trained on top of the concatenation of outputs.\n* FoE-generation, where the fuser learns the optimal choice of expert via cross-entropy.\n\nAdditionally, FrugalFoE is proposed, as a strategy to incrementally increase the queries until a cost (loss) criteria is matched. This approach reduces the number of experts being ran.\n\nThe experimental section is solid and shows the validity of the approach in various settings, ranging from classification, sentiment analysis, summarization, QA and text generation. Models like ResNet-18 and a wide plethora of LLMs (~7B params) are used, making this evaluation relevant in the state-of-the-art."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The approach is relevant given the availability of pre-trained models nowadays. Methods that smartly fuse models can have impact, since one does not need to re-train, and can incorporate new knowledge to previously trained models.\n\n* The experimental section is very complete, and explores several domains of interest. I specially call out the experiments with LLMs.\n\n* The mathematical derivations are sound.\n\n* The paper is written with clear language, and very few typos."
                },
                "weaknesses": {
                    "value": "* I found the explanation of FrugalFoE harder to follow than the rest. See questions below."
                },
                "questions": {
                    "value": "* To train using the loss in Eq 3.3, one needs to know a priori the labels, ie. which is the correct model for that input. How reasonable is that assumption? Do we also know \"exactly\" which model should be selected?\n\n* I have some doubts about Equations 4.3 and 4.4 that I would like the authors to clarify. As far as I understand, to obtain the optimal (argmin), we must execute all the experts individually (in Eq. 4.3) and all the subsets in $\\mathcal{F}\\backslash\\tilde{\\mathcal{S}}$ for Eq. 4.4. This sounds quite intensive, and definitely more intensive than just running $\\mathcal{S}$ experts once. I know there is something I am missing here, I kindly ask the authors to bring some clarity in this sense. \n\n* Why is the cost term in Equation 4.1 summed over $f_k\\in \\mathcal{S}$? I would have expected this sum to be over $f_k\\in \\tilde{\\mathcal{S}}$, otherwise the term becomes constant wrt. the queried experts, right?\n\n* How can we use $c_k$ in practice? Can we use it to model aspects like energy consumption (for running an expert), flops, etc.? \n\n* Can the authors comment on Figure 2? Why is FrugalML performing so poorly? Even much worse than randomly picking the experts? \n  * Additionally, it would be interesting to add std bars for the Random Experts (selecting different random subsets of them, specially at lower values of the x axis).\n\n* In Section 5.2, the authors claim `Though sentiment analysis is essentially a classification task, we train the fuser using the generative model strategy 3.3`. I believe this is a typo and should be \"using the classification model strategy\".\n\n* I enjoyed the discussion in Section 3.3.\n\n* Conversely, I found Section 4.3 (graph) somehow disconnected and not adding to the work. Unless graphs are used in practice in the code (did not check).\n\n* Minor notation consistency comment. The set $\\mathcal{S}$ is not defined when it first appears. Furthermore, one can find both  $k\\in\\mathcal{S}$ and $f_k\\in\\mathcal{S}$ in the manuscript, which complicates readability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4271/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4271/Reviewer_HyG2",
                        "ICLR.cc/2024/Conference/Submission4271/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698413997642,
            "cdate": 1698413997642,
            "tmdate": 1700575052812,
            "mdate": 1700575052812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0OFMQNd6eY",
                "forum": "PhMrGCMIRL",
                "replyto": "LSPRnPu81q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to reviewer HyG2"
                    },
                    "comment": {
                        "value": "**Concern**: *How reasonable is that assumption that one knows the correct model for each input? Do we also know \"exactly\" which model should be selected?*  \n\n**Response**: Please kindly refer to our general response.  \n\\\n\\\n**Concern**: *Clarifying Equations 4.1, 4.3, and 4.4.*  \n\n**Response**: Thank you for bringing up these questions regarding our equations. We have made changes in the main text to make things clearer. Regarding Eq. 4.1, we sum over $k:f_k \\in \\mathcal{S}$ because, in our notation, $\\mathcal{S}$ includes all experts in $\\mathcal{\\tilde{S}}$ (queried experts) plus some extra expert we want to query in the next step; therefore, it should not be constant when choosing a new expert. Regarding Eq. 4.3 and 4.4, we assume access to the experts\u2019 outputs for all data points in a validation set. For example, if we have a validation set with a hundred data points and five experts, we have a total of $100\\times 5 = 500$ predictions/generations in the validation set. In the first equation, we just choose the expert which had the best average performance in the validation set, while in the second we choose a new expert which is expected to reduce the loss by the biggest amount. The intensive part here is training many fuser models to combine experts in different ways. Still, that cost can be alleviated using k-nearest neighbors as explained in the paragraph \u201cObtaining fusers for subsets of experts\u201d (Section 4.2).  \n\\\n\\\n**Concern**: *Using $c_k$ in practice.*  \n\n**Response**: The $c_k$\u2019s can include different types of costs. It could include energy consumption if experts are run locally or API costs when a third party provides experts. We made this point more explicit in the text.  \n\\\n\\\n**Concern**: *More explanation on why FrugalML performed so poorly, and adding error bars for the random expert baseline.*  \n\n**Response**: FrugalML considers composing up to two experts. Thus, in our setting, its performance saturates after two experts (i.e., allowing FrugalML to query more experts is not going to improve its performance). However, when querying either one or two experts, FrugalML still outperforms the approach of selecting random experts. We updated Figure 2 to include error bars for the random expert baseline.  \n\\\n\\\n**Concern**: *Though sentiment analysis is essentially a classification task, we train the fuser using the generative model strategy 3.3. I believe this is a typo and should be \"using the classification model strategy\".*  \n\n**Response**: No, it\u2019s not a typo. The key difference between our generative model strategy and the classification strategy lies in their approaches: while the generative model strategy is designed to predict \"which expert to use,\" the classification strategy directly employs the fuser for classification tasks. In our experiment, even though sentiment analysis inherently involves prediction, we train the fuser to determine which expert should be used for the sentiment analysis task.  \n\\\n\\\n**Concern**: *Notation inconsistency.*  \n\n**Response**: Thank you for pointing this out. We have corrected this issue in the main text."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980681416,
                "cdate": 1699980681416,
                "tmdate": 1699980681416,
                "mdate": 1699980681416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9T9uwSp9mw",
                "forum": "PhMrGCMIRL",
                "replyto": "LSPRnPu81q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_HyG2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_HyG2"
                ],
                "content": {
                    "title": {
                        "value": "Answer to rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the detailed and concise answers to my questions. I greatly appreciate the effort you put in the rebuttal, specially running extra experiments that help clarifying the overall proposal.\n\nThe rebuttal provided answers my most important questions, and the experiments make the paper stronger in my opinion. \nGiven the above, I am willing to upgrade my score.\n\nCongratulations on your great work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575030430,
                "cdate": 1700575030430,
                "tmdate": 1700575071770,
                "mdate": 1700575071770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9msdE37qot",
            "forum": "PhMrGCMIRL",
            "replyto": "PhMrGCMIRL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4271/Reviewer_75JM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4271/Reviewer_75JM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an innovative approach for combining various models to optimize performance across different tasks. Recognizing that no single model excels in all tasks and considering the complementary strengths of various pre-trained models, the authors propose a lightweight MLP fuser network trained to either fuse outputs (in the discriminative case) or select the most suitable model (in the generative case) from a pool of $N$ models. To manage the computational expense of querying all $N$ models, they introduce a cost-effective strategy that selects and queries a much smaller subset, $M \\ll N$. Their approach, named FrugalFoE, demonstrates impressive results in fusing outputs or selecting expert models, showcasing superior performance in tasks like image classification, summarization, MMLU, and text evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper tackles a novel and significant issue: optimally leveraging different models for diverse tasks. While foundational models generally perform well across various tasks, they have differing strengths. Thus, an approach to effectively combine these models represents a significant advancement.\n- The methodology, FrugalFoE, is both technically sound and innovative. It offers a clear problem formulation and further minimizes the need to query all the expert models without sacrificing accuracy.\n- The experiments are extensive and cover a range of applications, including image classification, summarization, MMLU, and text evaluation. This demonstrates the method\u2019s versatility and effectiveness.\n- The paper effectively situates its work within the broader context of ensemble learning, mixture of experts, and federated learning, thoughtfully introducing these methodologies and their limitations."
                },
                "weaknesses": {
                    "value": "- A primary limitation is the assumption that data for training the fuser are readily available. The proposed approach requires a labeled dataset to train the fuser network. In the discriminative case, we feed the input example to different individual models, take model outputs as the inputs to the fuser network, and train the fuser to predict the label of the input example. Similarly, in the generative case,  we feed the fuser network with individual model outputs and train the fuser to predict the best model index, which is obtained by feeding the labeled data to individual models and selecting the model that achieves the best performance. This prerequisite may not be realistic in practical scenarios, where there is no or only a few labeled data. If a large labeled dataset is available, it might be more efficient to fine-tune a foundational model or employ few-shot learning. A pivotal aspect of this research should be the generalization capabilities of the trained fuser across different tasks (e.g,. train the fuser on some tasks and test the fuser on unseen tasks), which would significantly enhance the paper's contribution.\n- The paper lacks an in-depth analysis of the fuser network. Although it is described as a lightweight MLP network, there's no exploration of how different architectures (e.g., simpler networks like linear models or more complex ones like transformers) might impact performance. Additionally, details on the training configurations, such as learning rate, epochs, dataset, and stopping criteria, are missing. The influence of the $K$ parameter in the $K$-NN component of the fuser network is also unclear. What is the rationale for choosing $K=9$?\n- The experiment sections lack in-depth descriptions (see questions). The paper would benefit from reallocating less critical sections (like the connection between FrugalFoE and the A* algorithm) to the Appendix and expanding on the experimental details."
                },
                "questions": {
                    "value": "- Page 8, Table 2: Could you clarify the distinction between \"TFN Expert Only\" and \"TFN Expert,\" and between \"Poem Expert\" and \"Poem Expert Only\"?\n- Page 8, Table 2: What are the results for the \"confidence-based fusion\" and \"ensemble\" baselines? The same question applies to Tables 3 and 4. The confidence-based fusion seems less intuitive in the generative case, how about simply selecting the maximum confidence at each decoding step?\n- Page 9, Table 4: Could you explain more details of FoE (Expert 1), FoE (Expert 2), and FoE (Expert 3)? Why does adding more experts appear to degrade performance?\n- Page 4: The statement \"As long as there is a label shift among the domains, we expect $E[f(X_k)] = E[Y_k]$\" needs clarification. Why is this expected?\n- Page 6: The phrase \"Then $\\lambda$ can be interpreted as the final error rate reduction we want to achieve\" \u2013 could you expound on the reasoning behind this?\n- Page 1: The statement \"our emphasis is on generalization to test data distributions where none of the experts perform well individually\". Shouldn't it be \"a few\" instead of \"none\" based on the problem formulation?\n- Page 6: The condition \"If ... <0 we terminate the search\" should be\">0\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4271/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4271/Reviewer_75JM",
                        "ICLR.cc/2024/Conference/Submission4271/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646858924,
            "cdate": 1698646858924,
            "tmdate": 1700641307111,
            "mdate": 1700641307111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JC6isxKhxQ",
                "forum": "PhMrGCMIRL",
                "replyto": "9msdE37qot",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to reviewer 75JM (Part-1)"
                    },
                    "comment": {
                        "value": "**Concern**: *assuming data available for training the fuser. Not realistic as there is only a few labeled data. Fine-tune models if labeled data is available? Generalize the trained fuser to other tasks?*\n\n**Response**: Please refer to our general response to this point. \n\nRegarding the suggestion that \u201cIf a large labeled dataset is available, it might be more efficient to fine-tune a foundational model or employ few-shot learning,\u201d we contend that in FoE, only a very small fuser model needs to be trained, typically a small MLP or even a kNN, where no training is required. In terms of the number of parameters, the fuser models are usually orders of magnitude smaller than the expert models. This approach is computationally much cheaper than fine-tuning a foundational model, as the reviewer suggested, and also cheaper than fine-tuning any individual expert model.  \n\\\n\\\n**Concern**: *Various choices of the fuser network? More details about the hyper-parameters. Why choose a certain $\\kappa$ in the kNN setting?*  \n\n**Response**: Thanks for pointing it out! A promising aspect of the FoE is its flexibility in the architecture of the fuser model; it doesn't require any specific design. In our experiments, we explored using both simple MLPs and kNNs as fusers and found that kNNs offer certain advantages, particularly in frugal settings. Additionally, we experimented with simple linear models during the development of the FoE method, although these results are not yet included in the paper. The table below compares the performance on the CIFAR-100 dataset using both MLP and linear models as fusers, each trained on the combined training set of all experts. We observed that using models as complex as MLPs are sufficiently effective, as evidenced by our results in summarization and sentiment analysis tasks where our fuser performs comparably to the oracle models. Therefore, we believe there is no need to explore more complex models like transformers, especially since the input for the fusers is not sequence data, and doing so would compromise the FoE's advantage of having an easily trainable fuser. We will include a discussion on the specific choices of fuser architecture in the revised draft.  \n  \n| Fuser type      | Final Accuracy (%) |\n| ----------- | ----------- |\n| MLP as the fuser      | **84.23**       |\n| Linear model (logistic regression) as the fuser   | 83.12        |\n  \nFor questions regarding experimental details, we will revise the experiment section to include more comprehensive descriptions of our setups. The details of our hyper-parameters are already specified in Appendix C. The fuser training process is so simple that there aren\u2019t many hyper-parameters to report.  \n\nAs for the different choices of $\\kappa$, our approach is to select a relatively small and odd number, as is common practice in such experiments. We will update the manuscript with a plot showing results for various values of $\\kappa$ shortly, as these experiments are currently in progress. For additional experimental details, we have provided the code used in our experiments, which contains all the necessary information to replicate our results.  \n\\\n\\\n**Concern**: *Expanding experimental details?*  \n\n**Response**: We will expand our experiment section in the final version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979421750,
                "cdate": 1699979421750,
                "tmdate": 1700537761614,
                "mdate": 1700537761614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "76xmfjy8K8",
                "forum": "PhMrGCMIRL",
                "replyto": "9msdE37qot",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A follow-up response to Reviewer 75JM"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer again for the detailed reviews and constructive comments on our paper. As promised, we have updated the manuscript with a plot showing results for various values of $\\kappa$. Please refer to the new Figure 2 in the revised manuscript, where one can clearly observe that the performance of FoE does not appear to be sensitive to various values of $\\kappa$.  \n\nPlease let us know if our response has addressed your concerns. Thank you again!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537422096,
                "cdate": 1700537422096,
                "tmdate": 1700537604798,
                "mdate": 1700537604798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HJCyqSq2Wr",
                "forum": "PhMrGCMIRL",
                "replyto": "76xmfjy8K8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_75JM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Reviewer_75JM"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewers for providing a detailed response. As some of my concerns are (partially) addressed, specifically, the generalization of the fuser network to unseen tasks (W1) and the design choice of the fuser network (W2), I'm increasing my rating to 6. \n\nHowever, I still have reservations about their proposed advantages of the fuser network for W1. They mention that the fuser network is better because 1) it can be learned with a few examples, but few-shot in-context learning can also achieve this; 2) it can be trained more efficiently since it has fewer parameters, but many parameter-efficient training methods exist.\n\nFor W2, it is interesting to learn that MLP outperforms the linear fuser network. However, it is still unclear how other complex architectures, or just MLP with more layers, could improve the task. It is also a bit weird that the authors ablate K=7,9,11 in KNN instead of more general Ks.\n\nFurther question: could you explain more about what the \"FoE w/ TFN Expert Features Only\" is? How can you train the fuser with only one expert and apply the fuser to multiple experts?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641259070,
                "cdate": 1700641259070,
                "tmdate": 1700641259070,
                "mdate": 1700641259070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dT9uWrvdoM",
                "forum": "PhMrGCMIRL",
                "replyto": "9msdE37qot",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further responses to Reviewer 75JM"
                    },
                    "comment": {
                        "value": "Thank you so much for joining the discussion and for increasing your rating! We discuss your remaining questions below.\n\n> They mention that the fuser network is better because 1) it can be learned with a few examples, but few-shot in-context learning can also achieve this; 2) it can be trained more efficiently since it has fewer parameters, but many parameter-efficient training methods exist.  \n\nAn important aspect of FoE is the ability to reuse pre-trained expert models. It is not clear how to use PEFT to fuse multiple pre-trained LLM experts. One naive approach could be to fine-tune a single expert (or some base model) on all experts\u2019 training data using PEFT. This approach would require fine-tuning an LLM, even with PEFT this could be a fairly expensive task in comparison to training an MLP model as in FoE. For instance, in our experiments, we simply downloaded expert models already available on HuggingFace and did not need to fine-tune any LLM. Meanwhile, fine-tuning an LLM on a combination of experts' domains does not allow one to reuse existing experts and is likely to also require a larger base model to perform well on every domain simultaneously with a single LLM. We believe combining pre-trained experts is more appealing as it can take advantage of the many fine-tuned models open-source community releases.  \n  \nRegarding in-context learning, the performance is typically worse than that of an expert model. Prior work has demonstrated that training a smaller expert model is typically more efficient than prompting even the largest general model (we provide some references in the first paragraph of the introduction). We also note that the \u201cweak\u201d experts considered in our MMLU experiment are 5-shot prompted general models (which is the standard practice for evaluating on MMLU) and our results demonstrate that it is also beneficial to fuse such ICL-prompted general LLMs.  \n\n> However, it is still unclear how other complex architectures, or just MLP with more layers, could improve the task. It is also a bit weird that the authors ablate K=7,9,11 in KNN instead of more general Ks.    \n    \nWe are willing to explore more complex fuser model architectures and will demonstrate the results in the final paper draft. However, we have already demonstrated that FoE is not overly sensitive to the fuser\u2019s model architecture, and even simple MLPs and linear models work sufficiently well. We will also explore more $\\kappa$ values in our final paper draft, but we believe it is evident that FoE is reasonably robust to the choice of $\\kappa$. \n\n> Could you explain more about what the \"FoE w/ TFN Expert Features Only\" is? How can you train the fuser with only one expert and apply the fuser to multiple experts?\n\nFoE w/ TFN Expert features only implies that only the TFN Expert's features are used for training the fuser. For testing, it is also necessary to use only the TFN Expert\u2019s features. Let\u2019s consider a concrete example: suppose the TFN Expert\u2019s embedding dimension is 768, and there are six experts. In this case, the fuser (e.g., an MLP) only takes a 768-dimensional input to perform a six-class classification, i.e., to determine which expert to query for a given test data sample. In FoE w/ TFN Expert features only, we train the MLP using only the TFN Expert's 768-dimensional features. During testing, we also use only the TFN Expert\u2019s features to predict which expert to query. In contrast, in the regular FoE, the fuser takes a 6x768=4608-dimensional input for the same six-class classification task. During test time, all experts need to be queried for each test data sample to construct the 4608-dimensional input. This is precisely why we claim that FoE is naturally frugal for the sentiment analysis and summarization tasks, as using only one expert during the entire fusion process (both in training the fuser and using the trained fuser during testing) is sufficient to achieve good results."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694998685,
                "cdate": 1700694998685,
                "tmdate": 1700695464300,
                "mdate": 1700695464300,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rapCh4IHHm",
            "forum": "PhMrGCMIRL",
            "replyto": "PhMrGCMIRL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4271/Reviewer_2UmK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4271/Reviewer_2UmK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to train a set of experts by training the expert one at a time to be fused. Each expert is designed to complement each other during the training procedure by solving the residual gains upon introducing the new expert. Authors evaluated the method on various text domains -- classification, summarization, QA, and generation quality evaluations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed method provides an interesting direction that can train multiple models sequentially to train on the residuals of the previous mixture of experts.\n* Compared to pure residual approaches, because the transformation function is taken on top of each model\u2019s outputs, we can expect this may be more general than the pure residual learning setting."
                },
                "weaknesses": {
                    "value": "* The proposed algorithm requires multiple experts to be used together, unlike Sparse MoE, which means that the inference cost is multiple times that of each expert. Therefore, the correct baseline for the proposed algorithm is to compare it to an equal number of parameters with the sum of all individual experts. The authors should make this comparison in their paper.\n* Similar to the first point, authors provide experimental results on a variety of datasets, however, they did not include many common baselines for each dataset. For example, Figure 2 has a very crude baseline (random experts) or FrugalML/FrugalFoE. I suggest authors to consider common baselines. Few suggestions are Sparse MoE or just a single model with the similar # of parameters."
                },
                "questions": {
                    "value": "Please look at the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698893605942,
            "cdate": 1698893605942,
            "tmdate": 1699636394343,
            "mdate": 1699636394343,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JUgrmx4G2p",
                "forum": "PhMrGCMIRL",
                "replyto": "rapCh4IHHm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to reviewer 2UmK"
                    },
                    "comment": {
                        "value": "**Concern**: *The correct baseline for the proposed algorithm is to compare it to an equal number of parameters with the sum of all individual experts or Sparse MoE*.\n\n**Response**: \nWe would like to respectfully disagree with the reviewer\u2019s point. We view FoE as a lightweight paradigm that allows us to easily fuse pre-trained expert models to achieve better performance on a mixture of downstream tasks and domains.  \n\nIt is not very clear how to directly adopt Sparse MoE in a scenario where $K$ pre-trained experts are provided with the data used to build them. One possible approach is to freeze all pre-trained experts and only train the gating network using the available datasets. In such a case, the gating network would need to directly process data, including images and sequences of tokens, requiring a complex architecture like a BERT, a ViT, or a convolutional neural network. This requirement significantly increases the computational difficulty and model complexity needed to build such a Sparse MoE.  \n\nRegarding the suggestion to use a model with the combined number of parameters of all experts, we believe this approach is not quite practical. FoE allows us to leverage the resources created by the entire open-source community, such as trained and fine-tuned expert models or LLMs, to achieve better results. For example, one can easily download ten 13B expert LLMs from Hugging Face with just a few lines of code. However, obtaining and running inference on a 130B expert model, as suggested by the reviewer, is far more challenging. In most cases, such a 130B expert LLM does not even exist, and constructing one poses even greater challenges."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978962191,
                "cdate": 1699978962191,
                "tmdate": 1699978983572,
                "mdate": 1699978983572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]