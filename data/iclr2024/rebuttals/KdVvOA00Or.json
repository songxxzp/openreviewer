[
    {
        "title": "ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift"
    },
    {
        "review": {
            "id": "3jPImbfmYK",
            "forum": "KdVvOA00Or",
            "replyto": "KdVvOA00Or",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies unsupervised domain adaptation in the context of regression ($\\mathbb{R}$-valued labels), under the *label shift* assumption (i.e., that the label distribution $P_Y$ changes while the conditional distribution $P_{X|Y}$ of the features given the labels is invariant. It is first argued that this problem can be solved by training a model under a modified training loss, which is reweighted by the ratio of the label distributions in the test and training domains (the *importance weight function* $\\omega$). The remainder of the paper thus focuses on estimating $\\omega$. It is shown that $\\omega$ satisfies a particular intergral equation, whose other components can be estimated directly from observed data using kernel density estimation. Since this integral equation is typically undercomplete, Tikhonov regularization is added to identify a unique solution. Assuming the relevant data densities are sufficiently regular and bounded, the kernel and bandwidth are carefully selected, etc., the paper provides bounds on the rate at which the estimate of $\\omega$ converges to the true $\\omega$. The paper then presents experiments on both synthetic and real-world datasets, demonstrating that the proposed approach outperforms a prior kernel-mean-matching approach, both in terms of estimating $\\omega$ and out-of-distribution adaptation performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation and justification for the proposed approach is quite convincing; almost every step seems natural, and so it seems to me like the \"right\" solution to this problem, under the given assumptions. The high-level writing and flow of the paper are also quite clear. The method is supported both theoretically (with some caveats; see below) and empirically."
                },
                "weaknesses": {
                    "value": "**Major**\n\n1. The paper should discuss the theoretical computational complexity and practical scalability of ReTaSA, in terms of the source and target sample sizes $n$ and $m$, data dimension $p$, etc. Relatedly, under \"Related Work\", the paper claims \"empirical evidence from... our experimental studies confirms that KMM is computationally inefficient in categorical and continuous cases,\" but I couldn't find evidence of this in the paper.\n\n2. I found several parts of the paper a bit vague or missing some details that would help the reader:\n    1. Page 2, last sentence, \"the target shift assumption implies that... (1)\": I think it would be helpful to include a few more details on the steps by which the target shift assumption implies Eq. (1). I was eventually able to figure this out (using Bayes' rule), but it interrupted my reading of the paper and took a few minutes. This could easily be avoided by adding another intermediate equality in Eq. (1), without increasing the length of the paper.\n    2. Page 3, just after Eq. (3), \"$T$ and $T^\u2217$ are adjoint operators because $\\langle T\\phi, \\psi \\rangle = \\langle \\phi, T^* \\psi \\rangle$\": It's not immediately obvious why this is the case. Since this observation is important for the remainder of the paper, please include a more detailed explanation or proof to the main text, or indicate where this could be found (e.g., in an appendix).\n    3. Page 3, just after Eq. (4), \"where... $\\rho(y)$ is a unknown function to be solved\": I found this quite confusing because it sounded like $\\rho = \\omega$. Only later is it explained that $\\rho = \\omega - 1$. So perhaps this latter fact can be explained a few sentences earlier.\n\n3. First Paragraph of Section 3: $\\eta = p_t/p_s$ is estimated as the ratio of two density estimates. There is a significant body of work on density ratio estimation showing that estimating the ratio by the ratio of two density estimates is often suboptimal, both in theory and in practice (see, e.g., [K17, SSK10]). Given this, perhaps the paper should consider using direct density estimation methods for this step, both to improve practical performance and to relax the assumptions (specifically, Assumption 3).\n\n4. There are some gaps between the theoretical results Section 4 and the real-world OOD generalization problem the paper seeks to solve:\n    1. Theorem 1 bounds the $L^2$ error of the estimated $\\rho$. However, $\\rho$ is only a means to re-weighting the risk function to adapt to the test domain (as explained on Page 3), and it's not clear to me whether estimating $\\rho$ well in $L^2$ distance is necessary or sufficient to adapt to the test domain. I think the paper should provide some more concrete connection between estimation of $\\rho$ in $L^2$ distance and test-domain performance of the new risk minimizer.\n    2. It's unclear (to me) how some of the main assumptions in Section 4 relate to the real-world problem being solved; see Major Questions 3. and 4. below.\n\n**Minor**\n\n1. Page 1, Paragraph 2: It's unclear to me why the Sequential Organ Failure Assessment (SOFA) example described here satisfies the label shift assumption (in particular, why $P_{X|Y}$ is invariant between domains). I think a more convincing example here would strengthen the motivation of the paper. Perhaps it is also worth pointing up that label-shift assumptions appear naturally under anti-causal structural assumptions (see, e.g., Section 5 of [S22]).\n2. The paper would benefit from some discussion of ReTaSA's limitations or further open questions. Some examples:\n    1. The paper focuses on the non-parametric setting. While this makes weak assumptions on the relationship between $x$ and $y$, Theorem 1 suggests that its performance scales poorly with the dimension $p$ of the feature $x$. Perhaps it is worth briefly commenting on whether a parametric variant of ReTaSA (e.g., assuming a linear relationship between $x$ and $y$) could be useful, e.g., for high-dimensional data?\n    2. Do the authors believe the rate in Theorem 1 is minimax optimal under Assumptions 1-5?\n    3. How robust is ReTaSA to small violations of the label-shift assumption (e.g., small changes in $P_{X|Y}$)?\n3. Beginning of Page 4: Tikhonov regularization is added to address non-identifiability (i.e., $T^* T$ might not be invertible). Given this, it might be worth adding a sentence to point out that the regularized criterion has a unique solution (i.e., $\\alpha I + T^* T$ is always invertible). This isn't completely obvious, especially in the infinite-dimensional setting.\n4. Remark 1: If I am understanding correctly, perhaps it is worth noting that this estimate/approximation is simply the standard Nadaraya-Watson regression estimate of $\\mathbb{E}[\\rho(y)|x]$.\n5. Remark 8, \"Therefore, the assumption fits into the regime where the dimension of the feature is smaller than the smoothness level of densities and the order ofthe generalized kernel function.\": I didn't understand this sentence. It sounds like it is saying that dim$(x) = p \\leq \\min\\\\{k, \\ell\\\\} = \\gamma$, but I don't see how this follows from the previous sentence (which is about $\\alpha$).\n6. Page 7, under \"Evaluation Metrics\", \"We conducted all experiments with 50 replications on a Mac-Book Pro equipped with a 2.9 GHz Dual-Core Intel Core I5 processor and 8GB of memory.\": This seems like the wrong place to include this information. Perhaps it should be in the first paragraph of Section 5?\n7. Page 7, under \"Experimental Results\", Typo: \"performs significantly better KMM-Adaptation\" should be \"performs significantly better *than* KMM-Adaptation\"\n8. Figure 2: The lines plotted here are essentially all flat, so the plot does not illustrate much. Perhaps it would be useful to show a larger range of (smaller) sample sizes?\n9. Figure 3: I think the sub-captions are incorrect (they both say \"vs Sample Size\" but the $x$-axis here is $\\mu_t$).\n10. All Figures: Please increase the font size of the text in the plots (axis labels, legends, etc.).\n\n**References**\n\n[K17] Kpotufe, S. (2017, April). Lipschitz density-ratios, structured data, and data-driven tuning. In Artificial Intelligence and Statistics (pp. 1320-1328). PMLR.\n\n[S22] Sch\u00f6lkopf, B. (2022). Causality for machine learning. In Probabilistic and Causal Inference: The Works of Judea Pearl (pp. 765-804).\n\n[SSK10] Sugiyama, M., Suzuki, T., & Kanamori, T. (2010). Density ratio estimation: A comprehensive review (statistical experiment and its related topics). \u6570\u7406\u89e3\u6790\u7814\u7a76\u6240\u8b1b\u7a76\u9332, 1703, 10-31."
                },
                "questions": {
                    "value": "**Major**\n\n1. I found Definition 2 quite confusing, for a few reasons:\n    1. I don't see where Definition 2 is used anywhere in the paper.\n    2. In contrast to Section 3, where the (presummably translation invariant?) kernels are written as a univariate function, the kernel here is written as a bivariate function. Is there a reason for this?\n    3. I don't understand the condition $k_h(x, y) = 0$ if $x \\notin [y - 1, y] \\cap \\mathcal{C}$. For example, usually, bivaraite kernels are symmetric in their arguments, but this appears not to be the case here.\n    4. The use of $x$ and $y$ is a bit confusing here, as it suggests the kernel is applied to the covariate $x$ and the label $y$ discussed earlier in the paper (but I don't think this is the intent, since, e.g., the condition $x \\notin [y - 1, y]$ really would not make any sense in this case). Perhaps different variables (e.g., $z_1$ and $z_2$) should be used here?\n2. Page 3, Eqs. (2)-(3): I don't understand why $T$ and $T^*$ map $L^2$ to $L^2$. Is this an additional implicit assumption? Or does it follow from the forms (conditional expectations) of $T$ and $T^*$? Relatedly, just after Eq. (4), \"$\\mathbb{E}_s\\\\{\\eta(x)\\\\} = 0$ so that $\\eta(x) \\in L^2(x)$\": I didn't understand this implication; why is $\\mathbb{E}_s\\\\{\\eta^2(x)\\\\} < \\infty$?\n3. Assumption 2: Although I'm familiar with math behind this assumption, I found it hard to understand its ramifications in this context. I understand that the $\\beta$-regularity space is the set of $\\rho$ such that $T^* T \\rho$ is approximately invertible, but is there a more concrete intuition, or maybe some examples, of what $\\beta$-regularity looks like when $T$ is the conditional expectation operator? For example, if if $\\\\{\\phi_i\\\\}_{i = 1}^\\infty$ is the Fourier basis, then $\\beta$ is related to the smoothness/differentiability of $\\rho$.\n4. Assumption 5: Similar to the previous point, can you provide some intuition or examples for when Assumption 5 is satisfied?\n\n\n**Minor**\n\n1. Assumption 4: \"Letting $\\gamma = \\min\\\\{k, \\ell\\\\}$...\": $\\gamma$ is never used again in this assumption. Is this an error? Perhaps $\\gamma$ should be defined in Assumption 5 instead?\n2. Remark 6: Does \"consistent\" here mean \"in operator norm\"? If so, please make this more explicit.\n3. I didn't understand the definition of \"Delta Accuracy\" in the experiments. Could you please provide a mathematical formula? Would $100\\%$ correspond to perfect prediction?\n4. Page 8, Last Sentence, \"In each trial, we... randomly select 80% of the players with the other positions as the training source data.\": If I understand correctly, the idea here is to use the bootstrap to obtain confidence intervals on performance. If so, this resampling should be done *with replacement*. Or is there a different reasoning here?\n5. Page 9, First Paragraph, \"For the temperature value shift between the training source and testing dataset, we use the humidity for the importance weight estimation.\": I didn't understand this sentence. What does it mean to \"use the humidity for the importance weight estimation\" (as opposed to using all of the available features)?\n6. Page 9, Second Paragraph, \"our method improves about 4% for the SOCR dataset\": I don't see this 4% in Table 1. What exactly is this refering to?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission974/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s",
                        "ICLR.cc/2024/Conference/Submission974/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698323458026,
            "cdate": 1698323458026,
            "tmdate": 1700557167272,
            "mdate": 1700557167272,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rdX7Qmqjzz",
                "forum": "KdVvOA00Or",
                "replyto": "3jPImbfmYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer SE3s"
                    },
                    "comment": {
                        "value": "## **Weaknesses:**\n\n### **Major**\n\n> 1. *The paper should discuss the theoretical computational complexity and practical scalability of ReTaSA, in terms of the source and target sample sizes $n$ and $m$, data dimension $p$, etc. Relatedly, under \"Related Work\", the paper claims \"empirical evidence from ... our experimental studies confirms that KMM is computationally inefficient in categorical and continuous cases,\" but I couldn't find evidence of this in the paper.*\n    \n **Our Response**: Thanks for pointing this out. Our method computationally faster than the KMM method is because our method only needs to solve a linear equation, while KMM needs to optimize the constraint quadratic programming problem. More specifically, the major computational cost in our method is inverting a $n\\times n$ matrix, whose computational complexity is generally $\\mathcal{O}(n^3)$. The KMM method proposed by Zhang et al. (2013) needs to optimize a quadratically constrained\u00a0***quadratic***\u00a0program with a $n\\times n$ matrix.  We believe the computational efficiency can be attributed to the fact that our proposed method only requires a one-step linear system solving while the KMM method (as well as the added L2IWE method) needs to loop through iteration in the optimization process. **From the empirical aspect, we detail the running time comparisons in Figure S.4 in Section C.2.2 of the supplementary materials**.\n\n> 2. *I found several parts of the paper a bit vague or missing some details that would help the reader:*\n> 2.1. *Page 2, last sentence, \"the target shift assumption implies that... (1)\": I think it would be helpful to include a few more details on the steps by which the target shift assumption implies Eq. (1). I was eventually able to figure this out (using Bayes\\' rule), but it interrupted my reading of the paper and took a few minutes. This could easily be avoided by adding another intermediate equality in Eq. (1) without increasing the length of the paper.*\n        \n**Our Response**: Thanks for the suggestion. We have provided more details in the derivation in the revision in Equation (1).\n\n> 2.2. *Page 3, just after Eq. (3), \"$T$ and $T^{\\ast}$ are adjoint operators because $\\langle T\\phi,\\psi\\rangle = \\langle\\phi, T^{\\ast}\\psi\\rangle$\\\": It\\'s not immediately obvious why this is the case. Since this observation is important for the remainder of the paper, please include a more detailed explanation or proof in the main text or indicate where this could be found (e.g., in an appendix).*\n        \n**Our Response**: We have refined the definition of inner products and added a few more lines of equations between equation (3) and (4) to validate this fact.\n\n> 3. *Page 3, just after Eq. (4), \"where... $\\rho(y)$ is an unknown function to be solved\\\": I found this quite confusing because it sounded like $\\rho = \\omega$. Only later is it explained that $\\rho = \\omega - 1$. So perhaps this latter fact can be explained a few sentences earlier.*\n\n**Our Response**: Thanks for the suggestion; we have clarified the usage of notation $\\rho$ as you suggested and edited the identifiability subsection to reflect this change.\n\n> 3. *First Paragraph of Section 3: $\\eta = p_{t}/p_{s}$ is estimated as the ratio of two density estimates. There is a significant body of work on density ratio estimation showing that estimating the ratio by the ratio of two density estimates is often suboptimal, both in theory and in practice (see, e.g., \\[K17, SSK10\\]). Given this, perhaps the paper should consider using direct density estimation methods for this step, both to improve practical performance and to relax the assumptions (specifically, Assumption 3).*\n    \n**Our Response**: Thanks for providing closely connected references. Utilizing direct density estimation to yield an improved rate of consistency will be an interesting future theoretical work to pursue. In the meantime, we believe that assumption 3 in our setting is natural as the equation (1) would not be valid if violated. We have added a remark to explain connections between the assumption 3 and the equation (1).\n    \nFurthermore, in the revised paper, we compared two ways of computing the density ratio: directly computing the ratio of two estimated densities and the Relative unconstrained Least-Squares Importance Fitting (RuLSIF) method (Yamada et al. (2011)), which is proposed by the author of [SSK10]. The numerical study shows that these two methods have similar performance. The details of the numerical comparison can be found in Section C.2.3 of the Supplementary Materials."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107519539,
                "cdate": 1700107519539,
                "tmdate": 1700107519539,
                "mdate": 1700107519539,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5uh1SeVTQX",
                "forum": "KdVvOA00Or",
                "replyto": "3jPImbfmYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 4. *There are some gaps between the theoretical results of Section 4 and the real-world OOD generalization problem the paper seeks to solve:*\n> 4.1. *Theorem 1 bounds the $L^{2}$ error of the estimated $\\rho$. However, $\\rho$ is only a means to re-weighting the risk function to adapt to the test domain (as explained on Page 3), and it\\'s not clear to me whether estimating $\\rho$ well in $L^{2}$ distance is necessary or sufficient to adapt to the test domain. I think the paper should provide some more concrete connection between estimation of $\\rho$ in $L^{2}$ distance and test-domain performance of the new risk minimizer.*\n        \n**Our Response**: Thanks for pointing this out. The key reason for the weight estimation is motivated by the loss relationship between training and testing datasets.\n$$\n\\mathbb{E}_t[\\ell\\{f(x),y\\}]=\\mathbb{E}_s[\\omega(y)\\ell\\{f(x),y\\}]\n$$\nWith a well-estimated $\\rho$, we can recover the importance weight function $\\omega$ accurately, which can facilitate us to learn a predictive model with respect to the test-domain distribution. As $\\rho$ is a function, we use the $L^2$-distance to measure the estimation accuracy in the functional space.\n\n> 4.2. *It\\'s unclear (to me) how some of the main assumptions in Section 4 relate to the real-world problem being solved; see Major Questions 3. and 4. below.*\n        \n**Our Response**: Thanks for the comment. Please see our responses below under Major Questions 3-4."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107543363,
                "cdate": 1700107543363,
                "tmdate": 1700107623286,
                "mdate": 1700107623286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hg1kvqpYPk",
                "forum": "KdVvOA00Or",
                "replyto": "3jPImbfmYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
                ],
                "content": {
                    "title": {
                        "value": "Clarifying the \"gap\" between the $L^2$ error and test-domain performance"
                    },
                    "comment": {
                        "value": "Thanks to the authors for responding to my comments. For my comment 4.2, I was hoping for was some more detailed discussion along the following lines.\n\nUltimately, rather than estimating $\\omega$, what we care about is something like the test-domain generalization error\n$$E_s[\\omega(y) \\ell(\\hat f(x), y))] - \\frac{1}{n} \\sum_{i = 1}^n \\hat\\omega(y_i) \\ell(\\hat f(x_i), y_i)$$\nof the weighted risk minimization\n$$\\hat f = \\arg\\min_f \\frac{1}{n} \\sum_{i = 1}^n \\hat\\omega(y_i) \\ell(f(x_i), y_i).$$\nOne approach to bound this generalization error is by bounding the difference between the true and empirical risks uniformly over the hypothesis class, i.e.,\n$$\\sup_f \\left| \\frac{1}{n} \\sum_{i = 1}^n \\hat\\omega(y_i) \\ell(f(x_i), y_i) - \\mathbb{E}_s[\\omega(y) \\ell(f(x), y))] \\right|.$$\n\nFor simplicity, let's assume the source sample size $n$ is huge, the hypothesis class is reasonably bounded, and $y$ is bounded almost surely, so that the true and empirical risk functionals are basically identical; i.e., $\\sup_f \\left| \\frac{1}{n} \\sum_{i = 1}^n \\hat\\omega(y_i) \\ell(f(x_i), y_i) - \\mathbb{E}_s[\\hat\\omega(y) \\ell(f(x), y)] \\right| \\approx 0$ (this can be bounded more formally by standard techniques). Then, the generalization error mostly comes from the error in estimating $\\omega$, namely\n$$\\sup_f \\left| \\mathbb{E}_s[\\omega(y) \\ell(f(x), y)] - \\mathbb{E}_s[\\hat \\omega(y) \\ell(f(x), y)] \\right|\n  = \\sup_f \\mathbb{E}_s[(\\omega(y) - \\hat\\omega(y)) \\ell(f(x), y)]\n  \\leq \\sqrt{\\mathbb{E}_s[(\\omega(y) - \\hat\\omega(y))^2] \\sup_f \\mathbb{E}_s[\\ell^2(f(x), y)]}.$$\nFocusing on the term involving $\\hat\\omega$, it looks like we care about the *weighted* $L^2$ (weighted by $p_s(y)$) error, rather than the unweighted $L^2$ error that is bounded in Theorem 1. However, the weighted $L^2$ error can be bounded in terms of the unweighted $L^2$ error if we additionally assume $p_s(y)$ is bounded above.\n\nThis gives one bound on the test-domain generalization error in terms of the $L^2$ bound in Theorem 1, but there may be better ways (e.g., tighter bounds or making weaker assumptions). So I asking the authors to think a bit about this gap and to justify that bounding the $L^2$ error in Theorem 1 is useful and/or necessary."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127968609,
                "cdate": 1700127968609,
                "tmdate": 1700128032974,
                "mdate": 1700128032974,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RMKaTYAi0m",
                "forum": "KdVvOA00Or",
                "replyto": "s5kdtNzDBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
                ],
                "content": {
                    "title": {
                        "value": "Clarifying my question about $\\beta$-regularity"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their detailed responses.\n\nRegarding my Major Question 3. about Assumption 2.: Sorry, I realize my question was a bit too vague earlier. I was really asking about some intuition regarding what the spectrum *of a conditional expectation operator* looks like. Is there any nice way to think about this in terms of the underlying joint distribution $p(x, y)$ (if not in general, at least in a simple example)? It's ok if the answer is negative, but it would be great if there's some way to understand this $\\beta$-regularity assumption in the context of the original problem of regressing y over x under target shift. At present, I have no idea whether this assumption is reasonable in a realistic problem."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700131089953,
                "cdate": 1700131089953,
                "tmdate": 1700131089953,
                "mdate": 1700131089953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bocnt9AY3T",
                "forum": "KdVvOA00Or",
                "replyto": "3jPImbfmYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Our Response**: Thanks for the clarifications. The singular value decomposition (SVD) of general conditional expectation operators is non-trivial compared with its counterpart for matrices. To provide some intuitive explanations, we focus on the exponential family. More specifically, the Gaussian distribution in our response. Suppose we can factorize a joint distribution $p(x,y)$ into $p(x|y)p(y)$, where\n\n$$\np(x|y)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{(x-y)^2}{2\\sigma^2}),\\quad p(y)=\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp(-\\frac{y^2}{2\\sigma_0^2})\n$$\n\nIn fact, this joint distribution $p(x,y)$ corresponds to the model $x=y+\\epsilon$, where the random variable $y\\sim\\mathrm{Norm}(0,\\sigma_0^2)$ is independent of the random Gaussian noise term $\\epsilon\\sim\\mathrm{Norm}(0,\\sigma^2)$. Through SVD of the conditional expectation operator $T$ (associated with $p(x|y)$), **the singular values have a closed form and are given by** (see Makur and Zheng 2016 for more details)\n\n$$\n\\lambda_i=\\left(\\frac{\\sigma_0^2}{\\sigma_0^2+\\sigma^2}\\right)^{i/2}.\n$$\n\nWe can see that the decay rate of $\\lambda_i$ depends on the ratio $\\sigma^2/\\sigma_0^2$. Recall the model behind the joint distribution is $x=y+\\epsilon$, where $\\sigma^2$ is the variance of $\\epsilon$ and can be seen as noise; and $\\sigma_0^2$ is the variance of $Y$ and can be seen as information. Thus, we can see that if the **noise dominates the information** (i.e., $\\sigma^2\\gg\\sigma^2_0$), the decay is fast. On the other hand, if the **information dominates the noise** (i.e., $\\sigma_0^2\\gg\\sigma^2$), the decay is slow.\nIf $\\beta$ is given, and the noise-to-information ratio is high, the $\\beta$-regularity space becomes smaller. In other words, it becomes more difficult to identify functions in the high-noise setting. The reason of the resulting smaller space is that we require the Fourier coefficients to decay at a fast rate to ensure the series is finite.\n\nNow, back to the general model $p(x,y)$, qualitative speaking, the singular values $\\lambda_i$ of the operator $T$ describe how well we know about $x$ based on the information of $y$. If $y$ does not provide much information about $x$, or in other words, $x$ contains a lot of noise, the singular values decay fast, and we say the conditional expectation operator is severely ill-posed, and vice versa.\n\nThe $\\beta$-regularization spaces, as we have explained in the previous responses, are to characterize both the singular values of the conditional expectation operator $T$ and the Fourier coefficients of the function $\\rho(\\cdot)$. The appropriate value for $\\beta$ also depends on both $T$ and $\\rho(\\cdot)$. A larger $\\beta$ implies a smaller function space for $\\rho(\\cdot)$ to live in, given the operator $T$. Lastly, under our identifiability assumption, we would like to point out that the $\\beta$-regularity space can also be viewed as the range of the operator $(T^\\ast T)^{\\beta/2}$.\n\n### **Reference**\n\nMakur, A., & Zheng, L. (2016). Polynomial spectral decomposition of conditional expectation operators. In\u00a0*2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)*. IEEE."
                    },
                    "title": {
                        "value": "Responses to \"Clarifying my question about $\\beta$-regularity\""
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186907023,
                "cdate": 1700186907023,
                "tmdate": 1700250173170,
                "mdate": 1700250173170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vxkQp9o58z",
                "forum": "KdVvOA00Or",
                "replyto": "3jPImbfmYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Our Response: Thanks for your thought-provoking insights. First, we apologize for the confusion caused by our notation and terminology. Our norm is different, unlike the usual $L^2$ norm of functions, defined as $\\Vert f\\Vert=\\sqrt{\\int f(x)^2 dx}$, we define the inner product for the Hilbert space as $\\langle f(x,y), g(x, y)\\rangle = \\int f(x,y) g(x, y) p_s(x,y)dxdy=\\mathbb{E}_s(f(x,y)g(x,y))$ and norm as $\\Vert f\\Vert=\\langle f, f\\rangle^{1/2}$ (see page 3 of our paper). Thus, the norm we use for a function $f(y)$ is defined as $\\Vert f\\Vert=\\sqrt{\\mathbb{E}_s(f(y))^2}$, which is exactly the \u201cweighted $L^2$ norm\u201d you mentioned in your comments."
                    },
                    "title": {
                        "value": "Responses to \"Clarifying the \"gap\" between the  $L^2$ error and test-domain performance\""
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187004305,
                "cdate": 1700187004305,
                "tmdate": 1700187361464,
                "mdate": 1700187361464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AjKVBtX2WI",
                "forum": "KdVvOA00Or",
                "replyto": "3jPImbfmYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for reviewing our paper and providing discussion! Please let us know if there is any further questions for our revised paper and response."
                    },
                    "comment": {
                        "value": "Dear Reviewer SE3s,\n\nWe extend our sincere appreciation for your invaluable feedback and constructive suggestions. As we approach the conclusion of the rebuttal/discussion phase, we would like to inquire if there are any remaining questions or concerns that we can address or clarify.\n\nIf you find our revision and response satisfactory, could you please consider raising the score to support our efforts? Your support for our work and insightful suggestions for improving its quality have been immensely valuable to us. We are grateful for the time and consideration you have dedicated to our submission.\n\nThank you once again.\n\nAuthors."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507354858,
                "cdate": 1700507354858,
                "tmdate": 1700507354858,
                "mdate": 1700507354858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RWKx3p00Ub",
                "forum": "KdVvOA00Or",
                "replyto": "AjKVBtX2WI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to the authors for their responses."
                    },
                    "comment": {
                        "value": "I think the authors have done a good job responding to the reviewers' comments and revising the paper, so I am raising my rating from 6 to 8. I hope the authors will add some discussion of (a) $\\beta$-regularity in the Gaussian example and (b) how exactly estimating $\\omega$ in (weighted) $L_2$ relates to test-domain generalization error of the weighted empirical risk minimizer (since, as the authors pointed out, their result is already in weighted $L_2$ norm, I guess this is easy, just following the logic I laid out above)."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557140656,
                "cdate": 1700557140656,
                "tmdate": 1700557140656,
                "mdate": 1700557140656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zt01uOn2Wh",
                "forum": "KdVvOA00Or",
                "replyto": "3jPImbfmYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your suggestion and supporting!"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your invaluable feedback and guidance throughout the review process. Your insights have been incredibly helpful in improving our paper, and we have **implemented all the suggestions you provided accordingly in the revision**: We added the discussion on 1) $\\beta$-regularity in supplementary Section A.1 and 2) the relationship between the weighted $L^2$ norm and test-domain generalization error in supplementary Section B.1.\n\nOnce again, thank you for your time, commitment, and valuable suggestions. We appreciate your dedication to ensuring the highest standards and are grateful for your support."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629409174,
                "cdate": 1700629409174,
                "tmdate": 1700629716738,
                "mdate": 1700629716738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3H39j3mGqU",
                "forum": "KdVvOA00Or",
                "replyto": "Zt01uOn2Wh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_SE3s"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for making the revisions. One small error: in the new Appendix A.1, the phrase \"as we have explained in the previous responses,\" should be removed. :)"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655283694,
                "cdate": 1700655283694,
                "tmdate": 1700655283694,
                "mdate": 1700655283694,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cQU40eEzri",
            "forum": "KdVvOA00Or",
            "replyto": "KdVvOA00Or",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission974/Reviewer_4YDN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission974/Reviewer_4YDN"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript studies the continuous target shift problem. To adapt to the target shift, the authors reweighed the training samples with the density ratio of the responses: $\\omega(y)=p_t(y)/p_s(y)$. To estimate the weight $\\omega(y)$, a two-step procedure is proposed. First, the authors employed a kernel density estimator to estimate the conditional density and then used the outcomes to solve a regularized least-squares problem to obtain the weights. Additionally, the authors discussed conditions for identifiability of the weight $\\omega(y)$ and the consistency of the estimation results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, the manuscript is easy to follow and is technically sound. The empirical results also show improvement compared to the baselines."
                },
                "weaknesses": {
                    "value": "I found the major issue is insufficient comparison with other prior works.\nFor example, Nguyen et al., (2016) studied the same problem with a slightly different estimation procedure. It would be nice to discuss and compare the method. \n\nWhile the estimation procedure is straightforward, it would be nice to clarify what is the technical challenge and the novelty of this method. \n\n\n\n\nNguyen, T. D., Christoffel, M., & Sugiyama, M. (2016, February). Continuous target shift adaptation in supervised learning. In Asian Conference on Machine Learning (pp. 285-300). PMLR."
                },
                "questions": {
                    "value": "how to select $\\alpha$ in terms of the sample size?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission974/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission974/Reviewer_4YDN",
                        "ICLR.cc/2024/Conference/Submission974/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645749235,
            "cdate": 1698645749235,
            "tmdate": 1700695493346,
            "mdate": 1700695493346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2VyDPBQ0Uf",
                "forum": "KdVvOA00Or",
                "replyto": "cQU40eEzri",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 4YDN"
                    },
                    "comment": {
                        "value": "### Comment 1\n\n> *I found the major issue is insufficient comparison with other prior works. For example, Nguyen et al., (2016) studied the same problem with a slightly different estimation procedure. It would be nice to discuss and compare the method.*\n> \n> \n> *While the estimation procedure is straightforward, it would be nice to clarify what is the technical challenge and the novelty of this method.*\n> \n> *Nguyen, T. D., Christoffel, M., & Sugiyama, M. (2016, February). Continuous target shift adaptation in supervised learning. In Asian Conference on Machine Learning (pp. 285-300). PMLR.*\n> \n\n### Our Response\n\nIn our revised paper, we **added** **the** **L2IWE method** proposed by Nguyen et al. (2016) for extensive comparisons. Our proposed ReTaSA method still **outperforms** the L2IWE method consistently. The KMM and the L2IWE methods are similar in spirit as they both try to estimate the importance weight function through distribution matching; their difference is that they choose different function spaces to search for the importance weight function. The **novelty** of our method is that instead of matching, we formulate the problem as a task of solving an equation. Also, our proposed method only involves a one-step matrix inversion rather than an iterative optimization procedure.\nThe curse of dimensionality may pose some **challenges**. However, this problem can be addressed by adopting the black-box-shift estimation method, as detailed in the supplementary.\n\n### Comment 2 (Questions)\n\n> How to select\u00a0$\\alpha$\u00a0in terms of the sample size?\n> \n\n### Our Response\n\nThanks for pointing this issue out. We agree that hyperparameter tuning is not trivial. However, based on our Theorem 1, the regularization parameter needs to satisfy the condition that $\\lim_{n\\to\\infty}n\\alpha^2=\\infty$. Thus, in our experiment, we proposed to choose $\\alpha=n^{-1/4}$ as a rule of thumb to meet the condition for practical simplicity. In Section C.2.3 of the supplementary materials, we conducted a sensitivity analysis from which $\\alpha=n^{-1/4}$ achieves good performance. We use this rule of thumb for all the numerical studies, and the results are consistently good."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106796800,
                "cdate": 1700106796800,
                "tmdate": 1700506115439,
                "mdate": 1700506115439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XWss09PZLv",
                "forum": "KdVvOA00Or",
                "replyto": "cQU40eEzri",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for reviewing our paper! Any further questions for our revised paper and response?"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4YDN,\n\nWe extend our gratitude for your insightful feedback on our work. Your comments have been invaluable in shaping the refinement of our paper. Here's a summary of the key modifications we made in response to your suggestions:\n\n1. Inclusive comparison: We introduced the L2IWE method proposed by Nguyen et al. (2016) for a comprehensive comparative analysis.\n2. Enhanced clarity on novelty and challenge: We redefined the non-trivial target shift problem as a solvable linear equation problem, supported by a robust theoretical foundation.\n3. Efficient hyper-parameter tuning: We introduced a simple yet effective rule of thumb method for selecting the regularization parameter, along with a thorough ablation study.\n\nWith the end of the rebuttal phase approaching, we kindly invite you to review our responses and the revised paper. Your feedback is crucial to ensuring that our clarifications address the concerns you raised. If our revision and response meet your satisfaction, would you consider raising the score to support our work? We highly value your support and guidance in elevating the quality of our work.\n\nThank you for your time and consideration.\n\nAuthors."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507264840,
                "cdate": 1700507264840,
                "tmdate": 1700507264840,
                "mdate": 1700507264840,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eEf1kpImtQ",
                "forum": "KdVvOA00Or",
                "replyto": "cQU40eEzri",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_4YDN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Reviewer_4YDN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my comments and implementing the new baseline. The proposed method outperforms the baselines.\n If possible, it would be useful to discuss why L2IWE fails to beat non-adaptation in the SOCR and Szeged Weather dataset as Table 1 shows negative $\\Delta Pred$ MSE. Does any numerical instability occur in the estimation procedure? Then, it would be nice to address why the proposed method does not fail. As other reviewers pointed out, the kernel ratio density estimator could be unstable sometimes. More discussions on numerical stability can be helpful for practitioners to choose between algorithms.  \n\nOverall, I think the paper is well-written and the contribution is clear, so I raise my score a bit."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695471321,
                "cdate": 1700695471321,
                "tmdate": 1700695471321,
                "mdate": 1700695471321,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pGBiiZao8a",
                "forum": "KdVvOA00Or",
                "replyto": "cQU40eEzri",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "We appreciate your support for our work! Regarding the performance of L2IWE on the SOCR and Szeged Weather datasets, we observed that L2IWE's performance will decrease drastically when the shift becomes more severe. In Section C.3 of our supplementary materials, we presented several examples illustrating the estimated weights for each method. Notably, L2IWE exhibited higher weights concentrated in the middle of the response (i.e., $y$) support, with near-zero weights at the boundaries. However, in these experiments, the oracle weights shifted towards the boundaries. In summary, empirical evidence shows that L2IWE's performance is unsatisfactory under severe shifts. The shifts in the real datasets are more severe; we conjecture that this may contribute to the underperformance of the L2IWE method. Due to the time limit of the discussion period, we do not have enough time to find the causes more in-depth, but we shall investigate this later.\n\nAdditionally, we acknowledge the potential impact of ratio density estimation on our algorithm's performance. In Section C.2.3 of the supplementary materials, we conducted a sensitivity study involving two density ratio estimation methods. While we did not observe significant performance changes, we recognize the importance of a more in-depth analysis and discussion on numerical stability. Further exploration in this aspect would contribute to a comprehensive understanding of our algorithm's behavior.\n\nThanks again for your time and efforts!"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715887960,
                "cdate": 1700715887960,
                "tmdate": 1700717242397,
                "mdate": 1700717242397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HXLUcu0pOX",
            "forum": "KdVvOA00Or",
            "replyto": "KdVvOA00Or",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission974/Reviewer_2aLy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission974/Reviewer_2aLy"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of label shift within a continuous label space, such as in regression with label shift. To tackle this issue, the authors propose a method based on importance weighting, which transforms the learning task into an estimation problem concerning the density ratio of the continuous variable $y$. The method has statistical consistency in estimating the weight function under certain identifiability assumptions. A practical algorithm is derived from the theoretical analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper considers an interesting and important real-world problem with a compelling motivation.\n2. This work provides the statistical consistency analysis for the proposed estimator in the continuous label space. This technical analysis has the potential insights to benefit studies in this area."
                },
                "weaknesses": {
                    "value": "1. In the experiments, only low-dimensional and small-scale datasets are used for comparison. Thus, the empirical results are not sufficient to support the success of the proposed algorithm. Considering the increasing need of learning with high-dimensional data such as images and the poor performance of kernel density approximation in high-dimensional data, the comparison in high-dimensional data is important. \n\n2. Determining the hyperparameter $\\alpha$ for different data sets is difficult. \n\n3. In classical ratio estimation approaches, the estimation variance is sometimes large, e.g. $p_s(y)$ goes to zero, making the algorithm unstable. In the continuous label space, it is unclear whether this problem becomes worse. It is suggested to discuss this issue or to introduce an additional variance reduction mechanism."
                },
                "questions": {
                    "value": "See the comments in weakness part. As also, it is suggested to test the performance of the proposed algorithm on large-scale and high-dimensional datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649461850,
            "cdate": 1698649461850,
            "tmdate": 1699636022972,
            "mdate": 1699636022972,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5tfRLsNWi0",
                "forum": "KdVvOA00Or",
                "replyto": "HXLUcu0pOX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 2aLy"
                    },
                    "comment": {
                        "value": "### Comment 1\n\n> *In the experiments, only low-dimensional and small-scale datasets are used for comparison. Thus, the empirical results are not sufficient to support the success of the proposed algorithm. Considering the increasing need of learning with high-dimensional data such as images and the poor performance of kernel density approximation in high-dimensional data, the comparison in high-dimensional data is important.*\n> \n\n### Our Response\n\nIn the revision, we have demonstrated the effectiveness of our proposed method with larger datasets with higher dimensions (e.g., high dimension regression problem on UCI community and crime dataset and knowledge distillation regression problem on MNIST dataset). Due to space limits, we put the **additional numerical studies** in **Sections C.4.1 & 4.2** of the supplementary materials. Furthermore, to handle the density approximation problem with high-dimensional data, we adopted the black-box-shift-estimation method Lipton et al. (2018) proposed in our setting. Please refer to **Section C.4** for the details.\n\n### Comment 2\n\n> *Determining the hyperparameter $\\alpha$ for different data sets is difficult.*\n> \n\n### Our Response\n\nThanks for pointing this issue out. Based on Theorem 1, the regularization parameter must satisfy the condition that $\\lim_{n\\to\\infty}n\\alpha^2=\\infty$. Thus, in our experiment, we proposed to choose $\\alpha=n^{-1/4}$ as a rule of thumb to meet the condition for practical simplicity. In Section C.2.3 of the supplementary materials, we conducted a sensitivity analysis from which $\\alpha=n^{-1/4}$ achieves good performance. We use this rule of thumb for all the numerical studies, and the results are good. \n\n### Comment 3\n\n> *In classical ratio estimation approaches, the estimation variance is sometimes large, e.g.\u00a0$p_s(y)$\u00a0goes to zero, making the algorithm unstable. In the continuous label space, it is unclear whether this problem becomes worse. It is suggested to discuss this issue or to introduce an additional variance reduction mechanism.*\n> \n\n### Our Response\n\nIndeed, it is problematic when $p_s(y)$ goes to zero. This is why we need the assumption that $p_s(y)$ is bounded from below by some constant $\\epsilon>0$ (see Assumption 3). Also, we need the support of $p_t(y)$ to be a subset of the support of $p_s(y)$. When $p_s(y)$ goes to zero (while $p_t(y)$ is not zero), it corresponds to the out-of-distribution (OOD) setting. Although this is beyond the scope of our paper, it represents an interesting direction for future research. We discussed this issue as a future research direction in the final discussion.\n\n### Comment 4 (Questions)\n\n> *See the comments in weakness part. As also, it is suggested to test the performance of the proposed algorithm on large-scale and high-dimensional datasets.*\n> \n\nIn this revision, we **applied our method to larger datasets with higher dimensions** in additional experiments. More specifically, we include the experiments of high dimension regression problem on the UCI community and crime dataset and knowledge distillation regression problem on the MNIST dataset in the supplementary. The proposed method demonstrated good performance. Due to space limits, the numerical study results can be found in **Sections C.4.1 & 4.2** of the supplementary materials."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106683247,
                "cdate": 1700106683247,
                "tmdate": 1700106683247,
                "mdate": 1700106683247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CIeNpAsSQL",
                "forum": "KdVvOA00Or",
                "replyto": "HXLUcu0pOX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for reviewing our paper! Any further questions for our revised paper and response?"
                    },
                    "comment": {
                        "value": "Dear Reviewer 2aLy,\n\nWe express our gratitude for your time and efforts for reviewing our paper and offering valuable suggestions. In our rebuttal, we delved deeper into three key areas:\n\n1. Empirical performance comparison on large-scale and high-dimensional datasets.\n2. Elaboration on the regularization parameter selection, along with its ablation study.\n3. Further extension of discussions regarding $p_s(y)\\rightarrow 0$ and addressing out-of-distribution (OOD) problems.\n\nAs we approach the conclusion of the discussion phase, we hope you have had the opportunity to read our responses and the revised paper. In consideration of time constraints, we hope you can inform us of any lingering questions or concerns that require additional attention or clarification. If our revision and response meet your satisfaction, we would greatly appreciate your consideration in raising the score.\n\nYour time and consideration are highly appreciated.\n\nAuthors."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507239184,
                "cdate": 1700507239184,
                "tmdate": 1700507239184,
                "mdate": 1700507239184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Oe3M4mZiyZ",
            "forum": "KdVvOA00Or",
            "replyto": "KdVvOA00Or",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission974/Reviewer_D6fP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission974/Reviewer_D6fP"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses the challenge of distribution shifts in deploying modern machine learning models, specifically focusing on the target shift problem within a regression context. It tackles the situation where the continuous target variable y exhibits different marginal distributions between the training and testing domains, while the conditional distribution of features x given y remains constant. Notably, the regression problem's infinite-dimensional target space necessitates unique solutions. The authors propose ReTaSA, a nonparametric regularized approach to estimate the importance weight function and provide theoretical justification for it, thereby effectively addressing the continuous target shift problem. Extensive numerical studies on both synthetic and real-world datasets confirm the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The work focuses on addressing the challenge of classification tasks with an infinite-dimensional target space, while previous research on target shift primarily concentrated on scenarios where $y$ is categorical. This problem holds fundamental significance across various domains.\n\n- By precisely estimating the importance weight function, the model can effectively adapt to variations in the target variable's distribution between the training and testing domains. A continuous importance weight function offers greater flexibility compared to discrete methods.\n\n- The authors provide thorough theoretical justifications for consistency and error rate bounds, enhancing the paper's rigor and reliability.\n\n- The paper's organization is well-structured, maintaining a clear and easily-followed flow from start to finish.\n\n- Extensive references to related work offer a comprehensive overview of prior research, providing valuable context for the study and highlighting the authors' deep understanding of the field."
                },
                "weaknesses": {
                    "value": "- Given the importance weight function, is the estimation process sensitive to noise or outliers in the data and thus potentially leading to inaccurate estimates of the importance weight function? Or this limitation is conquered by applying the regularization technique in your paper? \n\n - Estimating a continuous importance weight function is a probabilistic process, together with the regularization process, this may inherent uncertainty in the estimates, affecting the model's reliability in certain cases. How do you solve this issue? \n\n - Regularization can help stabilize the estimation process, but accurate estimation of a continuous importance weight function may still require a substantial amount of data from both source and target domains. What is the sample complexity of your algorithm?"
                },
                "questions": {
                    "value": "See above \"weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission974/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission974/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission974/Reviewer_D6fP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842336737,
            "cdate": 1698842336737,
            "tmdate": 1699636022899,
            "mdate": 1699636022899,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EPFwlmGjkJ",
                "forum": "KdVvOA00Or",
                "replyto": "Oe3M4mZiyZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer D6fP"
                    },
                    "comment": {
                        "value": "### Comment 1\n\n> *Given the importance weight function, is the estimation process sensitive to noise or outliers in the data and thus potentially leading to inaccurate estimates of the importance weight function? Or this limitation is conquered by applying the regularization technique in your paper?*\n> \n\n### Our Response\n\nYes, the noise and outlier data will affect the estimation performance and the regularization applied in our method can enhance our ability to robustly adjust to uncertainties in estimating the importance weight function.\n\nPrevious studies have illustrated such advantage of regularization in classification tasks; for instance, Azizzadenesheli et al. (2019) introduced a regularized version of the label shift estimator called RLLS, building upon the BBSE method of Lipton et al. (2018), who did not employ regularization. The regularized estimator RLLS demonstrated superior performance to the original non-regularized version BBSE, indicating that the regularization does help.\n\nIn the continuous continuous, regularization serves a **dual** purpose: addressing the ill-posedness problem while providing the advantage of robustness. However, unlike the classification case where regularization is optional (i.e., Lipton et al. 2018), the problem we consider becomes ill-posed due to the 'explosive behavior' in the continuous case without regularization, as detailed in our paper. So, regularization is a must-have in our case.\n\n### Comment 2\n\n> *Estimating a continuous importance weight function is a probabilistic process, together with the regularization process, this may inherent uncertainty in the estimates, affecting the model's reliability in certain cases. How do you solve this issue?*\n> \n\n### Our Response\n\nThe functional estimation, and indeed any estimator, is inherently contingent upon the randomness of the data sample; uncertainty becomes an inherent characteristic of the estimation. In our study, **we establish a theoretical bound for the estimator\u2019s convergence rate in Theorem 1**, and the error bound measures the estimation uncertainty from the theoretical aspect. **Also, in our numerical evaluation, we presented all the estimation performance with confidence intervals and standard deviations, which can capture the estimation uncertainty.** \n\nA related aspect involves the uncertainty quantification for the continuous case. For instance, considering the randomness inherent in the estimation, how to construct a prediction set for the response. While this lies beyond the scope of our current paper, it represents an interesting direction for future research. Existing efforts have been made on uncertainty quantification in the classification case (e.g., Podkopaev and Ramdas 2021), but notably, there appears to be a gap in addressing the continuous case. We have added a discussion on future research in the paper.\n\n### Comment 3\n\n> *Regularization can help stabilize the estimation process, but accurate estimation of a continuous importance weight function may still require a substantial amount of data from both source and target domains. What is the sample complexity of your algorithm?*\n> \n\n### Our Response\n\nWe agree that the sample sizes play an important role in the weight estimation. Theorem 1 provides a stochastic order bound for our proposed functional estimation, in which the sample size will directly affect the convergence rate. In Equation (11), the bound is a function of $n$ (sample size of the source data) and $m$ (sample size of the target data) given other quantities (e.g., the tuning parameter $\\alpha$ and the bandwidth $h$).\n\n### References\n\nLipton, Z., Wang, Y. & Smola, A.. (2018). Detecting and Correcting for Label Shift with Black Box Predictors. *ICML*.\n\nAzizzadenesheli, K., Liu, A., Yang, F., & Anandkumar, A. (2019). Regularized learning for domain adaptation under label shifts. *ICLR*.\n\nPodkopaev, A., & Ramdas, A. (2021). Distribution-free uncertainty quantification for classification under label shift. *UAI*."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106503294,
                "cdate": 1700106503294,
                "tmdate": 1700106503294,
                "mdate": 1700106503294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rcs1K6O3Tx",
                "forum": "KdVvOA00Or",
                "replyto": "Oe3M4mZiyZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for reviewing our paper! Any further questions for our revised paper and response?"
                    },
                    "comment": {
                        "value": "Dear Reviewer D6fP,\n\nWe would like to thank you for your thorough review and insightful suggestions. In response to your feedback, we have incorporated additional discussion points in our rebuttal:\n\n1. Explanation of how regularization contributes to estimation robustness.\n2. Detailed insights into the measurement and quantification of estimation uncertainty.\n3. Clarification of the theoretical sample complexity of our proposed method.\n\nAs we near the conclusion of the rebuttal phase, we kindly invite you to take a moment to review our responses. We are eager to know if our efforts have addressed and clarified the concerns you raised. If our revision and response align with your satisfaction, we would sincerely appreciate your consideration in raising the score.\n\nYour time and consideration are greatly appreciated.\n\nThank you,\n\nAuthors."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507197283,
                "cdate": 1700507197283,
                "tmdate": 1700507197283,
                "mdate": 1700507197283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]