[
    {
        "title": "Neuron-Enhanced AutoEncoder Matrix Completion: Theory and Practice"
    },
    {
        "review": {
            "id": "jsdPi24iB8",
            "forum": "kPrxk6tUcg",
            "replyto": "kPrxk6tUcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4817/Reviewer_THxa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4817/Reviewer_THxa"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed an autoencoder method for matrix completion and applied it to collaborative filtering. The corresponding generalization error bounds are also provided. The experiments showed the superiority of the proposed method over the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This is a good work with new method, theory, and extensive experiments (many datasets, settings, baselines, and figures). \n* The motivation is clear and the theoretical analysis, such as the bounds related to MCAR and MNAR, are insightful and practical for the recommendation systems.\n* The proposed AEMC-NE is able to outperform SVD++, LLORMA, AutoRec, DMF+, etc."
                },
                "weaknesses": {
                    "value": "* The time cost of the proposed method AEMC-NE is higher than that of AEMC.\n* The motivation of conducting the experiments of Table 4 is not clear.\n* Some important results (e.g., time costs comparison, Theorem A.1) are presented in the appendix, not the main paper. Theorem A.1 is more useful than Theorem 3.2 because the former is based on the estimated probability."
                },
                "questions": {
                    "value": "1. According to equation (1), is the model $f$ actually a denoising autoencoder, with a partial reconstruction loss, not a classical autoencoder?\n2. How does the data dimension $m$ affect the generalization error bounds? The authors only discussed $n$, the number of samples.\n3. Regarding Theorem 3.2, if $p_{ij}$ is close to zero, is the bound useless? Could the authors give more explanation about the roles of $p_{ij}^{-2}$?\n4. Are the values in Table 1 RMSEs?\n5. In Figure 2, it seems that if the middle layer width of the main network is set to 10, the true latent dimension of the data, the performance of AEMC-NE is not better than those with larger width. What is the possible reason?\n6. As shown by Table 2 and Table 3, the proposed AEMC-NE outperformed AEMC with ReLU output. Did the authors test AEMC with other activation function, such as sigmoid, in the output layer? How about their performance?\n7. In Table 5, as the observation probabilities are known, what objective function have the authors used in the proposed method? The one in (9) or (7)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4817/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4817/Reviewer_THxa",
                        "ICLR.cc/2024/Conference/Submission4817/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698329269367,
            "cdate": 1698329269367,
            "tmdate": 1700402514404,
            "mdate": 1700402514404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DeRXGLnv1s",
                "forum": "kPrxk6tUcg",
                "replyto": "jsdPi24iB8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thanks for your comments. Our responses to Weaknesses and Questions are as follows.\n\n**Response to Weaknesses**:\n1. Yes, our method is slightly slower than AEMC. However, the accuracy of our method is higher than that of AEMC in all cases of the experiments. In many real scenarios, a slightly higher time cost is acceptable as we require high accuracy.\n2. In fact, we have explained the motivation in Section 5.4. In Tables 2 and 3, the rating matrices are close to square matrices and hence cannot fully demonstrate the superiority of our method over classical matrix factorization methods. Therefore, in Table 4, we use a subset of MovieLens-1M, where the matrix size is $3706\\times 500$. In this matrices, $n\\gg m$. According to our theorems, when $n$ is larger (compared to fixed network architecture), the generalization bound is tighter. Indeed, as shown by Table 4, the improvement over matrix factorization becomes more significant.\n3. This is due to the page length limitation. We have tried our best to include more important results in the main paper.\n\n\n**Response to Questions**:\n1. Yes, it is not a standard autoencoder. However, it is still a neural network to reconstruct the input (though partially). That's why we still call it autoencoder.\n2. Thanks for raising the question. The reason we didn't discuss it is that when $m$ changes, the size of the neural network will change, which influences many terms in the error bound, e.g. the number of parameters, the missing rate, and $\\Vert\\tilde{\\boldsymbol{X}}\\Vert_F$. It is difficult to directly analyze the impact of $m$. If we fix the size of the hidden layers and the missing rate and assume that the latent dimension of the data does not change, then $m$ has little impact on the bound.\n3. If $p_{ij}$ is close to zero, we may not observe the corresponding entry of the matrix. So in practice, $p_{ij}$ stays far from zero. Note that the bound is related to $\\sqrt{\\sum_{(i, j) \\in S} p_{i j}^{-2}} /(m n)$, where there is a denominator $mn$. Thus, a few small $p_{ij}$ may not have a big impact on the bound.\n4. No. The values in Table 1 are the relative recovery errors defined in the first paragraph of Section 5.1.\n5. Thanks for the insightful comment. The possible reason is that when the latent dimension is too small, it will be difficult to train the two autoencoders of our method.  Figure 3(c) shows that AEMC is not sensitive to the latent dimension if it is larger than or equal to 10. The optimization of AEMC is easier than that of our AEMC-NE.\n6. Sure, we tested Sigmoid and Tanh but their performances are much worse than that of ReLU.\n7. We tried both (7) and (9) and found that their performances are similar. This also verified that our method AEMC-NE is useful for both MCAR (missing completely at random) and MNAR (missing not at random)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700114696163,
                "cdate": 1700114696163,
                "tmdate": 1700114762338,
                "mdate": 1700114762338,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ONwSzVJSd5",
            "forum": "kPrxk6tUcg",
            "replyto": "kPrxk6tUcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4817/Reviewer_fr12"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4817/Reviewer_fr12"
            ],
            "content": {
                "summary": {
                    "value": "The paper presented a neuron-enhanced autoencoder based method for matrix completion. The main idea is to adds an element-wise autoencoder to each output of the main autoencoder to enhance the reconstruction. The paper has both theoretical results (generalization bounds) and numerical results (collaborative filtering)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed network architecture is composed of a main autoencoder and an element-wise. The idea is new.\n\n2. There are two theorems of generalization analysis. One is for the case of missing completely with random, and the other is for the case of missing not at random. They verified the effectiveness of the proposed method theoretically.\n\n3. The paper included a lot of experiments and the proposed method outperformed many baselines in most cases."
                },
                "weaknesses": {
                    "value": "1. The writing style could be improved. Currently, the section of experimental evaluation focuses on collaborative filtering. To be consistent, the section of introduction should provide more information or discussion about collaborative filtering rather than matrix completion. \n\n2. The time costs haven\u2019t been compared, though the theoretical analysis was provided at the bottom of page 4."
                },
                "questions": {
                    "value": "1. In Eq.(2), the loss function is the square loss. Has the authors tried other loss functions such as the absolute loss? Is the proposed method applicable to the setting of implicit feedback?\n\n2. In Figure 1, is there an activation function in the output layer of the main network?\n\n3. In Theorem 3.1, when $|S|$ or $|S^c|$ is close to zero, the bound is very loose. How to explain this phenomenon?\n\n4. The results in Figure 2 look nice. But the motivation for the specific network architectures such as 300-100-30-100-300 should be illustrated."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698372896586,
            "cdate": 1698372896586,
            "tmdate": 1699636465278,
            "mdate": 1699636465278,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zhffQ60rdn",
                "forum": "kPrxk6tUcg",
                "replyto": "ONwSzVJSd5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you very much for the positive assessment on our work.  Our responses are as follows.\n\n**Response to Weaknesses**: Thanks for pointing out the weaknesses. In the revised paper, we added more context about collaborative filtering to Introduction (e.g. paragraph 1).\nActually, we have provided the time cost comparison in the original submission. It was shown by Table 6 of the appendix (Appendix E).\n \n**Response to Question 1**:\nYes, we actually tested other loss functions. To ensure consistence with the evaluation metric RMSE, we use the square loss. If the evaluation metric is MAE, we\u2019d better use absolute loss. By the way, the square loss is the most widely-used loss in collaborative filtering and matrix completion. \nOur method can be extended to the scenarios of implicit feedback if we use negative sampling and other loss functions such as BPR loss (Bayesian Personalized Ranking, S Rendle\u00a02012). We added this discussion to the conclusion section of our paper.\n\n**Response to Question 2**:\nNo. Our element-wise network can learn an activation function adaptively. \n\n**Response to Question 3**:\nThis is inevitable in the setting of transductive learning. When $|S|$ or $|S^c|$ is close to zero, the uncertainty for the training error or test error will increase significantly. \n\n**Response to Question 4**:\nThanks for pointing out it. This architecture was motivated by the fact that the dimension of the synthetic data is 300, the latent dimension is 10, and in practice we may never know the true latent dimension. Thus we use a larger latent dimension 30 in the autoencoder. Figure 2(c) also showed the effectiveness of the larger latent dimension."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105161805,
                "cdate": 1700105161805,
                "tmdate": 1700105161805,
                "mdate": 1700105161805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ov6RbQGggB",
            "forum": "kPrxk6tUcg",
            "replyto": "kPrxk6tUcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4817/Reviewer_fbH8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4817/Reviewer_fbH8"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of matrix completion with neural network, which is quite different from classical low-rank matrix completion. The authors proposed a new method as well as theoretical analysis. The experiments on synthetic data and real data demonstrated showed the usefulness of the proposed method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The theory of neural network based collaborative filtering hasn\u2019t been well-studied in literature. This paper proposed a new autoencoder matrix completion method and analyzed the generalization ability for different missing mechanisms. The strengths of the paper are as follows.\n* The paper is well-organized. The literature review and related work are complete.\n* The paper provided two theorems. The analysis and results are solid.\n* The experiments are sufficient. The new method has been compared with more than five baselines on synthetic data, MovieLens 100k/1M/10, Doban and Flixter. And the performance is good."
                },
                "weaknesses": {
                    "value": "Several minor issues are as follows.\n* It is not very clear how to determine the architectures of the two autoencoders. Is there a rule of thumb?\n* Some claims should be explained more detailedly. For instance, in the discussion for Theorem 3.2, more explanation about \u2018$\\sqrt{\\sum_{(i, j) \\in S} p_{i j}^{-2}} /(m n)$ is at the scale of $1 / \\sqrt{|S|}$\u2019 should be provided. In Table 2, a few results are missing. Why?"
                },
                "questions": {
                    "value": "* Do the two theorems provide any guidance for determine the architectures of the two autoencoders?\n* According to the formulation of the proposed method, both the layer-wise network and the element-wise network can be any MLPs, not necessarily autoencoders. Right?\n* In Analysis B of Section 3.1, is it possible that mean filling is better than zero filling for $\\tilde{X}$?\n* In Analysis C of Section 3.1, the authors only discussed the impact of $n$ on the generalization bound. What is the impact of $m$? A larger $m$ leads to a tighter bound?\n* In Section 5.1, the latent dimension of the data generating model is 10, but as shown by the caption of Figure 2, the latent dimension of the layer-wise autoencoder was set to 30. What is the motivation of this setting? \n* How to apply the proposed method to the collaborative filtering problem in the case of implicit feedback?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4817/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4817/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4817/Reviewer_fbH8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698484189963,
            "cdate": 1698484189963,
            "tmdate": 1699636465176,
            "mdate": 1699636465176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yjX5iBQqy5",
                "forum": "kPrxk6tUcg",
                "replyto": "Ov6RbQGggB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you so much for the very positive evaluation. Our responses to your comments and questions are as follows.\n\n**Response to Weaknesses**:\n1. Theoretically, according to our theorems, the size of the neural networks should not be too large when compared to the size of the training data. The number of the parameters of the two autoencoders should be smaller than the number of observed elements in the rating matrix. Empirically, we do not use deep autoencoders unless the number ($n$) of samples is much larger than the number ($m$) of variables (features). For instance, for the real datasets considered in our paper, the main autoencoder has only one hidden layer and the element-wise autoencoder also has only one hidden layer. For the synthetic data, because the number of samples (3000) is much larger than the number of features (300), as shown by the caption of Figure 2, we use three hidden layers in the main autoencoder and two hidden layers in the element-wise autoencoder. Figure 2(b) shows that our method is not sensitive to the width of the hidden layers of the element-wise autoencoder. In sum, when $n$ is much larger than $m$, we use more hidden layers in the two autoencoders.\n2. The reason is that the average of $p_{ij}$ is about $\\frac{|S|}{mn}$. We have improved the explanations in the revised paper. In Table 2, some values are missing because in the original papers of the corresponding methods, the datasets were not tested.\n\n**Response to Questions**:\n1. Yes. This has been explained by our response to the Weakness 1.\n2. Thanks for pointing out this property. Indeed, the two neural networks are not necessarily autoencoders. Their architectures can be asymmetric. Our theoretical results apply to general MLPs.\n3. It is possible but it depends on data. In the theorem, when using zero filling, $\\Vert\\tilde{\\boldsymbol{X}}\\Vert_F$ can be reduced significantly; when using mean filling, $\\Vert\\tilde{\\boldsymbol{X}}\\Vert_F$ is large. If mean filling cannot reduce the training error significantly, the corresponding overall generalization bound will be looser.\n4. The reason we didn't discuss $m$ is that when $m$ changes, the size of the neural network will change, which influences many terms in the error bound, e.g. the number of parameters, the missing rate, and $\\Vert\\tilde{\\boldsymbol{X}}\\Vert_F$. If we fix the size of the hidden layers and the missing rate and assume that the latent dimension of the data does not change, then $m$ has little impact on the bound.\n5. There are two reasons for this setting. First, in real datasets, the true latent dimension of data is unknown. Thus we do not expect to use the true dimension in the experiment. Second, we find that, as shown by Figure 2(c), a larger $\\hat{d}$ often leads to a better performance.\n6. For implicit feedback, we need to use negative sampling and ranking loss functions such as BPR loss (Bayesian Personalized Ranking, S Rendle 2012). Our theoretical results apply to any loss functions. We added this discussion to the conclusion section of our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111006663,
                "cdate": 1700111006663,
                "tmdate": 1700111006663,
                "mdate": 1700111006663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ovk9apkArL",
            "forum": "kPrxk6tUcg",
            "replyto": "kPrxk6tUcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4817/Reviewer_uVGT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4817/Reviewer_uVGT"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a neuron-enhanced autoencoder matrix completion (AEMC-NE) method and applies it to collaborative filtering. AEMC-NE can adaptively learn an activation function for the output layer to approximate possibly complicated response functions in real data. The theoretical analysis and numerical results demonstrate the effectiveness of AEMC-NE in comparison to other methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The proposed algorithm is interesting and effective. It has lower RMSE than the compared method in the experiments of many benchmark datasets.\n2. The theoretical results are rich and nontrivial. Theorem 3.1 and Theorem 3.2 shows the generalization error bounds of matrix completion in the settings of MCAR and MNAR respectively. An appealing theoretical result is that \u2018Filling the missing values with zeros is helpful\u2019. This is an important result for collaborative filtering."
                },
                "weaknesses": {
                    "value": "Please refer to my questions."
                },
                "questions": {
                    "value": "1. As shown in Fig.1, for the autoencoder, items correspond to samples while users correspond to features. Can we exchange the roles of items and users? What is the possible impact on the recommendation performance?\n2. On page 4, it is stated that when the mechanism is missing not at random, the $Q$ in the objective function should be replaced by $S$. How to achieve $S$?\n3. On page 7, is it possible that $\\sqrt{\\frac{L_W^3 \\zeta \\bar{d}}{n}}$ is larger than 1? It would be much better if the authors can provide an example of the value of $L_W^3 \\zeta \\bar{d}$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4817/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753572101,
            "cdate": 1698753572101,
            "tmdate": 1699636465098,
            "mdate": 1699636465098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ek9IPSNmbR",
                "forum": "kPrxk6tUcg",
                "replyto": "Ovk9apkArL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4817/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4817/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thanks a lot for recognizing our work. Our responses to your questions are as follows.\n\n**Response to Question 1**: Fig. 1 just shows an example. We can exchange the roles of items and users. As mentioned in Analysis C of Section 3.1, increasing $n$ (the number of samples) will reduce the generalization error bound and we hence propose to construct an autoencoder for the users when there are more users than items and vice versa.  In Table 4, we reported the experimental results of a few baselines on a subset of the MovieLens-1M dataset. The matrix size of the subset is 3706x500 (3706 items and 500 users), where the autoencoder of our method is for the items. We see the improvement over the matrix factorization method is more significant that that in Table 2 (entire MovieLens-1M). Actually, if the autoencoder is for users, the performance will degrade. This verifies the impact of the roles of items and users.\n\n\n**Response to Question 2**: As stated in Section 3.2, $Q$ can be obtained by a few approaches (Wang et al., 2019; Ma & Chen, 2019) such as Naive Bayes or Logistic Regression (Schnabel et al., 2016).\n\n**Response to Question 3**:  In $\\sqrt{\\frac{L_W^3 \\xi_{\\bar{d}}}{n}}$, $L_W$ is the number of layers of the network, $\\bar{d}$ is the maximum width, and $\\zeta$ (sampling rate, usually very small) is in the range $(0,1)$. Usually, we do not use a large $L_W$. Suppose $n=5000$, $L_W=4$, $\\bar{d}=200$, and $\\zeta=0.1$ (quite large), then $\\frac{L_W^3 \\xi_{\\bar{d}}}{n}=0.256$, which is much less than 1. Indeed, it is possible that $\\frac{L_W^3 \\xi_{\\bar{d}}}{n}$ is larger than 1 when $n$ is too small compared to the size of the neural network. But in practice, $n$ is usually very large."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4817/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107567328,
                "cdate": 1700107567328,
                "tmdate": 1700107567328,
                "mdate": 1700107567328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]