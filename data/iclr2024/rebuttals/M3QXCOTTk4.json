[
    {
        "title": "The Curse of Diversity in Ensemble-Based Exploration"
    },
    {
        "review": {
            "id": "xuwLNxN4gS",
            "forum": "M3QXCOTTk4",
            "replyto": "M3QXCOTTk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5475/Reviewer_ZXyY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5475/Reviewer_ZXyY"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new way of ensembling RL agents. N neural network value functions are trained, consisting of a convolutional encoder and a value head. Each encoder has a main value head, so we have N main value heads. For each encoder, N value heads are trained with temporal difference, each using as the target one of the main value heads. That gives us N^2 value heads in total. This improves representation learning and data utilization. There are extensive experiment showing that training ensembles of RL agents with other methods are hard, while the proposed method works well. There are experiments on atari with DQN and mujoco with SAC."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The observation that training ensembles of RL agents is hard is interesting and is analyzed well\n- The proposed method is novel and sound\n- The experiments are comprehensive and demonstrate the benefits of the method"
                },
                "weaknesses": {
                    "value": "No major weaknesses"
                },
                "questions": {
                    "value": "\\-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5475/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698122316868,
            "cdate": 1698122316868,
            "tmdate": 1699636558663,
            "mdate": 1699636558663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QbOvX2LTdH",
                "forum": "M3QXCOTTk4",
                "replyto": "xuwLNxN4gS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you very much for your positive comments! We are pleased to hear that there are no major concerns regarding our paper. Your feedback is greatly valued and has been encouraging for us."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5475/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350791781,
                "cdate": 1700350791781,
                "tmdate": 1700350791781,
                "mdate": 1700350791781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k38nr4jDmX",
            "forum": "M3QXCOTTk4",
            "replyto": "M3QXCOTTk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5475/Reviewer_H7rr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5475/Reviewer_H7rr"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the problem of off-policy-ness in ensemble learning. Motivated by the recent discovery in offline RL (i.e., the tandem paper), and empirical observations in ensemble learning methods of BootstrapDQN and naive ensemble SAC, the authors introduce a representation learning approach that considers the off-policy data in auxiliary tasks.\nExperiments on continuous control and discrete control demonstrate the effectiveness of the proposed method.\n\n\n----\n\nAfter rebuttal, I increased my overall rating from 5 to 6, soundness score from 2 to 3, and contribution score from 2 to 3."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I like the way this paper is presented. It is clearly motivated by empirical discoveries, together with reasonings, and followed by solutions to the identified problems.\n\nThe authors made great efforts in conducting and presenting experiments. Results are reported in a statistically-identifiable way. I really appreciate it."
                },
                "weaknesses": {
                    "value": "### On high-level Motivation:\n\nI\u2019m lost in the motivation of using ensemble in **policy learning**. As has been demonstrated in [EDAC] and [REDQ], I acknowledge that using ensemble learning for the **value function** could lead to improved performance, as the value can be more accurate, with uncertainty. But what is the motivation for having **multiple policies** for ensemble (because they are sample generators, rather than learners). Should not those samplers aim at more efficiently decreasing the uncertainty in the value function?\n\nThis is a concern with the continuous setting. BootstrapDQN is purely a **value-based** method, and in discrete tasks, the epsilon-greedy exploration can be regarded as sampling from an off-policy uniform policy, this should not be a problem.\n\n### On BootstrapDQN:\n\nIn BootstrapDQN, different samples are assigned different weights to analog bootstrap sampling. It would not be surprising to see individual BootstrapDQN perform worse than DDQN. To verify the claim that *BootstrapDQN\u2019s performance improvement is purely based on voting* made by the authors, a comparison between Bootstrap Sampling/ normal sampling; and a comparison between majority voting and \n\n\nFigure 2: The reported results of SAC can not match the open-source implementations. This might be due to the usage of a different size of buffer (200k). Could the authors please explain more about this?\n\n\n### On Performance:\n\nThe performance of CERL seems marginal in Figure 7. Also in Figure 7, BootstrapDQN seems not helpful at all.\n\n\n### On related work:\n\n\nThis work [https://arxiv.org/pdf/2209.07288.pdf] discussed the ensemble learning that trades off between exploration and exploitation using different reward values may also be worth a discussion.\n\nThis work [https://openreview.net/forum?id=NOApNZTiTNU] and [REDQ] discussed the usage of the ensemble in Q-learning. REDQ has been considered to be state-of-the-art since 2021.\n\n\nReferences:\n\n[EDAC] An, Gaon, et al. \"Uncertainty-based offline reinforcement learning with diversified q-ensemble.\"\u00a0Advances in neural information processing systems\u00a034 (2021): 7436-7447.\n\n[REDQ] Chen, Xinyue, et al. \"Randomized ensembled double q-learning: Learning fast without a model.\"\u00a0arXiv preprint arXiv:2101.05982\u00a0(2021)."
                },
                "questions": {
                    "value": "Please see the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5475/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5475/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5475/Reviewer_H7rr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5475/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698609649446,
            "cdate": 1698609649446,
            "tmdate": 1700527827780,
            "mdate": 1700527827780,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "93n2018XBO",
                "forum": "M3QXCOTTk4",
                "replyto": "k38nr4jDmX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for liking our presentation and providing detailed feedback and suggestions! Please see our response below\n\n## On high-level motivation\n\n> As has been demonstrated in [EDAC] and [REDQ], I acknowledge that using ensemble learning for the value function could lead to improved performance, as the value can be more accurate, with uncertainty. But what is the motivation for having multiple policies for ensemble\n\nWe emphasize that even though both (1) ensemble-based exploration methods such as Bootstrapped DQN and Ensemble SAC and (2) methods such as EDAC and REDQ use ensembles, they use ensembles in completely different ways and purposes. Bootstrapped DQN and Ensemble SAC use ensembles *purely for better exploration*.  In contrast, methods such as EDAC and REDQ use ensembles *purely for improving value estimation*. The concrete differences are detailed below, which should clear up your confusion:\n\n* The number of policies:\n  * Both Bootstrapped DQN and Ensemble SAC maintain $N$ policies. Note that for Bootstrapped DQN the $N$ policies are *implicitly defined by the $N$ value functions*. Most importantly, all $N$ policies are used to collect samples. Concretely, for each episode, one policy is sampled for interaction. **The motivation for having $N$ policies in Ensemble SAC is the same as that of having $N$ different value functions (and hence $N$ *implicit* policies) in Bootstrapped DQN: to explore the state space with a diverse set of policies (explicit or implicit) concurrently in a temporally coherent manner**.\n  * EDAC and REDQ are simply not designed for exploration. They only have one policy and it is the only one used to interact with the environment.\n* How the value functions are used:\n  * In Bootstrapped DQN and Ensemble SAC, *each value function $Q_i$  evaluates one  policies $\\pi_i$*. These value functions are *independent* (though they may share networks) in the sense that the TD regression target for each ensemble member is computed independently with their target network. Concretely, the target for $Q_i(s, a)$ is $r(s, a) + \\gamma \\bar{Q}_i(s\u2019, a_i\u2019)$, where $a_i\u2019\\sim\\pi_i(a_i\u2019|s\u2019)$. They are not designed to produce more accurate value estimations than single-agent Double DQN or SAC. There is no explicit mechanism that encourages more accurate value estimations such as penalizing Q-value based on uncertainty (though the diverse data may help implicitly).\n  * In EDAC and REDQ, *all the $N$ value functions $Q_{1:N}$ evaluate the same single policy $\\pi$*. The $N$ value functions **share a single TD regression target** that is aggregated from all target networks. For examples, in EDAC, the target for $Q_i(s, a)$ -- regardless $i$ -- is $r(s, a) + \\gamma\\min_j \\bar Q_j(s\u2019, a\u2019)$ (ignoring the entropy term), where $a' \\sim\\pi(a'|s')$. Note how the target values are aggregated with the $\\min$ operator to counter overestimation.\n\nIn summary, the goal of having multiple policies in Ensemble SAC is to promote better exploration. This is the exactly same motivation as having multiple value functions (which implicitly define multiple policies) in Bootstrapped DQN. In contrast, EDAC and REDQ are not designed for better exploration; there is only one policy in them and it is the only policy interacting with the environment."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5475/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350669763,
                "cdate": 1700350669763,
                "tmdate": 1700352259770,
                "mdate": 1700352259770,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LhRUonhfWR",
                "forum": "M3QXCOTTk4",
                "replyto": "mkLr1eelSw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5475/Reviewer_H7rr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5475/Reviewer_H7rr"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Many thanks for the detailed response. My concerns are well addressed, and I have increased my rating accordingly."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5475/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527720327,
                "cdate": 1700527720327,
                "tmdate": 1700527720327,
                "mdate": 1700527720327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TPcH3Eyss4",
            "forum": "M3QXCOTTk4",
            "replyto": "M3QXCOTTk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5475/Reviewer_wgEM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5475/Reviewer_wgEM"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel observation for ensemble-based exploration: each ensemble member has significantly worse performance than single-agent baselines, even when the aggregate policies outperform them. The authors demonstrate this in experiments with Double DQN and SAC on Atari and Mujoco environments. They show that increasing the replay buffer and sharing layers reduce the effect, but do not lead to better performance. The paper proposes a novel representation learning method (CERL), in which ensemble members predict each other's targets with additional network heads. Results indicate that CERL improves the performance individual ensemble members without loosing too much diversity of their predictions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The reviewer liked the paper a lot. The main hypothesis makes sense and is substantiated in multiple experiments that show the effect nicely. The paper is well written and the figures are clearly readable. More detailed figures for individual environments are provided in the appendix, which is welcome to get an idea how trustworthy the aggregate performance measures are. The proposed method is not terribly innovative, but to the best knowledge of this reviewer novel. The discussion on other representation learning methods is nice, too."
                },
                "weaknesses": {
                    "value": "While the main paper is very well written and the experiments appear quite thorough, the reviewer took issue with the way that some conclusions were presented. In particular the connection to exploration (which is in the title) ignores some major alternative explanations of the results. While the reviewer recommends to accept the paper, some phrases *need* to be changed, and some discussion needs to be added, to prevent the casual reader from misinterpreting the text and results. These are:\n\n1. \"First, it challenges some previous work\u2019s claims that the improved performance of approaches such as Bootstrapped DQN in some tasks comes from improved exploration\" (p.3): as mentioned earlier in the paper, ensemble methods are widely used to drive exploration into unseen regions of the state-action space. However, the authors claim that \"Surprisingly, simply aggregating the learned policies at test-time provides a huge performance boost in many environments\" (p.3), and proceed to downplay the effect of exploration in favor of an explanation based on \"aggregation\". This is a problem in statements like \"the two cases[, DQN(indiv.) and Double DQN,] have access to the same amount of data and have the same network capacity\" (p.3), as the change in exploration means that the two algorithms are based on different data distribution and will therefore behave differently, irrespective of the on- and off-policy-ness of the data. This potential interaction between exploration (which part of the state-space is covered) and on-policy-ness (which member got to sample it) cannot be distinguished with the presented experiments, but the text reads as if the latter is the only obvious conclusion (see below).\n\n2. \"These results confirm our hypothesis regarding the cause of the observed performance degradation\" (p.5). This is dangerous abductive reasoning: experimental evidence that is consistent with your hypothesis does not make your hypothesis true (one can only falsify a hypothesis). This may sound like a nitpick, but the paper does not discuss the exploration effect of ensembles enough. For example, in Figure 3 (middle), the BootDQN (indiv.) curve is significantly above the Passive (10%) curve. If indeed Algo 2 and Algo 3 are exactly equivalent except for how often they sample the other (non-passive) ensemble members, and *only* this affects the performance due to off-policy sampling, then both curves (which have seen the same fraction of off-policy samples) should perform identical. The fact that they do not shows that there must be another effect at work here. This could be the exploration, but the experiments do not allow to distinguish these potential effects. To be fair, the reviewer also does not know how to distinguish exploration and on-policy-ness with the given setup, but another setup might be able to (e.g. with intrinsic-reward methods where the exploration can be separated from who acts). The presented analysis is strong enough for publication (a new setup is not needed), but these ambiguities must be discussed!\n\n3. While the reviewer liked most experiments (e.g. Figure 3 and 4 are great!), Figure 5 seems to test DQN with a very small replay buffer, where even the aggregated ensemble does not perform as well as Double DQN. This makes some conclusions suspect. For example, does the aggregated performance improves when more layers are shared? Or does the ensemble become indistinguishable from the baseline? If the authors would have picked a larger replay buffer (e.g. 4M), the performance would improve in the first and worsen in the second case. This would have an impact, as it shows that sharing layers in the ensemble reduces the positive effect it can have (e.g. by exploration).\n\n4. The main hypothesis of the paper is that \"the curse of diversity\" hurts individual member performance due the large number of off-policy samples. This makes sense, as an individual member can make 90% of all decisions correctly and still have poor performance, whereas a majority vote of the ensemble will agree with high certainty on the correct action. Ultimately this is the reason why BootDQN explores well. However, CERL seems to have almost as much diversity as the L=0 ensemble. CERL is a representation learning algorithm, which seems to be at odds with the main hypothesis. Contrast this with BootDQN which switches ensemble members mid-episode. If this algorithm would improve individual performance, the reviewer would accept it as a fix of the identified problem. With CERL presented as the \"solution\", the reviewer wonders: is the underlying problem is indeed \"diversity\"? Please discuss why representation learning fixes the off-policy-ness of the data (there are some attempts, but it is still vague). \n\nIn summary: there is a lot to love about the paper, but the authors need to reformulate some critical points (or convince the reviewer that those are not critical). Only under the condition that this happens, this reviewer recommends acceptance.  \n\n**Minor comments**\n\n- The figure captions should contain more information about the shown algorithms (ensemble size, replay buffer size, L, ...) to make them more self-contained for cross-reading.  \n- Ensembles are primarily used because they are good out-of-distribution detectors. An experiment measuring this would have been very welcome."
                },
                "questions": {
                    "value": "- If one accepts the idea that ensembles are primarily for exploration, then the diversity of responses is actually a feature, not a bug. Given that few wrong decisions per episode can significantly reduce the performance of an individual ensemble member, why do the authors actually consider the poor individual performance an issue, in particular when the aggregate performance is better than the baseline (e.g. on Mujoco)?\n- How did the baseline (Double DQN and vanilla SAC) explore? If the baseline worked so well, doesn't this imply exploration is not very important in the tested environments? How would the analysis look like if it would concentrate on hard-exploration environments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5475/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679621381,
            "cdate": 1698679621381,
            "tmdate": 1699636558438,
            "mdate": 1699636558438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QUZD0IjJUx",
                "forum": "M3QXCOTTk4",
                "replyto": "TPcH3Eyss4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for the insightful comments and detailed feedback! These comments lead us to think more deeply about our results and are very helpful for enhancing the quality of our work. Please see our response below:\n\n## Response to \u201cWeaknesses\u201d\n\n> **In Weaknesses 1**: This is a problem in statements like \"the two cases[, DQN(indiv.) and Double DQN,] have access to the same amount of data and have the same network capacity\" (p.3), as the change in exploration means that the two algorithms are based on different data distribution and will therefore behave differently, irrespective of the on- and off-policy-ness of the data. This potential interaction between exploration (which part of the state-space is covered) and on-policy-ness (which member got to sample it) cannot be distinguished with the presented experiments, but the text reads as if the latter is the only obvious conclusion (see below).\n\n> **In Weaknesses 2**: \u2026The fact that they do not show that there must be another effect at work here. This could be the exploration, but the experiments do not allow us to distinguish these potential effects\n\nThank you for raising the point regarding exploration/coverage. We have included a detailed discussion on this in the \u201cRemark on data coverage\u201d paragraph at the end of Section 3.2. The main points are summarized as follows:\n\n1. State-action space coverage is *purely a property of the data distribution*, but off-policy-ness depends on *both the data distribution and the policies*. So indeed they are different.\n2. Our p%-tandem experiment disentangles these two aspects: the active and passive agents are trained on the same data (thus same coverage), but experience different degrees of \u201coff-policy-ness\u201d.\n3. As a result, our experiment indicates that \u201coff-policy-ness\u201d is detrimental and sufficient to cause the performance degradation we see. At the same time, we recognize that it *does not* imply whether wider state-action space coverage is beneficial (more likely and intuitive) or harmful.\n4. On the other hand, as you mentioned, the fact that BootDQN (indiv.) performs better than the passive agent indicates that wider coverage is *likely* highly beneficial. Reasoning: \n   1. Due to the presence of $N$ different policies, the individual agents in BootDQN *likely* suffer from more severe off-policy-ness than the passive agent.\n   2. Therefore, if the wider coverage in BootDQN was not beneficial, due to more severe off-policy-ness BootDQN (indiv.) should have performed significantly worse than the passive agent.\n5. However, unlike the comparison between the active and passive agents which is perfectly controlled, the comparison between BootDQN (indiv.) and the passive agent may have other (unexpected) confounders. Therefore we avoid making conclusive claims about the effect of state-action space coverage in our paper.\n\nWe have also added \u201ceven though they are trained on different data distributions and thus are expected to behave differently,\u201d before the statement \u201cthe agents in these two cases have access to the same amount of data and have the same network capacity\u201d in Section 3.1 to emphasize that the data distributions in these two cases are different.\n\n> **In Weaknesses 1**: the authors claim that \"Surprisingly, simply aggregating the learned policies at test-time provides a huge performance boost in many environments\" (p.3), and proceed to downplay the effect of exploration in favor of an explanation based on \"aggregation\"\n\nTo avoid misinterpretation, we have revised the second paragraph in Section 3.1 to emphasize that even though we show that majority voting plays a more important role than previously attributed, it does not indicate that exploration/state-action space coverage is not important or beneficial. \n\n> **In Weaknesses 2**: \"These results confirm our hypothesis regarding the cause of the observed performance degradation\" (p.5). This is dangerous abductive reasoning: experimental evidence that is consistent with your hypothesis does not make your hypothesis true (one can only falsify a hypothesis).\n\nWe agree that there could be other factors at play (e.g., state-action space coverage, as discussed), even though we do believe that our results provide strong evidence that off-policy-ness plays a major role in the observed performance degradation. We have changed our statement to \u201cThese results offer strong evidence of a connection between the off-policy learning challenges in ensemble-based exploration and the observed performance degradation\u201d as a more accurate conclusion of our results."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5475/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350187424,
                "cdate": 1700350187424,
                "tmdate": 1700350187424,
                "mdate": 1700350187424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mB3twsVRLy",
                "forum": "M3QXCOTTk4",
                "replyto": "TPcH3Eyss4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (cont.)"
                    },
                    "comment": {
                        "value": "> **In Weaknesses 4**: Contrast this with BootDQN which switches ensemble members mid-episode. If this algorithm would improve individual performance, the reviewer would accept it as a fix of the identified problem.\n\n We have tested this variant In Figure 17 of Appendix D.3.4 . Specificially, we switch ensemble members at each step instead of only at the start of an episode. As shown in the result it does not mitigate the curse of diversity. This is not surprising because it does not change the proportion of \u201cself-generated\u201d data for each agent in the replay buffer. \n\n\n## Response to \u201cMinor comments\u201d\n\n>  The figure captions should contain more information about the shown algorithms (ensemble size, replay buffer size, L, ...) to make them more self-contained for cross-reading.\n\nThanks for your suggestion. We have included these in the captions in our revision.\n\n> Ensembles are primarily used because they are good out-of-distribution detectors. An experiment measuring this would have been very welcome.\n\nThank you for your suggestion regarding OOD detection. We agree it's an important aspect. However, we intentionally focused on the off-policy learning aspects of ensemble-based exploration for this paper, and incorporating OOD detection might go beyond our current scope. We look forward to exploring this in future research.\n\n## Response to \u201cQuestions\u201d\n\n> If one accepts the idea that ensembles are primarily for exploration, then the diversity of responses is actually a feature, not a bug. Given that few wrong decisions per episode can significantly reduce the performance of an individual ensemble member, why do the authors actually consider the poor individual performance an issue, in particular when the aggregate performance is better than the baseline (e.g. on Mujoco)?\n\nIt is widely accepted that a diverse ensemble can potentially result in better exploration and a more robust aggregate policy. The point of our work is not to deny these advantages, but to point out that despite these benefits, diversity also has a negative side effect. **The fact that its advantages (e.g., majority voting) sometimes outweigh its disadvantages (off-policy-ness) does not mean that the disadvantages do not exist or are not an issue.** This has practical significance. For example, without realizing the negative effect of diversity, one might think that having more ensemble members is always better (if we ignore computational costs), which is not the case as we have shown in Figure 5. The cause of this non-monotonic behavior is obvious from the individual agents\u2019 performance but not from the aggregate policy\u2019s performance. Also, knowing the cause of the curse of diversity allows targeted solutions that may give rise to better algorithms. CERL is just a first step in this direction. Finally, as mentioned in the second paragraph of Section 3.1, there are some scenarios such as hyperparameter sweep with ensembles where we do care about the performance of the individual ensemble members.\n\n> How did the baseline (Double DQN and vanilla SAC) explore? If the baseline worked so well, doesn't this imply exploration is not very important in the tested environments? How would the analysis look like if it would concentrate on hard-exploration environments?\n\nDouble DQN uses epsilon-greedy and vanilla SAC just uses stochastic policy + entropy regularization. These are very simple baselines and the performance on the tested environment is far from saturated, so the results by no means imply exploration is not very important in the tested environments. Note that the suite of 55 Atari games we tested contains several games that are traditionally considered hard exploration problems (e.g., Montezuma\u2019s Revenge. For a complete list see Table 1 in [3]). As shown in Figure 2 (top right) and Figure 10 of our paper, Bootstrapped DQN does not show clear advantages over Double DQN in most of these games. In fact, in the original Bootstrapped DQN paper, the author also finds that \u201cOn some challenging Atari games where deep exploration is conjectured to be important our results are not entirely successful, but still promising\u201d and shows that the improvements could be highly unstable (e.g., on Montezuma\u2019s revenge). This is not surprising as these hard exploration games often require special events that are very unlikely with primitive exploration techniques such as epsilon-greedy or ensemble.\n\nWe hope our response has addressed your concerns. Please let us know if you have any further questions!\n\n## References\n\n[1] Ostrovski, Georg et al. \u201cThe Difficulty of Passive Learning in Deep Reinforcement Learning.\u201d Neural Information Processing Systems (2021).\n\n[2] Fujimoto, Scott et al. \u201cOff-Policy Deep Reinforcement Learning without Exploration.\u201d International Conference on Machine Learning (2018).\n\n[3] Bellemare, Marc G. et al. \u201cUnifying Count-Based Exploration and Intrinsic Motivation.\u201d Neural Information Processing Systems (2016)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5475/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350342428,
                "cdate": 1700350342428,
                "tmdate": 1700351917277,
                "mdate": 1700351917277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tRDjIH7tTz",
                "forum": "M3QXCOTTk4",
                "replyto": "mB3twsVRLy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5475/Reviewer_wgEM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5475/Reviewer_wgEM"
                ],
                "content": {
                    "title": {
                        "value": "I vote for accept"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their exhaustive answer and the changes to the paper. In particular Figure 18 shows exactly what I expected: increasing the shared layers removes the off-policy-ness of the data (because the ensemble members become indistinguishable), but also loose the beneficial effect of the ensemble, in all likelihood exploration. The other changes are also welcome and address most of my criticisms.\n\nIn conclusion, I vote to accept the paper in its current state."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5475/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389572971,
                "cdate": 1700389572971,
                "tmdate": 1700389572971,
                "mdate": 1700389572971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k6sAfHsOM0",
            "forum": "M3QXCOTTk4",
            "replyto": "M3QXCOTTk4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5475/Reviewer_5Qz1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5475/Reviewer_5Qz1"
            ],
            "content": {
                "summary": {
                    "value": "This paper is a primarily empirical paper investigating what the authors term \"the curse of diversity\". This phenomenon is where\nindividual members of a data-sharing ensemble underperform relative to their single agent counterparts due the high proportion of off-policy data relative to each individual ensemble member and the agents' inability to learn effectively from such off-policy data.\n\nThe authors first demonstrate in the bootstrapped DQN setting (a common ensemble Q-learning method) that individual ensemble members perform poorly. Then, the authors run experiments to show the underperformance of an individual learner when there is a low self-generated to \"other-generated\" ratio in the replay buffer. This experiment, along with some reasoning, leads to the conclusion that the curse of diversity indeed exists.\n\nThey then investigate whether the curse of diversity can be ameliorated through larger replay buffers. The results are mixed, suggesting larger replay buffers may sometimes mitigate the curse, though largely do not. They then investigate ways to reduce the curse of diversity by directly reducing diversity in the ensemble, but find that it can adversely impact the underlying ensemble method.\n\nThey then introduce Cross-Ensemble Representation Learning (CERL), as a mechanism to mitigate the curse while preserving the benefits of the underlying ensemble method. CERL works by adding auxiliary tasks to ensemble members, having them learn value functions of other ensemble members and find that CERL mitigates the curse of diversity without adversely impacting performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Presentation - The paper is extremely clear and excellently written. The problem, motivation, and experiments are articulated very clearly. I like that they look introspectively at their own experiments and reason clearly about what can be inferred/concluded from their experiments without making unreasonable intellectual leaps.\n\nContributions:\n1. They show that perhaps the aggregation/majority voting aspect of ensembling methods may contribute to improved performance more than previously attributed. Previously, the intuition/hypothesis was more along the lines of ensembling methods inducing diversity in the data, which leads to higher-quality value estimates.\n2. They demonstrate that the curse of diversity exists, at least for ensemble Double DQN methods.\n3. They look at some natural ways to mitigate the curse of diversity and analyze their effectiveness.\n3. They start the steps towards mitigating the curse of diversity\n\nExperimentation: The experiments are reasonably thought out, and cover a wide range of environments (Atari/Mujoco)."
                },
                "weaknesses": {
                    "value": "I do not have any many major qualms with the paper, but I'll list a few thoughts.\n\nPerhaps this is out of scope of the paper, but I do feel it's difficult to draw conclusions about deep RL more broadly without investigating distributional RL. For example, this paper (https://openreview.net/pdf?id=ryeUg0VFwr) shows that distributional RL will likely do better with this off-policy data. It would be interesting to investigate the extent of this phenomena in the distributional setting. \n\nCERL does seem to scale poorly with the ensemble size, since if the ensemble size is N, there are N x N Q-functions. In practice, this does not seem like a major problem.\n\nFor the bar plots in Figure 2, it might be helpful to plot the top plot minus the bottom plot to highlight the point."
                },
                "questions": {
                    "value": "Question: Did you try different levels of passivity? Other than 10%?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5475/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5475/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5475/Reviewer_5Qz1"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5475/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698917086759,
            "cdate": 1698917086759,
            "tmdate": 1699636558354,
            "mdate": 1699636558354,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ey2HE8eWEI",
                "forum": "M3QXCOTTk4",
                "replyto": "k6sAfHsOM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5475/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for appreciating our contributions and providing valuable feedback! Please see our response below:\n\n> Perhaps this is out of scope of the paper, but I do feel it's difficult to draw conclusions about deep RL more broadly without investigating distributional RL\n\nThis is a great suggestion and we have added experiments on four Atari games with QR-DQN. The results are presented in Figure 16 of Appendix D.3.3. The significant performance gap between QR-DQN and Bootstrapped QR-DQN (indiv.) shows that the curse of diversity is also present in distributional RL. Note that due to limited time, we are not able to finish the full 200M frames of training. We will include the full results when the experiments are finished.\n\n> CERL does seem to scale poorly with the ensemble size, since if the ensemble size is N, there are N x N Q-functions. In practice, this does not seem like a major problem.\n\nIn practice, CERL scales very well with ensemble size. As mentioned in the \u201cMethod\u201d paragraph in Section 4, to implement CERL we only need to increase the output dimension of the *last linear layer* of the network (note this is the same architectural modification involved in distributional RL). So even though CERL requires N x N Q-functions as opposed to N Q-functions in a standard ensemble, the increase in the number of parameters and wall clock time is very small. For example, adding CERL to Bootstrapped DQN (L=0, N=10) increases the number of parameters by no more than 5% and the increase in wall clock time is barely noticeable. Even if we use N=20 which is already much larger than commonly used in the literature, applying CERL increases the number of parameters by no more than 11% and the increase in wall clock time is less than 5%. \n\n> For the bar plots in Figure 2, it might be helpful to plot the top plot minus the bottom plot to highlight the point.\n\nThanks for your suggestion! We have included this figure in Figure 12 of Appendix D.2.2. We do not include it in the main text to save space for other changes, and also because this plot mainly shows the benefits of majority voting which is not the primary focus of our work.\n\n> Question: Did you try different levels of passivity? Other than 10%?\n\nWe have also tried 0%, 5%, and 20% in four Atari games. The results are shown in Figure 19 of Appendix D.3.6. As expected, as the degree of passivity reduces (i.e., larger $p$), the active/passive agents\u2019 performance gap also reduces.\n\nPlease let us know if you have any further questions or suggestions!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5475/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700349808269,
                "cdate": 1700349808269,
                "tmdate": 1700349808269,
                "mdate": 1700349808269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fJ2mz88Yiq",
                "forum": "M3QXCOTTk4",
                "replyto": "Ey2HE8eWEI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5475/Reviewer_5Qz1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5475/Reviewer_5Qz1"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response to Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed response and for addressing my comments."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5475/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700386736881,
                "cdate": 1700386736881,
                "tmdate": 1700386736881,
                "mdate": 1700386736881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]