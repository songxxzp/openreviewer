[
    {
        "title": "Towards Fair Knowledge Distillation using Student Feedback"
    },
    {
        "review": {
            "id": "5f2hIL139x",
            "forum": "MIuimtOu0T",
            "replyto": "MIuimtOu0T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4352/Reviewer_QvRs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4352/Reviewer_QvRs"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets on the fairness problem in knowledge distillation. The proposed method, BIRD, collects the feedback from the student model through a meta-learning-based approach and selectively distill teacher knowledge. BIRD is orthogonal with existing methods and computationally effective. Extensive experiment results show that BIRD can enhance the fairness remarkably."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is overall well-written and easy to follow. \n2. This paper targets on an interesting problem, the fairness in knowledge distillation. \n3. The proposed method is orthogonal with existing methods, enhancing the fairness remarkably."
                },
                "weaknesses": {
                    "value": "1. I am curious about the results when we apply BIRD to existing methods, and test their performance according to the conventional criteria? eg. The Top-1 and Top-5 accuracy on CIFAR-100 and ImageNet. Will it degrade when pursuing fairness? I want to see more comparison results."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Reviewer_QvRs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4352/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698131230526,
            "cdate": 1698131230526,
            "tmdate": 1699636407675,
            "mdate": 1699636407675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OJclb00ZCW",
                "forum": "MIuimtOu0T",
                "replyto": "5f2hIL139x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response to Reviewer QvRs"
                    },
                    "comment": {
                        "value": "We are very excited to learn that the reviewer finds our work well motivated/presented, our method novel as compared to existing works, and our results remarkably enhancing fairness. We appreciate your helpful feedback and provide further details to answer your question below.\n\n**Including more conventional metrics**\n\nWe thank the reviewer for this valuable suggestion and agree that the inclusion of the aforementioned criteria will account for a more thorough representation of the accuracy results in our work. We would like to note that the number of classes in our benchmark datasets is 2 (for CelebA) and 4 (for UTKFace), respectively. Following the reviewers\u2019 feedback, we report the top-1 accuracies for the experiments in Table 1 and Table 2 in the Appendix of the updated manuscript (**see Table 12-13**). We are excited to share that our results follow the same trend and BIRD improves the fairness of the student model without sacrificing its predictive performance. Further, as explained below we cannot calculate metrics like top-5 accuracy in our experiments due to the limited number of classes in the fairness datasets.\n\nWe have included this clarification in the revised manuscript and hope we have addressed all your questions adequately. In light of these clarifications, we kindly request you consider increasing your score. We are happy to answer any further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4352/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242273802,
                "cdate": 1700242273802,
                "tmdate": 1700242273802,
                "mdate": 1700242273802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lFqd34t429",
            "forum": "MIuimtOu0T",
            "replyto": "MIuimtOu0T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4352/Reviewer_qpK3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4352/Reviewer_qpK3"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces a \"student-aware selective feature distillation\" approach, drawing inspiration from meta-learning, which enables the teacher to impart unbiased information effectively to the student."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors have lucidly articulated both the problem definition and the corresponding solution formulation.\n\n2. Distinguishing itself from prior research, this study emphasizes adapting the teacher's predictions to remain unbiased. This adjustment, in turn, facilitates the training of a student that naturally inherits this unbiased character due to the teacher's modified features.\n\n3. The introduction of a meta-learning inspired transfer approach is both well-conceived and aptly presented, offering a compelling solution for unbiased (or fair) knowledge transfer.\n\n4. In the results section, the authors comprehensively address pivotal concerns, including the problem's justification (5.2.1), the efficacy of their proposed method (5.2.2), the technique's adaptability across various KD frameworks (5.2.3), and insightful ablation studies that dissect various components of their framework (5.2.4 and 5.2.5)."
                },
                "weaknesses": {
                    "value": "1. While the paper's overarching framework appears to draw heavily from the \"learning how to teach\" paradigm (as detailed in Park et al., 2021; Liu et al., 2021; Zhou et al., 2021), its overall contribution may be perceived as somewhat incremental. This perception arises from the adaptation of a pre-established framework to address knowledge distillation. Despite this, the proposed solution stands out due to its technical novelty and apt alignment with the problem statement. A deeper dive by the authors into the technical distinctions between their work and prior studies would further solidify their contribution.\n\n2. For the benefit of practitioners, the authors might consider expanding their ablation section to detail not just the memory overhead, but also the time overhead associated with the distillation process. This is particularly relevant given the well-documented time-intensive nature of meta-learning-based approaches."
                },
                "questions": {
                    "value": "Can the authors detail how the CLIP-Resnet-50 to ResNet 18 KD is performed. How is the KD distillation performed in the absence of the class-probability distribution of CLIP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Reviewer_qpK3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4352/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718391012,
            "cdate": 1698718391012,
            "tmdate": 1699636407566,
            "mdate": 1699636407566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bX4eneHkCL",
                "forum": "MIuimtOu0T",
                "replyto": "lFqd34t429",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response to Reviewer qpK3"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the proposed framework as compelling and our results comprehensive. We greatly appreciate your feedback on solidifying the technical contributions of our work with the related works and extending our ablation studies. \n\n**Learning how to teach paradigm and technical differences with related works**\n\nWe thank the reviewer for recognizing the *technical novelty* and *alignment* with the problem statement. While we agree with the reviewer that the referenced works utilize some variant of learning how to teach framework, they focus on fine-tuning/re-training the teacher model, **which is not the goal of our work** as we assume black-box access of the teacher and only use its output representations. Below we provide a deeper dive into the technical distinctions between the referenced works and Bird.\n\n*Liu Et.al 2021: Learning to teach with student feedback*\n\nThe paper proposes IKD to establish a bidirectional interaction between the teacher and the student models. **Unlike traditional KD methods, where the teacher model is fixed** and provides static soft targets for the student model, IKD enables the teacher model to adapt its teaching strategy based on feedback from the student. Precisely, the **teacher model is trained to generate specific soft targets at each training step using meta-learning**, especially MAML (Finn, Abbeel, and Levine 2017). In contrast, our proposed framework, Bird, does not modify the weights of the original teacher model.\n\n*Park Et. al 2021: Learning Student-Friendly Teacher Networks for Knowledge Distillation (SFTN)*\n\nTheir goal is to learn networks that would serve as better teachers to the students. To achieve this, they add **proxy student branches to the teacher model** during its training and then **optimize the teacher**, such that it minimizes the representation difference between the teacher and student branches. The trained teacher is consequently used for knowledge distillation to standard student models using Vanilla KD (Hinton et. al). Their overarching principle is to learn a model that is better teacher in general and not optimize Knowledge Transfer with respect to a specific student.  Again, they modify the teacher model and its learned weights, **which is non-trivial and computationally expensive for state-of-the-art foundation models.**\n\n*Zhou et al 2022: BERT Learns to Teach: Knowledge Distillation with Meta-Learning*\n\nThey introduce MetaDistil, which combines knowledge distillation with meta-learning. Unlike traditional knowledge distillation frameworks where the teacher model is fixed, MetaDistil allows the teacher model to be trainable and adjust based on the student model\u2019s performance using meta-learning (similar to Liu et al).\n\nOne key distinction between the above methods and Bird is that we do not fine-tune or modify the weights of the teacher model and only use the student feedback to re-weigh **penultimate** teacher representations during distillation. As rightly pointed out by the reviewer, our approach is aligned and motivated but addresses an unexplored area of fairness in knowledge distillation. While existing works utilize the learning how-to-teach paradigm, **they do so primarily for optimizing the students' predictive performance**, which often comes at the cost of retraining the entire teacher model.\n\n**Time Ablation**\n\nWe thank the reviewer for this insightful question and agree that performing a time overhead study in addition to the memory overhead will be very helpful. In response to the reviewer\u2019s suggestion, we perform this and share the time taken per iteration by BIRD (*note that this number is computed by averaging the time for all iterations in an epoch*). As rightly pointed out by the reviewer, *meta-learning is usually time-intensive.* However, we would like to highlight that even then BIRD achieves competitive time per iteration in addition to the fairness performance gains it offers. Our analysis indicates the time complexity can be attributed to the library used for meta-learning (*higher library*), which can be further optimized and remains a part of our future work (as discussed in the limitation).\n\n|     **Teacher --> Student**     |   **MFD**   | **Base KD** |   **BIRD**  |\n|:-------------------------------:|:-----------:|:-----------:|:-----------:|\n| CLIP-Vit32 --> CLIP-Vit32       | 0.1192 s/it | 0.0986 s/it | 3.1507 s/it |\n| CLIP-ResNet50 --> CLIP-ResNet50 | 0.0985 s/it | 0.0762 s/it | 3.1124 s/it |\n| CLIP-Vit32 --> ResNet18         | 0.2585 s/it | 0.1346 s/it | 4.5481 s/it |\n| CLIP-ResNet50 --> ResNet18      | 0.2321 s/it | 0.1193 s/it | 4.3356 s/it |\n\nWe are very grateful to the reviewer for all their suggestions, which have helped us in differentiating our work from existing works. We tried our best to address all the questions in our response. In light of these updates, we kindly request the reviewer to consider increasing the score. We are very happy to answer any further questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4352/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242203267,
                "cdate": 1700242203267,
                "tmdate": 1700242203267,
                "mdate": 1700242203267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GTbylmdE0G",
            "forum": "MIuimtOu0T",
            "replyto": "MIuimtOu0T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4352/Reviewer_UAGV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4352/Reviewer_UAGV"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates making student models with less gender & racial bias in knowledge distillation. The meta-learning framework is used to achieve this goal, and the experiments are done on CelebA and UTK datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The motivation of this paper is sound, fairness in KD is indeed an important topic in the era of large models. This paper is well-presented."
                },
                "weaknesses": {
                    "value": "1. This paper proposes a meta-learning framework to solve the fairness issue in KD. I don't see why meta-learning can be used to resolve fairness here; the intuition and motivation are unclear. \n\n\n2. The experiments are mainly conducted on CelebA and UTKFace, two small datasets. I think it is necessary to evaluate a larger benchmark, such as a dataset that a mixup of gender/racial images with other non-biased images, i.e., animals,  to verify the effectiveness of the proposed method on a larger scale setting. The same issue is on the choice of student models; more models should be evaluated, such as ViT (not as a teacher but as a student model).\n\n3. It is disappointing that this paper is only focused on the fairness of classification. The application of classification is very limited and has been extensively explored over the past decades. I think it is essential to test KD fairness on more settings, such as multi-turn vqa. Otherwise, I suggest changing the title to a more specific topic, i.e. \"Towards Fair Knowledge Distillation on Image Classification\"."
                },
                "questions": {
                    "value": "See weakness. My main concerns are the following: 1) it is not clear why the meta-learning framework is effective for **fairness** (not the overall results.), 2) the experiments are insufficient regarding the size of the dataset, the model size of the students, and the tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4352/Reviewer_UAGV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4352/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733145891,
            "cdate": 1698733145891,
            "tmdate": 1699636407219,
            "mdate": 1699636407219,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sSxlZqkuoI",
                "forum": "MIuimtOu0T",
                "replyto": "GTbylmdE0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response to Reviewer UAGV (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the motivation and presentation of our work. We appreciate your helpful feedback and address all the mentioned questions/concerns below.\n\n**The intuition behind a Meta-Learning framework**\n\nWe would first like to thank the reviewer for appreciating the problem of fairness in knowledge distillation (KD). Further, we would like to highlight that none of the existing works in KD literature **adapt incoming biased teacher knowledge based on student feedback**, which can **lead to a fairer student model**. Here, the intuition is to identify and select teacher features that do not enhance the bias in the student model. We utilize a meta-learning framework as it provides **structure to the process of learning from feedback** and helps better optimization. This means, that when the incoming features from the teacher model are biased, it gets that feedback through the fairness properties of an inner-learner (a proxy/copy student model; Line 7: Algorithm 1), and it is subsequently updated (Line 8: Algorithm 1). The updated teacher model is then used to optimize the main student fairly. Thus, the teacher first uses the proxy student to understand the student\u2019s optimization process, which it leverages for fair teaching.  \n\nFurther, we conducted an ablation on the importance of the meta-step (research question 4 in Sec. 5.2 of the original and revised draft) in our BIRD framework and showed that the meta-step allows us to update the parameters of FairDistill such that the biased features from the teacher do not impact the student. More specifically, results show that Bird with meta-gradients shows a **6.45% improvement in the fairness** over Bird without meta-gradients, providing empirical evidence that the meta-gradients improve the fairness of the student model.\n\n**Evaluation of More Student Models**\n\nWe thank the reviewer for this experiment suggestion. Indeed an evaluation with more student model architecture choices would provide a deeper understanding of our approach. Further, we would like to point out to the reviewer that **Table 4** in our attached supplementary section includes the results of BIRD with students of different sizes: ResNet34, ShufflenetV2, and FLAVA. In addition, we perform the additional experiments suggested by the reviewer choosing CLIP/ViT-32 and CLIP/RN-50 as students and RN18, and RN34 as teachers on the CelebA dataset. Please find the results for the aforementioned ablation in **Table 11** of the updated draft. We note that BIRD comfortably outperforms all baselines by a large margin, proving its efficacy across model sizes.\n\n**Larger Datasets**\n\nThe two main datasets \u2013 UTKFace and CelebA \u2013 we utilize in this work are very popular and are widely employed in fairness research to date. For instance, several recent works in fairness published at **NeurIPS, AAAI, ICCV, CVPR, ECCV, and UAI conferences in 2022-23** have employed these datasets both to evaluate the efficacy of newly proposed methods, as well as to study the behavior of existing methods [1-10]. In addition, previous work in knowledge distillation, like MFD [1] and FWD [2], also rely on these datasets. Given these past works, we follow suit and employ these datasets in our benchmarking efforts. We agree with the reviewer that the necessity of evaluating models on a larger benchmark, such as a dataset with a mixup of images with different attributes, to verify the effectiveness of fairness methods is certainly important, and we aim to explore them in our follow-up work as they become available."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4352/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241498900,
                "cdate": 1700241498900,
                "tmdate": 1700241707134,
                "mdate": 1700241707134,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ap1mqxYEbm",
            "forum": "MIuimtOu0T",
            "replyto": "MIuimtOu0T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4352/Reviewer_qaWv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4352/Reviewer_qaWv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new fairness-aware KD method, BIRD (BIas-awaRe Distillation), for distilling foundation models. The main idea is to use a proposed FAIRDISTILL operator to collect feedback from the student through a meta-learning-based approach and selectively distill teacher knowledge.  This method can be used with several existing base KD methods for improved performance. Extensive experiments across three fairness datasets show the efficacy of the proposed method over other counterparts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. As FMs prevail day by day, distilling FMs is more important as well. The fairness problem of the distilled model is also of interest. This paper contributes to this axis.\n\n2. The idea of selecting part of the teacher's feature for debiased distillation under the meta learning framework is technically sound and intuitive.\n\n3. Empirically, the method is effective (\"Results show that BIRD improves the fairness of the knowledge distillation framework by 40.91%\") and it is ready to be used along with existing KD methods to enhance the fairness of the distilled student."
                },
                "weaknesses": {
                    "value": "1. One major problem with the proposed method is that it needs the internal features of the teacher model for distillation (Eq. 4). However, as the paper mentioned in the motivation, most of the FMs in the real world only provide APIs. I.e., their features are barely accessible. This limitation seems to undermine the practical value of the proposed method severely.\n\n2. Another concern is that the fairness is improved but in many cases at a price of degraded accuracy. E.g., CLIP- ResNet50 with UTKFace in Tab. 1\uff0c CLIP-ViT-32 \u2212\u2192ResNet18 with UTKFace in Tab. 2, CLIP-R50 \u2212\u2192ResNet18 with UTKFace in Tab. 2. Namely, the proposed method is not very strong. The fairness issue may be a concern, while accuracy also matters. \n\nA side concern is that, as seen above, the method does not perform well on the UTKFace dataset. Why? \n\n3. Presentation: Some of the results are mistakenly highlighted. In Tab. 1, \u2206mean-DEO, the highlighted results are sometimes not the best, which are quite confusing. \n\n4. Minor issues.\n- This paper seems quite relevant: https://aclanthology.org/2022.gebnlp-1.27.pdf. It reports a similar observation to Sec. 5.2 that KD amplifies biases."
                },
                "questions": {
                    "value": "NAN"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4352/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698898963188,
            "cdate": 1698898963188,
            "tmdate": 1699636406493,
            "mdate": 1699636406493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sc37CUqr8p",
                "forum": "MIuimtOu0T",
                "replyto": "ap1mqxYEbm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response to Reviewer qaWv (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your helpful feedback and for acknowledging our framework as intuitive and sound. We appreciate that you found our experiments extensive and method effective. Below, we address all your questions/concerns.\n\n**Practical Value of BIRD (Need for internal features)**\n\nWe would like to clarify a potential misunderstanding here. Most cloud service provider (e.g., OpenAI, Google, and Amazon) pre-train a general-purpose feature extractor (called an encoder) and deploys it as a cloud service called an **Encoder As a Service (EAS)** [1,2,3,4], where a client/end-users queries the cloud service APIs for the final layer feature embeddings of its training/testing inputs when training/testing a downstream classifier. For safety and security-critical applications, a client aims to build an accurate and fair downstream classifier to safeguard algorithmic decisions and promote fairness. Further, Knowledge Distillation of Foundational models is usually performed at two ends: 1) developers who train student models and have access to everything about the model and 2) end-users who use the foundation model as a service (EAS).\n\nFurther, we would like to clarify that **we do not use the internal feature representations of the foundation models** in Eqn. 4. Following the premise of EAS, BIRD **only utilizes the final layer representation of the models** and trains a single one-dimensional feature vector to re-weigh the teacher representations using FairDistill operator and improve the fairness of the distilled student model. Hence, using our proposed framework, an end-user can leverage the foundational model representations (from APIs) and use BIRD to distill its knowledge in a student network they are building to develop an accurate and fairer model.\n\n**Importance of Fairness and its Tradeoff with Predictive Performance**\n\nStudying fairness and its trade-off with predictive performance is imperative within the context of AI regulations and AI executive orders passed throughout the world. While accuracy is crucial, addressing fairness concerns is equally vital to prevent discriminatory outcomes. AI regulations like **the General Data Protection Regulation (GDPR) in the EU, the Personal Information Protection and Electronic Documents Act (PIPEDA) in Canada, the AI Executive Order in the USA, and the Data Protection Act (DPA) in the UK**, increasingly emphasize the importance of fairness, demanding that AI researchers and developers strike a balance between accurate predictions and equitable treatment of individuals. Executive orders underscore the need for transparency and accountability in AI systems, urging a thorough examination of how fairness considerations may impact predictive performance. Focusing solely on accuracy without regard for fairness can lead to biased outcomes, legal repercussions, and erosion of public trust in AI technologies, emphasizing the urgency of a nuanced understanding and integration of fairness principles into AI development practices.\n\n**Performance on UTKFace Dataset**\n\nWe thank the reviewer for bringing up this insightful observation. We would like to report that BIRD, on average, **improves** fairness by **58.78%** (DEO-max) and **7.31%** (DEO-mean), while the F1 score **decreases** by **1.27%** and AUROC decreases by **6.34%** on the UTKFace dataset (see Table 1-2) compared to the baseline. Our results show that BIRD improves model fairness without sacrificing its predictive performance. \n\nFurther, UTKFace is a small dataset that contains only 20,000 facial images with four ethnicities as opposed to the 200,000 images in the CelebA dataset. We hypothesize that one possible reason for the relatively poor performance of BIRD  on the UTKFace dataset could be the limited training dataset size. To this end, in response to the reviewers\u2019 feedback, we perform an additional experiment and run BIRD on the UTKFace dataset with image augmentations (randomly flip the image horizontally, apply color jitter (using a $\\sigma=0.2$), randomly rotate the image by 15 degrees and apply the random affine transformation). On average, we observe an increase of **+0.36% in AUROC** and **+1.17% in F1-score** for approximately the same fairness performance. Another notable observation is the massive performance boost when UTK augmentation is used with Clip-RN50 teacher \u2013 **+0.51% in AUROC** and **+1.53% in F1-score**, **-6.40% in DEO-mean**, and **-11.86% in DEO-max**. We added the exact ablation numbers in the appendix of the updated manuscript (see Table 10)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4352/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241301290,
                "cdate": 1700241301290,
                "tmdate": 1700241350915,
                "mdate": 1700241350915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w6KTVnyg0F",
                "forum": "MIuimtOu0T",
                "replyto": "ap1mqxYEbm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response to Reviewer qaWv (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Mistakenly highlighted results in Table 1**\n\nThank you for pointing this out. We apologize for the confusion and have made the appropriate changes in the updated version of the draft.\n\n**Relevant related work**\n\nThank you for sharing this work. We have cited and discussed the work in the revised related work section in our updated draft.\n\nWe are very grateful to the reviewer for all their suggestions, as they have helped us improve our paper significantly. We tried to incorporate all the reviewer suggestions in our write-up and include all clarifications in the revised version. We would kindly request the reviewer to consider increasing their score.\n\n**References**\n\n[1] Qu, W., Jia, J., & Gong, N. Z. REaaS: Enabling Adversarially Robust Downstream Classifiers via Robust Encoder as a Service. arXiv, 2023.\n\n[2] https://clarifai.com/clarifai/main/models/travel-embedding\n\n[3] https://clarifai.com/clarifai/main/models/face-identification-transfer-learn \n\n[4] https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/image-retrieval"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4352/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241337356,
                "cdate": 1700241337356,
                "tmdate": 1700241358774,
                "mdate": 1700241358774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NOiQI7yAoG",
                "forum": "MIuimtOu0T",
                "replyto": "ap1mqxYEbm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4352/Reviewer_qaWv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4352/Reviewer_qaWv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback!"
                    },
                    "comment": {
                        "value": "1. The concern regarding the need for internal features is well resolved. Thanks for the clarification!\n\n2. Meanwhile, about the accuracy vs. fairness tradeoff. I agree that fairness is very important, but it cannot explain why it must cost predictive performance in this work.\n\n2.1 After reading the comments of other reviewers (e.g., QvRs), I also have the concern about using the traditional metric for evaluating predictive performance. If we look at Tab. 12, BIRD underperforms many other methods in terms of Top1 accuracy (and F-1 score), such as BKD and FitNet. I do not understand why the authors still claim their method \"retains the predictive power (Top1-Acc, F1-score)\".\n\n2.2 Tab. 10. As seen, using Aug generally improves the predictive performance indeed, while in quite many cases (more than half of the presented results) Aug worsens the fairness metrics. So again, we see a tradeoff between predictive performance and fairness. I do not think the results can justify the claim in the caption \"*BIRD while keeping the fairness criterion same as w/o Augmentation.*\"\n\nAnother problem with this table is, the results of the other methods are missing. We need comparisons to see if the method is really effective *against the counterparts*.\n\n----\nA suggestion in rebuttal: If you revise the paper, it is highly suggested to **use some color to highlight the revised part**."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4352/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663787598,
                "cdate": 1700663787598,
                "tmdate": 1700674126385,
                "mdate": 1700674126385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pXGw7GXhxY",
                "forum": "MIuimtOu0T",
                "replyto": "ap1mqxYEbm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4352/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "We are glad to hear that we were able to clarify your concerns regarding the use of internal features. Below, we address your remaining concerns.\n\n**Concern about using the traditional metric for evaluating predictive performance**\n\nWe are excited to hear that the reviewer looked into our responses for other reviewers. In the light of using traditional metrics, we would like to clarify a potential misunderstanding. Classical computer vision benchmark datasets for image classification like CIFAR-10, CIFAR-100, and ImageNet used evaluation metrics like top-k accuracy (with k={1, 5} used in most papers) because these are balanced datasets, where each class had an equal number of images. However, fairness benchmark datasets like UTKFace and CelebA used in fairness work in computer vision are not well-curated and imbalanced datasets. Hence, it is common to use more robust evaluation metrics, such as AUROC and F1-score, when comparing predictive performance on these datasets.\n\n**Fairness vs Accuracy tradeoff in BIRD in Table 12**\n\nThank you for the great point! In response to the reviewers' feedback, we analyzed the results in Table 12 and found that on average across two datasets, two teacher-student pairs, and three knowledge distillation baselines (MFD, BKD, and FitNet), BIRD achieves a significant improvement in fairness (**41.71% in DEO-mean and 32.13% in DEO-max**) and only takes a small hit in the predictive performance (**-1.67% in Top-1 accuracy and -2.37% in F1-score**). Thank you for highlighting the misleading statement and we are happy to revise our statement. We intended to say that BIRD demonstrates that its predictive performance is closer to baseline methods and deem this as a new frontier in exploring the fairness-accuracy tradeoff, where we take a small hit in the accuracy and achieve $\\approx$**25**$\\times$ higher fairness performance.\n\n|                             | **BIRD** | **MFD** | **FitNet** | **BKD** |  **AD** |\n|-----------------------------|:--------:|:-------:|:----------:|:-------:|:-------:|\n| **Top-1 ($\\uparrow$)**      |   77.68  |  77.28 |   79.88   | 79.88 | 68.45 |\n| **F1-Score ($\\uparrow$)**   |   77.44  | 77.22 |   80.86  |  79.98 | 67.94 |\n| **DEO-mean ($\\downarrow$)** |  11.63 | 19.32 |   19.92   |  20.66 | 21.32 |\n| **DEO-max ($\\downarrow$)**  |  20.97 |  30.86 |    30.19   |   31.7  |  35.25  |\n\n**Comparisons to see if the method is really effective against the counterparts in Table 10**\n\nWe thank you for looking into these results in detail. We would like to clarify that we ran the augmentation experiment in response to your comment during the rebuttal phase and finished the BIRD experiments by the time of our initial rebuttal response. We have started running new experiments on computing the predictive and fairness performance of baseline methods and comparing them to the augmentation results of Table 10 and would like to request an additional 4-5 hours as we expect the baseline results to be complete by then.\n\n**A suggestion in rebuttal: If you revise the paper, it is highly suggested to use some color to highlight the revised part**\n\nThank you for your suggestion. We marked the changes in blue in the main tex of the revised manuscript and, in response to the reviewer's suggestion, we have marked the caption of the revised tables of the appendix in blue too in the updated draft of the manuscript.\n\nPlease let us know if you have any further questions or concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4352/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671426218,
                "cdate": 1700671426218,
                "tmdate": 1700677328856,
                "mdate": 1700677328856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xVV5WBb9eI",
                "forum": "MIuimtOu0T",
                "replyto": "pXGw7GXhxY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4352/Reviewer_qaWv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4352/Reviewer_qaWv"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion"
                    },
                    "comment": {
                        "value": "(1) The authors state:\n\n* *\u201cBIRD achieves a significant improvement in fairness (+41.71% in DEO-mean and +32.13% in DEO-max) and only takes a small hit in the predictive performance (-1.67% in Top-1 accuracy and -2.37% in F1-score). \"*\n* *\"where we take a small hit in the accuracy and achieve ~25x higher fairness performance.\"*\n\nClarification needed - How do you get these numbers: \"41.71%, 32.13%, -1.67%, -2.37%, 25x\". These are compared against which method?\n\n(2) The presented new table in the response above - Are they new results? I do not find it in the paper and supplementary. What are they supposed to show?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4352/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674965574,
                "cdate": 1700674965574,
                "tmdate": 1700674965574,
                "mdate": 1700674965574,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]