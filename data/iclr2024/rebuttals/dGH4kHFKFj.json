[
    {
        "title": "GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models"
    },
    {
        "review": {
            "id": "Ae3vGKf8ii",
            "forum": "dGH4kHFKFj",
            "replyto": "dGH4kHFKFj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2815/Reviewer_sJwu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2815/Reviewer_sJwu"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes joint shape matching (JSM), an unsupervised training schema based on three steps: the training of a generative model for implicit shapes; a template fitting to the shape generated by interpolation in the latent space; a final refinement provided by a chamfer fitting. The method is tested on humans and animals on relatively small datasets, and shows promising performance w.r.t. previous methods, comparable to ones that use implicit information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method is unsupervised and extrinsic but provides good performance, competitive with intrinsic approaches that uses the surface as regularization \n- The method seems novel, and the idea of enhancing the learning representation by interpolation in the latent space seems straightforward and reasonable"
                },
                "weaknesses": {
                    "value": "- The central part of the method is not easy to grasp. I find the overall principle clear, and it is probably possible to replicate the work in principle. Still, the technique is not well explained in detail, and the notation is often confusing. For example, the letter \"g\" is used both for the shape generator and for the mesh generator, and requires some back-and-forth to get used to it. I suggest revisiting the method explanation and clarifying the methodological details.\n- By my understanding, the method highly relies on the target shapes belonging to a given distribution that should not only bounded in terms of structure and class but in particular, should be possible to express by the registered template. Many methods rely on template registration, and it is not a weakness per se, but the proposed approach makes use of techniques that aim to be general and flexible (e.g., implicit representation, unsupervised learning), but the paper does not show any out-of-distribution results (and also, results only on shapes for which data are available and can be even generated synthetically). I also believe that the topological constraint given by the template limits the generality of the method. The limitations barely mention this, and it should be emphasized more.\n- Experiments are performed on a relatively small set of data. The datasets are outdated and do not deal with the literature's more recent and real challenges (e.g., partiality, noise, clutter, ...). In this sense, I suggest stressing the method further; I believe that they would be interesting to investigate the latent interpolation when the learned space represents more diverse shapes (i.e., I wonder if relying on implicit representation may help in the context of limited topological variation during the interpolation, or if the relying on a template do not let to generalize to these cases)"
                },
                "questions": {
                    "value": "Following the Weaknesses above:\n1) How would linear interpolation behave in the presence of significant diverse geometrical topology data? Is the template triangulation the main limitation in this case?\n2) Could you provide some measure of the computational cost of the method? I think would be important to understand the scalability of the method, and I assume that the method would require a significant computational effort. How does it compare with NeuroMorph?\n3) Would it be possible to test the method on a class for which a dense correspondence is not provided; for example, some classes of ShapeNet (e.g., cars, airplanes)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697577768995,
            "cdate": 1697577768995,
            "tmdate": 1699636224619,
            "mdate": 1699636224619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P9DPRsieuJ",
                "forum": "dGH4kHFKFj",
                "replyto": "Ae3vGKf8ii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer sJwu"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions. Below are our clarifications for your concerns. We value your input and would love to hear any follow-up you have to the response.\n\n> **Q: Confusing notation.**\n\nIn the original submission, the implicit generator uses lowercase letters $g^{\\phi}$. The explicit generator uses bold letters $\\mathbf{g}^{\\theta}$. In the revised manuscript, we have made changes to the notations to make them clearer:\n\n- $f^{\\phi}$ denotes the implicit generator, where $\\phi$ are the network parameters.\n\n- $f^{\\phi}(\\mathbf{x}, \\mathbf{z})=0$ denotes the implicit surface (i.e. the iso-surface where the signed distance value is 0). \n\n- $\\mathbf{g}^{\\phi}(\\mathbf{z})$ denotes the discretized mesh of the implicit surface $f^{\\phi}(\\mathbf{x}, \\mathbf{z})=0$. \n\n- $h^{\\psi}$ denotes the encoder in the VAE.\n\n- $\\mathbf{m}^{\\theta}$ denotes the mesh generator in the third stage.\n\nWe have also provided the explanations in the revised main paper. \n\n> **Q: Data is generated synthetically and does not deal with the real challenges (e.g., partiality, noise, clutter, \u2026)**\n\nWe have included additional experiments in the supplementary material, submitted during the rebuttal period, to demonstrate that our method is also applicable to real scans. We trained the implicit network on a real human dataset featuring various clothes and hairstyles. The experiments indicate that our method can still provide smooth interpolation even when the learned space represents a more diverse range of shapes. Please refer to the newly submitted supplementary material for more details. Additionally, we want to emphasize that the Human dataset we evaluated, which comes from SMPL, is sufficiently challenge for inter-shape matching. \n\n> **Q: The topological constraint given by the template limits the generality of the method.**\n\nThe template is introduced for the explicit mesh generator. In fact, any mesh generator in the literature requires a template. \nThe template is not used in the implicit generator, which is a key contribution of this paper. The implicit generator can already provide pair-wise correspondences between pairs of implicit surfaces, which are not constrained by topology. Specifically, given any two shapes, we can perform latent space interpolation between them and propagate the correspondences along the interpolation path using equation (7) in the main paper. This framework is applicable to any shape collection, regardless of its topology. These correspondences can be evaluated through the interpolation results shown in the paper; smoother and more rigid interpolations indicate better correspondences. In this paper, we utilize the template solely for the task of joint shape matching of deformable shapes, as they typically share similar topology and structure. \n\n> **Q: How would linear interpolation behave in the presence of significant diverse geometrical topology data? Is template triangulation the main limitation in this case?**\n\nIn this paper, we focus on deformable shapes (humans and animals), which usually do not have significant topological changes. For other types of shape collections with significantly diverse geometrical topologies, we leave this for future research. Note that we perform linear interpolation for the implicit surfaces, where template triangulation is not involved. \n\n> **Q: Would it be possible to test the method on a class for which a dense correspondence is not provided; for example, some classes of ShapeNet (e.g., cars, airplanes)?**\n\nWe propose a general framework for computing correspondences between implicit surfaces. We are encouraged by your recognition of our framework's potential applicability to a wide range of shape collections, including man-made shapes. For man-made shapes, we can adopt alternative energies, such as as-affine-as-possible (AAAP) energy, which enforces local region transformations to be affine rather than rigid, in place of ACAP energy. However, we want to underscore that deformations among man-made shapes are more complex than deformable shape collections. Also there are structural variations, where AAAP should be enforced among shapes or parts that are structurally similar. Those issues should be addressed by other papers (we are currently looking into this). \n\n> **Q: Computational cost of the method compared with NeuroMorph.**\n\nSimilar to SALD, we train our model with 8 NVIDIA TITAN V GPUs (12G). For the Human collections (about 1000 shapes), training the implicit generator takes about 3 days, while the explicit generator requires approximately 12 hours. For the FAUST dataset (80 shapes), NeuroMorph is trained with 1 GPU for roughly 3 days. Note that NeuroMorph requires a pair of shapes as input, so the training complexity is $O(n^2)$ for $n$ training shapes. In contrast, our method directly learns the shape space, meaning the complexity scales linearly with the number of training shapes."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536907131,
                "cdate": 1700536907131,
                "tmdate": 1700536907131,
                "mdate": 1700536907131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mMRSpdhNXA",
                "forum": "dGH4kHFKFj",
                "replyto": "P9DPRsieuJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2815/Reviewer_sJwu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2815/Reviewer_sJwu"
                ],
                "content": {
                    "title": {
                        "value": "Post Rebuttal Reply"
                    },
                    "comment": {
                        "value": "I thank the Authors for their effort in answering my concerns. The additional experiments address some of my concerns, and the answers are mainly satisfactory. I do not have further questions, and I am willing to raise my score.\nI am looking forward to hearing from other reviewers their opinion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672767849,
                "cdate": 1700672767849,
                "tmdate": 1700672767849,
                "mdate": 1700672767849,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1FdjZepMxS",
            "forum": "dGH4kHFKFj",
            "replyto": "dGH4kHFKFj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2815/Reviewer_pFHc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2815/Reviewer_pFHc"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for computing correspondences between target shapes from a collection of shapes. Namely, the method first learns an implicit shape generator from the input collection, enforcing several useful shape-preserving constraints. It then evaluates the learned implicit generator to interpolate between target shapes, and uses the interpolation to guide the learning of the explicit shape generator. After a final refinement step, the target shapes are corresponded with high quality and appropriate in-between interpolations. The main shortcoming of the approach is the need for a substantially large collection of shapes to begin with."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The high level idea is simple and sound: learn a shape generator with useful geometric priors, then apply its interpolation capabilities to guide explicit correspondence. The results are convincing, with rigid limbs bending appropriately between corresponded shapes, and the ablation study is informative."
                },
                "weaknesses": {
                    "value": "The paper is hard to follow, both in text and in math, which makes it difficult to clearly understand and implement the presented ideas. A few rounds of editing should help the flow. Much of the mathematical detail (especially in section 4.1) should be relegated to the appendices (with full derivations and descriptions), and only clearly readable final formulas should be shown in the main text (I'd carefully pick which symbols represent which concepts).\n\nThe demonstrated use case is pretty contrived -- all collections come from well parameterized datasets. I would love to see results using 3D body scans with various clothes, hairstyles, etc.\n\nThe human shape collection seems to include a multitude of body types, and is clearly able to correspond/interpolate between them. The animal collections seem to only have shapes of the same animal. The paper would be stronger if you show an interpolation between say a lion and a horse. It is not clear whether this method would blend well between very different quadruped shapes.\n\nA more suitable ARAP reference should be: As-rigid-as-possible shape interpolation by Alexa et al. 2000"
                },
                "questions": {
                    "value": "The main title in the PDF seems to be misspelled: \"CenCorres\" instead of \"GenCorres\".\n\nIn general, there are many symbols used in the mathematical notation, which need to be clearly mentioned and described.\n  * What are the different fonts for g (looks like implicit expressions use lowercase letters, while explicit use bold letters)\n  * What is Theta in g^Theta in the \"Problem statement\" paragraph (page 3)?\n  * What is Phi in \"Approach overview\" (page 3) and what does it map from and to? Why R^3 x Z? Does it compute the distance field value at the specific 3D coordinate for the shape defined by a latent code?\n  * What is Psi in h^Psi in equation (1)? Also mention lambdas for equations (1) and (2).\n  * Symbol x is introduced in section 4.1 without describing what it is.\n  * What does d stand for in equation 3, some displacement between a vertex and an infinitesimally close corresponding vertex? Is d of unit length, then scaled down by epsilon? Or should d be the small displacement that doesn't need to be further multiplied by epsilon?\n  * At some point we get exposed to several capital letter symbols (C, F, G, E) whose meaning is hard to follow.\n\nTable 2 caption should mention that the metric is mean and median geodesic distance between correspondences.\n\nAcronyms should be spelled out the first usage.\n  * First mention of ACAP is not accompanied by description and reference, maybe just remove it from section 3 until it's properly introduced in section 4.\n  * MP-pseudo inverse should just be Moore-Penrose inverse."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2815/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2815/Reviewer_pFHc",
                        "ICLR.cc/2024/Conference/Submission2815/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697734381540,
            "cdate": 1697734381540,
            "tmdate": 1700656826339,
            "mdate": 1700656826339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C86A1JnJDG",
                "forum": "dGH4kHFKFj",
                "replyto": "1FdjZepMxS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer pFHc"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions. Below are our clarifications for your concerns. We value your input and are keen to discuss any further questions or comments you may have following our response.\n\n> **Q: Math notations and what are the different fonts for g?**\n\nIn the original submission, the implicit generator uses lowercase letters because the signed distance is a scalar. The explicit generator uses bold letters because the output vertices are vectors. They can also be distinguished by the network parameters: $\\phi$ for the implicit generator and $\\theta$ for the explicit generator. We used g because both of them are generative models. In the revised manuscript, we have made changes to the notations to make them clearer:\n\n- $f^{\\phi}$ denotes the implicit generator, where $\\phi$ are the network parameters. Yes, the reason for $\\mathbb{R}^3 \\times \\mathcal{Z}$ is that it computes the signed distance value at the specific 3D coordinate for the shape defined by a latent code.\n\n- $f^{\\phi}(\\mathbf{x}, \\mathbf{z})=0$ denotes the implicit surface (i.e. the iso-surface where the signed distance value is 0). $\\mathbf{x}$ is the 3D query location and $\\mathbf{z}$ is the latent code.\n\n- $\\mathbf{g}^{\\phi}(\\mathbf{z})$ denotes the discretized mesh of the implicit surface $f^{\\phi}(\\mathbf{x}, \\mathbf{z})=0$.\n\n- $h^{\\psi}$ denotes the encoder in the VAE, where $\\psi$ are the network parameters.\n\n- $\\mathbf{m}^{\\theta}$ denotes the mesh generator in the third stage, where $\\theta$ are the network parameters.\n\nWe have also provided these explanations in the revised main paper.\n\n> **Q: What does d stand for in equation 3, some displacement between a vertex and an infinitesimally close corresponding vertex?**\n\nYes, $\\mathbf{d}_i^{\\mathbf{v}}(\\mathbf{z}) $  in equation (3) denotes the displacement between a vertex and an infinitesimally close corresponding vertex. In the original submission, we further multiplied it by $\\epsilon$ because this simplified equation (6), (7) and (8) by avoiding the constant $\\epsilon$. However, you are correct that this is not necessary. In the revised submission, we have followed your suggestions and did not scale it by $\\epsilon$. The final expressions of the regularization terms remain the same.\n\n> **Q: Explanation of capital letter symbols (C, F, G, E).**\n\nWe provide detailed expressions of $C$ and $F$ in the appendices of the main paper. $C$ and $F$ are derived from the matrix representation of Equation (3).\n\nThe expression of $G$ is given in equation (7). It is just a component of $\\mathbf{d}_i^{\\mathbf{v}}(\\mathbf{z})$ that is independent of the perturbation direction $\\mathbf{v}$.\n\n$E$ is derived in equation (8) and serves as a metric to measure the local rigidity and local conformality distortions between an implicit surface and its perturbation. The expression of $E$ can also be found in equation (8) and, like $G$, is independent of the perturbation direction $\\mathbf{v}$.\n\n> **Q: Results using 3D body scans with various clothes, hairstyles, etc.**\n\nWe have included additional qualitative results in the supplementary material submitted during the rebuttal period. We trained the implicit network on a new human dataset featuring various clothes and hairstyles. Our proposed regularizations again consistently improve the baseline network, SALD. Please refer to the newly submitted supplementary material for more details.\n\n> **Q: Interpolation between a lion and a horse.**\n\nWe have included more qualitative results in the supplementary material submitted during the rebuttal period. We trained the implicit network on a new quadruped shape dataset. Visualization of the interpolation results show that our method transitions smoothly between a lion and a horse. Please refer to the newly submitted supplementary material for more details.\n\n> **Q: Citation, acronyms and typos.**\n\nWe have added the new ARAP reference and corrected the acronyms and typos in the title."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536596116,
                "cdate": 1700536596116,
                "tmdate": 1700536596116,
                "mdate": 1700536596116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yBkU4DBbDm",
                "forum": "dGH4kHFKFj",
                "replyto": "C86A1JnJDG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2815/Reviewer_pFHc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2815/Reviewer_pFHc"
                ],
                "content": {
                    "title": {
                        "value": "Raising the rating"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal and the additional results. As as result I am happy to raise my rating. A few more comments:\n\n\nYou are missing a closed parentheses in Equations 4 and 5 just before the L2-norm closing.\n\nIt would be even more impactful if you blend between one pose of the lion and a very different pose of the horse (e.g. rearing).\n\nSince correspondence is a key output of your method, you could enhance the visualization by texture-mapping the initial mesh (e.g. with a color-coded grid) and showing where that texture map moves to in the corresponded final mesh."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656799277,
                "cdate": 1700656799277,
                "tmdate": 1700656799277,
                "mdate": 1700656799277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bOgFdQRmfu",
            "forum": "dGH4kHFKFj",
            "replyto": "dGH4kHFKFj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2815/Reviewer_MqJa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2815/Reviewer_MqJa"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents GenCorres, a method to solve the Joint Shape Matching (JSM) problem for a collection of unorganized shapes.  It is based on fitting the input shapes with a mesh generator that is constrained to preserve the local structure (conformal or isometric) of the shapes. Particularly, GenCorres first learns an implicit generator from the input shapes using a VA, which produces intermediate shapes between arbitrary pairs. The paper introduces a novel approach for computing correspondences between adjacent implicit surfaces, which is used to regularize the implicit generator. Second, synthetic shapes generated by the implicit generator guide the initial fittings (template-based deformations) for learning the mesh generator. Experimental results demonstrate that GenCorres outperforms state-of-the-art JSM techniques in collections of articulated body shapes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "GenCorres is a well-crafted method divided into three stages. Stage 1 learns an implicit generator from unorganized sets using a VAE, where local (ACAP) and cycle consistency is imposed between surfaces that are close along the embedding dimension, defining the geometric deformation and cycle-consistency regularization losses. Stage 2 learns the explicit mesh generator from the VAE encoder-decoder. Stage 3 refines and fits the mesh generator to the input set of meshes and enforces again local structure consistency with ACAP. The three stages are well constructed and contain novel contributions to this field, especially the regularization losses in Stage 1. The results in the paper reveal that the GenCorres' generative method produces high-quality meshes and the correspondences across the input set significantly improve the state-of-the-art.\nIn terms of the quality, the methodology is well described and motivated. The results offer enough evidence that GenCorres improves the state-of-the-art of JSM in collections of shapes describing articulated body shapes. The article also includes an ablation study to assess the importance of each individual step.\nIn terms of significance, this method offers an interesting solution to a difficult and open problem that has important applications in computer graphics."
                },
                "weaknesses": {
                    "value": "The main weakness of GenCorres, which is revealed by the experiments and commented by the authors, is the need of a relatively large set of input shapes to learn the shape generators properly. Solving this issue is a difficult task that requires further research.\n\nGenCorres seems to be especially suitable for a particular type of shapes (articulated body or animal shapes) to which ACAP and ARAP represent good deformation constraints. Other shape collections, such as man made objects, probably represent a challenge for this method."
                },
                "questions": {
                    "value": "How well does GenCorres perform with man-made shapes or other shapes that do not correspond to articulated objects? \n\nWhat is the influence of the hyperparameters (lambda and epsilon) in the final result? Are the values specified in the paper valid for other sets of input shapes?\n\nIf the number of input shapes is a critical factor, I suggest the authors to include an experiment to establish the limit number for which the method significantly degrades."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761745794,
            "cdate": 1698761745794,
            "tmdate": 1699636224467,
            "mdate": 1699636224467,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tGGRbQNWtM",
                "forum": "dGH4kHFKFj",
                "replyto": "bOgFdQRmfu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer MqJa"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and valuable feedback. Below are our clarifications for your concerns.\n\n> **Q: The need of a relatively large set of input shapes.**\n\nOur approach follows the stream of approaches that leverage big data (e.g. nearest neighbors and graph-based learning) to solve traditional problems. The goal is to open a new direction that has many problems to work on. For example, the approach we developed to train the implicit shape generators that preserve inter-shape deformations can be extended in another setting. As another example, training an explicit shape generator to solve the inter-shape correspondence problem can stimulate future research in this area.\n\n> **Q: How well does GenCorres perform with man-made shapes or other shapes that do not correspond to articulated objects?**\n\nWe propose a general framework for computing correspondences between implicit surfaces. Specifically, we analyze the changes of the generated shape under the infinitesimal perturbation of the latent code and formulate the correspondence computation as a linearly constrained quadratic program problem (refer to Equation (6) in the main paper). We are encouraged by your recognition of our framework's potential applicability to a wide range of shape collections, including man-made shapes. For man-made shapes, the key question pertains to identifying a suitable deformation model, as inter shape deformations among man-made shapes are notably complex. One possibility is that instead of using ACAP energy, we can adopt alternative energies, such as as-affine-as-possible (AAAP) energy, which enforces local region transformation to be affine rather than rigid. Also, using L1 norm rather than the L2 norm in this setting maybe more appropriate. Another challenge is to address structural variations and only enforce AAAP among structurally similar shapes. All these questions deserve other papers to study (we are looking into this as well). \n\n> **Q: What is the influence of the hyperparameters (lambda and epsilon) in the final result? Are the values specified in the paper valid for other sets of input shapes?**\n\nWe did not perform an extensive grid search for the optimal hyperparameters. However, we found our methods to be quite robust against variations in $\\lambda_{geo}$, $\\lambda_{cyc}$ and $\\epsilon$. From our experiments, when $\\lambda_{geo}$ and $\\lambda_{cyc}$ are set very large, the data term increases, leading to an over-smoothing of the learned shape due to the regularization term enforcing greater similarity between shapes. We set $\\epsilon$ to a small value in finite differences to achieve better gradient approximation. These hyperparameters are valid for both Human and Animal categories.\n\n> **Q: The limit number for which the method significantly degrades.**\n\nFor the pair-wise shape matching experiments on FAUST, we conducted ablation studies where we gradually decreased the number of additional DFAUST shapes. The results of these studies are presented in the table below.\n\nEvaluations of pair-wise matching on FAUST (in cm).\n\n| number of DFAUST shapes | 1000 | 500 | 250 |\n|:-----------------------:|:----:|:---:|:---:|\n| error                   | 1.6  | 2.5 | 4.1 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536250674,
                "cdate": 1700536250674,
                "tmdate": 1700536250674,
                "mdate": 1700536250674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BMqBAN9iqT",
                "forum": "dGH4kHFKFj",
                "replyto": "tGGRbQNWtM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2815/Reviewer_MqJa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2815/Reviewer_MqJa"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "The authors have made considerable efforts to improve the paper and have satisfactorily answered my questions. Looking at the rest of the reviewer's concerns, I support accepting this paper and am willing to keep or raise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732319358,
                "cdate": 1700732319358,
                "tmdate": 1700732319358,
                "mdate": 1700732319358,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zThhv63Oob",
            "forum": "dGH4kHFKFj",
            "replyto": "dGH4kHFKFj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2815/Reviewer_WbPs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2815/Reviewer_WbPs"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new algorithm for joint matching of a set of 3D shapes represented as polygon meshes or surface point clouds. The core idea is to learn a generative model for meshes, based on deformations of a base template, that can produce the input shapes and thereby infer correspondences between them. To achieve this in a robust and accurate way, the authors propose several innovations, including learning an initial implicit generator that informs the eventual mesh generator, and techniques to regularize the generator output by imposing distortion-minimizing/consistency-maximizing losses in epsilon-neighborhoods of the synthesized shape space (not just around the input landmarks)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I really like this paper. I think it addresses an important, general problem by bringing together a bunch of relevant ideas in a well-justified way: using generative models for discriminative problems, physically-inspired regularization, etc. The core insights are simple and compelling. The individual technical contributions are theoretically meaningful and elegant, well-presented, and show significant improvements over baselines in experiments. I particularly like the computation of optimal correspondence fields between similar implicit shapes in 4.1. And not being an expert in this precise area, I am impressed that this can be done both differentiably (is this where finite differences come in as mentioned in 4.4?) and fast (how long does the training take?)\n\nI am provisionally recommending acceptance and am open to revising my opinion further upwards (if the authors can address the critiques in Weaknesses) or downwards (if other reviewers find serious flaws)."
                },
                "weaknesses": {
                    "value": "This is a fairly complex system. I am not 100% certain that every design decision is fully justified since there are too many possible ablations (though the authors do study several obvious ones). In particular, the authors claim that the implicit approach is necessary since learning a mesh generator from scratch is too difficult and error-prone. While this may be true, it is not actually demonstrated within the ambit of the proposed pipeline. There is the 3D-CODED comparison, but that is an entirely different pipeline. I do understand that this would be an \"ablation\" that's at least half a research project by itself, but still... Maybe the authors have already done experiments to verify this which are not included in the paper?\n\nAlso, code (and preprocessed data) would be really helpful so that others can understand and verify the claimed contributions. Will it be released?\n\nA couple of relevant papers may be worth mentioning since they are in the overall spirit of this paper:\n\nMuralikrishnan et al., \"GLASS: Geometric Latent Augmentation for Shape Spaces\", CVPR 2022\n(learning generative models from sparse landmarks guided by ARAP regularization -- does require input with correspondences though so it's sort of complementary to this paper)\n\nBednarik et al., \"Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases\", ICCV 2021\n(using metric distortion energies for consistent reconstruction, and hence joint matching, of time-varying shape sequences)\n\nMinor:\np3: Chamber --> chamfer (spelling, and should be lowercase)"
                },
                "questions": {
                    "value": "Please see questions inline in Strengths and Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698952454644,
            "cdate": 1698952454644,
            "tmdate": 1699636224386,
            "mdate": 1699636224386,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rPN1l7B7Wz",
                "forum": "dGH4kHFKFj",
                "replyto": "zThhv63Oob",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2815/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer WbPs"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and valuable feedback. Below are our clarifications for your concerns.\n\n> **Q: This can be done both differentiably. (is this where finite differences come in as mentioned in 4.4?)**\n\nYes, both autograd and finite differences are differentiable.\n\n> **Q: How long does the training take?**\n\nSimilar to SALD, we train our model with 8 NVIDIA TITAN V GPUs (12G). For the Human collections, the training of the implicit generator takes about 3 days, while the explicit generator requires approximately 12 hours.\n\n> **Q: Ablation studies to validate the necessity of the implicit approach.**\n\nWe conducted ablation studies for the joint shape matching experiments on DFAUST. The results are shown in the table below. GenCorres-NoImp does not utilize the implicit approach (1st stage) and directly registers the template to each target shape as initialization. GenCorres-NoInit omits both the 1st and 2nd stages, using only the 3rd stage. The results show that GenCorres is superior to both GenCorres-NoImp and GenCorres-NoInit, thanks to better initialization. Without latent space interpolation to generate intermediate shapes, GenCorres-NoImp often fails to register the template directly to a vastly different target shape, leading to poor initialization. GenCorres-NoInit, which does not employ any correspondence initialization, tends to get stuck in undesired local minima. Note that the success of training 3D-CODED relies on a very large dataset (23000 human meshes with a large variety of realistic poses and body shapes). \n\nEvaluations of JSM on DFAUST using geodesic errors of the predicted correspondences (in cm).\n\n|                  | mean  | median |\n|:---------------- |:-----:|:------:|\n| GenCorres        | 1.30  | 1.13   |\n| GenCorres-NoImp  | 7.92  | 7.73   |\n| GenCorres-NoInit | 12.11 | 11.84  |\n\n> **Q: Will codes and preprocessed data be released?**\n\nIn the newly submitted supplementary material, we provide an additional experiment that applies the implicit generator to a new human dataset featuring various clothes and hairstyles. We have also included the corresponding code for this experiment along with a readme file. More code and data from the main paper will be released later.\n\n> **Q: Relevant papers and typos.**\n\nWe have added the relevant papers and corrected the typos."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536007430,
                "cdate": 1700536007430,
                "tmdate": 1700536007430,
                "mdate": 1700536007430,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Eesytf5Vp5",
                "forum": "dGH4kHFKFj",
                "replyto": "rPN1l7B7Wz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2815/Reviewer_WbPs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2815/Reviewer_WbPs"
                ],
                "content": {
                    "title": {
                        "value": "Support acceptance"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their responses to our questions. I supported acceptance earlier and continue to do so now (maybe with an even higher score), given that my not-super-critical concerns have been addressed, and given that other reviewers with more critical views appear to have had their concerns addressed as well."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708544547,
                "cdate": 1700708544547,
                "tmdate": 1700708544547,
                "mdate": 1700708544547,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]