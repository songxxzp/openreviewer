[
    {
        "title": "FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing"
    },
    {
        "review": {
            "id": "IuyTFVkuyl",
            "forum": "JgqftqZQZ7",
            "replyto": "JgqftqZQZ7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1423/Reviewer_Hpug"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1423/Reviewer_Hpug"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the text-to-video editing task. To improve the temporal consistency, they introduce dense spatio-temporal attention and flow-guided attention to acquire information from the whole video. The proposed method achieves good performance on the test videos."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed flow-guided attention is novel and effective.\n\n2. The proposed flow-guided attention can be applied to other base models to further improve the temporal consistency.\n\n3. Extensive experiments and ablation studies demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The comparison with previous works is very limited. For the video editing task, multiple images should be shown for qualitative comparison in the main paper (Figure 5), which is important to verify the temporal consistency. Furthermore, even in the supplementary material, only one example is compared with previous works, which is not convincing. I compared the ``wolf\u2019\u2019 example with the TokenFlow example on its website. The results of this paper are not good to me.\n\n2. From my perspective, the text information for editing is only acquired by cross attention with editing prompts. Other editing techniques like prompt-to-prompt are not used. This might cause the inaccurate editing of the background. For example, in the first example of Figure 5, the grass also turns yellow, while TokenFlow and FateZero can better keep the background.\n\n3. DSTA conducts cross attention across all 32 frames, which takes many computational resources. It is better to compare the computational cost and inference time with other methods.\n\n4. There are also some works based on optical flow trajectory for video generation [ref-1,ref-2,ref-3], which should also be discussed in the related work.\n\n[ref-1] Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory.\n\n[ref-2] Generative image dynamics.\n\n[ref-3] Motion-Conditioned Diffusion Model for Controllable Video Synthesis."
                },
                "questions": {
                    "value": "The paper is good, but I still have some concerns as in the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Reviewer_Hpug"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636639403,
            "cdate": 1698636639403,
            "tmdate": 1700963842371,
            "mdate": 1700963842371,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Usuu2JkNX",
                "forum": "JgqftqZQZ7",
                "replyto": "IuyTFVkuyl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "**Dear Reviewer Hpug**,\n\nWe appreciate your recognition of the novelty and effectiveness of our work! We will address your concerns as follows:\n\n**Q1: The comparison with previous works is very limited.**\n\nWe release more qualitative comparisons at the anonymous link: https://youtu.be/dkXcjVrp9x0. We cannot show many examples in Figure 5 due to space limitations. Further examples will be added in Appendix B in the revised manuscript. \n\nRegarding the \"wolf\" example, we notice that the source video we used (19 frames/15FPS) is different from that used in the concurrent work TokenFlow (40 frames/20FPS). Our source video has fewer frames and lower FPS, which may impact the viewing experience. When compared frame by frame, our results are also of high quality (per-frame comparison:https://i.imgur.com/F7FHgWA.jpg).\n\n**Q2: The text information for editing is only acquired by cross attention. Other editing techniques like prompt-to-prompt are not used. This might cause the inaccurate editing of the background. E.g., the grass color changed in the truck editing example.**\n\nOur task focuses on general video editing rather than object-level editing. The influence of editing prompts on undescribed regions (e.g., the background in the \u201ctruck\u201d example) is out of the scope of this work and should not be viewed as our weakness. We pursue semantic alignment and visual quality of the overall video instead of isolating edits to specific objects. We find background changes are beneficial as long as they contribute to the overall quality. For example, yellow grass serves as a more natural background than the original green grass for the editing target \"wooden truck\".\n\nWe believe that other image editing techniques, such as prompt-to-prompt, can also be beneficial for text-to-video editing. However, in this paper, our objective is to address the inconsistency issue in text-to-video editing, which is more pressing for improving the quality of generated videos. On the other hand, since we did not make any changes to cross-attention, our framework is compatible with editing techniques such as prompt-to-promp. Btw, in the \u201ctruck\u201d example, TokenFlow and FateZero failed to edit the trucks. Their output videos are quite similar to the source video and keep the background. These two works sometimes also change the background: \n\n \"cat \u279c red tiger\" by FateZero: https://i.imgur.com/mwRX3vd.gif. \n\nThe background is changed by TokenFlow: https://i.imgur.com/F7FHgWA.jpg\n\n**Q3: DSTA takes many computational resources. It is better to compare the inference time with other methods.**\n\nDSTA (dense spatio-temporal attention) is implemented using xFormers which can save a lot of computational resources. In fact, DSTA is also used in other works [1,2,3]. We have measured the inference time of different models for reference. Some models need finetuning and DDIM inversion before inference. The average time cost (for 32 frames) of different models is shown in the following table. Our model has a relatively short runtime in the sampling stage and there is scope for further improvement.\n\n|Model | Finetuning | DDIM Inversion | Sampling |\n|---|---|---|---|\n| Tune-A-Video  |11min15s | 3min52s  | 3min34s  |\n| Text2Video-Zero |  - | -  | 3min17s  | \n|  ControlVideo | -  |  - | 4min36s  |\n|  FateZero | -  |  4min56s  | 4min49s |\n|  TokenFlow | -  | 3min41s  | 3min29s |\n|  FLATTEN | -  |  3min52s  | 3min45s |\n\n[1] Controlvideo: Training-free controllable text-to-video generation.\n\n[2] Pix2Video: Video Editing using Image Diffusion\n\n[3] TokenFlow: Consistent Diffusion Features for Consistent Video Editing\n\n**Q4: There are also some works based on optical flow trajectory, which should also be discussed in the related work.**\n\nThanks a lot for noticing these amazing works! We will include [ref-1,ref-2,ref-3] in Section 2 of the revised manuscript to strengthen our storyline.\n\nYour comments will help us to improve the presentation and impact of our work, and we are grateful for the opportunity to address these concerns. If you have any new comments, please do not hesitate to let us know! We appreciate your consideration of our contributions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037191678,
                "cdate": 1700037191678,
                "tmdate": 1700216366657,
                "mdate": 1700216366657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7mnvEtnyBO",
                "forum": "JgqftqZQZ7",
                "replyto": "7Usuu2JkNX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1423/Reviewer_Hpug"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1423/Reviewer_Hpug"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors, \n\nThank you for the response and additional qualitative results.\n\nI hope to further clarify my concerns\uff1a\n\n- **The text information for editing is only acquired by cross attention.** \nMy concern is the textual alignment. The flow-guided attention has no influence on the textual alignment, which only relies on the cross-attention. Instead, techniques like prompt-to-prompt can emphasize the edited part. Therefore, the editing ability of purely cross-attention should be worse than such techniques. \nHowever, in the results shown in the paper and the video, this paper seems to have better textual editing ability than TokenFlow (which uses prompt-to-prompt), such as the truck example. I have no idea the reason for it. I hope the authors can directly compare their method with the examples shown in the TokenFlow website under the same setting.\n\n- **Computational Cost.** Another main computational cost is the VRAM. TokenFlow applies DSTA only in the key frames while Pix2Video and ControlVideo only apply cross-frame attention to two reference frames, which is much fewer than the whole video (32 frames). Therefore, I think the VRAM requirement is still high even when using xFormers. Note that not everyone has 80GB A100 and keeping an acceptable VRAM requirement is also important."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624183799,
                "cdate": 1700624183799,
                "tmdate": 1700624183799,
                "mdate": 1700624183799,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FHixBwB65R",
            "forum": "JgqftqZQZ7",
            "replyto": "JgqftqZQZ7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1423/Reviewer_F5Lm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1423/Reviewer_F5Lm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes text-guided video editing systems that considers optical flow to preserve temporal consitency\nIn detail the temporal attention is guided by the paths estimated from optical flow."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "[+] The idea makes sense that involves optical flow into the diffusion model for holding temporal consistency \n[+] Performances are enhanced compared to previous editing systems"
                },
                "weaknesses": {
                    "value": "[-] Optical flow can be effective when a single objective appears. However, we can easily come up with other cases including occlusion, objects' appearing or disappearing, or else. Therefore the method seems sensitive to the input video, which can ruin the attention or even worse than previous temporal attention methods. Can you explain why the method should be better than previous temporal attention?\n\n[-] The performances are better than many previous works. What are the samples that this method validates? I am quite curious about the videos that they evaluated.\n\n[-] Are there any qualitative or quantitative results about the trajectory patches in the aforementioned cases in question [1]? I want to see if the trajectory is truely following the flow of objectives in the video."
                },
                "questions": {
                    "value": "My questions are above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654275571,
            "cdate": 1698654275571,
            "tmdate": 1699636070763,
            "mdate": 1699636070763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BOXrqxGA8D",
                "forum": "JgqftqZQZ7",
                "replyto": "FHixBwB65R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "**Dear Reviewer F5Lm**,\n\nWe appreciate your recognition of our work in enhancing temporal consistency! We will address your concerns as follows:\n\n**Q1: We can come up with other cases including occlusion, object appearing or disappearing, or else. Therefore the method seems sensitive to the input video. Can you explain why the method should be better than previous temporal attention?**\n\nYour concern about sensitivity to input video is valid, and we acknowledge the complexity introduced by occlusion, object appearing or disappearing. In fact, our flow-guided attention is proposed to address these issues, where the previous temporal attention may be problematic. \n\nIn contrast to previous temporal attention techniques that lack explicit motion guidance, our approach utilizes optical flow information to guide attention. Note that when the occlusion, object appearing/disappearing, the corresponding patch trajectories are also created (e.g., appearing) or stopped (e.g., disappearing). In the previous temporal attention, the query patch attends to all other patches in the video and aggregates their features. The irrelevant patches can mislead the attention process. \n\nFor example, when an object disappears in the (t+1) frame, the object patch in the (t) frame still attends to the patches in the (t+1) frame. For our flow-guided attention, the trajectory of the object patch stops at the (t) frame and the object patch in the (t) frame doesn\u2019t attend to the patches in the (t+1) frame. The robustness of our flow-guided attention is derived from its ability to interpret complex motion patterns, addressing the limitations of previous temporal attention.\n\n**Q2: The performances are better than many previous works. What are the samples that this method validates? I am quite curious about the videos that they evaluated.**\n\nFor the quantitative study, we use the same evaluation set (LOVEU) as described in the main paper. The videos and prompts used for the quantitative results can be found at: https://sites.google.com/view/loveucvpr23/track4.  We also use some in-the-wild videos for qualitative results (e.g. the cute cat in Figure 1). \n\n**Q3: Are there any qualitative or quantitative results about the trajectory patches?** \n\nSure! The patch trajectories are predicted well even in complex scenarios. The qualitative results are shown as follows (Reviewer UKpY is also interested in).  The patches on the trajectories are marked with red dots. Note that our approach does not rely on any specific flow estimation module. We use the SOTA flow estimation module RAFT [1] in this work, and the trajectory could be more precise/robust with better flow models in the future.\n\nOcclusion: https://i.imgur.com/8PErmVI.gif\n\nAppearing: https://i.imgur.com/R22S7Ri.gif\n\nDisappearing: https://i.imgur.com/w3nRRwK.gif\n\nLarge motion: https://i.imgur.com/fmyPK9q.gif\n\n[1] Raft: Recurrent all-pairs field transforms for optical flow.\n\nYour comments will help us to improve the presentation and impact of our work, and we are grateful for the opportunity to address these concerns. If you have any new comments, please do not hesitate to let us know! We appreciate your consideration of our contributions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038948459,
                "cdate": 1700038948459,
                "tmdate": 1700038948459,
                "mdate": 1700038948459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "61Mo1KlEEa",
            "forum": "JgqftqZQZ7",
            "replyto": "JgqftqZQZ7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1423/Reviewer_1FeZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1423/Reviewer_1FeZ"
            ],
            "content": {
                "summary": {
                    "value": "To improve the visual consistency for text-to-video editing\uff0c FLATTEN is proposed to enforce the patches on the same flow path across different frames to attend to each other in the attention module. Experiment results on existing text-to-video editing benchmarks show that the proposed method achieves the new state-of-the-art performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed Flow-guided attention is intuitive and makes sense."
                },
                "weaknesses": {
                    "value": "1. The method is only suitable for scenarios where every pixel in the original video aligns spatially with the generated video. For misaligned areas, the optical flow trajectory of the original video is not appliable for the motion of the generated video, leading to incorrect key and query identifications. For instance, in the example of transforming a cat to a tiger in Figure 1, the tiger's face is larger than the cat and thus there exist pixels that belong to the tiger's face and belong to the background in the cat example. For the original video with the cat, these pixels belong to the background with an optical flow near zero. However, for the tiger, it's part of the face and should rotate with the head, requiring an optical flow describing a leftward movement. This sets too high a requirement for editing scenarios.\n\n2. If optical flow tracking is accurate enough, why not simply select a keyframe and then directly copy pixels following the same optical flow path to other frames? This approach seems more accurate than aligning through attention. For example, bilinear interpolation combined with optical flow is often used to predict the next frame in videos.\n\n3. In the provided MP4, there is only one visual comparison, which is too limited. It's suggested not to cherry-pick comparisons so that the effectiveness of the method can be judged intuitively."
                },
                "questions": {
                    "value": "See Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Reviewer_1FeZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737744023,
            "cdate": 1698737744023,
            "tmdate": 1699636070670,
            "mdate": 1699636070670,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T5i6sdsHtD",
                "forum": "JgqftqZQZ7",
                "replyto": "61Mo1KlEEa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "**Dear Reviewer 1FeZ**,\n\nWe appreciate your insightful thoughts on the application scenarios of our work! We will address your concerns as follows:\n\n**Q1: The method is only suitable for scenarios where every pixel in the original video aligns spatially with the generated video. E.g., in the example of transforming a cat to a tiger in Figure 1.**\n\nOur method intentionally avoids the requirement for pixel-level alignment between the original and generated videos. The optical flow is downsampled before being used as guidance (e.g. from 512 * 512 to 64 * 64) and the flow-guided attention works in low-resolution latent space. This improves the tolerance for misaligned areas between the original video and the generated video. In the example of \u201ccat to tiger\u201d, even though the tiger's face is larger than the original cat's. The pixels that belong to the tiger's face move naturally in the generated video and the tiger's face does not stick to the background.\n\nThe quantitative and qualitative results demonstrate that our model can output high-quality and highly consistent videos. The edited videos by FLATTEN have natural visual appearance and the same dynamic motion as the original videos. This allows our method to be credibly applied in many application scenarios including style transfer, coloring, texture editing, and shape editing (e.g., editing short videos on social media platforms). \n\nYour concern about misaligned areas is insightful! In fact, it is challenging not only for our method but also for all current video editing approaches. The other methods using optical flow [1,2] or structural information (e.g., edge/depth map) [3,4] from the original video also face this misalignment problem. Tune-A-Video [5] with finetuning has tried the large-shape editing but the original motion is lost in the result (e.g., https://i.imgur.com/NVT5QbY.gifv). For the task of video editing, the edited video should follow the motion/structure from the source video. Large misaligned areas are somehow in conflict with this task. This issue might be better solved using video generation techniques. \n\n[1] VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet\n\n[2] Tokenflow: Consistent diffusion features for consistent video editing.\n\n[3] Text2video-zero: Text-to-image diffusion models are zero-shot video generators.\n\n[4] Controlvideo: Training-free controllable text-to-video generation.\n\n[5] Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation.\n\n**Q2: If optical flow tracking is accurate enough, why not simply select a keyframe and then directly copy pixels (e.g. bilinear interpolation) following the same optical flow path to other frames?**\n\nWhile the idea of directly copying patches based on optical flow seems intuitive, our flow-guided attention in a latent space provides a more adaptive solution. First, selecting keyframes introduces additional complexity. The robustness of the model might be influenced by the effectiveness of the keyframe selection strategy. Moreover, even if a suited keyframe is successfully chosen, the patch embeddings are not easy to propagate in the latent space. In fact, we found that using bilinear interpolation for patch prediction often leads to high-frequency flickering, which introduces undesired artifacts, compromising the visual quality of the generated frames (see the highlighted region in the example at: https://i.imgur.com/OUIeHr9.png). In contrast, our flow-guided attention dynamically adjusts to complex scenes and provides a more robust and adaptive approach to frame alignment. We will stress this aspect in the revised manuscript. \n\n**Q3: In the provided MP4, there is only one visual comparison, which is too limited.**\n\nThe supplementary video is only a brief introduction and there is an upload limitation (<100MB). Therefore we only showed a challenging example with larger motion. We randomly sample more examples and release them at the anonymous link: https://youtu.be/dkXcjVrp9x0. Further examples will be added to Appendix B in the revised manuscript.\n\nYour comments will help us to improve the presentation and impact of our work, and we are grateful for the opportunity to address these concerns. If you have any new comments, please do not hesitate to let us know! We appreciate your consideration of our contributions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700044782849,
                "cdate": 1700044782849,
                "tmdate": 1700414098243,
                "mdate": 1700414098243,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kqi1UYWfGu",
            "forum": "JgqftqZQZ7",
            "replyto": "JgqftqZQZ7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1423/Reviewer_UKpY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1423/Reviewer_UKpY"
            ],
            "content": {
                "summary": {
                    "value": "Summary: The paper focuses on text guided video editing. Previous methods to tackle this problem extend the text-to-image U-net to the temporal dimension to implement spatiotemporal attention where patches from different frames attend to one another. The paper argues that such methods introduce irrelevant information, since they allow all patches in the video to attend to one another where in fact many of these spatiotemporal patch to spatiotemporal patch connections might be irrelevant. To address this problem, the paper suggests using optical flow to guide the attention. Specifically, a pre-trained optical flow network is used to estimate the flow field and tracks of patches along flow trajectories are aggregated to enforce only patches on the same trajectory to attend to one another in a second step of MHSA. This results in more visual-consistent videos as the paper demonstrate both qualitatively and qualitatively \n\nMethod: First, the \"standard\" the text to image U-net architecture is inflated to account for the temporal dimension and the image patch spatial self attention mechanism is replaced with spatiotemporal self-attention with all patches in the video used as tokens for Q,K,V. Secondly, a pre-trained optical flow network is employed to compute the flow field along the frames of the video. Tracks of patches (in the latent space) are aggregated using the downsampled flow field. Next, self attention is performed between patch-embeddings on the same track. Specifically, the queries are taken from the original dense spatiotemporal MHSA, but for every query associated with a specific patch - the keys and values in MHSA are only the ones which are associated with patches on the same track. Note that this method does not require re-training as it only refines the existing embedded spatiotemporal patch embedded tokens with additional information by applying MHSA again but with restrictions on which patches can attend to one another (where this \"restriction\" is derived from the flow field).\n\nExperiments: the paper compares the proposed method against 5 publicly available text-to-video editing methods on two standard benchmarks. The proposed method performs favourably both in terms of visual quality/alignment metrics  and visual consistency metrics. The paper also presents quantitative results as well as a user-study demonstrating the effectiveness of the method, particularly in the aspect of visual-consistency when motion is introduced."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is sound and original. The framework is very simple, does not require further training and can be easily plugged to various existing architectures. The paper is well written and the effectiveness of the method is demonstrated relatively well."
                },
                "weaknesses": {
                    "value": "In my opinion, a drawback of the method is the heavy reliance on pre-computed flow field using a pre-trained network that is used as a black box. Thus, errors in this step can negatively affect the results of the proposed pipeline. However, the paper does not address this issue and there are no results to measure the robustness of the method. See question in the section below."
                },
                "questions": {
                    "value": "As I understand, the method is designed to improve visual consistency, particularly with respect to motion. The method relies on optical flow to \"enhance\" the embedded tokens with motion information derived from the flow field. As the pipeline relies on flow field computation and errors introduce in that step may affect the results. Something that is missing in the paper in my opinion is some discussion/experiments/results on how robust is the method to mistakes in the flow field computation. Specifically: \n\n1. How well the proposed method can handle large motion (large displacement in the flow field) or abrupt motion? are there any examples you can provide? \n2. How well the proposed method can handle videos in which both global motion (camera movement) and local motion (object movement) are present? are there any examples you can provide? \n3. Are there any situations where the method can do \"more harm than good\"? I mean, cases where the errors in the flow-field computation can cause the method to produce worse results than the baseline? how often do they occur? \n4. Are there any examples that you can provide in which the flow field is far from accurate? In those cases, are the results worse than the baseline, meaning the method did \"more harm than good\"?\n5. How does the results change with respect to accuracy of the flow field? For example, by taking a specific video and flow field results from several models where some perform dramatically worse than others or by gradually corrupting the flow field and measuring the affect on the results?\n\nI find that the qualitative results provided the supplemental video are extremely helpful (particularly the \"racing trucks\" example in which the results of other methods are provided). I would be grateful if the authors would be able to provide more examples that address the questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1423/Reviewer_UKpY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781211911,
            "cdate": 1698781211911,
            "tmdate": 1700937387161,
            "mdate": 1700937387161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H3NxtJt8Vl",
                "forum": "JgqftqZQZ7",
                "replyto": "kqi1UYWfGu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response (1/2)"
                    },
                    "comment": {
                        "value": "**Dear Reviewer UKpY**,\n\nWe appreciate your positive recognition of our work and the insightful comments regarding the reliance on the pre-computed flow field. We will address your comments as follows:\n\nWe would like to clarify that our method does not directly employ the flow field on the pixel level. Instead, we utilize a downsampled flow field as guidance in the latent space and incorporate an attention mechanism. This can effectively mitigate the negative impact of potential errors in the pre-computed pixel displacement field, which is also one key novelty of our work. Furthermore, the flow estimator [1] we used has already demonstrated its superior performance in many applications. To strengthen our idea, we will add the results/answers to the following questions in the revised manuscript.\n\n[1] Raft: Recurrent all-pairs field transforms for optical flow.\n\n**Q1: How well the proposed method can handle large motion or abrupt motion?**\nThe proposed method based on RAFT overall performs well, even when there is a large motion/abrupt motion. In fact, \"racing trucks\" is the most challenging example in the dataset. It is a long-term video with large motion, camera movement, and object movement. Our model can still output a high-consistent edited video. The generated video has the same motion as the input video. Several patch trajectories in the example are visualized (the patches on the trajectory are marked with red dots) as follows:\n\n https://i.imgur.com/8PErmVI.gif\n\n https://i.imgur.com/R22S7Ri.gif (starts in the middle)\n\nSometimes some patches (e.g. the background) look quite similar and the predicted trajectories are not 100% accurate. However, they have little negative impact on the editing results and our method is robust in these cases.\n\nhttps://i.imgur.com/bpHWf7k.gif\n\nWe also show some additional editing results for the large motion videos:\n\nhttps://i.imgur.com/zlGGXpP.mp4\n\nhttps://i.imgur.com/D53sbDA.mp4\n\nhttps://i.imgur.com/fQQ8AOP.mp4\n\n**Q2: How well the method can handle videos in which both camera and object movement are present?**\n\nOur method works well with videos where camera movement and object movement occur at the same time. \"Racing trucks\" is a good example. Another example of \u201ccar drift\u201d is shown:\n\nSource video: https://i.imgur.com/qN3bsGS.mp4\n\nEdited video: https://i.imgur.com/Sc81qzN.mp4\n\nTrajectories:\n\n https://i.imgur.com/fmyPK9q.gif\n\nhttps://i.imgur.com/w3nRRwK.gif  (stops in the middle)\n\nhttps://i.imgur.com/0Y37Exi.gif  (starts in the middle)\n\n**Q3: Are there any situations where the method can do \"more harm than good\"? how often do they occur?**\n\nOur method demonstrates a high level of robustness, and instances where it causes \"more harm than good\" are very rare. While it's acknowledged that occasional inaccuracies may arise in the flow-field computation, it's crucial to emphasize that the majority of trajectories are accurately predicted. Importantly, the occurrence of all trajectories in a video being inaccurately predicted is an extremely rare anomaly that has not been observed in our extensive testing.\n\nOne notable advantage of our approach is the integration of the flow field into the attention mechanism, significantly enhancing adaptability and robustness. In situations where a patch is mistakenly assigned to a trajectory, the attention mechanism mitigates potential harm. When a patch is misassigned to a trajectory, other patches will avoid attending to the wrong patch due to the low similarity with the misassigned patch. Moreover, if the original motion is very complex and many predicted trajectories are not correct, it usually means the original video is also very challenging for the baseline method (which only uses spatial-temporal attention). The results in Table 2 have demonstrated the robustness/effectiveness of our method compared to the baseline."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047058408,
                "cdate": 1700047058408,
                "tmdate": 1700047058408,
                "mdate": 1700047058408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dLMszY8qRn",
                "forum": "JgqftqZQZ7",
                "replyto": "kqi1UYWfGu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1423/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response (2/2)"
                    },
                    "comment": {
                        "value": "**Q4: Are there any examples in which the flow field is far from accurate? In those cases, are the results worse than the baseline?**\n\nSince we don\u2019t have the ground truth field, it is difficult to determine whether the pre-computed flow field is far from accurate. As discussed, from the qualitative results, most of the precomputed flow fields are of good quality. Therefore, we follow your suggestion in Q5: we corrupt the pre-computed flow field (by adding noise) and use the corrupted flow field to predict the patch trajectories for flow-guided attention. The results are available at:\n\nSource video (prompt: Ski lift chairlifts with a desolate desert background): https://i.imgur.com/ES7ab8T.mp4\n\n\nBaseline video: https://i.imgur.com/IhBWAKI.mp4\n\nRAFT flow: https://i.imgur.com/XeaTSwV.mp4\n\nRAFT flow+noise: https://i.imgur.com/BdCaOTI.mp4\n\nAfter adding the noise to the pre-computed flow, the trajectories are not accurate. There are artifacts in the generated video. However, the video quality is still better than the baseline model.  Moreover, we emphasize that our approach does not rely on any specific flow estimation model, e.g., RAFT. The robustness of flow estimation is out of the scope of this work.\n\n\n**Q5: How do the results change with respect to the accuracy of the flow field?**\n\nWe use the optical flow computed by RAFT and GMA in our flow-guided attention. There is no obvious difference between the output videos:\n\nRAFT:  https://i.imgur.com/XeaTSwV.mp4\n\nGMA: https://i.imgur.com/qrdUFM4.mp4\n\nThe results show that our method is robust to small differences in patch trajectories..\n\nYour comments will help us to improve the presentation and impact of our work, and we are grateful for the opportunity to address these concerns. If you have any new comments, please do not hesitate to let us know! We appreciate your consideration of our contributions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047146837,
                "cdate": 1700047146837,
                "tmdate": 1700047146837,
                "mdate": 1700047146837,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]