[
    {
        "title": "Steering No-Regret Learners to Optimal Equilibria"
    },
    {
        "review": {
            "id": "IK0H6yhbeU",
            "forum": "EsjoMaNeVo",
            "replyto": "EsjoMaNeVo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7900/Reviewer_ogtK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7900/Reviewer_ogtK"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers a setting where there is a mediator whose goal is to direct (i.e. steer) the learning agents to desirable equilibria by paying the agents, thus affecting the learning agents' payoff. The authors show that if the total payment is upper bounded by a constant then it is not possible to steer the agents. However, it is possible when the per-iteration payments are o(1). In particular, in the setting where the agents use an algorithm that has sqrt{T} regret, they show that in the full information setting or normal setting, the per-iteration payment is O(T^{-1/4}) and, in the bandit setting for extensive-form games, the per-iteration payment is O(T^{-1/8}). The authors study both the full-information setting and the bandit setting.\n\nTo prove their results, one idea is to pay the agents so that playing a Nash equilibrium is a dominant strategy. However, the technical difficulty here is that we want to ensure that the per-iteration payment goes to 0 as t tends to infinity. The authors show how to do this by designing a payment function with essentially three components. The first component is to subsidize the agents when the other agents are not bidding the equilibrium. An important aspect of this is that when the agents are in equilibrium, this component should vanish. The second is a reward which incentives the agents to bid the equilibrium. And the last component is just to ensure the payments are non-negative."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem is quite original and is very interesting. The results are interesting and I believe will be of interest to researchers working in the intersection of AGT and learning. In particular, there is a large body of work on understanding dynamics of games and this paper should fit right in.\n\nIn addition, the writing is clear and the techniques seem quite sophisticated. I enjoyed reading the paper and appreciate that it is a novel problem."
                },
                "weaknesses": {
                    "value": "No weaknesses to discuss from my end."
                },
                "questions": {
                    "value": "p. 1: Maybe clarify what is meant by bounded regret. Is it constant regret per-iteration? Vanishing regret?\nAlgorithm 5.1: Do the sandboxing payments have the same property in the normal form case where having those payments ensures that d_i is a dominant strategy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7900/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698529176278,
            "cdate": 1698529176278,
            "tmdate": 1699636969378,
            "mdate": 1699636969378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w6UeBGnuMz",
                "forum": "EsjoMaNeVo",
                "replyto": "IK0H6yhbeU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ogtK"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. \n\n*Bounded regret (page 1)*\n\nWe mean that the regret should be bounded by a sublinear function of $T$, as is the usual assumption. We've changed it to \"sublinear regret\".\n\n*Algorithm 5.1:  Do the sandboxing payments have the same property in the normal form case where having those payments ensures that d_i is a dominant strategy?*\n\nYes--the ability to set these sandboxing payments is precisely what distinguishes the full-feedback case from the bandit case. As we discuss, in the bandit case, it is impossible in general to make $d_i$ dominant."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100223244,
                "cdate": 1700100223244,
                "tmdate": 1700100223244,
                "mdate": 1700100223244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Un9tk7ogg1",
                "forum": "EsjoMaNeVo",
                "replyto": "w6UeBGnuMz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_ogtK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_ogtK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for answering my questions. It looks good to me."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590494465,
                "cdate": 1700590494465,
                "tmdate": 1700590494465,
                "mdate": 1700590494465,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LQcLrFfzOj",
            "forum": "EsjoMaNeVo",
            "replyto": "EsjoMaNeVo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7900/Reviewer_yahc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7900/Reviewer_yahc"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of guiding no-regret-learning agents toward desirable equilibria using nonnegative payments. The authors first show that achieving vanishing average payments allows for steering in scenarios where complete player strategies are observable. However, steering is impossible with a finite budget across all iterations. In cases where only game tree trajectories are observable, the feasibility of steering varies, being possible in normal-form games or with growing per-iteration payments, but generally impossible in extensive-form games with constant per-iteration payments. The paper supplements its theoretical findings with experimental validation, demonstrating the efficacy of steering in large games."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "On the positive side, I found the paper to be well-written, and if you take the model as given, the paper gives a fairly satisfying and complete first investigation \u2013 the questions asked are exactly the ones I would hope are answered first."
                },
                "weaknesses": {
                    "value": "I have a major concern regarding the motivation behind this paper, particularly with respect to the assumption that agents willingly accept being directed. A key question arises when these agents are required to continue their interactions for extended rounds after being steered towards equilibrium \u2013 should compensation be provided once they reach this equilibrium? While the paper mentions that a small constant, P (P <= 8), is sufficient to guide them to the exact equilibrium within a finite number of iterations, it overlooks the potential utility loss (because of being steered into a different equilibrium) experienced by agents after this steering process. Intuitively, it seems reasonable that they should also receive compensation to offset this particular loss. Moreover, an alternative approach could be to simplify the model into a single step, instructing agents to directly adopt the ideal equilibrium. In this case, the mediator could provide one-time compensation equal to the total utility difference experienced under an alternative equilibrium.\n\nSpecifically, regarding the assumption that followers will consistently play a no-regret strategy in response to the leader's queries, especially when they are aware of the steering process towards equilibrium. Steering mechanisms have the potential to influence agents' decisions and behaviors, raising questions about the potential introduction of bias or preferential treatment for certain groups of agents. For instance, in the design of payment schemes, concerns about fairness may emerge. It is crucial for the paper to elaborate on the validity of this behavior model and provide justifications into the application domains where such assumptions hold.\n\nFrom a technical perspective, I found it challenging to understand how the algorithms can be effectively applied to mixed-strategy equilibria through a mediator-augmented game. Specifically, when dealing with pure strategies, solving equation (2) seems feasible. However, when considering mixed strategies, X_i transforms into an infinite set, and solving equation (2) would necessitate the optimization of a non-convex problem."
                },
                "questions": {
                    "value": "The concept of a mediator-augmented game reminds me of a paper by Deng, Yuan, Jon Schneider, and Balasubramanian Sivan titled \"Strategizing against no-regret learners.\" This paper, which addresses similar problems involving strategies against no-regret learners, was cited in your work but not extensively discussed. In a mediator-augmented game, the mediator essentially takes on the role of the leader who strategizes against the no-regret learner. I am curious whether you can provide a detailed explanation of the difference between your models and results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Steering mechanisms have the potential to influence agents' decisions and behaviors, raising questions about the potential introduction of bias or preferential treatment for certain groups of agents. For instance, in the design of payment schemes, concerns about fairness may emerge."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Reviewer_yahc"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7900/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777303802,
            "cdate": 1698777303802,
            "tmdate": 1699636969242,
            "mdate": 1699636969242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qvcJsabk1M",
                "forum": "EsjoMaNeVo",
                "replyto": "LQcLrFfzOj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yahc (I)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. Below we address the reviewer's questions and concerns.\n\n*Assumption that agents willingly accept being directed*\n\nWe're a bit confused what the reviewer means by this. The players do not need to willingly accept anything: the only assumption we make about their behavior is that they are no-regret. In the non-pure-equilibrium setting, the players are free to ignore or disobey the recommendations of the mediator if they wish.\n\n*Should compensation be provided once they reach this equilibrium?*\n\nIn our model, if the players are playing the equilibrium, they will continue to receive a small (and vanishing with time) amount of payment, just to ensure that they continue to play the equilibrium. Intuitively, this payment exists to address the case where the equilibrium is not strict.\n\n*Moreover, an alternative approach could be to simplify the model into a single step, instructing agents to directly adopt the ideal equilibrium.  [...] Specifically, regarding the assumption that followers will consistently play a no-regret strategy in response to the leader's queries, especially when they are aware of the steering process towards equilibrium. [...] It is crucial for the paper to elaborate on the validity of this behavior model*\n\nThe alternative model proposed by the reviewer, in which the agents would directly switch to the equilibrium upon being instructed to do so, is far stronger (and harder to justify) than the one we work with. As the reviewer also mentions, working in that model would essentially render the problem trivial, because steering to an equilibrium would then amount to nothing but telling the players what to play.\n\nWe agree that, if the players were aware of the steering process and could collude against our mediator, they could manage to \"break\" the algorithm by, for example, extracting high payments from the mediator. However, this would result in a \"prisoner's-dilemma-like\" situation where at least one player would end up having high regret. \n\nMany \"intuitive\" algorithms that could be adopted by learning agents in practice, such as (projected) gradient descent, multiplicative weights, or regret matching, are no-regret dynamics. Further, a player who incurs high regret has in some sense \"failed to learn\", because, in hindsight, it would have done better by acting in some other way. For these reasons, we believe that the assumption of no regret is a fairly weak and reasonable assumption in our eyes (and indeed that is the reason we work with it). \n\nWe have included this discussion in the revised version.\n\n*From a technical perspective, I found it challenging to understand how the algorithms can be effectively applied to mixed-strategy equilibria through a mediator-augmented game. Specifically, when dealing with pure strategies, solving equation (2) seems feasible. However, when considering mixed strategies, X_i transforms into an infinite set, and solving equation (2) would necessitate the optimization of a non-convex problem.*\n\nConsidering the infinite game in which each player selects a mixed strategy is just one way to address the issue of mixed strategies. Indeed, Monderer and Tennenholtz (2004) discuss this approach in Section 3 of their paper. It is not the way that our paper takes. Instead, our paper addresses the mixed equilibrium case by empowering the mediator to give action recommendations to the player. This augmented game is still finite (as long as the underlying game is finite, of course): the mediator has finitely many action recommendations to select from, and the player, upon seeing an action recommendation, picks among its finitely many choices of action. This augmented game is also extensive form---as such, the players' strategy spaces are still polytopes, and the optimization remains convex and efficient.\n\nWe continue our response below."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099974386,
                "cdate": 1700099974386,
                "tmdate": 1700099974386,
                "mdate": 1700099974386,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bAOxgRdlme",
                "forum": "EsjoMaNeVo",
                "replyto": "UKWaL6h9V6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_yahc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_yahc"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the clarification. I will take it into consideration during the discussion phase with AC.   \n   \nTo clarify what I mean by \"Assumption that agents willingly accept being directed\". Initially, I presumed that players were aware of the steering process but still chose to follow directions willingly. However, the author's response seems to suggest that the assumption is that players are unaware of the steering process and are myopically playing no-regret algorithms. While this setting may be suitable for theoretical studies, it might lack practical applications. Additionally, the result of vanishing payment is not surprising, considering the behavior assumption that players incur vanishing average regret. Therefore, I maintain my borderline decision."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671921952,
                "cdate": 1700671921952,
                "tmdate": 1700671921952,
                "mdate": 1700671921952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oqWkhOWSDP",
            "forum": "EsjoMaNeVo",
            "replyto": "EsjoMaNeVo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7900/Reviewer_VPEt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7900/Reviewer_VPEt"
            ],
            "content": {
                "summary": {
                    "value": "The paper targets the following problem can we steer no-regret dynamics towards optimal equilibria. The paper considers both settings of normal form and extensive form games as well as full feedback versus bandit feedback. The results can be roughly organized into two categories. Firstly, there are results targeting pure Nash equilibria. In this case the paper provides a number of results showing that such targeting is possible given sublinear total of payments. They also provide an example where finite amount of payments do not suffice for a specific sequence of no-regret play in a 2x2 game. Secondly, there are results targeting mixed NE, and different notions of CE. There is a significant departure in what the paper interprets as steering equilibria. Instead of considering standard no-regret algorithms playing e.g. Follow-the-Regularized-Leader playing the original game, they expand the setting to add a new \"action\" to each algorithm. The new  actions is not an action in the original game but effectively implements the policy \"I will play what my mediator tells me to do\" which mediator can e.g. solve for an optimal correlated equilibrium. This technique effectively allows the paper to leverages recent computational  reductions by (Zhang et al. (2023) and reduce the mixed equilibrium problem to a pure equilibrium problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper targets a rather interesting problem of how to steer no-regret learning algorithms to implement equilibria. It connects interesting areas in game theory, online learning and mechanism design. It presents a plethora of theoretical results in a wide range of different settings and solution concepts. The mathematical parts of the paper are generally well articulated. The authors show good technical grasp of the area and leverage expertly recent results. The paper has also shows some experiments to complement the theory."
                },
                "weaknesses": {
                    "value": "I have two different criticisms for the paper. One for the pure Nash results and one for the rest of the mixed solution concepts.\n\n1. For the case of non-pure Nash equilibria, if I am not mistaken, the paper does not solve the problem that is advertised in the title/abstract/intro. Specifically, from the first paragraph of the intro: \n\nAny student of game theory learns that games can have multiple equilibria of different quality\u2014for\nexample, in terms of social welfare (Figure 1). How can a mediator\u2014a benevolent third party\u2014steer\nplayers toward an optimal one? In this paper, we consider the problem of using a mediator, who\ncan dispense nonnegative payments to players, to guide players to a better collective outcome.\n\nThat is *not* what the paper is doing. The mediator in the case of mixed eq is a much more powerful entity that can actually broadcast advice to all players. Furthermore, the players are not playing any recoginzable no-regret algorithm but learning with advice algorithms,  which have a special option (ignore the actual algorithm and follow centralized advice according to a pre-configured scheme that allows for global instantaneous broadcast of signals that are unambiguously interpreted by all users and acted upon). This is not steering no-regret algorithms but a different class of algorithms altogether where agents learn in the presence of a broadcasting device. The current paper mentions in passing in the introduction some of these papers by Balcan et al (see also [1] for a more recent paper) but they fail to mention that these settings work with a more realistic broadcasting device, which only reaches a small random fraction of all agents (e.g. 1%) instead of all. More importantly, I would have expected that words such as public service advertising, learning with centralized advice, global information, etc. would play central role in the description of the approach. This is a critical part of the model and important difference between the case of pure and mixed eq. \nTo be clear, what is being done is still interesting but it is significantly easier to achieve than merely using payoffs to steer the dynamics.\n\n2. For the case of pure Nash equilibria, it is not clear to me that the problem is hard to begin with. The paper provides a lower bound where a simple game along with a specific type of very unnatural no-regret behavior can be hard to steer. Following results from e.g. [2-4], pure/strict Nash (e.g. like in Stag Hunt games explored in the paper) are locally asymptotically stable for effectively all no-regret algorithms and in fact arguably for all reasonable game dynamics. So here is how to informally stabilize all of them with a finite amount of money. Pad the target strategies with enough money at an initial phase of the algorithm. With a finite excess payoff any reasonable aggregate payoff based dynamics (not even no-regret) after some finite amount of investment will be in the attractor. This is a stronger convergence result since it is actually even guarantees last-iterate convergence (and it does so with a finite amount of money). \n\nMy question is the following: Why should we care enough for totally non-meaningful no-regret behaviors that do not capture any realistic algorithm either from an optimization or a behavioral point of view to be willing to accept paying an infinite amount of money when finite amount should suffice for all reasonable dynamics? \n\nYou also have an experiment with Exp3 where clearly a finite amount of money suffices to select the optimal equilibrium. Can you create a negative example where you would need unbounded money for Exp3? If not, then doesn't this show that the $T^{alpha}$ unbounded payoffs are too loose in comparison to real world lower bounds? \n\n\n[1] Balcan et al. Near optimality in covering games by exposing global information ACM Transactions of Economics and Computation, Volume 2 Issue 4, October 2014.\n[2] Vlatakis-Gkaragkounis et al. \"No-regret learning and mixed nash equilibria: They do not mix.\" Advances in Neural Information Processing Systems 33 (2020): 1380-1391.\n[3] Giannou et al. \"Survival of the strictest: Stable and unstable equilibria under regularized learning with partial information.\" Conference on Learning Theory. PMLR, 2021.\n[4] Giannou et al \"On the rate of convergence of regularized learning in games: From bandits and uncertainty to optimism and beyond.\" Advances in Neural Information Processing Systems 34 (2021): 22655-22666."
                },
                "questions": {
                    "value": "Please see my questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Reviewer_VPEt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7900/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698967995154,
            "cdate": 1698967995154,
            "tmdate": 1700607110303,
            "mdate": 1700607110303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1e27PrUC4W",
                "forum": "EsjoMaNeVo",
                "replyto": "oqWkhOWSDP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VPEt (I)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. Below we address the reviewer's concerns.\n\n*For the case of non-pure Nash equilibria, if I am not mistaken, the paper does not solve the problem that is advertised in the title/abstract/intro*\n\nFor mixed equilibria, we do not believe that allowing the mediator to also give recommendations to players is too strong or far-fetched an assumption, especially given that the mediator is already interacting with the players to provide payments. Indeed, for normal-form games, Monderer and Tennenholtz (2004, Section 6) use the same principle to address mixed equilibria in their paper. \n\nFurther, for mixed equilibria, without advice, there is no hope of performing steering. This is best illustrated by example. Consider the normal-form two-player coordination game where the payoff for both players is the identity, and suppose that the mediator wishes to steer the players toward the mixed (uniform) equilibrium. Since the target equilibrium is fully mixed, every action profile (terminal history) is reached with positive probability in equilibrium; therefore, the mediator cannot give positive limiting payments to any action profile because such payments would actually appear in equilibrium. We have made this formal in Appendix D of the new version.\n\n\nThe above shows that **without advice, steering to a mixed-Nash equilibrium is in general impossible, already in normal-form games**. In extensive-form games challenges related to steering only become more acute. All in all, the discussion above shows that the definition of steering that the reviewer suggests is impossible to achieve.\n\n*The current paper mentions in passing in the introduction some of these papers by Balcan et al (see also [1] for a more recent paper) but they fail to mention that these settings work with a more realistic broadcasting device, which only reaches a small random fraction of all agents (e.g. 1%)*\n\nThere are many crucial differences between our paper and the paper of Balcan et al. [1]. Namely, 1) their results apply only to a limited class of games, while our results apply to general extensive-form games; 2) using the broadcasting device of Balcan et al., which reaches only a small random fraction of all agents, does not suffice to yield optimal equilibria, but rather only yields an _approximation_ of the optimal welfare; and 3) our results apply under a weaker behavioral assumption, namely the no-regret property. \n\nTo elaborate more on point 3) above, in Section 3 of the paper of Balcan et al. it is assumed that the dynamics consist of two phases. In the first phase a subset of the players are assumed to follow the advertising strategy, while the rest of the players are assumed to be best responding. These assumptions are quite brittle compared to our behavioral assumption of assuming the no-regret property. Similar limitations apply to the learning protocol described in Section 4 of the paper of Balcan et al.\n\nWe have included a discussion on those points (and a comparison to [1]) in the related work section of the revised version.\n\n*Why should we care enough for totally non-meaningful no-regret behaviors that do not capture any realistic algorithm either from an optimization or a behavioral point of view to be willing to accept paying an infinite amount of money when finite amount should suffice for all reasonable dynamics?*\n\n*So here is how to informally stabilize all of them with a finite amount of money. Pad the target strategies with enough money at an initial phase of the algorithm. With a finite excess payoff any reasonable aggregate payoff based dynamics (not even no-regret) after some finite amount of investment will be in the attractor. This is a stronger convergence result since it is actually even guarantees last-iterate convergence (and it does so with a finite amount of money).*\n\nFirst of all, we do not believe that there is a definite answer as to what constitutes \"reasonable dynamics.\" For example, the papers cited by the reviewer do not (to the best of our understanding) capture regret matching, one of the most fundamental and natural learning dynamics. Making stronger assumptions regarding the class of update rules followed by the players would limit the scope of our results, and so instead we chose to treat the problem under the much broader condition of incuring sublinear regret. As we point out in the last paragraph of our related work section, there is a recent literature in mechanism design that operates under this exact assumption.\n\nWe continue our response below."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099509091,
                "cdate": 1700099509091,
                "tmdate": 1700099585937,
                "mdate": 1700099585937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pAKlegpwv9",
                "forum": "EsjoMaNeVo",
                "replyto": "aySVObj8nF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_VPEt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_VPEt"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response. \n\nI particularly appreciate the bold part of their response **without advice, steering to a mixed-Nash equilibrium is in general impossible, already in normal-form games**.\n\nMy main question to them is shouldn't this be unambiguously stated (ideally with a similarly bold font) in the introduction?\n\nI.e. We want to solve the problem of steering no regret algs with pure payoff signals to equilibria.\n1. We solve the problem for the case of pure Nash.\n2. One could naively hope that something similar would be true for mixed Nash/CE etc but due to reasons a, b, c, this is **impossible**.\n3. Hence we now expand the abilities of the mediator to do the following on top of providing payoffs (Here is how this is related/an improvement upon previous approaches)\n4. Here our results for the case of mixed Nash and other correlated solution concepts.\n\nAgain and this is a direct question above that you have not answered: Regardless of whether it is reasonable or not (I do not disagree here with this point), where in the introduction do you discuss that the mediator has the ability to give recommendations? \n\nCopying from your current introduction:\n\nSummary of Our Results Our formulation enables the mediator to reward players with nonnegative\npayments. The goal of the mediator is to reach an equilibrium, ....\n\nI find this to be a very important point to disambiguate and if you would be willing to rewrite the introduction to reflect this line of reasoning, that you seem to agree with me in your response above, I would raise my score accordingly to a weak accept.\n \nFor the issue of what constitutes reasonable dynamics I agree, this is in the eye of the beholder and I am willing to concede this point. Not all lower bounds have to be reflective realistic cases. But given that the issue of achieving stability with a finite budget is left as an interesting direction for future work for many practical cases, I feel that the contribution does not reach the level of a clear accept."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573568295,
                "cdate": 1700573568295,
                "tmdate": 1700573568295,
                "mdate": 1700573568295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ibo4V12WQv",
                "forum": "EsjoMaNeVo",
                "replyto": "Vmfb4k5HiE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_VPEt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_VPEt"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for the quick update. I have increased my score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607161712,
                "cdate": 1700607161712,
                "tmdate": 1700607161712,
                "mdate": 1700607161712,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qsSYbuP8vm",
            "forum": "EsjoMaNeVo",
            "replyto": "EsjoMaNeVo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7900/Reviewer_CT7x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7900/Reviewer_CT7x"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the problem of steering no-regret-learning agents to play desirable equilibria in games, using nonnegative payments. The authors present a framework for achieving this goal, and provide theoretical and experimental evidence to support its effectiveness. They also discuss the relationship between this framework and other areas of game theory, such as mechanism design and information design."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors study an interesting question that might have interesting potential applications in mechanism design (and in my opinion) in federated learning."
                },
                "weaknesses": {
                    "value": "I find the paper to be inaccessible and lack precision at times (for instance, a central notion to the paper such as optimal equilibrium is not well-defined\u2014is it one that is Pareto-optimal or that maximizes some notion of welfare?). The experiments seem also unrelated to the theoretical results (Pure Nash vs. Correlated). The definition of the steering problem in terms of pure Nash equilibria is also odd, since pure Nash equilibria are not guaranteed to exist."
                },
                "questions": {
                    "value": "Comments and questions:\n\nPage 2: full feedback is not defined\n\nIs this problem solved \n\nSection 3, regarding regret definition and repeated play. Are the time iterations, the time iterations of some online learning algorithm or is this the steps of an episode? I think as is standard in the literature, the author are considering repeated-play settings but (although this can be understood clearly in the learning literature, I do not think this is clear in this setting). A description of the learning setting would be appreciated.\n\n\nWhat is the meaning of the directness gap? I cannot read the math as the description given, an explanation would be appreciated.\n\nThe steering problem is defined pure strategy Nash equilibria, when are pure strategy Nash equilibria guaranteed to exist in extensive form and normal-form games? How much of the theory provided by the authors apply to mixed Nash equilibria?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7900/Reviewer_CT7x"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7900/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699169960828,
            "cdate": 1699169960828,
            "tmdate": 1699636968989,
            "mdate": 1699636968989,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JRWwnElkCY",
                "forum": "EsjoMaNeVo",
                "replyto": "qsSYbuP8vm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CT7x"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. Below we address the reviewer's questions and concerns.\n\n*The definition of the steering problem in terms of pure Nash equilibria is also odd, since pure Nash equilibria are not guaranteed to exist.*\n\n*The steering problem is defined pure strategy Nash equilibria, when are pure strategy Nash equilibria guaranteed to exist in extensive form and normal-form games? How much of the theory provided by the authors apply to mixed Nash equilibria?*\n\nAs we formalize in Section 6, our results directly apply to mixed-strategy Nash equilibria as well as correlated and communication equilibria. While pure-stratgy Nash equilibria are not guaranteed to exist in general, as the reivewer correctly points out, our results apply to any extensive-form game through the notion of a mediator-augmented game (Definition 6.1).\n\n*The experiments seem also unrelated to the theoretical results (Pure Nash vs. Correlated).*\n\nThere seems to be a misunderstanding here. As we explained above, our results also directly apply to correlated equilibria (see Section 6 of the paper). As a result, our experiments are directly under the umbrella of our theoretical results.\n\n*A central notion to the paper such as optimal equilibrium is not well-defined\u2014is it one that is Pareto-optimal or that maximizes some notion of welfare?*\n\nBy optimal we mean an equilibrium that, among the set of all equilibria, maximizes the objective of the mediator, as we already introduce in Definition 6.1 of the paper. This objective could be, for example, the social welfare or the revenue of a mechanism, but our results apply for any mediator objective function. Note that Sections 4 and 5 concern steering to a pure Nash equilibrium (without any underlying notion of optimality), which is why our notion of optimality is formally introduced later in Section 6 where we apply our earlier results to steer to optimal equilibria.\n\n*What is the meaning of the directness gap?*\n\nThe directness gap quantifies how close players' strategies are from the direct (i.e., obedient) strategies---as prescribed by the target equilibrium.\n\n*Page 2: full feedback is not defined*\n\nThe full feedback setting is introduced in the beginning of Section 5, and it means that the mediator can observe the players' strategies. We have added a reference to Section 5 in the revised version to help the reader.\n\n*Are the time iterations, the time iterations of some online learning algorithm or is this the steps of an episode? I think as is standard in the literature, the author are considering repeated-play settings...*\n\nWe are indeed operating in the standard setting from the repeated games literature, as we specify in Definition 3.1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098572753,
                "cdate": 1700098572753,
                "tmdate": 1700098652638,
                "mdate": 1700098652638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lReJoAV3pM",
                "forum": "EsjoMaNeVo",
                "replyto": "JRWwnElkCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_CT7x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_CT7x"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response, I have no further questions at this point."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545728268,
                "cdate": 1700545728268,
                "tmdate": 1700545728268,
                "mdate": 1700545728268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5zut7Lr9FE",
                "forum": "EsjoMaNeVo",
                "replyto": "Lhk9c6eeYZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_CT7x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Reviewer_CT7x"
                ],
                "content": {
                    "comment": {
                        "value": "My current concern is the presentation of the paper which I believe requires a thorough re-reading and re-writing. Throughout the paper, there exists either expositional inconsistencies or statements which confuse. Often, the paper mentions in a sentence that the results apply for some other notion. For instance, the paper states the results directly apply to correlated equilibria too, and experiments are ran with correlated equilibria. If it is so, then the mathematical model that is presented should present the definition of a steering equilibrium for all notions of equilibrium that the results apply to. Or for instance, take section 6: \n\n> \"We will assume the revelation principle, which allows us to fix a target pure strategy profile that we want to\nmake the equilibrium profile for the non-mediator players.\"\n\nWhat is the revelation principle, it is defined in mechanism design contexts traditionally, why can you just assume it? \n\nThe results seem interesting (but I cannot verify any of them intuitively but this might be my lack of knowledge) but the exposition can be made much more rigorous. As a result, I maintain my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642772984,
                "cdate": 1700642772984,
                "tmdate": 1700642772984,
                "mdate": 1700642772984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HiXcRbN8nV",
                "forum": "EsjoMaNeVo",
                "replyto": "qsSYbuP8vm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7900/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The revelation principle is applicable beyond mechanism design as well. Here are some examples, starting with the usual mechanism design version.\n\n* For mechanism design, the revelation principle states that, WLOG, messages are information reports and all players are honest in equilibrium. The direct strategy $\\boldsymbol d_i$ for each player is the strategy that always sends honest information. \n* For correlated equilibrium (and other versions thereof, e.g., EFCE), the revelation principle states that, WLOG, the signals sent to the players are action recommendations and players in equilibrium should always play the recommended actions. The strategy $\\boldsymbol d_i$ for each player $i$ in this case is the strategy that always plays the recommended actions.\n* Similarly, for communication equilibrium, the revelation principle states that players should *both* send honest information *and* play action recommendations in equilibrium. \n\nThese notions of revelation principle are ubiquitous when dealing with their respective equilibrium concepts, even if often they are not referred to as revelation principles (e.g., the correlated equilibrium revelation principle is often simply taken as part of the definition of correlated equilibrium). We refer the interested reader to any of the cited papers, especially to Appendix A of the cited paper Zhang et al (2023) which Section 6 uses, for more detail about augmented games and the revelation principle. We also commit to adding more detail about the revelation principle, including the above clarifications, in the final version.\n\nSection 6 *does* define a notion of equilibrium and notion of steering formally, via Definitions 6.1 and 6.2. It is the revelation principle that allows the construction of the augmented game (which only includes honest messages and action recommendations) and the definition of direct strategies to be without loss of generality (i.e., without excluding any equilibria). \n\nPlease let us know what else, if anything, you believe can be improved in the presentation. Due to the lack of time remaining in the discussion period, we may not be able to revise during the discussion period, but we promise to take them into consideration in the final version."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7900/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669391794,
                "cdate": 1700669391794,
                "tmdate": 1700669418815,
                "mdate": 1700669418815,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]