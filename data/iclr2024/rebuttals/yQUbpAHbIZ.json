[
    {
        "title": "Post-Nonlinear Causal Relationship with Finite Samples: A Maximal Correlation Perspective"
    },
    {
        "review": {
            "id": "NxydDL7a3S",
            "forum": "yQUbpAHbIZ",
            "replyto": "yQUbpAHbIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5258/Reviewer_hazx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5258/Reviewer_hazx"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the practical problem of the post-nonlinear model that focuses on the over-fitting issue and optimization issue in solving the non-linear function of PNL. The authors discuss several drawbacks of the independent test method, e.g., HSIC, and show that the randomized dependence coefficient (RDC) has the advantage in measuring the non-linear dependence, based on a set of simulation experiments. Moreover, The authors propose a novel method that incorporates maximal correlation into the PNL model learning (short as MC-PNL) such that the underlying nonlinearities can be accurately recovered. The experiment results verify the ability of non-linear function fitting of the proposed method and show that MC-PNL outperforms the baselines in causal discovery application."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n\n2. The authors gave a good overview on the related literature.\n\n3. The authors provide a novel framework to deal with PNL learning, in which the maximal correlation constraints may be beneficial to fit the non-linear function of the PNL model."
                },
                "weaknesses": {
                    "value": "1. There are extra assumptions to ensure the correctness of Lemma 4, such as \"composition of PNL functions and un-mixing nonlinearities are linear\" (provided in Proof Sec. F), which should be incorporated into the claim of Lemma 4.\n\n\n2. The contribution of this paper is a proposed learning framework for PNL with some correctness analysis. My main concern is whether the theoretical contribution is insufficient due to the identification bound of PNL has not improved. I am willing to raise my score if the authors can further illustrate the signification impact of proposed methods in the fields of causal discovery.\n\n\n3. In the \"NONLINEAR FUNCTION FITTING\" experiment, there are only two group generation mechanisms are used. More results with more types of non-linear functions should be provided if possible."
                },
                "questions": {
                    "value": "Have you applied this method to real-world data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5258/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5258/Reviewer_hazx",
                        "ICLR.cc/2024/Conference/Submission5258/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5258/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698546880378,
            "cdate": 1698546880378,
            "tmdate": 1700148819598,
            "mdate": 1700148819598,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5rJkv63uzi",
                "forum": "yQUbpAHbIZ",
                "replyto": "NxydDL7a3S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5258/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your careful reading and comments. We have carefully addressed all your questions below and revised the manuscript accordingly. We sincerely hope that our answers can fully address your concerns.\n\n**W1: Presentation of Lemma 4. Should add more assumptions?**\n\nLemma 4 is in the context of PNL learning and follows the same assumptions as Lemma 3. As we have assumed the **invertibility** of underlying functions $\\bar{f}$ and $\\bar{g}$ in Lemma 4, one can always find $\\hat{f},\\hat{g}$ such that \" the composition of PNL functions and un-mixing nonlinearities are linear\".\nWe have added more descriptions to make Lemma 4 complete.\n\n**W2: Identification bound of PNL has not improved. Require illustration of the signification impact.**\n\nWe acknowledge that the identification result has not been improved. However, we clarify the identifiability results in the context of PNL learning in Lemma 4, which has not been discussed in the existing literature. Besides, the estimation algorithm development is also fairly important, since there is a lack of effective and efficient algorithms to recover the underlying nonlinearities. Our proposed method is easy to implement and has a benign optimization structure, which provides a practical and efficient way to learn the underlying nonlinearities. We hope this can bring new thoughts to both the machine learning and causal inference communities.\n\n**W3: In the \"NONLINEAR FUNCTION FITTING\" experiment, there are only two group generation mechanisms are used. More results with more types of non-linear functions should be provided if possible.**\n\nThe two synthetic examples were presented for illustration, and one of them was used in Uemura & Shimizu (2020). Following your suggestions, we updated the supplement with more examples. Please find additional experiments in our anonymous code link.\n\n**Q1: Have you applied this method to real-world data?**\nThe gene data from the DREAM4 competition is designed to mimic real data. In addition, we also showed the causal discovery result with uncertainty quantification using a real dataset (Tuebingen) in Figure 10 in suppl.I.2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048417707,
                "cdate": 1700048417707,
                "tmdate": 1700048417707,
                "mdate": 1700048417707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w4tlPozNcP",
                "forum": "yQUbpAHbIZ",
                "replyto": "5rJkv63uzi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5258/Reviewer_hazx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5258/Reviewer_hazx"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Thank you for the response. Most of my concerns are addressed, so I will raise my score. I suggest the authors incorporate the responses to the final version of the paper if accepted."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148474444,
                "cdate": 1700148474444,
                "tmdate": 1700148474444,
                "mdate": 1700148474444,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CrbihZrmD3",
            "forum": "yQUbpAHbIZ",
            "replyto": "yQUbpAHbIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5258/Reviewer_7AyX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5258/Reviewer_7AyX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a maximal correlation-based method for discovering causal relationships under the post-nonlinear causal model. Some theoretical guarantees are given. A few synthetic experiments are conducted as proof of concept. A simple real data analysis is also performed, comparing the proposed method against several existing benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, the paper is clearly written, with sufficient details on the motivation, theoretical results, and experiments.\n\nThe paper is trying to address an important question in causal inference, possibly drawing attention from fields such as causal inference, robust machine learning, and computational biology.\n\nIn general, causal discovery algorithms are difficult to scale up. However, this paper also discusses how to convexify the proposed algorithm so one can, at least in principle, solve the optimization problem to recover the final causal structure."
                },
                "weaknesses": {
                    "value": "The numerical experiments look very simple (at least to me), and hence not entirely convincing about the use of overparameterized neural networks.\n\nThe optimization of over-parameterized neural nets itself, together with the \"implicit regularization\" effect of gradient-based methods, is a big problem in practice. I would hope that the authors discuss this in a remark.\n\nWhen $f_{1}$ is a linear function in some basis of $X$, say $\\beta^{\\top} \\phi (X)$, the proposed model is reduced to the single-index model. For single-index models, the identifiability of the parameter $\\beta$ will be problematic, and often one assumes $\\beta$ to be on the unit sphere. Is there a similar concern when one expands the modeling of $f_{1}$ to a purely nonlinear one?\n\nI do not have much else to say about the \"weaknesses\" but I do want to mention that in the Questions section, I list several questions and comments on the paper."
                },
                "questions": {
                    "value": "1. Why use maximal correlation? There are obviously many \"nonparametric correlations\" at our disposal, e.g. d-corr, maximal information coefficients (by Reshef et al.), and many others.\n\n2. Is there any connection between the proposed method (or any other related method) and canonical correlation analysis (CCA)? From the formulation of the optimization problem alone, they look quite similar."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5258/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768706829,
            "cdate": 1698768706829,
            "tmdate": 1699636524786,
            "mdate": 1699636524786,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aFkBWx2S4h",
                "forum": "yQUbpAHbIZ",
                "replyto": "CrbihZrmD3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5258/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[ Update: Response to W2 is updated ]\n\nThank you so much for your careful reading and comments. Our remarks and clarifications are as follows. We sincerely hope that our answers can fully address your concerns.\n\n**W1: The use of over-parameterized neural network (NN)**.\nWe need to clarify that NN has been used in some previous PNL-based methods (e.g., MLP-PNL, AbPNL), but they are not used in our method. We observed the *over-fitting* and *optimization* issues when we tried to reproduce their results using NN, which motivates the benefits of using our proposed method based on the RFF. We did use different NN architectures (over-parameterized wide NN and a relatively smaller deep-narrow NN) to illustrate the issues of previous works, see supp. C, i.e., they both produce unsatisfying results. \n\nThe reasons that we include the over-parameterized NN for comparison are as follows: 1) it works well in many practical applications, and 2) it has a nice loss landscape for optimization (no bad basin/spurious valley under mild assumptions [Sun et al. 2020]). \n\n**W2: In analogy with the single-index model, is there any identification issue when expanding $f_1$ to a purely nonlinear one?**\n\nThanks for pointing out the relation to the single-index model. If we understood correctly, you are referring to the noiseless case, $y =m(x)= f_2(f_1(x))$, which is not discussed in our manuscript. When $f_1(x)= \\beta^T\\phi(x)$, the PNL model reduces to a single-index model as defined in [Lin and Kulasekera, 2007]. And $\\beta$ is identifiable when it has a unit norm and $m$ is a non-constant continuous function.\n\n If $f_1$ is extended to a more flexible nonlinear function w.r.t the parameter $\\theta$ (e.g., a NN parameterized by $\\theta$), similar results **MAY NOT** hold, i.e., $m(x) = g(f(x))=h(q(x))$ but $\\{g\\neq h, f\\neq q \\}$. A counter-example (in scalar case)  is $m(x)= \\sin^2(x)$. The nonlinear transformations can be $\\{g(z) = (\\alpha z)^2,f(x)=\\frac{1}{\\alpha} \\sin(x) \\}$ and $\\{h(z)= sin^2(\\beta z), q(x) = \\frac{1}{\\beta} x \\}$, where $\\alpha,\\beta \\neq 0$ are some scaling factors.\n\n However, in the PNL model in our paper, **there is an independent noise term**, and our Lemma 2 gives the corresponding identifiability result.\n\nPlease correct us if we misunderstood your question. We'll be happy to address further questions/comments.\n\n**Q1: Why use maximal correlation? There are obviously many \"nonparametric correlations\" at our disposal, e.g. d-corr, maximal information coefficients (by Reshef et al.), and many others.**\n\nMaximal Correlation is a natural choice for PNL learning, as the optimized transformations $f$ and $g$ are closely related to the underlying nonlinear functions $f_1,f_2^{-1}$ in the PNL model. However, other dependence measures do not involve such a transformation learning process. Another consideration is computational efficiency. Since we need to compute dependence during optimization, many known dependence measures (e.g., maximal information coefficient) are not suitable, see the time comparison in the RDC paper (Lopez-Paz et al., 2013).  For the independence test, however, other dependence measures could be considered as potential replacements.\n\n**Q2: Any connection to CCA?**\nCertainly, some components in our method (e.g., HGR and RDC) are strongly related to nonlinear CCA. One famous work along this research line is Kernelized CCA by Bach and Jordan (2002), where the connection to CCA was thoroughly discussed. The distinction between Kernelized CCA and RDC lies in the representations of $f$ and $g$: Kernelized CCA uses $K_1\\alpha_1$ for $f(X)$ (representer theorem), while RDC uses the linear combination of RFFs, $\\Phi(X)^T\\alpha$. \n\n\n```\nR. Sun, D. Li, S. Liang, T. Ding and R. Srikant, \"The Global Landscape of Neural Networks: An Overview,\" in IEEE Signal Processing Magazine, vol. 37, no. 5, pp. 95-108, Sept. 2020, doi: 10.1109/MSP.2020.3004124.\n\nLin, Wei, and K. B. Kulasekera. \u201cIdentifiability of Single-Index Models and Additive-Index Models.\u201d Biometrika 94, no. 2 (2007): 496\u2013501. http://www.jstor.org/stable/20441387.\n\nBach F R, Jordan M I. Kernel independent component analysis[J]. Journal of machine learning research, 2002, 3(Jul): 1-48.\n\n```"
                    },
                    "title": {
                        "value": "Response to Reviewer 7AyX"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048216830,
                "cdate": 1700048216830,
                "tmdate": 1700665854750,
                "mdate": 1700665854750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d4ynKrDCw5",
                "forum": "yQUbpAHbIZ",
                "replyto": "aFkBWx2S4h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5258/Reviewer_7AyX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5258/Reviewer_7AyX"
                ],
                "content": {
                    "title": {
                        "value": "thank you for your response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response to my comments. Unfortunately, I do not think I can improve my original evaluation of the paper, given the overall content of the paper, in terms of novelty and contribution."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666036530,
                "cdate": 1700666036530,
                "tmdate": 1700666036530,
                "mdate": 1700666036530,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WxbKIHQEnF",
            "forum": "yQUbpAHbIZ",
            "replyto": "yQUbpAHbIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5258/Reviewer_m7U7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5258/Reviewer_m7U7"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers bivariate causal discovery without confounders. Under the assumption that the true model follows the post-nonlinear (PNL) model, prior work does causal discovery by learning the functions f_1,f_2 and then using some dependence measure to compare the dependence between the residuals and the input under both hypotheses. This paper proposes an alternate two-stage method, where the first stage learns the functions using a soft-version of HGR maximal correlation regularized by a dependency measure (Renyi, 1959) and the second stage is an independence test between the residual and the input. The dependency measure regularization is motivated by pointing out the con of maximal correlation that it doesn't provide usable residuals for the downstream independence test. Experimental results show that the proposed method is competitive w.r.t existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Bivariate causal discovery is an important, fundamental problem in causal inference. This paper advances the literature by providing an algorithm that uses a variant of maximal correlation to learn nonlinear functions of the PNL model. A major contribution of this paper is a systematic study of how to use maximal correlation based methods for causal discovery. Experimental results seem comprehensive in the sense that they cover a wide variety of datasets both simulated and real, barring a few concerns that I elaborate in the following section."
                },
                "weaknesses": {
                    "value": "1) The writing can be improved greatly. Few assertions are vague (e.g. \"HSIC can get easily stuck at \"meaningless\" local minima\", IGCI cannot provide \"transparent and interpretable transformations\") and undefined in the main paper (e.g. randomized dependence coefficient is not defined clearly despite it being used in the proposed method). \n2) It was also not clear to me how a dependence measure, HGR correlation, was used to motivate learning nonlinear functions; after all it is irrelevant whether X and Y are dependent. \n3) The main contributions seem overstated. Among a host of different causal disvoery methods compared in Table 2, the proposed method MC-PNL is faster compared to only AB-PNL by 300x, while there exists a competitive method in IGCI that is 60x faster than the proposed method. Overall, the experimental results don't seem to give the impression that MC-PNL outperforms other benchmarks. \n4) While one of the disadvantages of the PNL algorithms is claimed to be the optimization issue, thus motivating the RFF parametrization and linear HSIC kernel, neither is the experimental performance of this variant discussed, nor is its theoretical properties. MC-PNL still uses a gradient-based algorithm for the universal kernel and banded loss regularizer."
                },
                "questions": {
                    "value": "1) The authors repeatedly use the word \"meaningful\" (pg 5, under eq 9, pg 6, first para in 4.1) while criticizing existing methods without providing much explanation as to what is \"meaningful\". While the usage in pg 5 is backed by the observation that residuals can be matched to arbitrary noise profiles, other usages are unclear. These assertions seem important but without knowing the meaning it's unclear what the criticism means.\n2) Is it supposed to be < -\\delta in Line 3 in Algorithm 2? \n3) Is there any ablation study done to determine the parameters of the RFF?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5258/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5258/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5258/Reviewer_m7U7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5258/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846740415,
            "cdate": 1698846740415,
            "tmdate": 1700477573612,
            "mdate": 1700477573612,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rQ0DwE9XfZ",
                "forum": "yQUbpAHbIZ",
                "replyto": "WxbKIHQEnF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5258/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5258/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your careful reading and comments. We have carefully addressed your questions below and revised the manuscript accordingly. We sincerely hope that our answers have fully addressed your concerns.\n\n**W1 & Q1: On Presentation clarity.**\n\nWe refer to the \"meaningful solutions\" as those solutions that can reflect the underlying nonlinear transformations. As one can see in the last two rows in Figure 6, the learned solutions cannot reveal the underlying functions as shown in Figure. 4, and thus are less meaningful. We have added a short description in the main paper to make this clear.\n\nThe causal discovery method, IGCI, is not designed for PNL learning and cannot give explicit function transformations of $f_1,f_2$, thus cannot provide \"transparent and interpretable transformations\". We have rewritten this phrase to \"explicit nonlinear transformations\". \n\nFor the convenience of the audience, we have added the formal definition of randomized dependence coefficient (RDC) in the updated version.\n\n**W2: How HGR was used to motivate PNL learning?**\nAs $Y = f_2 (f_1(X)+\\epsilon)$ in the PNL setting, it is obvious that $X$ and $Y$ are highly correlated when the noise is small. Our goal is to learn $f_1,f_2$. Let $f(X) = f_1(X),g(Y) = f_2^{-1}(Y)$, and we hope to learn $f$ and $g$ through maximizing the HGR correlation $E(g(Y)f(X))$.\n\n**W3: The main contributions seem overstated.**\nThe reported numerical gain is compared with the SOTA PNL-based method. Our focus lies on comparing with interpretable baselines, as other baselines are incapable of providing explicit transformations for $f_1$ and $f_2$.\n\n**W4:  While one of the disadvantages of the PNL algorithms is claimed to be the optimization issue, thus motivating the RFF parametrization and linear HSIC kernel, neither is the experimental performance of this variant discussed, nor is its theoretical properties. MC-PNL still uses a gradient-based algorithm for the universal kernel and banded loss regularizer.**\n\nWe did include the theoretical properties in the text under eq (14). We show that the BCD algorithm can converge to a critical point and each sub-problem is a quadratic program.\nThe experimental convergence results have been shown in Figure 3. Only for fine-tuning, the gradient-based optimization is needed to handle the universal kernel and banded loss regularizer.\n\n**Q1: see W1**\n\n**Q2: Typo.** \nYes, it should be `$<-\\delta$` in Line 3 in Algorithm 2.\n\n**Q3: Is there any ablation study done to determine the parameters of the RFF?**\n\nWe are unsure about the specific type of ablation study you are expecting. It would be very helpful if you could specify the question a little bit more."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047768353,
                "cdate": 1700047768353,
                "tmdate": 1700047768353,
                "mdate": 1700047768353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ipn3x3rfSZ",
                "forum": "yQUbpAHbIZ",
                "replyto": "rQ0DwE9XfZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5258/Reviewer_m7U7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5258/Reviewer_m7U7"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for the clarification about how HGR is used to motivate PNL learning and more clarity on the vague terms. For the former, it would make the paper clearer if your explanation were added.\n\nW3: I agree that it is competitive w.r.t other PNL-based methods. My concern is about the overstatement in the abstract and the introduction which doesn't mention about comparing with respect to PNL-methods there. \n\nQ3: Ablation: RFF has two parameters - noise variance and the number of sinusoidal features. I am referring to ablation studies w.r.t. these parameters on ROC-AUC. \n\nI have improved my score to reflect the responses."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5258/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477554043,
                "cdate": 1700477554043,
                "tmdate": 1700477554043,
                "mdate": 1700477554043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]