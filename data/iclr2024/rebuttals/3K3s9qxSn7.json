[
    {
        "title": "On Representation Complexity of Model-based and Model-free Reinforcement Learning"
    },
    {
        "review": {
            "id": "7rnFT8FesY",
            "forum": "3K3s9qxSn7",
            "replyto": "3K3s9qxSn7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8968/Reviewer_kBCH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8968/Reviewer_kBCH"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the representation complexity of model-based and model-free reinforcement learning (RL) algorithms in the context of circuit complexity. The authors introduce a special class of majority MDPs and theoretically demonstrate that while the transition and reward functions of these MDPs have low representation complexity, the optimal Q-function exhibits exponential representation complexity. The paper then extends these findings empirically by examining MuJoCo Gym environments and measuring the relative approximation errors of neural networks in fitting these functions. The results demonstrate that the optimal Q-functions are significantly harder to approximate than the transition and reward functions which supports their theoretical findings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Novelty: this is the first work to study the representation complexity of RL under a circuit complexity framework. The authors introduced a general class Majority MDPs, such that their transition kernels and reward functions have much lower circuit complexity than their optimal Q-functions. And the authors provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective.\n- The theoretical results are supported by empirical demonstrations."
                },
                "weaknesses": {
                    "value": "- The theoretical nature of the Majority MDP might limit its direct applicability to practical problems. \n- A minor point that the experiment section only includes a limited set of small scale gym environments."
                },
                "questions": {
                    "value": "How might these insights inspire the design and development of improvements to RL algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8968/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8968/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8968/Reviewer_kBCH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8968/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698435039035,
            "cdate": 1698435039035,
            "tmdate": 1699637129138,
            "mdate": 1699637129138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CvUnSz6SzI",
                "forum": "3K3s9qxSn7",
                "replyto": "7rnFT8FesY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for reviewing our work and providing helpful and insightful comments. Below are our responses.\n\n>The theoretical nature of the Majority MDP might limit its direct applicability to practical problems.\n\nWe discussed in the global response that many MDPs in real-world applications are actually equivalent (or close) to the majority MDP class. Also, our experimental results further validate that our results can be applied to general RL problems.\n\n>A minor point is that the experiment section only includes a limited set of small scale gym environments.\n\nWe emphasize that our use of the Mujoco environments aligns with the standard benchmarks prevalent in the reinforcement learning community, as corroborated by multiple studies ([1][2][3]). This alignment not only substantiates our theoretical findings but also ensures their relevance in practical settings. To further fortify the credibility of our experimental results, we provided additional results depicted in Figures 4-7 on pages 20-21 of the revised supplementary material. These enhancements serve to provide a more comprehensive and robust demonstration of our research's applicability.\n\n>How might these insights inspire the design and development of improvements to RL algorithms?\n\nThe main message conveyed by our paper that \u201cMDPs with simple model structure and complex Q-function are common\u2019 could inspire researchers to use the knowledge of the model (either using a model-based learning algorithm or boosting existing model-free algorithms using the knowledge of the model or by planning) to achieve better performance than a purely model-free algorithm (as also empirically validated by [4]).\n\nAlso, when choosing an appropriate function to learn and deploying function approximation, it is worth thinking about whether the chosen function is simple or complex since to learn a complex function, one either needs a complicated function class or suffers a large approximation error. This is not only important for RL algorithm design but also important for a broader scope of general machine learning settings.\n\n\n**References**\n\n[1] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861\u20131870. PMLR, 2018\n\n[2] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In Proceedings of the 39th International Conference on Machine Learning, volume 162, 2022\n\n[3] Hanlin Zhu, Paria Rashidinejad, and Jiantao Jiao. Importance weighted actor-critic for optimal conservative offline reinforcement learning. arXiv preprint arXiv:2301.12714, 2023\n\n[4] Kefan Dong, Yuping Luo, and Tengyu Ma. On the expressivity of neural networks for deep reinforcement learning. In International Conference on Machine Learning (ICML), 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700117573014,
                "cdate": 1700117573014,
                "tmdate": 1700117573014,
                "mdate": 1700117573014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i4givwjGEo",
                "forum": "3K3s9qxSn7",
                "replyto": "CvUnSz6SzI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8968/Reviewer_kBCH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8968/Reviewer_kBCH"
                ],
                "content": {
                    "title": {
                        "value": "thank the authors for the rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the rebuttal. I don't have any further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659533609,
                "cdate": 1700659533609,
                "tmdate": 1700659533609,
                "mdate": 1700659533609,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L8RrK5qbEX",
            "forum": "3K3s9qxSn7",
            "replyto": "3K3s9qxSn7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8968/Reviewer_zTQ7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8968/Reviewer_zTQ7"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the representation complexity of model-based and model-free reinforcement learning (RL). Different from the prior work (Dong et al., 2020b), this paper considers a less restrictive family of MDP and incorporates circuit complexity for more fundamental and rigorous theoretical results. This work introduces the definitions of Parity MDP and Majority MDP and proves that the reward function and the transition function have lower circuit complexity than value functions. Experiments are conducted in MuJoCo continuous control tasks with SAC algorithm, quantitively corroborating the theoretical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and organized. The presentation is smooth and clear.\n- To my knowledge, the representation complexity study for the MDP family considered in this paper is novel and significant.\n- Both theoretical analysis and empirical investigation are provided."
                },
                "weaknesses": {
                    "value": "- Although the authors mentioned that the considered MDP family is general, it is not clear how the circuit representation complexity in visual-input MDPs (or POMDP) and stochastic-transition MDPs can be. I think at least some discussions and remarks on these points will be useful.\n- The experiments are not convincing to me. Some important details are missing, e.g., the exact computation of the approximation error of the optimal Q-function. Moreover, I think the variation in terms of the network scale (i.e., $d$ and $\\omega$) and environments (e.g., Atari) is insufficient.\n\n&nbsp;\n\nI would be willing to raise my rating if my concerns are addressed. Please see the concrete questions below."
                },
                "questions": {
                    "value": "1) What are the approximation errors of the optimal $Q$-function calculated exactly in the experiments? Fitting the $Q$ values given by the learned SAC critics or fitting the monte carlo returns of the learned policy (i.e., the actor)?\n\n2) How will the experimental results change when varying the size of neural network used? I think only one configuration (i.e., $d=2, \\omega = 32$) is insufficient.\n\n3) How can the circuit representation complexity be in visual-input MDPs (or POMDP) and stochastic-transition MDPs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8968/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763900352,
            "cdate": 1698763900352,
            "tmdate": 1699637129019,
            "mdate": 1699637129019,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mX0z3ulFNU",
                "forum": "3K3s9qxSn7",
                "replyto": "L8RrK5qbEX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your effort in reviewing our work and providing helpful and insightful comments. Below are our responses to your questions.\n\n>1. What are the approximation errors of the optimal Q-function calculated exactly in the experiments? Fitting the Q-values given by the learned SAC critics or fitting the monte carlo returns of the learned policy (i.e., the actor)?\n\t\nIn the experiments displayed in Section 4, the approximation errors are calculated by fitting the Q-values given by the learned SAC critics. The Q-values learned by SAC are good approximations to the optimal Q-values. In SAC training, the critic loss (average l2 Bellman error) is consistently bounded by 0.005 in InvertedPendulum-v4, by 0.01 in Ant-v4, HalfCheetah-v4, Hopper-v4, and by 0.1 in Walker-v4 respectively. These small Bellman errors suggest that the Q-functions calculated by the SAC experiments are able to fit the optimal Q-functions quite well. Moreover, we note that the critic losses are all smaller than the (un-normalized)  approximation errors in the Q-function, which are about 10X the relative approximation errors in the Q-function plotted in Figure 3.\n\n\n>2. How will the experimental results change when varying the size of neural network used? I think only one configuration (i.e., d=2,w=32) is insufficient.\n\nThank you for the valuable suggestion to make the experimental result more solid. As stated in the global response, we conducted further experiments on different configurations of neural networks, and the same result that the approximation errors of the Q-functions are greater than those of the model and reward functions consistently holds under different configurations:\n1. d=1 w=16\n2. d=2 w=64\n3. d=2 w=128\n4. d=3 w=64\n\nThe plots for the experimental results are shown in Figure 4-7, pages 20-21 in the updated supplementary material pdf.\n\n>3. How can the circuit representation complexity be in visual-input MDPs (or POMDP) and stochastic-transition MDPs?\n\nIt would definitely be an interesting and important future direction to extend our theories to stochastic MDP and POMDP. Below we briefly discuss possible approaches of generalizing to POMDP and stochastic MDP:\n\n**POMDP**: For POMDP, we can additionally assume a function $O(s) = o$ (or $O(s,a) =o$), which maps a state s (or a state-action pair (s,a))  to a corresponding observation o. One possible choice of $O$ could be a substring of the state s, which still has low representation complexity. In POMDPs, the Q-function is often defined on a sequence of past observations or certain belief states that reflect the posterior distribution of the hidden state. As a result, the Q-functions in POMDPs often have higher complexity.  Since the Q-function is proven to possess high circuit complexity for the majority MDP, the representation complexity for value-based quantities of a partially observable version could be even higher. We thank the reviewer for raising this very interesting question, and a more formal and rigorous analysis would be a fascinating direction for future research.\n\n**Stochastic-transition MDP**: As discussed in the global response, our framework can be extended to stochastic MDP. Here we discuss a natural extension of our majority MDP: After applying a = 1, if C(s[r]) = 1 (i.e., the condition is satisfied), instead of deterministically flipping the s[c]-th bit of representation bits, in the stochastic version, the s[c]-th bit will be flipped with probability 1/2, and nothing happens otherwise.  This resembles the real-world scenario where an action might fail with some failure probability. The representation complexity of the transition kernel $P$ still remains polynomial. For the optimal $Q$ function, if we change $H=2^b + 2n$, since in expectation each flipping will cost two steps, the representation complexity of the $Q$ function is still lower bounded by the majority function, which is exponentially large. Note that our deterministic majority MDP can be viewed as an embedded chain of a more \u2018lazy\u2019 MDP, and the failure probability can also be arbitrary. Hence, our conclusion can be generalized to this natural class of stochastic-transition MDPs.\n\nIn conclusion, our framework can be extended to more complex settings such as stochastic MDP and POMDP, but the main message of this paper can be effectively and more intuitively conveyed through our majority MDP class. We will add these discussions in the revision. Thank you again for the valuable suggestion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700117253817,
                "cdate": 1700117253817,
                "tmdate": 1700117253817,
                "mdate": 1700117253817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LTe4fXNVvW",
                "forum": "3K3s9qxSn7",
                "replyto": "mX0z3ulFNU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8968/Reviewer_zTQ7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8968/Reviewer_zTQ7"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the additional experiments and discussions provided by the authors. Some of my concerns are addressed. And I think the discussions on extending the theories presented in this paper to stochastic MDP and POMDP will be insightful to readers and I recommend the authors to add them in the paper (at least in the appendix).\n\nAs to the computation of the approximation error of the optimal Q-values, I am still not sure whether using the critics of SAC to be the target to fit is a proper choice. I think MC returns are usually considered to be unbiased in expectation while TD3 and SAC main have underestimation errors (since clipped double Q learning is adopted)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574986507,
                "cdate": 1700574986507,
                "tmdate": 1700574986507,
                "mdate": 1700574986507,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7r5ElJTzfK",
            "forum": "3K3s9qxSn7",
            "replyto": "3K3s9qxSn7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8968/Reviewer_9wTj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8968/Reviewer_9wTj"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the representation complexity of model-based and model-free reinforcement learning. It proposes to use circuit complexity and proves that for a large class of MDPs, the representation complexity of models is larger than that of Q-values. Furthermore, the paper conducts several experiments showing that the approximation error of the Q-value function class are typically larger than that of the model class, indicating learning Q-value functions is harder than learning models, which coincides with the intuition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a new perspective of representation complexity in learning MDPs, and the introduction of circuit complexity into this domain seems novel.\n2. Both theoretical and experimental results are solid and sound."
                },
                "weaknesses": {
                    "value": "1. While the majority MDPs is broad, it fails to see if common MDPs are in this class or close to this class such that in many real life applications the circuit complexity of the Q-value function class is higher than that of the model class."
                },
                "questions": {
                    "value": "1. While the circuit complexity is a good starting point to study the representation complexity, is it possible that there are other quantities or metrics such that the representation complexity might behave differently?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8968/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836904420,
            "cdate": 1698836904420,
            "tmdate": 1699637128886,
            "mdate": 1699637128886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PtmVeldbvE",
                "forum": "3K3s9qxSn7",
                "replyto": "7r5ElJTzfK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8968/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8968/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your support of our work and your time reviewing our paper and providing helpful and insightful comments. Below are our responses.\n\n>While the majority MDPs is broad, it fails to see if common MDPs are in this class or close to this class such that in many real life applications the circuit complexity of the Q-value function class is higher than that of the model class.\n\nWe discussed in the global response that many MDPs in real-world applications are actually equivalent (or close) to the majority MDP class. Of course, it is true that we can also construct MDPs of which the model complexity is high while the Q-function is simple, but the main message we aim to convey is that MDPs with simple model structure but complex Q-function are common, and thus using the knowledge of the model (either using a model-based learning algorithm or boosting existing model-free algorithms using the knowledge of the model or by planning) could achieve better performance than a purely model-free algorithm (as also empirically validated by [1]).\n\n>While the circuit complexity is a good starting point to study the representation complexity, is it possible that there are other quantities or metrics such that the representation complexity might behave differently?\n\nWe believe that there exist other quantities that can be used to measure the representation complexity, but circuit complexity is one of the most fundamental metrics as the operations in computers are represented by circuits, and this notion is also widely used in TCS. \n\nNote that many complexity measures, such as VC dimension, can be only applied to a function class instead of a single function, and previous attempts, such as using the number of segments of a piecewise linear function [1], are not applicable to more general functions. Instead, the circuit complexity is applicable to a general function (with bounded precision, which is reasonable and actually necessary in practice). \n\nProposing other reasonable representation complexity measures and studying whether this phenomenon might be different under that measure are definitely interesting future questions for study (We conjecture that the same behavior still happens even under different measures since our experimental results validate this phenomenon using neural networks instead of circuits). We thank the reviewer for raising this point.\n\n**References**\n\n[1] Kefan Dong, Yuping Luo, and Tengyu Ma. On the expressivity of neural networks for deep reinforcement learning. In International Conference on Machine Learning (ICML), 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116945879,
                "cdate": 1700116945879,
                "tmdate": 1700116945879,
                "mdate": 1700116945879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JObzCPaCDi",
                "forum": "3K3s9qxSn7",
                "replyto": "PtmVeldbvE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8968/Reviewer_9wTj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8968/Reviewer_9wTj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. I decide to keep my score to encourage the introduction of new concept in the study of complexity in RL."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8968/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698570238,
                "cdate": 1700698570238,
                "tmdate": 1700698570238,
                "mdate": 1700698570238,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]