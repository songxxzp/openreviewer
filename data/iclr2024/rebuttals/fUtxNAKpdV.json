[
    {
        "title": "Nougat: Neural Optical Understanding for Academic Documents"
    },
    {
        "review": {
            "id": "VEf9P2jfnt",
            "forum": "fUtxNAKpdV",
            "replyto": "fUtxNAKpdV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6127/Reviewer_YnWx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6127/Reviewer_YnWx"
            ],
            "content": {
                "summary": {
                    "value": "This article presents an end-to-end model for converting PDF image files into a markup language, with a focus on accurately reconstructing mathematical formulas. The authors also discuss the preprocessing pipeline used to align PDFs with LaTeX source code and provide details on the datasets used. The model can be applied to other types of scanned document images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The proposed model significantly improves the processing of scientific documents into a markup language.\n\n2. The model performs exceptionally well in parsing mathematical formulas, surpassing algorithm-based OCR engines.\n\n3. The method is simple yet effective."
                },
                "weaknesses": {
                    "value": "1. The lack of novelty in the model architecture\u2019s methodology is a weakness. It suggests that the approach may not bring any new or innovative ideas to the field. It would be better for an industrial track paper.\n\n2. The reported results being limited to English data is a weakness. It is unclear if the model can support other languages, such as Chinese or Japanese. However, it would be interesting to evaluate the transfer learning ability of the model for layout analysis, which could potentially address this limitation.\n\n3. Figure 5 and section 5.4 on repetitions during inference being difficult to follow. It indicates that the explanation or presentation of this aspect of the model may not be clear or well-explained, making it challenging for readers to grasp the concept."
                },
                "questions": {
                    "value": "1. What does \u201cout-of-domain documents\u201d refer to on page 8, and what is the objective of conducting experiments on repetition data?\n\n2. In Eq. X, does \u201ca\u201d or \u201cb\u201d denote a paragraph index?\n\n3. Why were Table, Plain text, and Math treated as separate modalities for experimentation, and how does this differ from conventional methods?\n\n4. Can the model be directly applied to visually rich document understanding tasks?\n\n5. Would it be better to move the first paragraph in Section 3 Model from page 2 to the related work section?\n\n\nOther suggestions:  give eq. no and full stops"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6127/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6127/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6127/Reviewer_YnWx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698301637023,
            "cdate": 1698301637023,
            "tmdate": 1699636663645,
            "mdate": 1699636663645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P8y1jgR85D",
                "forum": "fUtxNAKpdV",
                "replyto": "VEf9P2jfnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6127/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We appreciate your diligence in reviewing our work.\n## Weaknesses\n> The lack of novelty in the model architecture's methodology is a weakness. It suggests that the approach may not bring any new or innovative ideas to the field. It would be better for an industrial track paper.\n\nPlease refer to our general response 1.\n> The reported results being limited to English data is a weakness. It is unclear if the model can support other languages, such as Chinese or Japanese. However, it would be interesting to evaluate the transfer learning ability of the model for layout analysis, which could potentially address this limitation.\n\n\nPlease refer to our general response 2.\n> Figure 5 and section 5.4 on repetitions during inference being difficult to follow. It indicates that the explanation or presentation of this aspect of the model may not be clear or well-explained, making it challenging for readers to grasp the concept.\n\nThank you for your feedback. We will rework this section to make it easier to follow.\n\n## Questions\n> What does \u201cout-of-domain documents\u201d refer to on page 8, and what is the objective of conducting experiments on repetition data?\n\n\u201cout-of-domain documents\u201d are in our case academic documents found on the internet including scanned or digital books (for examples, see in Appendix B: Fig B.1 and Fig B.2). We realize this was not clear in the original text and updated it. Thank you for the pointer.\n> In Eq. X, does \u201ca\u201d or \u201cb\u201d denote a paragraph index?\n\nYes, in Eq. X, \u201ca\u201d or \u201cb\u201d denote a paragraph index.\n> Why were Table, Plain text, and Math treated as separate modalities for experimentation, and how does this differ from conventional methods?\n\nIn our study, we chose to categorize Tables, Math, and Plain Text as distinct text modalities, only for our evaluation process. This distinction was made to gain more detailed insights into the potentially varying results, acknowledging their inherent differences. For instance, the way Tables might yield different outcomes compared to Plain Text is noteworthy. It might be more apt to use the term \"category\" rather than \"modality\". Importantly, this differentiation was applied only in the evaluation phase, not during training. As such, the reference to 'conventional methods' isn't directly applicable here, since our approach didn't alter the training methodology, but rather focused on a more nuanced evaluation strategy.\n\n> Can the model be directly applied to visually rich document understanding tasks?\n\nPlease refer to our general response 3.\n> Would it be better to move the first paragraph in Section 3 Model from page 2 to the related work section?\n\nYes, that would be better, thank you for the suggestion!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673168230,
                "cdate": 1700673168230,
                "tmdate": 1700673168230,
                "mdate": 1700673168230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bbbv7Vag8L",
            "forum": "fUtxNAKpdV",
            "replyto": "fUtxNAKpdV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6127/Reviewer_jN3C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6127/Reviewer_jN3C"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a way to build a system that converts an image of a scientific document to its Markup representation from both modeling and data collection perspectives. The use of Markup as its representation enables to handle texts, math equations, tables uniformly by a single model. The model is a basic end-to-end transformer-based autoregressive model. In order to mitigate the common repetition problem of such models, a simple data augmentation method is proposed. It also describes the data construction procedure to create image-Markdown pairs from arXiv where several practical techniques are required. The experimental results show that the transformer-based model is capable for the task and outperforms a baseline system. The code and pre-trained model will be released on GitHub."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I see there are mainly three contributions in the paper:\n\n1. It proposes to use Markdown to extract structure information from images of scientific documents.\n\nThe use of such representations (e.g. LaTex, html, etc.) in the OCR domain is not novel (e.g. math equation recognition has been studied actively in the community for many years), but proposing to use such representations for this particular domain and tackling the problem seriously should get some credit. If scientific documents stored as images can be converted to Markdown, there should be great values to humanity.\n\n2. It describes the data construction procedure for the task.\n\nGiven the capability of latest ML models, the most challenging part is to find data to train models. Identifying arXiv as a good source for the problem and providing practical procedure to construct a dataset seems to have good values to subsequent studies.\n\n3. It shows a transformer-based end-to-end model works for the task and a simple technique further improves the accuracy.\n\nIt is not very surprising that the Donut-based model works for the in-domain data, but it should have a certain value to show that it works well in the setting. The proposed data augmentation is simple but effective, and good analyses are given."
                },
                "weaknesses": {
                    "value": "I am not very certain about its scientific contributions to the ML community. \n\nIn a sense, this paper narrows down the large problem space of OCR into the limited domain and proposes a specialized solution to it. There is definitely a practical value if we can extract Markdown for scientific documents and it is important to have a technical solution to the problem. However, it is not really surprising that a transformer-based model can do a reasonable job for the task. In my opinion, it is challenging to claim novelty for the adoption of transformer for the task and by releasing code and pre-trained models. In my view, the novelty could be claimed for the problem definition and the data construction procedure. Both may be valuable in a certain field, but I am not very sure how much they are for a top-tier general ML conference. I see a lot of high-quality work in the paper, but I feel ICLR is not the best destination for the paper."
                },
                "questions": {
                    "value": "I think the important details are well described and there is no particular question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655887280,
            "cdate": 1698655887280,
            "tmdate": 1699636663501,
            "mdate": 1699636663501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wVqmjqHems",
                "forum": "fUtxNAKpdV",
                "replyto": "bbbv7Vag8L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6127/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We appreciate your diligence in reviewing our work, however we disagree regarding the lack of novelty, please see our general response 1."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672888801,
                "cdate": 1700672888801,
                "tmdate": 1700672888801,
                "mdate": 1700672888801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VLmXc2ebTX",
                "forum": "fUtxNAKpdV",
                "replyto": "wVqmjqHems",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6127/Reviewer_jN3C"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6127/Reviewer_jN3C"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal. I'm going to have a discussion with other reviewers and make a final proposal."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740283032,
                "cdate": 1700740283032,
                "tmdate": 1700740283032,
                "mdate": 1700740283032,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CLGSAPe75F",
            "forum": "fUtxNAKpdV",
            "replyto": "fUtxNAKpdV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6127/Reviewer_SnU5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6127/Reviewer_SnU5"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new model (Nougat) for automatic understanding of academic documents.\nThe authors highlight the main limitations of SOTA methods in the conversion of scientific articles, mainly in dealing with complex objects like tables & mathematical equations. As a result, Nougat addresses end-to-end scientific article understanding with a focus on math equations. The authors develop a large dataset of scientific articles and train Nougat to predict their Markdown formatting. Results show that Nougat outperforms SOTA approaches by a large margin. Examples of article conversion are also presented in the appendix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experimental parameters are meticulously described in the article, with extensive information covering preprocessing, model architecture, and training parameters. Notably, the authors have made both the code and the model available on GitHub, which strengthens the reproducibility of their work. The level of detail in their descriptions provides confidence in the potential to replicate these results.\n\n- The methodology used to create the training dataset is also well described and will be available on GitHub. Scientific articles from three sources (ArXiv, PubMed, and IDL) are used. The process of automatic conversion from LaTeX to Markdown is explained step by step, with particular emphasis on the quality control measures taken. The appendix describes the dataset in detail.\n\n- Nougat outperforms GROBID by a substantial margin in the recognition of plain text, tables, and mathematical expressions. Notably, the significant improvement in handling mathematical equations is particularly impressive, given that this was a significant weakness of GROBID."
                },
                "weaknesses": {
                    "value": "- Nougat outperforms GROBID by a substantial margin in the recognition of plain text, tables, and mathematical expressions. However, I feel that the results section is lacking a qualitative comparison between GROBID, Nougat-base and Nougat-small, especially on mathematical expressions.\n\n- The GROBID system is not sufficiently described, and authors should comment its performance on plain text (what is LaTeX OCR?)\n\n- Two Nougat models are compared in the paper (base and small). However, the article does not provide a comprehensive comparison of these architectures (performance gain, training and inference times, energy consumption\u2026). Consequently, it leaves a significant question unanswered: whether it\u2019s justifiable to opt for the base model when the small model performs almost as well..\n\n- I found the authors\u2019 perspective on how to deal with repetitions very valuable, as this is a well-recognized challenge when working with transformer models. However, the gain should be measured for the repetition detection module, especially since the threshold is set manually and would be painful to adapt to other models/applications.\n\nFinally, some limitations of this work are highlighted, although I feel that this section could be developed further."
                },
                "questions": {
                    "value": "* Are the scores presented in % for the editdistance metric? This metric is referred to as CER (Character Error Rate) in the document understanding community. \n* This sentence is not clear, as Fig. 2 shows that LaTeX sources are recompiled. In Overleaf you can match paragraphs in the LaTeX source and their localization in the PDF - can this feature be used to simplify this step?\n> Since we are not recompiling the LaTeX sources for each paper, we must heuristically split the source file into parts, which correspond to different pages. To achieve that we are using the embedded text on the PDF page and match it to source text. \n* The label noise strategy is not clear. Is noise added to 10% of labels?\n> This process continues until the newly sampled number is greater than a specified threshold (in this case, 10%)\n* Text vs equations vs tables all have different semantics. Could you comment on which metrics are the most relevant for each text category? It would be interesting to use specific metrics for equation recognition (see the CHROME competition https://www.cs.rit.edu/~rlaz/files/CROHME+TFD%E2%80%932019.pdf)\n* The limitation of 4096 tokens seems low for a full page. Did you encounter pages with a larger number of tokens to predict, and how did you handle them? How would this limitation affect document-level recognition (sliding window/patch?).\n* Is any post-processing strategy used to normalize section/subsection formatting (example: `##` (section) predicted instead of `###` (subsection)) ?\n* Adaptability to other languages? While most publications are written in English, some of them are written in other languages. How much work to fine-tune Nougat? (since the tokenizer, dataset, pre-trained models are all specialized for English) \n* What if repetitions appear inside the page and not at the end? (ex: a sentence is repeated twice, but after that the model continues reading until the end of the page). In this case your method will not work as it will just stop the generation. 1) does this case happen? 2) how would you deal with this issue? \n* Do you handle references to other parts of the page? (Ref to tables, figures in the text?). It might not be crucial when dealing with pages, but it will be useful at document-level.\n* Do you handle references to the original image/document (e.g. select a word in the Markdown to highlight it in the PDF)? This feature could be used by users to check an equation if it does not make any sense, for example.\n* To go beyond recognition, how could Nougat be adapted to a multitask setting with e.g. Visual Question Answering / Neural Machine Translation / Summarization? It would certainly be useful to allow users to ask questions about the article: ask for an explanation, ask for a reference, ask to translate or summarize...\n\nErrors/typo\n* Table A.1. The total does not add up to the number of pages for each source (should be 8,494,841). As a consequence, % for each source are also wrong.\n* Typo p9 (last paragraph of section 5.4) \"to compute the to the end \""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688058279,
            "cdate": 1698688058279,
            "tmdate": 1699636663381,
            "mdate": 1699636663381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W66FXAwPzL",
                "forum": "fUtxNAKpdV",
                "replyto": "CLGSAPe75F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6127/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough review. We agree with your assessments and added more context regarding GROBID in the paper. In Section 5.3, we provide a clarification on the utilization of LaTeX OCR. That might not have been apparent solely from the table, so we included a reference there as well.\n\n> Are the scores presented in % for the editdistance metric? This metric is referred to as CER (Character Error Rate) in the document understanding community.\n\nThank you for the pointer. The scores are not presented in %. We will rename the metric in the paper.\n> Fig. 2 shows that LaTeX sources are recompiled\n\nRecompiling LaTeX sources using SyncTeX could indeed streamline our data generation process, but it comes at the cost of significantly increased computational requirements.\nTo mitigate this, we have opted for a more general approach by utilizing the original PDF files supplied by the authors. This choice allows us to accommodate various data sources, including cases such as PMC documents where no LaTeX source files are provided.\nFig. 2 was only supposed to show the relationship between the tex and pdf files, not to imply that we are recompiling the LaTeX sources for each paper. The image caption does clarify the relationship: \u201cd) the PDF file provided by the authors\u201d\n> The label noise strategy is not clear\n\nThank you for the feedback. The strategy is as follows:\nWith a chance of 10% we change one token in the ground truth text to another randomly selected token. The likelihood of changing two tokens simultaneously is 1%, calculated as (10%)^2. Similarly, the probability diminishes exponentially with each additional token substitution, such that the chance of changing three tokens is 0.1%, and so forth.\n> Text vs equations vs tables all have different semantics. Could you comment on which metrics are the most relevant for each text category?\n\nIn my opinion, the CER is the most important metric for all of the modalities, since we pre-process all equations and tables to reduce ambiguity. However, using a symbolic graph could help give a clearer signal. Thank you for the pointer, we will investigate it in the future.\n> The limitation of 4096 tokens seems low for a full page\n\nOn average a page has 1400 tokens. The 4k context size is more than enough for most of the pages. The only problem we noticed are with large tables, because they are token intensive since the GALACTICA tokenizer was not trained on LaTeX tables.\nWe will leave long context exploration to future work.\n> post-processing strategy used to normalize section/subsection formatting\n\nNo, we don\u2019t have a post-processing strategy for section titles.\n> Adaptability to other languages\n\nPlease refer to our general response 2.\n> What if repetitions appear inside the page and not at the end? (ex: a sentence is repeated twice, but after that the model continues reading until the end of the page). In this case your method will not work as it will just stop the generation. 1) does this case happen? 2) how would you deal with this issue?\n\nIf a sentence is repeated in the document, the model is able to do that as well and continue with the rest of the text. The repetition detection during inference will not stop the generation, because the logits variance does not drop because of it.\n> Do you handle references to other parts of the page\nAll references (to e.g. citations, formulas, tables, figures) are replaced with the corresponding label in the LatexML pre-processing step. So you would need to pair them up again during post-processing.\n> Do you handle references to the original image/document (e.g. select a word in the Markdown to highlight it in the PDF)? This feature could be used by users to check an equation if it does not make any sense, for example.\n> To go beyond recognition, how could Nougat be adapted to a multitask setting with e.g. Visual Question Answering / Neural Machine Translation / Summarization?\n\nThis is a good idea, but we did not look into it yet. One could possibly use the attention maps of the vision transformer to highlight the most important image sections for a given output token. \nWe agree, adapting Nougat to Visual Question Answering would be an interesting direction. Please refer to our general response 3."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673725663,
                "cdate": 1700673725663,
                "tmdate": 1700673725663,
                "mdate": 1700673725663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cHVfwjidI6",
            "forum": "fUtxNAKpdV",
            "replyto": "fUtxNAKpdV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6127/Reviewer_VwBw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6127/Reviewer_VwBw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an image-to-LaTeX model for converting PDF documents into the corresponding LaTeX source. The model comprises of Swin Transformer encoder, followed by a standard text decoder (BART). The paper also describes in detail an involved process of collecting the data to obtain the training dataset, which involves non-trivial steps to clean and split PDFs. The model and the code to generate the dataset are made available, and the model outperforms several existing benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Significance: The paper has an excellent contribution, first and foremost by creating the dataset for pdf-2-latex and describing the methodology and individual steps behind it - these two will greatly benefit the community. The availability of the model is also a significant contribution.\nOriginality: The proposed modelling approach is not original, but the construction of the dataset is.\nQuality & Clarity: The paper is well written and clear, and reproducibility is further fostered by the model / data generation release."
                },
                "weaknesses": {
                    "value": "No significant weaknesses."
                },
                "questions": {
                    "value": "One of the central topics in the results is about the repetition of the same sentence again, an artifact of the greedy sampling, and authors approach this problem by doing a data augmentation to help reduce those repetitions. Is there a particular reason why the authors chose to stick with greedy sampling instead of using Top-K / Top-p sampling, which is a standard approach to reduce the overconfidence during autoregressive decoding? (See ex. \"The curious case of neural text degeneration\")"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6127/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699019652631,
            "cdate": 1699019652631,
            "tmdate": 1699636663247,
            "mdate": 1699636663247,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OyOX8qU7jq",
                "forum": "fUtxNAKpdV",
                "replyto": "cHVfwjidI6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6127/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6127/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful feedback.\n\nIn our OCR problem domain, the task is to predict the next token, and there is only one correct token to be predicted at each step. Given this nature of the task, we opted for greedy decoding as it ensures determinism in our predictions. Nucleus sampling introduces a level of randomness that is unnecessary for our specific use case, as there is a single correct token at each decoding step.\n\nDuring the course of our experimentation, we did explore other sampling techniques such as top-k and top-p. However, we observed that these methods were also prone to introducing repetitions in the generated sequences. In the next version of our paper, we will report those additional observations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6127/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672847536,
                "cdate": 1700672847536,
                "tmdate": 1700672847536,
                "mdate": 1700672847536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]