[
    {
        "title": "Revisit and Outstrip Entity Alignment: A Perspective of Generative Models"
    },
    {
        "review": {
            "id": "MLtqFLqK1Q",
            "forum": "z3dfuRcGAK",
            "replyto": "z3dfuRcGAK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4469/Reviewer_sPPq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4469/Reviewer_sPPq"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a M-VAE based generative approach for embedding-based entity alignment. This paper theoretically justified the plausibility of formulating the problem as a generative task that is akin to what recent GAN-based EA approach has done. Specifically, for the problem with dangling entities, different from prvious work that has focused on detecting such dangling points, this work proposes a novel solution of entity synthesis. Experiments are done on common datasets shared by most prior works on this topic, with fair comparison ensured (by removing entity names, which has been violated by some prior works)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The contributions of this paper are from multiple perspectives. The plausibility of a generative formulation of the problem is theoretically justified. For the recently proposed dangling entity problem, a novel solution of entity synthesis is proposed to fulfill the missing targets in the target-side KG. Evaluation has covered the traditional setting of close-space entity alignment to show the effectiveness from that perspective, and a new, plausible setting is proposed for entity synthesis to show the effectiveness from this new angle of solution. The presented method and experiments look sound."
                },
                "weaknesses": {
                    "value": "While the proposed entity synthesis approach leads to an essentially different solution to dangling entities from the previous dangling detection approaches, I wonder if the proposed approach can still can contribute to (and be compared with) dangling detection.  \nThere is one detail that I might have missed: in the close-space EA setting without considering dangling entities, is there any technique of constrained generation/decoding that ensure the generation to fall into the candidate space?"
                },
                "questions": {
                    "value": "Please see weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4469/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697645642794,
            "cdate": 1697645642794,
            "tmdate": 1699636422532,
            "mdate": 1699636422532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6yXniGXTyz",
                "forum": "z3dfuRcGAK",
                "replyto": "MLtqFLqK1Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4469/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your encouraging comments and insightful observations. We hope the following responses address your concerns:\n\n\n### Weaknesses:\n\n- **I wonder if the proposed approach can still contribute to (and be compared with) dangling detection.**\n\n    Great idea. The methods for detecting dangling entities can be categorized into three groups: (1) Nearest Neighbor Classifier (**NNC**), which trains a classifier to determine whether a source entity lacks a counterpart entity in the target KG; (2) Margin-based Ranking (**MR**), which learns a margin value $\\lambda$. If the embedding distance between a source entity and its nearest neighbor in the target KG exceeds $\\lambda$, this source entity is considered a dangling entity; (3) Background Ranking (**BR**), which regards the dangling entities as background and randomly pushes them away from aligned entities.\n\n    All three types of dangling detection methods heavily rely on the quality of entity embeddings. Therefore, if the proposed GEEA learns better embeddings for entity alignment, it is expected to contribute to the detection of dangling entities. To verify this idea, we conducted experiments on new datasets, following [1]. We used the same parameter settings and employed MTransE as the backbone model. The results are presented in the table below:\n\n    | Methods  | DBP 2.0 ZH-EN Precision | DBP 2.0 ZH-EN Recall | DBP 2.0 ZH-EN F1 | DBP 2.0 JA-EN Precision | DBP 2.0 JA-EN Recall | DBP 2.0 JA-EN F1 | DBP 2.0 FR-EN Precision | DBP 2.0 FR-EN Recall | DBP 2.0 FR-EN F1 |\n    |---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n    | NNC  | .604 | .485 | .538 | .622 | .491 | .549 | .459 | .447 | .453 |\n    | GEEA (NNC)  | .617 | .509 | .558 | .637 | .460 | .534 | .479 | .449 | .464 |\n    | MR  | .781 | .702 | .740 | .799 | .708 | .751 | .482 | .575 | .524 |\n    | GEEA (MR)  | .793 | .709 | .749 | .812 | .714 | .760 | .508 | .594 | .548 |\n    | BR  | *.811* | **.728** | *.767* | *.816* | *.733* | *.772* | *.539* | *.686* | *.604* |\n    | GEEA (BR)  | **.821** | .724 | **.769** | **.833** | **.735** | **.781** | **.549** | **.694** | **.613** |\n\n    Clearly, incorporating GEEA leads to significant performance improvements in dangling entity detection across all three datasets. The performance gains are particularly notable in terms of precision and F1 metrics. We have added the dangling entity detection experiments in the revision.\n\n- **In the close-space EA setting without considering dangling entities, is there any technique of constrained generation/decoding that ensures the generation to fall into the candidate space?**\n\n    The reparameter trick of VAEs, prior reconstruction, and post reconstruction ensure the generation of new entities within the target space.\n\n    We apply the reparameter trick by adding a small noise vector to the latent embedding in VAEs. The latent embedding is then converted back to the input embedding. This process allows the model to learn the conversion between distributions rather than fixed features.\n\n    Recall that we consider four different reconstruction flows. Take x\u2192y as an example, we use the entity alignment training set as supervised data, where each example is in the form of (x, y), representing a pair of aligned entities. The model takes the raw features of x from the source KG as input and reproduces the raw features of its counterpart y as output. We minimize the difference between the generated features and the true features of y as the prior reconstruction loss.\n\n    Additionally, we introduce post reconstruction. This involves reinputting the output embeddings of sub-VAEs to the fusion layer, resulting in a generated joint embedding. We then minimize the difference between this generated joint embedding and the joint embedding of y to ensure a consistent generation process across different modalities.\n\n\n[1] \"Knowing the no-match: entity alignment with dangling cases.\" ACL, 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699865149373,
                "cdate": 1699865149373,
                "tmdate": 1699865149373,
                "mdate": 1699865149373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rJbWM9wLI0",
                "forum": "z3dfuRcGAK",
                "replyto": "6yXniGXTyz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4469/Reviewer_sPPq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4469/Reviewer_sPPq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the thorough response. It would be nice to have these results in the draft. Since my score is already 8, I keep it unchanged."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713925512,
                "cdate": 1700713925512,
                "tmdate": 1700713925512,
                "mdate": 1700713925512,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q5l1S6AWXg",
            "forum": "z3dfuRcGAK",
            "replyto": "z3dfuRcGAK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4469/Reviewer_ALxa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4469/Reviewer_ALxa"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the task of entity alignment. The authors introduce a generative EEA framework, leveraging the mutual variational autoencoder (M-VAE) to facilitate the encoding and decoding of entities between source and target KGs. A series of experiments have been executed to ascertain the efficacy of the GEEA, and the results affirm the prowess of the model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experiments showcased provide evidence of the efficacy of the GEEA model."
                },
                "weaknesses": {
                    "value": "- The paper would benefit from enhanced clarity. Several concepts are mentioned without a clear definition, leading to potential confusion for readers. See the questions listed below for specifics. \n- The objective of prior reconstruction could be made more comprehensible. There's ambiguity regarding the priors of features in different sub-embeddings. Specifically, when dealing with images, how does one retrieve its original, tangible feature?\n- The paper could provide a more extensive set of experiments to offer insights into the rationale behind the design of individual components.\n\n- Eq. (19)  is missing a right parenthesis."
                },
                "questions": {
                    "value": "- Could you elaborate on what constitutes the multi-modal information within the knowledge graph (KG) area?\n- What types of attribute information are being referred to in this context?\n- How would you define \"sub-embeddings\" in the framework?\n- What does $\\mathcal{L}_{mf}$ represent within the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4469/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4469/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4469/Reviewer_ALxa"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4469/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618627806,
            "cdate": 1698618627806,
            "tmdate": 1700778710816,
            "mdate": 1700778710816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gbNhohIkFx",
                "forum": "z3dfuRcGAK",
                "replyto": "Q5l1S6AWXg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4469/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for your helpful feedback and suggestions. We will carefully incorporate them into our paper.\n\n### Weaknesses:\n\n- **Several concepts are mentioned without a clear definition as listed in **Questions**.**\n\n    We appreciate your constructive feedback and hope that our responses to **Questions** have adequately addressed your concerns.\n\n- **The objective of prior reconstruction: when dealing with images, how does one retrieve its original, tangible feature?**\n\n    Sorry for the confusion. As stated in Section 3.6 Implementation, we move the details of reconstructing the tangible features of each module to Appendix B due to space constraints. Specifically, the reconstruction of neighborhood and attribute information can be viewed as multi-label classification. As for image reconstruction, we also hightlight the process in Section 4.3 Entity Synthesis Results. We state, \"Since the image features are pretrained embeddings of a vision model, we use the image corresponding to the nearest neighbor of the reconstructed feature as the output image.\" In the revision, we have added a breif introduction to the reconstruction of images in Section 3.6.\n\n- **The paper could provide a more extensive set of experiments to offer insights into the rationale behind the design of individual components.**\n\n    Thanks for your suggestion. We conducted ablation studies to assess the effectiveness of each component and present the results in Table 5. Specifically, we evaluated various alternative methods by removing different components while keeping all parameter settings identical to GEEA. We assessed the performance on both tasks and found that the complete GEEA achieved the best results, with any module removal leading to a performance loss.\n\n    Additionally, we have incorporated several new experiments in the revision, thanks to the valuable comments from all reviewers. These experiments contribute to a better understanding of our method. Please let us know if you have any further suggestions.\n\n- **Eq. (19) is missing a right parenthesis.**\n\n    Thank you very much. We have fixed it.\n\n### Questions:\n\n- **Could you elaborate on what constitutes the multi-modal information within the knowledge graph (KG) area?**\n\n    Sorry for the confusion. We have clarified the description in Section 4.1 Experiment Settings in the revised version. Our paper considers three different modalities: relational graphs (including the relations and entities linked to each entity), (textual) attributes, and images. We also present the statistics of each modality in each dataset in Table 7.\n \n- **What types of attribute information are being referred to in this context?**\n\n    Similar to previous works, we treat attribute information as bags of words and represent them using pre-trained word embeddings.\n\n- **How would you define \"sub-embeddings\" in the framework?**\n\n    We have updated Section 2.1 Embedding-based Entity Alignment to include the definitions of \"sub-embedding\" and \"joint embedding\" in the revised version. The term \"sub-embeddings\" is introduced in the introduction section (the second paragraph and Figure 1). It refers to the output embeddings generated by different encoders. \"The encoder module uses different encoders to encode multi-modal information into low-dimensional embeddings. The fusion layer then combines these sub-embeddings to a joint embedding as the output.\" \n\n\n- **What does $\\mathcal{L}_{m_f}$ represent within the paper?**\n\n    The reviewer may have overlooked the explanation on the next page, specifically after Equation (22). \"where $\\mathcal{F} = \\{x\\rightarrow x, y\\rightarrow y, x\\rightarrow y, y\\rightarrow x\\}$ is the set of all flows, and $\\{g, a, i, ...\\}$ is the set of all available modalities.\"  In this context, $m$ denotes the modality (e.g, $g$), and f denotes the flow (e.g., $x\\rightarrow y$). $\\mathcal{L}_{m_f}$ is similar to\n\n    $\\mathcal{L}_{g, x\\rightarrow y}$, which is defined in Equation (19) (We have to insert a newline here because the original Latex formula collapsed)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699865110790,
                "cdate": 1699865110790,
                "tmdate": 1699866975293,
                "mdate": 1699866975293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PC1VsMSsOs",
                "forum": "z3dfuRcGAK",
                "replyto": "gbNhohIkFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4469/Reviewer_ALxa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4469/Reviewer_ALxa"
                ],
                "content": {
                    "title": {
                        "value": "Thank you to the authors for their detailed response"
                    },
                    "comment": {
                        "value": "Thank you to the authors for their detailed response. Your revisions have addressed some of my questions and concerns, and have enhance the paper's clarity. As a result, I am considering increasing my review score. However, further improvements are necessary for the paper to reach the acceptance threshold for ICLR. Specifically, there are still numerous instances of improper writing and grammatical errors. To highlight a few:\n\n1. Improper use of references. The first sentence in the Introduction cites over 15 papers, which seems excessive and unnecessary.\n2. Some grammar issues:\n- The phrase \"are friendly to development and deployment\" should be revised to \"are friendly for development and deployment.\"\n- \"low-dimension vectors\" should be corrected to \"low-dimensional vectors.\"\n- Clarification is needed regarding which models are referred to in some \"existing EEA models.\" or \"existing EEA works\"\n- \"one major issue with the existing GAN-based method is mode collapse\" so this issue is only for one model proposed by (Srivastava et al.,2017)? How about other models? Should it be the \"existing GAN-based methods\"?\n\nFurthermore, the definitions of modalities (i.e., relational graph information, attribute information, and image information) are presented in an unclear manner. It is not immediately apparent how images can be represented as discrete features.\n\nThe paper could also benefit from a stronger motivation, particularly regarding the necessity of a generative approach to the EA task. Additionally, the rationale behind the Entity Synthesis task should be more thoroughly explained in the introduction. How it related to the EA task?\n\nLastly, the structure of the paper requires refinement. Essential preliminary knowledge is introduced after Section 2, which already employs the VAE for modeling. This sequence may lead to confusion for readers unfamiliar with the background concepts."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346920953,
                "cdate": 1700346920953,
                "tmdate": 1700346920953,
                "mdate": 1700346920953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ug9ks02fFe",
            "forum": "z3dfuRcGAK",
            "replyto": "z3dfuRcGAK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4469/Reviewer_gcSh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4469/Reviewer_gcSh"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce the entity synthesis task and propose an M-VAE model that can convert entity embeddings back to the native concrete features. They also propose prior reconstruction loss and post-reconstruction loss to control the generation process. Empirical results show that entity synthesis has a positive effect on entity alignment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Through theoretical analysis from the perspective of generative models, the authors point out that generative objectives contribute to the optimization of EEA models.\n2. A generative EEA framework is proposed. By introducing reconstruction loss and distribution matching loss, GEEA further improves the performance of previous EEA models."
                },
                "weaknesses": {
                    "value": "The author needs to briefly introduce the metrics of alignment prediction. Is it consistent with the EEA model used in GEEA?\n\nThe proposed M-VAE model is a generative model, but there are no other generative models compared to baseline models. It's better to compare with some GAN-based models like NeoEA(\"Understanding and improving knowledge graph embedding for entity alignment.\" International Conference on Machine Learning. PMLR, 2022.)"
                },
                "questions": {
                    "value": "GEEA is a general method, but all experiments are completed in multi-modal settings. In a single-modal scenario, will GEEA still be competitive?\nThe time and memory overhead of GEEA should be reported. \nThe number of entities in each data set is 15k. Can GEEA be run on larger datasets(such as 100k)?\nAdditionally, there are some other problems:\nThere are two \"right hand\" in the above line of Eq.43, one of them should be \"left hand\".\nThe weight of prediction matching loss is not included in Table 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4469/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4469/Reviewer_gcSh",
                        "ICLR.cc/2024/Conference/Submission4469/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4469/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698934399973,
            "cdate": 1698934399973,
            "tmdate": 1700670063880,
            "mdate": 1700670063880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eMOg3k71Hv",
                "forum": "z3dfuRcGAK",
                "replyto": "Ug9ks02fFe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4469/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your detailed and insightful comments. We have addressed your concerns below and hope our responses provide clarity:\n\n### Weaknesses:\n\n- **The author needs to briefly introduce the metrics of alignment prediction. Is it consistent with the EEA model used in GEEA?**\n\n    Many thanks. We have incorporated an introduction to the metrics employed in entity alignment experiments, which align with those used in the EEA models. Specifically, we use Hits@1 and Hits@10 to assess the proportion of target entities that appear within the top 1 and top 10, respectively. We also employ MRR to measure the mean reciprocal ranks of the target entities.\n\n\n- **It's better to compare with some GAN-based models like NeoEA [1].**\n\n    Great idea. NeoEA is an impressive work which we have introduced in Section 2.3 and Section 5. However, it is important to note that NeoEA is not a typical generative model as it does not have the ability to generate new entities. Instead, NeoEA trains a discriminator to distinguish between entities from the source KG and others. It achieves this by sampling entities from the respective KGs and employing a Wasserstein-distance-based loss to train the discriminator and the EEA model in an adversarial manner.\n\n    To compare the performance of the proposed GEEA with NeoEA, we adapted NeoEA to MCLEA (also used in GEEA) based on the source code provided in its official repository. The results are presented in the following table:\n\n\n    | Methods  | ZH-EN Hits@1 | ZH-EN Hits@10 | ZH-EN MRR | JA-EN Hits@1 | JA-EN Hits@10 | JA-EN MRR | FR-EN Hits@1 | FR-EN Hits@10 | FR-EN MRR |\n    |---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n    | EVA  | .680 | .910 | .762 | .673 | .908 | .757 | .683 | *.923* | .767 |\n    | MSNEA  | .601 | .830 | .684 | .535 | .775 | .617 |.543 | .801 | .630 |\n    | MCLEA  | .715 | .923 | .788 | .715 | .909 | .785 | .711 | .909 | .782|\n    | NeoEA (MCLEA)  | *.723* | *.924* | *.796* | *.721* | .909 | *.789* | *.717* | .910 | *.787* |\n    | GEEA  | **.761** | **.946** | **.827** | **.755** | **.953** | **.827** | **.776** | **.962** | **.844** |\n\n\n    Although NeoEA slightly improves the performance of MCLEA, the results are still significantly lower compared to those achieved by GEEA. We believe this is due to the fact that NeoEA was primarily designed for single-modal models, focusing on enhancing relational graph embedding only.\n\n\n### Questions:\n\n- **In a single-modal scenario, will GEEA still be competitive?**\n\n    We first remove the image encoder from multi-modal EEA models. The results are shown in the following table:\n\n    | Methods  | ZH-EN Hits@1 | ZH-EN Hits@10 | ZH-EN MRR | JA-EN Hits@1 | JA-EN Hits@10 | JA-EN MRR | FR-EN Hits@1 | FR-EN Hits@10 | FR-EN MRR |\n    |---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n    | EVA  | .680 | .910 | .762 | .673 | .908 | .757 | .683 | *.923* | .767 |\n    | MSNEA  | .601 | .830 | .684 | .535 | .775 | .617 |.543 | .801 | .630 |\n    | MCLEA  | *.715* | .923 | *.788* | *.715* | .909 | *.785* | .711 | .*909* | .782|\n    | MCLEA w/o image  | .658 | .915 | .726 | .662 | .904 | .740 | .662 | .902 | .747 |\n    | GEEA  | **.761** | **.946** | **.827** | **.755** | **.953** | **.827** | **.776** | .**962** | **.844** |\n    | GEEA w/o image | .709 | *.929* | .782 | .708 | *.935* | .784 | *.717* | *.946* | *.796* |\n\n\n    Notably, our GEEA without the image encoder still achieves state-of-the-art performance on several metrics, such as Hits@10.\n\n\n- **Can GEEA be run on larger datasets(such as 100k)?**\n\n    Then, the OpenEA 100K datasets [2] do not have a multi-modal version. However, it is still interesting to explore the performance of GEEA with single-modal EEA models on OpenEA 100K, similar to NeoEA. We conducted experiments following the NeoEA and present the results in the following table:\n\n    | Methods  | EN-FR 100K Hits@1 | EN-FR 100K MRR | EN-DE 100K Hits@1 | EN-DE 100K MRR | DBPedia-WikiData Hits@1 | DBPedia-WikiData MRR | DBPedia-Yago Hits@1 | DBPedia-Yago MRR |\n    |---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n    | SEA  | .225 | .314 | .341 | .421 | .291 | .378 | .490 | .578 |\n    | NeoEA (SEA)  | *.254* | *.345* | *.364* | *.446* | *.325* | *.416* |*.569* | *.651* |\n    | GEEA (SEA) | **.269** | **.355** | **.377** | **.459** | **.349** | **.436** | **.597** | .**685** | \n\n    It is clear that our method can significantly enhance the performance of SEA, which can be attributed to the more stringent objectives analyzed in Section 2. We have also included all the aforementioned tables in Appendix C of the revision."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699865020594,
                "cdate": 1699865020594,
                "tmdate": 1699865020594,
                "mdate": 1699865020594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "558AH1Y3Pa",
                "forum": "z3dfuRcGAK",
                "replyto": "Ug9ks02fFe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4469/Reviewer_gcSh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4469/Reviewer_gcSh"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. I would like to increase my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670030631,
                "cdate": 1700670030631,
                "tmdate": 1700670030631,
                "mdate": 1700670030631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]