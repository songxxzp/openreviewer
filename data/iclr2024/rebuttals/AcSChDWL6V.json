[
    {
        "title": "Transformers vs. Message Passing GNNs: Distinguished in Uniform"
    },
    {
        "review": {
            "id": "UXcNVQI15q",
            "forum": "AcSChDWL6V",
            "replyto": "AcSChDWL6V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5555/Reviewer_MBi3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5555/Reviewer_MBi3"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the expressiveness of graph transformers and message-passing gnns. As its first step, the authors exploit the universality of MLP and the property of positional encoding, i.e., injective up to isomorphism, to prove the non-uniform universality of graph transformer and message-passing gnns. Then, the scope of discussion is extended to uniform expressivity, that is, whether these kinds of neural architectures can approximate arbitrary function no matter how large the input graph is. Basically, the authors show that both graph transformers and message-passing gnns are not universal approximator in this setting. Moreover, they offer important insight that these two kinds of neural architectures do not subsume each other. Specifically, graph transformers cannot perform unbounded aggregation, yet its attention mechanism allows asymmetric weighting of incoming messages. Accordingly, the authors design and conduct experiments on synthetic datasets to validate their theoretical results. On practical datasets, these models are also comparable, especially the virtual node trick, which helps a lot for message-passing gnns. In summary, this paper tells the community that attention is NOT all you need in graph learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper is well-written. I can effortlessly pick the main points up.\n2.\tThe theoretical results introduced in this paper seem to be crucial for developing machine learning models dedicated to graph data. Notably, these results explain some interesting phenomena emerging in recent years, including the superiority of graph transformers in some competitions, the surprising usefulness of virtual node trick, and the existence of some real-world datasets on which message-passing gnns are still state-of-the-art.\n3.\tThe experiments are convincing, where the difference between these two kinds of neural architectures is remarkable."
                },
                "weaknesses": {
                    "value": "1.\tIt seems that the presented theoretical results in the non-uniform are relatively trivial, as they are straightforward results of the combination of MLP\u2019s universality and PE\u2019s discrimination capacity.\n2.\tThe difference and respective advantages deserve to be connected to practical tasks on molecular graphs, as there have been many public tasks, some of which graph transformers outperform traditional message-passing gnns, yet some are not. Such connections must be helpful for the community and make the theoretical results practical."
                },
                "questions": {
                    "value": "In the synthetic experiments, the authors said they were interested in the generalization behavior of the train models. However, the setting is not as usual. It is not an i.i.d. generalization but o.o.d. extrapolation (graph size <= 50 during training and > 50 in test). Generally and intuitively, a model that is more sophisticated with o.o.d. extrapolation is often due to its more reasonable inductive bias or limited hypothesis space, such that it captures the underlying actual mapping rather than fitting the training data by other consistent yet different mappings. Thus, I need clarification about the rationale behind experimental design. Could you explain this to me?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697683771845,
            "cdate": 1697683771845,
            "tmdate": 1699636571046,
            "mdate": 1699636571046,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b8ySaJ1D4b",
                "forum": "AcSChDWL6V",
                "replyto": "UXcNVQI15q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. Please find below our response to your question:\n\nBy \"generalization\" we mean correctly handling graphs of sizes larger than seen during training, which is indeed generalization in the o.o.d. sense. We hypothesize that the latter corresponds with uniform expressivity i.e.:\n1) That uniform inexpressivity implies an extremely low probability of learning a GNN that generalizes (to o.o.d. graph sizes).\n2) That there is a good probability of learning uniformly expressive networks (which by definition generalize) when they exist.\n\nIf our hypothesis is true, then differences (between architectures) in uniform expressivity are meaningful in practice, making theoretical results regarding uniform expressivity practically relevant. Our synthetic-data experiments are meant to examine the above:\n1) To test the magnitude of the generalization error of a learned GNN when the architecture is uniformly inexpressive of the target function.\n2) To test how well we can learn from data (using SGD) a well-generalizing GNN, when a uniformly expressive GNN is known to exist.\n\nWe will try to add clarification regarding the above, in the updated manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146022510,
                "cdate": 1700146022510,
                "tmdate": 1700146022510,
                "mdate": 1700146022510,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WvPwzj7t5i",
                "forum": "AcSChDWL6V",
                "replyto": "b8ySaJ1D4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5555/Reviewer_MBi3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5555/Reviewer_MBi3"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for your detailed explanation. I will keep the rating unchanged."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554184452,
                "cdate": 1700554184452,
                "tmdate": 1700554184452,
                "mdate": 1700554184452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AQrtW4KCW8",
            "forum": "AcSChDWL6V",
            "replyto": "AcSChDWL6V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5555/Reviewer_EnRw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5555/Reviewer_EnRw"
            ],
            "content": {
                "summary": {
                    "value": "Summary: This work can be contextualized along a recent line of study in graph learning which is focused on comparing graph transformers (GTs) and message passing GNNs (MPGNNs) and finding out which is better and why. This paper in particular theoretically and empirically compares the expressive power of GTs and MPGNNs with virtual nodes (MPGNN+VNs) in the uniform setting where a single model must work for graphs of all sizes. It shows that neither model is uniformly universal, but they can express some unique functions, making their expressive power incomparable. \n\nthe paper's contributions:\n- Presents important insights that can be useful to understand the working capabilities of GTs and MPGNNs.\n- Proves that GTs and MPGNN+VNs cannot uniformly approximate every computable graph function, even with polynomial-time positional encodings.\n- Shows GTs cannot uniformly approximate unbounded summation like |V|2, while MPGNN+VNs can.\n- Shows MPGNN+VNs cannot uniformly approximate functions exploiting softmax attention's asymmetric neighbor weighting, while GTs can."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n- The paper makes an important theoretical contribution by formally proving distinguishing functions for GTs and MPGNN+VNs. This helps characterize their expressive power, and informs more about the strengths and weaknesses of these two model class in addition to what is known in recent literature (eg Cai et al., 2023). \n- The proofs identifying unique functions are non-trivial and provide insight into the core operations enabling GTs and MPGNN+VNs to express different functions.\n- The theoretical findings are verified through special designed experiments on synthetic data, showing the distinguishing functions are learnable in practice.\n- Experiments on real-world benchmarks demonstrate MPGNN+VNs can be competitive with GTs in some cases, due to global communication via virtual nodes. however, this is known in the literature, to the best of my understanding"
                },
                "weaknesses": {
                    "value": "Limitations and Questions:\n- The theoretical analysis focuses on comparing one variant of GTs (GPS) and MPGNN+VNs. Results could vary for different architectures within these families. How accurate would this generalization be?\n- On some realworld datasets, MPGNN+VNs do not fully close the performance gap compared to GTs. It is unclear if this limitation is fundamental or if deeper MPGNN+VNs could match GTs."
                },
                "questions": {
                    "value": "in the Weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698485819174,
            "cdate": 1698485819174,
            "tmdate": 1699636570932,
            "mdate": 1699636570932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NV7LaU8Ag5",
                "forum": "AcSChDWL6V",
                "replyto": "AQrtW4KCW8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough review. Below, we comment on the two weaknesses pointed out in the review. \n\n**Q1**: The theoretical analysis focuses on comparing one variant of GTs (GPS) and MPGNN+VNs. Results could vary for different architectures within these families. How accurate would this generalization be?\\\n**A1**: The first question regards the families of MPGNN+VN and GT architectures to which our theory applies. \nFormally reasoning about these architectures naturally requires us to fix some specific design choices, but we would argue that both GPS and MPGNN+VN represent very large and practically relevant classes of models.\nIn particular, our theory is completely agnostic to the choice of the message passing module and holds true for any popular MPGNN layer such as GCN, GIN, GAT, or GatedGCN.\nThis is clear when looking at the graphs we use to prove the differences because they both contain no edges so no messages are sent during the message passing step.\\\nThe intuition underlying Theorem 4.3 and Corollary 4.4 is that weighted averages like softmax self-attention struggle to uniformly approximate functions that depend on the absolute number of vertices.\nWe expect that this generalize to most graph transformers with global information exchange built on such weighted averages.\nOf course, the formal proof details may need to be adapted for GTs outside of the GPS framework.\nWe will update the camera-ready version to make this part clearer.\n\n**Q2**: On some realworld datasets, MPGNN+VNs do not fully close the performance gap compared to GTs. It is unclear if this limitation is fundamental or if deeper MPGNN+VNs could match GTs.\\\n**A2**: This is a broad, but highly important question, we are also very interested in. At least in our experiments, going deeper (while keeping the parameter limits of LRGB) did not lead to better results. Most notably, there is a gap on the vision dataset COCO where pure vision transformers achieve even better results. Nevertheless, it is an intriguing question what aspects are key to the vastly different performance of the various graph learning models on that dataset."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141481279,
                "cdate": 1700141481279,
                "tmdate": 1700141481279,
                "mdate": 1700141481279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cCyLBEc8fo",
            "forum": "AcSChDWL6V",
            "replyto": "AcSChDWL6V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5555/Reviewer_BMpU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5555/Reviewer_BMpU"
            ],
            "content": {
                "summary": {
                    "value": "This paper comprehensively compares the expressivity of Graph Transformers and Message-Passing GNNs. \n\nThis paper presents the following conclusions:\n\n1) Neither Graph Transformers nor Message-Passing GNNs are universal in the uniform setting. \n\n2) There are functions that Graph Transformers can express while Message-Passing GNNs with virtual nodes can not.\n\n3) Even with perfect positional encoding, the expressiveness of Graph Transformers and MPGNNs with virtual nodes differs substantially. \n\nThis paper conducts experiments on real data and synthetic data to verify the theoretical analysis proposed in the paper.\n\n---- After rebuttal---\nThanks for the authors' response. My main concern is strong assumption of the uniform setting and the experimental results.\n\u00a0\nFor the strong assumption of the uniform setting, the author explained that\n\ufeffthe uniform setting is a good representation of scenarios because the graph sizes at\ntraining time are smaller than the graph sizes during inference. I generally agree with the author's response.\n\u00a0\nFor the novelty. The author reclaimed their novelty, which is not summarized (or even mentioned) in the introduction. Now, the author summarize this paper's novelty about the theoretical provements and the proposed synthetic data. I agree these two thing are new. However, the experimental results on real data still exists. The authors can carefully fix the minor mistakes or typos in their revised version.\n\u00a0\nGiven the promise that the author will add the detailed proof and more analysis on the real dataset, I raise the score from reject to broadline accept."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This paper relates two common designs in graphs, i.e., graph transformer and message-passing GNN. \n\n2) This paper gives a theoretical analysis of the capabilities\tof two basic GNN designs and conducts comprehensive experiments to verify the theoretical analysis.\n\n3) The study provides insightful results that add depth to our understanding of the expressiveness of Graph Transformers and MPGNNs in scenarios with optimal positional encoding.\n\n4) The differentiation in the capabilities of Graph Transformers and MPGNNs is well highlighted, offering clarity on their respective strengths and limitations."
                },
                "weaknesses": {
                    "value": "The important concern is the writing. Although this is primarily a theoretical paper, it does not express its flow of proof clearly. I recommend the author to improve their writing.\n\n1) The most important weakness of this paper is writing. The introduction is not easy to understand.\n\n2\uff09**Lack of Justification**: The paper focuses on scenarios where positional encoding is injective. However, there is a noticeable lack of justification for why this particular scenario is important or realistic. To ensure that the results derived hold value in practical applications, it is essential to provide a clear context and relevance for the chosen scenario.\n\n3) The process of proof is not so clear. For example, in Section 4.2, the author attempts to prove GPS do not subsume MPGNN+VNs. However, I can not understand the main path of proof of Theorem 4.3 and Corollary 4.4.\n\n4) The strong assumption of uniform setting made in the paper may not align with the practical case.\n\n5) **Redundancy in Experimental Results**: Similar experimental outcomes have been presented in multiple prior works, notably:\n    - T\u00f6nshoff, Jan, et al. \"Where did the gap go? Reassessing the long-range graph benchmark.\" arXiv preprint arXiv:2309.00367 (2023).\n    - Cai, Chen, et al. \"On the connection between MPNN and graph transformer.\" arXiv preprint arXiv:2301.11956 (2023).\n   While building upon prior work is a hallmark of research progression, it's crucial to ensure that the presented findings either provide a novel perspective or build significantly upon the existing literature.\n\nGiven the value of the derived results, this paper can contribute substantially to the field with some revisions. I recommend a clear justification for the chosen scenario of positional encoding and a distinction between the presented findings and existing literature. This would fortify the paper's novelty and relevance, making it a more significant contribution to the domain."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5555/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5555/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5555/Reviewer_BMpU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762141304,
            "cdate": 1698762141304,
            "tmdate": 1700806610246,
            "mdate": 1700806610246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pqPGtYePfG",
                "forum": "AcSChDWL6V",
                "replyto": "cCyLBEc8fo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. \nPlease find below our clarifications and responses to some of your comments.\n\n**Q**: Weaknesses (1): \"The most important weakness of this paper is writing. ...\"\\\n**A**: We agree that the writing can be improved throughout the manuscript and will address this in the revised version. We will also update the paper with a clearer introduction.\n\n**Q**: Weaknesses (2): \"the paper focuses on scenarios where positional encoding is injective...\"\\\n**A**: We consider injective positional encoding only for section 3, where we extend known results in the non-uniform expressivity setting. The main focus of the paper is the uniform expressivity setting, discussed mainly in section 4. Our results there are *not limited to* having (injective) positional encodings. Rather, the inexpressivity results for GPS hold *even when* (pg 7, lines 1;4;8) the initial graph is featured with positional encodings.\nWe will review the presentation to see how these points can be made clearer in the camera-ready version.\n\n**Q**: Weaknesses (3): \"the process of the proof is not so clear...\"\\\n**A**: We assume that you refer to the proof overview (in the main part), we agree that it should be made clearer, and we intend to do so in the camera-ready version.  \n\n**Q**: Weaknesses (4): \"The strong assumption of uniform setting...\"\\\n**A**: We argue that the uniform setting is highly relevant to practice for the following reasons:\nAs demonstrated in our experiments, the uniform setting is a good representation of scenarios where the graph sizes at training time are smaller than the graph sizes during inference. In practice, it is desirable to learn models that robustly infer missing labels on data outside of the training distribution.\nFurthermore, due to compute-resource limitations and relevancy requirements, it may be desired not to re-train whenever new graphs are larger than seen before e.g. when the system modeled by the graphs is growing over time.\nWe agree that a justification like the above should be mentioned explicitly in the paper and we intend to add it in the camera-ready version.\n\n**Q**: Weaknesses (5): \"Redundancy in Experimental Results...\"\\\n**A**: First, we would like to clarify that the experimental part of this work is meant to complement the theoretical part, it is *a* contribution but not the main contribution.\nThe core contributions of our experiments are two-fold:\n1) Our study on synthetic data is new. It provides the first direct empirical comparison of graph transformers and virtual nodes with regard to uniform function approximation.\n2) Our experiments on real-world data provide a thorough evaluation of MPGNNs with virtual nodes on LRGB and other benchmark datasets.\nThere is some overlap with the experimental setup of Cai et al., but we should point out that their models do not adhere to the official 500k parameter budget and are comparatively under-tuned, as our configurations yield better results with smaller models. \nT\u00f6nshoff et al. identify some weaknesses in the LRGB results reported for MPGNNs and provide improved values, but only for models without VN. \nOur experiments extend their methodology to MPGNNs *with* VN to obtain novel and improved results for this class of models.\n\nWe will improve the presentation of the experiments in the updated manuscript to highlight this more clearly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139719572,
                "cdate": 1700139719572,
                "tmdate": 1700139719572,
                "mdate": 1700139719572,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SpwxZur6bZ",
                "forum": "AcSChDWL6V",
                "replyto": "pqPGtYePfG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5555/Reviewer_BMpU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5555/Reviewer_BMpU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I would like to raise some follow-ups:\n\n**On the Choice of Positional Encoding in Uniform vs. Non-Uniform Settings:**\nCould you elaborate on the decision to utilize injective positional encoding in the uniform setting and PTIME positional encoding in the non-uniform setting? What is the theoretical or practical significance of applying different strengths of positional encoding in these distinct scenarios?\n\n**Comparative Strength of Positional Encodings:**\nIt appears that injective positional encoding is more robust compared to PTIME positional encoding. Can you clarify whether this is indeed the case, and if so, why the stronger encoding was not applied to the more challenging non-uniform case?\n\n**Impact on Expressiveness and Generalization:**\nHow does the choice of positional encoding impact the expressiveness and generalizability of the graph transformers and message-passing GNNs in practical applications? Is there a trade-off that is being optimized by using injective encoding in one scenario over the other?\n\n**Consistency in Methodology:**\nWould the results and conclusions presented in the paper benefit from a consistent application of positional encoding across both settings? Could this potentially highlight differences in expressiveness due to the model architecture rather than the encoding strength?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660548307,
                "cdate": 1700660548307,
                "tmdate": 1700660548307,
                "mdate": 1700660548307,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wZZM28EbuX",
            "forum": "AcSChDWL6V",
            "replyto": "AcSChDWL6V",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5555/Reviewer_Uru7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5555/Reviewer_Uru7"
            ],
            "content": {
                "summary": {
                    "value": "This paper compares model expressivity between graph transformers (GT) and massage passing GNNs with virtual nodes (MPGNN+VN). The authors theoretically demonstrate that GT and MPGNN+VN are universal function approximators on the graph under uniform setups, where different neural networks can be utilized for every graph size. However, under the non-uniform case where one neural network is supposed to work for all the graphs, both are not universal approximators and express different sets of functions, indicating they do not subsume each other. The authors also conduct numerical experiments to validate the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "First of all, thank the authors for their submission to ICLR. This paper offers valuable insights into the expressive power of graph neural networks. Specifically, it exhibits the following notable strengths:\n(1)\tThe authors provide a novel and insightful comparison between Graph Transformer and MP-GNN (with VN) in both non-uniform and uniform setups. The latter setup, in particular, marks the first time it has been explored from this perspective, revealing that GT and MPGNN-VN do not subsume each other.\n(2)\tThe paper includes numerical experiments that complement the theoretical insights. The authors also attach the code, which will benefit the community. \n(3)\tLast but not least, the paper is well organized, and the writing is straightforward, so it is easy to follow even though the underlying proof is remarkable."
                },
                "weaknesses": {
                    "value": "Despite the strengths listed above, there are still areas in the paper where improvements can be made to enhance clarity and overall significance.\n(1)\tRegarding the writing: \na.\tThe paper utilizes many acronyms, which may necessitate repeated explanations. For instance, in Figure 1, it would be helpful if the author could reiterate the meanings of \"MP,\" \"VN,\" \u201cSA,\u201d \u201cPH,\u201d and \"FF\" in the figure caption. Additionally, in Figure 2(b), clarifying the definitions of \"l\" and \"r\" would enhance writing clarity. It is also recommended that the authors create a table in the appendix with all the acronyms. \nb.\tThe utilization of some math symbols may confuse and misleading. For example, in subsection 4.3, the \u201clr\u201d represents \u201cl*r\u201d instead of learning rate or number of layers. \nc.\tThere are some missing values in Table 1, which may be better to illustrate the reason in the table caption in addition to the other place. \nd.\tSome minor typos and grammars, such as \u201cBased on the the positional encoding LapPE\u201d.\n(2)\tRegarding the theory and numerical experiments:\na.\tThe assumptions and limitations of the proposed theory are somewhat unclear. For instance, it is not clearly defined whether the theory is applicable to various graph tasks or solely focused on graph classification tasks.\nb.\tThe message conveyed in Section 5.2 is also vague and disconnects with previous sections. Given that Table 1 indicates that the best performance appears somewhat random, it is not convincing how the theoretical insights can guide the practice. It may be valuable to conduct a more thorough investigation into the utilization of theoretical insights for model selection based on the characteristics of the dataset."
                },
                "questions": {
                    "value": "The questions are mainly related to the \u201cweakness\u201d:\n(1)\tPlease add a list of math symbols and abbreviations in the appendix and clarify the above writing questions.\n(2)\tDoes the theory also satisfy different graph learning tasks?\n(3)\tHow can we utilize theory to understand the results of section 5.2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699315937641,
            "cdate": 1699315937641,
            "tmdate": 1699636570760,
            "mdate": 1699636570760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XGPanxUl41",
                "forum": "AcSChDWL6V",
                "replyto": "wZZM28EbuX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments, we will carefully revise our paper taking them into account. \nPlease find below our clarifications and responses to some of your comments.\n\n**Q**: Weaknesses (1): \"Regarding the writing...\"\\\n**A**: We agree with the suggestions and will implement them in the manuscript.\n\n**Q**: Weaknesses (2a): \"The assumptions and limitations of the proposed theory are somewhat unclear...\"\\\n**A**: Our theoretical results concern graph-level function-approximation tasks. Some of the results can be extended to node-level tasks without much effort, while for other results more work is required to make conclusions concerning node-level tasks.\nFor example, it is not difficult to create a node-level function that GTs cannot learn while MPGNN+VNs can (generalizing Corollary 4.4).\nWe will add a clarification to the manuscript.\n\n**Q**: Weaknesses (2b): \"The message conveyed in Section 5.2 is also vague...\"\\\n**A**: Unlike the synthetic data experiments, our experiments on real-world problems are not meant to demonstrate our theoretical results directly. Rather, they offer another view leading to the general message of the paper: Neither GPS nor MPGNN+VN performs strictly better than the other across all learning tasks. Hence, the first conclusion from both our theoretical and experimental results is that both models should be considered in the general case. However, the theoretical insights suggest that if the target function is estimated to include the sum of some data of the nodes then MPGNN+VN may be the better choice to start with, and if the target function is estimated to include sophisticated weighted averages where the weight depends on both sides then GPS may be the better choice.\nWe will try to add a clarification in the manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138948148,
                "cdate": 1700138948148,
                "tmdate": 1700138948148,
                "mdate": 1700138948148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]