[
    {
        "title": "Towards Poisoning Fair Representations"
    },
    {
        "review": {
            "id": "WOqFVf89gL",
            "forum": "YLJs4mKJCF",
            "replyto": "YLJs4mKJCF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_LzqR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_LzqR"
            ],
            "content": {
                "summary": {
                    "value": "This paper designs a new data poisoning attack tailored for fair representation learning. The key idea is to generate poisoning samples that maximizes the mutual information between the representation (of the poisoned sample) and the sensitive attribute. This can be solved by bi-level optimization, yet the outer level problem (estimating high-dimensional mutual information) is intractable. To solve this issue, the authors propose to use Fisher\u2019s linear discriminant (FLD) score as a cheap proxy to MI, which has a closed-form solution. For inner level problem, it is solved by matching the gradients of a victim model and a clean model. Through the approximations made, the original objective is now fully tractable and can be learned by SGD. The effectiveness of the method is evaluated on two tabular datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Significance**: The problem studied in this paper (data poisoning to attack the learned representation in fairly trained models) is interesting and important;\n- **New information-theoretic viewpoint for poisoning attack**: The authors propose an information-theoretic objective for poisoning attack in fair machine learning. This framework, to the best of my knowledge, is new within the specific context considered and is very-well motivated. The reminder on the advantage of MI-based fairness as compared to conventional metric (e.g. DP) is also useful;\n- **Cheap proxy to mutual information**: I also like the authors\u2019 idea to use Fisher\u2019s linear discriminant analysis (FLD) as a cheap-but-still-effective proxy to MI. I would also highly praise the authors\u2019 efforts to mention the possibilities of other analytic proxies, as well as a discussing between FLD and these alternative methods. \n- **Feature selection for fairness**: in addition to the main contribution, the authors also show how their developed attack can be further applied to identify robust features for fair classification. This is quite interesting, especially in that it offers a new perspective for understanding and interpreting the behavior of a fairly trained model. I personally think this part deserves a separate section and can be highlighted as a 2nd main contribution of the paper."
                },
                "weaknesses": {
                    "value": "- **On the necessity of working at representation level**. After reading the paper, it is still unclear to me why do we need to consider I(Z; a) rather than I(Y(X); a). Here Y(X) is the prediction of the model. In fact, considering I(Y(X); a) has several benefits: first, its maximization still yields an unfair model; second, its estimation is much easier due to the low-dimensionality of Y(X) and a (for example, we can estimate it easily by the well-known KSG estimator [1] which typically works quite well in low-dimensional cases). Importantly, maximizing I(Y(X); a) also do not require an access to Y. Can the authors justify the reason behind considering I(Z; a) instead?\n\n- **Issues in the discussion related to other MI proxies**: some comments in remark 2.1 do not seem completely sensible to me. For example, the author mention that other analytical proxies like (K)CCA and sliced mutual information suffer from differentiation difficulties. This may not be true in my view (for example, in CCA, you can first solve the optimal weights analytically, then substitute it back to the formula of CCA. The resultant formula has a structure very similar to eq2 in your paper). In addition, I think the authors miss the possibility of using non-parametric dependence measure e.g. distance correlation (dCorr) [2]. This is also fully analytical and may potentially be applicable in your scenario. Ultimately, I think the real advantage of your FLD-based method is it provably optimises a variational lower bound of MI, whereas other proxies (KCCA, slice MI, dCorr) may not. This seems to be a better justification of the use of your method. \n\n- **Slightly limited evaluation**: most of the evaluations in this work are conducted on tabular data where the network size is small. Whether the method developed in this work will scale to larger networks e.g. those in computer vision and NLP remain unclear to me. However, given the nature of many existing literature in fairness research, which also only focus on tabular data, this should not be a main criticism for the paper.\n\n[1] Estimating mutual information, Physical Review E, 2004\n\n[2] Measuring and testing dependence by correlation of distances, Annals of Statistics, 2007\n\n----------------------------------------------\nUpdate after rebuttal:\n\nThe authors have addressed most of my concern above and I have updated my score to 8 accordingly."
                },
                "questions": {
                    "value": "How does the proposed FLD-based method scale with the dimensionality of representation d? I am concerned about this since Gaussianity assumption typically violates in high-dimensional cases. Will the method still work well for e.g. d=128?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2584/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2584/Reviewer_LzqR",
                        "ICLR.cc/2024/Conference/Submission2584/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2584/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697902863452,
            "cdate": 1697902863452,
            "tmdate": 1700568805665,
            "mdate": 1700568805665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OigIeZaZi6",
                "forum": "YLJs4mKJCF",
                "replyto": "WOqFVf89gL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We highly appreciate your effort and time spent reviewing our paper and thank you for your expertise and constructive comments. In the following, we address your comments and questions one by one.\n\n>**On the necessity of working at representation level.**\n\nAttacking mutual information at representation level aligns better with fair representation learning (FRL) and is more universal than attacking $I(Y(X), a)$. For example, a bank may want to obtain a representation for each user that can be used to determine their eligibility for **existing** and **upcoming** financial products such as credit cards without concerning about fairness again [1]. \nHere each financial product is associated with a (unique) label, and determining the eligibility entails a classification task.\nIn this case, there are two challenges for delivering a $Y(X)$-based attack. \nFirst, one has to determine and justify \nwhich classifier $Y(\\cdot)$ to use and why consider the fairness of this specific classification task. \nSecond, for any **upcoming** financial product, its label $Y$ does not exist and one cannot obtain classifier $Y(X)$ (one needs $Y$ to train such a classifier), \nlet along attacking it. \nIn contrast, a representation-level attack can overcome the two challenges in a single shot. \nAs discussed in section 2.2,\nby maximizing $I(z, a)$, any $I(Y(X), a)$ **will be** maximized so long as the fairness concern exists. \nThis implies launching attack on all labels $Y$ simultaneously, including the ones where classifies $Y(X)$ cannot be trained. \n\n>**Issues in the discussion related to other MI proxies.**\n\nWe appreciate the detailed explanation and valuable feedback from the reviewer. \nWe've revised the remark 2.2 in the paper as follows: \n\n>*Maximizing I(z,a) is a general framework to poison FRL and admits other proxies such as sliced mutual information (Chen et al., 2022), kernel canonical correlation analysis (Akaho, 2007), and non-parametric dependence measures (Szekely et al., 2007). In this work, we use FLD because of its conceptual simplicity, interpretability, and good empirical performance. As one may recognize, when $p(z | a = 1)$ and $p(z | a = 0)$ are Gaussian with equal variance, FLD is optimal (Hamsici \\& Martinez, 2008) whose BCE loss attains the tight lower bound of $I(z, a)$ up to constant $H(a)$. In this case, our method provably optimize the lower bound of I(z, a) whereas other proxies \nwhereas other proxies may not due to the lack of direct connections to mutual information. While the Gaussianity may not hold in general, FLD score as a measure of data separability is still valid (Fisher, 1936), and we verify its efficacy for our goal in Appendix D.2, where we show that FLD score is highly informative for the empirical minimal BCE loss of a logistic regression.*\n\n>**Slightly limited evaluation.**\n\nAs pointed out by the reviewer, we focus on simple tabular data because many fair representation learning methods focus on these data [1, 2, 3, 4]. Moreover, previous work on attacking classical fair machine learning also only focused on tabular data [5, 6, 7], and we follow this convention. \n\nTo better evaluate the effectiveness of our method, we added experiments on two more benchmark datasets COMPAS and Drug Consumption. \nResults have been updated in Appendix D.8. \nWe reported decrease of BCE losses in Figure 15, increase of DP violations in Figure 16, and accuracy of predicting $y$ from $z$ in Figure 17.\nAgain, on two new datasets our attacks successfully outperformed AA baselines to a large extent. \nObserving these good performance, we will study the vulnerability of bias mitigation methods for CV and NLP tasks as our future work. \n\n[1] Zhao et al. Conditional learning of fair representations, 2019.\n\n[2] Moyer et al. Invariant representations without adversarial training, 2018.\n\n[3] Creager et al. Flexibly Fair Representation Learning by Disentanglement, 2018.\n\n[4] Reddy et al. Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics, 2021.\n\n[5] Solans et al. Poisoning attacks on algorithmic fairness, 2020.\n\n[6] Mehrabi et al. Exacerbating algorithmic bias through fairness attacks, 2021.\n\n[7] Chang et al. On Adversarial Bias and the Robustness of Fair Machine Learning, 2020."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468312526,
                "cdate": 1700468312526,
                "tmdate": 1700468312526,
                "mdate": 1700468312526,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7sLws63Xru",
                "forum": "YLJs4mKJCF",
                "replyto": "WOqFVf89gL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_LzqR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_LzqR"
                ],
                "content": {
                    "title": {
                        "value": "Reply to author"
                    },
                    "comment": {
                        "value": "I thank the authors for their effective rebuttal. Most of my concerns have been addressed. I therefore update the score to 8 in response to your efforts. The discussion on why to attack at a representation level is indeed insightful and instructive. \n\nMy final suggestions are to (a) highlight the motivation for attacking at a representation level in the revised manuscript; (b) improve the presentation by e.g. avoiding inline figures and moving all figures to the top/bottom of a page; and (c) remind the reader the potential risk of violating the Gaussianity assumption in high-dimensional cases (and refer them to the ablation study). I would be very interested to see when will FLD begin to fail (and even if it fails for some d, it is still a good contribution)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568258996,
                "cdate": 1700568258996,
                "tmdate": 1700568995050,
                "mdate": 1700568995050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vJOjxtt2P0",
            "forum": "YLJs4mKJCF",
            "replyto": "YLJs4mKJCF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_LCr6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_LCr6"
            ],
            "content": {
                "summary": {
                    "value": "This work studies an interesting topic i.e. how to conduct data poisoning against fair representation learning tasks. Experiments are conducted on Adult and German datasets to demonstrate its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Motivation is well-stated and interesting.\n- Authors develop related  theoretical analysis on the needed number of poisoning samples is derived and shed light on defending against the attack.\n- Personally I like the organization of Introduction section : ) It's clear and easy for reviewers to know the meaning of this work."
                },
                "weaknesses": {
                    "value": "- Authors use their own defined vanilla metric, and lack related fairness-aware metrics like Equality odds (EO)\n- Authors are encouraged to conduct more experiments on more datasets like COMPAS and Drug Comsumptionm, please kindly follow this AAAI paper which authors have cited: Exacerbating Algorithmic Bias through Fairness Attacks.\n- Personally, I reckon authors are encouraged to conduct experiments on deeper NN (I think simple MLP is not that DEEP to be called \"DNN\"), though the datasets are relatively simple. I'm curious about these experiments to investigate ENG. Authors are encouraged to conduct more analysis on the further version of this work, which is good for community: )"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2584/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2584/Reviewer_LCr6",
                        "ICLR.cc/2024/Conference/Submission2584/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2584/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697904561340,
            "cdate": 1697904561340,
            "tmdate": 1700494756526,
            "mdate": 1700494756526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g17tYhadVo",
                "forum": "YLJs4mKJCF",
                "replyto": "vJOjxtt2P0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We highly appreciate your effort and time spent reviewing our paper and thank you for your expertise and constructive comments. In the following, we address your comments and questions one by one.\n\n>**Authors use their own defined vanilla metric, and lack related fairness-aware metrics like Equality odds (EO).**\n\nIn section 2.2 we analyzed how our attack can deteriorate the well-known fairness-aware metric demographic parity (DP).\nIn experiments we verified that **our attack successfully exacerbated DP violations of predicting label $y$ from  representation $z$ learned by four victims on four datasets** (Adult, German, and new COMPAS, Drug Consumption). Due to page length these results were deferred to Appendix D.3 (Adult and German datasets) and Appendix D.8 (COMPAS and Drug Consumption datasets) respectively. \n\n>**Authors are encouraged to conduct more experiments on more datasets like COMPAS and Drug Consumption.**\n\nThanks for your suggestions, we conducted experiments on COMPAS and Drug Consumption datasets and reported results in Appendix D.8 of our updated paper. \nWe reported decrease of BCE losses in Figure 15, increase of DP violations in Figure 16, and accuracy of predicting $y$ from $z$ in Figure 17. Again, on two new datasets our attacks successfully outperformed AA baselines to a large extent. \n\n\n\n>**Personally, I reckon authors are encouraged to conduct experiments on deeper NN (I think simple MLP is not that DEEP to be called \"DNN\"), though the datasets are relatively simple.**\n\nTo see how our attacks perform on deeper and larger NNs, \nwe tested how fair representations learned by CFAIR and CFAIR-EO can be attacked when the NNs are larger. Experiments were conducted on the largest Adult dataset. \nWe increased depths of encoder to 3 hidden layer and adversarial classifier to 5 hidden layer. We increased the dimension of representations to 128 and set all hidden sizes to 60. Due to time constraint, we did not tune the model architectures to obtain the best clean performance. Training epochs were increase from 50 to 100, and all other hyper-parameters such as batch size and learning rates were unchanged. As shown in the table below, **our attacks succeeded in all settings while three baselines suffered from several failures.**\n\nFinally, we would like to mention that the choices of model architecture and training settings can have crucial influence on the performance with and without attack, so these results are just for illustration. Moreover, motivated by the promising results, we plan to extend our attack to fair machine learning on large-scale image and text datasets.\n\n| Victim | Attack | 5\\% | 10\\% | 15\\% |\n| ------ | ------------- | ----- | ----- | ----- |\n CFAIR | NRAA-a  |  0.0047     | 0.0204 | -0.4015 \n|  | NRAA-y  |  0.0163 | -0.278 | -0.3552 \n|  | RAA-a   |  0.0086     | -0.0138 | -0.3312 \n|  | RAA-y   |  0.011      | -0.0226 | -0.8761 \n|  | ENG-EUC |  0.003      | 0.0058 | 0.01 \n|  | ENG-FLD |  0.0019     | 0.0055 | 0.0105 \n|  | ENG-sFLD|  0.0033     | 0.0052 | 0.0037 \n| CFAIR-EO | NRAA-a  |  0.0171 | 0.0279 | -0.3513 \n|  | NRAA-y  |  0.0194 | -0.3161 | -0.2261 \n|  | RAA-a   |  -0.1668 | -0.2093 | -0.0896 \n|  | RAA-y   |  -0.0585 | -0.5564 | -0.3719 \n|  | ENG-EUC |  0.0021 | 0.007 | 0.0046 \n|  | ENG-FLD |  0.0075 | 0.0128 | 0.0205 \n|  | ENG-sFLD|  0.0073 | 0.0056 | 0.0213"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469517751,
                "cdate": 1700469517751,
                "tmdate": 1700469517751,
                "mdate": 1700469517751,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yGodAZ3rh7",
                "forum": "YLJs4mKJCF",
                "replyto": "vJOjxtt2P0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_LCr6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_LCr6"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thank you for your good rebuttal. Question 1 is important to conduct this work since I see other reviewers also raise similar concerns, and authors conduct extra experiments on this concern. I think this work's contribution is improved after rebuttal so I rasie my rating for contribution evaluation.\nHope other reviewers can retrospect the rebuttal content : )"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494676816,
                "cdate": 1700494676816,
                "tmdate": 1700494873395,
                "mdate": 1700494873395,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b6YSpQBdyq",
            "forum": "YLJs4mKJCF",
            "replyto": "YLJs4mKJCF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_93wi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_93wi"
            ],
            "content": {
                "summary": {
                    "value": "This study proposes a novel data poisoning attack against fair representation learning algorithms. Compared to previous attack methods against fair classification, this method proposes to craft the training dataset, in order to maximize the mutual information between the learned representation and sensitive raw features. This mutual information powered attack algorithm shows superior attack performances compared to anchor attacks against 4 different fair representation learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1/ This is the first research effort in organising data poisoning attacks against fair representation learning attacks. Different from fair classification problems, manipulating fair representation needs to control the statistical relation between high-dimensional embeddings  and raw feature inputs. This is challenging for directly extending previous fair learning poisoning methods. I'd appreciate the efforts poured towards this difficult problem. \n\n2/ It is intuitive to increase the mutual information between the embeddings and sensitive raw features, in order to violate the fairness constraint of the victim embedding learning algorithm. However directly maximising mutual information of high-dimensional embeddings is very difficult. I know there are some differentiable approximation tool to MI, like MINE. But it is computationally costly and prone to the potential estimation gap. It is interesting to read the theoretical analysis and practices of using Fisher Linear Discriminator scores to bound MI. Apparently, optimising FLD scores is much easier and economic in computation. \n\n3/ Inspired from Geiping et al's gradient matching work, this study propoes to matching the upper and lower bound of gradients instead of solving the bi-level poisoning problem. This smart optimization strategy enables an analytical solution to the proposed attack."
                },
                "weaknesses": {
                    "value": "One of the problem of introducing elastic penalty is how to choose properly the two penalty parameters $\\lambda_{1}$ and $\\lambda_2$. Though it can be chosen empirically, it can be dataset-dependent. Would it make significantly difference if we simply choose the L1 norm penalty instead?"
                },
                "questions": {
                    "value": "Discussion over the choice of the two penalty parameters $\\lambda_{1}$ and $\\lambda_2$ in the elastic penalty."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2584/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698572734905,
            "cdate": 1698572734905,
            "tmdate": 1699636196003,
            "mdate": 1699636196003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Mt5NAelDmN",
                "forum": "YLJs4mKJCF",
                "replyto": "b6YSpQBdyq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We highly appreciate your effort and time spent reviewing our paper and thank you for your expertise and constructive comments. In the following, we address your comments and questions one by one.\n\n>**One of the problem of introducing elastic penalty is how to choose properly the two penalty parameters...Would it make significantly difference if we simply choose the L1 norm penalty instead?**\n\nWe chose the penalty parameters following the convention in previous work [1] and the choices were briefly selected from a relatively large range, therefore, we believe that the attack performance can benefit from a fine-grained search for better parameter values. \n\nRegarding the advantage of using elastic-net penalty than using $L_1$ norm penalty only, \nconceptually, elastic-net penalty is able to choose a group of correlated features whereas using $L_1$ or $L_2$ penalty only cannot [2]. \nEmpirically, to check the effect of using $L_1$ norm penalty only, we tried the same choices of $\\lambda_1$ as in the main experiment \nand compare the corresponding attack performance and perturbation magnitude. \nDue to time constraint we only conducted experiments on Adult dataset and reported results in Appendix D.5 of the updated paper. \n\nIn short, we note that **comparing with elastic-net penalty, $L_1$ norm penalty usually resulted in perturbations with larger $L_1$ and $L_2$ norms but the attack performance was hardly improved, especially when small- to intermediate- level penalty parameter values were used.**\n\n[1] Chen et al. Ead: elastic-net attacks to deep neural networks via adversarial examples, 2018.\n\n[2] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net, 2005."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469785058,
                "cdate": 1700469785058,
                "tmdate": 1700469785058,
                "mdate": 1700469785058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mGHrKFRt73",
            "forum": "YLJs4mKJCF",
            "replyto": "YLJs4mKJCF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_yTSY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_yTSY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies data poisoning attacks against fair representation learning (FRL) on deep neural networks. To achieve the attack goal, the authors propose a new MI (mutual information) maximization paradigm. Besides, the experiments show that the proposed attack outperforms baselines by a large margin and raises an alert of the vulnerability of existing FRL methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is a pioneering data poisoning attack on deep learning-based fair representation learning to degrade fairness while the existing fairness attacks are focusing on shallow-model classifiers. \n\nThe authors propose a new attack goal based on MI to amplify the difference between representations from different subgroups. \nThe authors derive the first theoretical minimal number of poisoning samples required by their attack, which is crucial for practical attacks."
                },
                "weaknesses": {
                    "value": "The assumption of the threat model is strong. The proposed attack is under the assumption of a white-box threat model, where the attacker has full access to and control over the victim's trained model. This implies that the attack is primarily effective in scenarios where the victim has already trained a model and relies on the attacker's data for subsequent fine-tuning. Such a specific condition might limit the general applicability of the attack in diverse real-world scenarios.\n\nLack the reason why the method can attack FRL. The foundational principle of Fair Representation Learning (FRL) is to ensure fairness by removing sensitive features from the intermediate representation. The proposed attack, on the other hand, seeks to amplify the presence of sensitive information within these representations. The paper does not adequately elucidate why FRL techniques, designed to minimize sensitivity, are unable to counteract or mitigate the effects of the proposed attack. This leaves a gap in understanding the inherent vulnerabilities of FRL against the described attack strategy."
                },
                "questions": {
                    "value": "Similar to weakness 1, could the authors clarify their threat model more clearly? Specifically, does the attacker need to know the victim\u2019s trained model to generate poisoning data? If not, does the attacker only need to know the structure of the victim\u2019s model and the model will be trained on the poisoning data from the attacker?\n\nSimilar to weakness 2, could the authors give more insight to explain why their attack cannot be mitigated by fair representation learning (FRL)?\n\nCould the proposed attack be applied to deeper neural networks? The experiments of this paper are just on two-hidden layer CNNs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2584/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812927204,
            "cdate": 1698812927204,
            "tmdate": 1699636195920,
            "mdate": 1699636195920,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WDUgipBhPL",
                "forum": "YLJs4mKJCF",
                "replyto": "mGHrKFRt73",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We highly appreciate your effort and time spent reviewing our paper and thank you for your expertise and constructive comments. In the following, we address your comments and questions one by one.\n\n>**Could the authors clarify their threat model more clearly?**\n\nAs the first work towards conducting poisoning attack against fair representation learning (FRL) methods, we followed previous works on attacking classical fair machine learning such as [1, 2, 3] and assumed a strong attacker that has full knowledge to the victim model: including its architecture, training details, and parameters. \nThis type of attack is known as white-box attack and has been widely used to benchmark the vulnerability (robustness) of a model [1, 2, 3, 4] and is necessary to develop a black-box attack in general (in the sense that an optimization-based attack is often conducted by attacking a collection of white-box surrogated victim models) [5, 6]. Therefore, we believe that this requirement does not undermine our contribution on understanding how vulnerable FRL methods are. Nonetheless, we agree with the reviewer that a black-box attack would be more practical and we will delve into developing stronger and more transferable attack towards FRL methods in our future work. \n\n[1] Solans et al. Poisoning attacks on algorithmic fairness, 2020.\n\n[2] Mehrabi et al. Exacerbating algorithmic bias through fairness attacks, 2021.\n\n[3] Chang et al. On Adversarial Bias and the Robustness of Fair Machine Learning, 2020.\n\n[4] Koh et al. Stronger data poisoning attacks break data sanitization defense, 2022. \n\n[5] Huang et al. MetaPoison: Practical General-purpose Clean-label Data Poisoning, 2021. \n\n[6] Geiping et al. Witches\u2019 brew: Industrial scale data poisoning via gradient matching, 2021.\n\n>**Could the authors give more insight to explain why their attack cannot be mitigated by fair representation learning (FRL)?**\n\nThanks for your question. We plan to explore this direction in our next step towards understanding the vulnerability of existing fair representation learning (FRL) method. Here we give two possible causes. \n\nFirst, fair machine learning methods often assume that the underlying data distribution is unchanged.\nWhen a distribution shift appears, many of these methods showed degraded performance [1, 2].\nWe presume that FRL methods suffer from a similar vulnerability as well, i.e., their training is not robust enough against distribution shift. \nConsequently, a data poisoning attack, which injects carefully-crafted poisoning samples to the training data, can be seen as imposing an adversarial distribution shift to the empirical training distribution. This may deteriorate the fairness performance of FRL methods on the validation (targeted) data that follows the original distribution.\n\nSecond, as mutual information is difficult to estimate and optimize, existing FRL methods also rely on approximate solutions that require training of some auxiliary models such as a sensitive feature predictor to conduct adversarial training or a deep neural network-based mutual information estimator.\nIn practice, these auxiliary models may also encounter unstable training and result in sub-optimal fairness results, and the performance degradation can be exacerbated by the data poisoning attack. \n\n\n[1] An et al. Transferring Fairness under Distribution Shifts via Fair Consistency Regularization, 2022.\n\n[2] Jiang et al. Chasing Fairness Under Distribution Shift: A Model Weight Perturbation Approach, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470201090,
                "cdate": 1700470201090,
                "tmdate": 1700582385121,
                "mdate": 1700582385121,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RuI9oMkD6z",
            "forum": "YLJs4mKJCF",
            "replyto": "YLJs4mKJCF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a poisoning attack strategy to compromise fair representation learning, aiming to increase the fairness gap for underprivileged groups. The attack relies on approximations to solve a bilevel optimization problem where the outer problem, which describes the attacker\u2019s objective, aims to maximize the mutual information between the representation for the privileged and the underprivileged groups. Since the optimization of this objective is not tractable, the authors use Fisher\u2019s Linear Discriminant (FLD) score as a proxy. Then, the whole bilevel optimization problem is approximated using a gradient matching strategy. The authors also provide some theoretical analysis on the poisoning ration required to compromise the target models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Poisoning fair representations have received less attention in the research literature on data poisoning and preliminary works show that these attacks can have a significant impact on the fairness of the target algorithms. Exploring more scalable poisoning attack strategies capable of increasing the fairness gap for deep neural networks is timely and a topic of interest. \n\n+ The authors strived to provide a theoretical analysis on the ratio of poisoning points required to compromise the target algorithms."
                },
                "weaknesses": {
                    "value": "+ The paper lacks a clear threat model. For example, it is unclear whether the attacker\u2019s objective is useful for compromising algorithms with and without mechanisms for mitigating the fairness gap. On the other side, it is unclear what is the attacker\u2019s objective and the relation of the attack strategy with respect to the model\u2019s performance. Other works in the research literature, like Chang et al. or Van et al. (\u201cPoisoning Attacks in Fair Machine Learning\u201d) have already considered the trade-off between targeting performance and the fairness gap. \n\n+ In the end the attack proposed by the authors rely on the maximization of the FLD score in the outer optimization objective. This relies on strong assumptions on the distribution of the data and its representation and may not hold for many practical scenarios. In this sense, it is unclear why this strategy is better compared to other attacks already proposed in the research literature, like Solans et al., Mehrabi et al., Chang et al., or Van et al. (\u201cPoisoning Attacks in Fair Machine Learning\u201d), which have strong connections to this work. On the other side, there is not mention to \u201cSubpopulation Poisoning Attacks\u201d by Jagielski et al., which are also very relevant to this work and proposes more scalable alternatives for crafting poisoning attacks targeting fairness. \n\n+ Given the existing works in the research literature, I believe it is to bold that the authors claim that \u201cWe propose the first data poisoning attack on FRL as outlined in Figure 1.\u201d I think that the authors should clarify this and position better their paper and contributions with respect to other existing works. \n\n+ The experimental evaluation is not convincing: In the experiments the authors just reported results evaluating the BCE loss but did not consider any of the existing metrics for measuring the fairness gap. On the other hand, there is not mention to how the attack affects the accuracy at all. Apart from that, it is necessary a more comprehensive comparison with other methods in the research literature, e.g., Solans et al., Chang et al. Van et al. (\u201cPoisoning Attacks in Fair Machine Learning\u201d), Jagileski et al. (\u201cSubpopulation Poisoning Attacks\u201d). For some of these attacks, the authors can use the same outer objective and use the same approximation for solving the bilevel optimization problem. \n\n+ The authors used a gradient matching strategy for approximating the solution of the bilevel optimization problem. However, Jagielski et al. (\u201cSubpopulation Poisoning Attacks\u201d) shows that other strategies can be more efficient for this. I think this aspect requires further analysis. \n\n+ The authors say: \u201cHeuristics such as label flipping (Mehrabi et al., 2021) lack a direct connection to the attack goal, thereby having no success guarantee and often performing unsatisfactorily.\u201d I think this is not true. Although more limited on the attacker\u2019s capabilities, smart manipulation of the labels can lead to successful attacks. See for example \u201cSubpopulation Poisoning Attacks\u201d by Jagielski et al."
                },
                "questions": {
                    "value": "+ Equation (2) relies on strong assumptions about the distribution (Gaussian distribution and continuous variables) of the different subpopulations. How is this a good proxy for approximating the original problem? How does this compare with existing attacks (as the ones mentioned before)?\n\n+ Could the authors provide more details on the threat model (see comments above)?\n\n+ How assumption 2 works in the current threat model for the attack? The authors say: \u201cBefore the attack, the victim is well trained.\u201d What does this mean?\n\n+ Also, for the theoretical analysis: Why do the authors think that assumption 3 is reasonable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2584/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699273105105,
            "cdate": 1699273105105,
            "tmdate": 1699636195685,
            "mdate": 1699636195685,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zIrT2vRASz",
                "forum": "YLJs4mKJCF",
                "replyto": "RuI9oMkD6z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We highly appreciate your effort and time spent reviewing our paper and thank you for your expertise and constructive comments. In the following, we address your comments and questions one by one.\n\n>**The paper lacks a clear threat model: it is unclear whether the attacker\u2019s objective is useful for compromising algorithms with and without mechanisms for mitigating the fairness gap. On the other side, it is unclear what is the attacker\u2019s objective and the relation of the attack strategy with respect to the model\u2019s performance.**\n\nIn this work we proposed a new objective to attack fair representation learning (FRL) models by maximizing the mutual information (MI) between high-dimensional representations $z$ learned by the victim and the sensitive attribute $a$.\nThis attacking objective aligns well with the well-known MI-based fairness goals used in FRL as detailed in section 2.1.\nMoreover, this objective jeopardizes the well-known fairness metric demographic parity (DP) of any classifiers predicting label $y$ from representation $z$ as discussed in section 2.2, and is empirically verified in our experiments covering four FRL models trained on four well-studied benchmark datasets.\nWe further added accuracy of predicting label $y$ from representation $z$, and found that **our attacks had minimal impact on the accuracy.**\n\n>**It is unclear why this strategy is better compared to other attacks already proposed in the research literature**\n\nExisting works, including the listed one, focused on attacking fairness in classification tasks, i.e., the fairness is directly defined and attacked based on scalar predictions. \nWe instead seek to degrade the *fairness* of *high-dimensional* representations directly, which is a much more challenging task and requires a new formulation as discussed in section 1.  \nNotably, \nFRL methods do not always need access to label $y$ during the training time, see ICVAE-US for an example. For such victims, all existing poisoning attack formulation fall short to apply while our formulation is able to handle them.\n\n>**There is not mention to \u201cSubpopulation Poisoning Attacks\u201d by Jagielski et al., which are also very relevant to this work and proposes more scalable alternatives for crafting poisoning attacks targeting fairness. & The authors used a gradient matching strategy for approximating the solution of the bilevel optimization problem. However, Jagielski et al. (\u201cSubpopulation Poisoning Attacks\u201d) shows that other strategies can be more efficient for this. I think this aspect requires further analysis.**\n\nThanks for bringing to us this work.\nAfter reading through it, we agree that subpopulation attack is a direction that deserves more exploration in the future study on deriving stronger attack against FRL methods, and we have updated the paper to discuss this accordingly. \nAt the same time, we found subpopulation attack less relevant to the current main focus of our paper. \nFirst, a subpopulation attack, which identifies and attacks the most vulnerable subgroups of data, fills the gap between the availability and targeted attack. While in this work, we focus on an availability-like attack, where the attacker seeks to degrade fairness guarantee for as many samples as possible (on the whole validation set). \nSecond, as mentioned by the reviewer, subpopulation attack provides more scalable alternatives to craft poisoning samples, \nwhile the main focus of this paper is to formulate an attack goal towards fairness of high-dimensional representations and to propose a tractable proxy that can be used as an objective for poisoning samples crafting."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471081123,
                "cdate": 1700471081123,
                "tmdate": 1700500132795,
                "mdate": 1700500132795,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f7RjXrZlZA",
                "forum": "YLJs4mKJCF",
                "replyto": "RuI9oMkD6z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors, Cont'd."
                    },
                    "comment": {
                        "value": ">**I think that the authors should clarify this and position better their paper and contributions with respect to other existing works.**\n\nThe main contributions of our work lie in two-fold. \nFirst, we provide a new MI-based attack goal to degrade fairness of *high-dimensional* representations $z$. Specifically, we maximize $I(z, a)$ where $a$ is the sensitive feature. \nIn contrast, previous attacks mostly focused on *scalar* predictions on $\\hat{y} = h(z)$ and seeks to maximize $I(h(z), a)$ (loosely speaking) which is much easier to evaluate than $I(z, a)$ as $z$ is high-dimensional. \nAs discussed in section 2.2, the benefit of using this new MI-based attack goal is that by maximizing $I(z, a)$, for any classifier $h$, $I(h(z), a)$ is free to grow (so DP violation increases [1]) so long as the fairness concern exists. Notably, this condition holds even if ground truth $y$ is unobserved so that previous attack goals cannot be defined.\n\nSecond, we provide a theoretical analysis on the required poisoning samples to launch a gradient-matching based attack.\nThis analysis also sheds light on defending against the proposed attack method and can be of independent interest to other gradient-matching based attackers. \n\nTo avoid possible confusion, we've updated this statement as \"*We propose the first data poisoning attack that directly targets on fairness of high-dimensional representations as shown in Figure 1.*\" \n\n[1] Zemel et al. Learning Fair Representations, 2013. \n\n>**The experimental evaluation is not convincing: In the experiments the authors just reported results evaluating the BCE loss but did not consider any of the existing metrics for measuring the fairness gap.**\n\nWe reported violation of demographic parity (DP), one of the most widely-used fairness notion, from the four victim models trained on Adult and German datasets in Appendix D.3.\nIn the revised paper we further added corresponding results on COMPAS and Drug Consumption datasets in Appendix D.8 of updated paper. \n**In all cases, our attacks successfully increased DP violations, clearly showing its effectiveness.**\n\n>**For some of these attacks, the authors can use the same outer objective and use the same approximation for solving the bilevel optimization problem.**\n\nAs analyzed in the responses above, we believe that previous outer objectives may not be good choices for our purpose and they indeed fail to apply in certain cases. \nFor example, in [1] the outer objective is a weighted average of classification accuracy and relaxed group fairness gap. \nHowever, ICVAE-US does not involve any classification in its training, therefore, these two terms are undefined.\n\n[1] Van et al. Poisoning Attacks on Fair Machine Learning, 2021. \n\n>**The authors say: \u201cHeuristics such as label flipping (Mehrabi et al., 2021) lack a direct connection to the attack goal, thereby having no success guarantee and often performing unsatisfactorily.\u201d I think this is not true. Although more limited on the attacker\u2019s capabilities, smart manipulation of the labels can lead to successful attacks.**\n\nThanks for your thought. We meant to express that label flipping-based attacks do not directly optimize the attack goal, and the goodness of these manipulations can be hard to quantify. As a result, one may not provide theoretical analysis on the minimal number of poisoning samples as we did for the gradient-matching based attack. However, we agree that smart label flipping attack can lead to successful attacks in practice, and we have updated this statement as \"*Heuristics such as label flipping (Mehrabi et al., 2021) do not directly optimize the attack goal, thereby often requiring great effort to design good manipulations to success.*\""
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471442678,
                "cdate": 1700471442678,
                "tmdate": 1700500094915,
                "mdate": 1700500094915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R1ximiU9VA",
                "forum": "YLJs4mKJCF",
                "replyto": "RuI9oMkD6z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors, Cont'd (2)"
                    },
                    "comment": {
                        "value": ">**Equation (2) relies on strong assumptions about the distribution (Gaussian distribution and continuous variables) of the different subpopulations. How is this a good proxy for approximating the original problem? How does this compare with existing attacks (as the ones mentioned before)?** \n\nFair representations are often real-valued [1, 2, 3, 4] and thereof we believe that the \"continuous variable\" assumption naturally holds in general fair representation learning scenarios.\n\nIn terms of the Gaussian assumption, as discussed in remark 2.2, when this assumption holds, Eq (2) is not only a proxy, but also an optimal solution. When this assumption does not hold, using FLD (Eq (2)) as a data separability measure is still valid [5] and we believe that it is straightforward to use data separability as a measure of how difficult it is to classify them and as a proxy for the optimal BCE loss one can achieve on this classification task (which also depicts the classification difficulty). Notably, this BCE loss is always a lower bound of the mutual information in the original problem. In comparison, to our best knowledge, there is no analysis on how existing attack goals (as the ones mentioned before) relate to the mutual information between high-dimensional representations $z$ and sensitive feature $a$.\n\nEmpirically, Figure 4 in Appendix D.2 verified that FLD score acted pretty well in approximating the optimal BCE loss to classify different classes of fair representations learned by four different fair representation learning methods. In short, in this work, we use FLD because of its conceptual simplicity, interpretability, and good empirical performance.\n\n[1] Zhao et al. Conditional learning of fair representations, 2019.\n\n[2] Moyer et al. Invariant representations without adversarial training, 2018.\n\n[3] Creager et al. Flexibly Fair Representation Learning by Disentanglement, 2018.\n\n[4] Reddy et al. Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics, 2021.\n\n[5] Fisher. The use of multiple measurements in taxonomic problems, 1936. \n\n>**How assumption 2 works in the current threat model for the attack? The authors say: \u201cBefore the attack, the victim is well trained.\u201d What does this mean?**\n\nA gradient matching-based attack relies on a *trained* victim model [1]. \nTo simplify the theoretical analysis, assumption 2 states that this trained model is converged to a local stationary point of the lower-level objective before being attacked. \nThis assumption is reasonable because the lower-level objective is the one that the model is trained on. \nMoreover, it allows us to bound the influence of clean gradient from clean training data, and to derive a lower bound for the minimal number of poisoning samples. This bound is valid in the sense that when the assumption is violated, typically a larger amount of poisoning samples is needed. \n\nThe rational behind is as follows. \nWhen training the victim on poisoned data, each mini-batch consists of two types of samples: poisoning samples will push the victim model to optimize the upper-level objective (attack goal), whereas clean sample will counteract with this goal and instead force the victim to optimize the lower-level objective. By assuming that the victim has converged with respect to the lower-level objective, how clean samples counteracts with poisoning samples will be minimized: they only add randomness to the poisoning gradients. This allows us to derive the bound in a similar way to stochastic gradient descent. \n\n[1] Geiping et al. Witches\u2019 brew: Industrial scale data poisoning via gradient matching, 2020.\n\n>**Also, for the theoretical analysis: Why do the authors think that assumption 3 is reasonable?**\n\nSimilar to assumption 2, \nassumption 3 allows us to simplify the derivation of the lower bound. \nThis assumption assumes that all poisoning samples in hand are well-crafted. \nThis assumption can be achieved by conducing the gradient matching on poisoning samples one by one.\nAs gradient matching is a standard optimization problem, modern optimizers can solved it readily. \nIn addition, assumption 3 does not require all poisoning samples to match the upper-level gradient exactly.\nInstead, each of them only needs to form an unbiased estimation. \nNotably, when the assumption is violated, typically a larger amount of poisoning samples is needed, and the derived lower bound is still valid."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472051181,
                "cdate": 1700472051181,
                "tmdate": 1700673163939,
                "mdate": 1700673163939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4ljDen80rt",
                "forum": "YLJs4mKJCF",
                "replyto": "zIrT2vRASz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your comments and clarifications. I understand the contribution in terms of attacking FRL, but still the threat model deserves to be analyzed more carefully. I also believe that the experimental evaluation would require the comparison with other poisoning attacks in the research literature, apart from Mehrabi et al. I also see very relevant the comparison with the subpopulation poisoning attacks in Jagielski et al."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676377458,
                "cdate": 1700676377458,
                "tmdate": 1700676377458,
                "mdate": 1700676377458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6efaJX1Ed5",
                "forum": "YLJs4mKJCF",
                "replyto": "R1ximiU9VA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments. Gradient matching attacks are typically used for targeted attacks but for more indiscriminate attacks with a higher amount of poison, assumption 2 can be too strong. This should be discussed further. Actually, Jagielski et al. (\"Subpopulation data poisoning attacks\") make a good analysis of different approximations to this type of optimization problems. I believe this aspect deserves more attention. \n\nApart from this, I see that the first approximation to solve the problem with the BCE loss is reasonable. However, the justification of the approximation in (2) is unclear and, again, it relies on some assumptions that may be reasonable or not. Again, I think this aspect deserves more attention."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676769353,
                "cdate": 1700676769353,
                "tmdate": 1700676769353,
                "mdate": 1700676769353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ba20QLMT97",
                "forum": "YLJs4mKJCF",
                "replyto": "f7RjXrZlZA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments. As mentioned before, I think that the paper needs to compare with other methods in the state of the art, taking into account different fairness metrics. I think that measuring the BCE loss, as reported in the main results of the paper is not the best to assess the quality of the attack in this type of scenarios. On the other hand, the results shown in the appendix (Figure 5) for DP, do not show a clear advantage of the proposed method with respect to the other competing method. In some cases, as for example, for the German datasets, we can see that the DP score decreases when increasing the number of poisoning points. I believe this deserves a more detailed analysis and these experiments should be in the main part of the paper, not in the appendix."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677057798,
                "cdate": 1700677057798,
                "tmdate": 1700677057798,
                "mdate": 1700677057798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SDf2wJMM3q",
                "forum": "YLJs4mKJCF",
                "replyto": "R1ximiU9VA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2584/Reviewer_Ddgu"
                ],
                "content": {
                    "comment": {
                        "value": "After all these comments, I would like to thank the authors for all their effort in addressing my comments and the comments from the other reviewers. I do believe that the research direction of the paper is very interesting, but I also believe that the paper can be improved in different ways: 1) justifying better the threat model and the assumptions made and, perhaps, relaxing some of them; 2) providing a more comprehensive and convincing experimental evaluation, comparing with other methods in the state of the art and paying more attention to the typical metrics used for fair ML. \nAlthough I appreciate the effort, I am keeping my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2584/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677293477,
                "cdate": 1700677293477,
                "tmdate": 1700677293477,
                "mdate": 1700677293477,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]