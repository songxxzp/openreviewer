[
    {
        "title": "SEER: Towards Efficient Preference-based Reinforcement Learning via Aligned Experience Estimation"
    },
    {
        "review": {
            "id": "e0jKTV7aS1",
            "forum": "PH0L3ABwM2",
            "replyto": "PH0L3ABwM2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7946/Reviewer_Atku"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7946/Reviewer_Atku"
            ],
            "content": {
                "summary": {
                    "value": "It is known in PbRL that the inaccurate reward model compounding with overestimation bias on Q functions can lead to sub-optimal policies, and subsequently cause poor trajectory sampling and low feedback efficiency. This paper tackles this issue by proposing a non-parametric, graph-based model to learn a lower bound empirical Q-value, and then combined it with a SAC-style policy learning objective. Although the motivation is good, the proposed method suffers from a number of noticeable drawbacks. Please see the following strengths and weaknesses for detailed comments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The compounding issue of inaccurate reward and overestimation bias is an important problem in PbRL, and worth investigating.\n- The idea of correcting overestimated Q values in online PbRL is interesting.\n- The proposed method has shown reasonable performance and low variance in the test environments."
                },
                "weaknesses": {
                    "value": "- One of the biggest problems of this paper is that its graph-structured non-parameterized model is only applicable to tasks with discrete states space, or tasks with states and actions that can be enumerated (like the image-based task considered in this paper, there are only finite possible image outcomes). For general continuous control tasks, this method is very impractical. This limits the technical contribution of this work.\n- The reason to introduce the non-parametric model as claimed in this paper, is to avoid query unseen actions and avoid overestimation. If this is the case, then why not consider incorporating well-established techniques from offline RL on the replay buffer, like in [1]? Some in-sample learning offline RL methods such as IQL[2] (and a few other methods) can achieve exactly the same purpose but in a much simpler way. The only extra benefit of adopting a graph-based model is the ability to sample new trajectories from the graph, but this part is not carefully ablated, and we do not know whether the performance gains are primarily due to non-overestimated Q values or trajectory sampling.\n- Most of the proposed method is to prevent the overestimation of Q values, and it is only weakly relevant to the PbRL problem. Of course, the inaccurate rewards in PbRL can cause the overestimation issue to have a greater impact, but I do not see too many technical designs that are specifically designed for the PbRL problem.\n- As for Theorem 3.1, the authors only proved $\\hat{Q}$ will lower bound and converge to $Q^*$ learned using Bellman optimality equation under tabular case. However, it says nothing about the property of the final Q-value learned using Eq.(5). There is no analysis on the final Q-value learned with the soft Bellman residual and the distribution-constrained loss, which makes the theoretical analysis insignificant.\n- The evaluations are only conducted in two special test environments that are compatible with the graph-structured model. Common PbRL benchmarks like B-Pref are not evaluated. I suppose the proposed method simply cannot run on such continuous control tasks.\n\n\n**References:**\n\n[1] Ji, T., et al. Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. arXiv preprint arXiv:2306.02865.\n\n[2] Kostrikov I, Nair A, Levine S. Offline Reinforcement Learning with Implicit Q-Learning ICLR 2022.\n\n[3] Lee, K. et al. B-Pref: Benchmarking Preference-Based Reinforcement Learning. NeurIPS 2021."
                },
                "questions": {
                    "value": "- I suspect that there should be a trade-off weight hyperparameter on $L_{dc}$ in Eq.(5). As the bellman loss and the regularization term $L_{dc}$ need to be properly balanced to enable stable learning. Have you used a weight hyperparameter here? And if so, how is it tuned?\n- The variances in PbRL methods are typically large, but the reported variance of the proposed method is surprisingly small. Why is that?\n- The preferences in the experiments are collected from script teachers. How will the method perform if the preference labels come from humans, which contain more noise?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7946/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698409083629,
            "cdate": 1698409083629,
            "tmdate": 1699636976327,
            "mdate": 1699636976327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4TPnX2nN2V",
                "forum": "PH0L3ABwM2",
                "replyto": "ZWmgrL8yPT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Reviewer_Atku"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Reviewer_Atku"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I've read the responses from the authors. However, many of my previous concerns still remain.\n- For the response to Q1, it confirmed my previous understanding that the proposed method is not directly applicable to continuous control tasks, and many task-specific adaptations have to be made before this method can work.\n- For the response to Q2, the method needs to store an expanding graph during policy learning. While it is OK for small-scale problems, but can be costly and memory-inefficient for large problems. Therefore, I'm not convinced that the \"model maintains a lighter computational burden\".\n- For the response to Q3, PEBBLE actually introduces unsupervised pre-training and new query selection schemes to enhance query efficiency; SURF not only introduce semi-supervised learning, but also introduces the temporal cropping technique to improve PbRL performance. These treatments are specifically designed based on the property of PbRL problems. However, I didn't find too many such PbRL-specific designs in this paper, the majority of the method is focused on handling the overestimation issue.\n- For the response to Q4, such an important hyperparameter should be mentioned in the main text rather than in the Appendix.\n- For the response to Q5, my original question is asking why the variance of the proposed method is unnaturally small as compared to common PbRL methods? For some tasks, there is almost no variance in the learning curve. Why is that? My question is not answered.\n- For the response to Q6, as the introduction of this paper clearly uses the benefit of human feedback as a motivation (see also \"human preferences\" in Fig. 1), I feel it is inappropriate to completely ignore experiments with human preference labels. Simply using a script teacher with noisy labels sometimes is insufficient. \n- Lastly, my previous concern about the gap in theoretical analysis in Theorem 3.1 is not discussed in the authors' response."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474640533,
                "cdate": 1700474640533,
                "tmdate": 1700474640533,
                "mdate": 1700474640533,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nMZnqKsphS",
            "forum": "PH0L3ABwM2",
            "replyto": "PH0L3ABwM2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7946/Reviewer_pCcb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7946/Reviewer_pCcb"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers preference-based reinforcement learning and proposes a new policy learning algorithm which can fit into the current framework of PbRL. In each iteration, it first constructs an empirical estimation of the optimal policy and its corresponding distribution-constrained loss. Then it computes the Q function of the current policy via soft Bellman residual and the distribution-constrained loss. After that it applies soft improvement to the current policy with the estimated Q function. \n\nThe authors prove that the empirical Q function is an asymptotical lower bound of the optimal Q function. They also conduct numerical experiments to validate the performance of PEBBLE with the proposed policy learning algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed algorithm seems to achieve better performance with PEBBLE in the empirical tasks than SOTA."
                },
                "weaknesses": {
                    "value": "(1) The proposed algorithm seems to only modify the existing framework of maximum entropy RL a little bit and not very novel.\n\n(2) The paper only shows that empirical Q function is a lower bound of the ground-truth optimal Q function. I think the distribution-constrained loss is more reasonable if the authors can further show the empirical Q function is close to the ground-truth optimal Q function."
                },
                "questions": {
                    "value": "Why do the authors use the KL divergence between $\\hat{\\pi}(s)$  and $\\pi_{ soft(\\theta)}(s)$ as the regularization term? A more intuitive choice would be the direct distance between $Q_{\\theta}$ and $\\hat{Q}$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Reviewer_pCcb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7946/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694119542,
            "cdate": 1698694119542,
            "tmdate": 1699636976207,
            "mdate": 1699636976207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zBe57X8sqX",
                "forum": "PH0L3ABwM2",
                "replyto": "nMZnqKsphS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pCcb"
                    },
                    "comment": {
                        "value": "We thank reviewer pCcb for the constructive comments. We value your insights and address each point you raised. Below are our point-wise responses:\n\n**Q1. The proposed algorithm seems to only modify the existing framework of maximum entropy RL a little bit and not very novel.**\n> **A1**: Thanks for your question. The general framework of PbRL consists of the reward learning and policy learning. SEER employs SAC as its backbone algorithm in policy learning, similar to baselines like PEBBLE [1], SURF [2], and MRN [3]. Main Contirbutions lie in, SEER leverages the non-parametric model to sample informative trajectories and regularize neural Q function for feedback-efficient learning.\n\n**Q2. The paper only shows that empirical Q function is a lower bound of the ground-truth optimal Q function. I think the distribution-constrained loss is more reasonable if the authors can further show the empirical Q function is close to the ground-truth optimal Q function.**\n> **A2**: Thank you for your question. In response to your concern about the empirical Q function's closeness to the ground-truth optimal Q function, we have conducted additional experiments. The results are as follows:\n> \nTable 1: MSE of $\\hat{Q}$ and $Q_\\theta$ across ten runs\n| Task/Q | $\\hat{Q}$ | $Q_\\theta$|\n| -------- | -------- | -------- |\n| Push-5x5-1   | 0.05\u00b10.07  | 0.06\u00b10.10 |\n| Block-shaped | 0.07\u00b10.03  | 0.08 \u00b10.03|\n\n>This result represents the performance of the empirical Q and the neural Q  across different tasks. As indicated by the result, the empirical Q closely approximates the ground-truth optimal Q value.\n\n**Q3. Why do the authors use the KL divergence between $\\hat\\pi(s)$ and $\\pi_{soft(\\theta)}(s)$ as the regularization term? A more intuitive choice would be the direct distance between $Q_{\\theta}$ and $\\hat{Q}$.**\n> **A3**: Thanks for your thoughtful question. The decision to use KL divergence is rooted in our objective to align the learned policy as closely as possible with the optimal policy derived from the Q-function. This choice is inspired by the SAC, where the KL divergence is used to minimize the distance between the current policy and the exponentiated softmax of the Q-function, as detailed in [4] Equation 4.\n\n**Reference**: \\\n[1] PEBBLE: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. ICML, 2021. \\\n[2] SURF: Semi-supervised reward learning with data augmentation for feedback-efficient preference-based reinforcement learning. ICLR, 2022. \\\n[3] Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning. NeurIPS, 2022. \\\n[4] Soft Actor-Critic Algorithms and Applications. ICML, 2018."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222139514,
                "cdate": 1700222139514,
                "tmdate": 1700222139514,
                "mdate": 1700222139514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H7OMyZTYwH",
                "forum": "PH0L3ABwM2",
                "replyto": "zBe57X8sqX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Reviewer_pCcb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Reviewer_pCcb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response! I still feel the novelty of this work is limited, so I will maintain my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579664343,
                "cdate": 1700579664343,
                "tmdate": 1700579664343,
                "mdate": 1700579664343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3d5RkDCs8k",
            "forum": "PH0L3ABwM2",
            "replyto": "PH0L3ABwM2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7946/Reviewer_pgTP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7946/Reviewer_pgTP"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces SEER, an efficient framework for preference-based Reinforcement Learning (PbRL). Traditional RL poses challenges in creating reward functions. SEER tackles this problem by learning rewards based on human preferences among various trajectories, creating a virtuous circle of learning. The main innovation lies in the empirical Q function derived from past trajectories, improving the sampling process and policy regularization. Experimental results demonstrate SEER's significant edge over state-of-the-art methods, particularly in scenarios with limited human feedback."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Innovative Approach: SEER presents a novel method of leveraging historical trajectories to construct an empirical Q function. This Q function aids in bootstrapping and enhances policy learning.\n2. Efficiency: On the domains tested, SEER appears to outperform baselines. \n3. Theoretical Underpinning: The paper provides a theoretical demonstration that the empirical Q function serves as a lower-bound of the oracle Q under human preference."
                },
                "weaknesses": {
                    "value": "The main weakness with the paper is in the experimental evaluation. All of the other baselines test on the same broad suite of simulated robotics tasks. Since this paper follows from those papers and directly compares against them, those environments should definitely be included in the evaluation. \n\nAnother weakness is with the presentation. There are a lot of grammar mistakes throughout, for example the first sentence in abstract should be \u201cOne of the challenges\u2026 \u201c also in abstract \u201coptimizing policies\u201d, also citation (III and Sadigh) should be Hejna III and Sadigh. I think related works could be improved a lot. For example, the main baseline methods are not well-described in this section."
                },
                "questions": {
                    "value": "Evaluation: How well does SEER do on the benchmarks that are common in the literature? Is there a reason those benchmarks are not included? \nGeneralizability: How adaptable is SEER across different domains or problems? Can it be seamlessly integrated into other existing RL algorithms?\nHuman Feedback: How does SEER handle potentially conflicting or inconsistent human preferences? Is there a mechanism to resolve such conflicts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Reviewer_pgTP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7946/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812534235,
            "cdate": 1698812534235,
            "tmdate": 1700590636566,
            "mdate": 1700590636566,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9t6dNPmFTi",
                "forum": "PH0L3ABwM2",
                "replyto": "3d5RkDCs8k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pgTP"
                    },
                    "comment": {
                        "value": "We thank reviewer pgTP for the constructive comments. We will give our point-wise responses below.\n\n**Q1. The main weakness with the paper is in the experimental evaluation. All of the other baselines test on the same broad suite of simulated robotics tasks. Since this paper follows from those papers and directly compares against them, those environments should definitely be included in the evaluation.**\n> **A1**: Thank you for your valuable feedback. SEER primarily utilizes a discrete graph for conservative estimations, which inherently aligns more with discrete action domains and therefore is not very suitable for continuous action domains. However, we highlight that SEER demonstrates the benefit of regularization based on conservative estimation for feedback-efficient learning. This finding opens up a wide range of research possibilities for future studies. In terms of addressing continuous action scenarios, we are considering the adoption of alternative approaches, such as offline RL methods [1], to achieve conservative estimation without explicitly constructing a graph.\n\n\n**Q2. Another weakness is with the presentation. There are a lot of grammar mistakes throughout, for example the first sentence in abstract should be \u201cOne of the challenges\u2026 \u201c also in abstract \u201coptimizing policies\u201d, also citation (III and Sadigh) should be Hejna III and Sadigh. I think related works could be improved a lot. For example, the main baseline methods are not well-described in this section.**\n> **A2**: Thank you for your valuable feedback regarding the presentation aspects of our paper. We will review the entire manuscript to correct these issues. Regarding the citations, we will ensure that all references, such as 'Hejna III and Sadigh', are correctly formatted and cited according to the required style guidelines.\n\n\n**Q4. How adaptable is SEER across different domains or problems? Can it be seamlessly integrated into other existing RL algorithms?**\n> **A4**: Thanks for your question. SEER uses a discrete graph to obtain conservative estimations, which is not very suitable for continuous action domains. However, we have strategies to extend its application to continuous action scenarios. Firstly, one approach is to employ discretization techniques to transform continuous action spaces into discrete equivalents. This method is commonly used in contexts such as robotic arm manipulation, exemplified by RT-2 [2], and in complex games like DOTA [3] and StarCraft [4]. Secondly, SEER can be adapted using an action translator for generating continuous actions, such as the VMG model [5], involves using a graph-structured world model to convert actions via an action translator. The PbRL framework generally contains reward learning and policy learning, with the latter often directly employing standard RL algorithms. SEER is designed to be compatible with any value-based RL algorithm during the policy learning stage.\n\n\n**Q5. How does SEER handle potentially conflicting or inconsistent human preferences? Is there a mechanism to resolve such conflicts?**\n> **A5**: Thanks for pointing our this. While directly resolving such conflicts was not the primary focus of this work, we recognize its importance in the robustness of SEER. To address this, we have conducted experiments to understand how SEER performs under conditions of noisy or conflicting data, which can be seen as a proxy for inconsistent human preferences. Specifically, we tested SEER in Push-5x5-1 and Strip-shaped with varying ratios of noisy data, including {0%, 10%, 15%, 20%, 25%, 30%}. This noisy data simulates scenarios where human preferences might be inconsistent or conflicting. The results, as shown in the following tables, indicate that SEER's performance degrades with increasing levels of noise. It fails with 30% noisy labels in Push-5x5-1 and with 25% noisy data in Strip-shaped.\n\nTable 1: Performance of SEER with different noise levels\n| Noise lable|0\\%   | 5\\%  |10\\%  | 15\\% |20\\%  |25\\%  | 30\\% |\n| --------   | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| Push-5x5-1 | 10.8 | 10.6 | 9.3  | 9.3  | 8.7  | 5.6  | -1.9 |\n|Strip-shaped| 11.4 | 11.2 | 10.5 | 8.0  | 6.6  | 2.1  |   /  |\n\nTable 2: Performance of PEBBLE with different noise levels\n| Noise lable|0\\%   | 5\\%  |10\\%  | 15\\% |20\\%  |25\\%  | 30\\% |\n| -   | -|- | - | - | - | - | ---- |\n| Push-5x5-1 | 6.1  | 5.7  | 1.3  |-0.9  | -9.1 | /    |  /   |\n|Strip-shaped| 7.2  | 5.8  | 3.9  | 1.2  | /    | /    |  /   |\n\n**Reference** \\\n[1] Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv, 2020. \\\n[2] Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv, 2023. \\\n[3] Dota 2 with large scale deep reinforcement learning. arXiv, 2019. \\\n[4] Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature 575.7782 (2019): 350-354. \\\n[5] Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning. ICLR, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221872345,
                "cdate": 1700221872345,
                "tmdate": 1700221872345,
                "mdate": 1700221872345,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yrRDzdyBm2",
                "forum": "PH0L3ABwM2",
                "replyto": "3d5RkDCs8k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A mild reminder"
                    },
                    "comment": {
                        "value": "We are grateful to you for the feedback. We have endeavored to address the issues highlighted in your review. It would be helpful to know if the revisions we have made align with your expectations and satisfactorily address your concerns. If you find that they do, we would appreciate it if you could consider reflecting this in an updated evaluation of our work. Our aim is to ensure that the changes we have implemented effectively improve the paper's quality. We are fully open to further discussion and eagerly await any additional guidance or questions you may have."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585739621,
                "cdate": 1700585739621,
                "tmdate": 1700585739621,
                "mdate": 1700585739621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RFogn04jYe",
                "forum": "PH0L3ABwM2",
                "replyto": "yrRDzdyBm2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Reviewer_pgTP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Reviewer_pgTP"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. I have bumped my score up but still vote for reject because there isn't experiments on the standard benchmarks."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590625558,
                "cdate": 1700590625558,
                "tmdate": 1700590625558,
                "mdate": 1700590625558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WinKPLYTvW",
            "forum": "PH0L3ABwM2",
            "replyto": "PH0L3ABwM2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7946/Reviewer_AZLG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7946/Reviewer_AZLG"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a novel framework for preference-based reinforcement learning, asserting that their results enhance label efficiency. They provide experimental verification of this claim.\n\nIn my understanding, the proposed framework is roughly as follows. \n\nAfter unsupervised exploration without rewards, iterate \n* 1) Sample from trajectories based on the constructed graph and get the preference feedback  \n* 2) Update rewards from a pair of trajectories and preferences.\n* 3) Update a graph and an empirical Q-function with updated rewards  (in a conservative way) \n* 4) Learn a soft Q-function and an associated policy by using soft Bellman loss + regularization based on a policy corresponding to an optimal policy from the empirical conservative Q-function in Step 3."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The framework appears to be novel. Experimental results are solid."
                },
                "weaknesses": {
                    "value": "Certain aspects of the paper remain unclear. My primary concern revolves around the justification for the effectiveness of the proposed framework. While there are several intuitive statements provided and there are solid experiments, they often lack the formal exposition for readers to gain a comprehensive understanding.\n\n* I comprehend the author's assertion regarding the conservatism of the empirical Q-function. However, I am seeking clarification regarding the formal properties of the resulting policy, denoted as SAC $\\pi_{\\phi}$. Are we anticipating it to exhibit conservatism or optimism? Additionally, the author contends that it \"aligns with human preference.\" Could this alignment be elucidated in a more rigorous manner?\n\n* The author state that \"theoretically, we demonstrate that the empirical Q-function is a lower-bound of ....\" in the Abstract. However, it is challenging to discern the precise details from Theorem 3.1. in a main text. Several elements remain undefined, such as the exact meanings of $Q_t$ and $\\hat Q_t$ in the main text, as well as the underlying assumptions (e.g., do we need assumptions for rewards to say $\\hat Q_t$ converges to $Q^{\\star}$?  ) \n\n* The proposed framework appears to be tailored for tabular settings, primarily due to its reliance on an empirical Q-function. How does the author intend to extend this approach to accommodate continuous settings, particularly in terms of both algorithmic and theoretical considerations?\n\n* In a related context, update (3) seems somewhat naive in addressing the data coverage concern. I agree it might be beneficial to differentiate between actions that have not been visited, actions that have been visited. But how can we distinguish actions that fall in between \u2013 perhaps those visited frequently versus those visited infrequently?"
                },
                "questions": {
                    "value": "I raised several questions in the weakness part. Furthermore, \n\n* Would you explain Line 8 in Algorithm 1 more? How did you sample a pair of trajectories? Does this correspond to the \"Sampling informative trajectories part\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7946/Reviewer_AZLG"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7946/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698907685809,
            "cdate": 1698907685809,
            "tmdate": 1699636975994,
            "mdate": 1699636975994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f8ddzt1kJ3",
                "forum": "PH0L3ABwM2",
                "replyto": "WinKPLYTvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AZLG"
                    },
                    "comment": {
                        "value": "Thank you, Reviewer AZLG, for your constructive comments. We appreciate the insights and suggestions provided. Below are our point-wise responses addressing each of your concerns:\n\n**Q1. I comprehend the author's assertion regarding the conservatism of the empirical Q-function. However, I am seeking clarification regarding the formal properties of the resulting policy, denoted as SAC $\\pi_\\phi$. Are we anticipating it to exhibit conservatism or optimism? Additionally, the author contends that it \"aligns with human preference.\" Could this alignment be elucidated in a more rigorous manner?**\n> **A1**: Thanks for your question. In our approach, the primary objective is to obtain an accurate Q function to enhance policy learning in PbRL. To this end, we employ an empirical Q-value, which embodies a conservative estimation approach, to constrain the neural Q function. This strategy is designed to accelerate and refine the policy learning process. Additionally, the term 'aligns with human preference' in this context implies that a more precise Q-value estimation correlates with greater alignment to human preferences within the PbRL framework.\n\n**Q2. The author state that \"theoretically, we demonstrate that the empirical Q-function is a lower-bound of ....\" in the Abstract. However, it is challenging to discern the precise details from Theorem 3.1. in a main text. Several elements remain undefined, such as the exact meanings of $Q_t$ and $\\hat{Q}_t$ in the main text, as well as the underlying assumptions (e.g., do we need assumptions for rewards to say converges to?)**\n> **A2**: Thank you for your valuable feedback. $Q_t$ represents the Q-values at time step t learned following the Bellman optimality equation, which is a standard approach in reinforcement learning for estimating the expected utility of actions in a given state. $\\hat{Q}_t$ denotes the Q-values estimated at the same time step t but derived from our empirical approach as outlined in Eq. (3). The theorem and its proof are constructed under the general RL framework, without additional assumptions about their nature or convergence properties.\n\n**Q3. The proposed framework appears to be tailored for tabular settings, primarily due to its reliance on an empirical Q-function. How does the author intend to extend this approach to accommodate continuous settings, particularly in terms of both algorithmic and theoretical considerations?**\n> **A3**: Thank you for your insightful question. To adapt our framework to continuous settings, there are several strategies. Firstly, it can consider the incorporation of discretization techniques to transform continuous action spaces into discrete equivalents. This method is common in similar contexts; for instance, RT-2 [1] successfully applies it for discretizing actions in robotic arm applications. Secondly, it can also introduce an action translator to generate continuous actions, akin to the VMG model [2], which uses a graph-structured world model and convert generated actions via an action translator.\n\n**Q4. In a related context, update (3) seems somewhat naive in addressing the data coverage concern. I agree it might be beneficial to differentiate between actions that have not been visited, actions that have been visited. But how can we distinguish actions that fall in between \u2013 perhaps those visited frequently versus those visited infrequently?**\n> **A4**: Thank you for your valuable feedback regarding update rule (3) in our paper. In our method, we use $N(s,a,s')$ to represent the number of visits to different actions. The empirical dynamics $\\widehat{p}(s'|s,a) = N(s, a, s')/\\sum_{s'} N(s, a, s')$ already encapsulate a distinction in visit frequencies. For instance, actions that have been visited frequently will have a higher value of $N(s, a, s')$, reflecting in a higher empirical probability, whereas infrequently visited actions will have lower values. This differentiation is crucial for ensuring that our update mechanism does not overlook less explored actions.\n\n**Q5. Would you explain Line 8 in Algorithm 1 more? How did you sample a pair of trajectories? Does this correspond to the \"Sampling informative trajectories part\"?**\n> **A5**: Thanks for your question. Yes, Line 8 indeed corresponds to the 'Sampling informative trajectories' part of our algorithm.\n\n\n**Reference** \\\n[1] Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv, 2023. \\\n[2] Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning. ICLR, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221651909,
                "cdate": 1700221651909,
                "tmdate": 1700221651909,
                "mdate": 1700221651909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ePQJmAZOKW",
                "forum": "PH0L3ABwM2",
                "replyto": "WinKPLYTvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A mild reminder"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments. And we would like to know whether we have addressed your concerns. If so, might you be able to update your rating to reveal this?  Our goal is to ensure that our responses and modifications meet your expectations and enhance the quality of our work. We are eager to continue this dialogue and are available for any further discussion that may be helpful."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585665958,
                "cdate": 1700585665958,
                "tmdate": 1700585665958,
                "mdate": 1700585665958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "asu8KY6NmU",
                "forum": "PH0L3ABwM2",
                "replyto": "f8ddzt1kJ3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7946/Reviewer_AZLG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7946/Reviewer_AZLG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the detailed response. I will take this into account in the discussion phase. But, I think my big questions are still remaining. \n\n* Q1. Answer to \"I am seeking clarification regarding the formal properties of the resulting policy.... \"\n\nI am not sure my intention is properly conveyed. I want to see more quantitative formal explanations, not informal explanations. For example, why can you claim in your response, \"This strategy can accelerate and refine the policy learning process. \"?  There is a gap. Fundamentally,  my biggest question is the formal properties of resulting output policies. It is still not explained properly. \n\n* Q1. Answer to --- Additionally, the author contends that it \"aligns with human preference.\" Could this alignment be elucidated in a more rigorous manner?\"----- \n\nI feel the author's response is still hard to decode. I am curious how the author really showed \"a more precise Q-value estimation correlates with greater alignment to human preferences within the PbRL framework\"?   I want to see some theorem here in the ideal case. Or is it just an informal conjecture? \n\n* Q3. The author says discretization works in a continuous setting. But, why do we need to bypass empirical Q-functions to get conservative values? Why don't we get conservative values directly? There are tons of more formal works to do that in offline RL without discretization. \n\n* Answer to Q2.  Here, my suggestions is to change the way of writing the theorem to a more informal one. \"Standard approach \" does not mean readers can see what is $\\hat Q_t$ exactly. Of course, readers can roughly guess. But, when people write a theorem, every part must be clearly defined.  This would not be a proper way to write a theorem. \n\n* Answer to Q2:  Answer to \"The theorem and its proof are constructed under the general RL framework, without additional assumptions about their nature or convergence properties.\"\n\n I am not sure what you mean by \"general\".  What do you mean by \"without additional assumptions \"?  I am suggesting you clearly specify in the main text. For example, did you assume environments are tabular?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7946/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587106891,
                "cdate": 1700587106891,
                "tmdate": 1700587106891,
                "mdate": 1700587106891,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]