[
    {
        "title": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks"
    },
    {
        "review": {
            "id": "meQBhtAi8m",
            "forum": "LfhG5znxzR",
            "replyto": "LfhG5znxzR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2820/Reviewer_v2JW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2820/Reviewer_v2JW"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the concept of \"codebook features\" to make the hidden states of neural networks sparse, discrete, and more interpretable. By introducing a vector quantization bottleneck at each layer of the network, the authors achieve this sparse and discrete representation with only a modest performance degradation. The resulting codebook features serve as a promising unit for understanding and controlling neural network behavior, validated through experiments on finite state machines and large-scale language models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors show a method of using codebook features introduces a novel way to create sparse and discrete hidden states in neural networks. This approach facilitates the unsupervised discovery of both algorithmic and linguistic features within language models, tackling challenges like the superposition problem and thereby advancing the field of interpretability.\n\n* The paper successfully demonstrates that the sparse, discrete nature of codebook features simplifies the complexity of a neural network's hidden state. This makes it easier to identify specific features and control network behavior, suggesting that this could be a powerful tool for more granular and sophisticated control in future applications."
                },
                "weaknesses": {
                    "value": "* While Transformers are prevalent, there are many architecture differences between different models (e.g. novel layers, group-query attention, etc.). In this sense the study is limited in scope by focusing only on Transformer neural networks and examining their performance on a singular algorithmic dataset and two natural language datasets. This leaves unanswered questions about the generalizability of codebook features to other neural network architectures or different types of data, such as visual information.\n\n* While the paper demonstrates the capability of codebook features in topic manipulation for language models, it does not explore other linguistic features like sentiment, style, or logical flow. This limitation narrows the understanding of how versatile and broadly applicable codebook features might be for controlling various aspects of neural network behavior."
                },
                "questions": {
                    "value": "1. In authors' two-phase method for understanding and controlling the network's behavior, you focus on generating hypotheses for the role of codes and then steering the network by activating these codes. How robust is this method to the presence of adversarial or noisy input?\n\n2. Authors mention that codebook features reduce the complexity of a neural network\u2019s hidden state, making it easier to control the network\u2019s behavior. Could you provide more details on the trade-offs involved? Specifically, how does the use of codebooks affect the model's capacity for generalization across different tasks or data distributions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698427717714,
            "cdate": 1698427717714,
            "tmdate": 1699636225791,
            "mdate": 1699636225791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q2zPmzgfAf",
                "forum": "LfhG5znxzR",
                "replyto": "meQBhtAi8m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review! We are glad they thought highly of the soundness, presentation, and contribution of the work.\n\n**Generalizability to other forms of neural networks**\n\nIt is certainly true that we focus on Transformer neural networks in this work due to how widely used the architecture is. We present results applying codebook features to different parts of the transformer (attention and MLP layers), different size transformers (from 1 to 24 layers) and different datasets (synthetic data to natural language). We agree that an exciting direction for future work is in extending our method to other modalities and network architectures.\n\n**Other ways to control the network such as sentiment, style, and logical flow**\n\nWe certainly agree; as we note in our paper, this is an exciting opportunity for future work.\n\n**How robust is the method of generating hypotheses and then steering the network?**\n\nIn our experiments, we were able to find topic codes across a wide range of inputs, including a wide range of different domains within Wikipedia as well as the TinyStories dataset. However we agree that studying the relationship between codebooks and adversarial robustness would be interesting more broadly; we discuss this direction in a bit more detail in Appendix G3.\n\n**Tradeoffs of codebook features and generalization to other data distributions**\n\nYes, there are a few trade-offs to using codebooks. First, codebooks require additional computations to be performed on the network in exchange for their interpretability benefits. Second, the performance of the codebook model is often slightly lower than the original model, although the difference is small in practice (Table 2). In general, the loss of a language model is a good measure of its performance across distributions it is evaluated on, so the small difference in loss between the codebook and base models aligns with our experience that the model continues to perform well across distributions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204043761,
                "cdate": 1700204043761,
                "tmdate": 1700204043761,
                "mdate": 1700204043761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5P4BbZYayO",
            "forum": "LfhG5znxzR",
            "replyto": "LfhG5znxzR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2820/Reviewer_aakk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2820/Reviewer_aakk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to select Top-k hidden units with the highest similarity score to learn sparse and discrete codebook features in an unsupervised way. They have also tried to apply this technique to transformers for language modeling tasks. Experiments show that the model can do well in the task of topic manipulation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow.  The proposed method is straight forward. The experiment results are interesting in table 4. I believe that the proposed method may be applicable to many use cases, which potentially can lead to applications in future work."
                },
                "weaknesses": {
                    "value": "1. The paper is missing the comparison for computational time. With a sparse and discrete codebook, it should lead to increased efficiency.\n2. A chart or figure showing the learned topics for each layer may be missing."
                },
                "questions": {
                    "value": "1. How do you find the activated codes and their corresponding topics? How do you know for example 'code 123' leads to the topic of 'dragon'?\n2. Will the model lead to better computational time?\n3. There are dead codes during training and how do we avoid them? Will the activated codes learn repeated semantics (For example, will one of the codes be activated all the time)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2820/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2820/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2820/Reviewer_aakk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768846775,
            "cdate": 1698768846775,
            "tmdate": 1699636225703,
            "mdate": 1699636225703,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IK63oeyYzK",
                "forum": "LfhG5znxzR",
                "replyto": "5P4BbZYayO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review! We are glad they find the results interesting and applicable in a wide range of use cases.\n\n**Computational efficiency and sparsity**\n\nThis is a good question. We have provided some information about computational efficiency in Table 9. Indeed, leveraging sparsity through fast maximum inner product search does increase the throughput of the model. We suspect that custom sparse kernels would continue to increase the efficiency of codebook features further.\n\n**How do we associate codes with topics?**\n\nWe follow a two-stage process. First, we take the code and retrieve examples where that code is activated. A person (or in the future, a language model) then looks at the examples and develops a hypothesis for the role of the code (e.g. \u201cdragon,\u201d if many dragon-related passages are retrieved). We then verify this role by activating the code during generation and seeing if the model\u2019s behavior changes as we expected. Section 2.2 describes this process in more detail, and Sections 3 and 4 describe our automated evaluations for verifying the role of codes.\n\n**How are dead codes avoided?**\n\nWhile we see some number of dead codes during training, they do not significantly impact the success of the method. For example, in the 410M parameter model we train, fewer than 2% of codes were dead across the 384 codebooks we train on all the attention heads. Moreover, these dead codes do not appear to negatively affect the performance of the model, and they can be discarded after training.\n\n**Are some codes activated all the time?**\n\nWe do see a small number of codes that are activated very frequently (see Figure 7). However, the majority of codes are activated far more rarely. The codes that occur most frequently tend to be associated with very general topics or very common words (e.g. articles like \u201cthe\u201d)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203935561,
                "cdate": 1700203935561,
                "tmdate": 1700203935561,
                "mdate": 1700203935561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ld7BlA4r8g",
                "forum": "LfhG5znxzR",
                "replyto": "IK63oeyYzK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2820/Reviewer_aakk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2820/Reviewer_aakk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' reply. I think most of my concerns have been addressed. I think it would be better if the author could explain more about the motivation for why we need to use sparse and discrete codes in the main paper. Could we just use discrete codes or just sparse codes? Why do we need to have both discrete and sparse? I will keep my score as it is."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612707807,
                "cdate": 1700612707807,
                "tmdate": 1700612707807,
                "mdate": 1700612707807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DPyqbcUmQI",
            "forum": "LfhG5znxzR",
            "replyto": "LfhG5znxzR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2820/Reviewer_AYvz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2820/Reviewer_AYvz"
            ],
            "content": {
                "summary": {
                    "value": "In this work, a method is proposed to improve the interpretability and controllability of a transformer network by quantizing the activations per token with a sparse combination of entries from a codebook. To select the sparse codes, the cosine similarity between token activations and codebook entries are computed and a weighting is taken based on top-k most similar entries. Experiments on a dataset of state transitions and on language datasets show that the use of codebook entries correlate with certain aspects of the dataset (e.g., states or semantic concepts). Furthermore, experiments show that codebook entries can be applied as a token to steer the output of the transformer network."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This work considers the problem of interpreting the intermediary layers and controlling the output of transformer networks, which are of significant interest to the machine learning community. Furthermore, quantizing features with codebooks is a popular technique which the work demonstrates leads to a minimal degradation in model performance.\n* Experiments with the TokFSM dataset are intuitive and clearly demonstrate the ability to intervene in the output of a transformer trained with codebook quantization. Experiments such as the JS divergence with the target token distribution in Figure 4, are used to demonstrate effective intervention. Given that one has access to the semanticity of entries in the learned codebook, the proposed method seems to be effective at steering the output of transformer network.\n* The work contains a comprehensive related works section in the appendix that clarifies the benefits of a discrete codebook over using a dictionary approach (referred to as \u201cfeatures-as-directions\u201d)"
                },
                "weaknesses": {
                    "value": "* One shortcoming of the experiment in table 1 is that it has two dependent variables: both the quantization level k and the codebook size C. It is important to ablate changes to these two variables separately to better understand if they both independently provide benefits. There is a similar issue of two dependent variables being tested at the same time in Table 2 where both k is modified as well the features being quantized (i.e., attention vs mlp features). \n* The benefit of sparsity in the combination of codebook entries has not been clearly articulated. In fact, in section F.1.1 of the manuscript, it is argued that the continuous combination of different atoms reduces interpretability. Selecting codebook entries via top-k cosine similarity is a naive approach that has not been compared to more recent sparse coding techniques (e.g., variational sparse coding methods like in Tonolini et al. 2020 and Fallah et al. 2022). Except for the potential increase in modeling capacity, the work does not demonstrate the benefit of quantizing with multiple codebook entries per token.\n* Interpretation of codebook entries still seems to require manual intervention. It requires a user to find input data for which a codebook entry is often activated, which can be timely and costly. It is unclear how one would use current methods to find a codebook entry that corresponds with a certain semantic concept without manually performing a forward pass using data corresponding to that concept.\n\n\nMinor:\n* Some citations need revising in the bibliography (e.g., \u201cOn the role of scientific thought\u201d)."
                },
                "questions": {
                    "value": "* Can the authors clarify the benefit from increasing k? Since the authors use cosine similarity between the activations and the codebook to pick the top-k entries, what would the difference be between each of these k codebook entries? It seems that taking k entries contradicts the viewpoint of \u201cfeatures-as-points\u201d, and may even lead to what the authors refer to as \u201csmuggling of information\u201d. I would expect that increasing k may improve performance of the model, at potential cost to interpretability.\n* Out of curiosity, have the authors considered quantizing each vector along the feature components (i.e., divide the N features of each token into k blocks)? Is this what the authors refer to as \u201cgrouped codebooks\u201d in E3? If so, I believe this warrants discussion and attention in the main text. This would be an alternative to quantizing each token with k codebook entries that are very close in cosine similarity and be closer to the VQ-VAE setting.\n* The notation and presentation of the different loss terms in section 2.1 can be made more clear. In the cross-entropy loss, x is used to denote a categorical random variable corresponding to a token being selected. In the reconstruction loss, to my understanding, x is referring to a continuous random variable corresponding to the activation in an intermediary layer, even though the variable a is used in an earlier section. Furthermore, k-codes are combined to quantize each activation. Is the MSE taken with the sum of these codes or each code individually?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798706137,
            "cdate": 1698798706137,
            "tmdate": 1699636225625,
            "mdate": 1699636225625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NcsUpth8Gf",
                "forum": "LfhG5znxzR",
                "replyto": "DPyqbcUmQI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review!\n\n**Untangling dependent variables in Table 1**\n\nThis is a good point. To address this issue, we reran our experiments to hold $C$ constant while changing $k$. We include these results in Table 1, which mirror the results from our previous version. We also include an experiment with an MLP codebook with $k=8$ and $16$ groups in Table 2; this enables a direct comparison to the attention $k=8$ model because our model has $16$ attention heads.\n\n**Reason for using multiple codes per layer**\n\nYes, the primary benefit of working with $k>1$ codebooks is expressivity; because models likely represent more than 1 feature per layer, our interpretability method needs to be able to model multiple features. While we chose a simple method for our initial experiments, we agree that more sophisticated methods, including variational sparse coding approaches, are promising. We have mentioned this, including Tonolini et al. 2020 and Fallah et al. 2022, in our newest revision.\n\n**What happens with increasing k?**\n\nYes, increasing $k$ does improve the performance of the codebooks model. In our updated Table 1, we show several runs where we only change $k$, demonstrating steady improvements in performance. In terms of interpretability, we agree that in the limit increasing $k$ could make the hidden state harder to understand. However, as Table 5 shows, even large values of $k$ transmit many fewer bits of information than a standard neural network layer, limiting the complexity of the hidden state. We will include this discussion in our revision.\n\n**Additional motivation for top-k, and whether the top-k codes have different roles**\n\nHigh dimensional spaces enable code vectors containing different information to be selected through top-k cosine similarity. As an example, consider the vector [$a$; $b$], the concatenation of two smaller vectors $a$ and $b$. This vector would have relatively high cosine similarity with [$a$; 0] and [0; $b$], despite the two vectors being potentially very distant from each other. Thus, we believe the $k>1$ models are fully compatible with the features-as-points viewpoint. However, there are other cases we have seen where several of the $k$ codes appear to activate on similar tokens. This may indicate that the model needs to represent fewer features than $k$, or that the redundancy is helpful for the network computation. \n\n**Is interpretation manual?**\n\nYes, currently interpreting the codebook features requires manual interpretation. However, other work (e.g. [1, 2]) has demonstrated that language models can be used to interpret neurons or features as well, which we expect to be a more scalable solution.\n\n**Does interpretation require forward-passes?**\n\nYes, currently interpretation does require a forward pass over a subset of the data. However, forward passes on this subset of data can be computed once and then the resulting data can be used to interpret the entire network. In practice, we expect the amount of compute needed to do this will be much smaller than that used to finetune the network to produce the codebook features.\n\n**Grouped codebooks**\n\nYes, we believe the reviewer\u2019s description is the same as our grouped codebooks method. We have included more discussion of this in the paper, however we note (per the example given in the previous paragraph) the $k>1$ codebooks can implement something quite similar to grouped codebooks. Thus, it is possible the primary benefit of grouped codebooks may be in improving optimization of the network, rather than in the expressivity or interpretability of the codebook itself.\n\n**Notation in 2.1**\n\nYes, for the reconstruction loss, $x$ refers to the continuous activations. We changed this to $a$ in the most recent revision. The MSE is taken with respect to the output of the codebook (i.e., the sum of the codes).\n\n[1] Natural Language Descriptions of Deep Visual Features, Hernandez et al 2022\n\n[2] Language models can explain neurons in language models, Bills et al 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203828883,
                "cdate": 1700203828883,
                "tmdate": 1700203828883,
                "mdate": 1700203828883,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rNBMCv2hVM",
                "forum": "LfhG5znxzR",
                "replyto": "NcsUpth8Gf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2820/Reviewer_AYvz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2820/Reviewer_AYvz"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications in your response to the review. It is still unclear to me how to reconcile the use of `k > 1` with the \"features-as-points\" perspective. You provided one possible way this may occur, but is there any evidence that suggests that the `k` different codes represent different features? I view this as an important point, since the possibility of the network learning something closer to \"features-as-directions\" makes the token selection strategy (and potentially the weighting of the discrete tokens) very important. Also, the work currently contrasts codebook features with dictionary learning by stating the former prevents \"smuggling of information.\" This seems to contradict the fact that with `k > 1` this phenomena would likely occur."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436128498,
                "cdate": 1700436128498,
                "tmdate": 1700436128498,
                "mdate": 1700436128498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "00YDRIzgbN",
            "forum": "LfhG5znxzR",
            "replyto": "LfhG5znxzR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2820/Reviewer_XCvz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2820/Reviewer_XCvz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the discretization of intermediate features within deep Transformers, ensuring that the network's outputs (decisions) are contingent solely on finite, interpretable, and sparse codes. The authors demonstrate that by exploring the connections between specific codes and semantic or high-level topics and by adjusting these intermediate codes, users can exert intuitive control over the network's behavior. A comprehensive set of experiments reveals that the modified networks maintain competitive performance levels after fine-tuning."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors introduce an intriguing inquiry into the performance of deep Transformers when their intermediate features are discretized. Surprisingly, the results appear promising for both small-scale and large-scale Transformers.\n* Introducing discrete features (codes) which are shown associated to specific semantics in the paper brings interpretability to some extent. More importantly, such an approach enables users to control the models' output by modifying codes that have human understandable semantics. In fact, a similar approach has been introduced in computer vision to facilitate the creation of interpretable inference procedures [1] and controllable image synthesis [2].\n* The paper is technically sound, and it provides a thorough discussion of the related literature.\n* The paper is well-written and easy to follow.\n\n\n>[1] Schema Inference for Interpretable Image Classification. (ICLR 2023)\n>\n>[2] Taming Transformers for High-Resolution Image Synthesis. (CVPR 2021)"
                },
                "weaknesses": {
                    "value": "[Major]\n1. **Experiments:** As mentioned in the paper that a code can be related to some specific semantics; however, the results supporting such claim appears insufficient. The authors may consider conducting comparative analyses of the distribution disparities between code associated with similar and dissimilar semantics to substantiate this claim. \nIn addition, is it possible that a (some) certain code(s) may correspond to a multitude of distinct semantic contexts?\n2. **Experiments:** It is interesting that model outputs is controlled by the codes. However, based on the results on TokFSM dataset, it appears that the code following the MLP layer plays a more significant role. Nevertheless, the experiments conducted by the authors on WikiText-103 dataset only involve discrete attention layers (in Table 2 (b)). Does this incongruity potentially render it challenging to control the model?\n3. \n4. **Experiments:** In Section 2, the authors sum top-k codes weighted by the same value (specifically, 1). How will each code influence to the model's decision? (For example, does codes having semantics related to the target task contributes most to the model's decision evaluated by attribution methods?) In particular, the authors can utilize feature attribution methods [3-5] to present quantitative and qualitative analyses.\n5. What is the extent of the contributions made by these pieces of code to the final outcome? It may be worthwhile to investigate this using feature ablation techniques to discern whether the words crucial for the model's decision-making align with human intuition.\n\n    >[3] Deep inside convolutional networks: Visualising image classification models and saliency maps.\n    >\n    >[4] Did the model understand the question? (ACL 2018)\n    >\n    >[5] Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions.\n\n[Minor]\n1. The font size in the figures is excessively small, making them particularly challenging to decipher when printed (e.g., Figure 1, 2 and 3). Furthermore, it is advisable for the authors to employ vector graphics to enhance the quality of the illustrations.\n2. The authors do not provide codes for reproducibility check.\n3. The authors could provide some failure cases to facilitate further analysis of how the proposed method yields incorrect results. If feasible, this could also serve as a basis for advancing future work."
                },
                "questions": {
                    "value": "My questions are listed in the \"Weaknesses\" section. I am looking forward to the authors' relply."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2820/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2820/Reviewer_XCvz",
                        "ICLR.cc/2024/Conference/Submission2820/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2820/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836240509,
            "cdate": 1698836240509,
            "tmdate": 1700449602119,
            "mdate": 1700449602119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mqwFUOKrTV",
                "forum": "LfhG5znxzR",
                "replyto": "00YDRIzgbN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2820/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2820/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review! We are glad they found the results surprising and promising.\n\n**Can multiple concepts be associated with a single code?**\n\nIt is certainly possible that a code could be associated with multiple distinct concepts. We study this in Appendix D.4 and Figure 8, where we find that almost every code holds a single concept at the first layer. In later layers, we find that more concepts share a code, however we observe a good reason for this\u2014namely, that these concepts are related to one another and tend to share the same next token (Figure 9). We agree that further investigation of this question is a fruitful direction for future work and have noted this in our revision.\n\n**Why we use attention codes in one model and MLP codes in another**\n\nThis is a good question. The reason we use different types of codes here is that we are trying to control different aspects of the sequence/text in each model. In the TokFSM environment, we are trying to alter the prediction of an individual state/token. We find codes in the MLP layers are most associated with these single tokens. For the language modeling experiments, we are trying to alter the global topic of a generation. Topics typically manifest across many tokens, rather than a single token, and we find the attention layers are most associated with these features. However, we believe it is likely that, e.g., for more local linguistic features (such as word choice) editing the MLP codes in a language model may prove to be the best choice.\n\n**\u201dIn Table 3, the authors have presented PriViT and MPCViT in distinct hyper-parameter configurations, giving rise to concerns regarding the fairness of the comparisons.\u201d**\n\nWe believe this comment may be about another paper, as we do not present or discuss results on methods called PriViT or MPCViT.\n\n**How does each code influence a model\u2019s decisions?**\n\nThis is a good question. To evaluate this, we choose several families of topic codes and compute how varying the number of topic codes we activate changes the fraction of generations that follow that topic. As expected, we find that increasing the number of topic codes changes the fraction of decisions (Figure 10). Qualitatively, when we look at the activated tokens, we note that there is significant overlap in the tokens activated by each code; however, there are still many tokens which are activated by some codes and not others, suggesting that the multiple codes together may help the network form a more robust version of the topic concept.\n\n**Computer vision papers**\n\nWe thank the reviewer for the pointers to related work and have included them in our most recent revision. \n\n**Code for reproducibility**\n\nWe have included an anonymized codebase in the newest supplementary materials of our paper. We have also uploaded the same codebase along with our model weights here: https://drive.proton.me/urls/C205JG7GQM#l7CfinlSZUHM\n\n**Vector graphics**\n\nThe newest version of our paper now uses vector graphics for most of the figures, which should result in improved readability. We thank the reviewer for the suggestion."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203154512,
                "cdate": 1700203154512,
                "tmdate": 1700203295414,
                "mdate": 1700203295414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ktKAO7GfUh",
                "forum": "LfhG5znxzR",
                "replyto": "mqwFUOKrTV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2820/Reviewer_XCvz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2820/Reviewer_XCvz"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' reply, and my main concers have been addressed. In addition, I hope the authors could add the discussion  about \"Question 2\" in the revised version. In spite of the interesting topic of the paper, I am not an expert in NLP, and I will keep my current score (6)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2820/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449824309,
                "cdate": 1700449824309,
                "tmdate": 1700449824309,
                "mdate": 1700449824309,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]