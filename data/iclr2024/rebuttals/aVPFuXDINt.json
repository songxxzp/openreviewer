[
    {
        "title": "DURENDAL: Graph deep learning framework for temporal heterogeneous networks"
    },
    {
        "review": {
            "id": "gwyulHh0Wr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5264/Reviewer_fQVE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5264/Reviewer_fQVE"
            ],
            "forum": "aVPFuXDINt",
            "replyto": "aVPFuXDINt",
            "content": {
                "summary": {
                    "value": "This paper introduces DURENDAL, a deep learning framework tailored for THNs. DURENDAL adapts to evolving networks and offers two methods to update embeddings. Through testing on new datasets, including one from a Web3 platform and an e-commerce site, DURENDAL proves to be more effective in predictive tasks compared to existing models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The authors provide a large number of experiments to analyze the effectiveness of the model.\n2. THGs are worth exploring."
                },
                "weaknesses": {
                    "value": "1. The shortcomings of other THNs aren't clarified clearly. For instance, what does *easily incorporate state-of-the-art designs from static GNNs* mean?  And what are the specific drawbacks of these methods? The current presentation lacks clarity, diminishing the paper's motivation when compared to other THNs.  Besides, related work should be cited in the introduction section.\n2. This paper's contribution is limited for ICLR standard. The authors primarily employ the ROLAND framework and conventional techniques for heterogeneous graphs. Despite its efficacy, it lacks innovation, potentially falling short of ICLR's acceptance criteria.\n3. Recent studies on THNs warrant citation and comparison.\n* (1)Fan, Yujie, et al. \"Heterogeneous temporal graph neural network.\"\u00a0Proceedings of the 2022 SIAM International Conference on Data Mining (SDM). Society for Industrial and Applied Mathematics, 2022.\n* (2)Yang, Qiang, et al. \"Interpretable Research Interest Shift Detection with Temporal Heterogeneous Graphs.\"\u00a0Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. 2023.\n4. The presentation of this paper is poor.  Additionally, there are typographical errors in the article, such as writing THG as TNH."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697456448969,
            "cdate": 1697456448969,
            "tmdate": 1699636525780,
            "mdate": 1699636525780,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ERU08S4tkj",
                "forum": "aVPFuXDINt",
                "replyto": "gwyulHh0Wr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Shortcomings of other Temporal Heterogenous Graph Learning models"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing our contribution to a subfield worth exploring and that we provide a large number of experiments to validate our model. Below we address specific points of the review in detail. We ask the reviewer to increase their score if we address all their main concerns.\n\n> **W1** The shortcomings of other THNs aren't clarified clearly. For instance, what does easily incorporate state-of-the-art designs from static GNNs mean? And what are the specific drawbacks of these methods? The current presentation lacks clarity, diminishing the paper's motivation when compared to other THNs. Besides, related work should be cited in the introduction section.\n\nTemporal heterogeneous networks (THNs) are the datasets on which we perform forecasting tasks. Hence, \u201cother THNs\u201d haven\u2019t shortcomings. Current models that learn from THNs have shortcomings. Regarding temporal graph neural networks (Temporal GNNs), current state-of-the-art approaches are designed for temporal homogeneous graphs only. Factorization-based models (FMs), which represent state-of-the-art models for reasoning over temporal knowledge graphs (the most well-known examples of THNs), are heavily designed to solve link prediction, specifically knowledge graph completion, and cannot be used to make forecasting or any other graph learning task (e.g. node classification), as highlighted in https://openreview.net/pdf?id=Sq9Orta9l5i . For those concerning GNNs for Temporal Heterogeneous Networks, most of the models use a fixed split setting to evaluate the performance of dynamic link prediction (https://dl.acm.org/doi/abs/10.1145/3534678.3539300 ), or do not evaluate it at all (https://dl.acm.org/doi/10.1145/3366423.3380027  see also official comment to reviewer **fYw8). All these statements are already included in the Related Work Section of our paper**. Finally, regarding the meaning of \u201cdo not easily incorporate state-of-the-art designs from static GNNs\u201d, considering the following example we hope that they will clarify this aspect. Skip Connections, which are beneficial for GNN architectural design (https://proceedings.mlr.press/v139/xu21k.html ), imply an additional aggregation step for each GNN layer where initial node features are combined with the learned node embeddings. DURENDAL decomposes any heterogeneous GNN layer in two steps: a message-passing operator for each relation and a semantic-level aggregation step. Moreover, it adds a customizable embedding update module before or after any semantic-level aggregation (based on the chosen update scheme). In this way, we allow to include skip connections at every possible aggregation level of a temporal heterogeneous GNN architecture. In our framework, you can easily add a skip connection after the message-passing, and/or after the semantic aggregation, and/or after the update over time. Other temporal heterogeneous GNN models, such as https://dl.acm.org/doi/10.1145/3366423.3380027, https://arxiv.org/abs/2110.13889, or https://aclanthology.org/2020.emnlp-main.462/, treat a GNN as a feature encoder and then build a temporal encoder on top of the GNN (i.e. after the entire GNN architecture, with several GNN layers). Using this model design, you cannot easily incorporate skip connections. Indeed, you can add skip connections only after each semantic aggregation, but not after any possible aggregation function of a THGNN architecture."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233266148,
                "cdate": 1700233266148,
                "tmdate": 1700233517608,
                "mdate": 1700233517608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pMuieBxcIt",
                "forum": "aVPFuXDINt",
                "replyto": "gwyulHh0Wr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **W2** This paper's contribution is limited for ICLR standard. The authors primarily employ the ROLAND framework and conventional techniques for heterogeneous graphs. Despite its efficacy, it lacks innovation, potentially falling short of ICLR's acceptance criteria.\n\nThe reviewer recognizes that we \u201cprovide a large number of experiments to analyze the effectiveness of the model\u201d. As W2, they wrote, \u201cDespite its efficacy, [...]\u201d. Other reviewers, such as **Kepe**, writes \u201cExperimental results look promising on the considered dataset\u201d. From our viewpoint, the main objective of a new proposed solution is to show its effectiveness on the experimental field. A simple solution that works well benefits the machine learning community, especially on a subfield \u201cworth exploring\u201d (as the reviewer recognized as another strength of the work), where there are few or no baselines specifically developed for it.\n\nMoreover, in our paper, we also introduce two new datasets benchmark for temporal heterogeneous networks, which is recognized as a strength by all the other reviewers.\n\n> **W3** Recent studies on THNs warrant citation and comparison.\n\nWe thank the reviewer for suggesting these related works. \nHowever, \u201cInterpretable Research Interest Shift Detection with Temporal Heterogeneous Graphs\u201d is a work on the explainability of shift detection methods for temporal heterogeneous networks, and it is off-topic with respect to temporal heterogeneous graph forecasting, which is the topic of our work.  As written in their work, \u201cWe develop a node-type aware and interpretable detection model on temporal heterogeneous graphs. Our model first uses the heterogeneous GNN to learn representations and then constructs sampled sub-graphs as the interpretations\u201c (see Main contribution in the Introduction). Hence, they utilize heterogeneous GNN, not temporal heterogeneous one, and the temporal information is only utilized to evaluate the shift detection. There are no experiments related to dynamic link prediction as no temporal information is learned.\n\nRegarding the \u201cHeterogeneous temporal graph neural network\u201d, they employ intra and inter-relation aggregation functions between node features to capture the heterogeneous structure of the networks, which are followed by a positional encoding technique to capture temporal information. The model can be easily reconducted in our framework by following the Aggregate-Then-Update schema, where the aggregate function is their intra and inter-relational aggregation, while the update module is represented by the positional encoding technique. However, as mentioned to reviewer **fYw8**, PE cannot be used in the live-update setting. We invite the reviewer to check our official comment to reviewer fYw8. Moreover, we remind the reviewer that our work contains experiments on the effectiveness of our model design (see **Section 5**), showing that similar models to HTGNN, such as HGT, enhance their performance if repurposed in our framework.\n\n> **W4** The presentation of this paper is poor. Additionally, there are typographical errors in the article, such as writing THG as TNH.\n\nWe ask the reviewer to improve the quality of their review explaining why s/he finds the presentation of this paper poor. The reviewer reports just one typographical error in the work, which we believe is not sufficient to assign a score of one. In addition, reviewer **qqSc** finds the presentation of our work excellent, while reviewer **Kepe** says that \u201cThe paper is overall well written, easy to follow and with good references for people that might be approaching the field of dynamic graphs for the first time\u201d."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233473799,
                "cdate": 1700233473799,
                "tmdate": 1700235323990,
                "mdate": 1700235323990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QFL4wOIBXx",
                "forum": "aVPFuXDINt",
                "replyto": "pMuieBxcIt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5264/Reviewer_fQVE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5264/Reviewer_fQVE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' feedback. I agree with the contribution of introducing two new temporary heterogeneous graphs datasets, which is beneficial for data mining community.  However, the paper's proposed model is a straightforward extension of ROLAND, specifically tailored for discrete dynamic graphs rather than consistent dynamic graphs, which limits the insight offered by this paper.  Thus, I maintain that the contribution of this paper is limited for ICLR. It appears to be more appropriately suited for data mining community, given it's extension on dynamic GNN and two new datasets."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551520962,
                "cdate": 1700551520962,
                "tmdate": 1700551520962,
                "mdate": 1700551520962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EL23APiqcZ",
            "forum": "aVPFuXDINt",
            "replyto": "aVPFuXDINt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5264/Reviewer_Kepe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5264/Reviewer_Kepe"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an extension of ROLAND for discrete-time temporal heterogeneous graphs (where the temporal graph is described as a sequence of snapshots), together with two new datasets that can be used for evaluation. The main methodological novelty of the paper is how to incorporate an aggregation mechanism across various edge types through two possible different schemes: update-then-aggregate and aggregate-then-update. For what concerns the datasets, two new temporal heterogeneous graphs are introduced in the paper: TaobaoTH (a dataset of user behaviour provided from Taobao - an online shopping platform) and SteemitTH (a dataset of user interactions from Steemit - a blockchain-based social network). The proposed model is evaluated on multi-relation and mono-relational link prediction on the proposed datasets plus two other datasets that have already been used in the literature (GDELT18, ICEWS18). The approach appears to perform well on the considered datasets when compared to 9 selected baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is overall well written, easy to follow and with good references for people that might be approaching the field of dynamic graphs for the first time. While the approach appears rather straightforward (especially when compared to ROLAND), experimental results look promising on the considered dataset. The introduction of new datasets is also something that the community will most likely benefit from."
                },
                "weaknesses": {
                    "value": "As it might have emerged from my comments in the \u201cStrengths\u201d section, the approach appears to be a not particularly original improvement over ROLAND (unless my understanding is wrong, the main addition is the introduction of an aggregation mechanism across multiple relations and the use of heterogeneous GNNs for feature extrapolation). On top of this, while yes the method appears to show good results on the considered datasets compared to the baselines, I\u2019ve some doubts about the experimental evaluation. In particular, have the baselines considered in the experiment been tuned for the dataset? Taking for instance TGN from Rossi et al, the model was not evaluated on any of the datasets used in the paper. As such, if such architecture was not tuned (as instead the proposed approach was), we might be observing lower performance for such methods (as well as the other baselines), which are simply due to a suboptimal architectural choice. I\u2019d greatly appreciate if the authors could comment on this in their rebuttal"
                },
                "questions": {
                    "value": "Besides what highlighted above, I have a few questions / comments that I would the authors to address:\n\n1) Many methods appear to achieve on TaobaoTH a PR AUC that is consistent with random guessing in a balanced binary classification problem. This suggests that many models are actually not learning anything meaningful on that dataset. Can you please clarify why this might be the case? \n\n2) The fact that TGN and CAW do not compute in their implementation the MRR, I believe it\u2019s not a good reason to avoid computing such statistics for these methods. I\u2019d encourage the authors to fix the implementation in this case to provide a better comparison of all methods.\n\n3) I\u2019m confused why many MRRs appear equal to 0.5, can the authors provide some details on the implementation they used for this and how negatives were sampled?\n\n4) In section 4 it is stated that \u201cminimum number of snapshots to allow live-update evaluation\u201d is four, can you provide some details on why that is the case? From algorithm 2 in ROLAND my understanding is that 2 steps are enough for live update evaluation"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5264/Reviewer_Kepe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418015325,
            "cdate": 1698418015325,
            "tmdate": 1700560311131,
            "mdate": 1700560311131,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XSZ2DS9fza",
                "forum": "aVPFuXDINt",
                "replyto": "EL23APiqcZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment (Weaknesses)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and for finding that our work is overall well written, easy to follow, and with good references for people who might be approaching the field of dynamic graphs for the first time. Below we address specific points raised in the review in detail. We ask the reviewer to increase their score if we address all their concerns.\n\n### Weaknesses\n\nAs stated in our work, the two main methodological innovations are i) proposing a novel graph deep learning framework that allows an easy repurposing of any heterogenous GNNs to a dynamic setting; hence, the introduction of an aggregation mechanism across multiple relations and the use of heterogeneous GNNs for feature extrapolation **are not the main addition of our work** but the way we decompose any heterogeneous GNN layer to allow its extension in a dynamic setting. ii) the introduction of two different update schemes, namely Upgrade-Then-Aggregate and Aggregate-Then-Update, for obtaining temporal heterogeneous node embeddings, highlighting their strengths and weaknesses and their practical use scenarios. The straightforward way to use ROLAND on heterogeneous networks would be just adding an update mechanism after each heterogeneous GNN layer. In our work,  instead, we propose two new different update schemas that are different from the ROLAND one and are more suitable for heterogeneous graph learning as they allow to capturing relational temporal dynamics (see **Section 3**). We invite also the reviewer to see the paragraph on \u201ceffectiveness of update schemes\u201d in **Section 5**, where we compare our update schemes with the ROLAND one, showing an improvement for both schemes.\n\nOn top of this, We believe **hyperparameter-tuning all the baselines is a crucial task** for the experimental evaluation of graph deep-learning models as not tuning existing models on new datasets leads to an unfair evaluation of the new proposed ones, as unfortunately highlighted in very few works on GNNs (https://openreview.net/forum?id=HygDF6NFPB). We invite the reviewer to read the **original Appendix** of our work to find all the information about hyperparameter tuning for all the models, including baselines. For the two continuous-time models, TGN and CAW, as highlighted in our paper, we believe their continuous-time representation for temporal networks is not beneficial in application scenarios where datasets are snapshots-based. Indeed, for instance, TGN works typically with a list of timed events that happen one before another, while in our datasets there are large batches of events (i.e. appearance of new links) that happen all at the same time (i.e. in snapshots). Hence, we believe the problem for TGN is **architectural rather than related to tuning** and its core mechanism should be revised to work with snapshot-based datasets. However, we conducted a **hyperparameter search for TGN** using the following grid search: learning rate {0.1, 0.01, 0.001, 0.0001, 0.00001}, embedding_dim {50, 100, 200, 300, 400, 500}, memory_dim {50, 100, 200, 300, 400, 500}. The results show the best architecture is the one reported in the paper, which uses lr equal to 1e-5, and embedding_dim = memory_\\dim = 100.\n\n| embedding_dim / learning_rate | 0.1 | 0.01 | 0.001 | 0.0001 | 0.00001 |\n| ----------------------------- | --- | ---- | ----- | ------ | ------- |\n| 50                            | 0.5 | 0.5  | 0.880 | 0.888  | 0.885   |\n| 100                           | 0.5 | 0.5  | 0.878 | 0.886  | **0.889**   |\n| 200                           | 0.5 | 0.5  | 0.770 | **0.889**  | **0.889**   |\n| 300                           | 0.5 | 0.5  | 0.811 | 0.860  | 0.866   |\n| 400                           | 0.5 | 0.5  | 0.810 | 0.828  | 0.881   |\n| 500                           | 0.5 | 0.5  | 0.821 | 0.832  | 0.883   |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232872800,
                "cdate": 1700232872800,
                "tmdate": 1700232872800,
                "mdate": 1700232872800,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rk6b4fXykW",
                "forum": "aVPFuXDINt",
                "replyto": "pyz2HaWesA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5264/Reviewer_Kepe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5264/Reviewer_Kepe"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewers for their response. I have a couple of follow-up comments based on their rebuttal:\n\n1. Thank you for highlighting that the baselines have indeed been tuned, can you please confirm that the tuning has been done per dataset and not only on a reference one (e.g. SteemitTH) and then the same architecture re-applied across all datasets?\n\n2. For what concerns Q2, I still believe that not reporting MRR for some architecture is suboptimal (this is irrespective of what some prior work might have done), and it would have been better to have such metric listed for all approaches"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475409360,
                "cdate": 1700475409360,
                "tmdate": 1700475409360,
                "mdate": 1700475409360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xo3mjzq5Ly",
                "forum": "aVPFuXDINt",
                "replyto": "OULjJcBtPm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5264/Reviewer_Kepe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5264/Reviewer_Kepe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. In general I find this paper to be very borderline. I tend to agree with reviewer fQVE that the contribution is not particularly novel if compared to ROLAND. However, given the generally good results, in light of the rebuttal I'm willing to raise my score to a 6"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560282962,
                "cdate": 1700560282962,
                "tmdate": 1700560282962,
                "mdate": 1700560282962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4Qz2hvAeYh",
            "forum": "aVPFuXDINt",
            "replyto": "aVPFuXDINt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5264/Reviewer_qqSc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5264/Reviewer_qqSc"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a generic framework of adapting static heterogeneous GNNs to the dynamic setting through two types of schemes: Update-then-Aggregate (UTA) and Aggregate-then-Update (ATU). The authors also introduce two new datasets of dynamic heterogeneous graphs (TaobaoTH and SteemitTH) for future benchmarking. The proposed method achieves better performance in future link prediction tasks on all four datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The designed framework is generic and can be integrated with any static heterogeneous GNNs. Given its simplicity and wide adaptivity, it can facilitates future research on dynamic heterogeneous graph learning.\n2. This work introduces two new benchmark datasets of dynamic heterogeneous graphs, including one dataset from e-commerce recommendation and one dataset from blockchain-based online social network. Specifically, the TaobaoTH is of a relatively large size with ~360k nodes.\n3. The designed method achieves a better performance compared to the existing baselines including static GNNs, and dynamic GNNs."
                },
                "weaknesses": {
                    "value": "1. Based on my understanding of the differences between dynamic graphs and temporal graphs, I think it would be better if this work is positioned for dynamic heterogeneous networks instead of temporal heterogeneous networks. Dynamic networks are snapshot-based networks, i.e., aggregating edges and nodes within certain time windows, which is exactly what this paper considers. In contrast, temporal networks are more dynamically changing where each edge is associated with a timestamp (not a snapshot).\n2. It is not clear what scheme for the proposed method is applied in Table 2."
                },
                "questions": {
                    "value": "1. What are the differences or new aspects between the existing Taobao benchmark (https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Taobao.html#torch_geometric.datasets.Taobao) and the one introduced by this paper?\n\n2. The number of edges of TaobaoTH is even smaller than the number of nodes. Can you elaborate why this graph is so sparse?\n\n3. The evolutivity of TaobaoTH is extremely low. Does it mean there are very few new edges across snapshots? Or is it because at different snapshots, edges are repetitive (e.g., user viewed an item at snapshot-1 and viewed the same item at snapshot-2)? On this question, I think it's also worth reporting the repetitive metrics of the datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784206855,
            "cdate": 1698784206855,
            "tmdate": 1699636525585,
            "mdate": 1699636525585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Frpn8mI3kF",
                "forum": "aVPFuXDINt",
                "replyto": "4Qz2hvAeYh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and for finding that our work facilitates future research on dynamic heterogeneous graph learning. Below we address specific points raised in the review in detail. We ask the reviewer to increase their score if we address all their concerns.\n\n> **W1** Based on my understanding of the differences between dynamic graphs and temporal graphs, I think it would be better if this work is positioned for dynamic heterogeneous networks instead of temporal heterogeneous networks.\n\nBased on the literature (https://dl.acm.org/doi/abs/10.5555/3455716.3455786), the description provided by the reviewer refers to the distinction between discrete-time and continuous-time dynamic graphs. Temporal and dynamic, instead, are typically used as synonyms (https://dl.acm.org/doi/10.1145/3534678.3539300). There are also examples in the literature where the term dynamic is used for continuous-time graphs ( https://openreview.net/forum?id=1GVpwr2Tfdg). In this sense, the same ambiguity is also present in network science where temporal networks and dynamic networks are often interchanged. Also from the network science perspective we prefer referring to temporal networks since dynamic networks are more related to dynamic models, while the former include also snapshot-based representation (for instance https://link.springer.com/book/10.1007/978-3-030-23495-9 )\n\n> **W2** It is not clear what scheme for the proposed method is applied in Table 2.\n\nWe thank the reviewer for exposing this doubt. Table 2 reports the best DURENDAL results across both update schemes. Specifically, the best results for GDELT18, ICEWS18, TaobaoTH, and SteemitTH are obtained using UTA, ATU, ATU, and UTA, respectively. We plan to add a Table on update schemes in the Appendix.\n\n### Questions \nWe thank the reviewer for the insightful questions on the TaobaoTh dataset that helped us to clarify some aspects of the construction of this benchmark. We plan to integrate the three following answers to the content of our work. We remind to reviewer that **the code of our work is available** in an anonymized Github Repository provided in the pdf. The repository contains the code to construct TaobaoTH starting from the PyG version.\n\n> **Q1** What are the differences or new aspects between the existing Taobao benchmark (https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Taobao.html#torch_geometric.datasets.Taobao) and the one introduced by this paper?\n\nThe original benchmark dataset for Taobao was a dataset of user behaviors used in prior works on recommendation systems and provided by the Tianchi Alicloud platform https://tianchi.aliyun.com/dataset/649. Recently, as indicated by the reviewer, has been added as a temporal heterogeneous network in PyTorch Geometric, one of the most well-known and used libraries for graphML. As stated in our paper, \u201cwe construct heterogeneous graph snapshots with time granularity equal to five minutes. We consider a heterogeneous subgraph induced by 250k random sampled items for scalability issues\u201d(See Section 4). Hence, our dataset is a subgraph induced by 250k random sampled items of the PyG dataset aggregated in snapshots with a time granularity of five minutes. We obtained 288 snapshots considering the first day of user-item interactions.\n\n> **Q2** The number of edges of TaobaoTH is even smaller than the number of nodes. Can you elaborate why this graph is so sparse?\n\nThe experimental evaluation is conducted in a transductive setting. For TaobaoTH, the users considered in the dataset are the nodes active (i.e. they have at least interacted with one item) on the first five minutes of the dataset (from t=0 to t=300). Hence, the datasets consider all the relations between active users on the first session, which represent the seen nodes during training, and 250k random sampled items. This choice led to a very sparse dataset compared to the original PyG but it allowed us to study simple temporal patterns between the relation \u201cpageview\u201d, \u201cadd to cart\u201d, and \u201cbuy\u201d within e-commerce sessions in a day, as described in **Section 5**.\n\n> **Q3** The evolutivity of TaobaoTH is extremely low. [...]\n\nAs defined in the paper, \u201cevolutivity is the average number of new links in the snapshots (i.e. $\\frac{1}{|T-1|}\\sum_{t=1}^{T} |E_t|$). This definition implies that new edges are defined as edges with a future timestamp w.r.t the current one; hence, an extremely low evolutivity means there are few new edges across snapshots. In our intuition, this is reasonable as the dataset follows the entire buying process in a day of the users active in the first 5 minutes of the session. For repetitive metrics, we compute the average cardinality of the intersection between the first snapshot and the others for each relation type. Only about 7% of pageviews are repetitive, while all the interactions with the cart and the purchases are completely new."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232144352,
                "cdate": 1700232144352,
                "tmdate": 1700232144352,
                "mdate": 1700232144352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G0MQZKyDe4",
            "forum": "aVPFuXDINt",
            "replyto": "aVPFuXDINt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5264/Reviewer_fYw8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5264/Reviewer_fYw8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DURENDAL, a training framework for temporal heterogeneous networks. It introduced two training schemes, Update-Then-Aggregate and Aggregate-Then-Update, which are different aggregation methods for training. It then benchmarks the performance on four datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Benchmarking dynamic heterogeneous graphs is important.\n2. Two datasets are introduced by transforming the original open datasets.\n3. Experiments on performed."
                },
                "weaknesses": {
                    "value": "1. The mechanism of why DURENDAL outperforms baselines is unclear.\n2. The comparison of UTA and ATU is not clear. System-level (e.g. run time, memory usage) evaluation might be helpful.\n3. More commonly used datasets are needed if the paper wants to be a benchmark paper (e.g. Open Academic Graph)."
                },
                "questions": {
                    "value": "1. Where is the figure for Aggregate-Then-Update (ATU)?\n2. Why does DURENDAL have better accuracy than baselines?\n3. Why does not the paper compare with [1]?\n\n[1] Hu, et al. \"Heterogeneous graph transformer.\" Proceedings of the web conference 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5264/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5264/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5264/Reviewer_fYw8"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5264/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871153340,
            "cdate": 1698871153340,
            "tmdate": 1699636525511,
            "mdate": 1699636525511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6VfzHxtXxW",
                "forum": "aVPFuXDINt",
                "replyto": "G0MQZKyDe4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5264/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their questions. Before addressing the specific points raised in the review, we ask the reviewer what \u201cExperiments on performed\u201d (Strengths 3) means. Below we address specific points raised in the review in detail. We ask the reviewer to increase their score if we address all its concerns.\n\n> **W1** The mechanism of why DURENDAL outperforms baselines is unclear. **Q2** Why does DURENDAL have better accuracy than baselines?\n\nDURENDAL is a general framework for temporal heterogeneous networks. It introduces hierarchical heterogeneous node states as node embedding, allowing the repurposing of heterogeneous GNNs to a dynamic setting. Moreover, it introduces two new update schemas for updating heterogeneous node embedding over time, which are based on modeling relational temporal dynamics (i.e. different temporal patterns for each specific type of relation). The latter represents an innovation in temporal graph learning models, as typically the temporal encoder is placed only before or after a heterogenous feature encoder (GNN of Factorization-based), without a strictly intertwine between the heterogeneity and dynamicity aspects (https://dl.acm.org/doi/10.1145/3366423.3380027, https://arxiv.org/abs/2110.13889 ). We remind the reviewer that our two newly introduced update schemes outperform the ROLAND one (see \u201ceffectiveness of update schemes\u201d). To clearly understand what relation temporal dynamics are and potential use cases for the two update schemes, we invite the reviewer to read the Appendix of our work. \n\nBelow we explain, in our intuition, why DURENDAL outperforms each of the other baselines:\n- GAT, HAN, ComplEx: They are static models so they do not capture the temporal information, which is crucial to understanding the network evolution, and predicting future links. We invite the reviewer to read also the paragraph \u201cEffectiveness of model-design\u201d, where we repurpose HAN and other well-known heterogenous GNNs in our framework, showing an improvement in their performance.\n- TGN, CAW: They are continuous-time dynamic graph models for future link prediction. As stated in our paper, \u201c their continuous-time representation for temporal networks is not beneficial in application scenarios where datasets are snapshots-based\u201d, as in our case. It is worth noting that discrete-time dynamic graphs in live update settings represent an appealing scenario for many real-world applications (https://dl.acm.org/doi/abs/10.1145/3534678.3539300, https://arxiv.org/abs/2307.01026)\n- EvolveGCN, GCRN-GRU, TNTComplEx: the first two models are discrete-time dynamic graphs models, the latter is a strong factorization-based model baseline for temporal knowledge graphs, which are typically collected by snapshots. DURENDAL achieves better performance than these baselines because they do not model relational temporal dynamics. Hence, they do not model in different ways the evolution of each relation; instead, they use a single strategy to model the overall graph dynamics.\n- HetEvolveGCN: it is a baseline we developed to show the effectiveness of our solution. It is based on EvolveGCN and uses an EvolveGCN model for each relation, combining the generated node embeddings through a semantic aggregation function. Hence, it is a snapshot-based architecture that also models relational temporal dynamics. DURENDAL achieves better performance because it captures relational temporal dynamics in an effective way. Indeed, EvolveGCN uses a recurrent neural architecture to update only the learnable parameters of the model, while DURENDAL is based on embedding evolution, updating directly the generated node embeddings (https://arxiv.org/abs/2302.01018)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5264/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231183091,
                "cdate": 1700231183091,
                "tmdate": 1700234362748,
                "mdate": 1700234362748,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]