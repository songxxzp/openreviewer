[
    {
        "title": "Transport meets Variational Inference: Controlled Monte Carlo Diffusions"
    },
    {
        "review": {
            "id": "xb89MEUPaf",
            "forum": "PP1rudnxiW",
            "replyto": "PP1rudnxiW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
            ],
            "content": {
                "summary": {
                    "value": "This paper present a framework to perform variational inference with a score-based algorithm. Specifically, it introduces HJB-regulariser to the diffusion normalizing flow process and provide some justification for the framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This connection between score-based algorithm and variational inference is an interesting topic. It remains open to make these score-based model really works in VI literature."
                },
                "weaknesses": {
                    "value": "The idea is relatively direct and the whole motivation looks incremental.\n\nThe empirical results are not strong enough from both VI/transport side.\n\nThe ablation study of HJB-regulariser is weak."
                },
                "questions": {
                    "value": "In Variational inference literature, the hidden dimension $z$ is a low dimensional manifold that can be interpretable factors? The latent of VAE uses smaller latent dimension than the data. However, in diffusion model/transport-based algorithms. The space of $x$ and $z$ are in the same dimension, which makes $z$ encode all the information of $x$ (and loses the ability to perform inference). How can you justify this compensation? What are the benefits of the (almost) lossless VI?\nMoreover, it is not clear that how good is this \u201cVI\u201d compared to other VAEs, such as image generation, latent variable inference. The current setting is relatively toy.\n\n\nFrom the transport side, although it is understandable that Figure 2 is more well behaved than non-regularized transport, we still found that Figure 3 is good enough in real practice. Can you find an example that the non-regularized way fail?\n\nIn general, I do believe that the HJB-regulariser is useful in some cases, but given the current justifications and evidence, the significance is not clear at this moment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727654803,
            "cdate": 1698727654803,
            "tmdate": 1700547137436,
            "mdate": 1700547137436,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Uy7duUHLxB",
                "forum": "PP1rudnxiW",
                "replyto": "xb89MEUPaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review refers to experiments from prior workshop paper, without acknowledging sampling experiments in the main section of this paper."
                    },
                    "comment": {
                        "value": "Dear Reviewer ndxa,\n\nWe thank you for the time and effort taken to review our manuscript and for the many careful and insightful comments presented.\n\nWe believe that there might have been a small confusion and you have accidentally reviewed the wrong manuscript. It might be possible that you wrote your review based on a much earlier workshop version of this paper that was publicly available online before the start of the review period. Alternatively it may have been the case that you did not see the main contributions of our paper being a new sampling algorithm, which our extensive experimental section focuses on, instead providing comments on some additional toy 2D experiments that are only in the appendix.\n\nWe believe this may be the case since the focus of your feedback is oriented to experiments that are not present in the main manuscript that we have submitted on openreview. \n\nFurthermore, we sincerely request the reviewer to revisit the experimental section of our openreview submission, where we have high dimensional non-toy experiments on sampling from densities, which we believe is the main focus of this work, and quite different from the workshop submission. \n\nTo explain why we believe this is the case we would like to point to the following comments and feedback the reviewer has provided:\n\n1. \u201cThe ablation study of HJB-regulariser is weak.\u201d\n2. \u201cthat Figure 2 is more well behaved than non-regularized transport, we still found that Figure 3 is good enough in real practice.\u201d\n\nIf you revisit the experiments in the main pdf, you will notice we have experiments on 8 different datasets (target densities) ranging in dimensions from 2 to 1600, and averaging a dimension of ~ 231 which is quite high-dimensional for our focus of sampling from unnormalised densities. We sincerely request that the reviewer draw conclusions based on these 8 experiments in the main text where we compare them to 7 different competing methods.\n\nThe reviewer's feedback refers only to the toy 2D experiments that are at the end of the appendix, which we are happy to address but we would reiterate that our main experimental section on sampling experiments is the focus of the paper, not the final appendix section, which we don't mention in the main text and is mainly included as an interesting aside. \n\nWe would further like to remark that the HJB-regulariser is not our main proposed method / algorithm and as the title of the paper motivates, the controlled Monte Carlo diffusion sampler CMCD (which we have ablated thoroughly in our experiments) is the main contribution of this work. We apologise if this was a point of confusion. The HJB regulariser is mostly a conceptual/theoretical remark that is in line with our overarching framework whilst our experimental focus is on our proposed method for sampling (CMCD). Therefore, the minor empirical ablations on the HJB regularizer are left to the appendix and are by no means the central contribution of our paper. \n\nYou will see in our main manuscript submitted here you can find quite a comprehensive array of experiments for sampling comparing our CMCD method to prior work (note these are not toy experiments):\n\n1. Ionosphere Logistic Regression Target.       Dimension = 35\n2. Log Sonar Logistic Regression Target.        Dimension = 61\n3. Brownian Motion Posterior Target.               Dimension = 32\n4. Lorenz Model Posterior Target.                    Dimension = 90\n5. Seeds (Random effect regression) Target.   Dimension = 26\n6. Log Gaussian Cox Process Target.              Dimension = 1600\n7. Neals Funnel Target.                                     Dimension = 10\n8. Toy Difficult GMM [1]                                     Dimension = 2\n\nNote that across all these targets we significantly outperform an array of sampling methods that we compare to:\n\n1. ULA [2]\n2. UHA [3]\n3. MCD-UHA [4,5]\n4. MCD-UHA [4,5]\n5. SMC          [6]\n6. PIS            [7]\n7. DDS          [8]\n\n\nWe would also like to highlight that these experiments are standard baselines for sampling used consistently across many prior works focused on sampling and partition function estimation [1,2,3,4,5,6,7,8]. Note that for sampling, unlike generative modelling, dimensions >= 50 are already considered to be very high dimensional, as many of the provided references suggest.  \n\nFinally, to further the extensive nature of these experiments we would like to cite reviewer LiYc\u2019s remark:\n\n\u201cExtensive experiments show that the CMCD method outperforms existing methods in relatively high dimensional examples.\u201d\n\nWe hope the reviewer can agree that these experiments are significantly different and much more extensive than the toy 2D experiments from the appendix. We sincerely request the reviewer to reconsider the comprehensive sampling experiments we ran to support our new sampling algorithm as the core contribution, and we would be grateful if they could reevaluate their score accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699661451623,
                "cdate": 1699661451623,
                "tmdate": 1699664633582,
                "mdate": 1699664633582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3XZZIPmgWn",
                "forum": "PP1rudnxiW",
                "replyto": "xb89MEUPaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part II - Refferences"
                    },
                    "comment": {
                        "value": "[1] Arbel, M., Matthews, A. and Doucet, A., 2021, July. Annealed flow transport monte carlo. In International Conference on Machine Learning (pp. 318-330). PMLR.\n\n[2] Girolami, M. and Calderhead, B., 2011. Riemann manifold langevin and hamiltonian monte carlo methods. Journal of the Royal Statistical Society Series B: Statistical Methodology, 73(2), pp.123-214.\n\n[3] Geffner, T. and Domke, J., 2021. MCMC variational inference via uncorrected Hamiltonian annealing. Advances in Neural Information Processing Systems, 34, pp.639-651.\n\n[4] Geffner, T. and Domke, J., 2023, April. Langevin Diffusion Variational Inference. In International Conference on Artificial Intelligence and Statistics (pp. 576-593). PMLR.\n\n[5] Doucet, Arnaud, Will Grathwohl, Alexander G. Matthews, and Heiko Strathmann. \"Score-based diffusion meets annealed importance sampling.\" Advances in Neural Information Processing Systems 35 (2022): 21482-21494.\n\n[6] Del Moral, P., Doucet, A. and Jasra, A., 2006. Sequential monte carlo samplers. Journal of the Royal Statistical Society Series B: Statistical Methodology, 68(3), pp.411-436.\n\n[7] Zhang, Q. and Chen, Y., 2021. Path integral sampler: a stochastic control approach for sampling. arXiv preprint arXiv:2111.15141.\n\n[8] Vargas, F., Grathwohl, W. and Doucet, A., 2023. Denoising diffusion samplers. arXiv preprint arXiv:2302.13834."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699661506841,
                "cdate": 1699661506841,
                "tmdate": 1699661526730,
                "mdate": 1699661526730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3iNMMLRcPk",
                "forum": "PP1rudnxiW",
                "replyto": "3XZZIPmgWn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your quick response. \n\nWhat I am interested in is the comparison with DNF and ablation study about different parameters and settings. Although your main experiments show some advantages against baselines, it is not clear that how does your proposed algorithm help the performance. \n\nI do believe that there needs some ablation study to analyse different parts of your algorithms, rather than just running it and get the SOTA conclusion."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699723514119,
                "cdate": 1699723514119,
                "tmdate": 1699723514119,
                "mdate": 1699723514119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aY7u5fviSC",
                "forum": "PP1rudnxiW",
                "replyto": "xb89MEUPaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our paper is not about DNF our empirical contribution is a new sampler CMCD (unrelated to the HJB reguralizers)"
                    },
                    "comment": {
                        "value": "Dear reviewer ndxa,\n\nThank you for the prompt response. It is possible you are looking at a different paper, as DNF is not applicable to our task\n\nOur paper is focused on sampling from unnormalized densities. DNF is a method for generative modeling when you have access to data, this method is not applicable or relevant to the sampling task that we consider (where we do not have access to data or samples). \n\nWe cannot compare/do ablations to DNF as DNF is focused on a different task tangential to our paper. Could you please check the experiment section here https://openreview.net/pdf?id=PP1rudnxiW (this is the pdf for this paper on openreview). \n\nOur proposed algorithm CMCD has nothing to do with the HJB regularisation remark, and this is what we explore empirically. We do a careful ablation of its performance across different timesteps, furthermore, in Appendix F.3 you can find further ablations."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699725890546,
                "cdate": 1699725890546,
                "tmdate": 1699727281611,
                "mdate": 1699727281611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i1hFxpSaGw",
                "forum": "PP1rudnxiW",
                "replyto": "xb89MEUPaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Variational Inference extends beyond learning lower dimensional representations, it is also used in the very well-studied application of sampling from densities (e.g. MCMC styled tasks) + Revised Version clarifying task and contributions"
                    },
                    "comment": {
                        "value": "Dear Reviewer ndxa,\n\nWe would like to thank you for the time you spent reading our work and the feedback you have provided. In what follows we aim to **address your main question** and detail how our **revised version** clarifies this.\n\nWe have uploaded a revised version of the manuscript, which introduces **the sampling problem**, that is the task of sampling from an unnormalised density and estimating its normalising constant. This is a seminal task in both statistics and machine learning [1-8] that predates generative modelling. \n\nWe further reiterate in the revised version that our task is focused on **sampling**, not generative modelling. The goal here is as follows; given we have access to a probability density function of the form:\n\n$$p(x) = \\frac{e^{-U(x)} }{\\int  e^{-U(x)} dx}$$\n\nWhere $U(x)$ is a function that we can evaluate and differentiate pointwise, however we are unable to evaluate the normalising constant $\\int  e^{-U(x)} dx$. \n\nGiven such a distribution $p(x)$, the goal is to learn or design a model that can generate samples that are distributed according to $p(x)$, notice **unlike generative modelling we do not have access to any data or samples**. One way to do so is via minimising KL w.r.t to a parametric distribution $q_{\\theta}(x)$:\n\n$$\\arg_{\\theta}\\min D_{KL} (q_{\\theta}(x) || p(x))$$\n\nNote that the above KL can only be computed up to a constant in practice ($\\int  e^{-U(x)} dx$), and thus an ELBO is what we typically minimise. This family of approaches is typically also **referred to as Variational Inference**, and is a superset of techniques involving approximating intractable distributions [13], VAEs are simply a particular, unrelated sub-instance of VI and are not particularly applicable to our task. Therefore, **there are no low-dimensional latents in this formulation**. \n\nTypically one wants 2 things from $q_\\theta$: \n\n1. It is easy to sample from numerically (not intractable)\n2. It is a very flexible distribution so that $D_{KL} (q_{\\theta}(x) || p(x))$ can be brought very close to $0$.\n\nA common way this is achieved is through [9,14] applying a series of tractable transformations to a simple Gaussian, e.g. : \n\n\\begin{align}\nz_0 \\sim \\mathcal{N}(0,I) \\\\\n\\end{align}\n\\begin{align}\nz_1  \\sim  \\mathcal{N}(f_{\\theta_1}(z_0), \\sigma_1^2I) \\\\ \n\\end{align}\n\\begin{align}\n\\vdots \\\\\n\\end{align}\n\\begin{align}\nx = z_K  \\sim  \\mathcal{N}(f_{\\theta_{K-1}}(z_{K-1}), \\sigma_{K-1}^2I) ,\n\\end{align}\nthe resulting $q_\\theta(x)$ is then a very flexible distribution that is easy to sample from.\n\nNotice there is no need for the latents to be lower-dimensional as our goal is to minimise $D_{KL} (q_{\\theta}(x) || p(x))$ so that we can sample $p(x)$. We do not care about $z_0$ being an interpretable quantity, we just care about it being a tractable quantity. This is a very common use case of VI [9,10,11,12]. \n\nAlso, notice that these flavours of deep latent models with equal latent dimensions have been studied quite extensively already in the context of both neural ODEs and SDEs [14,15,16].\n\nOur main contribution is a novel algorithm (CMCD) that allows us to parametrise $q_{\\theta}(x)$ with a tractable and learnable SDE that starts at a Gaussian $z \\sim \\mathcal{N}(0,I)$ and then approximately (with guarantees that we prove) transports the Gaussian to the target distribution $p(x)$.\n\nWe hope this answers your question on VI and equal dimensional latents:\n\n> \u201cIn Variational inference literature, the hidden dimension is a low dimensional manifold that can be interpretable factors? The latent of VAE uses smaller latent dimension than the data... \u201d\n\nWe look forward to any further doubts you might have and are happy to provide any further information. In particular, we hope now that the task and focus of this paper are clear and you might be able to appreciate our sampling experiments and ablations better, as they are by no means toy examples, and are a comprehensive ablation on standard benchmarks, targets and baselines across sampling literature.\n\n\n[9] Rezende, D. and Mohamed, S., 2015, June. Variational inference with normalizing flows. ICML\n\n[10] Doucet, Arnaud, Will Grathwohl, Alexander G. Matthews, and Heiko Strathmann. \"Score-based diffusion meets annealed importance sampling.\" Nuerips\n\n[11] Geffner, T. and Domke, J., 2023, April. Langevin Diffusion Variational Inference. In AISTATS\n\n[12] Wu, H., K\u00f6hler, J. and No\u00e9, F., 2020. Stochastic normalizing flows. Neurips, 33\n\n[13] Blei, D.M., Kucukelbir, A. and McAuliffe, J.D., 2017. Variational inference: A review for statisticians. Journal of the American Statistical Association,\n\n[14] Tzen, B. and Raginsky, M., 2019. Neural stochastic differential equations: Deep latent Gaussian models in the diffusion limit. \n\n[15] Chen, R.T., Rubanova, Y., Bettencourt, J. and Duvenaud, D.K., 2018. Neural ordinary differential equations. Neurips\n\n[16] Xu, W., Chen, R.T.,  et al. 2022, May. Infinitely deep Bayesian neural networks with stochastic differential equations."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699918692177,
                "cdate": 1699918692177,
                "tmdate": 1699959192754,
                "mdate": 1699959192754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jbpt4noHW3",
                "forum": "PP1rudnxiW",
                "replyto": "WUADApQdoz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback"
                    },
                    "comment": {
                        "value": "First, we have to highligh that the flow-based algorithm can indeed perform the sampling task as you mentioned [1]. It is important to compare the baseline. Besise, some concurrent work also add similar baselines (VI-NF) [2]. Due to the similar formulation of DNF, it is also important to consider the direct baseline VI-DNF and other VI-NFs. Moreover, some other baselines such as Underdamped Langevin, [3] is missing. Also, it is not fair to compare $K$ with ULA, as you introduce additional parameterization in the model. Even with other methods, the complexity may not directly compared. A wall-clock time and computation overhead comparison would be helpful.\n\nFinally, as Reviewer yH6G mentioned, the presentation of the paper needs to be revised extensively. Too many unnecessary propositions mislead the readers and have negative impacts for the work.\n\n\n[1] variational inference with normalizing flows\n[2] Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization\n[3] Continual repeated annealed flow transport monte carlo"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977209508,
                "cdate": 1699977209508,
                "tmdate": 1699977209508,
                "mdate": 1699977209508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zJCmoBFFlY",
                "forum": "PP1rudnxiW",
                "replyto": "xb89MEUPaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added requested comparisons to NF, DNF-VI , AFT + Significantly Improved Presentation Part I"
                    },
                    "comment": {
                        "value": "Dear Reviewer ndxa,\n\nThank you so much for engaging with us and for the very helpful comments that are significantly improving our manuscript.\n\nUpon your recommendation, we have added the following additional experiments and ablations (see Appendixes F.2. - F.5. in the revised manuscript https://openreview.net/pdf?id=PP1rudnxiW):\n\n1. A VI Normalising flow baseline (VI-NF) on the GMM, Funnel and LGCP targets.\n2. Comparisons to VI-DNF across the GMM, Funnel and LGCP targets. As expected (from our theory but also comparisons to DDS and PIS) **we significantly outperform VI-DNF.** \n3. Comparisons to AFT, across all datasets with tractable distributions, where outperform on **6/8 targets and remain competitive on 2**.\n4. Comparisons to **VI-NF** across all datasets , where we outperform on 5/8 datasets and remain competitive on 1. The 2 target densities **VI-NF** performs better are on the more lower-moderate dimensional end and are unimodal.\n6. We compare to **Underdampened Langevin Annealing** comparisons in the main section of the paper **(UHA)** is the acronym we use for it.\n7. Wallclock times comparing different methods. Note all methods are trained for the same amount of iterations and parametrised (if needed) by neural networks of the same size across methods.\n\nThese ablations can now be found in Appendix F.4. - F.6 colored in teal, **Tables 5,6 and z** (apologies if color is unhelpful). It is **important** to note that for the VI-NF we match $K$ to the number of IAF flows which quickly results in **VI-NF** having much more parameters than our modest drift network used in CMCD.\n\nNow we would like to address your questions:\n\n> Moreover, some other baselines such as Underdamped Langevin, is missing\n\n**We already compare to Unaderdampened Langevin Dynamics**. Underdampened Langevin Dynamics is in fact Hamiltonian dynamics + Brownian noise, and we use the acronym UHA (Unadapted Hamiltonian Annealing) to refer to it. If you look carefully at our plots in Figure 1 a) and b), you will notice there is a column for Underdampened (UD) methods and a column for Overdamped methods (OD). UHA corresponds to Underdampened Langevin dynamics. We further explain the OD and UD acronyms in the experiment section.\n\n> Also, it is not fair to compare $K$  with ULA, as you introduce additional parameterization in the model\n\nI think there is a tiny misconception here. Our algorithm provides us with an optimisation objective that basically allows us to train / improve on ULA in way that variance is reduced and we can obtain better estimators for ln Z . **Notice when $\\phi=0$, our approach is in fact exactly the same as ULA**. Our propositions and lemma provide us with an objective to improve on top of ULA. As the reviewer pointed out, the $\\phi$ parametrisation will increase the compute time, but correspondingly, there is **significant improvement in performance over ULA** that cannot be achieved otherwise. We trade off some extra compute for much lower variance and better performance.\n\nRather than an unfair comparison, the ULA line serves as a baseline to see how much our approach improves from its starting point. However, for transparency, we have added wall-clock times for ln Z estimation, to showcase the additional compute that is required for this significant improvement in performance.\n\nWe have ensured the other methods that are neural SDEs have the same number of parameters, and their computational complexity is the same as CMCD for sampling, However, we have also added wallclock times for some these (e.g. MCD).\n\n> Finally, as Reviewer yH6G mentioned, the presentation of the paper needs to be revised extensively \n\nWe wholeheartedly agree with the reviewer that the initial presentation needed extensive revision, and we have significantly changed our presentation to address reviewer yH6G\u2019s remarks. This can be seen in the revised pdf (https://openreview.net/pdf?id=PP1rudnxiW) ). Additionally, we wrote a 2nd alternative draft for the the introduction with a different presentation flow so that reviewers had an extra option to choose from (https://anonymous.4open.science/r/CMCD-6BF6/rebuttal_version.pdf ), please note this version does not contain the updated experiments the only bit to read here is the different introduction (presented differently).\n\nWe would like to remark **that reviewer yH6G was very positive about our new presentation**, and they **increased their scores** from a 3 to an 8, stating that we addressed their remarks:\n\n> I sincerely thank the authors for their comments and their extensive modifications. I think the presentation is much clearer now and I also now finally understand the greater context  \u2026 I believe with this modification the paper has reached finally a place where it is broadly significant and I have modified my original rating to reflect this..\n\nWe would love to hear your feedback on our revised ablations and presentations, and we hope that this has addressed some of your concerns and questions."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098117696,
                "cdate": 1700098117696,
                "tmdate": 1700234609569,
                "mdate": 1700234609569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8Gt2ZVnUOO",
                "forum": "PP1rudnxiW",
                "replyto": "xb89MEUPaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added requested comparisons to NF, DNF-VI , AFT + Significantly Improved Presentation - Part III (Wallclock + References)"
                    },
                    "comment": {
                        "value": "# Wallclock Times\n\nHere we provide comparisons for wallclock times for sampling and lnZ estimation (the number of samples kept the same across methods, more precise details can be found in our appendix). Notice how the overhead from our proposed approach whilst higher is still not a large computational overhead. \n\n\n| Method | Average Time (s) | Min Time (s) | Max Time (s) |\n| :--- | :--- | :--- | :--- |\n| CMCD (OD) | 9.665 | 5.592 | 21.475 |\n| ULA | 9.204 | 4.673 | 20.721 |\n| UHA | 9.427 | 5.588 | 20.263 |\n| MCD | 9.204 | 4.673 | 20.721 |\n\n# Method Descriptions + References\n\n* **AFT**: Anealed Flow Transport Montecarlo [1],  **a more modern and scalable flow method than NFVI**\n* **VI-NF**: Stacked Inverse autoregressive flows (IAF) for variational inference (uses the codebase of [1]) . **We match $K$ to the number of stacked IAF layers**.\n* **DNF-VI**: Diffusion Normalising Flow Baseline adapted to VI **suggested by the reviewer.**\n\n**Note:** AFT was unable to train successfully (became unstable and did not converge) on the Lorenz and Brownian targets. We sweeped over learning rates and hyperparameters to try and stabilise this but were not able to produce reasonable results. \n\n[1] Arbel, M., Matthews, A. and Doucet, A., 2021, July. Annealed Flow Transport Monte Carlo. In International Conference on Machine Learning (pp. 318-330). PMLR."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098587978,
                "cdate": 1700098587978,
                "tmdate": 1700136748236,
                "mdate": 1700136748236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NMbgKTpgvc",
                "forum": "PP1rudnxiW",
                "replyto": "xb89MEUPaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Feedback on revised presentation + new baselines"
                    },
                    "comment": {
                        "value": "Dear reviewer ndxa,\n\nWith the revision of our manuscript for which reviewer **yH6G** stated **\u201chas reached finally a place where it is broadly significant\u201c** and **\u201cthe presentation is much clearer\u201d**, we hope to have addressed any concerns you may have had regarding the presentation of our manuscript.\n\n Furthermore, as per your suggestion, we have implemented new baseline algorithms **(NF-VI, DNF-VI, AFT)** and compared our algorithm against them on a suite of target distributions, showing the strong performance of our algorithm. \n\nFinally, we also provide wallclock times of evaluation of our baselines and computational complexity comparisons. We look forward to engaging with you further during the rebuttal period to address any concerns you might have remaining and hope to hear your feedback and review of our revised manuscript."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508621412,
                "cdate": 1700508621412,
                "tmdate": 1700511359757,
                "mdate": 1700511359757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yyv2t3eM6L",
                "forum": "PP1rudnxiW",
                "replyto": "NMbgKTpgvc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback. I have reevaluate the rating for the current version. The added VI-DNF/VI-NF experiments look good. However, it looks that some comparisons are missing in the table, so I would suggest to include them for completeness. Moreover, I still think the clarity can be further improved. For example, some important part such as minimising Eq 24 in Algorithm might be further elaborated, which I do think is an important part for this paper."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547655681,
                "cdate": 1700547655681,
                "tmdate": 1700547655681,
                "mdate": 1700547655681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q8UHyQlsnM",
            "forum": "PP1rudnxiW",
            "replyto": "PP1rudnxiW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_nqRv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_nqRv"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel framework for generative modelling and Bayesian computation that bridges variational inference and optimal transport using the theory of diffusion models. Starting from the Schr\u00f6dinger problem and iterative proportional fitting, the authors draw a connection to the EM-algorithm, and finally present a novel regularized diffusion objective with a unique minimizer. They validate their method on several experimental benchmarks where it outperforms competing state-of-the-art methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper is as far as I (the reviewer) can judge highly original and will be greatly valued by the community. \n- The paper is well written and generally clear. The contextualization of their work with respect to other frameworks is well done. It unifies and generalizes several previously introduced frameworks, such as Monte Carlo diffusions or unadjusted Langevin annealing. The motivation and lead-up to their final results is in my opinion very convincing.\n- The proposed method outperforms other state-of-the-art-methods on a variety of experimental evaluations. The experimental section is sufficiently exhaustive wrt compared data sets and competing methods."
                },
                "weaknesses": {
                    "value": "I don't have major comments regarding weaknesses other than that the source code for the experimental validation needs to be provided in the supplement and not only via some possibly non-permanent web link."
                },
                "questions": {
                    "value": "- In my opinion the paper could benefit from a shorter introduction and motivation and instead an extended experimental section with method limitations, experimental details, computational trade-offs, etc. for more applied audiences. Since this is likely not possible due to page limits, an extended treatment in the appendix would be very welcome.\n- What are the training times of the method, for instance, in comparison to running SMC?\n- How were the neural networks architectures chosen and how is the hyperparameter selection and other details impacting the experimental results (e.g., see Table F1)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Reviewer_nqRv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749812411,
            "cdate": 1698749812411,
            "tmdate": 1699636114714,
            "mdate": 1699636114714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KfjvTDoXB9",
                "forum": "PP1rudnxiW",
                "replyto": "q8UHyQlsnM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review !  We have expanded the experimental details and included further computation time ablations."
                    },
                    "comment": {
                        "value": "Dear Reviewer nqRv, \n\nWe thank you so much for the very positive comments on our work, we are thrilled to hear such positive and constructive feedback. We thoroughly agree that more work can be done towards improving the details in the experimental section, and thus we have uploaded some changes to the experimental section within the appendix as requested (we have highlighted your requested changes in orange). \n\nIn what follows we will go over each of the points raised:\n\n> In my opinion the paper could benefit from a shorter introduction and motivation and instead an extended experimental section with method limitations, experimental details, \n\nWe have moved the HJB remarks which were not central to our story to the appendix. As a result we expanded the practical motivation of the work further and added Algorithm 1 detailing CMCD which should heavily benefit more applied audiences.\n\n> computational trade-offs, etc. for more applied audiences. Since this is likely not possible due to page limits, an extended treatment in the appendix would be very welcome.\n\nWe have added Table 8 which details the sampling and loss complexity of the main methods that we compare to. In particular, we have also added wallclock times for inference (sampling + ln Z estimation) comparing several of the ULA + MCD-based methods and ours. We can observe that once trained, sampling time is comparable across approaches.\n\n>  What are the training times of the method, for instance, in comparison to running SMC?\n\nIt is not entirely straightforward to compare SMC to our approach taking training into account, as given our fixed training iterations we dramatically outperform SMC across challenging high dimensional targets. \n\nTo carry out an insightful comparison, we chose the LGCP target, which is our most numerically intense target, and we phrase the following question:\n\n\u201cFor how long do we have to train CMCD to outperform the best-run of SMC\u201d\n\nFor this, we look at Figure 1 and see that at $K=8$, CMCD already outperforms SMC at $K=256$ with 2000 particles. So we choose these two approaches to compare to each other.  Below is a brief comparison of training-time calculation, where we have included the tuning time for SMC, which is akin to our training time (as without tuning SMC\u2019s hyperparameters, ELBOs and ln Z estimations were much worse). \n\n| Method | Train + Sample Time (min) | ln Z                | ELBO                  |\n|--------|---------------------------|---------------------|-----------------------|\n| CMCD   | $33.12 \\pm 0.12$          | $491.059 \\pm 3.553$  | $469.475 \\pm  0.2589$ |\n| SMC    | $62.62 \\pm 0.1$           | $477.162 \\pm 4.998$ | $453.395 \\pm 4.43$ |\n\n\n> How were the neural networks architectures chosen and how is the hyperparameter selection and other details impacting the experimental results (e.g., see Table F1).\n\nWe did not tune the architectures, we inherited them from the LDVI [1] paper, they are super minimal feedforward architectures, where the only interesting design choice is on the embedding layer used to encode the timesteps.  We have added a diagram for the architecture as well as some additional details like activations (see Figure 2 in the revised version). \n\n>  weaknesses other than that the source code for the experimental validation needs to be provided in the supplement\n\nWe have uploaded a zip file to the supplement with our source code as requested. We will also open-source the code on Github at the camera-ready stage (if the paper is accepted), where it will be available publicly.\n\nWe hope that with the added figures, discussions, and further ablations we have addressed the reviewer\u2019s remarks and look forward to further feedback!!\n\n[1] Geffner, T. and Domke, J., 2023, April. Langevin Diffusion Variational Inference. In International Conference on Artificial Intelligence and Statistics (pp. 576-593). PMLR."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235134054,
                "cdate": 1700235134054,
                "tmdate": 1700245738201,
                "mdate": 1700245738201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R5juGHb7XX",
                "forum": "PP1rudnxiW",
                "replyto": "KfjvTDoXB9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_nqRv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_nqRv"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for clarifications and answering my questions. I will keep my current rating."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643131173,
                "cdate": 1700643131173,
                "tmdate": 1700643131173,
                "mdate": 1700643131173,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PrsY5UsWIr",
            "forum": "PP1rudnxiW",
            "replyto": "PP1rudnxiW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_2i1s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_2i1s"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to revisit sampling through the lens of generative modeling. In particular, they show that the EM algorithm can be formulated as an Iterative Proportional Fitting when there is no restriction on the kernels underlying the generative models. Based on their presentation and observations, they propose a score-based annealing (Controlled Monte Carlo Diffusion) that betters the state of the art on two datasets: funnel and GMM"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I find the paper very clear and the presentation very interesting. I believe the main novelty to be in the score-based annealing CMCD. I am unsure how novel the restricted correspondence between EM and IPF is; however, presenting such a relation is very interesting."
                },
                "weaknesses": {
                    "value": "I am unsure how novel the restricted correspondence between EM and IPF is; in particular, it seems to me that the correspondence only holds when there is no restriction on $p^\\theta(x\\vert z)$; in the general setting, EM does not allow for the forward generative model $\\pi^{2n+1}(x,z)$ to have as a marginal $\\pi_x(x) = \\mu(x)$ and the backward to have as a marginal $\\pi_z(z) = \\nu(z)$. If I am not mistaken, I think it would be very helpful to have some comments on this point in the paper."
                },
                "questions": {
                    "value": "I would be very happy to have more information on the relation between the correspondence between EM and IPF as discussed in limitations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Reviewer_2i1s"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698971168165,
            "cdate": 1698971168165,
            "tmdate": 1699636114650,
            "mdate": 1699636114650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xWnbBmQWOX",
                "forum": "PP1rudnxiW",
                "replyto": "PrsY5UsWIr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review, some quick follow up questions, to help us update the manuscript"
                    },
                    "comment": {
                        "value": "Dear Reiveiwer 2i1s,\n\nWe would like to thank you for the very helpful comments and for taking the time to read our work with such care. \n\nBefore we upload an updated version addressing your feedback, we would like to clarify some of your questions if possible in order to understand how we can improve the EM contributions and their clarity:\n\n1. \u201cI am unsure how novel the restricted correspondence between IPF and EM is\u201d\n\nTo the best of our knowledge, our paper is the first to make this connection. In case this is not true, we would appreciate it if you could point us to any references where this connection is already proved. Is there any other aspect of our contribution where you might be uncertain about the novelty, which we would be happy to clarify?\n\n2. \u201cEM does not allow for the forward generative model  $\\pi^{2 n+1}(x, z)$ to have as a marginal $ \\pi_x(x)=\\mu(x)$ and the backward to have as a marginal $\\pi_z(z)=\\nu(z)$ \u2026 .\u201d\n\nWhen you say the general setting, do you mean the case where $p^\\theta(x|z)$ and $p^\\phi(z|x)$ are restricted? You are completely right that the connection between EM and IPF only holds when these transitions are made to be flexible enough. We mention this briefly in the introduction, but will indeed emphasize this more explicitly. Indeed if the transition densities are restricted it is not possible to match the marginals via EM, however in our paper we focus on the unrestricted setting and thus our construction of EM\u21d4 IPF  is valid. \n\n3.  I think it would be very helpful to have some comments on this point in the paper.\n\nWe aim to address this in the main text by adding the following remark:\n\n\u201cNote that the correspondance between the EM and IPF iterates can only be made true if the transition densities are unrestricted. This can be achieved by parametrising them as transition densities of SDEs and thus can be carried out approximately via discretising these SDEs in practice. For the setting where one may restrict them to Gaussians as done with VAEs it becomes clear that the EM iterates cannot be minimised fully to enforce the constraint required by the IPF iterates.\u201d\n\nWe hope that this remark addresses your concerns, and would be happy to add any additional details that might clarify this further. We would like to highlight that this is mostly a theoretical connection that unifies classical algorithms from VI and OT. We intended this as a bridging element that can allow readers from VI to familiarise themselves with OT through something they are already familiar with and vice-versa.\n\n4.   they propose a score-based annealing (Controlled Monte Carlo Diffusion) that betters the state of the art on two datasets: funnel and GMM\n\nWe thank the reviewer for pointing out the state-of-the-art results that our algorithm achieves, however, we would like to clarify that our method does indeed obtain state-of-the-art performance on 7 of the 8 datasets we consider, and is highly competitive on the remaining dataset (Lorenz, d=90). All of these 8 datasets are highly popular in the literature as standard benchmarks. \n\nFinally in order to further enhance the EM contributions, we have recently been able to show that our EM\u21d4 IPF correspondence extends beyond KL divergences and to f-divergences (via applying results from Proposition 6 in [1]). We will add a further proposition and proof to the appendix extending our EM\u21d4 IPF result to f-divergences, and we hope that this further novel connection strengthens the novelty of this connection for the reviewer and the community.\n\nOnce again, we thank the reviewer for their insightful comments, and would be very grateful if the reviewer could clarify whether these additions address their concerns regarding the EM contribution, and any further clarifications we could make in case we don\u2019t address their concerns.\n\n[1] Baudoin, F., 2002. Conditioned stochastic differential equations: theory, examples and application to finance. Stochastic Processes and their Applications, 100(1-2), pp.109-145."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699664865109,
                "cdate": 1699664865109,
                "tmdate": 1699664865109,
                "mdate": 1699664865109,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cAckr1lfu1",
                "forum": "PP1rudnxiW",
                "replyto": "xWnbBmQWOX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_2i1s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_2i1s"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for your reply"
                    },
                    "comment": {
                        "value": "The authors' response addresses my question and provides additional information."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699696388618,
                "cdate": 1699696388618,
                "tmdate": 1699696388618,
                "mdate": 1699696388618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pnlNwqKCZz",
                "forum": "PP1rudnxiW",
                "replyto": "PrsY5UsWIr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 2i1s,\n\nWe thank you for your comment and are glad to see that our response addresses your question. With this additional information and clarification that we have provided, as well as our experimental results establishing **SOTA across 7 datasets**, we humbly ask if the reviewer could consider re-evaluating their score if their major questions have been addressed. Alternatively, we are very happy to address any additional suggestions that could improve our contribution and make it a stronger candidate for acceptance with a higher score and are glad to engage with the reviewer throughout the rebuttal period!\n\n**Update:** We have updated a revised version of our manuscript, including your suggested comments and additional information. These changes have been colored in blue and can be found on pages 6 and 31."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699711414971,
                "cdate": 1699711414971,
                "tmdate": 1699757053626,
                "mdate": 1699757053626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iFVGoJzxBK",
            "forum": "PP1rudnxiW",
            "replyto": "PP1rudnxiW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_LiYc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_LiYc"
            ],
            "content": {
                "summary": {
                    "value": "This paper is divided in two parts. In the first part, the authors provide a unifying framework for VAEs and diffusion models using stochastic differential equations. In the second part connect the iterative proportional fitting procedure with the expectation maximization algorithm and provide a new objective for solving IPF that avoids the mode forgetting phenomenon that happens when one implements the diffusion schrodinger bridge. The added regularization term is null iff the HJB equation holds. Finally, the authors introduce the MCMD algorithm which makes uses of forward and backward SDEs that admit as marginals a prescribed sequence of distributions $(\\pi_t)_t$."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- I enjoyed reading this paper; it is highly pedagogical and sheds light on interesting connections between variational inference, diffusion models, stochastic control as well as optimal transport. The connection with the expectation maximization algorithm is particularly interesting. \n- The proposed algorithm CMCD generalizes existing methods such as Monte Carlo VAE aswell as MCD. I believe that the adopted point of view here is more rigourous and elucidates what the backward process should \"look like\", which was not clear in the Monte Carlo VAE paper for example. \n- Extensive experiments show that the CMCD method outperforms existing methods in relatively high dimensional examples."
                },
                "weaknesses": {
                    "value": "- It is quite unfortunate that there are no experiments for the loss proposed in Proposition 3.2 to back the fact that the proposed loss does not suffer from mode forgetting. Furthermore, it is not clear how this is implemented in practice since a Laplacian term is involved."
                },
                "questions": {
                    "value": "i have no further questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Reviewer_LiYc",
                        "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699214105001,
            "cdate": 1699214105001,
            "tmdate": 1700693493375,
            "mdate": 1700693493375,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "88SWIWMNRG",
                "forum": "PP1rudnxiW",
                "replyto": "iFVGoJzxBK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Mode collapse experiments + pointer to reference forgetting experiments in Appendix"
                    },
                    "comment": {
                        "value": "Dear Reviewer LiYc,\n\nThank you for your review and very insightful comments which have led to major improvements in our revised version.\n\n> It is quite unfortunate that there are no experiments for the loss proposed in Proposition 3.2 to back the fact that the proposed loss does not suffer from mode forgetting.\n\nOut of the alternate losses we discuss, it is the log-variance divergence (introduced after Equation 24)  that can handle mode collapse: \n\n$$ \\mathcal{L}^{\\mathrm{Var}}(\\phi)=\\mathrm{Var}\\left(\\ln \\frac{\\pi_T\\left(\\boldsymbol{Y}_T\\right)}{\\pi_0\\left(\\boldsymbol{Y}_0\\right)}+\\int_0^T \\Delta \\phi_t\\left(\\boldsymbol{Y}_t\\right) \\mathrm{d} t-\\sigma \\sqrt{2} \\int_0^T \\nabla \\ln \\pi_t\\left(\\boldsymbol{Y}_t\\right) \\circ \\mathrm{d} \\boldsymbol{W}_t-\\sigma^2 \\int_0^T\\left|\\nabla \\ln \\pi_t\\left(\\boldsymbol{Y}_t\\right)\\right|^2 \\mathrm{~d} t\\right)$$\n\nThe loss with the HJB/PINN regularizer in Proposition 3.2 does not address the issue of mode collapse, and its application is more relevant to different problems (relevant to molecular dynamics).\n\n> Furthermore, it is not clear how this is implemented in practice since a Laplacian term is involved.\n\nAs you have pointed out, there are some Laplacian terms that are not immediately obvious how we could compute. However, as it turns out the term inside the variance is simply the RND between the forward and backwards SDEs, so we can use the exact same discretisation that we used for KL to discretise it and thus: \n\n$$ \\mathcal{L}^{\\mathrm{Var}}(\\phi)  \\approx \\mathrm{Var}\\left[\\ln\\frac{\\pi_0(Y_0)}{\\hat{\\pi} (Y_T)}\\prod_{k=0}^{K-1}\\frac{\\mathcal{N}(Y_{t_{k+1}} |Y_{t_k}  + (\\nabla \\ln \\pi_{t_{k}}+\\nabla \\ln \\phi_{t_k})(Y_{t_k}) \\Delta t_k , 2 \\sigma^2 \\Delta t_k)}{\\mathcal{N}( Y_{t_k} |Y_{t_{k+1}}   + (\\nabla \\ln \\pi_{t_{k+1}}-\\nabla \\ln \\phi_{t_{k+1}})(Y_{t_{k+1}}) \\Delta t_k , 2 \\sigma^2 \\Delta t_k)}\\right]$$\n\n\nThis makes this loss as scalable as the KL divergence CMCD objective that we ablate. In particular, it is a bit more scalable as the Variance can be taken w.r.t to any measure, thus allowing us to detach the samples and circumvent backpropagating through timesteps.\n\n>  .. There are no experiments for the . ..\n\nTo address your main weakness, we explore the log-variance (logvar) loss in the very challenging 40 GMM target [2] for which reverse KL-based methods all mode collapse.\nWe compare CMCD KL to CMCD with logvar, and we demonstrate how it significantly outperforms the KL-based loss, both in ELBO but also qualitatively in its ability to overcome mode collapse. The results and plots illustrating mode collapse can be found in **Appendix F.3. (highlighted in olive)** in particular **Tabe 2, Figures 3 and 4**, and here is a table to summarise them\n\n|                     |         ELBO        |        $\\ln Z$       |   $\\mathcal{W}_2$  |\n|---------------------|:-------------------:|:--------------------:|:------------------:|\n| *log-variance* loss |  -1.279 $\\pm$ 0.096 | -0.065 $\\pm$ $0.101$ | 0.0143 $\\pm$ 0.001 |\n| KL loss             | -2.286 $\\pm$ 0.1109 |  -0.244 $\\pm$ 0.3309 | 0.0441 $\\pm$ 0.012 |\n\n\n> \u2026  for the loss proposed in Proposition 3.2 to back the fact that the proposed loss does not suffer from mode forgetting ..\n\nThe loss in proposition 3.2 (now moved to the appendix) solves a different problem to mode collapse. Rather than addressing mode forgetting, it deals with forgetting the reference SDE. This does not solve practical issues (e.g. such as mode collapse) in sampling or generative modelling, but it might be useful in transition path problems in MD. **In appendix G**, we do some small ablations and demonstrate across a series of toy targets (including an MD double well problem) that it does indeed do well in not forgetting the reference process.  To address the Laplacian term computation in this loss in this case, we used specialised architectures from OT [1], which allow for fast calculation of these terms.\n\nFinally, we note we have added a series of further experiments and ablations to address the other reviewers as well as improved the presentation of our work.\n\nWe hope that this addresses your main weakness, and we sincerely request any further feedback that you may be able to provide us with regarding our comprehensive changes to the presentation of the paper, as well as additional ablations and baselines. Thank you for your continued feedback!\n\n[1] Onken, D., Fung, S.W., Li, X. and Ruthotto, L., 2021, May. Ot-flow: Fast and accurate continuous normalizing flows via optimal transport. In Proceedings of AAAI\n\n[2] Midgley, L.I., Stimper, V., Simm, G.N., Sch\u00f6lkopf, B. and Hern\u00e1ndez-Lobato, J.M., 2022. Flow annealed importance sampling bootstrap. ICLR 2023"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176530613,
                "cdate": 1700176530613,
                "tmdate": 1700245637550,
                "mdate": 1700245637550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FmoG0rxgJk",
                "forum": "PP1rudnxiW",
                "replyto": "Sbabr5PZOo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_LiYc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_LiYc"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n\nThank you for your response and your hard work. In my initial comment, I meant reference SDE and not mode forgetting as you pointed out, sorry for that. \n\nI am satisfied by the numerical experiments and the paper overall. I have chosen to increase my score."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693481631,
                "cdate": 1700693481631,
                "tmdate": 1700693481631,
                "mdate": 1700693481631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "De8pik4v9w",
            "forum": "PP1rudnxiW",
            "replyto": "PP1rudnxiW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_yH6G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1846/Reviewer_yH6G"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the controlled Monte Carlo diffusion sampler (CMCD). CMCD builds upon previous work in the literature by adapting both the forward and the backward dynamics. The authors also posit a regularised iterative proportional fitting for schrodinger bridges that improve upon standard IPF. Finally, the authors link CMCD to Jarzinsky identity and demonstrate its performance through numerical experiments."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The technical contributions of this paper is unquestionable. The authors have managed to link key and fundamental ideas from various fields. I especially liked the connection between regularised IPF and EM algorithm, which is illuminating. I think the paper has good contribution and would be helpful to researchers in the field if presented correctly."
                },
                "weaknesses": {
                    "value": "Unfortunately, I think the presentation of the paper needs to be revised extensively. The paper is confusing, and posits ideas without forewarning or motivation. The introduction is especially difficult to read, and not because of its technicality. In general, I believe the authors' math is quite readable. It is the writing and motivation that needs improvements. As an example, nowhere in the introduction the authors explicitly state the problem they are tackling (CMCD). It takes until page 7, eq. (22) for the authors to state the explicit problem that they address. \n\nAs another example, the authors never explicitly state the necessity of various connections they draw. For example, the text above Proposition 3.1 explains why IPF fails to perform well. However, there is no explanation afterwards to say why going through the lens of EM solves this issue. Rather, the text proceeds to describe optimality conditions in what seemed to me to be like a tangent. In summary, although I could follow the math, it was extremely hard for me to follow the context of the paper.\n\nIt pains me to say that I cannot suggest the authors any single thing that could drastically improve the paper. I appreciate the technical contribution. However, given how difficult it is to follow its context, I don't believe that the paper would be helpful to the wider community in its current state.\n\nAt the very least, I think the first two pages of the introduction should be thoroughly revised to focus more on the problem addressed in the paper, rather than overloading on related technical terms and connections. I also think the propositions which are not key contributions of the paper (for instance, 2.1) could be better placed in the supplements. \n\nI also list some minor typos:\n\n1. A bracket has not been closed in page 2, paragraph 3, after \"(Section 2.2\"\n2. Page 7, last paragraph, \"Proposition2.2\" is missing a white-space.\n\nPost Rebuttal: I thank the authors for the extensive revision. I think it addresses most of my comments and the authors truly went above and beyond. I have revised my scores to reflect this. I think that in its current form the paper can definitely be useful and has interesting insights which are presented clearly. I believe, if accepted this will be an important contribution to the community."
                },
                "questions": {
                    "value": "Is eq. (10) the same as eq (1)? I failed to find the difference."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1846/Reviewer_yH6G",
                        "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699304460999,
            "cdate": 1699304460999,
            "tmdate": 1699988735517,
            "mdate": 1699988735517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "joeMgjlStz",
                "forum": "PP1rudnxiW",
                "replyto": "De8pik4v9w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revised version of manuscript with significant changes to the presentation structure  - Part I"
                    },
                    "comment": {
                        "value": "Dear Reviewer yH6G,\n\nWe thank you for the time and the very detailed feedback, especially since the presentation difficulties you have encountered must have required more time in parsing through our manuscript, and we apologise for the same.\n\nWe would also like to thank you for the very positive comments regarding the strength of the technical contribution of our work. We really appreciate your feedback and suggestions for improvement despite the presentation of the paper, while still being able to provide excellent actionable feedback on our technical contribution.\n\nWe would like to work with you as much as possible through this discussion period to understand how we can further improve the presentation. We appreciate that you must have many other commitments but we would greatly appreciate it if you could engage with us in improving the presentation of our work.\n\nWe have uploaded a revised version of the manuscript where we have attempted to address some of the changes recommended by the reviewer, these changes have been coloured in **magenta/pink**. \n\nThe revised manuscript (https://openreview.net/pdf?id=PP1rudnxiW) contains the following changes which are highlighted in the revised version.\n\n1. We have mentioned that our focus is the sampling task from a very early stage in the introduction and we formally introduce the sampling task in a titled paragraph, connecting it to framework 1 and thus providing motivation to our setup. \n2. We have reworded the abstract to acknowledge the focus on the sampling task and we have removed the remark on the HJB regularisers from the abstract as this is not our central contribution. \n4. We have also highlighted that the EM \u21d4 IPF correspondence mostly serves a conceptual purpose that connects two adjacent fields rather than an algorithmic one.\n5. We have changed our contributions to shift the focus to CMCD and the sampling task.\n6. On page 6, we removed non-central results such as the paragraph on HJB regularizers and the corollary that followed. As before, we highlight that the EM and IPF connection is conceptual rather than practical. \n7. Also on page 6 we explain more clearly the issues present with EM and IPF, emphasising that an accumulation of errors occurs due to their sequential nature. We then signpost to the next section motivating that our proposed method CMCD can be trained end to end with simultaneous updates and thus solve this sequential error accumulation issue.\n8. We have also added pseudocode for sampling and $\\ln Z$ estimation with CMCD and we hope that this also draws the focus/attention to the task and enhances the readability.\n\nAdditionally, we have also tried restructuring the introduction in a different way which can be found in this anonymised link https://anonymous.4open.science/r/CMCD-6BF6/rebuttal_version.pdf\n(same link as the anonymised codebase), here we changed the way the framework is introduced to try and make it more amenable to sampling. We hope that one of these revisions addresses some of the presentation issues raised by the reviewer.\n\nWe hope that the revised presentation and rebuttal pave the way to addressing the reviewer's concerns and we are eager to hear the reviewer's feedback plus further points of improvement."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699749787206,
                "cdate": 1699749787206,
                "tmdate": 1699749873114,
                "mdate": 1699749873114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yIDvYMI4PE",
                "forum": "PP1rudnxiW",
                "replyto": "De8pik4v9w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revised version of manuscript with significant changes to the presentation structure - Part II"
                    },
                    "comment": {
                        "value": "In what follows we will directly address some of your other questions/comments (which have been updated in the revised version of the manuscript):\n\n> Is eq. (10) the same as eq (1)? I failed to find the difference.\n\nThe following equations are taken from the unrevised version of the manuscript that you read.\n\nEquation (1): \n\n$$\\boldsymbol{z} \\sim \\nu(\\boldsymbol{z}), \\quad \\boldsymbol{x} \\mid \\boldsymbol{z} \\sim p^\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})$$\n\nEquation (10): \n\n$$\\mathcal{L}^{ext}(\\phi, \\theta) :=D(q^{\\mu, \\phi} ( y_{0: L} ) \\vert\\vert p^{\\nu, \\theta} ( y_{0: L}))$$\n\nEquation (1) specifies a \u201cstatic\u201d generative model whilst equation (10) details the divergence between two pathwise (many latent) generative models. Is it possible the reviewer was referring to a different pair of equations? Maybe they meant equations 10 and 5: \n\nEquation (5)\n\n$$ \\mathcal{L}_D(\\phi, \\theta):=D\\left(q^\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\mu(\\boldsymbol{x}) \\vert\\vert p^\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) \\nu(\\boldsymbol{z})\\right)$$\n\nBoth Equations 10 and 5 define the loss as a divergence between backwards and forwards generative models. However, Equation 10 (as the text preceding it details) is a path KL divergence because the generative model has many intermediate latent variables (which becomes an SDE in the limit) whilst Equation 5 does not have any of these intermediate latents. It is a static/non-dynamic setup (you can think of the intermediaries as being marginalised out). \n\nThe purpose of having both these equations is, as reviewer LiYc highlights (i.e. \u201cI enjoyed reading this paper; it is highly pedagogical and sheds light \u2026 \u201d), pedagogical. We present our work and framework in a pedagogical manner, first starting from simple static generative models that readers may be familiar with from VAE literature, and then we slowly build up from there to the non-static / dynamic SDE setting. \n\n> Proposition 3.1 explains why IPF fails to perform well. there is no explanation afterwards to say why going through the lens of EM solves this issue. \n\nIndeed this was presented in a very confusing manner and has been changed in the revised version. \n\nThe purpose of the connection to EM was not to solve an issue with IPF but to highlight how there exists a correspondence between these two seminal algorithms that have been developed almost in isolation from one another. This provides us a conceptual link between VI and OT which can help when learning one field given knowledge of the other.\n\nThank you for bringing this point to our attention.\n\n> . Rather, the text proceeds to describe optimality conditions in what seemed to me to be like a tangent. \n\nThis is a really good point we were unable to explain this fully due to the page constraint, we have moved this section to the appendix and taken more time to explain the issues with VM and IPF.\n\n> It pains me to say that I cannot suggest the authors any single thing that could drastically improve the paper. I appreciate the technical contribution. \n\nWe are very sorry that the initial presentation of our work has made you feel this way, however, we still believe your comments have been very helpful and have led us to make significant restructuring changes across the paper. \n\n> At the very least, I think the first two pages of the introduction should be thoroughly revised to focus more on the problem addressed in the paper\n\nWe have added the sampling task (which is the task CMCD solves) to the start of the introduction and modified the signposting and motivational elements in the introduction to refer more to this task.\n\n> I also think the propositions which are not key contributions of the pape \u2026 could be better placed in the supplements.\n\nWe have moved proposition 3.2 to the appendix as well as the section on HJB reguralisers which are not part of our central algorithmic contribution/experiments. We hope this allows the reader to have a more clear and contextual focus on the story.\n\n> Rather than overloading on related technical terms and connections. \n\nWe have not fully removed the connections / related work from the introduction as you requested for the following reasons\n\n1. Variational inference is effectively the flavour of the task we are solving. We are doing sampling through VI using ideas from optimal transport. This requires us to introduce and discuss VI (Framework 1) in the introduction.\n2. ELBOs are used for solving the second part of the sampling problem, namely estimating the normalising constant, and thus it is essential for us to introduce these earlier on.\n3. OT and coupling allow us to highlight some of the pitfalls that doing full-blown VI can have, and as part of the motivation we want to discuss these issues in the introduction since they motivate our proposed improvements.\n\nInstead, we have tried to reinforce their relevance to the sampling problem."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699749854440,
                "cdate": 1699749854440,
                "tmdate": 1699791652148,
                "mdate": 1699791652148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YpuipN8ay4",
                "forum": "PP1rudnxiW",
                "replyto": "yIDvYMI4PE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_yH6G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_yH6G"
                ],
                "content": {
                    "comment": {
                        "value": "I sincerely thank the authors for their comments and their extensive modifications. I think the presentation is much clearer now and I also now finally understand the greater context. I believe that both of the revised manuscripts are good, although the second one (4openscience) is slightly clearer.\n\nI believe with this modification the paper has reached finally a place where it is broadly significant and I have modified my original rating to reflect this."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988519153,
                "cdate": 1699988519153,
                "tmdate": 1699988519153,
                "mdate": 1699988519153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]