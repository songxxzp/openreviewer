[
    {
        "title": "ProteiNexus: Illuminating Protein Pathways through Structural Pre-training"
    },
    {
        "review": {
            "id": "rmVyTORsbF",
            "forum": "iBAWiEjogY",
            "replyto": "iBAWiEjogY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a general architecture for protein representation learning, utilizing a pre-training method of masked residue type prediction. Following this, the model is fine-tuned using lightweight decoders for a range of downstream tasks such as model quality assessment, binding affinity change prediction, EC and fold classification, protein design, and antibody design. It manages to achieve state-of-the-art performance in certain areas."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper's ambition to create a universal protein pre-training model is commendable. Pursuing this aim, the author presents a transformer architecture that seamlessly integrates sequence and structural data through a straightforward pre-training objective."
                },
                "weaknesses": {
                    "value": "1. The majority of the components within the proposed method are adaptations from prior studies, which the paper fails to acknowledge. The transformer approach mirrors that of [1], and the pre-training objective resembles [2]. Consequently, the work's novelty is somewhat questionable.\n2. The paper's \"related work\" section is not exhaustive. Notable omissions in the protein language domain include TAPE [3], ProtTrans [4], Ankh [5], and ProGen2 [6]. Additionally, recent advancements in protein structure representation, such as ProNet [7] and CDConv [8], are overlooked. A discussion on the connection between this study and previous ones is conspicuously absent.\n3. The presented experimental results have serious issues like data leakage and the absence of critical baselines, undermining the paper's claims.\n\nFor details, please refer to the Question section."
                },
                "questions": {
                    "value": "1. The authors motivate the use of transformers as encoders by critiquing graph-based representation learning methods for two reasons: 1) their inability to capture detailed atomic information, and (2) their disregard for long-range interactions. However, recent studies [7,9,10] demonstrate that graph-based methods can indeed be extended to the atom level. Additionally, the authors' method focuses solely on backbone-level structures, thus overlooking side-chain details. Regarding long-range interactions, the paper lacks experiments substantiating their claim. Thus, the critiques they raise against graph-based methods lack evidence.\n2. It's inadvisable for the authors to describe their method as \"groundbreaking\" in the introduction\u2014this is a clear exaggeration.\n3. The model quality assessment, used by the authors for evaluation, has a potential data leakage risk during pre-training. This task aims to predict the GDT-TS score of certain model predictions without revealing the ground-truth structures. Yet, using the PDB data up to May 1st, 2023, means the model may have encountered target structures from CASP14 and CASP15 during pre-training. Despite different loss functions, this could pose a significant issue.\n4. In the model quality assessment, the authors omit essential baselines. Notably, this task has been included in the Atom3D benchmark [11], where baselines [9,12] are essential references.\n5. For binding affinity prediction, the authors neglect to explain their dataset splits\u2014critical to avoid data leakage. Given the small test datasets, it's standard to conduct multiple cross-validations under varying random seeds. Traditional methods like FlexDDG [13] should also be considered for comparison.\n6. In the EC and fold classification benchmarks, there's an absence of vital baselines, notably CDConv [8] and ESM-GearNet [14]. The authors might also explore the more challenging GO prediction tasks detailed in [15]. Even without these benchmarks, the authors' method falls behind leading approaches.\n7. For protein design tasks, there are serious data leakage problems due to the presence of test data in pre-training dataset. As discussed in App. E.3, such leakage can dramatically affect performance. The authors have not provided a fair comparison with other methods, which makes the evaluation here not convincing.\n8. In both protein and antibody design tasks, the metrics of perplexity and aar have been misleadingly employed in the field to evaluate protein folding models. The focus of these metrics on \"local\" recovery rather than entire sequences can inflate performance figures. A more accurate gauge would be to use the AF2 metric to assess structure recovery.\n\nOverall, I commend the authors' ambition to introduce a universal model for protein-related tasks. However, their review of prior works appears incomplete, and the comparisons in their experiments lack rigor. Consequently, this paper does not meet the acceptance standards of ICLR.\n\n[1] Shan et al. \u201cDeep learning guided optimization of human antibody against sars-cov-2 variants with broad neutralization\u201d, PNAS, 2022\n\n[2] Zhang et al. \u201cProtein representation learning by geometric structure pretraining\u201d, ICLR, 2023\n\n[3] Rao et al. \"Evaluating protein transfer learning with TAPE.\"\u00a0NeurIPS, 2019\n\n[4] Elnaggar et al. \u201cProttrans: Toward understanding the language of life through self-supervised learning\u201d, PNAS, 2021\n\n[5] Elnaggar et al. \u201cAnkh: Optimized Protein Language Model Unlocks General-Purpose Modelling\u201d, 2023\n\n[6] Madani et al. \u201cLarge language models generate functional protein sequences across diverse families\u201d, Nature Biotech, 2023\n\n[7] Wang et al. \u201cLearning Hierarchical Protein Representations via Complete 3D Graph Networks\u201d, ICLR, 2023\n\n[8] Fan et al. \u201cContinuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins\u201d, ICLR, 2023\n\n[9] Jing et al. \u201cEquivariant Graph Neural Networks for 3D Macromolecular Structure\u201d, 2021\n\n[10] Zhang et al. \u201cPre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction\u201d, 2023\n\n[11] Townshend et al. \u201cATOM3D: Tasks On Molecules in Three Dimensions\u201d, NeurIPS Dataset and Benchmark Track, 2022\n\n[12] Pages et al. \u201cProtein model quality assessment using 3d oriented convolutional neural networks\u201d, Bioinformatics, 2019\n\n[13] Barlow et al. \"Flex ddG: Rosetta ensemble-based estimation of changes in protein\u2013protein binding affinity upon mutation.\"\u00a0The Journal of Physical Chemistry, 2018\n\n[14] Zhang et al. \u201cEnhancing protein language models with structure-based encoder and pre-training\u201c, 2023\n\n[15] Gligorijevic et al. \u201cStructure-based protein function prediction using graph convolutional networks\u201d, Nature Communications, 2021"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5058/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5058/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5058/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698093053034,
            "cdate": 1698093053034,
            "tmdate": 1699636495673,
            "mdate": 1699636495673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DqyW3rNAF5",
                "forum": "iBAWiEjogY",
                "replyto": "rmVyTORsbF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5058/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oPJy (Part1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate the careful review and insightful comments from the reviewer. Below, we provide responses to each comment individually.\n\n> Weaknesses 1: The majority of the components within the proposed method are adaptations from prior studies, which the paper fails to acknowledge.\n\n- The novelty of our work lies in the protein structure encoder, the design of the pretraining method, and the generality of the pretrained model. Our approach is applicable to a wide range of downstream tasks, such as affinity prediction, antibody sequence design, and generation of corresponding CDR structures. These are areas in protein representation learning that have not been extensively explored before. \n  - In reference [1], the initial features of residues are based on the local geometric structure of the C\u03b1 atoms, and the atomic coordinate information is considered only within the residues as a supplementary structural feature. In contrast, we construct the position direction between any residues i and j using a spatial position encoder. \n  - Additionally, the distance encoder provides distance information between any backbone atoms. We no longer treat atomic-level coordinate information as an auxiliary property of local residue structure, but instead calculate it across the entire protein structure. The pairwise relationships constructed in this way complement the sequence-based self-attention mechanism, continuously updating the model's parameters to enhance representational capacity and performance. In addition, our pretraining objective also differs slightly from the distance prediction, angle prediction, and dihedral prediction mentioned in [2]. As far as I know, those approaches are conducted at residue level. In our case, we recover the distances between atoms by using residue-level pairwise representations, ensuring that while capturing interactions between residues, we can preserve atomic-level fine-grained information as much as possible. \n\n> Weaknesses 2: The paper's \"related work\" section is not exhaustive.\n\nWe appreciate your valuable feedback and acknowledge the oversight in our \"related work\" section. We recognize the significance of including recent advancements in protein language models like TAPE, ProtTrans, Ankh, and ProGen2, as well as advancements in protein structure representation such as ProNet and CDConv. In the revised manuscript, we will ensure to provide a comprehensive discussion on the connection between our study and these relevant works to enhance the overall context of our research. Thank you for bringing these important references to our attention.\n\n> Q1: About side-chain details and long-range interactions\n\n- For example, in [7,10], the authors represent protein structures using graphs, where each amino acid is treated as a node and edges are defined based on a cutoff radius. This is a common construction method in graph representation learning. However, it often ignores the interactions between distant residues in inter-chain interactions, as the distance scale differs between intra-chain and inter-chain residues. In contrast, our approach considers both the distance and direction information between any two residues, reducing the impact of the cutoff radius on capturing long-range interactions. \n- As you mentioned, the details of side chains are worth exploring, and we have also taken note of this issue. Building all-atom model of proteins is challenging due to the different types and numbers of atoms in different residue side chains. Moreover, considering the low degrees of freedom in side chain atoms, introducing side chain atoms would increase computational complexity without matching the benefits. In [7], the all-atom model added four torsion angles to the backbone atom model but did not model all side chain atoms explicitly like backbone atoms. Instead, we aggregate the position information of side chain atoms into an additional virtual atom representation. We used the same training settings as the backbone atom pretraining and tested it on downstream tasks. The results showed that incorporating side chain information increased computational complexity without significantly improving the model's predictive performance. We hope to find a more reasonable approach to extend it to all-atom representation in future work.\n\n|         | fold  | super | family | EC   | CATH |\n|---------|-------|-------|--------|------|------|\n| All-atom| 45.5  | 78.3  | **98.2** | **88.7** | 50.7 |\n| ProteiNexus| **47.6** | **79.7** | 98.0 | **88.4** | **53.5** |\n\n\nRegarding Weaknesses 3 and the remaining questions about additional experimental details, we will provide a detailed response in the Part2."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468154216,
                "cdate": 1700468154216,
                "tmdate": 1700468154216,
                "mdate": 1700468154216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b2lNQgGEe1",
                "forum": "iBAWiEjogY",
                "replyto": "DqyW3rNAF5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
                ],
                "content": {
                    "comment": {
                        "value": "I\u2019d like to thank the authors\u2019 response, but I find many questions I raised have not been addressed during rebuttal. Here is my detailed response:\u2028\n>**Weakness 1: Novelty**\n\nThe authors claim \u201cThe novelty of our work lies in the protein structure encoder, the design of the pretraining method, and the generality of the pretrained model.\u201d I respectfully disagree with that.\n\n**Comparison with [1].** During rebuttal, the authors propose that the difference between their architectures and [1] is the pairwise position modeling. However, [1] does model the pairwise positional information through their design of spatial attention. Also, if the authors want to emphasize the novelty of their architecture design over [1], there should be at least some experimental comparison with [1] with the same pre-training setting. \n\n**Modeling pairwise distance information between backbone atoms.** There have been tons of works, such as [15], using contact maps to model protein structures, before geometric deep learning was proposed. I don\u2019t think this is the novelty of this paper and clearly the authors have not discussed the relation with these works.\n\nIt is unclear in the paper how the authors \u201crecover the distances between atoms by using residue-level pairwise representations\u201d during pre-training. Also, the distance prediction methods proposed in [2] can be easily adapted to atom-level as baselines, just as a baseline in [10].\n\n>**Q1: About side-chain details and long-range interactions**\n\nThe authors\u2019 response do not answer my questions.\n\n**About long-range interactions.** My question is not about how the method is designed to model long-range interactions, but \u201cthe paper lacks experiments substantiating their benefits of long-range interaction\u201d.\n\n**About side-chain atom modeling.** I\u2019m not asking for proving the benefits of side-chain atom modeling. My question is: \u201cthe paper claims to use atom-level structures, which graph-based methods cannot.\u201d If the author admit that the graph-based methods are able to model side-chain information, they should change their criticism about graph-based methods. Another question is \u201cthe authors' method focuses solely on backbone-level structures\u201d. Typically, we only consider models with side-chain information as atom-level encoders. The author should change their claim about the model. \n\nBesides, I find the authors miss the discussion with the atom-level GVP work [9] during rebuttal."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587329055,
                "cdate": 1700587329055,
                "tmdate": 1700587329055,
                "mdate": 1700587329055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "shK4oKIRGW",
                "forum": "iBAWiEjogY",
                "replyto": "yldnskEhJe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_oPJy"
                ],
                "content": {
                    "comment": {
                        "value": ">**Q3: Data leakage in MQA**\n\nEven with different pre-training objectives, the author cannot overlook the data leakage problem. If the methods see the native structures during pre-training, it can learn and memorize some characteristics about its structures, which may lead to overestimated results in downstream tasks. I appreciate the authors\u2019 efforts in alleviating this issue during rebuttal. I would like to see more analysis on that on the final version.\n\n>**Q4: Missing baseline.**\n\nAs the authors claim that \u201cour model exhibits outstanding performance, establishing\nnew benchmarks across a range of tasks\u201d, I would not change my opinion if the authors choose to ignore the important baselines I suggest.\n\n>**Q5: Dataset splits for binding affinity prediction.**\n\nIf the authors just randomly split mutations into training and test sets, then the benchmark in Table 2 is unfair and meaningless. The correct evaluation protocol is to split the dataset by the protein similarity, as done in [1]. Predicting mutations on the same protein is much easier than predicting mutations across different proteins.\n\n>**Q6: Missing datasets and baselines.**\n\nAgain, as the authors claim that \u201cour model exhibits outstanding performance, establishing\nnew benchmarks across a range of tasks\u201d, I would not change my opinion if the authors choose to ignore the important datasets and baselines I suggest.\n\n>**Q7: Missing datasets and baselines.**\n\nEven if \u201cthe decline in protein design performance is not solely attributed to data leakage\u201d, the issue still makes the benchmark in Table 4 invalid.\n\n>**Q8: Evaluation of protein design and antibody design.**\n\nThe problem of AAR as a metric for protein design is that we do not know \u201cwhether larger portion of residues can be recovered, the closest structure it can fold into\u201d. That\u2019s why we need to refold the designed sequence with structure recovery metrics."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587360186,
                "cdate": 1700587360186,
                "tmdate": 1700587360186,
                "mdate": 1700587360186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RS6H5TqUk3",
            "forum": "iBAWiEjogY",
            "replyto": "iBAWiEjogY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5058/Reviewer_wGDE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5058/Reviewer_wGDE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel pretraining approach for protein representation learning that integrates sequential and structural information. The structural encoding mechanism enables the encoder to learn protein distance information and spatial relative positions of residues, overcoming the inherent drawbacks of ignoring long-range interactions of graph-based representations. As a result, the authors present the model ProteiNexus pretrained by a hybrid masking strategy and mixed-noise strategy to comprehensively capture the structural information. The model is fine-tuned with lightweight task-specific decoders, culminating in exemplary performance across a range of downstream tasks, especially in the understanding of protein complexes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: The paper proposes a novel pretraining strategy which effectively integrates the sequential and structural encoding for the representation learning of proteins. Therefore, the paper uniquely contributes to the field by implementing a simple, yet potent, architecture to capture structural information comprehensively.\n\nQuality: The paper carefully designs the experiments to support the idea. In particular, the extensive experimental results and experimental details presented in the manuscript reflect the comprehensive work of the authors. \n\nClarity: The paper effectively communicates its ideas and findings with clarity. The paper is well-written, and the logic is coherent. The experimental settings and findings are structured and easy to find the related contents. \n\nSignificance: The paper focuses on improving representation learning for proteins as a foundation model for multiple downstream tasks. The model proposed in the manuscript is able to encode sequential and structural data, and surpass baselines on many downstream tasks, illustrating its potential in even more applications in protein design and discovery."
                },
                "weaknesses": {
                    "value": "1. Although the paper is well-written, logically coherent, and self-consistent, I'm afraid the novelty of the paper is not too high. The pertaining strategy which combines sequential and structural information is not new to the field. Furthermore, the major contribution of the paper, which is encoding both the atom-level and the finer-grained distance information, has also been studied extensively in recent years. Therefore, I cannot be persuaded of the novelty of the manuscript unless the authors can provide more evidence about how their model differentiates from existing methods and how their adaptations contribute to the enhanced model performance."
                },
                "questions": {
                    "value": "1. For pretraining experiments, I'm wondering how the noise level is determined. Besides, I'm wondering whether the authors have evaluated the data efficiency of the proposed pertaining approach by varying the pertaining data sizes. \n\n2. For fine-tuning experiments, are the encoder parameters also fine-tuned or frozen? Besides, since the focus of this paper is to evaluate the capability of the encoder and the decoders are already lightweight, why not simply fix the decoder architecture for every downstream task?\n\n3. The authors mention in the conclusion that \"... an efficient pre-training model ...\", but I'm wondering how the \"efficiency\" is illustrated: does it enhance model performance, have less computation cost, or require less training data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5058/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5058/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5058/Reviewer_wGDE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5058/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698271135354,
            "cdate": 1698271135354,
            "tmdate": 1699636495572,
            "mdate": 1699636495572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xsuy9ANik0",
                "forum": "iBAWiEjogY",
                "replyto": "RS6H5TqUk3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5058/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wGDE (Part1)"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive comments. I will address your questions one by one and provide insights about novelty.\n\n> Q1: 1. For pretraining experiments, I'm wondering how the noise level is determined. Besides, I'm wondering whether the authors have evaluated the data efficiency of the proposed pertaining approach by varying the pertaining data sizes.\n\n - We have detailed our chosen noise levels in Table 11 in Appendix D.2. We use different probabilities to mask the proportions of residues and determine the level of noise introduced. Specifically, we employ probabilities of 0.6, 0.2, and 0.2 to mask 15%, 50%, and 100% of sequence lengths, respectively. Additionally, we apply varying degrees of perturbation to the atom coordinates of these masked residues. More specifically, we introduce noise with probabilities of 0.2 and 0.8, following normal distributions with parameters N(0,0.1) and N(0,1), respectively. This introduces noise in the spatial relationships between atoms.\n\n - Simultaneously, we conducted an in-depth investigation into the impact of different noise levels on pretraining effectiveness. Firstly, we explored uniformly distributed noise within the range of (-1,1). Secondly, we adjusted the proportion of protein residues being masked and increased the probability of completely masking sequence lengths. We applied masking to 15%, 50%, and 100% of sequence lengths with probabilities of 0.3, 0.2, and 0.5, respectively. The results indicate that the strategy of masking residues and the introduced noise level significantly influence pretraining effectiveness. Increasing the difficulty of the pretraining task contributes to enhancing the model's generalization. \n\n|            | fold | super | family | EC  | CATH |\n|------------|-------|-------|--------|------|------|\n| noise-level| 34.1 | 52.7 | 95.4   | 78.0   | 49.6 |\n| mask-level | **50.8** | **81.7** | **98.1** | 87.2 | **55.9** |\n| ProteiNexus| 47.6 | 79.7 | 98.0 | **88.4** | 53.5 |\n\n - Regarding the evaluation of the impact of pretraining data on downstream tasks, we have not yet experimented with changing the size of the pretraining data. From the results shown in Experiment 6 of Table 13 and Table 14, it is evident that the size of the pretraining data has a significant influence on the results. We will conduct more detailed evaluations by selecting different sizes of pretraining data and applying different levels of redundancy reduction to assess their effects. \n\n> Q2: For fine-tuning experiments, are the encoder parameters also fine-tuned or frozen? Besides, since the focus of this paper is to evaluate the capability of the encoder and the decoders are already lightweight, why not simply fix the decoder architecture for every downstream task?\n\nDuring the fine-tuning process, we did not freeze the parameters of the encoder. \n - First, we conducted an experimental comparison on whether to freeze encoder parameters during fine-tuning, and the results are as follows:\n\n|                    | fold | super | family |  EC  | CATH |\n|-------------------- |------|-------|--------|------|------|\n| fixed pretrain model| 35.1 | 58.9  | 96.9   | 84.9 | 52.6 |\n| ProteiNexus         | 47.6 | 79.7  | 98.0   | 88.4 | 53.5 |\n\n - Although we obtained a universal protein representation through pretraining, when fine-tuning for specific downstream tasks, we preferred to transition from the universal representation to task-specific representations. Additionally, during the fine-tuning stage, we still required the pretrained model's ability to process input data with noise. In binding affinity task, the structures of mutants are often unknown. Most computational methods typically use the wild-type structure as a substitute or predict the mutant structure based on the wild-type using other software. However, these approaches do not accurately represent the native mutant structure. By leveraging the denoising capability of the pretrained model, we can mitigate the impact of such introduced errors. In the task of antibody CDR sequence structure co-design, we can also update the representation of CDR using the pretrained model. This allows us to generate accurate CDR sequences and structures from known framework regions, thereby avoiding the need for an additional decoder and improving computational efficiency."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497244081,
                "cdate": 1700497244081,
                "tmdate": 1700497244081,
                "mdate": 1700497244081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0dCwN7b9Un",
                "forum": "iBAWiEjogY",
                "replyto": "EhWbfPbwJB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_wGDE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_wGDE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response to my questions and concerns. I really appreciate that you have helped me better understand your contributions and resolved some of my concerns.\n\nHowever, I'm afraid the novelty part is still not convincing: although the authors provide several strengths of their approach, these have already been seen in many other works (also suggested by other reviewers), and the model in this paper does not stand out in terms of the model architecture, training strategies, downstream tasks, etc. \n\nTherefore, given my concerns which haven't been fully addressed and the criticisms pointed out by other reviewers, I'm sorry that I cannot raise my score. Wish you all the best in the future."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666423848,
                "cdate": 1700666423848,
                "tmdate": 1700666423848,
                "mdate": 1700666423848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bhXG5MNoTo",
            "forum": "iBAWiEjogY",
            "replyto": "iBAWiEjogY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5058/Reviewer_tXRC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5058/Reviewer_tXRC"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new pretraining approach for learning protein representations, which integrates both structural information and information about downstream tasks. The paper compares their approach to the state-of-the-art on a range of different tasks and report very favourable results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a potential solution to a highly relevant problem. The authors have compiled a very comprehensive list of relevant downstream tasks, and thereby make a good the case for a generally useful pretrained model. The reported results are highly competitive. If they hold, the method could thus have a real impact for practitioners in the application domain."
                },
                "weaknesses": {
                    "value": "First of all, I have a slight concern about how well this manuscript fits within the scope of the ICLR venue. Although the title and the introduction point towards a new method, the effective focus of the paper seems to be on benchmarking their method, rather than on the method itself. It seems odd to me that most of the description of the model itself has been moved to the appendix, despite the fact that this would presumably be the most interesting part for most of the ICLR community. For instance, the procedure for fine-tuning on downstream tasks is described in a single sentence in the main text - although it is quite central to their approach.\n\nSecondly, despite the fact that the focus is on the benchmarking, there are details missing about the experiments that makes it difficult for me to judge how much faith can be placed in the reported results. For several experiments (see details below), it is unclear how data was split between training, validation and test - and whether this was done in the same way for all methods that were compared in the result tables. Of particular importance, I did not find it clear whether there was overlap in the pre-training data and the data used for downstream testing. In appendix E.3, the authors have a few reflections on this topic, and seem to show a substantial drop in performance for the protein design task when removing part of the pretraining set. This seems like a red flag to me, which should be investigated more thoroughly - and not only for the protein design task.\n\nFinally, the paper itself does not provide a good explanation for why their approach outperforms prior methods. For the community, it would be useful to know if this relies primarily on the structural signal or the fine-tuning on downstream tasks. These questions are potentially partially addressed by the ablation study in appendix E, but the ablation results are never discussed in the main paper. Also, as far as I could see, the effect on fine-tuning on downstream tasks is not ablated - i.e. the difference between fine-tuning the entire pre-trained model or training a downstream model on a fixed pre-trained model."
                },
                "questions": {
                    "value": "Page 2. *\"Predominantly, graph-based representations struggle to preserve \ufb01ne-grained atom information effectively. Moreover, they tend to accentuate interactions among neighboring residues while often disregarding the in\ufb02uence of longrange interactions.\"*\nCould you provide a reference to back up this statement?\n\nPage 3. *\"Additionally, there are methods that transfer protein structures into distance matrices and attempt to denoise noisy distance matrices while simultaneously predicting the types of corresponding residue types. These approaches undergo pre-training on large-scale datasets to improve the quality and generalizability of the representations.\"*\nCould you provide citations for these methods?\n\nPage 4. *\"Lastly, we partition the continuous coordinates into bins of equal width and transform each bin into an embedding\"*\nCould you describe the motivation for this choice to discretize?\n\nPage 4. *\"the \"distance tokenizer\" method\"*\nAs far as I can see, this method has not been introduced in the paper. Could you elaborate?\n\nPage 4. *\"Speci\ufb01cally, we employ a one-hot encoding scheme to represent the relative distance between position i and position j in the sequence\"*\nWhy use a one-hot encoding to represent a distance? - doesn\u2019t this mean that there is no distinction between bins that are similar in distance and bins that are far apart?\n\nPage 4. *\"To better capture the global features and interactions of protein structures, we have opted for the transformer architecture as the backbone of our network. This decision is grounded in the inherent self-attention mechanism of the transformer, which enables computations across the entire protein sequence.\"*\nI don\u2019t understand the distinction you make here. If the graph attention is not capturing enough of the interactions you wish for, can\u2019t you then change the graph to include more interactions? In particular, as far as I can see, graph attention in a fully connected graph would be identical to the attention in a transformer. From that perspective, isn't your approach just a special case of graph attention?\n\nPage 5. *\"masked residues\"*\n\"What does \u201cmasked residue\u201d mean exactly. Are you masking the identity, or also the atom coordinates?\"\n\nPage 5. *\"encourage the model to recover authentic atom-level coordinate from noise-induced residue-level pair representations.\"*\nCould you be more precise? How is this \"encouraged\"?\n\nPage 5. *\"Our training dataset includes decoys derived from 7992 unique native protein structures, obtained from DeepAccNet. In the end, we have a collection of 39057 structures in our training dataset, with a fraction representing native structures.\"*\nSince it is central to the data generation process, you should explain in detail what DeepAccNet is, and why it makes sense to use decoys generated by this method as training data. EDIT: I see that you introduce DeepAccNet in the next paragraph, so part of the problem could be resolved by moving that introduction up here. But even when doing so, it is still not clear how the decoys are generated by this method, since it as far as I can see normally produces LDDT scores as output.\n\nPage 5. *\"our test set is meticulously curated. It includes targets with experimentally resolved structures from CASP14 and CASP15, paired with their corresponding predicted structures. To ensure diversity and representativeness, we perform a redundancy reduction process on the test set, limiting sequence identity between targets to within 70%.\"*\nDo you also ensure that there is no overlap (high sequence similarity) between the test set and the 7992 structures in your training set? \nThe choice of homology reduction to 70% seems rather high to me (we generally use values at 30% to avoid leakage). Why was this choice made? I guess you could verify whether this is a problem by plotting the performance as a function of homology to the nearest protein in the training set.\n\nPage 5. *\"We validate our pre-training model on \ufb01ve datasets\"*\nDoes this mean that you in this case do not train a downstream model, but directly use the frequencies of the pretrained model to obtain and estimate of the binding affinity. This should be clarified. If you do use a downstream model, you should clarify how the splits for training/test were constructed (and whether they overlap with the pretraining set). In particular, it is important to establish whether the methods in Table 2 are actually comparable (i.e whether we believe that none of them have been trained on the current test set).\n\nTable 3\nWere these other methods run with exactly the same train/test splits as you run your method. In other words, are the results comparable?\n\nPage 7. *\"LSTM (Rao et al., 2019), mLSTM (Alley et al., 2019) and CNN Shanehsazzadeh et al. (2020).\"*\nIt us a bit odd that you use architecture names to refer to specific trained models. It would be clearer if you for instance referred to the first as TAPE-LSTM, and the second as the UniRep model.\n\nPage 7. *\"ESM-1b\"*\nThe collection of baseline methods was a bit confusing. For instance, you mention language models like ESM-1b and ProtBert-BFD. How are these employed for fold and enzyme-catalyzed reaction classification? Do you somehow use them in an unsupervised way, or do you put a classification layer on top. If so, it would be clearer if you referred to them by a different name than the language model on which they are based.\n\nTable 4. \nAgain, it is unclear if these methods has been trained and tested on exactly the same datasets - in particular since some of the results are copied from other papers. Please clarify.\n\n\n\n### Minor comments:\n\nPage 1. \"For instance antibodies (such as SARS-CoV2)\"\nRephrase. SARS-CoV2 is not an antibody.\n\nPage 1. *\"in protein sequences (Consortium, 2019)\"*\nChange reference to reflect which consortium\n\nPage 1. *\"triumph in various tasks including...protein structure prediction (Rao et al., 2020;\"*\nThis paper is about contact prediction, not directly about protein structure prediction.\n\nTable 3. Caption. The title currently says *\"Results of classification\"*. Would be helpful if you could specify the experiment in the title."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5058/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697967349,
            "cdate": 1698697967349,
            "tmdate": 1699636495483,
            "mdate": 1699636495483,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N7DbdtFJtI",
                "forum": "iBAWiEjogY",
                "replyto": "bhXG5MNoTo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5058/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5058/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tXRC (Part1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review and valuable suggestions. We appreciate the insightful comments you provided, and we have carefully considered each one. Your feedback has been instrumental in improving the quality of our work.\n\n> Weaknesses 1: First of all, I have a slight concern about how well this manuscript fits within the scope of the ICLR venue.\n\nThank you for your review and valuable feedback. We understand your concerns regarding the limited description of the method itself in the main text, and we will make adjustments in the final version to better highlight the method we propose. Our structure-based pretraining is aimed at discovering general patterns for solving various protein computing tasks, and the extensive testing across downstream tasks is intended to demonstrate the effectiveness and applicability of the model. Regarding the description of the fine-tuning process you mentioned, we do recognize it as the core of our method, and we will further elaborate on this aspect in the main text to present the key steps of our method more clearly. With these adjustments, we hope to better meet the expectations of the ICLR community for a comprehensive description of the model method itself.\n\n> Weaknesses 2: Secondly, despite the fact that the focus is on the benchmarking, there are details missing about the experiments that makes it difficult for me to judge how much faith can be placed in the reported results. \n\nRegarding the issue of dataset split, we will respond to each specific question below. Regarding the potential data leakage between pretraining data and downstream task fine-tuning data, we would like to make the following points:\n - Firstly, tasks susceptible to data leakage are limited to model quality assessment and protein design. For other tasks such as binding affinity prediction, folding classification, etc., not only is additional annotated data introduced, but also the protein sequence and structural information serve as known conditions for the tasks. Hence, there is no conflict between the self-supervised pretraining tasks and downstream tasks, avoiding the impact of data leakage.\n - Secondly, concerning the data leakage issue in protein design, we have provided a brief explanation in Appendix E.3. We believe that the distribution of pretraining data significantly influences protein design outcomes. Therefore, we plan to cluster the pretraining data based on redundancy and elaborate on the actual reasons leading to the decrease in model performance. For the potential data leakage issue in model quality assessment, you can refer to the explanation provided in response to Reviewer oPJy (Part 2) Question 3.\n\n> Weaknesses 3: Finally, the paper itself does not provide a good explanation for why their approach outperforms prior methods. \n\nDuring the fine-tuning stage, we optimize the parameters of the pre-trained model simultaneously. Freezing the parameters of the pre-trained model may lead to a certain degree of performance decline. Our approach relies on both structural signals and fine-tuning on downstream tasks. As evident from the experimental results in Appendix E, our model functions as an organic whole. The effective integration of structural information, modeling of backbone atoms, and the choice of pre-training data and strategies contribute to our competitive performance across various tasks. While our pre-trained model extracts universal protein representations, different downstream tasks exhibit varying preferences for the information contained in these representations. We conducted experiments by training downstream models on a fixed pre-trained model, and the results are as follows:\n\n|                     | fold   | super  | family | EC    | CATH  |\n|---------------------|--------|--------|--------|-------|-------|\n| fixed pretrain model | 35.1   | 58.9   | 96.9   | 84.9  | 52.6  |\n| ProteiNexus     | **47.6** | **79.7** | **98.0** | **88.4** | **53.5** |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585205559,
                "cdate": 1700585205559,
                "tmdate": 1700585205559,
                "mdate": 1700585205559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4peGQRoXsR",
                "forum": "iBAWiEjogY",
                "replyto": "C3CEkaTL34",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_tXRC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5058/Reviewer_tXRC"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their rebuttal. While the authors make reasonable arguments, they do not seem to have updated their submitted PDF, and only promise to make changes in a \"final version\". Since several of the necessary modifications are substantial changes to the structure of the paper, not having these available makes it difficult to judge the quality of the promised update. Also, for the many requests for minor changes the authors provide a response in the rebuttal but do not state how they will modify the main text to clarify these issues in the paper itself - which is ultimately what matters.\n\nFor these reasons, I will not change my score of this paper at this time, but encourage the authors to resubmit a new version to a future venue."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5058/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664660888,
                "cdate": 1700664660888,
                "tmdate": 1700664660888,
                "mdate": 1700664660888,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]