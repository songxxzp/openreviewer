[
    {
        "title": "Sliced Denoising: A Physics-Informed Molecular Pre-Training Method"
    },
    {
        "review": {
            "id": "SKYKOmF21e",
            "forum": "liKkG1zcWq",
            "replyto": "liKkG1zcWq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3064/Reviewer_nTFT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3064/Reviewer_nTFT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a improved denoising pretraining method for 3D GNN in molecular machine learning tasks. The proposed method, called sliced denoising (SliDe), is inspired by classical intramolecular potential theory. SliDe adds different levels of noise to the length of bonds, the magnitude of angles, and the magnitude of torsion angles. Through a series of theoreticall derivation, the authors demonstrate the equivalence of SliDe method and learning the molecular force field. Lastly, the authors show that the SliDe has outperformed previous denoising training methods on the QM9 and MD17 dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Comparing to previous work such as the Fractional Denoising, the noise designing of this work is a more \"physical\" formulation because changing either one of the bond length, angle degree, or torsion angle degree does not affect the rest. \n\n2. The writing and proof process are of high clarity. \n\n3. Some efforts to the direction of explainablity are interesting. For example, the correlation coefficient between the learned force field and ground-truth force field (Table 1) is a nice way to quantified the learned force field."
                },
                "weaknesses": {
                    "value": "1. In section 3.1, the claim above Eq. 6 is not correct. Modeling long-range electrostatic (coulomb) interaction is critical in many areas including energy calculation and md simulation. For large system, long-range vdW outside of a cutoff distance may be neglected, but for small molecules in QM9 and MD17, it should not be neglected. When using GNN that takes 3D coordinate and atom type as input approximate energy function, the model should be able to learn those two terms, thus not affecting the approximation of Eq.6. However, the authors should rephrase the sentence. The two citations associated with the sentence does support the claim so they should be removed.\n\n2. The noise design is very import in this work. In the BAT noise (Eq. 9), the parameter vectors are critical but there isn't detailed explaination of them. The authors briefly discussed in the section C.1, but I do think more details and example of those parameter vectors can substentially help reader in understanding the noise design. \n\n3. Missing parenthesis in the second exponential term of Eq. 8. $(\\theta_{i} - \\theta_{i, 0})^2$\n\n4. I do think the superiority of the SliDe method can be strengthen by more downstream experiments, especially energy prediction. For example, the ANI-1x dataset (www.nature.com/articles/s41597-020-0473-z) is a excellent dataset for such task."
                },
                "questions": {
                    "value": "1. Table 7 is confusing. My understanding is that \u201cTraining from Scratch\u201d meaning no pretraining, and Coord and Frad meas pretrained with different method and then fine-tuned on MD17-Aspirin. What does DFT label supervised mean? Isn\u2019t \u201cTraining from Scratch\u201d also supervised? The authors should elaborate. The unit of the prediction MAE should also be included in the table."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3064/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3064/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3064/Reviewer_nTFT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698546690359,
            "cdate": 1698546690359,
            "tmdate": 1700582010410,
            "mdate": 1700582010410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6gmDJb4K40",
                "forum": "liKkG1zcWq",
                "replyto": "SKYKOmF21e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your expert feedback and your insightful concerns. Your concerns and questions are responded as follows:\n\n>Weakness 1: The claim above Eq. 6 is not correct and the authors should rephrase the sentence. The two citations should be removed.\n\nWe appreciate the valuable suggestion. Indeed, electrostatic and van der Waals interactions are important in molecular simulation to maintain the long-range structure of molecules in classical simulations. However, we do not expect the self-supervised pretraining force target to be as accurate as precise computational methods, which would greatly increase the burden of the pretraining task. We strike a balance between complexity and effectiveness, and raise the force field accuray by 42%(as shown in experiment in section 4.1) without substentially increasing the time complexity of pre-training(as shown in responses to ft62 question 1). \n\nAs for the explaination for the approximation, we would like to rephase is as follows. \"Secondly, we drop the last two terms in order to get a quadratic form of energy function in equation 6. Despite these long-range electrostatic and van der Waals interactions are important in classical simulations, we find the accuracy of the approximated energy function is much higher than existing self-supervised pre-training methods in section 4.1.\" If you have a better explanation or further suggestions, please feel free to respond us. \n\n>Weakness 2: I think more details and examples of those parameter vectors can substantially help the reader in understanding the noise design.\n\nThanks for the helpful suggestion. Here we provide some examples of BAT noise in the figure https://anonymous.4open.science/r/SliDeRebuttal-EEC9/STDBat32.jpg. (a)(b) Standard deviations of BAT noise for acetaldehyde. (c) Standard deviations of BAT noise for 2-methylpyridine. We can see that the noise scale can distinguish different bond types and atom types, resulting in better molecular modeling.\nWe have added the examples in appendix C.1 in the new version.\n\n>Weakness 3: Missing parenthesis in the second exponential term of Eq. 8.\n\nThank you for the kind reminder, and we have revised it in the new version.\n\n>Weakness 4: The superiority of the SliDe can be strengthened by more downstream experiments, especially energy prediction, for example, on the ANI-1x dataset.\n\nWe add an experiment on ANI-1x energy prediction as below. \nThe result is compared with baseline nonequilibrium denoising (Noneq) [1]  that performs coordinate denoising as Coord (Zaidi et al., 2022), while Noneq is pre-trained on ANI-1 and ANI-1x that contain nonequilibrium structures. Noneq also adopts TorchMD-NET as the backbone model.\nSliDe performs better in both pre-train improvement and without pre-train setting, indicating the superiority of SliDe pre-training method over nonequilibrim coordiante denoising as well as the improved backbone model GET over TorchMD-NET.\n| ANI-1x  energy prediction (MAE, kcal/mol) | Noneq | SliDe |\n| --- | --- | --- |\n|  w/o pre-train | 1.50 | 1.362 |\n|  pre-train | 1.01 | 0.786 |\n|  pre-train improvement | 32.7% | 42.3%  |\n\n[1] Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials. J. Chem. Theory Comput. 2023.\n\n>Question 1.1: \"What does DFT label supervised mean?\"  Isn\u2019t \u201cTraining from Scratch\u201d also supervised? The authors should elaborate.\n\n\"DFT label supervised\" means pre-training by learning the DFT force labels. Besides, your understanding of \u201cTraining from Scratch\u201d, \"Coord\" and \"Frad\" is exactly right. We add more explanations in appendix B.2 in the revised version.\n\n>Question 1.2:The unit of the prediction MAE should also be included in the table.\n\nThank you for the useful suggestion, and we have added the unit in the table."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409961618,
                "cdate": 1700409961618,
                "tmdate": 1700409961618,
                "mdate": 1700409961618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nCd8HPADOY",
                "forum": "liKkG1zcWq",
                "replyto": "SKYKOmF21e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We have carefully considered your valuable suggestions and have made necessary revisions. The revised content is shown in blue text in the latest version of the paper, which can be found in: https://anonymous.4open.science/r/SliDeRebuttal-EEC9/SLIDE_ICLR2024.pdf\n\nIf you have any further questions and feedbacks, please feel free to contact us. If our answer has successfully resolved your issue, we kindly request that you consider raising the rating. Thanks!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410205475,
                "cdate": 1700410205475,
                "tmdate": 1700410205475,
                "mdate": 1700410205475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EMQJOoMPFt",
                "forum": "liKkG1zcWq",
                "replyto": "BgVAEMVFkN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Reviewer_nTFT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Reviewer_nTFT"
                ],
                "content": {
                    "title": {
                        "value": "Response to revision"
                    },
                    "comment": {
                        "value": "Thanks you for answering my questions and concerns point-by-point. I do believe that the revision has elevated the clarity and quality of the manuscript. I'll raise the score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581988447,
                "cdate": 1700581988447,
                "tmdate": 1700581988447,
                "mdate": 1700581988447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xUhOZhPhtk",
            "forum": "liKkG1zcWq",
            "replyto": "liKkG1zcWq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3064/Reviewer_j2Vc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3064/Reviewer_j2Vc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new pre-training method, Slide, that is based on intramolecular potential theory. To lower the computational expense of Slide, the authors introduce a random slicing approach. In addition, a new MLFF architecture is introduced (GET)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strength 1: The writing and motivation of this paper is very clear, with a good description of related works.\n\nStrength 2: Molecular pre-training is an important problem which the authors propose a novel approach for."
                },
                "weaknesses": {
                    "value": "Weakness 1 (Major): The baselines used in this paper are not up to date. In fact, Gemnet (2021) and Nequip (2022) can outperform Slide on basically all of the MD17 and require no pre-training. In addition, the authors claim they set a new state of the art on these benchmarks, which is incorrect.\n\nWeakness 2 (Major): For the downstream task only one random seed is used and the gains over other methods are relatively minor (i.e. compared to Coord and Frad). This makes me doubt that the results are really significant or if they are just due to tuning. I think that multiple random seeds should be reported.\n\nWeakness 3 (Minor): Showing the best result over random seeds in table 1 is kind of strange. I think that the mean result should be shown."
                },
                "questions": {
                    "value": "Question 1: How does your slicing method related to sliced score matching [1]?\n\n[1] Song, Yang, et al. \"Sliced score matching: A scalable approach to density and score estimation.\" Uncertainty in Artificial Intelligence. PMLR, 2020.\n\nQuestion 2: How does the scale of the pre-training data effect performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3064/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3064/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3064/Reviewer_j2Vc"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784202869,
            "cdate": 1698784202869,
            "tmdate": 1699636251697,
            "mdate": 1699636251697,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QJtFgVMkep",
                "forum": "liKkG1zcWq",
                "replyto": "xUhOZhPhtk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your expert feedback and your insightful concerns. Your concerns and questions are responded as follows:\n\n>Weakness 1:  Gemnet (2021) and Nequip (2022) can outperform Slide on basically all of the MD17 and require no pre-training. The claim of new state of the arts is incorrect.\n\nFirst of all, please note that the results of MD17 in Gemnet and Nequip use a different unit (meV/$\\mathring{\\textnormal{A}}$) than ours (kcal/mol/$\\mathring{\\textnormal{A}}$). After unifying the unit, SliDe's results in paper surpass Gemnet on 6/8 molecules and NequIP((l= 1) on all 8 molecules in MD17, as shown below. \n\nFurthermore, we find the hyperparameter WoFE indicating weight of force over energy in loss functions is important for MD17 dataset[1]. For fair comparisons with Gemnet, we add a setting that uses the same WoFE as Gemnet that notably raises performance. (Due to the time limit, we have completed the test for five molecules so far, and it is enough to show SliDe's competence.)\n\nWe also notice that NequIP reports two versions of results on MD17: l=1 and l=3, where l is the order of spherical harmonics. Recent works[1][2] choose to only compare with NequIP(l=1) because NequIP with high order spherical harmonics is substantially slower[2]. For the same reason,  we compare with NequIP(l=1). \n\nFinally, our main contribution is the pre-training method which is applicable to many backbone models. Therefore, SliDe can actually apply to Gemnet, Nequip and other competitive backbone models.\n| Force prediction MAE (kcal/mol/ $\\mathring{\\textnormal{A}}$) | WoFE | ASP | BEN | ETH | MAL | NAP | SAL | TOL | URA |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Gemnet-T | 1000 | 0.219  | **0.145** | **0.085**  | 0.155  | 0.055  | 0.127 | 0.060 | 0.097  |\n| GemNet-Q | 1000 | $\\underline{0.217}$  | **0.145** | $\\underline{0.088}$  | 0.159  | 0.051  | 0.125  | 0.060 | 0.104  |\n| NequIP((l= 1) | 1000  | 0.348 | 0.187 | 0.208 | 0.337 | 0.097 | 0.238 | 0.101 | 0.173 |\n| SliDe(in paper) | 4 | **0.174**   | $\\underline{0.169} $ | $\\underline{0.088} $ | $\\underline{0.154 }$ | $\\underline{0.048}$ | $\\underline{0.101}$ | $\\underline{0.054}$ | $\\underline{ 0.083 }$  |\n| SliDe | 1000 | - | - | **0.085** | - | **0.043** | **0.095** | **0.049** | **0.079** |\n\n[1] Spherical Message Passing for 3D molecular Graphs, ICLR2022\n\n[2] TorchMD-NET: Equivariant Transformers for Neural Network based Molecular Potentials, ICLR2022\n\n>Weakness 2: I think that multiple random seeds should be reported.\n\nAll the results reported in paper use the same random seed 1. This follows the practice of previous literature[1][2][3], because the performance is quite stable for random seeds on QM9 dataset, as shown in [3][4][5]. However, to further validate the stability and to answer this question, we conduct experiments with three different seeds (seed=0,1,2) on homo and lumo tasks in QM9 and Aspirin force prediction in MD17. The results are given in the table below, showing that the performance is quite stable across these tasks.\n| MAE | homo(meV) | lumo(meV) | ASP(kcal/mol/ $\\mathring{\\textnormal{A}}$) |\n| --- | --- | --- | --- |\n| seed=0 | 14.0 | 12.3 | 0.1714 |\n| seed=1 | 13.6 | 12.3 | 0.1740 |\n| seed=2 | 13.3 | 12.2 | 0.1744 |\n| Mean(Standard deviation) | 13.63(0.35)  | 12.27(0.06) | 0.1733(0.0016) |\n\n\n\n[1]Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, and Wei-Ying Ma. Fractional denoising for\n3D molecular pre-training. \n\n[2]Shengchao Liu, Hongyu Guo, and Jian Tang. Molecular geometry pretraining with SE(3)-invariant\ndenoising distance matching. \n\n[3]Yi Liu, Limei Wang, Meng Liu, Yu-Ching Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spheri\ncal message passing for 3D molecular graphs. \n\n[4]Kristof Sch \u0308utt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction\nof tensorial properties and molecular spectra. \n\n[5]Kristof T Sch \u0308utt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R M \u0308uller.\nSchnet \u2013 a deep learning architecture for molecules and materials. \n\n>Weakness 3: \"Showing the best result over random seeds in table 1 is kind of strange.\"\n\nAs discussed in the response to weakness 2, we do not select the best result over random seeds. All the results in our submission are from seed=1."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489248614,
                "cdate": 1700489248614,
                "tmdate": 1700489248614,
                "mdate": 1700489248614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8htJJIKFK2",
                "forum": "liKkG1zcWq",
                "replyto": "xUhOZhPhtk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Question 1: How does your slicing method related to sliced score matching?\n\nThe similarity is that they are both random slicing method which uses random projection to reduce computational cost. The word \"slicing\" both originates from Sliced Wasserstein distance. The difference is that the slicing technique is applied to different targets. In sliced score matching, the technique is used to reduce the computation of the gradient of the score matrix, while it is used to reduce the computation of the Jacobi matrix of the coordinate transformation function in our paper.\n\n>Question 2: How does the scale of the pre-training data affect performance?\n\nWe randomly extract 10W, 50W, 100W data from PCQM4Mv2 dataset. We pre-train on these subsets and finetune on homo(QM9). The results are provided below. We find more pre-train data contributes to better downstream performance. Meanwhile, SliDe is able to achieve competitive results when data is relatively scarce, such as 50W.\n| Number of pre-training data | homo(MAE,meV) |\n| --- | --- |\n| w/o pretrain | 17.6 |\n| 10W | 16.05 |\n| 50W | 14.53 |\n| 100W | 14.21 |\n| ~300W(whole) | 13.6 |\n\n\n We have carefully considered your valuable suggestions and have made necessary revisions. The revised content is shown in blue text in the latest version of the paper, which can be found in: https://anonymous.4open.science/r/SliDeRebuttal-EEC9/SLIDE_ICLR2024.pdf\n\nIf you have any further questions and feedbacks, please feel free to contact us. If our answer has successfully resolved your issue, we kindly request that you consider raising the rating. Thanks!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489872271,
                "cdate": 1700489872271,
                "tmdate": 1700489872271,
                "mdate": 1700489872271,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r7qpdxkHiz",
                "forum": "liKkG1zcWq",
                "replyto": "1sypZ4lowp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Reviewer_j2Vc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Reviewer_j2Vc"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive response. I still have a few questions.\n\n1. With regards to the weakness 1, the authors' claims that they beat the state of the art should be removed for all molecules where they do no truly beat the state of the art, including Nequip with higher order harmonics. From what I can the tell the authors achieve SOTA on two MD17 molecules: uracil and toluene. I do not think the argument that Nequip should be excluded because it is slower than Slide is really fair, since Slide does pre-training. I do not think that not achieving state of the art is detrimental to this paper, but the authors should correctly report results.\n\n2. Thank you for running more seeds. My concern was more regarding whether the improvements over coord and frad on MD17 are statistically significant? It seems like a lot of the gains are very small, so it is hard to tell if these results happen because slide has more tuning. I am not sure if displaying the average MAE across molecules would make the gain of slide clearer. If Slide does achieve significant gains over Coord and Frad, I think that the authors should include more discussion on why it does not."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619583116,
                "cdate": 1700619583116,
                "tmdate": 1700619583116,
                "mdate": 1700619583116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OVKjUEnThM",
            "forum": "liKkG1zcWq",
            "replyto": "liKkG1zcWq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3064/Reviewer_mZU2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3064/Reviewer_mZU2"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel approach to molecular pre-training called Sliced Denoising (SliDe) that leverages physical principles to improve molecular property prediction. The authors introduce a new noise distribution strategy that improves sampling over conformations and a denoising task that learns the force field of the energy function. They evaluate SliDe on benchmark datasets QM9 and MD17 and show that it outperforms traditional baselines in terms of physical consistency and molecular property prediction accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main contribution of the paper, the pre-training algorithm, seems to be extremely relevant to the drug discovery domain with the magnitude of improvement achieved across all benchmark tasks. The paper could be of great interest to scientists in this area. \n\nThe pre-training method introduced leverages physical principles and is more interpretable. In addition, the experimental results seem very thorough."
                },
                "weaknesses": {
                    "value": "While the paper is interesting, and makes an important contribution to the field of drug discovery, I would like to raise the question of if ICLR is the correct venue for this submission. This is an important area, and there will be a subset of audience interested in the field, but I would assume that a broader audience will have trouble understanding the paper due to the about of domain knowledge involved. I will leave it to the AC to determine this.\n\nI found the paper hard to read and understand due to the amount of domain knowledge involved. I understand that it is not possible to introduce all the background information in 8 pages, but I would urge the authors to rewrite the paper in a more accessible way for non-domain but ML experts."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851983004,
            "cdate": 1698851983004,
            "tmdate": 1699636251630,
            "mdate": 1699636251630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K1WGeoDIoZ",
                "forum": "liKkG1zcWq",
                "replyto": "OVKjUEnThM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">I would like to raise the question of if ICLR is the correct venue for this submission. I would assume that a broader audience will have trouble understanding the paper due to the amount of domain knowledge involved.\n\nResponse: We believe our paper is very suitable for ICLR for the following reasons.\n- First of all, AI for drug discovery has become an important area in machine learning. To our knowledge, there are around 41 papers about molecules and proteins accepted by ICLR2023, accounting for 2.9% of the total accepted papers. It is in the same scale as federated learning, an important branch of the machine learning field, having 44 papers accepted. Besides, there are around 130 papers about molecules or proteins submitted to ICLR2024. \n- Additionally, many of our pre-training and property prediction baselines are ICLR conference papers: Coord(ICLR2023), SE(3)-DDM(ICLR2023), Uni-mol(ICLR2023), Transformer-M(ICLR2023), SphereNet(ICLR2022), ET(ICLR2022), DimeNet(ICLR2020). \n- Finally, our paper is closely relevant to the topics of \"self-supervised representation learning\" and \"applications to physical sciences (physics, chemistry, biology, etc.)\"  among the Subject Areas of ICLR2024. \n\nFor the convenience of a broader audience, we provide a supplementary introduction to the involved background knowledge in Appendix E in the new version, which can be found at: https://anonymous.4open.science/r/SliDeRebuttal-EEC9/SLIDE_ICLR2024.pdf\n\nIf you have any further questions and feedbacks, please feel free to contact us. If our answer has successfully resolved your issue, we kindly request that you consider raising the rating. Thanks!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485874906,
                "cdate": 1700485874906,
                "tmdate": 1700485874906,
                "mdate": 1700485874906,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F7kwnH2Vc7",
            "forum": "liKkG1zcWq",
            "replyto": "liKkG1zcWq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3064/Reviewer_ft62"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3064/Reviewer_ft62"
            ],
            "content": {
                "summary": {
                    "value": "the paper introduces \"sliced denoising\" (slide), a novel molecular pre-training method that enhances the physical interpretation of molecular representation learning. traditional denoising methods, though physically interpretable, can suffer from inaccuracies due to ad-hoc noise design, leading to imprecise force fields. slide addresses this by utilizing classical mechanical intramolecular potential theory, leading to a 42% improvement in force field estimation accuracy over existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. innovative approach: slide introduces an innovative noise strategy (bat noise) and a random slicing technique. this approach significantly enhances the accuracy of force field estimations, making it a pioneering method in the field.\n\n1. alignment with physical principles: the method closely aligns with classical mechanical intramolecular potential theory. it appears to improve the realism of molecular representations as well as help that learned representations are physically interpretable, a critical aspect in molecular sciences.\n\n1. empirical validation: slide demonstrates empirically strong results in force field estimation accuracy and downstream task performance on benchmark datasets qm9 and md17.\n\n1. methodology: the paper combines theoretical soundness with methodological innovations effectively. the use of a quadratic energy function approximation and the consequent noise strategy is interesting.\n\n1. network architecture integration: integrating a transformer-based network architecture that encodes relative coordinate information is a notable strength. this architectural choice complements the novel denoising method, enhancing its adaptability to other works using transformer backbones."
                },
                "weaknesses": {
                    "value": "1. computational complexity: while the random slicing technique addresses computational challenges associated with jacobian matrix estimation, the overall computational demand and efficiency, especially in large-scale applications, are not comprehensively addressed\u200b\u200b\u200b\u200b\u200b\u200b.\n\n1. robustness to noisy data: the robustness of slide to noisy or imperfect real-world data is not thoroughly examined. this aspect is crucial for practical applications where data quality can vary significantly\u200b\u200b."
                },
                "questions": {
                    "value": "1. regarding computational efficiency: can the authors provide more details on the computational requirements of slide, especially when applied to large molecular datasets? how does its computational efficiency compare to existing methods?\n\n2. on generalizability and applicability: what are low-hanging fruits to test the generalizability of slide to other types of geometric data or applications beyond molecular science? how might the method need to be adapted for such scenarios?\n\n3. empirical validation across diverse datasets: could the authors elaborate on potential plans to validate slide on a broader range of datasets, particularly those that may present different challenges than qm9 and md17, such as des15k or oc20 as in the coord paper https://arxiv.org/abs/2206.00133?\n\na curiosity question:\n1. dependence on equilibrium structures: the method's reliance on equilibrium structures, to be clear same as most other methods in this space, for training may limit its effectiveness in scenarios where such structures are not readily available or accurate. are there ways to advance molecular representation learning in such a setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699618507506,
            "cdate": 1699618507506,
            "tmdate": 1699636251559,
            "mdate": 1699636251559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ezNTGvsH13",
                "forum": "liKkG1zcWq",
                "replyto": "F7kwnH2Vc7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your expert feedback and your insightful concerns. Your concerns and questions are responded as follows:\n\n>Weakness 1 & question 1: (1)Can the authors provide more details on the computational requirements of slide, especially when applied to large molecular datasets? \n\nPlease note that all the terms in the loss function (eq.17)  except for $GNN_\\theta(x)$, are calculated in data preprocessing. As the deep learning package (specifically PyTorch in this case) generates multiple workers in the DataLoader through parallel processes, each responsible for loading and processing individual data samples before adding the processed batches to a queue, this approach ensures that the time-consuming training process remains unimpeded, as long as the queue is consistently filled with batches. Therefore, it will not become the bottleneck of the training process, as $GNN_\\theta(x)$ is what takes most of the time.\n\nThe computational complexity of the regression target for one molecule is $O(N_v\\times\\max$ { N,M,F }) for SliDe, where N is the number of atoms, M is the dimension of relative coordinates (total number of bond lengths, bond angles, and bond torsions), F is the complexity of coordinate transformation between Cartesian coordinates and relative coordinates. \n\nTwo factors affecting the complexity is $N_v$ and $F$. As for coordinate transformation, the complexity $F=O(\\max$ { $N,M$ })[1], which is executed efficiently by RDKit. As for the $N_v$ times sampling procedure, in fact, for every $i\\in\\{1,\\cdots,N_v\\}$, the regression target can be calculated in parallel. Therefore the time complexity above can be further reduced to $O(\\max${N,M,F}).  This makes pre-training on large molecular datasets possible. \n\n[1]On Updating Torsion Angles of Molecular Conformations, Journal of Chemical Information and Modeling 2005\n\n>(2)How does its computational efficiency compare to existing methods?\n\nAs for comparison to existing method, SliDe and Frad are theoretically in the same scale. The computational complexity of the regression target for one molecule is $O(\\max${ $M_{rb},N,F_{rb}$ }) for Frad, where $M_{rb}$ is the number of rotatable bonds and $F_{rb}$ is the coordinate transformation between Cartesian coordiantes and diheral angles of rotatable bonds. It is in the same scale to SliDe's $O(\\max${N,M,F}).\n\nIn practise, although we do not process all $N_v$ targets in parallel and incorporate edge update in network architecture, the training time of Frad and SliDe does not vary significantly. Frad pre-training takes 1d1h14m on 8 NVIDIA A100 GPU, and SliDe pre-training takes 1d17h1m on 8 Tesla V100 GPU.\n\n>Question2: Generalizability and applicability to other types of data. \n\nExcept for small molecules, our SliDe can potentially be used in pre-training for protein and material. To be specific, SliDe can replace coordinate denoising in existing protein pre-training method[1][2] and replace force label prediction in material pre-training method[3]. The parameters in the energy function should be adapted for these scenarios and they can be obtained from AMBER Force Field[4].\nAs a consequence, SliDe can expand its applications in a series of downstream tasks such as binding affinity prediction, structure-based virtual screening, and material property prediction.\n\n[1] Uni-mol: A universal 3D molecular representation learning framework, ICLR2023\n\n[2] General-purpose Pre-trained Model Towards Cross-domain Molecule Learning, submitted to ICLR2024\n\n[3]From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction, submitted to ICLR2024\n\n[4]https://ambermd.org/AmberModels.php"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408269568,
                "cdate": 1700408269568,
                "tmdate": 1700408269568,
                "mdate": 1700408269568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KxPdTBVgXD",
                "forum": "liKkG1zcWq",
                "replyto": "F7kwnH2Vc7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Question 3: Elaborate on potential plans to validate slide on a broader range of datasets that present different challenges than qm9 and md17.\n\nWe plan to evaluate SliDe on larger number and size of molecules. To this end, we choose to test SliDe on MD22[2] and ANI-1x[3] datasets, whose comparison to MD17 and QM9 is shown below. ANI-1x provides large numbers of molecules with multiple equilibrium and nonequilibrium conformations. MD22 contains larger and diversified molecules than QM9 and MD17, including protein, lipids, carbohydrates, nucleic acids and supramolecules.\n\n| Dataset | Molecule size | Number of molecules  | Number of conformations |\n| --- | --- | --- | --- |\n| QM9    | \u223c18 (3-29)  | 133,885 | 133,885 |\n| MD17    | \u223c13 (9-21)  | 8 | 3611115  |\n| MD22 | \u223c67 (42-370) | 7 | 223422 |\n| ANI-1x | \u223c 15 (4-63) | 63865 | 5496771 |\n\n- Baseline: The result is compared with baseline nonequilibrium denoising (Noneq) [1]  that performs coordinate denoising as Coord (Zaidi et al., 2022) in our paper, while Noneq is pre-trained on ANI-1 and ANI-1x that contain nonequilibrium structures. Noneq also adopts TorchMD-NET as the backbone model.\n- ANI-1x:  The result of experiment on ANI-1x energy prediction is shown below. SliDe performs better in both pre-train improvement and without pre-train setting, indicating the superiority of SliDe pre-training method over nonequilibrim coordiante denoising as well as the improved backbone model GET over TorchMD-NET.\n| ANI-1x  energy prediction (MAE, kcal/mol) | Noneq | SliDe |\n| --- | --- | --- |\n|  w/o pre-train | 1.50 | 1.362 |\n|  pre-train | 1.01 | 0.786 |\n|  pre-train improvement | 32.7% | 42.3%  |\n\n- MD22: Due to the time limit, we only test on nucleic acids, consisting of ATAT(60 atoms) and ATATCGCG(118 atoms). The data split follows [1] (train:validation:test=8:1:1). Since Noneq focus on energy prediction and does not report force prediction result, we reevaluate it with both energy and force tasks as a fair comparison. The results below demostrate SliDe is consistently effective for molecules with varied type and size, indicating the ability to apply to broader range of molecules.\n| MD22 MAE (kcal/mol/$\\mathring{\\textnormal{A}}$)  | ATAT(60 atoms)  |  | ATATCGCG(118 atoms)  |  |\n| --- | --- | --- | --- | --- |\n|  | Force | Energy | Force | Energy |\n| Noneq w/o pre-train | 0.2744 | 0.1098 | 0.7581 | 0.2993 |\n| Noneq  w/ pre-train | 0.2194 | 0.0776 | 0.6574 | 0.2764 |\n| SliDe w/o pre-train | 0.0700 | 0.1021 | 0.0873 | 0.1049 |\n| SliDe w/ pre-train | 0.0444 | 0.0872 | 0.0596 | 0.0904 |\n\n[1] Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials. J. Chem. Theory Comput. 2023.\n\n[2]  Accurate global machine learning force fields for molecules with hundreds of atoms, Science Advances, 2023.\n\n[3] The ANI-1ccx and ANI-1x datasets, coupled-cluster and density functional theory properties for\nmolecules. Scientific data, 2020.\n\n\n>Weakness 2 & Curiosity question: (1) Dependency on equilibrium structures\n\nWe find pre-training with inaccurate conformation also works. Due to the time limit, we extract subsets of 10W and 50W molecules from PCQM4Mv2 dataset and generate inaccurate conformations by RDKit (the conformation is optimized by MMFFOptimizeMoleculeConfs function provided by RDKit, but still not as accurate as DFT). We pre-train on the dataset we construct and test them on homo(QM9), as shown below. \n| homo(MAE,meV) | PCQ(DFT) | PCQ(RDKit) |\n| --- | --- | --- |\n| 10W | 16.05 | 16.13 |\n| 50W | 14.53 | 15.39 |\n\nCompared with training from scratch MAE=17.6 meV, pre-training with RDKit data is notably effective. Compared with pre-training with accurate conformations, less accurate conformations compromise the performance. Overall, SliDe is robust to inaccurate conformations, revealing the potential of SliDe in larger scale of pre-training. \n\n>(2) Are there ways to advance molecular representation learning in such a setting?\n\nAlthough this issue is beyond the scope of this article, it is an academic issue that we believe is worth exploring. We deem the force and energy labels of the nonequilibrium conformation may be helpful to model the local energy function landscape and design a reasonable pre-training task[4][5]. But the specific usage of labels and task design still needs more research.\n\n[4] May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations, Neurips 2023\n\n[5] Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields, submitted to ICLR2024"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409263249,
                "cdate": 1700409263249,
                "tmdate": 1700409263249,
                "mdate": 1700409263249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vr5MSPCk7S",
                "forum": "liKkG1zcWq",
                "replyto": "F7kwnH2Vc7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We have carefully considered your valuable suggestions and have made necessary revisions. The revised content is shown in blue text in the latest version of the paper, which can be found in: https://anonymous.4open.science/r/SliDeRebuttal-EEC9/SLIDE_ICLR2024.pdf\n\nIf you have any further questions and feedbacks, please feel free to contact us. Thanks!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410300846,
                "cdate": 1700410300846,
                "tmdate": 1700410300846,
                "mdate": 1700410300846,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fwzZ5vzYKO",
                "forum": "liKkG1zcWq",
                "replyto": "Vr5MSPCk7S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3064/Reviewer_ft62"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3064/Reviewer_ft62"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal reply"
                    },
                    "comment": {
                        "value": "dear authors,\n\nthank you very much for your extensive answers!\n\ni maintain my rating of 8 (accept) and want to congratulate you on an interesting paper.\n\nin good spirits,\n\nreviewer  ft62"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587668427,
                "cdate": 1700587668427,
                "tmdate": 1700587668427,
                "mdate": 1700587668427,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]