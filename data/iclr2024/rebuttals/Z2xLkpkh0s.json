[
    {
        "title": "FARSE-CNN: Fully Asynchronous, Recurrent and Sparse Event-Based CNN"
    },
    {
        "review": {
            "id": "zfrhSqniA7",
            "forum": "Z2xLkpkh0s",
            "replyto": "Z2xLkpkh0s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5174/Reviewer_WJKv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5174/Reviewer_WJKv"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a model for efficient asynchronous event processing that exploits sparsity. They design the Fully Asynchronous, Recurrent and Sparse Event-Based CNN (FARSE-CNN), a novel multi-layered architecture which combines the mechanisms of recurrent and convolutional neural networks. To build efficient deep networks, they propose compression modules that allow to learn hierarchical features both in space and time."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1: The authors try to present an inherently spiking/event domain based approach for processing asynchronous data from event-like sensors. Often in the related literature you see efforts at converting the data to frame like representations in order to process it using traditional algorithms developed for the synchronous domain. This approach tries to deal with the problem of processing the raw data directly without performing this limiting transformation which could hinder latency. As a result this is the main strength of the paper in my opinion\n\nS2: overall the paper is well written and the algorithm seems interesting, so i think it will be of interest to a subset of the community interested in on the edge based processing approaches"
                },
                "weaknesses": {
                    "value": "W1: it is not clear to me if source code will be provided. Please clarify\n\nW2: I would have liked to see a more thorough discussion on the number of events produced internally by this architecture. To run something like this on neuromorphic hardware efficiently, you need to ensure that a sparse number of events are created internally. A discussion on this in the paper, would improve it"
                },
                "questions": {
                    "value": "See comments above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698333365589,
            "cdate": 1698333365589,
            "tmdate": 1699636513226,
            "mdate": 1699636513226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mLHedX3L8L",
                "forum": "Z2xLkpkh0s",
                "replyto": "zfrhSqniA7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for pointing out the necessity of an analysis of the number of internal events. This will be included in the updated manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231830927,
                "cdate": 1700231830927,
                "tmdate": 1700231830927,
                "mdate": 1700231830927,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hQq13pfjaq",
                "forum": "Z2xLkpkh0s",
                "replyto": "mLHedX3L8L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5174/Reviewer_WJKv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5174/Reviewer_WJKv"
                ],
                "content": {
                    "comment": {
                        "value": "I have gone through the reviewers' and authors' comments. As I indicated in my review, I appreciate the effort of the authors to present an algorithm that does not entail converting events to frames, which is not the case in most papers I see on event based datasets. As a result I have decided to keep my rating unchanged"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594620981,
                "cdate": 1700594620981,
                "tmdate": 1700594620981,
                "mdate": 1700594620981,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JqnoGjZsFp",
            "forum": "Z2xLkpkh0s",
            "replyto": "Z2xLkpkh0s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5174/Reviewer_Dqva"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5174/Reviewer_Dqva"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the asynchronous processing of individual event data without converting them into image-like inputs, thereby significantly reducing the overall model energy consumption. The paper designs and implements modules such as FARSE-CNN, SUB-FARSE-CNN, Sparse Pooling, Temporal Dropout, and validates the model's effectiveness in tasks like recognition and detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Based on LSTM design, the paper has implemented a FARSE-CNN for event data, with Sub-FARSE-CNN specifically producing outputs for the central pixel of each cell and updating the state of each cell, addressing the issue of a sharp increase in the number of events after passing through the module.\n\n2. Sparse Pooling compresses event data in the spatial dimension, while Temporal Dropout considers discarding some data in the temporal dimension to encourage the model to learn long-term features, fully utilizing the spatiotemporal characteristics of events.\n\n3. In the tasks of object recognition and object detection, the paper validates the role of the proposed modules in the network, showing their ability to balance computational complexity and accuracy, achieving performance comparable to or better than previous methods. It also achieves performance similar to synchronous methods in gesture recognition tasks."
                },
                "weaknesses": {
                    "value": "1. Although the asynchronous method proposed in the paper handles event data, it does not demonstrate the real-time performance and execution speed of this method. Is there any data available regarding this aspect?\n\n2. For the Temporal Dropout discussed in Contribution 2 and the \"l\" parameter mentioned in Section 3.5, the experimental section does not provide relevant configurations or discussions.\n\n3. While the paper mentions both FARSE-CNN and SUB-FARSE-CNN, with the latter being an optimized improvement of the former, there is no experimental data to prove the effectiveness of this optimization. For example, there is no performance or computational complexity comparison."
                },
                "questions": {
                    "value": "see Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698694731169,
            "cdate": 1698694731169,
            "tmdate": 1699636513137,
            "mdate": 1699636513137,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O8iezO11np",
                "forum": "Z2xLkpkh0s",
                "replyto": "JqnoGjZsFp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding the effectiveness of our optimizations, Temporal Dropout and Submanifold FARSE-Convolutions. In Table 1 we have shown that, by changing the window size parameter of Temporal Dropout from 2 to 4, we can reduce complexity by 4 times at the cost of 1% accuracy loss on the NCars dataset. In Figure 4 we also compare the complexity of our network with one that does not use Temporal Dropout, and one that uses normal FARSE-Convolutions instead of Submanifold ones. The complexity growth in those cases (_e.g._, about 100x when using normal FARSE-Convolutions) makes it impractical to train a network due to time and memory constraints since this complexity is directly related to the number of internal events that are generated, hence why we don\u2019t show performance comparisons."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231625312,
                "cdate": 1700231625312,
                "tmdate": 1700231625312,
                "mdate": 1700231625312,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z8E7piugRi",
            "forum": "Z2xLkpkh0s",
            "replyto": "Z2xLkpkh0s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5174/Reviewer_bxjc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5174/Reviewer_bxjc"
            ],
            "content": {
                "summary": {
                    "value": "A new deep learning architecture - RNN for processing event data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors show similar or better performance at higher computation efficiency than other approaches that uses asynchronous methods."
                },
                "weaknesses": {
                    "value": "- How does the complexity of the architecture affect the implementation? Does this architecture of asynchrony give actual speedup?\n- Are the datasets shown here sufficient? I am aware of a few other event vision work that looks at some other event data-streams. Can the authors do more SOTA comparisons ?\n- Temporal dropout while interesting seems to be an already existing technique? [1] uses some dynamic temporal exit. Further, there are some temporal coding works [2] that use some interesting forms of temporal representation. Can the authors comment on how dropout is different from these?\n\n[1] Li, Yuhang, et al. \"SEENN: Towards Temporal Spiking Early-Exit Neural Networks.\" arXiv preprint arXiv:2304.01230 (2023).\n[2] Zhou, Shibo, et al. \"Temporal-coded deep spiking neural network with easy training and robust performance.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 12. 2021."
                },
                "questions": {
                    "value": "See above weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780836364,
            "cdate": 1698780836364,
            "tmdate": 1699636513051,
            "mdate": 1699636513051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WG6TFfa9nJ",
                "forum": "Z2xLkpkh0s",
                "replyto": "z8E7piugRi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "In our evaluation, we chose datasets that have been used by previous asynchronous methods to allow for a direct comparison. We believe that additional SOTA comparisons are unfortunately not feasible at this time due to the lack of public code for the baselines. To the best of our knowledge, no public implementation is available for the strongest competitor, EventFormer, as the official project repository is currently empty (https://github.com/udaykamal20/EventFormer), while only the test/inference code is available for AEGNN, and not the training code.\n\nThe reviewer noted that there are connections between our Temporal Dropout and previously existing techniques for SNNs. Indeed, the purpose of Temporal Dropout is to introduce a mechanism resemblant of spiking networks in our model, and guarantee that intermediate activations of the network are sparse temporally, as well as spatially. Without this mechanism, our architecture would propagate all events to the output.  As mentioned in our Conclusions, we believe that more sophisticated techniques to exploit temporal structure could be investigated, including Reinforcement Learning approaches as done in SEENN (Li et al., 2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231471033,
                "cdate": 1700231471033,
                "tmdate": 1700231471033,
                "mdate": 1700231471033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nekd84tCMo",
            "forum": "Z2xLkpkh0s",
            "replyto": "Z2xLkpkh0s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5174/Reviewer_oYpd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5174/Reviewer_oYpd"
            ],
            "content": {
                "summary": {
                    "value": "The paper intorduces an RNN architecture that is tailored to event-based processing in asynchronous manner. The architecture is evaluated against several other methods on 3 event-based datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is written clearly, and the illustrations support the text well. I believe the problem that is being addressed in the paper is important, as the authors mentioned, many modern event-based camera methods rely on image-like representations and thus are suboptiomal for event data processing."
                },
                "weaknesses": {
                    "value": "1) Abstract: It would be better to clarify or paraphrase, as these two sentences seem to contradict each other: \"most successful algorithms for event cameras convert batches of events into dense image-like representations\" and \"achieves similar or better performance than state-of-the-art\nasynchronous methods\". What was the goal of the paper - to beat the best methods or do develop a sota asynchronous pipeline? I assume the latter, but this needs to be stated more clearly in the abstract.\n\n2) It would help if the evaluation was expanded, since there are not so many event-based datasets available. E.g. CIFAR10-DVS and SL-Animals could be added. A more complex EV-IMO (https://better-flow.github.io/evimo/download_evimo_2.html) could strengthen the paper further.\n\n3) It would be also great to see the performance of the method measured (train / inference separately) on a modern computer or embedded platform. Theoretical computations are valuable, but in practice there are many factors besides flops that can affect the performance. A side-by-side comparison of a few methods would make it more clear to the reader what the implications of the architecture are.\n\n4) From table 1, it seems that the accuracy is not the best (or significantly better compared to competition). The compute cost seems not the lowest as well. I believe a better explanation should be provided to explain the results."
                },
                "questions": {
                    "value": "1) The authors mention, in the introduction, 3D convolutional networks. What is the main difference / advantage of the presented asynchronous scheme compared to 3D cnns, given that both leverage temporal information and, in theory, could be ran event-by-event? An example paper that explores this: https://openaccess.thecvf.com/content_CVPR_2020/papers/Mitrokhin_Learning_Visual_Motion_Segmentation_Using_Event_Surfaces_CVPR_2020_paper.pdf - it would be beneficial to add it to the review section as well.\n\n2) Are there plans to release the source code as a (e.g. Pytorch) package? I believe this would add to the overall contribution of this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699141440703,
            "cdate": 1699141440703,
            "tmdate": 1699636512949,
            "mdate": 1699636512949,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MiwYxtfz24",
                "forum": "Z2xLkpkh0s",
                "replyto": "Nekd84tCMo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5174/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "About the confusing sentence in the abstract, the goal of the paper is indeed to develop a SOTA asynchronous pipeline. Therefore, the meaning we intended for our claim is \"achieves similar or better performance than the state-of-the-art *among* asynchronous methods\" *only*.  We will clarify this.\n\nIn our evaluation, we focused on datasets used by previous asynchronous methods to allow a comparison with those works. We agree that it will be important to validate our method also on more complex tasks and datasets, such as EV-IMO for motion segmentation and structure from motion. Since this involves non-trivial implementation and would warrant some additional discussion, we left it for future work.\n\nRegarding the performance of our method compared to competitors. It is true that, although our method achieves among the best trade-offs between accuracy and complexity, it is surpassed by EventFormer on some benchmarks. We already acknowledged this in Section 4.4, where we also provide a brief discussion of these results. EventFormer uses a very light, non-hierarchical architecture augmented with an associative memory that is able to achieve good performance on object recognition, particularly so in the semi-realistic scenario of N-Caltech101 that does not present challenging motion-rich information. On the other hand, we emphasize that our FARSE-CNN is easily adapted to the more complex task of object detection on real data, where it achieves the best results in both accuracy and complexity (especially given the revised complexity numbers of AEGNN). Finally, we believe that different methods could prove more effective in different tasks due to their inherent features, and so investigating diverse approaches is beneficial for a field that is still at its early stages, like event vision.\n\nOn the differences with 3D convolutional networks. While 3D convolutions can indeed leverage temporal information, standard 3D CNNs require dense inputs, which introduce a lot of redundant computation to the sparse activations of event cameras. For this reason, a line of work focuses on 3D convolutions on sparse graphs that can be updated in event-by-event fashion with less computational burden. The work by Mitrokhin et al. (2020), referenced in the review, also belongs to this family, together with the Graph Neural Network baselines that we considered (NVS-S, EVS-S, AEGNN). We thank the reviewer for pointing us to it, and we will include a reference in the updated manuscript. Differently from these methods, our approach uses a naturally recurrent architecture that can memorize past events in its state, instead of using fixed, explicit time windows. A similar observation, on the difference with LSTM-like approaches, is also made in Mitrokhin et al. (2020) in Section 6."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231152844,
                "cdate": 1700231152844,
                "tmdate": 1700231152844,
                "mdate": 1700231152844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]