[
    {
        "title": "Object-Centric Semantic Vector Quantization"
    },
    {
        "review": {
            "id": "cGsakfg5bP",
            "forum": "HYyRwm367m",
            "replyto": "HYyRwm367m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7823/Reviewer_41Cm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7823/Reviewer_41Cm"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a modification of VQ-VAE called Semantic Vector-Quantized Variational Autoencoder (SVQ), which provides semantic discrete object representation by learning block-level factors, leveraging advances in object-centric learning. Different from patch-based VAE models, SVQ first applies slot attention to obtain object-level codebooks, then splits the slots into blocks following a binding mechanism to represent semantic factors. In comparison with previous methods, the model demonstrates better image sampling quality considering object fidelity. The proposed representation also shows promising performance on several downstream tasks in the out-of-distribution setting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\uff081\uff09The paper is overall well-written and the idea of learning semantic representation is well-motivated.\n\uff082\uff09Extensive and thorough experiments have been carried out to verify the effectiveness of the approach on multiple datasets and settings."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\uff081\uff09\tNovelty: The major limitation of this paper is the lack of theoretical novelty, since it seems the idea of disentangling object-level slots into blocked-level factors as well as the autoregressive transformer decoder for image reconstruction have been introduced in prior work SysBinder. It\u2019s not elaborated on how the proposed approach is developed further in representing discrete semantics, especially when there\u2019s not a large margin between the performance of SVG and other baselines.\n\uff082\uff09\tSemantic illustration: Throughout the paper (including the name of the proposed model), the authors emphasize that the model is capable of learning semantic representation of objects such as shapes, colors, etc. However, how and what semantics are actually learned with unsupervised learning in the codebook is never illustrated or visualized, which makes the statement less convincing.\n\uff083\uff09\tExperiment: First, the specification of multiple hyper-parameters (e.g. number of blocks, dimension of vectors) of SVG and other implementation details is never found. Second, the artificial designs of the constraints of the scene (metric Generation Accuracy) as well as the odd-one-out downstream task are not explained. Third, as the authors mentioned in the section of Limitations, the evaluations are restricted within synthetic datasets with simple scenarios. It\u2019s unclear whether the proposed method will work in realistic, complex datasets, especially when the representation is object-related whose effectiveness is likely to be related to the size of the codebook."
                },
                "questions": {
                    "value": "My questions are listed in weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Reviewer_41Cm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758179288,
            "cdate": 1698758179288,
            "tmdate": 1700724280063,
            "mdate": 1700724280063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wGsWMF2p0B",
                "forum": "HYyRwm367m",
                "replyto": "cGsakfg5bP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 41Cm"
                    },
                    "comment": {
                        "value": "Thank you for the review! We kindly refer the reviewer to our supplementary material which we believe should clarify several of your questions.\n\n> Novelty: The major limitation of this paper is the lack of theoretical novelty, since it seems the idea of disentangling object-level slots into blocked-level factors as well as the autoregressive transformer decoder for image reconstruction have been introduced in prior work SysBinder. It\u2019s not elaborated on how the proposed approach is developed further in representing discrete semantics, especially when there\u2019s not a large margin between the performance of SVG and other baselines.\n> \n\nWhile our work builds on the foundation of SysBinder, it introduces significant and crucial extensions. Unlike SysBinder, which lacks the abilities to model prior distributions and perform sampling, we observe that by discretizing the block representations, we can train a novel semantic prior transformer. This enables us to achieve the **first semantic and composable modeling of representations and distributions from raw images**, which can be seen as an advancement towards learning a semantic language of visual scenes, similar to words in text.\n\nWe want to clarify that while SysBinder and SVQ both use a transformer decoder for reconstruction, SVQ additionally is able to train a separate autoregressive transformer decoder as the Semantic Prior on top of the discrete latents (Section 3.1 and Figure 2b), allowing for sampling of new scenes. **This ability is not supported by SysBinder and is a major contribution of our work and a key motivation for obtaining discrete latents.** Our generated samples are shown to be better than several baselines in terms of quantitative metrics (FID and generation accuracy) as well as qualitatively as shown in Figures 3 and 4 and discussed in Section 5.1.\n\n> Semantic illustration: Throughout the paper (including the name of the proposed model), the authors emphasize that the model is capable of learning semantic representation of objects such as shapes, colors, etc. However, how and what semantics are actually learned with unsupervised learning in the codebook is never illustrated or visualized, which makes the statement less convincing.\n> \n\n**We actually provide this result** in the section A.1 of the Appendix in the supplementary material where we analyze the codebook for a sample scene (\u201dLatent Traversal\u201d section) and show the representations captured in the codebook (\u201dBlock Analysis\u201d).\n\n> First, the specification of multiple hyper-parameters (e.g. number of blocks, dimension of vectors) of SVG and other implementation details is never found.\n> \n\nWe also already provide the hyperparameters and additional training and implementation details of our experiments in section B in the Appendix in the supplementary material.\n\n> Second, the artificial designs of the constraints of the scene (metric Generation Accuracy) as well as the odd-one-out downstream task are not explained.\n> \n\nIn the beginning of Section 5 in the Dataset section, we describe the 2D Sprites dataset:\n\n\u201dIn the 2D Sprites datasets, objects of varying shapes and colors are placed in a scene. In total, there are 7 possible colors and 12 possible shapes. In each image, one object has a single property that is unique from the other objects. All other properties are shared by at least two objects. This structure allows us to evaluate if the prior correctly models the dependencies between the properties of the scene.\u201d\n\nWe then describe at the beginning of section 5.1.1 the Generation Accuracy metric:\n\n\u201dWe additionally calculate generation accuracy by manually inspecting 128 images per model to check if the generated images follow the constraints of the dataset. That is, each image must have exactly one object that has a unique property. All other properties in the scene will have at least one duplicate among the other objects.\u201d\n\nFor the downstream task, we describe in section 5.2.1:\n\n\u201dWe modify the dataset by first dividing each image into four quadrants and ensuring exactly one object will be in each quadrant. As in our previous experiments, one object has a single property that is unique from the other objects. The goal of the task is to identify the quadrant of the odd-one-out object.\u201d\n\nPlease let us know if there is still any confusion about this metric and task based on our description in the submitted manuscript. We would be glad to revise the text to make this more clear.\n\n> Third, as the authors mentioned in the section of Limitations, the evaluations are restricted within synthetic datasets with simple scenarios. It\u2019s unclear whether the proposed method will work in realistic, complex datasets, especially when the representation is object-related whose effectiveness is likely to be related to the size of the codebook.\n> \n\nPlease see the above common response for a discussion about the synthetic nature of the datasets."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478381713,
                "cdate": 1700478381713,
                "tmdate": 1700478381713,
                "mdate": 1700478381713,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oh7kvvetzn",
                "forum": "HYyRwm367m",
                "replyto": "wGsWMF2p0B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_41Cm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_41Cm"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "Thanks for the detailed response from the authors. Most of my concerns have been addressed, so I improve my score to 6. \nIn the rebuttal, the authors also stated that \"Unlike SysBinder, which lacks the abilities to model prior distributions and perform sampling, we observe that by discretizing the block representations, we can train a novel semantic prior transformer.\" It seems \"discretization\" is an advantage over SysBinder. However, in subsection 5.2.2 about the experimental results in Table 5,  the authors stated as \"This shows that despite adding a discretization bottleneck, the latents in SVQ are still useful for downstream tasks that rely on the properties of the objects in the scene.\". My concern is about discretization, which is an advantage or a bottleneck?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726588590,
                "cdate": 1700726588590,
                "tmdate": 1700726588590,
                "mdate": 1700726588590,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YtGkIxyzt9",
            "forum": "HYyRwm367m",
            "replyto": "HYyRwm367m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7823/Reviewer_qSSD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7823/Reviewer_qSSD"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for decomposing scenes hierarchically from low level factors such as color and shape to objects. The method learns a prior over these concepts which allows for generating data from the data distribution which most object-centric papers cannot do. In the the experiments they show that the representation can be used to solve downstream tasks which require reasoning about the properties of different objects in the scene. Additionally they show that the inductive biases proposed by the work leads to better FID score compared to VQ-VAE and similar generative models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper proposes an interesting idea and the experimental setup for the downstream tasks are well done. Evaluating whether the model has learned underlying generative rules of learning is a neat and, to my knowledge, novel way of probing the model. I think the idea would be impactful and the community would be interested if it works for more realistic and larger scale datasets. \n\n- The qualitative results in figure 6 where the color, position and size are disentangled are impressive even for a simple, synthetic dataset. \n\n- The paper is well written and figures are illustrative of the main idea. \n\n- Overall the experiment are well done, with reasonable baselines and clear analysis of the results."
                },
                "weaknesses": {
                    "value": "- The experiments are only conducted on synthetic, simple datasets. The variants of CLEVR are procedurally generated based on disentangled factors such as shape, color, and texture so it's unclear if the factor based approach proposed by this work generalizes to more realistic scenes. \n\n- The main metric used to evaluate the method and baseline is FID. This metric seems orthogonal to the goal of object-centric decomposition. Typically object-centric papers such as Slot-Attention measure segmentation with adjusted random index (ARI). I ask for clarification on why FID is the primary metric for evaluating object-centric models. I think if generation is the primary goal, then the work should compare to generative models and VQ-VAE on more realistic generative datasets. \n\n- I would be happy to improve my score if my concerns are addressed, particularly if the method works on real, more complex datasets than CLEVR."
                },
                "questions": {
                    "value": "- How sensitive are the results to the codebook size and number of blocks? It seems that for CLEVR we can a priori know these hyper-parameters, but for real-world or more complex datasets how would we set these?\n\n- It seems natural to compare the ARI with slot-attention or a variant of slot-attention. Is there a reason why the method shouldn't be compared in this manner?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Reviewer_qSSD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826123348,
            "cdate": 1698826123348,
            "tmdate": 1700699277241,
            "mdate": 1700699277241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8YA2VgaSTo",
                "forum": "HYyRwm367m",
                "replyto": "YtGkIxyzt9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qSSD"
                    },
                    "comment": {
                        "value": "Thank you for the review and the thoughtful questions!\n\n> The experiments are only conducted on synthetic, simple datasets. The variants of CLEVR are procedurally generated based on disentangled factors such as shape, color, and texture so it's unclear if the factor based approach proposed by this work generalizes to more realistic scenes.\n> \n\nPlease see the above common response for a discussion about the synthetic nature of the datasets.\n\n> The main metric used to evaluate the method and baseline is FID. This metric seems orthogonal to the goal of object-centric decomposition. Typically object-centric papers such as Slot-Attention measure segmentation with adjusted random index (ARI). I ask for clarification on why FID is the primary metric for evaluating object-centric models. I think if generation is the primary goal, then the work should compare to generative models and VQ-VAE on more realistic generative datasets.\n> \n> \u2026\n> \n> It seems natural to compare the ARI with slot-attention or a variant of slot-attention. Is there a reason why the method shouldn't be compared in this manner?\n> \n\nThank you for this suggestion. It is true that as an object-centric learning method, it seems natural to include FG-ARI as an evaluation metric as is typically done in previous works. We originally did not include this because object segmentation was not a key focus of our work. Instead, we emphasized the ability to **generate new samples through a composition of high-level semantic concepts** as a key contribution and instead focused our main evaluations on FID, generation accuracy, and qualitative samples. Since our method adds an additional discrete bottleneck, we also do not expect SVQ to improve segmentation performance over previous methods. Nonetheless, we agree with the reviewer that it is important to include this metric to see the effect of the discrete bottleneck on segmentation performance. We will include these numbers in an updated version of the manuscript and summarize the results below. We see that when compared to SysBinder, SVQ performs similarly in terms of FG-ARI on CLEVR-Easy and CLEVR-Hard and slightly underperforms on CLEVR-Tex. Compared to vanilla Slot Attention, SVQ achieves higher FG-ARi on all 3 datasets. **Importantly, note that SVQ achieves this by incorporating the crucial abilities of prior distribution modeling and sampling, which are not present in Slot Attention, SLATE, or SysBinder.**\n\n|  | Slot Attention | SLATE | SysBinder | SVQ |\n| --- | --- | --- | --- | --- |\n| CLEVR-Easy | 85.85 | 91.65 | 92.58 | 91.37 |\n| CLEVR-Hard | 81.29 | 76.79 | 90.43 | 90.48 |\n| CLEVR-Tex | 24.67 | 73.85 | 78.12 | 70.93 |\n\n> How sensitive are the results to the codebook size and number of blocks? It seems that for CLEVR we can a priori know these hyper-parameters, but for real-world or more complex datasets how would we set these?\n> \n\nThis is an important question. The codebook size and number of blocks are currently hyperparameters like the number of heads or convolution filters in other models. Our method does not require the true codebook size or number of blocks as it learns in an unsupervised way, but there exists a range of hyperparameters that make it work better. Making the model work on more complex real-world scenes is definitely an important future direction. \n\nTo answer your question, we ran an ablation on the number of blocks for the 2D Sprites (3 obj) dataset. We are additionally running an ablation on the codebook size and will respond with the results once those experiments are finished running.\n\nThe results for the ablation on number of blocks are shown in the table below. We see that when the number of blocks is too small, the model performs poorly and fails to generate scenes corresponding to the data distribution. For a sufficiently large number of blocks, the model is able to segment the scene, but Generation Accuracy decreases when the number of blocks is too large. We suspect this to be because with more blocks, the model may require a higher capacity prior, which we kept fixed in this ablation.\n\n| Number of Blocks | FID | Generation Accuracy (in %) |\n| --- | --- | --- |\n| 1 | 465.60 | 0.00 |\n| 2 | 80.71 | 1.56 |\n| 4 | 7.76 | 75.78 |\n| 8 | 6.61 | 75.00 |\n| 16 | 7.17 | 55.47 |\n| 32 | 8.74 | 54.69 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478312397,
                "cdate": 1700478312397,
                "tmdate": 1700479079457,
                "mdate": 1700479079457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iu7JAcHjAN",
                "forum": "HYyRwm367m",
                "replyto": "8YA2VgaSTo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_qSSD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_qSSD"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response. Most of my concerns have been addressed. While it would be nice to see this method used on real-world scenes or data, I realize that most current methods evaluate on the synthetic CLEVR variants. Overall I think the method and experiments are interesting, and the experimental setups such as odd-one-out are a novel way of probing a visual models understanding. I improve my score to 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699257499,
                "cdate": 1700699257499,
                "tmdate": 1700699257499,
                "mdate": 1700699257499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uyZ7EGZ9Zs",
            "forum": "HYyRwm367m",
            "replyto": "HYyRwm367m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7823/Reviewer_gjT4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7823/Reviewer_gjT4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach to perform semantic-based vector quantization and learn a corresponding codebook as well as a generative model under object-centric learning scenarios. It uses a slot-attention-based encoder to obtain N object-centric features for each image, where the hidden dimension is divided by M groups and each group shares the parameters. The M denotes the number of attributes. Further, it learns a codebook with MxK codes to quantize the features from the encoder. For every slot, each attribute would be quantized to the code with the closest L2 distance among the corresponding K codes, resulting in the final NxM quantized results for the reconstruction task. Experiments are conducted on various synthetic datasets to showcase the effectiveness of the proposed model. The results show that it is capable to capture high-level information beyond patch and generate better results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The writing is clear and easy to follow.\nThe motivation looks good to me, and the proposed idea is interesting, the codebook analysis results are impressive. The performance looks good on these synthetic datasets."
                },
                "weaknesses": {
                    "value": "I'm not up-to-date to the latest slot-attention models and their performance on synthetic datasets such as CLEVR. It seems that the authors only choose a few baselines (GENESIS-v2, VQ-VAE, dVAE) for the generation quality evaluation as well as other downstream baselines. The VQ-VAE/dVAE model may requires larger model capacity to handle this tasks as they works on the local patch level. The generation results in Fig.4 is surprisingly bad, given the fact that they actually perform well on real data generation (VQGAN/DALL-E), it should be easy to reconstruct/generate good results when the hyperparameters are appropriate. I'm not sure whether the current results is convincing enough.\n\nI also have several questions in the following section, it would be great if the authors could address them. Overall I think this paper shows some interesting results and could inspire people. But the current experiments only show results on synthetic data which is somewhat weak."
                },
                "questions": {
                    "value": "1. In Tab.4, the authors only compare with dVAE and VQ-VAE Indices, which makes the comparison unfair as SVQ Indices also perform bad. Why not the authors also showcase the VQ-VAE code (like Tab.5) as well as dVAE feature (using the code id to index the weight of the first layer in the decoder)?\n2. The Tab.5 seems not a fair comparison as well. VQ-VAE / dVAE are not designed for the object-centric tasks so it is expected they will not perform well on this task, on the other hand, SysBinder shows comparable and even better performance on this task.\n3. Following 3, the paper lacks of ablation studies, which makes the reader hard to understand which part plays the critical role and is useful to support the author's claim, for example, what would happen when M varies and becomes smaller than the actual number of attributes?\n4. The authors have shown some interesting codebook analysis results, would be great if the authors could also showcase whether the learned codebook is composable and can perform controllable generation (other than based on reconstruction) and by solely manipulate the code.\n5. Recently there is a paper present similar idea (group level quantization https://arxiv.org/abs/2309.15505), which may give the authors some inspiration as well, note that this is a concurrent work so I'm not asking the authors to compare with them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7823/Reviewer_gjT4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833917454,
            "cdate": 1698833917454,
            "tmdate": 1700755201649,
            "mdate": 1700755201649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nv27Djb85I",
                "forum": "HYyRwm367m",
                "replyto": "uyZ7EGZ9Zs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gjT4 (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the review and insightful questions.\n\n> The VQ-VAE/dVAE model may requires larger model capacity to handle this tasks as they works on the local patch level.\n> \n\nFor VQ-VAE, we had used the same 20-layer PixelCNN as proposed in the original paper. For dVAE, which is a more direct ablation of the semantic prior because it shares the same image decoder as SVQ, we had the same suspicion that perhaps a larger capacity decoder could improve the results. We therefore ran several experiments with larger capacity priors (12-layer and 16-layer transformers compared to 8-layer from our paper) on CLEVR-Hard to see the effect. We present the results below:\n\n| Model | FID |\n| --- | --- |\n| dVAE (8-layer) | 65.89 |\n| dVAE (12-layer) | 61.74 |\n| dVAE (16-layer) | 60.75 |\n| SVQ (8-layer) | 43.12 |\n\nWe see that while increasing the size of the transformer for the prior does slightly improve the FID for dVAE, it still underperforms when compared to SVQ, indicating that the performance issue may not be solely due to model capacity.\n\nNevertheless, we would like to emphasize that, although we use generation quality as one of the evaluation metrics, the **main contribution of the paper is not to propose a better image synthesizer but to answer the question \u201chow can we learn semantic and composable representations (like language of visual thoughts) from raw images similar to the role of words in text.** Although patch-level quantized representation is known to be good for image generation, it does not provide semantic (or conceptual) and composable representation as we do.\n\n> In Tab.4, the authors only compare with dVAE and VQ-VAE Indices, which makes the comparison unfair as SVQ Indices also perform bad. Why not the authors also showcase the VQ-VAE code (like Tab.5) as well as dVAE feature (using the code id to index the weight of the first layer in the decoder)?\n> \n\nThank you for this suggestion. As suggested, we\u2019ve ran these additional experiments and will update the manuscript. We\u2019ve reproduced the results below (new results highlighted in **********bold**********). We see that while the VQ-VAE Codebook performs better in the 2D Odd-One-Out experiments when compared with the other patch-based methods, **it still take more steps to reach 98% ID accuracy and has lower OOD Accuracy than SVQ.** The continuous version of dVAE does not seem to improve performance on the CLEVR-Hard downstream task. We will include these results in an updated version of the paper.\n\n2D Odd-One-Out Downstream Experiment:\n\n| Model | Steps to 98% (ID) | OOD Accuracy % |\n| --- | --- | --- |\n| dVAE Discrete | 37,000 | 26.7 |\n| **dVAE Continuous** | 32,000 | 29.5 |\n| VQ-VAE Indices | 77,000 | 24.0 |\n| **VQ-VAE Codebook** | 54,500 | 55.6 |\n| SysBinder | 27,000 | 67.6 |\n| SVQ Indices | 77,000 | 46.8 |\n| SVQ Codebook | 27,000 | 99.1 |\n\nCLEVR-Hard Property Comparison Downstream Experiment:\n\n| Model | ID Accuracy % | OOD Accuracy % |\n| --- | --- | --- |\n| dVAE Discrete | 27.52 | 19.87 |\n| **dVAE Continuous** | 24.51 | 20.51 |\n| VQ-VAE Indices | 24.53 | 17.74 |\n| VQ-VAE Codebook | 23.73 | 18.80 |\n| SysBinder | 79.60 | 70.09 |\n| SVQ Indices | 68.21 | 64.53 |\n| SVQ Codebook | 75.86 | 71.15 |\n\n\n> The Tab.5 seems not a fair comparison as well. VQ-VAE / dVAE are not designed for the object-centric tasks so it is expected they will not perform well on this task, on the other hand, SysBinder shows comparable and even better performance on this task.\n> \n\nHere, we respectfully disagree. We believe that the experiment comparing SVQ with VQ-VAE and dVAE is fair and serves an important purpose. The VQ-VAE and dVAE baselines for the downstream task utilize an architecture similar to the Vision Transformer (ViT) architecture, where patch-codes are used as inputs and transformer layers are employed for predictions. The results highlight that the general ViT architecture struggles to handle reasoning tasks effectively, despite the presence of transformer layers. This outcome may not be immediately expected, as ViTs and their variants are widely recognized as powerful representation learners. We believe that this experiment is crucial in supporting the motivation behind our semantic representations that go beyond patch representations.\n\nIt is worth noting that while SysBinder performs slightly better in the ID setting and slightly worse in the OOD setting compared to SVQ, it lacks the ability to provide a prior distribution and sampling. The comparison to SysBinder is, therefore, still crucial because it suggests that SVQ's representations remain effective even after incorporating prior modeling and sampling through the addition of the discrete bottleneck."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477976574,
                "cdate": 1700477976574,
                "tmdate": 1700478899282,
                "mdate": 1700478899282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b8mPL4ZmN6",
            "forum": "HYyRwm367m",
            "replyto": "HYyRwm367m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7823/Reviewer_cC8S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7823/Reviewer_cC8S"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a novel image-quantized method, called semantic vector-quantized variational autoencoder. The SVQ constructs representations hierarchically from low-level discrete concept schemas to high-level object representation. The author conducts experiments on various 2D and 3D object-centric datasets and validates the effectiveness of the SVQ."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "i) Compared with widely used VAE and VQ-VAE, SVQ models the discrete abstraction and object-level representations simultaneously. \n\nii) The proposed semantic prior based on discrete latent codes can be directly used in generation tasks, and the visualization results show it is superior compared with the baseline methods."
                },
                "weaknesses": {
                    "value": "I am not an expert in the quantization area, my main works are related to self-supervised learning. So I want to ask some questions about the application of self-supervised learning.\n\ni) The patch-level quantization method VQ-VAE can maintain the geometry structure. So it is widely used in mask-image-modeling methods, such as BEiT.  Can SVQ also maintain such a geometry structure? \n\nii) If the SVQ can be applied in the general 2D-image generation methods, such as stable diffusion? I think it is a promising application of the quantization method."
                },
                "questions": {
                    "value": "Refer to the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842647345,
            "cdate": 1698842647345,
            "tmdate": 1699636957727,
            "mdate": 1699636957727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m4JS73nUnR",
                "forum": "HYyRwm367m",
                "replyto": "b8mPL4ZmN6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7823/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comments and the positive review! \n\n> The patch-level quantization method VQ-VAE can maintain the geometry structure. So it is widely used in mask-image-modeling methods, such as BEiT. Can SVQ also maintain such a geometry structure?\n\nI\u2019m not sure I completely understand what is meant by maintaining the geometry structure in this context, but the discrete latents in SVQ correspond to the different properties of the objects in the scene, rather than a fixed receptive field in the image. While it\u2019s certainly possible to do a form of masked reconstruction training (and a potentially interesting future research direction), this would be very different than what is typically done in methods such as BEiT.\n\n> If the SVQ can be applied in the general 2D-image generation methods, such as stable diffusion? I think it is a promising application of the quantization method.\n\nSimilar to the previous response, since the SVQ latents correspond to the objects in the scene, it seems that they are not directly applicable to methods such as stable diffusion. That being said, as we mention in the general response, combining diffusion-based approaches with our method may be a promising direction to scale our method to work on more realistic scenes."
                    },
                    "title": {
                        "value": "Response to Reviewer cC8S"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477917340,
                "cdate": 1700477917340,
                "tmdate": 1700477937153,
                "mdate": 1700477937153,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0rawWQpo4t",
                "forum": "HYyRwm367m",
                "replyto": "m4JS73nUnR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_cC8S"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_cC8S"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the explanation of my questions. I think it is an interesting work, and the technical seems sound. I will keep my original positive ratings.\n\nIn addition, I think it is a promising area to apply the proposed method in the general diffusion method and mask-image-modeling pretraining area. \n\nP.S. I want to declare that I am not an expert in this area. The AC may consider other reviewers' opinions with higher weights."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486676476,
                "cdate": 1700486676476,
                "tmdate": 1700486676476,
                "mdate": 1700486676476,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]