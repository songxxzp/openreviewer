[
    {
        "title": "Uncertainty for Active Learning on Graphs"
    },
    {
        "review": {
            "id": "j79Pqn4P7v",
            "forum": "GpGJg1gsjl",
            "replyto": "GpGJg1gsjl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3559/Reviewer_FXsp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3559/Reviewer_FXsp"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a comprehensive study of applying Uncertainty Sampling (US) within the Active Learning (AL) framework for node classification within graphs. The authors provide a benchmark for AL and evaluate the performance of AL baselines using real word datasets. Additionally, they propose novel Bayesian uncertainty estimation methods based on the ground truth labels, and illustrate their effectiveness using synthetic CSBM dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper offers a thorough evaluation of AL performance through a series of well-conducted experiments and a qualitative analysis."
                },
                "weaknesses": {
                    "value": "The proposed ground truth uncertainty is not so useful and the uncertainty sampling US based on it is not practical. During prediction procedure, the ground truth label remains unknown and therefore it is inappropriate to define an uncertainty based on it. \n\nUS with knowledge of ground truth label would benefit from the information leakage and so the good performance in the CSBM dataset is not achievable in real-world datasets. For example, in traditional AL algorithms, it's difficult to select a node for query when the classifier gives the ground truth label of the node a low predictive probability, although querying such node would provide the classifier a lot information. Take, for instance, a scenario where  p(ground truth class| y_i ) = 0.1 and p(incorrect class| y_i ) = 0.9. Typically the prediction to incorrect class of y_i might be considered confident and AL algorithm will not choose y_i for querying, and such error will cause general AL algorithm not perform as good as random sampling. But for US based on ground truth uncertainty, the epistemic uncertainty is large and the node will be selected. Therefore, the good performance in the CSBM dataset is not practical."
                },
                "questions": {
                    "value": "Please explain the practical application of the ground truth uncertainty."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698453190446,
            "cdate": 1698453190446,
            "tmdate": 1699636310517,
            "mdate": 1699636310517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5oLaB9YFFz",
                "forum": "GpGJg1gsjl",
                "replyto": "j79Pqn4P7v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for their valuable time and feedback to review our manuscript.\n\nThe reviewer is correct that the ground-truth uncertainties actually use the ground-truth labels and this technically implies data leakage. However, our argument goes for the existence of such optimal (oracle-like) uncertainty measures and not utilizing them in a practical context.\n\nOur main goal is to study the link between US and AL. Our theoretical result therefore implies that using the optimal epistemic uncertainty (which, again, can not be computed exactly in practice) is an optimal AL strategy. We first supply this with an empirical study on CSBMs under the idealized setting of full access to ground-truth uncertainties. This allows us to study the effects of disentangling uncertainty in an isolated fashion. We do not intend to propose ground-truth uncertainty as an AL strategy on CSBMs. We also want to point out that our theoretical analysis still fully translates to any dataset, albeit that ground-truth uncertainty, while also existing in this context, is not available and needs to be approximated instead. In our revised manuscript, we also accommodate learning the generative process (e.g. using a GNN) into our theory while retaining all optimality guarantees.\n\nWe agree with the concern whether the strong performance in an ideal setting (CSBMs) does imply a practically usable AL strategy. Therefore, to further strengthen the claim that our findings translate to real data, we also added an experimental study on real-world graph datasets. Here, we used simple GNN-based approximations to ground-truth uncertainty (see Appendix H for details) and did not facilitate any ground-truth labels (i.e. data leakage) hence being a practical approach to AL. We find that disentangling uncertainty according to our theory already outperforms many SOTA methods and US methods on most datasets (see Figures 6 and 15). We believe that this strongly underlines that our findings also translate well to realistic settings and can inform US strategies towards aligning with AL.\n\nWhile the ground-truth uncertainty is defined with respect to information that is not available in practice (data generating process, labels of unobserved nodes), we empirically verify that trying to approximate them according to our theoretical framework gives a very strong AL strategy off-the-shelf. We believe that this shows that our theoretical analysis is highly relevant to the field of AL on graphs and can guide the development of novel US methods in a theoretically sound and well-motivated fashion.\n\nWe again want to point out that regardless of outperforming other AL strategies on multiple datasets in a realistic setting without data leakage, the core contribution of our work is its theoretical analysis. Understanding if and how UQ relates to AL bridges an important aspect of uncertainty modeling neglected by previous advances in the field of UQ. The efficacy of an approximative application of our insights is just one instance of how our work can impact the development of UQ methods. For example, future work could aim at better modeling the unknown data-generating process, e.g. by considering non-edges. We also make a case for considering AL when evaluating the effectiveness of novel UQ strategies on graphs. Consequentially, we updated our manuscript to highlight the contribution and goal of our paper more distinctly (see Sections 1 and 6).\n\n## Final Remarks\n\nWe hope that the addition of a practical, novel method together with an experiment on real-world data without data leakage that outperforms SOTA US approaches convinces the reviewer that our analysis poses a significant and valuable, practical contribution and is worth sharing with the scientific community. If so, we kindly ask the reviewer to reconsider their score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166485115,
                "cdate": 1700166485115,
                "tmdate": 1700166485115,
                "mdate": 1700166485115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L5DMYr61x3",
                "forum": "GpGJg1gsjl",
                "replyto": "lN0a5wYpCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Reviewer_FXsp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Reviewer_FXsp"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate your thoughtful response to my feedback. After careful consideration of your arguments and the points raised in your rebuttal, I maintain my score for your paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600280611,
                "cdate": 1700600280611,
                "tmdate": 1700600280611,
                "mdate": 1700600280611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G0QxsNHFcL",
            "forum": "GpGJg1gsjl",
            "replyto": "GpGJg1gsjl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3559/Reviewer_KZ6P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3559/Reviewer_KZ6P"
            ],
            "content": {
                "summary": {
                    "value": "The authors establish a benchmark for uncertainty sampling based active learning approaches for graph data. The paper also proposes a Bayesian uncertainty estimation to actively select the node. This estimation is based on the knowledge of data-generating process. The authors validate the effectiveness of their approach with both theoretical analysis and empirical experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "a. This paper studies the active learning problem with graph data from an interesting perspective--uncertainty sampling strategy and propose a new Bayesian uncertainty estimation.\n\nb. The authors provide both theoretical analysis and empirical results to show the effectiveness of the proposed estimation."
                },
                "weaknesses": {
                    "value": "a. Theoretical contributions in this paper appear to be somewhat limited. The proposed uncertainty estimation is based on the posterior probability given the ground-truth label of the unobserved nodes. However, the essence of active learning lies in addressing this problem without access to ground-truth information, which remains inadequately addressed.\n\nb. The experimental results provided are restricted to synthetic data, and the method's reliance on knowledge of the true data generation process probabilities pose practical challenges. How to approximately compute the estimation remains unclear.\n\nc. In the empirical evaluation, the compared baselines are only random queries and other uncertainty-based methods, the state-of-the art methods are missing, e.g. SEAL[1] and IGP[2]. \n\nd. The paper's presentation could be improved. For instance, when introducing concepts like aleatoric and epistemic uncertainty, the authors provide limited explanations and intuitions, potentially causing readers unfamiliar with these terms to struggle to follow the paper.\n\n[1] Li Y, Yin J, Chen L. Seal: Semisupervised adversarial active learning on attributed graphs[J]. IEEE Transactions on Neural Networks and Learning Systems, 2020, 32(7): 3136-3147. \n[2] Zhang W, Wang Y, You Z, et al. Information Gain Propagation: a new way to Graph Active Learning with Soft Labels[J]. arXiv preprint arXiv:2203.01093, 2022."
                },
                "questions": {
                    "value": "a. How can the proposed uncertainty estimation be computed in practical scenarios where true data generation probabilities are unknown? Are there methods or approaches to approximate this estimation without relying on ground-truth knowledge?\n\nb. What's the performance of non-US based active learning methods on CSBMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3559/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3559/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3559/Reviewer_KZ6P"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743618285,
            "cdate": 1698743618285,
            "tmdate": 1699636310442,
            "mdate": 1699636310442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gof4JhKFQj",
                "forum": "GpGJg1gsjl",
                "replyto": "G0QxsNHFcL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the time and the thorough review. We are delighted that the reviewer agrees with us, that we study the AL problem from an interesting perspective.\n\n## Missing methods\n\n**Addressing weaknesses a and b:**\nWe understand the concerns the reviewer has about the applicability of our results to a practical setting in which neither all labels nor the generative process is available. Our analysis, however, is purely theoretical: We show that ground-truth epistemic uncertainty is an optimal AL strategy. This result about the alignment of uncertainty estimation and AL also holds on real graphs, even though the proposed ground-truth uncertainties can not be computed exactly in practice. Our goal is to formally justify why we should expect US sampling to be an effective strategy in the first place. Furthermore, we can formally prove that disentanglement plays a crucial role in effective US, which again translates to real-world scenarios. Our work therefore provides a sound, theoretical basis on which novel US can be developed and applied to AL for interdependent data, which we believe to be a valuable contribution to the field.\n\nWe want to point out that it is not the access to ground-truth data that is central to the effectiveness of epistemic US: Measures of total, aleatoric, and epistemic uncertainty are defined in terms of ground-truth information (see Defintions 1-3) and exhibit very different efficacy to AL.\n\nNonetheless, we agree that our manuscript benefits from a more practical evaluation as well. To that end, we extended the theoretical framework to also allow a classifier to model the parameters of an underlying process in a learnable fashion, a framework that GNNs for example fit into. Based on this, we apply our theoretical insights to real-world data using GNNs as an approximative approach to our proposed, unknown ground-truth uncertainties. In this evaluation, we do not utilize any information about unavailable labels or the generative process. Nonetheless, this very off-the-shelf application of our findings elevates the uncertainty of a GNN classifier to outperform SOTA methods on multiple datasets. We hope to specifically also address weakness b with this additional empirical study, as we now propose an approximate estimator of epistemic uncertainty that aligns with our theory. We additionally discuss potential approximative errors of this simplistic approach in Appendix H.\n\nOverall, we also adapted the manuscript to address the overall intention of our analysis more clearly: Our goal is not the proposal of a novel AL strategy, but rather to bridge the gap between principled US and AL on graphs. We theoretically prove that disentangled US and AL align in general. We reveal pitfalls in UQ and how to address them with an evaluation in an idealized setting on a CSBM. Lastly, we confirm the applicability to real-world scenarios by evaluating a simple approximative realization of our insights on real graphs and observing strong performance.\n\n**Addressing Question a:** Estimating the true generative process of graph data is indeed a challenging topic. The aforementioned approach utilizes a simple GNN to address this problem and exhibits already reasonable performance. Nonetheless, we believe that our work can guide US toward the development of more faithful estimators: For example, on CSBMs, we observed non-edges to play a crucial role in successful US. Contemporary GNNs do not explicitly utilize such information which we conjecture to be one interesting direction for future research."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166415579,
                "cdate": 1700166415579,
                "tmdate": 1700166415579,
                "mdate": 1700166415579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "atEBBepfgV",
                "forum": "GpGJg1gsjl",
                "replyto": "fzVX9X4i5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Reviewer_KZ6P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Reviewer_KZ6P"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I'm interested in understanding more about the auxiliary classifiers used in the approximation method. Could you provide additional details regarding these auxiliary classifiers, such as their architecture or training process? Additionally, regarding the baseline methods, I'm curious whether they also utilize pseudo-labels from the auxiliary classifiers for training. Would the performance of these baselines benefit from using pseudo-labels? Furthermore, while I noticed that the results for Citeseer are included in the main content, I found more detailed results in Appendix H. However, I couldn't find sufficient text description accompanying the figures in Appendix H. Could you please provide more elaboration on what these figures represent and demonstrate?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546683566,
                "cdate": 1700546683566,
                "tmdate": 1700546683566,
                "mdate": 1700546683566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QJkawNBrTl",
            "forum": "GpGJg1gsjl",
            "replyto": "GpGJg1gsjl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3559/Reviewer_5NFL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3559/Reviewer_5NFL"
            ],
            "content": {
                "summary": {
                    "value": "This work is an empirical study of a typical Active Learning method, Uncertainty Sampling (US) for node classification on graphs. The authors present an extensive benchmark for US methods that goes beyond predictive uncertainty, revealing that, the US employing modern uncertainty estimators struggles to outperform random queries consistently. The authors establish ground-truth Bayesian uncertainty estimates for a Bayesian classifier based on the underlying graph generative process, providing formal evidence of the alignment between US and AL. When they apply their approach using a Clustered Stochastic Block Model (CSBM), they empirically confirm the effectiveness of US when uncertainty estimates are accurately disentangled into aleatoric and epistemic uncertainty while considering all available graph information."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work provides an empirical study for US with node classification on graphs, highlighting both its efficacy and potential limitations.\n\n- An important finding is that the existing AL methods cannot outperform random sampling benchmarks."
                },
                "weaknesses": {
                    "value": "- The study primarily concentrates on a specific graph type, the CSBM, which might not fully represent the characteristics of all real-world graphs.\n\n- Novelty concern: undoubtedly, this work offers an extensive exploration of uncertainty-based Active Learning (AL) within the context of graphs. However, it does not introduce any novel methods for active learning in the graph domain."
                },
                "questions": {
                    "value": "- In the experimental results, such as Figure 3, the curves depicting acquired labels versus accuracy exhibit significant fluctuations. Did the authors conduct repeated trials to mitigate these fluctuations in the model's performance?\n\n- In the Introduction section, the authors dedicated an extensive portion of the text to explain uncertainty sampling. This level of detail might be excessive as uncertainty sampling is a straightforward concept. It would be more beneficial to present the essential formulations and allocate additional space to elaborate on active learning in graph-related tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787920326,
            "cdate": 1698787920326,
            "tmdate": 1699636310337,
            "mdate": 1699636310337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mo9pNCsyDy",
                "forum": "GpGJg1gsjl",
                "replyto": "QJkawNBrTl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for their time and feedback and are delighted that the reviewer agrees with us that it is an interesting and important finding that US AL does not outperform random sampling.\n\n## Application to real-world graphs (addressing weaknesses 1 and 2)\n\nWe follow the reviewer\u2019s suggestion to provide more evidence for applicability of our results to real-world problems. Nonetheless, the result that US is an optimal strategy for AL still holds, even if the true epistemic uncertainty can not easily be computed in practice. We both theoretically prove and empirically (on CSBMs) this optimality. Hence, we expect a strong, disentangled UQ method to perform well in AL. Our work motivates principled UQ and shows its relationship to AL formally and does not explicitly aim at proposing a novel improved AL strategy. We updated our paper to make this point more clear.\n\nWe also acknowledge the concerns regarding transferability to real-world settings. To that end, in our updated manuscript, we now introduce a method to apply our theory to real-world data in Section 6, paragraph \u201cReal-World Data\u201d. We detail a straightforward framework that models the epistemic uncertainty as the difference between total and aleatoric. This strategy is directly derived from our theoretic results and does not introduce any other components or tuning: The total uncertainty originates from a classifier trained solely on labeled data. We then apply the predictions from this classifier as pseudo labels to train a subsequent model on all labeled nodes (pseudo labels + real label) to model the aleatoric uncertainty. Our method outperforms many state-of-the-art US strategies on most datasets (all Figures can be seen in the updated version, Section 6 and Appendix H). We also provide a discussion of potential failure modes of this framework in Appendix H.\n\nThis experiment shows that our analysis translates to real-world graphs, where we explicitly do not use any unavailable data or knowledge about the generative process. Additionally, we adapted our theory to a setting where the Bayesian classifier has to learn the parameters of the underlying unknown generative process. GNNs that predict marginal probabilities for each node fit into this framework. Since our optimality guarantees still hold under these relaxed assumptions, we believe the applicability of our findings to be even more clearly relevant to practical scenarios.\n\nWe hope that with these results on real data, we can convince the reviewer that the theoretical insights on CSBMs apply to real-world graphs as the newly proposed framework is a simple straightforward implementation thereof. We want to mention that we still believe that, even though we outperform other SOTA AL strategies on multiple datasets, our theoretic analysis is the main contribution, as it bridges the gap between understanding the relationship between US and AL on graphs which has not been discussed in previous literature. As an example of how to incorporate our findings, one could try to explicitly tailor GNNs towards better modeling an assumed generative process, e.g. by considering the non-edges in a transformer-like fashion. Furthermore, as a consequence of our result that US and AL are well aligned, we strongly motivate using AL as a test of quality for newly devised uncertainty estimators (which are currently mostly evaluated on the detection of distribution shifts).\n\nWe also revised our manuscript to make the structure and value of our analysis clear: We first theoretically prove the alignment between epistemic US and AL and then proceed to support this with empirical evidence. First, we focus on a simplified CSBM setting and access to all labels. This analysis isolates the effects stemming from proper disentanglement and data modeling. We then proceed to a real-world setting, where we have to rely on various approximations to apply our theoretical insights. We observe our framework to show high efficacy nonetheless. This structure enables us to both propose theoretical soundness of the results we provide as well as high practicability at the same time.\n\n## Further changes to the manuscript\n\n**Addressing Question 1**: As we use multiple dataset splits and model initializations, all curves are already average performances. We added plots for the requested experiments including the standard deviation to the Appendix (Figure 9), such that readers can see the high variance. Further, we added a description of the repeated trial setting to the main text in Section 4.\n\n**Addressing Question 2**: We adapted the manuscript to give the background about US more concisely and added more explanation of epistemic and aleatoric effects to the background Section 2 at the same time. All additions are made visible with a blue font color in the new PDF version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166312674,
                "cdate": 1700166312674,
                "tmdate": 1700166312674,
                "mdate": 1700166312674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gaTF3N9QVs",
                "forum": "GpGJg1gsjl",
                "replyto": "mo9pNCsyDy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Reviewer_5NFL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Reviewer_5NFL"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. 1) On new real-world dataset experiments, I do see there is no advantage of your model compared with baselines; 2) In Figure 9, the variance is too high, maybe the author needs to find the reason and try to solve the unstable model performance problem."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695430079,
                "cdate": 1700695430079,
                "tmdate": 1700695430079,
                "mdate": 1700695430079,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xNwNl1gNNd",
                "forum": "GpGJg1gsjl",
                "replyto": "QJkawNBrTl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**A)** We are not sure why the reviewer thinks that there is no advantage over US baselines. We will emphasize that the dark green line (RHS) represents our framework in Figure 4 and Figure 15. In these figures, we show that this RHS approximation outperforms all other US strategies on all datasets! For a better quantitative comparison, we provide the area under the curve (AUC) for all of these plots which summarises the average accuracy over acquisitions in this. We highlight the best performing US strategy in bold and the best overall with a dagger.\n\n\n\n|           | Random    | Coreset | Age | ANRMAB | GEEM | Ensemble | MC-Dropout | Energy | GPN | BGCN | Aleatoric | Ours (LHS) | Ours (RHS) |\n|-----------|-----------|---------|-----|--------|------|----------|------------|--------|-----|------|-----------|------------|------------|\n|           |  |  non-US |  non-US | non-US  | non-US |         US | US | US | US | US | US | US | US |\n| CoraML    | $63.85$ | $65.23$ | $67.56$         | $61.14$ | $71.39$         | $63.47$  | $59.17$       | $63.97$ | $54.75$ | $44.45$ | $65.66$ | $67.73$ | **71.45**$^\\dagger$ |\n| Citeseer  | $81.04$ | $79.38$ | $84.21$         | $81.03$ | $85.25^\\dagger$ | $82.94$  | $78.86$       | $81.59$ | $65.31$ | $58.68$ | $78.85$ | $83.26$ | **83.43**         |\n| Pubmed    | $56.79$ | $64.48$ | $69.20^\\dagger$ | $60.49$ | $64.82$         | $63.70$  | $58.67$       | $59.64$ | $58.82$ | $55.19$ | $61.55$ | $58.80$ | **64.36**         |\n| Photos    | $80.52$ | $82.32$ | $74.01$         | $80.92$ | $86.43^\\dagger$ | $84.46$  | $72.42$       | $74.66$ | $54.78$ | $70.83$ | $71.43$ | $71.07$ | **85.52**         |\n| Computers | $72.39$ | $71.53$ | $69.31$         | $71.62$ | $74.49^\\dagger$ | $68.38$  | $51.02$       | $59.62$ | $39.21$ | $58.64$ | $59.34$ | $62.16$ | **72.54**     |\n\n\n\nThe only consistently equally well-performing (non-US) strategy is GEEM, which our RHS approach matches. We also point out that our method is a straightforward implementation of the theory our paper provides without tuning any models. Nonetheless, we clearly (qualitatively and quantitatively outperform other US strategies and match the best non-US strategy's performance. Thus, we are confident to conclude that our results show significant merits for practical real-world uncertainty estimation, as evidently shown by the AUC scores.\n\n**B)** We agree that the variance is extremely high on CSBMs. We point out two potential reasons: First, AL settings can inherently be noisy and since the CSBM graphs are relatively small, differences in acquired nodes (especially when picked almost randomly) may be even more pronounced. This is due to the fact that the CSMBs we study have a relatively low signal-to-noise ratio (SNR), otherwise, the whole task would be very easy to solve with GNNs (achieving near-perfect accuracy after only one acquisition per class). In this SNR regime, however, we observe high variance. Second, we study not one graph but average results over five CSBMs sampled from the same distribution: Again, due to relatively small graph sizes, we observe very different graphs in practice. We chose to average over multiple graphs to avoid \u201ccherry-picking\u201d one CSBM graph, where our ground-truth uncertainty performs well, and instead drew 5 random graphs to give an overall more truthful impression of performance.\n\nHowever, due to the new real-world graph experiments, where we show that our RHS method outperforms all US strategies, showing the superiority of ours compared to baselines on CSBMs might be obsolete. What still is interesting is the performance of our approach using various uncertainties but we can remove the other figure if the reviewer is unhappy with it.\n\nIn conclusion, we want to say that we show that: (1) there is a gap between non-US AL and US AL strategies, and no SOTA method for US seems to work on par with the traditional methods. (2) we further analyze the problem and can prove that US and AL are well aligned and propose a new principled framework for US where the uncertainties are properly disentangled. (3) we show on synthetic and real-world data that we outperform all other US significantly and close the gap to non-US.\nThus, we lay principled groundwork for new uncertainty estimators on graphs. In particular, we don\u2019t try to \u2018sell\u2019 our method as a novel AL strategy but provide a theoretically sound and empirically well-supported analysis of the relationship of US and AL that we hope to inspire more research in this area. We firmly believe, and hope that the reviewers agree with us, that this is of value to the scientific community and in our view even worth more than proposing a new acquisition strategy."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740763748,
                "cdate": 1700740763748,
                "tmdate": 1700742347105,
                "mdate": 1700742347105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ptfTYm1MlN",
            "forum": "GpGJg1gsjl",
            "replyto": "GpGJg1gsjl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3559/Reviewer_fMcb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3559/Reviewer_fMcb"
            ],
            "content": {
                "summary": {
                    "value": "This article studies the application of AL to graph data, with a focus on the approach of uncertainty sampling. The authors demonstrated through an extensive empirical analysis that many AL strategies, uncertainty-based or not, failed to surpasse random sampling on graph data. A curious observation is that uncertainty estimators which distinguish the reducible uncertainty caused by the randomness of training data from the irreducible uncertainty due to the underlying data generating process and use only the reducible uncertainty to guide the label queries work well on i.i.d. data but not on graph data. Motivated by this observation, the authors proved theoretically that, under a Contextual Stochastic Blockmodel (CSBN) with known parameters, minimizing the reducible uncertainty leads to an optimal AL strategy. This remark is  confirmed on simulated data of (CSBN)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work is well guided with a series of inquiries, starting with open questions in literature review, conducted with empirical observation, theoretical investigation, ending with experimental confirmation.\n\n- The thorough empirical analysis and the original theoretical insights are of interest to the scientific community."
                },
                "weaknesses": {
                    "value": "- The theoretical investigation, which is a major contribution of the article, not only considers a specific model (which is perfectly acceptable), but also assumes the full knowledge of the parameters underlying the model. As in practice the model parameters are rarely known and have to be estimated from data, their estimation error will contribute to the reducible uncertainty. Therefore defining the reducible uncertainty while assuming the model parameters to be pre-known seems to be problematic and needs at least to be discussed.\n\n- It should be made clear earlier in the article (e.g. in the abstract or introduction) that the proposed uncertainty measure is not directly applicable in practice, and rather of theoretical interest."
                },
                "questions": {
                    "value": "My questions are related to the first point of Weaknesses:\n\n- How will the reducible uncertainty change without the knowledge of model parameters ?\n\n- Will the conclusion regarding the optimality of using the reducible uncertainty to guide AL stay the same ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3559/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3559/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3559/Reviewer_fMcb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834310916,
            "cdate": 1698834310916,
            "tmdate": 1699636310232,
            "mdate": 1699636310232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qBZrONzXMC",
                "forum": "GpGJg1gsjl",
                "replyto": "ptfTYm1MlN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for their valuable feedback and the invested time to read our manuscript. We are delighted that the reviewer agrees with us that this is beneficial to the scientific community. \n\n## Knowledge of model parameters (Adressing Weakness 1 and Q 1&2)\n\nWe thank the reviewer for this comment and question as it is a valid point that we need to discuss in our work. We generalized our definitions to incorporate a parametrized classifier that opts to learn the true generative process: \n\n$f_\\theta^*(A, X, y_O^{gt}) =  \\text{argmax}_{c \\in [C]^{|U|}} \\textcolor{blue}{\\int}\\text{Pr}[{y_U= c \\mid A, X, y_O = y_O^{gt}, \\textcolor{blue}{\\theta}}] \\textcolor{blue}{p(\\theta \\mid A, X, y_O)d\\theta}$\n\nWe updated and uploaded a revision of our paper where we highlight changes in blue. The Bayesian classifier averages its prediction over its parameters $\\theta$ according to a posterior distribution $p(\\theta | \\mathbf{X}, \\mathbf{A}, \\mathbf{y})$. For example, these parameters $\\theta$ could be estimates of the parameters of the underlying data-generating process in the CSBM setting or GNN parameters for real data. These changes can be found in Definitions 1-3 and the corresponding proofs.\nWe adapt definitions 2 and 3 accordingly. Importantly, the proposed optimality results still hold, as only the computation of the ground-truth uncertainty is affected by this change.\n\nWhile we can also try to learn the parameters of the underlying CSBM, this would  introduce unwarranted noise into the evaluation. We have further clarified in our manuscript that the analysis on CSBMs opts to isolate the effect of disentangling and correctly modeling uncertainty (and therefore assumes no modeling errors).\n\nIn a new paragraph in Section 6, we detail an AL strategy that models the epistemic uncertainty as the difference between total and aleatoric (as in our theoretic result). Here, the total uncertainty originates from a classifier trained solely on labeled data. We then apply the predictions from this classifier as pseudo labels to train a subsequent model on all labeled nodes, with the exception of one (pseudo labels + real label) to model the aleatoric uncertainty for this one node.\n\nOur method outperforms state-of-the-art US strategies on many datasets. All Figures can be seen in the updated version, Section 6 and Appendix H).\n\nThis experiment does not rely on explicit knowledge of either the generative process or unavailable labels but uses multiple approximations instead and thus could even be used as an AL strategy in practice. We, therefore, believe to provide even more valuable insights by closing the gap between an idealized synthetic setting and real scenarios. Looking ahead, future work could include improving the GNN approximators based on our other findings, such as modeling non-edges, for example in a transformer-like model.\n\nWe hope that with the adaptation of our theoretic results and the new experiments we properly addressed the parameter knowledge concern. If desired by the reviewer, we are open to including an additional experiment in the CSBM setting, conducted without parameter knowledge in the camera-ready version of our work.\n\nWe thank the reviewer for this pointer since framing and defining the problem in this more general parametrized way helps to better exemplify the applicability towards real-world scenarios which improves our manuscript substantially.\n\n## Adding that we don\u2019t propose a novel method earlier in the text\n\nWe believe that the most substantial contribution of our work lies in the analysis of the problem. This groundwork paves the way for the principled development of new UQ strategies and strongly motivates AL as a valuable evaluation benchmark. For example, future studies could explore whether their uniquely disentangled uncertainties, potentially designed for different tasks, are also effective in an AL context.\n\nTherefore, while we now even showcase a straightforward and practical application of our findings, it is our intent to present it as an experimental validation of our theory's applicability to real-world graphs, rather than as the centerpiece of a methods-focused paper. Hence, we further emphasize in our updated manuscript that a new method is not the goal of our work in the introduction and hope that the reviewer agrees with our view.\n\n## Final Comments\n\nWe updated our manuscript with all the changes highlighted in blue and would be delighted to learn whether the reviewer agrees with us that these changes improve our work significantly. We again want to again thank for the reviewer's valuable feedback that helped us improve the paper and kindly ask them to consider reevaluating their score."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166107057,
                "cdate": 1700166107057,
                "tmdate": 1700166107057,
                "mdate": 1700166107057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u7gYQBh4Fd",
                "forum": "GpGJg1gsjl",
                "replyto": "qBZrONzXMC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Reviewer_fMcb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Reviewer_fMcb"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their reply. The update proposed by the authors to address my concern about the inaccessible knowledge of model parameters still seems problematic: for the aleatoric confidence, the learning of parameters should be conditioned on all the ground truth labels expect that of data point $i$, however in the equation (3) of the updated version it is conditioned on the labels of already observed instances, as is the total confidence. This is why the proposed optimality results remain unchanged."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694414708,
                "cdate": 1700694414708,
                "tmdate": 1700694414708,
                "mdate": 1700694414708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K5gTduSUmt",
                "forum": "GpGJg1gsjl",
                "replyto": "ptfTYm1MlN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3559/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the author for their answer. This is indeed a typo in our updated manuscript: instead of being conditioned on $y_O$ the parameters have to be conditioned on $y_{-I}$ when computing aleatoric confidence.  We adapted this in our document and decorated the $\\theta$ with a hat to highlight that this is a different parameter belief than in Equation 2. We hope this adequately adresses the remark. \n\nThe parameter learning from (partially unobserved) labels $y_{-i}$ is indeed problematic in the real world. There, one has to rely on approximations: One idea, besides our framework, would be using one of the newer graph foundation models for aleatoric learning where the model has a lot more data outside the current graph. Another idea would be to only model aleatoric uncertainty in the absence of graph effects, i.e. using a LLM on the text of e.g. CoraML. \n\nWhat we propose in our real-world experiment is a model that uses the pseudo labels from the learned classifier. That is, we learn $f(A, X, y_O)$ and infer all unseen data $\\hat{y}_U$. Then we condition the parameter learning of an auxiliary model (for aleatoric uncertainty) on $p(\\hat{\\theta} | A, X, y_O, \\hat{y}_U)$, i.e. we approximate $y_U$ with $\\hat{y}_U$. \n\nIn our study on real-world datasets, we in fact use **different parameter sets** $\\theta$ and $\\hat{\\theta}$ for estimating total and aleatoric confidence (LHS). This fits into the framework, as we assume different parameter beliefs for both uncertainty types $p(\\theta | A, X, y_O)$ and $p(\\hat{\\theta} |\u00a0A, X, y_O, \\hat{y}_U)$.\n\n\nOur results show that this method (LHS) and the other approximation we use (RHS) work well in practice: We again summarise the AUC scores for US and non-US method, where the best US strategy is highlighted bold and the best overall strategy (ie also non-US) is highlighted with a dagger in this table:\n\n|           | Random    | Coreset | Age | ANRMAB | GEEM | Ensemble | MC-Dropout | Energy | GPN | BGCN | Aleatoric | Ours (LHS) | Ours (RHS) |\n|-----------|-----------|---------|-----|--------|------|----------|------------|--------|-----|------|-----------|------------|------------|\n|           |  |  non-US |  non-US | non-US  | non-US |         US | US | US | US | US | US | US | US |\n| CoraML    | $63.85$ | $65.23$ | $67.56$         | $61.14$ | $71.39$         | $63.47$  | $59.17$       | $63.97$ | $54.75$ | $44.45$ | $65.66$ | $67.73$ | **71.45**$^\\dagger$ |\n| Citeseer  | $81.04$ | $79.38$ | $84.21$         | $81.03$ | $85.25^\\dagger$ | $82.94$  | $78.86$       | $81.59$ | $65.31$ | $58.68$ | $78.85$ | $83.26$ | **83.43**         |\n| Pubmed    | $56.79$ | $64.48$ | $69.20^\\dagger$ | $60.49$ | $64.82$         | $63.70$  | $58.67$       | $59.64$ | $58.82$ | $55.19$ | $61.55$ | $58.80$ | **64.36**         |\n| Photos    | $80.52$ | $82.32$ | $74.01$         | $80.92$ | $86.43^\\dagger$ | $84.46$  | $72.42$       | $74.66$ | $54.78$ | $70.83$ | $71.43$ | $71.07$ | **85.52**         |\n| Computers | $72.39$ | $71.53$ | $69.31$         | $71.62$ | $74.49^\\dagger$ | $68.38$  | $51.02$       | $59.62$ | $39.21$ | $58.64$ | $59.34$ | $62.16$ | **72.54**     |\n\nIn conclusion, we want to say that the main goal of our paper is to investigate why all SOTA US strategies fail on AL. We find that this is due to wrong modeling choices. With a simple fix, we outperform all SOTA US strategies. We hope that our findings spark even more sophisticated strategies for US on graphs, which we believe to be underdeveloped. In particular, we don\u2019t try to \u2018sell\u2019 our method as a novel AL strategy but provide a theoretically sound and empirically well-supported analysis of the relationship of US and AL that we hope to inspire more research in this area.\nWe firmly believe, and hope that the reviewer agrees with us, that this is of value to the scientific community and in our view worth more than \u2018just\u2019 proposing a new acquisition strategy."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3559/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740750816,
                "cdate": 1700740750816,
                "tmdate": 1700742422986,
                "mdate": 1700742422986,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]