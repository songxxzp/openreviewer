[
    {
        "title": "Constrained Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability"
    },
    {
        "review": {
            "id": "BvHPMZrzhz",
            "forum": "VNyIVrKrqv",
            "replyto": "VNyIVrKrqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5916/Reviewer_3BVe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5916/Reviewer_3BVe"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the methodology of addressing constrained reinforcement learning by framing sequential decision-making problems as probabilistic inference problems. The focus is on utilizing the Wasserstein distance as a measure to discern the difference between the approximate posterior distribution of the trajectories and the posterior conditioned on the optimality operator. However, due to the computational intractability of the Wasserstein distance, the study employs the Generalized Sliced Wasserstein Distance (GSWD) as a proxy for the original distance. This is subsequently expanded by incorporating a neural network to determine the hyperparameters in GSWD, resulting in a metric named the Adaptive Generalized Sliced Wasserstein Distance (AGSWD).\n\nIn the development of the algorithm, the paper introduces a distributional representation to represent the cumulative rewards' distribution. The Distributional Bellman operator is then utilized to upate the critic function within the algorithm. The paper also provides theoretical results for the method, demonstrating that AGSWD is a metric and justifying the global convergence as well as the convergence rate.\n\nMoreover, the paper conducts empirical studies on the performance of the proposed algorithm within both a simulated robotic control environment and a realistic Unmanned Aerial Vehicle control environment. These results highlight a leading convergence rate and robust, safe control performance."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The overall structure of the paper is generally easy to follow.\n\n2. The paper adeptly integrates a multitude of concepts such as Wasserstein distance approximation, interpretability, distributional Reinforcement Learning (RL), variational inference, constrained RL, and probabilistic inference. These topics have been at the forefront of recent research trends.\n\n3. The empirical results emphasize the superior performance of the proposed Adaptive Wasserstein Variational Optimization (AWaVO) relative to other baseline models in both simulated and realistic environments. This underscores the practical effectiveness of the proposed methodology in real-world applications."
                },
                "weaknesses": {
                    "value": "1. This paper covers a wide variety of concepts. However, there seems to be issues with key definitions and notations, which remain ambiguous or undefined, thus making several key ideas challenging to comprehend. For instance:\n\n- In Formula 2), the subscript 'i' is left undefined, and $b_i$ is also undefined.\n\n-  In Section 4.1, the definition of $q_\\theta$ appears flawed. There seems to be a duplicate probability over '$a$'. The second term should likely be $p(s|a,\\theta)$ instead.\n\n- In Section 4.1, the definition of $p(\\tau|O)$ seems incorrect. The term \"equivalence\" should likely be replaced with \"proportional to\" (please refer to formula (1)).\n\n- Formula 3) is incomprehensible without the definitions provided in the appendix. The hyperparameter $l$ and the function $\\mathcal(G)$ are undefined in the formula and its introductory explanation. It would be helpful to move this content from the appendix into the main body of the paper.\n\n- In Formula 3), it's unclear what the omitted term in $\\cdot$ is. According to the appendix, it's supposed to be a hyperparameter $l$. If that's the case, it's generally inappropriate to omit the hyperparameter in the formula.\n\n- The definition of $R_\\mu(l,\\tilde{\\theta})$ in the appendix is also confusing. Please verify whether the $x$ in the formula should be $\\mu$ and provide clarification.\n\n- In Section 4.2, for consistency with the expectation in the objective (4), there should be an expectation in the definition of the trajectory reward $\\tilde{r}(\\tau)$. Or is it intended to be the random variable of cumulative rewards, with the aim being to estimate the distribution? If so, perhaps it should not be termed a function.\n\n- The inference step of $p(\\theta|D)$ in Algorithm 2 needs clarification. Its reference cannot be found in Section 3. Please specify the exact location or formula. Please also distinguish between $\\theta$, $\\theta^\\mu$, and $\\theta^Q$. The rationale behind employing different updating methods for $\\theta$ and the others also needs to be elucidated.\n\nTypos:\n- The \"hupersurface\" before the formula (3) -> hypersurface.\n-  In Section 4.2, the defintion of $\\tilde{g}_i(\\tau)$ lacks a square bracket on the right.\n\n\n2. The theoretical results appear to lack key assumptions. Many conclusions are questionable and would benefit from further contemplation.\n\n- The proof of Proposition 1 claims that A-GSWD is a pseudometric rather than a metric. More significantly, it's questionable whether the proof can leverage the fact that \"Wasserstein distance is a metric,\" given that A-GSWD is defined between $\\mu$ and $\\nu$, not $\\mathcal{G}\\mu$ and $\\mathcal{G}\\nu$. If you swap $\\mu$ and $\\nu$, their corresponding hyperparameters in $\\mathcal{G}$ (as output by the neural network) will not be interchanged. Thus, symmetry might not generally hold.\n\n- In the proof of Proposition 3, the expectation following the max operator should be eliminated. If the action is selected based on an existing policy, the max operator becomes irrelevant.\n\n- In Equation (16), the first inequality does not generally hold. It often holds when $\\pi_{new}$ corresponds to the argmax Q in the Bellman optimality function. This point needs clarification.\n\n- The Wasserstein distance cannot be related to the discrepancy between quantiles in (18) unless $p$ (parameter) and $d$ (dimension) are both 1 in Wasserstein distance.\n\n- In Formula (19), Lemma 5.1 in Cai et al. (2019) relies on the linearity of the action-value function, which does not generally apply in value networks.\n\n3. The empirical results lack certain key definitions, and their credibility is questionable in several respects:\n\n- The constraint limit in the plots is undefined. Is this intended to represent the tolerance?\n\n- In Figure 3, the plots for cartpole, walker, and guard do not seem to demonstrate signs of convergence. The curves are still rising. An early stop might give the impression that the baseline methods are inferior, but they could potentially surpass the proposed method in the end. The incomplete nature of these experiments means that no definitive conclusions can be drawn.\n\n4. **Motivation.** The implementation of distributional Reinforcement Learning (RL) in this paper lacks a clear rationale. Despite its application, it remains unclear why it's necessary and what benefits it brings to the study.\n\n5. **Novelty.** While the introduction of AGSWD stands as a significant contribution, given that GSWD has already been investigated, simply integrating GSWD with neural network outputs doesn't constitute a major advancement in the field."
                },
                "questions": {
                    "value": "- How should we interpret $g_{rl}$? Its role in Formula (3) isn't clear. Is it intended to be a hyperparameter that defines the plane on which $\\Tilde{\\theta}$ lies? Or is it a term returned by the RL algorithm?\n\n- In the proof of Proposition 3, what is the relationship between the operators and the new and old policies? Is the output of the operators meant to be the new policy? And, what does $\\pi^{\\prime}$ represent?\n\n- In Formula (18), what does the symbol $H$ denote?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5916/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697908260778,
            "cdate": 1697908260778,
            "tmdate": 1699636629185,
            "mdate": 1699636629185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YnlgRjGHzS",
                "forum": "VNyIVrKrqv",
                "replyto": "BvHPMZrzhz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3BVe (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful and constructive feedback. Your concerns are addressed below. Please let us know if further clarification is needed.  \n\n***  \n>__Weaknesses:__ * In Formula 2), the subscript 'i' is left undefined, and $b_i$ is also undefined.\n\nThis aligns with comments from Reviewer x9MY. We have clarified that $b_i$ is a fixed limit for the i-th constraint (see the red highlight in Page 3).\n\n>* In Section 4.1, the definition of $q_\\theta$ appears flawed. There seems to be a duplicate probability over '$a$'. The second term should likely be $p(s|a,\\theta)$ instead.\n\nWe appreciate your thoughtful review. From a macro perspective, the definition of $q_\\theta$ is derived from Equation 1, where the probability $p(s,a|\\theta)$ originates directly from the second term on the right-hand side. Please consider our response in Reviewer x9MY (Q1) regarding the correctness of the term $p(s,a|\\theta)$ in Equation 1 and in underscoring the formulation as a Markov Decision Process.\n\nOn a micro level, we do not perceive any redundancy in probability over 'a' as $q(a)$ is not $p(a)$. The individual $q(a)$ serves as a probability function with respect to 'a,' representing the approximation (variational distribution) for the optimality likelihood $p(\\tau|\\mathcal{O})$.\n\nWe appreciate this may have led to some confusion, and to ensure transparency, we incorporate the use of $p(\\tau|\\theta)$ instead of $p(s,a|\\theta)$ in the revised version (see the red highlight in Equation 1, and the definition of $q_\\theta$ and $p(\\tau|\\mathcal{O})$), consistent with the response to Q1 in Reviewer x9MY.\n\n>* In Section 4.1, the definition of $p(\\tau|\\mathcal{O})$ seems incorrect. The term \"equivalence\" should likely be replaced with \"proportional to\" (please refer to formula (1)).\n\nThank you for your feedback. Indeed, in principle, \"proportional to\" is a more accurate formulation, and it was duly considered in our initial algorithmic framework construction. However, the implementation posed a challenge. Subsequently, we discovered that the direct \"equivalence\" rather than \"proportional to\" yields the same convergence performance in real implementation (as detailed in Lemma 2 and Theorem 2 for theoretical understanding). In response to the reviewer's comment, the revised version use \"proportional to\" instead of \"equivalence\" along with a clarification that \"equivalence\" is used in our actual implementation.\n\n>* Formula 3) is incomprehensible without the definitions provided in the appendix. ...\n\nWe move the definitions from the appendix into the main body of the paper. Please refer to the red highlights around Equation 3 for details.\n\n>* In Formula 3), it's unclear what the omitted term in $\\cdot$ is. According to the appendix, ...\n\nThe symbol $\\cdot$ signifies the hyperparameter $l$. We would like to retain the use of the symbol $\\cdot$ in Equation 3 for consistency with previous papers, such as [1] (Equation 11) and [2] (Equation 15). This choice is made to clearly depict the slicing process in Wasserstein metrics.\n\n>* The definition of $\\mathcal{R}_{\\mu}(l,\\widetilde{\\theta})$ in the appendix is also confusing. Please verify ...  \n\nWe thoroughly verify that $x$ in the formula is correct as $\\mu$ represents probability measures and $x$, defined as $\\{x\\in \\mathbb{R}^d|\\langle x,\\widetilde{\\theta} \\rangle=l\\}$, denotes hyperplanes. It is essential to integrate over $\\mu$ rather than $x$. For further clarification, please refer to both [2] (Equation 6/7) and [3]. We appreciate your observation.\n\n>* In Section 4.2, for consistency with the expectation in the objective (4), there should be an expectation in the definition of the trajectory reward. ...  \n\nYes - we have fixed it in the revised version. Thank you for the comment.  \n\nReference:  \n\n[1] Bonneel, Nicolas, et al. \"Sliced and radon wasserstein barycenters of measures.\" Journal of Mathematical Imaging and Vision 51 (2015): 22-45.  \n\n[2] Chen, Xiongjie, Yongxin Yang, and Yunpeng Li. \"Augmented Sliced Wasserstein Distances.\" International Conference on Learning Representations. 2021.  \n\n[3] Kolouri, Soheil, et al. \"Generalized sliced wasserstein distances.\" Advances in neural information processing systems 32 (2019)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243384401,
                "cdate": 1700243384401,
                "tmdate": 1700243384401,
                "mdate": 1700243384401,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1WeSOmohXy",
                "forum": "VNyIVrKrqv",
                "replyto": "BvHPMZrzhz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3BVe (Part 2)"
                    },
                    "comment": {
                        "value": ">* The inference step of $p(\\theta|D)$ in Algorithm 2 needs clarification. Its reference cannot be found in Section 3. Please specify the exact location or formula. ...  \n\nThanks for your comment.   \n\nRegarding the reference of $p(\\theta|D)$: this information can be found in the second paragraph of Section 3 (where we state, \"the prior probability $p_{D}(\\theta)$ is derived from the posterior probability $p(\\theta|D)$, where the parameter $\\theta$ is inferred from the training dataset $D$\"). The term 'inference' here signifies the process of obtaining parameters $\\theta$ from the dataset $D$, which, in our specific implementation, corresponds to the training process to determine the parameter values of ${\\theta}=[\\theta^\\mu,\\theta^Q]$ in actor and critic networks.  \n\nDistinguishing $\\theta$, $\\theta^\\mu$ and $\\theta^Q$: The term $\\theta$ serves as a general representation of the parameters. Specifically, in our implementation, actor-critic networks are employed, and the parameters are denoted as $\\theta^\\mu$ and $\\theta^Q$ (refer to 'Initialize:' in Algorithm 2).  \n\nRationale of different updating methods: The specific updating methods are elucidated in Section 4.2 (refer to 'Policy Updating'), where the detailed gradients of the actor and critic networks, $\\delta_{\\theta^\\mu}$ and $\\delta_{\\theta^Q}$, are defined in Equation 10 in Appendix B.  \n\n***  \n>* Typos:  \n\nWe have fixed in the revised version. Thank you!  \n\n*** \n__Theoretical results:__ \n>* The proof of Proposition 1 claims that A-GSWD is a pseudometric rather than a metric. More significantly, ...  \n  \nThanks for this detailed and thoughtful comment. For clarity, it can be proven that A-GSWD is still symmetric, given that it is defined between $\\mu$ and $\\nu$. To prove symmetry, since the k-Wasserstein distance is a metric [1]:  \n  \n$$W_k\\left(\\mathcal{G}\\_\\mu(\\cdot,\\widetilde{\\theta};g_{rl}),\\mathcal{G}\\_\\nu(\\cdot,\\widetilde{\\theta};g_{rl})\\right)=W_k\\left(\\mathcal{G}\\_\\nu(\\cdot,\\widetilde{\\theta};g_{rl}),\\mathcal{G}\\_\\mu(\\cdot,\\widetilde{\\theta};g_{rl})\\right)$$  \n  \nThus, there exists:  \n  \n$$\\bf{A-GSWD}\\_{k}(\\mu,\\nu)=\\left(\\int\\_{\\mathcal{R}\\_{\\widetilde{\\theta}}} W\\_k\\left(\\mathcal{G}\\_\\mu(\\cdot,\\widetilde{\\theta};g_{rl}),\\mathcal{G}\\_\\nu(\\cdot,\\widetilde{\\theta};g_{rl})\\right)\\mathrm{d}\\widetilde{\\theta}\\right)\\^{\\frac{1}{k}}=\\left(\\int\\_{\\mathcal{R}\\_{\\widetilde{\\theta}}} W\\_k\\left(\\mathcal{G}\\_\\nu(\\cdot,\\widetilde{\\theta};g_{rl}),\\mathcal{G}\\_\\mu(\\cdot,\\widetilde{\\theta};g_{rl})\\right)\\mathrm{d}\\widetilde{\\theta}\\right)\\^{\\frac{1}{k}}=\\bf{A-GSWD}\\_{k}(\\nu,\\mu)$$   \n  \nTherefore, symmetry holds. We emphasize this in the proof of proposition 1 in Appendix C.3 (please see the red highlight in the revised manuscript).  \n  \n>* In the proof of Proposition 3, the expectation following the max operator should be eliminated. ...  \n  \nTo clarify, $\\pi$ represents a policy in the Policy Improvement process (to get the optimized policy that achieves the highest value), not a stationary policy. Consequently, we believe that the expectation should not be eliminated. \n  \n>* In Equation (16), the first inequality does not generally hold. It often holds ...  \n  \nThe first inequality in Equation 16 relies on Equation 15 (please note that 'According to Equation 15 and Equation 8, it yields: ...' ). We highlight (red) this relationship in the proof in Appendix C.3. Thank you for highlighting this.  \n  \n>* The Wasserstein distance cannot be related to the discrepancy between quantiles in (18) unless $p$ (parameter) and $d$ (dimension) are both 1 in Wasserstein distance.  \n  \nYes, we agree with this comment. Our proof in Equation 18 is consistent with this statement, and uses the Wasserstein distance based on $W_1$ ($p=1$) and one-dimensional quantiles $q_t^{i,*}$ ($d=1$). We appreciate the detailed feedback, and in the revised version, we highlight (red) this alignment in the proof.  \n  \n>* In Formula (19), Lemma 5.1 in Cai et al. (2019) relies on the linearity of the action-value function, ...  \n  \nWe agree with this comment. In Equation 19, both the terms $f_i^{(H)}$ and $f_{0,i}^{(H)}$ represent quantiles, serving as local linearization that can be applied with Lemma 5.1 in Cai et al. (2019). In the revised version, we emphasize this point (red highlight) in the proof.  \n  \n>* The constraint limit in the plots is undefined. Is this intended to represent the tolerance?  \n  \nThe constraint limit relies on the benchmark setting [2] (Acrobot) and the specific tasks, with the intention of facilitating a clear comparison of constraint convergence. We provide one additional sentence in Section 6 to clarify this in the revised version.\n  \nReference:  \n  \n[1] Villani, C\u00e9dric. Optimal transport: old and new. Vol. 338. Berlin: springer, 2009.  \n  \n[2] Xu, Tengyu, Yingbin Liang, and Guanghui Lan. \"Crpo: A new approach for safe reinforcement learning with convergence guarantee.\" International Conference on Machine Learning. PMLR, 2021."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243820916,
                "cdate": 1700243820916,
                "tmdate": 1700243820916,
                "mdate": 1700243820916,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vmhkOi5MX1",
                "forum": "VNyIVrKrqv",
                "replyto": "SAe7gbGTxF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_3BVe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_3BVe"
                ],
                "content": {
                    "comment": {
                        "value": "It appears that most of my concerns are indeed valid, but there are a few places that require further discussion.\n\n- \"The individual q(a) serves as a probability function with respect to 'a,' representing the approximation (variational distribution) for the optimality likelihood .\" In fact, I don't think it is correct. Please show me the derivation of your variational inference.\n\n- Regarding the symmetry of AGSWD, **your new proof appears to reiterate my concern**. Please take note that in your proof, $\\mathcal{G}\\mu$ and $\\mathcal{G}\\nu$ should be integral parts of the matrix and they are parameterized by the neural network. The question then is, when you interchange $\\mu$ and $\\nu$, how can you simultaneously swap $\\mathcal{G}\\mu$ and $\\mathcal{G}\\nu$? **Be aware that you metric is evaluating the divergence between $\\mu$ and $\\nu$, instead of \\mathcal{G}\\mu(\\cdot)$ and $\\mathcal{G}\\nu$**. Following your line of reasoning, all metrics could be considered symmetric, as long as they can be expressed in some form of the Wasserstein metric. \n\n- \"Consequently, we believe that the expectation should not be eliminated.\" **Please doublecheck your notation in equation (15)**, if you take the expection over policy, it appears $\\mathbb{E}_{\\pi}(Q^\\pi(s,a))=V^\\pi(s)$. then what's the meaning of **$\\max_a V^\\pi(s)$**? If you want to achieve policy improvement, please just follow the Bellman Optimality Equation.\n\n- \". As each quantile can be viewed as a form of local linearization\". I have not heard of this, please point me to the reference notes or papers.\n\n- \"The constraint limit relies on the benchmark setting [2] (Acrobot) and the specific tasks, with the intention of facilitating a clear comparison of constraint convergence.\" If \"constraint limit\" is indeed part of the benchmark, it would be helpful if you could provide more detail about what it signifies or represents, and how you selected these hyperparameters. Initially, I assumed it referred to \"tolerance\" in the paper, but based on your response, it seems that my assumption was incorrect.\n\n- Regarding \"In Figure 3, the plots for cartpole, walker, and guard do not seem to demonstrate signs of convergence\". The proper response is to expand the running episodes of your experiments and present the results once the algorithm under evaluation has converged."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401292578,
                "cdate": 1700401292578,
                "tmdate": 1700401292578,
                "mdate": 1700401292578,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1VMYJIuzMF",
                "forum": "VNyIVrKrqv",
                "replyto": "BvHPMZrzhz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3BVe (Part 2)"
                    },
                    "comment": {
                        "value": ">* \"The constraint limit relies on the benchmark setting (Acrobot) and the specific tasks, ... ...\n\nThank you for this comment. Initially, we established the constraint limit with the intention of facilitating a straightforward comparison of constraint convergence, as the constraint limit is introduced in the benchmark. After reviewing your comment, we recognize a potential connection with \"tolerance\", wherein the agent's stable performance for specific tasks occurs when it operates below the constraint limit. In our view, the constraint limit serves as a lower boundary that indicates the level of tolerance the constraints can withstand. We believe there might be advantages in setting a fixed limit, $b_i$, by decoupling the cumulated value into each specific fixed limit. This aspect will be explored in future work. We add this clarification in Section 6 and Appendix D.2.\n\n>* Regarding \"In Figure 3, the plots for cartpole, walker, and guard do not seem to demonstrate signs of convergence\". The proper response is to expand the running episodes of your experiments and present the results once the algorithm under evaluation has converged.\n\nThank you for this suggestion. We have expanded our experiments in cartpole, walker and drone to showcase the convergent results. We have modified Figure 3 in the updated version to reflect this.\n\nReference:\n\n[1] Kolouri, Soheil, Gustavo K. Rohde, and Heiko Hoffmann. \"Sliced wasserstein distance for learning gaussian mixture models.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n\n[2] Kolouri, Soheil, et al. \"Generalized sliced wasserstein distances.\" Advances in neural information processing systems 32 (2019).\n\n[3] Chen, Xiongjie, Yongxin Yang, and Yunpeng Li. \"Augmented Sliced Wasserstein Distances.\" International Conference on Learning Representations. 2021.\n\n[4] Bellemare, Marc G., Will Dabney, and R\u00e9mi Munos. \"A distributional perspective on reinforcement learning.\" International conference on machine learning. PMLR, 2017.\n\n[5] Koenker, Roger, and Gilbert Bassett Jr. \"Regression quantiles.\" Econometrica: journal of the Econometric Society (1978): 33-50.\n\n[6] Gannoun, Ali, J\u00e9r\u00f4me Saracco, and Keming Yu. \"Comparison of kernel estimators of conditional distribution function and quantile regression under censoring.\" Statistical Modelling 7.4 (2007): 329-344.\n\n[7] Koenker, Roger. Quantile regression. Vol. 38. Cambridge university press, 2005."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561907787,
                "cdate": 1700561907787,
                "tmdate": 1700607922498,
                "mdate": 1700607922498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DD3iVS7Dc1",
            "forum": "VNyIVrKrqv",
            "replyto": "VNyIVrKrqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5916/Reviewer_e4UV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5916/Reviewer_e4UV"
            ],
            "content": {
                "summary": {
                    "value": "Treating RL problems in terms of variational inference is a promising approach\nto building interpretable policies, but it suffers from high computational\nexpense. In particular, the cost of computing distance metrics between\nprobability distributions over high dimensional spaces is intractable. This\npaper proposes a new metric (A-GWSD) on probability distributions which can be\ncomputed efficiently and used to solve RL-as-inference problems. This metric is\nused to solve constrained RL problems. This algorithm is shown to theoretically\nsatisfy a global convergence property, and empirically achieves similar rewards\nand constraint violation rates to state-of-the-art safe RL algorithms. In\naddition, the proposed approach yields a clear interpretation of policy actions\nbased on conditional probability evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Interpretability is a key challenge in reinforcement learning, and this paper\noffers a promising approach to providing explanations for agent behavior.\n\nConstraints are also important in real-world reinforcement learning. This paper\nproposes a way to extend a popular RL framework (RL as inference) to the\nconstrained setting.\n\nThe experimental results show that improvements in interpretability can be\nachieved with minimal sacrifices in terms of rewards or constraint violations."
                },
                "weaknesses": {
                    "value": "Given that the main improvement over prior work is in intpretability, I would\nhave liked to see more discussion of interpretability in the experimental\nresults. There is some information in Figure 4 (but see the questions below),\nbut I think the paper would benefit from some discussion of intepretability in\nthe other environments."
                },
                "questions": {
                    "value": "I found it difficult to understand Figure 4, which is where the key advantage of\nthis approach over prior work is shown. Some explanation of why this diagram is\nevidence of improved interpretability would be helpful. I can see that roughly\nspeaking, when the wind estimation in 4(a) is higher, the probability in 4(b) is\nhigher as well, which demonstrates some sensitivity to the wind. But the two\ngraphs do not line up that closely and I'm not sure if I'm interpreting them\ncorrectly. What should I be looking for in this figure?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5916/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5916/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5916/Reviewer_e4UV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5916/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788325717,
            "cdate": 1698788325717,
            "tmdate": 1699636629083,
            "mdate": 1699636629083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p5v1N21Ao9",
                "forum": "VNyIVrKrqv",
                "replyto": "DD3iVS7Dc1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e4UV"
                    },
                    "comment": {
                        "value": "We appreciate your positive feedback. Your concerns are addressed below. Please let us know if further clarification is needed. \n\n \n\n*** \n\n>* __Question__: I found it difficult to understand Figure 4, which is where the key advantage of this approach ... ... What should I be looking for in this figure? \n\n \n\nFigure 4 (a) is provided as a reference; i.e., the curves give our estimated values of external aerodynamic forces (winds) in real time. These are derived from the signals collected from onboard sensors. In Figure 4 (b), we illustrate the quantitative impact of external forces, denoted as $L\\_0$, on the current sequential decisions, i.e., the planned trajectory, denoted as $\\tau$, and the corresponding Pulse Width Modulation signals that are fed to the motors.  \n\n \n\nFigure 4 (b) becomes particularly important when the agent makes suboptimal decisions leading to events like crashes or collisions in quadrotors. These curves can be leveraged for interpretation and conducting quantitative analyses of specific environmental factors, such as winds and obstacles, in order to understand their magnitudes of influence on current decisions taken. \n\n \n\nIn more detail, consider Reference State 02 (RS 02) as an example. In Flight Task 3 (FT 3), represented by the red curve, we observe how the external forces (i.e., the aerodynamic effects generated by the winds and obstacles) influence the current trajectory planning decisions. With RS 02 situated in an area featuring a combination of wind and obstacles, the value is approximately $0.5$. This implies that the current planned trajectory (sequential decisions) has about a $50%$ probability of being influenced by the aerodynamic effects. Comparing FT 1 and FT 2, where the values in RS 02 are approximately $0.24$ (FT 1) and $0.15$ (FT 2), we can decouple the aerodynamic effects generated by the wind on the body (FT 1) and obstacles (FT 2), respectively. Quantitatively, the red $p(\\tau|L_0)\\_{FT3}$ is approximately equal to the sum of $p(\\tau|L_0)\\_{FT1}$ (only wind) and $p(\\tau|L_0)\\_{FT2}$ (only obstacles). \n\n \n\n*** \n\n>* __Weaknesses:__ Given that the main improvement over prior work is in intpretability, ... There is some information in Figure 4 (but see the questions below), but I think the paper would benefit from some discussion of intepretability in the other environments. \n\n \n\nThank you for raising this. Although there is limited scope to expand in this paper, we are excited to implement and examine our work for various real-world environments and applications, particularly focusing on safety-critical scenarios with variable uncertainties. Examples of such applications include self-driving ground vehicles, aerial delivery aircraft systems and other robotic platforms. We have commented on this as part of our future work in Section 7."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240291443,
                "cdate": 1700240291443,
                "tmdate": 1700242510580,
                "mdate": 1700242510580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k7s3GKUlyg",
                "forum": "VNyIVrKrqv",
                "replyto": "p5v1N21Ao9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_e4UV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_e4UV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. This helps clarify the interpretability claims for me. I look forward to seeing future work in more diverse and safety-critical experimental settings.\n\nAm I correct in interpreting FT 3 as a kind of combination of FT 1 and FT 2 (in that FT 1 has wind and FT 2 has obstacles)? If so, is there a reason I shouldn't expect $p(\\tau | L_0)$ for FT 3 to _always_ be approximately the sum of $p(\\tau | L_0)$ for FT's 1 and 2, rather than just at RS 02? It seems this property does no hold at other points along the trajectory."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512212994,
                "cdate": 1700512212994,
                "tmdate": 1700512212994,
                "mdate": 1700512212994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iw857q3CtT",
                "forum": "VNyIVrKrqv",
                "replyto": "unztpYkTSw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_e4UV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_e4UV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification. I will keep my score in favor of acceptance. In general, I like this technique and the results presented are promising, but I still believe the paper would be stronger with a more extensive discussion of interpretability."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588291759,
                "cdate": 1700588291759,
                "tmdate": 1700588291759,
                "mdate": 1700588291759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3PEO48nu9V",
            "forum": "VNyIVrKrqv",
            "replyto": "VNyIVrKrqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5916/Reviewer_x9MY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5916/Reviewer_x9MY"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a distributional approach to infer a policy in the constrained reinforcement learning setting. The central idea is to alternatively perform Wasserstein variational inference (WVI) and distributional policy optimization (PPO-DR). WVI learns a variational approximation $q(a)$ to the optimality likelihood $p(O|\\tau)$ (similar to the control as inference framework [1]) while PPO-DR maximizes the expected reward within the feasible region, and minimizes the expected constraint outside the feasible region (using distributional networks). The variational approximation $q(a)$ is represented by the critic distribution, while the Wasserstein distance is computed using the hypersurfaces given by the actor distribution. Overall, the idea of using adaptive GSWD for distributional constrained RL is interesting, and supported by theoretical and empirical results, but the work can be improved in terms of clarity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Extensive discussion on convergence results\n2. The variational inference process is interesting, and it minimizes a Wasserstein distance (better for distributional RL) while ensuring that the actor distribution is used in computing the distance.\n3. Real world experiments"
                },
                "weaknesses": {
                    "value": "On page 3, last line: \"where, upon satisfying the constraints, the agent enters a state considered as safe\". In my understanding, in the expected constraint formulation (Equation 2), if the constraints are satisfied, then the policy is considered \"safe\". \"Safe/unsafe\" states typically refer to a CMDP formulation with constraint sets, i.e. a part of the state-action space is safe, and the rest is unsafe. While it is useful to obtain a graphical model framework equivalent to the control as inference framework [1], a more appropriate justification of Figure 1 should have been that the optimality variables are influenced by the constraint, rather than saying that the agent enters a safe state. This means that $p(\\tau|O)$ becomes 0 as soon as the constraint is not satisfied in expectation. The authors do not model the probability in this way, but rather learn a policy by maximizing reward when within the feasible region, and minimizing constraint when outside the feasible region (to get within the feasible region). This makes sense intuitively, but equivalence to constrained RL (equation 2) is not formally established."
                },
                "questions": {
                    "value": "1. In Equation 1, isn't it more appropriate to use $p(\\tau|\\theta)$ instead of $p(s,a|\\theta)$ for the second term of the right hand side? \n2. In equation 2, what is $b_i$? Maybe I missed it, but why are there two thresholds?\n3. (Suggestion) Page 4, after equation 3: \"$\\tilde \\theta$ are the returns from the actor networks\" could be re-worded. Returns have a specific meaning in RL literature, and in this context, I think return just means the output of the actor network, and not the usual return. (please ignore if my understanding is incorrect)\n4. Maybe I missed this, but how are the individual $p(L_i|D)$ obtained (in the flight task setup)?\n\n**References**\n\n1. Reinforcement learning and control as probabilistic inference: Tutorial and review, Levine (2018)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "_"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5916/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5916/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5916/Reviewer_x9MY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5916/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698909999161,
            "cdate": 1698909999161,
            "tmdate": 1699636628950,
            "mdate": 1699636628950,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S2qess2NWX",
                "forum": "VNyIVrKrqv",
                "replyto": "3PEO48nu9V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x9MY"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful and positive feedback. Your concerns are addressed below. Please let us know if further clarification is needed. \n\n***  \n>* __Weaknesses:__  \n\nWe are grateful for your assessment of our research. We agree with many of your statements like '\"Safe/unsafe\" states typically refer to a Constrained Markov Decision Process formulation with constraint sets' and 'learn a policy by maximizing reward when within the feasible region, and minimizing constraint when outside the feasible region'. However, we would like to address some specific statements to improve our shared understanding:\n\n> 1. a more appropriate justification of Figure 1 should have been that the optimality variables are influenced by the constraint,\n\nYes - we concur that 'the optimality variables are influenced by the constraint'. To elaborate, when $\\mathcal{O}=1$, it signifies that the trajectory $\\tau$ is __not only optimized but also satisfied the constraints__. In other words, it denotes an ideally optimized trajectory that meets the prescribed constraints. A clear explanation has been highlighted (red) at the outset of Section 3 on page 3. Thank you for highlighting this.\n\n> 2. rather than saying that the agent enters a safe state. ... This makes sense intuitively, but equivalence to constrained RL (equation 2) is not formally established.\n\nIt is important to clarify that, in principle, constrained Reinforcement Learning (RL) cannot guarantee full adherence to constraints. Throughout the learning process, an agent is awarded or penalized, pushing it towards a 'safe' state. Consequently, there exists a certain probability of violating constraints in states or actions. This challenge poses a significant barrier for applying constrained RL in safety-critical applications.\n\nMore importantly, while AWaVO does not ensure absolute safety, we offer a clear theoretical insight in Theorem 2. This theorem quantitatively interprets the violation: the convergence rate will follow a sublinear $\\Theta(1/\\sqrt{T})$ if constraints are violated with an error of $\\Theta(1/{m^{\\frac{H}{4}}})$ (which decreases as the width and layer count of the neural network, denoted as $m$ and $H$ respectively, increase), with a probability of at least $1-\\delta$. For detailed information, we refer to Equations 26-28 in the proof of Theorem 2.  \n\nConsequently, we believe that AWaVO is not 'equivalent' to Equation 2, but surpasses constrained RL in terms of interpretability. \n\n***  \n>* __Questions:__ 1. In Equation 1, isn't it more appropriate to use $p(\\tau|\\theta)$ instead of $p(s, a|\\theta)$ for the second term of the right hand side?\n\nYes - we appreciate this insightful interpretation of Equation 1. Using $p(\\tau|\\theta)$ in the second term would bring Equation 1 closer in form to Bayes' theorem, making it more intuitively understandable. Essentially, the formation process of the second term outlines a Markov Decision Process, wherein $p(a_t|s_t,\\theta)$ and $p({s}_{t+1}|{s}_t,{a}_t)$ represent the policy state $\\pi$ and transitions, respectively.   \n\nThe rationale behind our use of $p(s, a|\\theta)$ in this context lies in the Markov property, indicating that the present state encompasses all the information necessary to predict future states, irrespective of historical states. Consequently, we adopt $p(\\tau|\\theta)$ in Equation 1 (see the red highlight in our revision) and explicitly clarify the Markov property for better comprehension. \n\n>2. In equation 2, what is $b_i$? Maybe I missed it, but why are there two thresholds?\n\n$b_i$ is a fixed limit for the i-th constraint, while $\\tau_c$ is the tolerance. The reason we use the tolerance $\\tau_c$ here is for the convergence process when the approximated constraints $J_{g,i}(\\pi)$ violate the fixed constraint limit $b_i$, necessitating the bounding of errors (cf. Equation 27 in the proof of Theorem 2).\n\n>3. (Suggestion) Page 4, after equation 3: ... Returns have a specific meaning in RL literature, ... I think return just means the output of the actor network, ...  \n\nThanks for highlighting this. We use 'outputs' in the revised paper when conveying this concept.  \n\n>4. Maybe I missed this, but how are the individual $p(L_i|D)$ obtained (in the flight task setup)?\n\nThe individual $p(L_i|D)$ represents the agent's capability to detect the factor $L_i$ within the environments $D$, typically acquired from sensors or perception modules. In the context of our flight task, $L_0$ represents external forces, and $D$ represents depth images sampled from a depth camera, namely the Intel Realsense D435i. Therefore, $p(L_i|D)$ individually represents the estimation of external forces $L_i$ within the environment $D,' and this estimation is obtained through VID-Fusion [1].  \n\nReference: \n\n[1] Ding, Ziming, et al. \"Vid-fusion: Robust visual-inertial-dynamics odometry for accurate external force estimation.\" 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240046267,
                "cdate": 1700240046267,
                "tmdate": 1700240046267,
                "mdate": 1700240046267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sqz1pFoNp4",
                "forum": "VNyIVrKrqv",
                "replyto": "S2qess2NWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_x9MY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_x9MY"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "In the new text, the authors clarify that \"$\\mathcal O_t=1$ signifies that the trajectory $\\tau$ is both optimized and compliant with the constraints\". While this tries to address my earlier comment that optimality variables are influenced by the constraint, I don't think this is still the right formulation. With no constraints, the optimality variables induce a distribution $p(\\mathcal O_t=1|s_t,a_t)=e^{r(s_t,a_t)}$ as given in [1], but with expected constraints, how would this change? I suspect that since expected constraints are not satisfied per step, we cannot write a similar expression for each $\\mathcal O_t$. \n\n>  Throughout the learning process, an agent is awarded or penalized, pushing it towards a 'safe' state. \n\nAgain, since this is the expected constraint formulation, there is no safe state. Safeness is ensured if the current policy yields trajectories that in expectation (across several trajectories) satisfies the expected constraint. I will still suggest to change this wording, if possible.\n\nThanks for the rest of the clarifications. I'm inclined to keep my score as some of my concerns remain.\n\n**References**\n\n1. Reinforcement learning and control as probabilistic inference: Tutorial and review, Levine (2018)"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683423505,
                "cdate": 1700683423505,
                "tmdate": 1700683423505,
                "mdate": 1700683423505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rWUMZVXS24",
                "forum": "VNyIVrKrqv",
                "replyto": "3PEO48nu9V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x9MY - thank you for your response"
                    },
                    "comment": {
                        "value": "We value your additional feedback, and the remaining concerns you raised are addressed below. Please let us know if further clarification is needed.\n\n>* With no constraints, the optimality variables induce a distribution ... ... how would this change?\n\nThank you for your thoughtful comment. To enhance clarity, we have revised the statement in Section 3 (page 3), introducing the optimality family $\\mathcal{O}\\_{t}=\\\\{\\mathcal{O}\\_{r,t},\\mathcal{O}\\_{g,t}\\\\}\\in\\\\{0,1\\\\}$ and specifying that $\\mathcal{O}\\_{r,t}=1$ and $\\mathcal{O}\\_{g,t}=1$ signify the trajectory $\\tau$ is optimized and compliant with the constraints, respectively. Then, we present the following clarifications:\n\n1) With no constraints, the optimality likelihood induce a distribution as $p(\\mathcal{O}\\_{r,t}=1|s\\_t,a\\_t)$;\n\n2) With expected constraints, the optimality likelihood induce a distribution as $p(\\mathcal{O}\\_{g,t}=1|s\\_t,a\\_t)$.\n\nPlease also refer to Equation 4 and Section 4.2 for a slightly revision.\n\n>* ... ... Safeness is ensured if the current policy yields trajectories that in expectation (across several trajectories) satisfies the expected constraint. I will still suggest to change this wording, if possible.\n\nCertainly, and thank you for this suggestion. To provide clarity on the formulation of expected constraints, we have revised the statement in Section 3, found at the beginning of page 4."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696814937,
                "cdate": 1700696814937,
                "tmdate": 1700698629953,
                "mdate": 1700698629953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aoDtqWftjC",
                "forum": "VNyIVrKrqv",
                "replyto": "3PEO48nu9V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_x9MY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Reviewer_x9MY"
                ],
                "content": {
                    "title": {
                        "value": "Further comments regarding response"
                    },
                    "comment": {
                        "value": "Thanks for your clarification. I think the language is better now, but again, like I mentioned before, the constraints are expected, so they hold for a batch of trajectories, not for a single trajectory. Therefore, the optimality distributions eg. $p(\\mathcal O_g|\\tau)$ should rather be for multiple trajectories, not a single trajectory, right?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698674486,
                "cdate": 1700698674486,
                "tmdate": 1700698694787,
                "mdate": 1700698694787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pTa5UiF7Jr",
                "forum": "VNyIVrKrqv",
                "replyto": "3PEO48nu9V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5916/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x9MY - thank you for your addtional comment"
                    },
                    "comment": {
                        "value": "Thanks for your comment. Indeed, both the distributions $p(\\mathcal{o}\\_r|\\tau)$ and $p(\\mathcal{o}\\_g|\\tau)$ hold across multiple trajectories. We have made a slight revision in Section 3, located in page 3, to clarify the 'expected formulation' within the context of 'several trajectories'. If further clarification is needed please let us know."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5916/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701243081,
                "cdate": 1700701243081,
                "tmdate": 1700701495480,
                "mdate": 1700701495480,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]