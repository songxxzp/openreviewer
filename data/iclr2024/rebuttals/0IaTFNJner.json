[
    {
        "title": "On the Embedding Collapse When Scaling up Recommendation Models"
    },
    {
        "review": {
            "id": "Sb5jATn86P",
            "forum": "0IaTFNJner",
            "replyto": "0IaTFNJner",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission668/Reviewer_eFdv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission668/Reviewer_eFdv"
            ],
            "content": {
                "summary": {
                    "value": "This paper observes scaling the embedding dimensionality does not lead to satisfactory improvement in recommendation models and claims this is due to the dimensional collapse problem where the learned embedding vector spans only on lower dimensional subspace. The paper further claims such problem will propagate by feature interaction module when a feature embedding interacts with collapsed embeddings and regularization could mitigate collapse but harms the performance. Thus this paper proposes an alternative approach to address the embedding collapse issue, namely to replace single embedding in the original model with multiple embeddings. The paper shows the proposed approach lead to higher \"information abundance\" which is ratio between sum of absolute singular values and the largest singular value, indicate the \"spreadness\"/concentration of the singular value distribution. The paper also applied the proposed approach on several recommender models on two datasets, shows a absolute improvement of AUC on 1e-3 ~ 1e-4 level, when scaling the number of multi-embeddings. \n\ninitial recommendation: weak reject, for reasons please see the weak points."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The claim about the dimensional collapsing problem when scaling the dimensionality of feature embeddings in recommender system is reasonably supported in the paper and sound.\n* The analysis about the trade off between feature embedding collapsing and the regularization level of feature interaction is interesting and reasonable. \n* The paper is easy to follow"
                },
                "weaknesses": {
                    "value": "* The computation of \"information abundance\" for multi-embedding setting is not clearly defined in the paper. As show in the Figure 1.b for features with low predictive power would have embedding with low \"information abundance\" ratio, and scaling the dimensionality would further lower the ratio. Thus if the ratio for multi embedding is computed by averaging over the small embeddings, the lower ratios for some small embeddings will be compensated. This possibility makes the proposed abundance ratio less reliable. \n* The idea of multi-facet embedding or polysemy embedding has been studied quite extensively in the past. From network embedding (Liu et al. Is a single vector enough? exploring node polysemy for network embedding) to recommender systems (Weston et al. Nonlinear latent factorization by embedding multiple user interests). However, non of the related work on multi-embedding has been discussed in the paper.\n* The finding 1 was claimed to \"applicable to general recommendation models instead of only the models with sub-embeddings\" without being linked to any evidences. Also, I personally find find 1 is much a theory rather than a \"law\"."
                },
                "questions": {
                    "value": "* How is the \"information abundance\" for multi-embeddings computed? \n* How would the proposed multi-embedding approach be positioned in the literature?\n* Please point out the evidence for the claim quoted in the third weak point."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736644154,
            "cdate": 1698736644154,
            "tmdate": 1699635994167,
            "mdate": 1699635994167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DomGkScB8g",
                "forum": "0IaTFNJner",
                "replyto": "Sb5jATn86P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eFdv (Part I)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. We have addressed the comments in the rebuttal below.\n\n**Q1:** How the information abundance for the setting of ME is computed.\n\nWe acknowledge the reviewer's concern regarding the computation of information abundance under the ME settings in our paper. In our initial version, we did not provide sufficient clarity on this aspect, which may have led to confusion.\n\nTo address this issue, we have revised our paper and would like to provide a more accurate and detailed explanation of how the information abundance is computed in the ME setting. Specifically, under the ME settings, we *concatenate the matrices of all embedding sets along the embedding size dimension*. For instance, in the case of calculating the information abundance with a 10x size for ME, as shown in Fig.9 (a), we concatenate all the 10 matrices, each with an embedding size of 10, and then calculate the information abundance of the resulting concatenated matrix. This approach ensures that the information abundance of both the ME and SE (Single Embedding) settings is calculated *under exactly the same matrix shape*, enabling a **fair** comparison between the two.\n\n**Q2:** Related works of multi-facet embedding and polysemy embedding.\n\nWe appreciate the reviewer for bringing up the works related to multi-facet embedding and polysemy embedding. We have taken their suggestions into account and incorporated these works into our paper. It is important to noted that the ME design is **only** a small part of our work to reflect how our theory and analysis are applicable to practical scenarios. Our work also makes **significant contributions** to studying the *collapse phenomenon behind the lacked scalability* of recommender models and the *two-sided effect of feature interactions* in recommender models, which are **not covered** by the two works the reviewer provided.\n\nIn comparing the ME design with the two works mentioned, we have identified both similarities and differences:\n\n- Similarity: Both ME and Multi-Facet Embedding & Polysemous Embedding introduces *multiple embedding sets* to improve the performance. Multi-Facet Embedding also aims for *better dataset utilization*.\n- Difference (ME vs. Multi-Facet Embedding): While Multi-Facet Embedding utilizes graph decomposition as **prior** knowledge and **explicitly** models the node-aware transition probability to obtain various embedding representations, the ME design focuses more on the collapse phenomenon and works **generally** without prior knowledge or explicit variation modeling.\n- Polysemy Embedding is **specifically** designed for factorization machines (FM) and aims to introduce more non-linearity into FM models. On the other hand, the ME design is applicable to general recommendation models and focuses on improving **dataset utilization**."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203666918,
                "cdate": 1700203666918,
                "tmdate": 1700203666918,
                "mdate": 1700203666918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cdocMnq2dm",
                "forum": "0IaTFNJner",
                "replyto": "Sb5jATn86P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eFdv (Part II)"
                    },
                    "comment": {
                        "value": "**Q3:** How Finding 1 is believed to applicable to general recommendation models.\n\nWe appreciate the reviewer's question and would like to clarify the applicability of Finding 1 to general recommendation models.\n\nFirstly, we acknowledge that our earlier statements may have caused some misunderstanding. It is important to note that merely analyzing the raw embedding matrices is **insufficient** for determining the causes of embedding collapse in general recommendation models, particularly when considering feature interaction. The learning of embeddings is a complex process that involves interactions with **all** other fields, making it challenging to isolate the specific mechanisms responsible for embedding collapse or to evaluate the impact of **field-pair-level** interaction on embedding learning, as discussed in our paper.\n\nTo overcome this challenge, we propose a comprehensive analysis from two perspectives, which leads to the formulation of the high-level interaction-collapse law:\n\n1. Empirical analysis: We conduct empirical analysis *specifically on sub-embedding-based models*. Sub-embeddings serve as bridges to identify field-pairwise influence. It is important to note that the concept of sub-embedding is simply an auxiliary tool for our analysis, and the conclusions drawn from this analysis are not necessarily limited to sub-embedding-based models alone.\n\n2. Theoretical analysis: We also perform a theoretical analysis focusing on FM-based interaction *without explicit sub-embeddings*, where the embedding matrix interacts directly with all other fields. This theoretical analysis demonstrates that the empirical finding mentioned above is not confined to sub-embedding-based models.\n\nBy combining the empirical analysis of sub-embedding-based models and the theoretical analysis of FM-based interaction, we arrive at Finding 1. We argue that this finding should be applicable to **general** recommendation models, as it provides insights into the phenomenon of embedding collapse and its underlying mechanisms.\n\nWe hope this clarification addresses the reviewer's concerns and highlights the broader applicability of our findings to the field of recommendation models.\n\n**Q4:** Whether Finding 1 should be a \"law\" or a \"theory\".\n\nWe appreciate the reviewer's question regarding the classification of our finding as a \"law\" or a \"theory.\" After careful consideration, we **agree** with the reviewer's viewpoint that the term \"law\" is typically applied to principles that are derived from extensive experimentation and have a strong empirical basis. In contrast, our Finding 1 is a pattern that has been derived through rigorous analysis and theoretical considerations. Therefore, we agree that it would be more accurate and appropriate to classify it as a \"theory\" rather than a \"law\"."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203695696,
                "cdate": 1700203695696,
                "tmdate": 1700203695696,
                "mdate": 1700203695696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uc9qklrqnR",
                "forum": "0IaTFNJner",
                "replyto": "Sb5jATn86P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThis is a kind reminder that the 12 days reviewer-author discussion period only left **less than 1 day**. Please let us know if our response has addressed your concerns.\n\nFollowing your suggestion, we have answered your concerns and improved the paper in the following aspects:\n- We have **clarified how the information abundance of ME** is computed. Specifically, we compute them by concatenating all embedding sets along the embedding dimension.\n- We have **compared our paper and Multi-Facet Embedding & Polysemous Embedding** to address the contribution and novelty of our work. Our paper contributes distinctively on the embedding collapse issue and the two-sided effect of feature interaction, and ME also differs from these work in some aspects.\n- We have **clarified and rearranged the logic of how to deduce Finding 1** and further **discussed how Finding 1 is applicible to general models**, including how empirical analysis on specific sub-embedding-based models and theoretical analysis on general FM-based models contributes to the deduction.\n- We have **revised the paper and modified *law* into *theory*.**\n\nThanks again for your valuable review. We are looking forward to your reply and are happy to answer any future questions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659487491,
                "cdate": 1700659487491,
                "tmdate": 1700659487491,
                "mdate": 1700659487491,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9kPJm1DFwm",
            "forum": "0IaTFNJner",
            "replyto": "0IaTFNJner",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission668/Reviewer_DK5H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission668/Reviewer_DK5H"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses challenges in scaling up recommendation models, identifying a phenomenon called \"embedding collapse\" while enlarging models. The authors introduce information abundance as a metric to measure and evaluate collapse. They show that embedding matrix reside mostly in a low-dimensional subspace in scaled up models. The study analyzes feature interactions in models, noting they reduce overfitting but can also exacerbate embedding collapse. To tackle this, the authors introduce a multi-embedding design, scaling independent embedding sets and integrating specific interaction modules. This approach claims to improve scalability across various recommendation models. Main contributions:\n- Highlight non-scalability in recommendation models and define the \"embedding collapse\" phenomenon.\n- Empirical and theoretical analysis reveals the dual impact of feature interaction on scalability.\n- Introduction of the multi-embedding design to achieve scalability improvements for recommendation models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\n- \u2018Information Abundance' as a quantitative novel measure to measure the embedding layer collapse.\n- The 'Interaction-Collapse Law' and the two sided effect of feature interaction process helps improve the understanding of embeddings' behavior in recommendation systems.\n\nQuality:\n- The authors have detailed exploration of embeddings and their behavior, particularly in the context of information collapse with rigorous visualizations. \n\nSignificance:\n- Broad Implications for Recommendation Systems: Given the ubiquitous nature of recommendation systems in today's digital platforms, insights into their workings, particularly regarding embeddings, have widespread implications.\n- Potential for Future Research: Introducing novel concepts and metrics invariably opens the door for future studies, both to validate and to build upon these ideas. The 'Interaction-Collapse Law', for instance, may become a focal point in subsequent research."
                },
                "weaknesses": {
                    "value": "1. Insufficient Empirical Validation on Large-Scale Data: The authors have shown with empirical evidences that large scale recommendation models scale poorly. However it is a common knowledge that large scale models are inherently data hungry to achieve better model convergence. This is an important premise that the paper relies on, it would good if authors can follow up to prove/disprove this as additional data points in this paper. The experiments seem to be on same amount of training data on scaled up models which does not unlock the full power of the scaled up model\n\n2. Studies on Collapse and its effect on scalability and overfitting seem to be limited to Single Embedding studies. To make a stronger case about the proposed method, it would be great if authors can provide discussion on the same for proposed Multi Embedding Design."
                },
                "questions": {
                    "value": "1. How do you justify the experiment setup when we know that bigger models are inherently data hungry and need more training data to achieve full convergence?\n\n2. From the experiment results shown, the single embedding and multi embedding cases show marginal improvement in AUC values. Can authors provide more context on the significance of the improvements here and if they are well outside the noise region?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission668/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission668/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission668/Reviewer_DK5H"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801912398,
            "cdate": 1698801912398,
            "tmdate": 1699635994074,
            "mdate": 1699635994074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "92DPxf1WlX",
                "forum": "0IaTFNJner",
                "replyto": "9kPJm1DFwm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DK5H (Part I)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. We have addressed the comments in the rebuttal below.\n\n**Q1:** Scaling law under different data size.\n\nWe sincerely appreciate the reviewer's suggestion to evaluate the scalability of our model on larger datasets. However, it is important to note that the Criteo and Avazu datasets, which we used for our experiments, are currently among the **largest publicly available benchmark datasets**, containing approximately $4\\times 10^8$ instances. Unfortunately, there are **no** larger datasets available for us to incorporate into our analysis. Nevertheless, we are confident that our proposed approach can be effectively applied to real-world industrial scenarios where significantly larger amounts of data are available.\n\nMoreover, we understand the reviewer's concern about the scalability of recommendation models and the need to examine their performance under different data sizes. To address this concern, we performed additional experiments by *downsampling the original Criteo dataset to 10% and 50% of its size*. We evaluated both the base model and the 4x model on these reduced datasets, and the results are presented in the following table:\n\n| DCNv2 |    1x   |    4x   |\n|:-----:|:-------:|:-------:|\n|  10%  | 0.79494 | 0.79560 |\n|  50%  | 0.80892 | 0.80920 |\n|  100% | 0.81339 | 0.81372 |\n\nAs shown in the table, even when the amount of data is significantly reduced, the 4x model consistently outperforms the base model. These results clearly demonstrate that scaling up the model size leads to performance improvements, **even with smaller datasets**. Therefore, we firmly believe that our approach is scalable and capable of delivering enhanced recommendation performance in various data scenarios. We hope this clarification adequately addresses the reviewer's concern and reinforces the validity and scalability of our proposed algorithm.\n\n**Q2:** Study of collapse and overfitting for ME.\n\nWe appreciate the reviewer's question regarding the study of collapse and overfitting for ME (Multi-Embedding) models. In response, we have made significant revisions in Appendix to the paper and *conducted additional experiments* to address this concern. Allow us to present the updated findings:\n\n1. Evidence II on ME: To further investigate the feature interactions of ME, we have *calculated the information abundance of sub-embeddings in an ME DCNv2 model*. These sub-embeddings are obtained by concatenating the projected embeddings from each embedding set. Specifically, we define sub-embedding $E_i^{\\to j}$ as $\\left[E_i^{(1)}(W_{i,j}^{(1)})^{\\top}, ..., E_i^{(m)}(W_{i,j}^{(m)})^{\\top}\\right]$. We visualize these sub-embeddings in Fig. 14. Notably, we observe that the influence of the field $j$ on $\\mathrm{IA}(E_i^{\\to j})$ is significantly reduced in the ME model compared to the SE (Single-Embedding) model. This finding suggests that *ME mitigates the collapse caused by feature interaction*.\n\n2. Evidence III on ME: In order to further investigate the impact of ME on collapse and overfitting, we conducted *regularization experiments* on the ME DCNv2 model. The results are summarized in the table below and Fig. 15:\n\n|            DCNv2 |    1x   |    2x   |    3x   |    4x   |   10x   |\n|-----------------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| SE (regularized) | 0.81313 | 0.81318 | 0.81311 | 0.81299 | 0.81307 |\n|    SE (standard) | 0.81339 | 0.81341 | 0.81345 | 0.81346 | 0.81357 |\n| ME (regularized, new) |         | 0.81324 | 0.81343 | 0.81349 | 0.81364 |\n|    ME (standard) |         | 0.81348 | 0.81361 | 0.81372 | 0.81385 |\n\nFrom the table, we observe that the regularized ME DCNv2 model exhibits a similar increasing trend in performance with respect to the embedding size as the regularized SE model. However, the performance decline in the regularized ME model is less significant compared to the regularized SE model. This finding provides further evidence that ME provides model scalability even when applyed on an interaction layer to cause overfitting.\n\nIn conclusion, the evidence from our experiments supports the effectiveness of ME in mitigating collapse caused by feature interaction."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203560115,
                "cdate": 1700203560115,
                "tmdate": 1700203560115,
                "mdate": 1700203560115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u6nk3r08Cw",
                "forum": "0IaTFNJner",
                "replyto": "9kPJm1DFwm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DK5H (Part II)"
                    },
                    "comment": {
                        "value": "**Q3:** Significance of performance improvement.\n\nIn response to the reviewer's question regarding the significance of our performance improvement, we would like to highlight several key points. Firstly, we acknowledge that previous works have regarded a gain of **1e-3** as already significant [1-3]. However, our ME design, while being relatively simple, is comparable with this threshold by achieving a performance gain of 7e-4 and 1.4e-3 on the Criteo and Avazu datasets respectively, solely through the scaling up of the model size. This achievement, which was **not** accomplished by the original SE, demonstrates the remarkable nature of our approach.\n\nFurthermore, we would like to draw attention to the standard deviation of the performance, as presented in Table 1 of our paper (Appendix C.3). Our improvements **surpass** the magnitude of the standard deviation, indicating that they are **not** merely a result of random variation. To further validate the significance of our findings, we conducted a t-test and obtained a p-value of 1.6e-3 for DCNv2 on the Criteo dataset (similar results were observed for other experiments). This p-value confirms that the observed performance improvement is not attributable to randomness, but rather represents a substantial and meaningful advancement.\n\n\n[1] Deepfm: a factorization-machine based neural network for ctr prediction. In IJCAI, 2017.  \n[2] Autoint: Automatic feature interaction learning via self-attentive neural networks. In CIKM, 2019.  \n[3] DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW, 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203604282,
                "cdate": 1700203604282,
                "tmdate": 1700203604282,
                "mdate": 1700203604282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zQm8g4WGUt",
                "forum": "0IaTFNJner",
                "replyto": "9kPJm1DFwm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThis is a kind reminder that the 12 days reviewer-author discussion period only left **less than 1 day**. Please let us know if our response has addressed your concerns.\n\nFollowing your suggestion, we have answered your concerns and improved the paper in the following aspects:\n- We have **discussed the data size issue in recommender systems**, and show with **supplemental experiments** that, even with small data size, the existing embedding size is still not enough.\n- We have **added analysis study on ME models** as those in Sec 4. The results show that ME can properly mitigate collapse and improve scalability.\n- We have **highlighted the significance of performance improvement**, including how the improvement is not marginal and how our improvement is a breakthrough, and further discussed how the improvement surpasses the scope of variance.\n\nThanks again for your valuable review. We are looking forward to your reply and are happy to answer any future questions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659469142,
                "cdate": 1700659469142,
                "tmdate": 1700659469142,
                "mdate": 1700659469142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IlRLbALEdX",
            "forum": "0IaTFNJner",
            "replyto": "0IaTFNJner",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission668/Reviewer_JdNi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission668/Reviewer_JdNi"
            ],
            "content": {
                "summary": {
                    "value": "This paper suggests that the embedding collapse phenomenon restricts the scalability of existing recommendation models. Empirical and theoretical analysis show that interaction with collapsed embeddings constrains embedding learning. Also, this paper proposes a multi-embedding design incorporating embedding-set-specific interaction modules to capture diverse patterns and reduce collapse."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "S1. This paper provides empirical and theoretical analysis of the embedding collapse phenomenon.\n\nS2. This paper provides information abundance for quantifying the degree of collapse for such matrices with low-rank tendencies."
                },
                "weaknesses": {
                    "value": "W1. The novelty of this paper seems to be limited. The method of dividing the single embedding into multi-embedding sets is similar to DMRL[1] for disentangled representation learning. DMRL divides the feature representation of each modality into k chunks. As a result, the features of different factors are entangled.\n\nW2. The motivation is not completely solid. The reason for increasing the embedding size of the model is inappropriate.\n\nW3. The experimental results of the paper are insufficient. When the embedding size was scaled up through multi-embedding, the experimental results show that performance increases. However, the performance improvement is marginal.\n\n[1] Disentangled Multimodal Representation Learning for Recommendation, IEEE\u201922"
                },
                "questions": {
                    "value": "Depending on the model and dataset, the model will have an appropriate embedding size. Therefore, it seems reasonable that the performance will drop if the embedding size deviates from the proper value. Then. do we need to scale up the embedding size for the same dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822478538,
            "cdate": 1698822478538,
            "tmdate": 1699635993963,
            "mdate": 1699635993963,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lA33HP1bCy",
                "forum": "0IaTFNJner",
                "replyto": "IlRLbALEdX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JdNi (Part I)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. We have addressed the comments in the rebuttal below.\n\n**Q1:** Relation between multi-embedding and DMRL.\n\nWe appreciate the reviewer's question regarding the relationship between ME and DMRL, and we have incorporated it into our paper. It is important to note that the ME design is **just one aspect** of our overall work that **reflects** how our theory and analysis apply to practical scenarios. Our work also makes **significant contributions** to studying the collapse phenomenon behind the lack of scalability of recommender models and the two-sided effect of feature interactions in recommender models, which are never covered in DRML.\n\nIn comparing the ME design and DMRL, we identify the following similarities and differences:\n- Similarity: Both ME and DMRL propose the idea of introducing *multiple embedding layers*.\n- Differences: (1) DMRL is a **specific** method, whereas ME is a **general** framework that can be applied to various feature interaction designs. (2) While DMRL focuses on learning different factors and **explicitly \"decoupling\"** them, ME takes a collapse perspective and demonstrates that even without decoupling, ME performs better than Single Embedding (SE) as it mitigates the collapse, a point that DMRL does **not** explore.\n\n**Q2:** Reason for increasing the embedding size of the model.\n\nThis question can be divided into two parts: why the embedding size should be increased instead of the size of other modules (e.g., the number of layers in the cross layer or the number of layers in the MLP), and why the existing embedding size is not large enough, which can be summarized as *\"why embedding\" and \"why increase\"*.\n\nRegarding \"why embedding\", it is widely recognized in both the academic and industrial communities that the embedding layer accounts for the **largest** number of parameters (>92% in our DCNv2 baseline for Criteo, and even higher for industrial models), making it a crucial and informative **bottleneck **in the model. To further illustrate this point, we conducted experiments to *increase the number of interaction layers and MLP layers* in the DCNv2 baseline and recorded the results in the following table:\n\n|                 DCNv2 |    1x   |    2x   |    4x   |\n|----------------------:|:-------:|:-------:|:-------:|\n|              standard | 0.81339 | 0.81341 | 0.81346 |\n| *+ #cross layer* | 0.81325 | 0.81338 | 0.81344 |\n|   *+ #MLP depth* | 0.81337 | 0.81345 | 0.81342 |\n\nFrom the table, it can be observed that increasing the number of cross layers or MLP layers does **not** lead to improved performance. Therefore, it is reasonable and necessary to scale up the embedding size.\n\nAs for \"why increase,\" the motivation lies in the fact that existing models have **too small** an embedding size that is inadequate for capturing and preserving data information effectively. For instance, the Johnson-Lindenstrauss lemma suggests that a projection over a $d$-dimensional space needs to have a dimension of approximately $\\Omega\\left(\\frac{\\log d}{\\epsilon^2}\\right)$ to preserve pairwise distances within $1\\pm \\epsilon$. However, the existing embedding size of 10 does not match the maximum field cardinality, which can be on the order of $10^5$. Our experimental results using ME further validate that increasing the embedding size in a proper manner is promising to lead to performance improvements, indicating that the existing embedding size is indeed insufficient.\n\nFurthermore, it is important to emphasize that our work is **not** solely focused on enhancing model performance at the same small size. Instead, our goal is to **overcome** the size limitations imposed on recommendation models by existing architectures and achieve consistent positive performance improvements when scaling up the models (as stated in the Introduction section). ME is a simple yet effective design that can be applied to general architectures and yield performance gains. We anticipate that ME can be combined with future works that aim to improve performance through other means."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203420843,
                "cdate": 1700203420843,
                "tmdate": 1700203432620,
                "mdate": 1700203432620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KxjAXGd0c1",
                "forum": "0IaTFNJner",
                "replyto": "IlRLbALEdX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JdNi (Part II)"
                    },
                    "comment": {
                        "value": "**Q3:** Significance of experiment results.\n\nNumerous existing studies have indicated that even a gain as small as **1e-3** is considered significant [1-3]. In our ME design, despite its simplicity, we were able to achieve a performance gain of 7e-4 on the Criteo dataset and 1.4e-3 on the Avazu dataset by **solely scaling up the model size**. Notably, this level of improvement was **never** achieved by the original SE approach. We firmly believe that such remarkable enhancement demonstrates the effectiveness and potential of our proposed algorithm.\n\n[1] Deepfm: a factorization-machine based neural network for ctr prediction. In IJCAI, 2017.  \n[2] Autoint: Automatic feature interaction learning via self-attentive neural networks. In CIKM, 2019.  \n[3] DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW, 2021.\n\n**Q4:** Appropriate embedding sizes.\n\nWe appreciate the reviewer's question regarding the \"appropriate embedding size\". In our paper, as demonstrated in Table 1, we acknowledge that the determination of the appropriate embedding size (*underlined* or **bolded**) relies on *both the dataset and the model itself*. Interestingly, our findings indicate that ME models generally require larger embedding sizes compared to SE models.\n\nHowever, it is important to note that our objective is **not** to indiscriminately increase the model size *using any fixed (possibly SE) model architecture*. While SE models may typically have smaller appropriate embedding sizes, it does **not** necessarily mean that recommendation models employing such embedding sizes effectively exploit the dataset's information. This discrepancy arises from inherent limitations in the model design, as we have identified through our analysis.\n\nAlso due to this limitation of existing (SE) models, our approach focuses on: firstly proposing a *more scalable* model design, and secondly *scaling up* the newly introduced scalable model. Our ME model serves as a straightforward and convenient scalable design, exhibiting a *larger appropriate embedding size* in conjunction with improved performance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203495282,
                "cdate": 1700203495282,
                "tmdate": 1700203495282,
                "mdate": 1700203495282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KTrVV5ki8J",
                "forum": "0IaTFNJner",
                "replyto": "IlRLbALEdX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThis is a kind reminder that the 12 days reviewer-author discussion period only left **less than 1 day**. Please let us know if our response has addressed your concerns.\n\nFollowing your suggestion, we have answered your concerns and improved the paper in the following aspects:\n- We have **compared our paper and DMRL** to address the contribution and novelty of our work. Our paper contributes distinctively on the embedding collapse issue and the two-sided effect of feature interaction, and ME also differs from DRML in some aspects.\n- We have **clarified the reason for increasing the embedding size**, including how the embedding serves as a bottleneck and how the existing embedding size is too small.\n- We have **highlighted the significance of performance improvement**, including how the improvement is not marginal and how our improvement is a breakthrough.\n- We have **discussed the concept of \"appropriate embedding size\"** and show how our ME is releated with the concept of appropriate embedding size under the perspective of scaling up models.\n\nThanks again for your valuable review. We are looking forward to your reply and are happy to answer any future questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659447528,
                "cdate": 1700659447528,
                "tmdate": 1700659447528,
                "mdate": 1700659447528,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nWhcrWyPwG",
            "forum": "0IaTFNJner",
            "replyto": "0IaTFNJner",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission668/Reviewer_9BGA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission668/Reviewer_9BGA"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies recommendation model performance when scaling up the embedding layers of the model. The paper identifies a phenomenon of embedding collapse, wherein the embedding matrix tends to reside in a low-dimensional subspace. Through empirical experiments on FFM and DCNv2 and theoretical analysis on FM, the paper shows that the feature interaction process of recommendation models leads to embedding collapse and thus limits the model scalability. The paper also performed empirical experiments on regularized DCNv2 and DNN which led to less collapsed embeddings, but the model performance got worse. The paper proposes multi-embedding, which leads to better performance when scaling up the embedding layers. Experiments demonstrate that this proposed design provides consistent scalability for various recommendation models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: \n- The paper investigates the enlarged embedding layers of recommendation models and identifies a phenomenon of embedding collapse, wherein the embedding matrix tends to reside in a low-dimensional subspace. The discovery is novel as far as I know.\n- The paper proposed information abundance to measure the degree of collapse for embedding matrices.\n\nQuality:\n- The paper is well-written. It starts with a novel finding of embedding collapse when increasing embedding dimension which might lead to poor scalability, performs empirical experiments and theoretical analysis to show that it is caused by feature interaction, and proposes a solution to increase scalability.\n\nSignificance:\n- The scaling law of recommendation models is an important topic in both academia and industry. This paper investigates the scaling law of embedding layers and shows why naively increasing embedding dim is not sufficient for scalability. The paper proposed multi-embedding which could help alleviate the phenomenon of embedding collapse and improve scalability."
                },
                "weaknesses": {
                    "value": "- In section 3, the paper proposes Information Abundance to measure the degree of collapse of embedding matrices. As the paper focuses on the scaling law of embedding layers, the paper should discuss whether Information Abundance is a fair metric when comparing embedding matrices of different dimension sizes.\n- In section 4.2, the paper uses regularized DCNv2 as an example to show that suppressing feature interaction is insufficient for scalability. It is unclear to me why feature interaction in regularized DCNv2 is suppressed.\n- The performance of multi-embedding lacks ablation study. One example is that in Figure 7 (right), we can feed all 4 small embeddings into a single feature interaction layer, and test its performance against the proposed approach. We can also test the Information Abundance of such design versus the proposed multi-embedding design.\n- It would be interesting to test the scaling law of embedding layers with increased feature interaction complexity. This can help us better understand whether the embedding collapse phenomenon is caused by insufficient feature interaction complexity."
                },
                "questions": {
                    "value": "My questions are listed in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission668/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission668/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission668/Reviewer_9BGA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829848014,
            "cdate": 1698829848014,
            "tmdate": 1699708302074,
            "mdate": 1699708302074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "exaVvW3i7m",
                "forum": "0IaTFNJner",
                "replyto": "nWhcrWyPwG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9BGA (Part I)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. We have addressed the comments in the rebuttal below.\n\n**Q1:** How the information abundance is a fair metric.\n\nWe appreciate the reviewer's question regarding the fairness of information abundance as a metric. Allow us to address this concern and clarify our usage of information abundance throughout the paper.\n\nFirstly, the embedding size influences information abundance. To ensure fair comparisons and mitigate this influence, we specifically compare the information abundance of matrices **only** with the same embedding size throughout this paper. We have listed *all the instances* where we utilize information abundance in this manner:\n\n- Fig.2: We compare the learned embedding matrix with a randomly initialized one, both having an *embedding size of 100*.\n- Fig.3: We compare the information abundance of two sub-embedding matrices, denoted as $E_i^{\\to j_1}$ and $E_i^{\\to j_2}$, which have the same *sub-embedding size of 10*.\n- Fig.4: We compare the information abundance of embedding matrices with the same shape but varying field configurations. *The embedding size in this case is 5*.\n- Fig.5 & 6: We compare the regularized DCNv2 and DNN models with the standard DCNv2 model. All models considered in these figures have the same *embedding size of 100*.\n- Fig.9 (a): We compare two types of embedding methods, SE and ME, with an embedding size scaled by 10x. *The embedding size in this case is 100.*\n- Fig.9 (b): We compare four sets of embeddings, each with the same *embedding size of 10*.\n\nFurthermore, it is essential to note that information abundance serves as a **simple and convenient** metric for assessing the degree of collapse in our work. The purpose of information abundance in our work is to provide a quantifiable measure that enables meaningful analysis. While it is not explicitly adapted to different embedding sizes in this work, we acknowledge the potential for such extensions. For example, possible extensions could include normalizing information abundance by the embedding size, such as using $\\frac{\\mathrm{IA}(E)}{K}$ or $\\frac{\\mathrm{IA}(E)}{\\mathbb{E}[\\mathrm{IA}(\\text{randn}(E))]}$, where $K$ represents the embedding size. We *add in Fig. 12* as the comparision with different embedding sizes through the former approach, and observed that the *normalized information abundance decreases with the embedding size*, which is consistent with the observation in Fig.1 (b).\n\n**Q2:** How feature interaction is surpressed in regularized DCNv2.\n\nWe appreciate the reviewer's question regarding the suppression of feature interactions in regularized DCNv2. We recognize the confusion caused by our previous description and would like to clarify and provide a more accurate explanation.\n\nIn our study, we investigated the issue of embedding collapse from the perspective of feature interactions. To address this problem, we made two modifications to DCNv2, a well-designed explicit-interaction model: (1) revising the modules in DCNv2 that **contribute to collapse** (referred to as Evidence III of Regularized DCNv2) and (2) directly avoiding explicit feature crossings (referred to as Evidence IV of DNN).\n\nRegarding Evidence II, we recognize that $W_i^{\\to j}$ can **lead to collapse** in sub-embeddings by projecting embeddings into a more collapsed space. By regularizing $W_i^{\\to j}$ to be a unitary matrix (or the multiplication of unitary matrices), we ensure the **preservation** of all singular values of the sub-embedding. Consequently, the information abundance of sub-embeddings in regularized DCNv2 is larger than standard DCNv2. We provide a *supplementary figure, Fig. 13*, illustrating the information abundance of sub-embeddings in standard and regularized DCNv2. This figure clearly demonstrates that sub-embeddings in regularized DCNv2 *exhibits a higher information abundance*. Based on our Finding 1, regularized DCNv2 mitigates the problem of embedding collapse since sub-embeddings are *directly interacted with the embeddings*.\n\nWe hope this clarification addresses the concerns raised by the reviewer regarding the suppression of feature interactions in regularized DCNv2."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203284864,
                "cdate": 1700203284864,
                "tmdate": 1700203284864,
                "mdate": 1700203284864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "89QbL85q2l",
                "forum": "0IaTFNJner",
                "replyto": "nWhcrWyPwG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9BGA (Part II)"
                    },
                    "comment": {
                        "value": "**Q3:** Ablation study of multi-embedding.\n\nThank you for your question regarding the ablation study of multi-embedding. In our initial submission, we presented an ablation experiment in Fig. 9 (d) where the feature interaction layers were **not strictly shared but implicitly weight-aligned**. However, we acknowledge that we should have included the reviewer's proposed *most straightforward ablation experiment*. We have addressed this oversight by adding the suggested experiment and present the results in the latest revision, as shown in the table below:\n\n|                              DCNv2 |    1x   |    2x   |    3x   |    4x   |   10x   |\n|-----------------------------------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n|                                 SE | 0.81339 | 0.81341 | 0.81345 | 0.81346 | 0.81357 |\n| ME, independent interaction (ours) |         | 0.81348 | 0.81361 | 0.81372 | 0.81385 |\n| ME, weight norm aligned (ablation) |         | 0.81332 | 0.81340 | 0.81334 | 0.81337 |\n| *ME, shared interaction (ablation, new)* |         | 0.81302 | 0.81283 | 0.81291 | 0.81221 |\n\nFrom the table, it is evident that the multi-embedding (ME) with shared interaction performs *worse than ME with specific interactions* (and even worse than the single embedding, SE). This finding is consistent with our analysis that ME *works from the diversity of interaction layers*.\n\n**Q4:** Scaling law with increased feature interaction complexity.\n\nIn addressing the impact of feature interaction complexity on scaling laws, we bifurcate the complexity into two distinct components: *interaction architectures* and *the number of interaction parameters*.\n\nRegarding interaction architectures, even when we consistently employed the DCNv2 architecture, **renowned for its complexity**, the results did not exhibit substantial performance enhancement. This finding suggests that the complexity of the interaction architecture, while important, may **not** be the primary driver in the scaling laws of recommendation models.\n\nFor the second component \u2013 the number of interaction parameters \u2013 we further augment our analysis. We *increased the number of feature interaction layer parameters* with the scaling of embedding sizes. The table below encapsulates our findings:\n\n|                 DCNv2 |    1x   |    2x   |    4x   |\n|----------------------:|:-------:|:-------:|:-------:|\n|              standard | 0.81339 | 0.81341 | 0.81346 |\n| *+ #cross layer* | 0.81325 | 0.81338 | 0.81344 |\n\nThis table reveals that increasing the number of cross layers does **not** proportionally enhance performance even in scenarios of increased embedding sizes, indicating that merely increasing the number of interaction parameters does not necessarily correspond with an improvement in model performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203335455,
                "cdate": 1700203335455,
                "tmdate": 1700203335455,
                "mdate": 1700203335455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n82ATwvzjj",
                "forum": "0IaTFNJner",
                "replyto": "nWhcrWyPwG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request of Reviewer's attention and feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThis is a kind reminder that the 12 days reviewer-author discussion period only left **less than 1 day**. Please let us know if our response has addressed your concerns.\n\nFollowing your suggestion, we have answered your concerns and improved the paper in the following aspects:\n- We have **clarified how information abundance can be properly applied**, including details of usage in our experiments and possible extensions for cases with different embedding sizes.\n- We have **clarified how regularized DCNv2 can mitigate collapse** by surpressing the mechenism in feature interaction that leads to collapse, and supplement detailed results in the revised paper.\n- We have **added new ablation study** that all embeddings sets shares one feature interaction module, and results show that the utilization of embedding-set-specific feature interaction is crucial.\n- We have **add analysis experiments of scaling up with increased feature interaction complexity**, and show that improving feature interaction complexity does not lead to performance gain, inferring that the bottleneck of performance is the embedding size.\n\nThanks again for your valuable review. We are looking forward to your reply and are happy to answer any future questions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659424218,
                "cdate": 1700659424218,
                "tmdate": 1700659424218,
                "mdate": 1700659424218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]