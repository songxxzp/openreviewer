[
    {
        "title": "Chain-of-Experts: When LLMs Meet Complex Operations Research Problems"
    },
    {
        "review": {
            "id": "yenXIqqOr0",
            "forum": "HobyL1B9CZ",
            "replyto": "HobyL1B9CZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5639/Reviewer_suKy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5639/Reviewer_suKy"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates solving complex operations research problems via the cooperation of multiple LLM-based agents.\nThe authors suggest Chain-of-Experts, which is a multi-agent framework comprised of 11 experts, for different aspects, and a conductor to coordinate these experts.\nThe experts are powered by common techniques such as In-context Learning and Reasoning based on LLMs.\nThe CoE framework sovle OR problems in an iterative way, where failed answers will get feedback via the reflection step.\nThis workflow will stop when the answer passes the evaluator or the iteration exceeds the given number.\nA new benchmark, ComplexOR, is contributed to evaluate on 20 complex OR problems.\nExperiments on LPWP and ComplexOR demonstrates that the proposed CoE outperforms previous LLM-agent methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. the CoE framework.\n2. A combination of existing techniques to solve OR problems.\n3. A new small-scale real-world dataset"
                },
                "weaknesses": {
                    "value": "1. The results on ComplexOR seem not sense. A too small dataset.\n2. The description of  CoE is not clear. It should be well-moviated and started with several backgrounds.\n\nThrough the response,  indeed the construction of ComplexOR is very difficult, and the authors acknowledge that the dataset will continue to be updated, which could be a potential contribution to the field and answer our questions."
                },
                "questions": {
                    "value": "1. Is the CoE suitable for other reasoning tasks? What is the difference if applied to other tasks?\n2. I suggest the paper give more attention to the CoE framework.\n\nThe answers have already addressed our questions.  Thanks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Reviewer_suKy",
                        "ICLR.cc/2024/Conference/Submission5639/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698568800847,
            "cdate": 1698568800847,
            "tmdate": 1700898333077,
            "mdate": 1700898333077,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4qr9wkkJ06",
                "forum": "HobyL1B9CZ",
                "replyto": "yenXIqqOr0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the constructive suggestions and comments on our work, which have definitely helped us to enhance the paper. The following are our detailed responses to each of the reviewer's comments. \n\n**Comment 1:**\n\n> The results on ComplexOR seem not sense. A too small dataset.\n\nWe understand the reviewer's concern. ComplexOR can be viewed as the first dataset that has been well annotated for complex operations research problems and it has great potential to evaluate the reasoning capabilities of LLMs. The dataset is not large-scale because it is rather labour intensive for domain experts to help annotate the problems.  To construct an annotated problem instance, we need to\n\n1. Find problems with suitable complexity from sources like academic papers, textbooks, or tutorials\n2. Manually extract the textual description of the problem\n3. Construct the mixed integer programming model and double check if it is correct\n4. Programming for this model\n5. Design several test cases with varied difficulty\n\nSince we are firmly aware of its value to the community, we continued to collect more samples after the paper submission. Till now, we have expanded the size of ComplexOR from 20 instances to 37 instances. The new experimental results on ComplexOR have been updated in the revision.\n\n**Comment 2:**\n\n> The description of CoE is not clear. It should be well-moviated and started with several backgrounds.\n\nIn revision, we will follow the reviewer's advice to better motivate the description of CoE. Our thought started with Solo Performance Prompting, a multi-agent system. While SSP shows potential in solving complex tasks like logical grid puzzles, it falls short in more domain-specific taks such as the OR problem. We identified two primary reasons for this limitation. Firstly, in SSP, the agents are prompt templates initialized by leader personas, which restricts their ability to integrate extensive knowledge. To address this limitation, we introduce 'experts' to play specific roles in the problem-solving process. Secondly, we find that the collaborative approach in SSP is too straightforward to tackle challenging problems. It lacks the ability to selectively choose experts based on the specific problem and refine based on external feedback. To this end, we propose forward thought construction and backward reflection mechanisms inspired by real-world collaboration processes.\n\n**Comment 3:**\n\n> Is the CoE suitable for other reasoning tasks? What is the difference if applied to other tasks?\n\nThis is a good point. In this paper, the agents/experts for collaborative reasoning are customized from the domain of operations research. As to agent design, the **terminology interpreter** who is responsible for supplying knowledge about specific terminology is integrated with a scenario modeling knowledge base. The **programming expert** is designed to generate modeling code, specifically in the LP file format.\n\nTo extend CoE to support other reasoning tasks, the first step is to reconstruct proper agents for the target domain. Luckily, the collaborative reasoning framework among the agents with forward thought construction and backward reflection is re-usable. \n\nTo evaluate the reusability of our framework, we conducted a preliminary experiment on Text2SQL task. Specifically, we selected 12 hard test instances related to a manufacturer scenario from the spider dataset[1]. In this experiment, we configured a CoE with two experts: a manufacturer terminology interpreter and a dataset grammar expert integrated with an SQLite grammar document[2]. We compared this setup with a standard prompting baseline. The results are as follows:\n\n|       | Execution Accuracy | Exact Matching Accuracy |\n| ----- | ------------------ | ----------------------- |\n| GPT-4 | 50%                | 37.5%                   |\n| CoE   | 75%                | 58.3%                   |\n\nWe regret that time constraints prevented us from conducting more comprehensive experiments.   In our upcoming revisions, we are committed to adding a formal experiment to further validate our approach.\n\n**Comment 4:**\n\n> I suggest the paper give more attention to the CoE framework.\n\nIn this work, we focus on automatically solving complex OR problems. It is motivated by the real demand of our industry partners. Such an automatic tool can greatly help them to reduce the overhead of modeling and programming for operations research problems from diversified industry sectors. \n\nWe totally agree with your comment that it would generate more impact if we focus on CoE and apply it to more types of reasoning tasks.  In the previous comment, we have validated its potential to support complex Text2SQL task.  We believe this is an exciting avenue for future research to explore if we can further improve the generality of our reasoning framework among various reasoning tasks.\n\nReference:\n\n[1] https://paperswithcode.com/sota/text-to-sql-on-spider\n\n[2] https://www.sqlite.org/lang.html"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528663564,
                "cdate": 1700528663564,
                "tmdate": 1700528740963,
                "mdate": 1700528740963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4NE0dF4ZQ5",
            "forum": "HobyL1B9CZ",
            "replyto": "HobyL1B9CZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5639/Reviewer_ocDx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5639/Reviewer_ocDx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes chain of experts (CoE), a framework that uses multiple LLM agents to solve operations research (OR) problems. Most complex OR problem requires coordination among multiple experts, each solving a subproblem. In CoE, these experts are implemented using specialized LLMs augmented with, e.g., knowledge bases or reasoning skills targeting the subproblems they are designed to solve. A separate conductor model orchestrated this coordination process. This framework is further augmented by a backward reflection process, that, conditioning on the feedback provided by the program execution environment, recursively runs backward to identify potential errors in the chain. CoE does not require updating the parameters of the LLM agents, and thus is applicable to both proprietary and open-source models.\n\nCoE is evaluated on LPWP (elementary linear programming problems), and complexOR (a newly created dataset by the paper, containing 20 expert-annodated OR problems). Experiments with GPT-3.5, GPT-4, and Claude-2 suggest that CoE outperforms baselines. An ablation analysis quantifies the contribution of each design choice in CoE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- CoE is an interesting and novel framework for solving complex problems with multiagent collaboration.\n- CoE\u2019s design is grounded in real-world applications and proves effective.\n- Requiring no training, CoE is applicable to both open-source and proprietary models.\n- The presentation is reasonably clear."
                },
                "weaknesses": {
                    "value": "- The paper would be more interesting to the LM community and have a larger impact if it could test out CoE on some of the well-established benchmarks\n- ComplexOR is very small; I wonder how significant the results are\n- The paper does not provide enough details on how the experts are specialized."
                },
                "questions": {
                    "value": "- ComplexOR is very small. Can the authors provide more details on the consistency of the results across multiple runs?\n- It would be interesting to compare to a baseline that applies CoE, and uses the same model to play all the different roles.\n- Eq. 3 reads like applying the LLM to the prompt template outputs a new set of parameters, which does not align with what happens with prompting. At a higher level, do we really need the $\\theta$ notations in Eqs. 2 and 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Reviewer_ocDx"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809601514,
            "cdate": 1698809601514,
            "tmdate": 1700623828014,
            "mdate": 1700623828014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MzbOEVaJne",
                "forum": "HobyL1B9CZ",
                "replyto": "4NE0dF4ZQ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments. We have thoughtfully incorporated them into our revised paper. Below, we restate your comments and provide our point-by-point responses.\n\n**Comment 1: test out CoE on some of the well-established benchmarks**\n\nThis is a good point. In this paper, the agents/experts for collaborative reasoning are customized from the domain of operations research. As to agent design, the **terminology interpreter** who is responsible for supplying knowledge about specific terminology is integrated with a scenario modeling knowledge base. The **programming expert** is designed to generate modeling code, specifically in the LP file format.\n\nTo extend CoE to support other reasoning tasks, the first step is to reconstruct proper agents for the target domain. Luckily, the collaborative reasoning framework among the agents with forward thought construction and backward reflection is re-usable. \n\nTo test the reusability of CoE, we conducted a preliminary experiment using the Text2SQL dataset. Specifically, we selected 12 hard test instances related to a manufacturer scenario from the spider dataset[1]. Here, we configured a CoE with two experts: a manufacturer terminology interpreter and a dataset grammar expert integrated with an SQLite grammar document[2]. We compared this setup with a standard baseline. The results are as follows:\n\n|       | Execution Accuracy | Exact Matching Accuracy |\n| ----- | ------------------ | ----------------------- |\n| GPT-4 | 50%                | 37.5%                   |\n| CoE   | 75%                | 58.3%                   |\n\nWe regret that time constraints prevented us from conducting more comprehensive experiments.   In our upcoming revisions, we are committed to adding a formal experiment to further validation.\n\n\n\n**Comment 2: ComplexOR is too small**\n\nWe understand the reviewer's concern. ComplexOR can be viewed as the first dataset that has been well annotated for complex operations research problems and it has great potential to evaluate the reasoning capabilities of LLMs. The dataset is not large-scale because it is rather labour intensive for domain experts to help annotate the problems.  To construct an annotated problem instance, we need to\n\n1. Find problems with suitable complexity from sources like academic papers, textbooks, or tutorials.\n2. Manually extract the textual description of the problem.\n3. Construct the mixed integer programming model and double check if it is correct.\n4. Programming for this model.\n5. Design several test cases with varied difficulty.\n\nSince we are firmly aware of its value to the community, we continued to collect more samples after the paper submission. Till now, we have expanded the size of ComplexOR from 20 instances to 37 instances. The new experimental results on ComplexOR have been updated in the revision, as shown in Table 1 (overall performance), Table 2 (ablation study), and Table 3 (Robustness under different LLM).\n\n\n\n**Comment 3: a baseline that use the same model**\n\nThis is an inspiring comment. We followed the reviewer's advice to implement a baseline that uses the same model. In this experiment, we use a uniform system prompt across all roles, without any additional knowledge bases. We also find that the removal of experts' features leads to a decrease in accuracy, which suggests that the CoE benefits from using specialized experts over a singular, generalized model. We have added this baseline in our revision.\n\n|                     |           | LPWP     |          |           | ComplexOR |          |\n| ------------------- | --------- | -------- | -------- | --------- | --------- | -------- |\n|                     | Accuracy  | CE rate  | RE rate  | Accuracy  | CE rate   | RE rate  |\n| CoE without Experts | 55.1%     | 4.0%     | 11.9%    | 18.8%     | 7.9%      | 15.0%    |\n| Chain-of-Experts    | **58.9%** | **3.8%** | **7.7%** | **25.9%** | **7.6%**  | **6.4%** |\n\n\n\n**Comment 4: role of theta in Eqs.2 and 3**\n\nWe sincerely appreciate your comment and it actually pushes us to think about the roles of $\\theta$ deeply. We agree that the use of $\\theta$ in Equations 2 and 3 could be misleading as CoE don't actually generate new parameters through prompting. To clarify, we've revised our approach to align with the notation used in the paper Reflexion[3]. Here, we treat the Conductor as a policy, with its parameters indicated in the subscript. The revised equation is as follows:\n\n$$Conductor_{\\mathcal{F}^{\\theta'}(\\mathbb{PT}_t)}(e | s) = P_r\\{E_{\\phi_t} = e | S_t = s\\}$$\n\n\n\nReference\n\n[1] https://paperswithcode.com/sota/text-to-sql-on-spider\n\n[2] https://www.sqlite.org/lang.html\n\n[3] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. CoRR, abs/2303.11366, 2023. doi: 10.48550/arXiv.2303.11366."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527705770,
                "cdate": 1700527705770,
                "tmdate": 1700528308846,
                "mdate": 1700528308846,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X5qJhItrbY",
                "forum": "HobyL1B9CZ",
                "replyto": "MzbOEVaJne",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5639/Reviewer_ocDx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5639/Reviewer_ocDx"
                ],
                "content": {
                    "comment": {
                        "value": "The authors' response has addressed most of my concerns. I have revised my review accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623812173,
                "cdate": 1700623812173,
                "tmdate": 1700623812173,
                "mdate": 1700623812173,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WVN4KRyJVv",
            "forum": "HobyL1B9CZ",
            "replyto": "HobyL1B9CZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5639/Reviewer_B8wx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5639/Reviewer_B8wx"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a multi-agent reasoning method for operations research problems solving. In particular, all the expert agents called in a sequence by another conductor agent, and all the agents are based on LLMs, acting different roles. The approach (named Chain-of-Experts) achieves better results compared with other SOTA models on the LPWP dataset and they also release a new dataset on complex OR problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Propose a multi-agent method for OR problem solving with one conductor and multiple experts; and achieves better empirical results\n* Release a dataset on complex OR for the community"
                },
                "weaknesses": {
                    "value": "* Lack of evaluation on individual expert agents, as well as the conductor\n* The comparison with other models might not be fair, since they call the LLM differently. Maybe add some measurements of how different methods use the LLMs."
                },
                "questions": {
                    "value": "* If we use other less competent LLMs, like smaller models or open sourced models, how much the performance will be affected?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Reviewer_B8wx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828096868,
            "cdate": 1698828096868,
            "tmdate": 1699636585978,
            "mdate": 1699636585978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xbqD2UOTKH",
                "forum": "HobyL1B9CZ",
                "replyto": "WVN4KRyJVv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the constructive suggestions and comments on our work, which have definitely helped us to enhance the paper. The following are our detailed responses to each of the reviewer's comments. \n\n**Comment 1:**\n> Lack of evaluation on individual expert agents, as well as the conductor\n\nIn our original submission, we have conducted an ablation study to evaluate individual expert agents. The reviewer can check Figure 5 in Appendix A.4.2. Due to space limit, we regret that it was not put in the main body of the paper.\n\n\n**Comment 2:**\n> The comparison with other models might not be fair, since they call the LLM differently. Maybe add some measurements of how different methods use the LLMs.\n\nIn our experiments, we unify the way to use the LLMs. Each method within an experiment utilizes the same model API, facilitated by the 'chat complete' interface LangChain framework[1]. Please let us know if we did not interprete the reviewer's comment correctly.\n\n\n**Comment 3:**\n> If we use other less competent LLMs, like smaller models or open sourced models, how much the performance will be affected?\n\nThis is a good angle. If we use smaller models such as Llama 2, the performance degrades dramatically. We have conducted an experiment in chapter 4.7 to show the sensitivity to different types of LLMs. For smaller models such as Llama 2, we also conducted additional experiments on dataset LPWP and presented the results in the following:\n\n|                  | Accuracy | CE rate | RE rate |\n| ---------------- | -------- | ------- | ------- |\n| Standard         | 0%       | 100%    | 0%      |\n| Chain-of-Thought | 0%       | 100%    | 0%      |\n| Reflexion        | 0%       | 100%    | 0%      |\n| ReAct            | 0.7%     | 99.3%   | 0%      |\n| Solo Performance | 0%       | 100%    | 0%      |\n| Chain-of-Experts | 0.7%     | 99.3%   | 0%      |\n\nIn this experiment, we use the Llama2 13b model[2] as a foundational model, applying it across various baselines and the CoE on the LPWP dataset. Our findings revealed that the Llama2 model was largely ineffective in resolving the OR problems. Approaches like CoT and Reflexion showed no significant impact. Only 2 out of 289 instances were successfully resolved using ReAct and CoE, which incorporated external knowledge. These results suggest that Large Language Models must achieve a certain scale to effectively tackle complex problems.\n\n\n\nReference\n\n[1] https://github.com/langchain-ai/langchain\n\n[2] Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Jie Luo, Cheng Chen, Sekeun Kim, Jiang Hu, Haixing Dai, Lin Zhao, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Tianming Liu, Quanzheng Li, Xiang Li: Radiology-Llama2: Best-in-Class Large Language Model for Radiology. CoRR abs/2309.06419 (2023)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527853119,
                "cdate": 1700527853119,
                "tmdate": 1700527853119,
                "mdate": 1700527853119,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P1AxOxL82C",
            "forum": "HobyL1B9CZ",
            "replyto": "HobyL1B9CZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5639/Reviewer_CHCN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5639/Reviewer_CHCN"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on utilizing large language models (LLMs) to address operations research problems. It employs an approach where LLMs role-play as agents in the problem-solving pipeline, collaboratively breaking down and resolving problems. The paper also incorporates external feedback for the backpropagation reflections in the problem-solving pipeline, allowing the LLMs within the pipeline to self-improve. Moreover, the research introduces a new operations research dataset, which appears to be more intricate compared to existing ones. The proposed approach is tested on the newly-created dataset as well as another benchmark, and results indicate that it outperforms used baseline prompting methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Overall, the methodology presented in this paper is straightforward, easy to implement, and demonstrates strong empirical results across two benchmarks.\n2. The paper offers a new operations research dataset that, based on experimental outcomes, is more challenging than existing ones.\n3. I find the mechanism of propagating feedback from external sources to enhance the performance of language models both innovative and interesting. The results suggest that this mechanism also boosts model performance."
                },
                "weaknesses": {
                    "value": "1. While the paper focuses on tackling complex operations research problems, it doesn't seem to introduce any techniques specifically tailored for operations research challenges.\n2. I believe the novelty of this work is somewhat limited, as several studies have already explored the \"planning with feedback\" approach with LLMs. Please refer to \"A Survey on Large Language Model based Autonomous Agents (https://arxiv.org/pdf/2308.11432.pdf)\" for more details. I think the authors should offer a more in-depth comparison with these existing works. Moreover, though the methodology is described as a multi-expert framework, it essentially relies on deploying various prompts to the same LLM."
                },
                "questions": {
                    "value": "Why can't the method proposed in this paper be represented through Solo Performance Prompting, and where exactly does it differ from Solo Performance Prompting? From the description, it seems that the approach is entirely representable under the Solo Performance Prompting framework."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5639/Reviewer_CHCN",
                        "ICLR.cc/2024/Conference/Submission5639/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5639/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829312345,
            "cdate": 1698829312345,
            "tmdate": 1700722532591,
            "mdate": 1700722532591,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DCPy57okfH",
                "forum": "HobyL1B9CZ",
                "replyto": "P1AxOxL82C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5639/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5639/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment!"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the constructive suggestions and comments on our work, which have definitely helped us to enhance the paper. The following are our detailed responses to each of the reviewer's comments. \n\n**Can CoE be Represented by the SSP Framework?**\n\n> Why can't the method proposed in this paper be represented through Solo Performance Prompting, and where exactly does it differ from Solo Performance Prompting? \n\nThere are two key differences between Solo Performance Prompting and our CoE. First, in terms of agent generation, Solo Performance Prompting prompts a single LLM to identify and simulate multiple agents. In contrast, the agents/experts in CoE are tailored to collaboratively solve an OR problem. For example, the Modeling Knowledge Supplement Expert is integrated with the knowledge of games-cutting edge modeling to its knowledge base. Second, in terms of collaborative reasoning framework, the agents in Solo Performance Prompting cooperate in a round-robin manner. In contrast, our CoE works with forward thought construction and backward reflection.\n\nWe also offer a case study in Appendix 4.3 to illustrate the differences of reasoning processes between Solo Performance Prompting  and our CoE.\n\n\n\n**Techniques Designed for OR Problems?**\n\n> While the paper focuses on tackling complex operations research problems, it doesn't seem to introduce any techniques specifically tailored for operations research challenges.\n\nThis is a good point. In this paper, the agents/experts for collaborative reasoning are customized from the domain of operations research. As to agent design, the **terminology interpreter** who is responsible for supplying knowledge about specific terminology is integrated with a scenario modeling knowledge base. The **programming expert** is designed to generate modeling code, specifically in the LP file format.\n\nTo extend CoE to support other reasoning tasks, the first step is to reconstruct proper agents for the target domain. Luckily, the collaborative reasoning framework among the agents with forward thought construction and backward reflection is re-usable. \n\nTo test the reusability of CoE, we conducted a preliminary experiment using the Text2SQL dataset. Specifically, we selected 12 hard test instances related to a manufacturer scenario from the spider dataset[1]. Here, we configured a CoE with two experts: a manufacturer terminology interpreter and a dataset grammar expert integrated with an SQLite grammar document[2]. We compared this setup with a standard baseline. The results are as follows:\n\n|       | Execution Accuracy | Exact Matching Accuracy |\n| ----- | ------------------ | ----------------------- |\n| GPT-4 | 50%                | 37.5%                   |\n| CoE   | 75%                | 58.3%                   |\n\nWe regret that time constraints prevented us from conducting more comprehensive experiments.  In our upcoming revisions, we are committed to adding a formal experiment to further validation.\n\n\n\n**Comparison with Other Planning with Feedback Methods**\n\n>  I think the authors should offer a more in-depth comparison with these existing works. Moreover, though the methodology is described as a multi-expert framework, it essentially relies on deploying various prompts to the same LLM.\n\nWe thank the reviewer for providing the reference. The unique features of our CoE include forward agent selection and backward reflecion. We followed the reviewer's advice and provided an in-depth comparison with these existing works in the following table.\n\n|                  | Multi-agnets | Forward agent selection | External knowledge access | Refine by Feedback |\n| ---------------- | ------------ | ----------------------- | ------------------------- | ------------------ |\n| ReAct            | X            | X                       | \u2713                         | X                  |\n| Voyager          | X            | X                       | \u2713                         | \u2713                  |\n| Ghost            | X            | X                       | \u2713                         | \u2713                  |\n| SayPlan          | X            | X                       | \u2713                         | \u2713                  |\n| MetaGPT          | \u2713            | X                       | \u2713                         | X                  |\n| NLSOM            | \u2713            | X                       | \u2713                         | X                  |\n| SSP              | \u2713            | X                       | X                         | X                  |\n| ChatEval         | \u2713            | X                       | X                         | X                  |\n| Chain-of-Experts | \u2713            | \u2713                       | \u2713                         | \u2713                  |\n\n\n\nReference:\n\n[1] https://paperswithcode.com/sota/text-to-sql-on-spider\n\n[2] https://www.sqlite.org/lang.html"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528007143,
                "cdate": 1700528007143,
                "tmdate": 1700530218378,
                "mdate": 1700530218378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RrW1NRyXbo",
                "forum": "HobyL1B9CZ",
                "replyto": "DCPy57okfH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5639/Reviewer_CHCN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5639/Reviewer_CHCN"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the author's response, and some of my concerns have been alleviated. I've adjusted my scores accordingly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5639/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722593555,
                "cdate": 1700722593555,
                "tmdate": 1700722593555,
                "mdate": 1700722593555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]