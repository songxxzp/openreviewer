[
    {
        "title": "LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning"
    },
    {
        "review": {
            "id": "GVFTDil4E1",
            "forum": "M36m3buBVD",
            "replyto": "M36m3buBVD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission772/Reviewer_zHY5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission772/Reviewer_zHY5"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a prompt tuning method called Long-term Spatial Prompt Tuning (LSPT). LSPT leverages LSTM to address the long-term forgetting issue that occurs in the standard VPT method. Additionally, LSPT introduces class-aware spatial prompt coding to integrate global visual representation into the prompts. Experiments conducted on image classification demonstrate the effectiveness of LSPT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is overall well-written and easy to understand.\n2. The proposed LSPT consistently outperforms VPT and GaPT baselines, and ablation studies are thoroughly conducted."
                },
                "weaknesses": {
                    "value": "1. The working reason of CSPC is not entirely clear. The author should provide a more detailed explanation of the fundamental distinctions among the three types of sources used to construct the input prompts mentioned in Sec 3.2 and elaborate on why \"adding the average embedding of patch tokens to the output prompts\" leads to performance improvements.\n\n2. About generalizability. The authors have exclusively conducted experiments on the standard ViT architecture under self-supervised pretraining strategies. This raises questions about how LSPT performs when applied to a supervised pretrained model and more advanced ViT architectures. Additionally, comparing LSPT with more advanced parameter-efficient fine-tuning methods [a] [b] could further substantiate its effectiveness.\n\n3. The training and inference processes may be more complex compared to VPT or other parameter-efficient methods. It would be beneficial to include metrics such as FLOPs or inference times to assess the efficiency of the proposed method.\n\n4. The authors do not list the limitations and broader impact of this work. Some potential solutions should also be given.\n\n[a] Revisiting the parameter efficiency of adapters from the perspective of precision redundancy. ICCV, 2023.\n[b] Adapting shortcut with normalizing flow: An efficient tuning framework for visual recognition. CVPR, 2023."
                },
                "questions": {
                    "value": "1. If I understand correctly, the authors propose that VPT may be susceptible to the so-called long-term forgetting problem. In response, LSPT solves this problem by integrating information from prompt tokens of different layers using an LSTM.  This raises the question: what is the connection between long-term forgetting problem and performance improvement? If this question can be addressed, would it be more effective to employ an attention-based architecture instead of an LSTM?\n\n2. [c] seems quite closely related to LSPT . This concurrent work is worthy to cite and discuss.\n\n[c] Learning Expressive Prompting With Residuals for Vision Transformers. ICML 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698564150507,
            "cdate": 1698564150507,
            "tmdate": 1699636004645,
            "mdate": 1699636004645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1xN6t3hCvp",
                "forum": "M36m3buBVD",
                "replyto": "GVFTDil4E1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zHY5"
                    },
                    "comment": {
                        "value": "Dear Reviewer zHY5,\n\nThank you for appreciating our approach. We will address your comments below.\n\n> The working reason of CSPC is not entirely clear. The author should provide a more detailed explanation of the fundamental distinctions among the three types of sources used to construct the input prompts mentioned in Sec 3.2 and elaborate on why \"adding the average embedding of patch tokens to the output prompts\" leads to performance improvements.\n\nThe proposed CSPC (Class-aware Spatial Prompt Coding) is strategically designed to perpetually amass class conscious features, thereby fortifying the model\u2019s prowess in distinguishing and identifying visual categories. The role of the mean of patch tokens behind CSPC is used for enhancing classification performance, as  the mean of patch tokens is commonly used for linear probing in self-supervised ViTs, such as DINO and MAE. Adding them to the prompt tokens will help improve the model\u2019s prowess in distinguishing and identifying visual categories. \n\nMore importantly, computation overhead is minimal to keep all patch semantics. Furthermore, we conduct additional ablation studies exploring more sophisticated methods for accumulating spatial and positional information. Specifically, we investigate alternative techniques like k-means clustering on spatial patch tokens to enrich the spatial coding process and provide a comparative analysis of these methods. The overall performance get an increase on all the compared datasets when we use k-means to incorporate spatial information, while the additional computational cost leads to a trade-off. To be noticed, more exploration space on this spatial prompt coding will leave for future work.\n\n| Spatial Prompt | CUB       | Flowers   | Cars      | Dogs      | NABirds   |\n|----------------|-----------|-----------|-----------|-----------|-----------|\n| average        | 73.86     | 82.32     | 74.75     | 82.05     | 71.73     |\n| k-means        | **74.32** | **82.56** | **74.87** | **82.23** | **71.86** |\n\n> About generalizability. The authors have exclusively conducted experiments on the standard ViT architecture under self-supervised pretraining strategies. This raises questions about how LSPT performs when applied to a supervised pretrained model and more advanced ViT architectures. Additionally, comparing LSPT with more advanced parameter-efficient fine-tuning methods [a] [b] could further substantiate its effectiveness.\n\n[a] Revisiting the parameter efficiency of adapters from the perspective of precision redundancy. ICCV, 2023. \n[b] Adapting shortcut with normalizing flow: An efficient tuning framework for visual recognition. CVPR, 2023.\n\n\nTo address concerns about generalizability, we extend our experiments to include supervised pre-trained models and more advanced ViT architectures on ViT-B/16 pre-trained on supervised ImageNet-21k in the Table below. This provide a broader perspective on LSPT's applicability and effectiveness. \n\n| Method                   | Nature | Specialized | Structured | AVG   |\n|--------------------------|--------|-------------|------------|-------|\n| VPT (ECCV\u201922)            | 78.48  | 82.43       | 54.98      | 69.42 |\n| EXPRES (CVPR\u201923) [c]        | 79.70  | 84.00       | 55.00      | 70.21 |\n| SNF (CVPR\u201923)[b]            | 83.79  | 86.13       | 59.61      | 74.10 |\n| Bi-AdaptFormer (ICCV\u201923) [a] | 82.11  | 86.40       | 62.43      | 74.73 |\n| LSPT (ours)              | **85.26**  | **88.57**       | **66.25**      | **77.95** |\n\n\n[a] Jie et al. Revisiting the parameter efficiency of adapters from the perspective of precision redundancy. ICCV, 2023. \n\n[b] Wang et al. Adapting shortcut with normalizing flow: An efficient tuning framework for visual recognition. CVPR, 2023.\n\n[c] Das et al. Learning Expressive Prompting With Residuals for Vision Transformers. CVPR, 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673065629,
                "cdate": 1700673065629,
                "tmdate": 1700673065629,
                "mdate": 1700673065629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NiSliWOlvo",
            "forum": "M36m3buBVD",
            "replyto": "M36m3buBVD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission772/Reviewer_HDEH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission772/Reviewer_HDEH"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents LSPT for prompt tuning of self-supervised pre-trained ViTs. The authors highlighted the importance of long-range dependency on successive blocks' prompts and presented two techniques: class-aware spatial prompt coding and long-term prompt coding. The proposed method achieves better performance than existing techniques on popular benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well-written and easy to understand. \n\n+ The paper successfully includes closely related works and compares with them."
                },
                "weaknesses": {
                    "value": "- My major concern is the motivation. Why preserving the spatial information is crucial? For classification, proper abstraction can be more beneficial, which discards unnecessary spatial details or provides not interpretable attention maps. \n\n- In addition, I think the long-term modeling of prompts is not well-motivated. Why the long-term dependency is necessary? Why the long-term modeling is helpful for the classification? The authors have tried to explain this with the human brain, but it seems too ambiguous. No technical evidence was given.\n\n- The class-aware spatial prompt coding is not motivated, too. What is the role of the mean of patch tokens? Why they are added to the prompt tokens? \n\n- The long-term modeling ability is not supported by evidence. The visualization of attention maps do not represent the long-term modeling ability, and in addition, they can be cherry-picked. The authors should find more direct evaluation ways to show the long-term modeling ability, especially with quantitative measures. \n\n- The proposed method achieves better performance than previous SOTAs, but it uses more learnable parameters. For FGVC, 4 times more parameters were used, and for VTAB-1K, 5 times more parameters were used. For a fair comparison, the proposed method should decrease the number of introduced prompts.\n\n- In addition, the proposed method may require more inference time due to LSTM module. Please compare the inference time with other VPT-based methods."
                },
                "questions": {
                    "value": "- For long-term modeling, why LSTM is used? The auto-aggressive Transformer layer can also be applied?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727436519,
            "cdate": 1698727436519,
            "tmdate": 1699636004567,
            "mdate": 1699636004567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zbGpB9lwzV",
                "forum": "M36m3buBVD",
                "replyto": "NiSliWOlvo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HDEH"
                    },
                    "comment": {
                        "value": "Dear Reviewer HDEH,\n\nThank you for the detailed review. We will address your concerns below.\n\n> My major concern is the motivation. Why preserving the spatial information is crucial? For classification, proper abstraction can be more beneficial, which discards unnecessary spatial details or provides not interpretable attention maps.\n\nWe appreciate the reviewer's query about the importance of preserving spatial information in classification tasks. In our revised response, we aim to align more closely with the underlying intuition of self-supervised Vision Transformers (ViTs) and the role of Visual Prompt Tuning (VPT) in leveraging spatial information.\n\nThe core intuition behind our approach is grounded in the understanding that self-supervised ViTs inherently encode semantics in attention weights, which highlight the relevance of different patch tokens and thus, the underlying spatial information. In such architectures, the attention mechanism is adept at discerning which parts of the input are more relevant for a given task, making the spatial context of these patch tokens particularly significant. VPT, in this context, emphasizes and harnesses this spatial information. The visual prompts in VPT are designed to encode information through an aggregation process driven by learnable attention weights. This mechanism allows for a more nuanced and detailed understanding of spatial relationships within the data, which is particularly beneficial for tasks that require fine-grained differentiation between classes.\n\nTo empirically validate this concept, we have conducted experiments where the Class-Aware Spatial Prompt Coding (CSPC) module was applied in isolation. The results, as detailed in our ablation study, show a marked improvement in performance when CSPC is used. This demonstrates that the explicit encoding of spatial information via CSPC significantly enhances the model's ability to discern and classify visual data with a higher degree of accuracy.\n\n| LPC | CSPC | CUB   | Flowers | Cars  | Dogs  | NABirds | Nature | Specialized | Structured |\n|-----|------|-------|---------|-------|-------|---------|--------|-------------|------------|\n|  &cross;  | &cross;    | 68.33 | 80.05   | 67.67 | 78.83 | 65.22   | 67.34  | 82.26       | 37.55      |\n| &cross;    | &check;    | **76.28** | **85.23** | **72.16** | **80.51** | **70.63** | **72.36** | **83.56**   | **45.15**  |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672879168,
                "cdate": 1700672879168,
                "tmdate": 1700672879168,
                "mdate": 1700672879168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y6upJI2ltf",
                "forum": "M36m3buBVD",
                "replyto": "NiSliWOlvo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HDEH (cont. 2)"
                    },
                    "comment": {
                        "value": "> The long-term modeling ability is not supported by evidence. The visualization of attention maps do not represent the long-term modeling ability, and in addition, they can be cherry-picked. The authors should find more direct evaluation ways to show the long-term modeling ability, especially with quantitative measures.\n\nTo address the reviewer's concerns about the lack of direct evidence for the long-term modeling ability, we introduce deeper quantitative analysis on the forgetting gate in learned LSTM to further justify our design choices. Specifically, we calculate the forget gate to decide how much of the previous prompt will be forgotten and how much of the previous prompt will be used in next steps, as shown in Table below. The number in the *Forgetting* colomn means how much information are discarded on average before feeding to the next layer. These quantitative results further elucidate how the forgetting gate aids in reducing forgetting in LSPT compared to VPT and GaPT. This analysis include detailed examinations of the forgetting metrics across different prompt tuning methods, demonstrating the effectiveness of LSPT's approach.\n\nTo be noticed, compared to VPT and GaPT whose ratios of forgetting are fixed and decides merely by the architecture or hyperparameter, the ratio of forgetting for our LSPT is an average over all the layers and data, controlled dynamically by the learned LSTM. This flexibility and selectivity of rememberring and forgetting also contributes to the final improvement. \n\n\n| Method | Forgetting | Peformance |\n| -------- | -------- | -------- |\n| VPT (ECCV'22)     | 1.00     | 68.33     |\n| GaPT (ICML'23)     | 0.85     | 70.56     |\n| LSPT (ours)     | **0.68**     | **73.86**     |\n\n\n> The proposed method achieves better performance than previous SOTAs, but it uses more learnable parameters. For FGVC, 4 times more parameters were used, and for VTAB-1K, 5 times more parameters were used. For a fair comparison, the proposed method should decrease the number of introduced prompts.\n\nWe acknowledge the concern regarding the increased number of learnable parameters in LSPT. To address this, we conduct experiments with a reduced number of prompts, ensuring a fairer comparison with previous SOTAs. Comparison results in Table below demonstrate the efficiency of LSPT in achieving high performance with the same tunable parameters.\n\n| Method         | Params | CUB   | Flowers | Cars  | Dogs  | NABirds | AVG   |\n|----------------|--------|-------|---------|-------|-------|---------|-------|\n| VPT (ECCV'22)  | 1.02x  | 68.33 | 80.05   | 67.67 | 78.83 | 65.22   | 72.02 |\n| GaPT (ICML'23) | 1.02x  | 70.56 | 78.55   | 71.7  | 78.9  | 67.26   | 73.39 |\n| LSPT (ours)    | 1.02x  | **72.18** | **81.35**   | **73.82** | **81.02** | **70.15**   | **75.70** |\n\n> In addition, the proposed method may require more inference time due to LSTM module. Please compare the inference time with other VPT-based methods.\n\nThanks for the suggestion! The impact of the LSTM module on inference time is an important consideration. We include a detailed analysis of metrics like training and inference time per batch in the table below, where we fine-tune MAE pre-trained ViT-B/16 weights on CUB dataset. Our LSPT achieves comparable time computational overhead to GaPT, the state-of-the-art VPT baseline on self-supervised ViTs.\n\n| Method           | Training   time per batch (s) | Inference   time per batch (s) |\n|------------------|-------------------------------|--------------------------------|\n| GaPT   (ICML'23) | 0.2406                        | 0.0871                         |\n| LSPT   (ours)    | **0.2428**                        | **0.0872**                         |\n\n\n> For long-term modeling, why LSTM is used? The auto-aggressive Transformer layer can also be applied?\n\nThe choice of LSTM for long-term modeling helps introduce forget gate to enlarge the variance for prompt tokens. The auto-aggressive Transformer layer is applied among tokens, which could increase their similarity by weighted sum of token values. So, we believe there exists a risk of losing discrepancy among latent toke embeddings during training.  Additionally, we include experimental results comparing the performance of LSPT in the Table below when using LSTM versus auto-aggressive Transformer layers, providing insights into the effectiveness of our approach.\n\n| LPC         | CUB       | Flowers   | Cars      | Dogs      | NABirds   | AVG       |\n|-------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Transformer | 72.52     | 81.26     | 73.58     | 81.23     | 70.86     | 75.89     |\n| LSTM        | **73.86** | **82.32** | **74.75** | **82.05** | **71.73** | **76.94** |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672974364,
                "cdate": 1700672974364,
                "tmdate": 1700673034284,
                "mdate": 1700673034284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JHhiZntEXT",
            "forum": "M36m3buBVD",
            "replyto": "M36m3buBVD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission772/Reviewer_isM7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission772/Reviewer_isM7"
            ],
            "content": {
                "summary": {
                    "value": "Traditional Visual Prompt Tuning (VPT) relies on short-range learnable prompts from a model's immediate previous block, overlooking the potential of leveraging long-range interactions. This paper introduces LSPT to address this gap by incorporating long-range gated prompts, akin to temporal coding in the human brain, to prevent the loss of earlier learned parameters. Additionally, it employs patch tokens as spatial coding to continuously gather class-specific features, thus improving the model\u2019s capability in recognizing visual categories. The effectiveness of LSPT over conventional VPT methods was validated through extensive tests on 5 Fine-Grained Visual Categorization (FGVC) and 19 VTAB-1K benchmarks, where LSPT demonstrated remarkable performance improvements, setting new standards in the field of visual representation learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The performance of LSPT is outstanding. The ablation studies highlight the effectiveness of both CSPC and LPC, while the visualizations further underscore this efficacy.\n\n2. The authors address the problem of catastrophic forgetting in Visual Prompt Tuning, which is an insightful contribution.\n\n3. This paper is well organized and easy to follow."
                },
                "weaknesses": {
                    "value": "1. This paper appears to exhibit a substantial degree of similarity to the methodologies presented in the GaPT paper (Yoo et al., 2023). For instance, section 3.1.2 seems to be a rephrased version of Section 2, 'Visual Prompt Tuning,' from GaPT. Equations (1, 2, 3) in this paper are identical to equations (4, 5) in GaPT. Furthermore, Table (1, 2) and the corresponding experimental results are exactly the same as those in GaPT. The overall structure of this paper closely mirrors that of GaPT. It is highly irregular and unacceptable to replicate aspects of a research paper to such an extent.\n\n2. Including a delineated algorithmic framework would significantly improve the replicability of the proposed LSPT method.\n\n3. The connection between the two proposed modules, CSPC and LPC, is weak. There is no strong motivation presented to justify the inclusion of CSPC, especially since the title of the paper pertains solely to LPC. Nonetheless, the authors introduce CSPC first, which suggests a greater emphasis on the importance of CSPC.\n\n4. More visual prompt tuning papers should be included in the related works. There are only two papers introduced currently."
                },
                "questions": {
                    "value": "Besides the questions included in the weaknesses, some other questions are listed below. \n\n1.The scope of the proposed LSPT within the realm of visual prompt tuning is confined to SSL-ViTs. This limitation constrains the real-world applicability of the proposed LSPT method. Additionally, are there any other baselines within this specific experimental context?\n\n2. The visual representation of the pipeline in Figure 2 could be aesthetically improved. The rationale for using a non-standard rectangle to depict the LSTM is unclear. Moreover, the relationship between CSPC and LPC is not adequately illustrated. Implicitly, Figure 2 suggests that LPC serves as an incremental enhancement to CSPC, solely to boost performance.\n\n3. Have the authors replicated the baseline studies themselves, or have they merely cited the results from GaPT? It would be advantageous to detail the variations encountered during the reproduction of baseline and proposed methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698971852594,
            "cdate": 1698971852594,
            "tmdate": 1699636004485,
            "mdate": 1699636004485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iVrbrco6Qf",
                "forum": "M36m3buBVD",
                "replyto": "JHhiZntEXT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer isM7"
                    },
                    "comment": {
                        "value": "Dear Reviewer isM7,\n\nThank you for appreciating our approach. We will address your comments below.\n\n> This paper appears to exhibit a substantial degree of similarity to the methodologies presented in the GaPT paper (Yoo et al., 2023). For instance, section 3.1.2 seems to be a rephrased version of Section 2, 'Visual Prompt Tuning,' from GaPT. Equations (1, 2, 3) in this paper are identical to equations (4, 5) in GaPT. Furthermore, Table (1, 2) and the corresponding experimental results are exactly the same as those in GaPT. The overall structure of this paper closely mirrors that of GaPT. It is highly irregular and unacceptable to replicate aspects of a research paper to such an extent.\n\n\nWe acknowledge the reviewer's observation regarding the similarities between our work and the GaPT paper. It is important to note that our work initially builds upon the foundational concepts of GaPT, intending to improve upon them. We revised Section 3.1.2 to clarify our advancements beyond GaPT's scope and ensure that the text reflects our original contributions more distinctly. Furthermore, we updated our equations and tables to better differentiate our work from GaPT, emphasizing the unique aspects of LSPT.\n\n\n> Including a delineated algorithmic framework would significantly improve the replicability of the proposed LSPT method.\n\nTo enhance the replicability of our method, we agree with the reviewer's suggestion to include a detailed algorithmic framework. We add a pseudo-algorithm in our revised manuscript in Appendix Section B, providing clear steps and methodologies for implementing LSPT.\n\n\n> The connection between the two proposed modules, CSPC and LPC, is weak. There is no strong motivation presented to justify the inclusion of CSPC, especially since the title of the paper pertains solely to LPC. Nonetheless, the authors introduce CSPC first, which suggests a greater emphasis on the importance of CSPC.\n\nWe appreciate the feedback on the connection between CSPC and LPC. Our intention is to demonstrate that both spatial and long-term aspects are crucial for addressing forgetting in visual prompt tuning. We provide a more detailed explanation in the revised manuscript to justify the inclusion and parallel importance of both CSPC and LPC within LSPT. \n\nAdditionally, our title also includes spatial prompt tuning, which refers to the proposed CSPC (Class-aware Spatial Prompt Coding) strategically designed to perpetually amass class conscious features, thereby fortifying the model\u2019s prowess in distinguishing and identifying visual categories. Our method named Long-term Spatial Prompt Tuning in the title include both spatial and long-term aspects, reflecting the comprehensive scope of our work.\n\n\n\n> More visual prompt tuning papers should be included in the related works. There are only two papers introduced currently.\n\nOur current related works focus on vpt for self-supervised ViTs, but we recognize the necessity of including a broader range of related works in visual prompt tuning. In the revised manuscript, we incorporated additional references [1,2,3] on VPT to both self-supervised and supervised ViTs to provide a more comprehensive background and context for our research.\n\n[1] Das et al. Learning Expressive Prompting With Residuals for Vision Transformers. CVPR, 2023.\n\n[2] Wang et al. Adapting Shortcut With Normalizing Flow: An Efficient Tuning Framework for Visual Recognition. CVPR, 2023.\n\n[3] Jie et al. Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy. ICCV, 2023.\n\n> The scope of the proposed LSPT within the realm of visual prompt tuning is confined to SSL-ViTs. This limitation constrains the real-world applicability of the proposed LSPT method. Additionally, are there any other baselines within this specific experimental context?\n\nWe understand the reviewer's concern about the confined scope of LSPT to SSL-ViTs. To address this, we expand our experimental setup to compare baselines on ViT-B/16 pre-trained on supervised ImageNet-21k in the Table below, thereby demonstrating the generality and applicability of LSPT across different types of vision transformers.\n\n\n| Method                   | Nature | Specialized | Structured | AVG   |\n|--------------------------|--------|-------------|------------|-------|\n| VPT (ECCV\u201922)            | 78.48  | 82.43       | 54.98      | 69.42 |\n| EXPRES (CVPR\u201923) [1]        | 79.70  | 84.00       | 55.00      | 70.21 |\n| SNF (CVPR\u201923) [2]           | 83.79  | 86.13       | 59.61      | 74.10 |\n| Bi-AdaptFormer (ICCV\u201923) [3] | 82.11  | 86.40       | 62.43      | 74.73 |\n| LSPT (ours)              | **85.26**  | **88.57**       | **66.25**      | **77.95** |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672795010,
                "cdate": 1700672795010,
                "tmdate": 1700672795010,
                "mdate": 1700672795010,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CsDN9M4hil",
            "forum": "M36m3buBVD",
            "replyto": "M36m3buBVD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission772/Reviewer_7rGz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission772/Reviewer_7rGz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method called Long-term Spatial Prompt Tuning (LSPT) for adapting pre-trained Vision Transformers (ViTs) to downstream visual tasks using learnable prompt tokens. The key contributions are:\n\n- LSPT incorporates long-term gated prompts as a temporal coding layer to mitigate forgetting of parameters learned from earlier ViT blocks. This helps address the issue of \"temporal forgetting\" in previous prompt tuning methods. \n\n- LSPT introduces patch tokens with spatial prompt coding to accumulate class-specific features across blocks, addressing the issue of \"spatial forgetting\". \n\n- Extensive experiments on 5 FGVC and 19 VTAB benchmarks show LSPT achieves new state-of-the-art results compared to prior prompt tuning methods like VPT and GaPT.\n\n- Analysis shows LSPT's temporal and spatial coding help alleviate forgetting issues in attention maps and prompt-patch similarity compared to prior methods.\n\nIn summary, LSPT advances visual prompt tuning by integrating ideas from neuroscience to address forgetting issues, leading to better adaptation and transfer learning performance. The temporal and spatial coding in LSPT are novel techniques for prompt tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Here is a critical assessment of the strengths of this paper:\n\n**Originality**: The ideas of incorporating temporal and spatial coding for prompt tuning are novel and not explored before in prior VPT methods. The use of long-term gated prompts and patch tokens as spatial prompts are creative ways to address the issues of forgetting in VPT. Applying concepts from neuroscience like temporal and spatial coding to transformer prompt tuning is an original combination.\n\n**Quality**: The paper is technically strong, with a clearly explained intuition and motivation behind LSPT's design. The method is evaluated thoroughly on multiple datasets, convincingly demonstrating its effectiveness over baselines. The results are state-of-the-art, showing the quality of the approach.\n\n**Clarity**: The paper is well-written and easy to follow. The description of the temporal and spatial forgetting issues in VPT provides good motivation. The Class-aware Spatial Prompt Coding and Long-term Prompt Coding modules are explained clearly. The ablation studies isolate their contributions. \n\n**Significance**: LSPT makes a significant advance in visual prompt tuning, an important area for adapting pre-trained vision models. The ideas could inspire more work on alleviating forgetting in prompt tuning and transfer learning. Outperforming prior SOTA like GaPT demonstrates the significance of the improvements. The gains are substantial across multiple benchmarks.\n\nIn summary, I found this paper to exhibit strong originality in applying neuroscience-inspired concepts to VPT, technically sound modeling and evaluation, with clearly presented ideas that significantly advance prompt tuning research. The novelty of temporal and spatial coding for prompts is a compelling contribution."
                },
                "weaknesses": {
                    "value": "* While the proposed modules make intuitive sense, the explanations lack quantitative analysis or theorems to rigorously justify the designs. Some ablation studies analyze contributions but more analysis connecting the methods to mitigating forgetting could strengthen the approach.\n* The spatial prompt coding uses a simple patch token averaging, which seems like a heuristic. More sophisticated ways to accumulate spatial/positional information may exist. This component could likely be improved.\n* The long-term prompt coding relies on a single LSTM layer. Ablations could explore using multiple LSTM layers or comparing to other sequential modeling approaches like GRUs.\n* The computational overhead and memory requirements of LSPT are not analyzed. This could be important for deployments.\n* There is no investigation of how the approach may fair for other vision tasks beyond classification like detection and segmentation."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699247975586,
            "cdate": 1699247975586,
            "tmdate": 1699636004423,
            "mdate": 1699636004423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dc5YCcIc9w",
                "forum": "M36m3buBVD",
                "replyto": "CsDN9M4hil",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission772/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7rGz"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7rGz,\n\nThank you for appreciating our approach. We will address your comments below.\n\n> While the proposed modules make intuitive sense, the explanations lack quantitative analysis or theorems to rigorously justify the designs. Some ablation studies analyze contributions but more analysis connecting the methods to mitigating forgetting could strengthen the approach.\n\n\nWe appreciate the reviewer's suggestion for a more rigorous quantitative analysis. We introduce deeper quantitative analysis on the forgetting gate in learned LSTM to further justify our design choices. Specifically, we calculate the forget gate to decide how much of the previous prompt will be forgotten and how much of the previous prompt will be used in next steps, as shown in Table below. The number in the *Forgetting* colomn means how much information are discarded on average before feeding to the next layer. These quantitative results further elucidate how the forgetting gate aids in reducing forgetting in LSPT compared to VPT and GaPT. This analysis include detailed examinations of the forgetting metrics across different prompt tuning methods, demonstrating the effectiveness of LSPT's approach.\n\nTo be noticed, compared to VPT and GaPT whose ratios of forgetting are fixed and decides merely by the architecture or hyperparameter, the ratio of forgetting for our LSPT is an average over all the layers and data, controlled dynamically by the learned LSTM. This flexibility and selectivity of rememberring and forgetting also contributes to the final improvement. \n\n\n| Method | Forgetting | Peformance |\n| -------- | -------- | -------- |\n| VPT-deep (ECCV'22)     | 1.00     | 68.33     |\n| GaPT (ICML'23)     | 0.85     | 70.56     |\n| LSPT (ours)     | **0.68**     | **73.86**     |\n\n\n> The spatial prompt coding uses a simple patch token averaging, which seems like a heuristic. More sophisticated ways to accumulate spatial/positional information may exist. This component could likely be improved.\n\nWe acknowledge the reviewer's concern regarding the simplicity of our spatial prompt coding method. To address this, we conduct additional ablation studies exploring more sophisticated methods for accumulating spatial and positional information. Specifically, we investigate alternative techniques like k-means clustering on spatial patch tokens to enrich the spatial coding process and provide a comparative analysis of these methods. The overall performance get an increase on all the compared datasets when we use k-means to incorporate spatial information, while the additional computational cost leads to a trade-off. To be noticed, more exploration space on this spatial prompt coding will leave for future work.\n\n| Spatial Prompt | CUB       | Flowers   | Cars      | Dogs      | NABirds   |\n|----------------|-----------|-----------|-----------|-----------|-----------|\n| average        | 73.86     | 82.32     | 74.75     | 82.05     | 71.73     |\n| k-means        | **74.32** | **82.56** | **74.87** | **82.23** | **71.86** |\n\n\n> The long-term prompt coding relies on a single LSTM layer. Ablations could explore using multiple LSTM layers or comparing to other sequential modeling approaches like GRUs.\n\n\nThanks for the valuable suggestion! We extend our ablation studies to include experiments with multiple LSTM layers and compare their efficacy with other sequential modeling approaches, such as GRUs. Table below shows the comparison results with two LSTM layers and one GRU layers. We can observe that here is a trade-off between params and performance, and a single LSTM layer achieves the good balance on both. This is why our long-term prompt coding module using a single LSTM layer is not too complex. These results further help us to validate the choice of LSTM in our approach and explore potential improvements. \n\n\n| LPC      | Params    | CUB       | Flowers   | Cars      | Dogs      | NABirds   |\n|----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| 1 # LSTM | 1.08x     | 73.86     | 82.32     | 74.75     | 82.05     | 71.73     |\n| 2 # LSTM | 1.14x | **74.57** | **82.95** | **75.52** | **82.97** | **72.45** |\n| 1 # GRU  | **1.06x**     | 72.95     | 81.53     | 73.91     | 81.26     | 70.97     |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672694136,
                "cdate": 1700672694136,
                "tmdate": 1700672694136,
                "mdate": 1700672694136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]