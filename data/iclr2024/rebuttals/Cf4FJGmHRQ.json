[
    {
        "title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images"
    },
    {
        "review": {
            "id": "a96vmp8Zcb",
            "forum": "Cf4FJGmHRQ",
            "replyto": "Cf4FJGmHRQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3238/Reviewer_pDdt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3238/Reviewer_pDdt"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel solution named PAC-FNO for image recognition, demonstrating the ability to simultaneously handle images of varying resolutions and resist the impact of various types of input-induced natural variations within a singular model in recognition tasks. The proposed parallel-structured and all-component Fourier neural operator (PAC-FNO), building on the resolution invariance of FNOs in the frequency domain, notably eliminates the ideal low-pass filter found in vanilla FNOs. Additionally, it transforms the traditional serial architectures into a parallel structure, thereby considering a broader range of frequency components, retaining high-frequency details, and notably enhancing performance, especially in fine-grained datasets. The proposed approach introduces a two-stage training method that fine-tunes pre-trained image recognition models in conjunction with PAC-FNO, allowing the acquisition of commonalities among various input resolutions with minimal modifications to the backbone classification network. Through conducted experiments, the authors effectively showcase the performance of PAC-FNO, significantly improving accuracy in comparison to existing baseline models. The manuscript is well-written, and the experiments conducted are comprehensive and convincingly articulated."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper exhibits a high level of innovation. Although neural network operators based on the Fourier domain transformation for learning, due to their excellent characteristics in resolution invariance, have been recognized and applied in various areas, especially for enhancing pre-processing operators in variable-resolution input networks. However, the authors, motivated by the rational desire to retain high-frequency image details, proposed for the first time to eliminate the inherent low-pass filters in the model. Additionally, they introduced a popular parallel structure similar to Multi-head Self-Attention, further enhancing the network's performance while expanding the design philosophy of relevant operators.\n\n2. The presentation of this paper is professional and fluent. It has almost no expression errors and clearly elucidates the authors' contributions.\n\n3. The paper conducted extensive and meticulous experiments, utilizing seven image recognition benchmark datasets and applying the operators to four different backbone networks. The authors closely follow the cutting-edge developments in the field, employing more advanced VIT and ConvNeXt for experimentation, which makes the results highly persuasive."
                },
                "weaknesses": {
                    "value": "1. Although the author compared super-resolution (SR) models for variable resolution inputs, the compared SR models are outdated and lack representation across various upscaling factors for super-resolution reconstruction. The field of super-resolution has seen significant advancements recently; thus, it is recommended to select more appropriate comparative algorithms.\n\n2. The primary advantage of Fourier Neural Operators (FNOs) lies in their use of frequency domain processing for resolution invariance. As a learnable enhancement operator, it's expected to exhibit some resilience to input natural variations. However, the author hasn't provided a detailed and explanatory analysis of the mechanisms where the operator shows robustness against natural variations. Moreover, the chosen input variations in the experiments, like fog, brightness, spatter, and saturate, represent basic degradation scenarios that can be addressed without deep learning methods. Therefore, regarding resilience to input natural variations, this might not be sufficiently emphasized as a highlight of the paper. The paper suggests exploring degradation in real-world scenarios in future work, indicating that the authors are aware of the limitations in terms of experimental performance or the algorithm proposed. However, such scenarios represent fundamental problems studied in the field of Image Recognition (IR) and hold significant practical application implications. Actually, certain degradation processes might affect high or low frequency details in the image's frequency domain. For instance, blur involves the loss of high-frequency details, prompting the author to conduct a mechanistic analysis combining frequency domain and degradation processes to enhance this aspect's interpretability.\n\n3. The original intent behind the existence of ideal low-pass filters was to reduce the number of parameters and computational complexity. While the author's innovative design to remove the inherent low-pass filter is intuitively comprehensible, the associated trade-offs are not discussed in the manuscript. It would be beneficial to provide supplementary explanations to demonstrate the worthiness of such a modification.\n\n4. The experiments thoroughly prove the advantages of parallel architectures and claim that this approach encapsulates more frequency components. However, they lack further detailed explanations and justifications."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3238/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3238/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3238/Reviewer_pDdt"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3238/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646911645,
            "cdate": 1698646911645,
            "tmdate": 1700718065403,
            "mdate": 1700718065403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tlFvKuEUoT",
                "forum": "Cf4FJGmHRQ",
                "replyto": "a96vmp8Zcb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the encouraging remarks about our contribution and extensive experimental results and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer\u2019s questions.\n\n> 1. Although the author compared super-resolution (SR) models for variable resolution inputs, the compared SR models are outdated and lack representation across various upscaling factors for super-resolution reconstruction. The field of super-resolution has seen significant advancements recently; thus, it is recommended to select more appropriate comparative algorithms.\n\n$\\to$ As suggested, we conduct the experiments of other methods with a state of the art SR model and summarize the results in the revision (will update it by  November 20th (AOE)). \n\n> 2. The primary advantage of Fourier Neural Operators (FNOs) lies in their use of frequency domain processing for resolution invariance. As a learnable enhancement operator, it's expected to exhibit some resilience to input natural variations. However, the author hasn't provided a detailed and explanatory analysis of the mechanisms where the operator shows robustness against natural variations. Moreover, the chosen input variations in the experiments, like fog, brightness, spatter, and saturate, represent basic degradation scenarios that can be addressed without deep learning methods. Therefore, regarding resilience to input natural variations, this might not be sufficiently emphasized as a highlight of the paper. The paper suggests exploring degradation in real-world scenarios in future work, indicating that the authors are aware of the limitations in terms of experimental performance or the algorithm proposed. However, such scenarios represent fundamental problems studied in the field of Image Recognition (IR) and hold significant practical application implications. Actually, certain degradation processes might affect high or low frequency details in the image's frequency domain. For instance, blur involves the loss of high-frequency details, prompting the author to conduct a mechanistic analysis combining frequency domain and degradation processes to enhance this aspect's interpretability.\n\n$\\to$ As you mentioned, degradations change the high-frequency and low-frequency information in original images. Figure 8 in the Appendix F.8 shows that there are many changes in high-frequency when degradations are visualized in the frequency domain. Since existing FNO-based models that use a low-pass filter remove high-frequency, the degraded image loses not only the degradation factor but also the information of the original image. However, our proposed PAC-FNO uses high-frequency information, so it shows better performance in degradation.\nRegarding the future work, we would like to address combined degradation types as the natural variations (e.g., mixed with motion blur and fog that may occur in the real-world), not try to address new types of variations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232355670,
                "cdate": 1700232355670,
                "tmdate": 1700232355670,
                "mdate": 1700232355670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tWdCblN2JS",
                "forum": "Cf4FJGmHRQ",
                "replyto": "a96vmp8Zcb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 3. The original intent behind the existence of ideal low-pass filters was to reduce the number of parameters and computational complexity. While the author's innovative design to remove the inherent low-pass filter is intuitively comprehensible, the associated trade-offs are not discussed in the manuscript. It would be beneficial to provide supplementary explanations to demonstrate the worthiness of such a modification.\n\n$\\to$ We provide FLOPs and runtime on data at different resolutions. Existing FNO models used low-pass filters due to computational complexity, but the channel size was increased to fully utilize low-frequency components. However, PAC-FNO maintained the 3 channel size of the image to use both high-pass filter and low-pass filter. As a result, PAC-FNO showed similar levels of FLOPs and Runtimes as existing FNO models, but showed better performance because it used additional high-frequency components.\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\n> 4. The experiments thoroughly prove the advantages of parallel architectures and claim that this approach encapsulates more frequency components. However, they lack further detailed explanations and justifications.\n\n$\\to$ The parallel configuration of AC-FNO blocks captures more information than a serial structured model in the first layer, which is directly related to the data. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration captures more high frequencies than the serial configuration. In other words, the parallel structured model captured both more low-frequency and high-frequency components than the serial structured model. As a result, parallel configuration of AC-FNO blocks show a better performance than serial configuration in Figure 4 in Section 4.3. We will add this explanation and justification in Appendix F.8 of the revision. Thank you!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232392034,
                "cdate": 1700232392034,
                "tmdate": 1700232392034,
                "mdate": 1700232392034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IA6eLbtbY2",
                "forum": "Cf4FJGmHRQ",
                "replyto": "a96vmp8Zcb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450334332,
                "cdate": 1700450334332,
                "tmdate": 1700460952781,
                "mdate": 1700460952781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hQif8ZTUMR",
                "forum": "Cf4FJGmHRQ",
                "replyto": "a96vmp8Zcb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer pDdt,\n\nWe appreciate the reviewer\u2019s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review\u2019s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635315844,
                "cdate": 1700635315844,
                "tmdate": 1700635315844,
                "mdate": 1700635315844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9SGDGfXicx",
                "forum": "Cf4FJGmHRQ",
                "replyto": "hQif8ZTUMR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Reviewer_pDdt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Reviewer_pDdt"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for your time! My major concerns have already been addressed. However, I have several problems.\n- Some degradation-resistant algorithms seem to do the same thing as you and I want to see further discussions in your paper to make me better understand your contribution, such as Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23) and HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS).\n\n- Although this paper focuses on solving the recognition problem, I'd like to see the author expand it to other extreme tasks, such as camouflaged object segmentation [1, 2] and concealed object segmentation [3]. Perhaps there isn't enough time for the author to continue with the corresponding experiments, but I think it's also more useful to see the author's point of view on a high level for other interested readers to get inspiration from this paper.\n\n[1] Camouflaged object detection, CVPR20.\n[2] Camouflaged Object Detection with Feature Decomposition and Edge Reconstruction, CVPR23.\n[3] Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping, NeurIPS23."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637291260,
                "cdate": 1700637291260,
                "tmdate": 1700637291260,
                "mdate": 1700637291260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PA3p1JyTEN",
                "forum": "Cf4FJGmHRQ",
                "replyto": "a96vmp8Zcb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy to hear that your major concern has been resolved. We hope our additional answers will also help you with your problems.\n> 1. Some degradation-resistant algorithms seem to do the same thing as you and I want to see further discussions in your paper to make me better understand your contribution, such as Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23) and HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS).\n\n$\\to$ Thank you for pointing out the work. In a nutshell, both papers aim to convert low-quality images into high-quality images.\n[1] generates a high-quality image by fusing low-quality images generated from multiple sensors. [2] generates high-quality medical images using a model trained with unpaired low-quality medical images and high-quality medical images.\n\nHowever, there is a difference between the two studies and our PAC-FNO. PAC-FNO handles low-quality images including low-resolution and input variations. The low-quality images in these two studies include only input variations such as images with fog or noise. In other words, PAC-FNO handles not only the input variations mentioned in those studies, but also low-resolution images, which occurs more frequently in real world deployment scenarios, *e.g.*, CCTV. \n\n[1] Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion (ICCV 23)\n\n[2] HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance (TNNLS)\n\n\n> 2. Although this paper focuses on solving the recognition problem, I'd like to see the author expand it to other extreme tasks, such as camouflaged object segmentation [1, 2] and concealed object segmentation [3]. Perhaps there isn't enough time for the author to continue with the corresponding experiments, but I think it's also more useful to see the author's point of view on a high level for other interested readers to get inspiration from this paper.\n\n$\\to$ Very interesting suggestion!  As PAC-FNO addresses the semantic mapping (*i.e.*, classification) problem as it has the most wide applicability, we believe it could be extended to suggested extreme segmentation tasks with a trivial extension. Since the the dense (*i.e.*, pixel-wise) prediction task is essentially the semantic mapping (*i.e.*, classification) problem by taking into account the nearby pixels, given sufficiently large receptive fields tuned by convolution filters for CNNs or token patches for transformers. Empirical validation on this suggestion would be a great future work to widen the applicability of the PAC-FNO. Thank you!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697190067,
                "cdate": 1700697190067,
                "tmdate": 1700697281652,
                "mdate": 1700697281652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gR1XvNcfN8",
                "forum": "Cf4FJGmHRQ",
                "replyto": "PA3p1JyTEN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Reviewer_pDdt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Reviewer_pDdt"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "The discussions above are excellent, and I wish I could see them in the revised manuscript, which I think would help readers better understand the contributions of this article.\n\nGiven that my major concerns have already been addressed, I am now happy to update my rating to 6. Best of luck to the authors!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718049681,
                "cdate": 1700718049681,
                "tmdate": 1700718049681,
                "mdate": 1700718049681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Ae6eUADAv",
            "forum": "Cf4FJGmHRQ",
            "replyto": "Cf4FJGmHRQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3238/Reviewer_WCy8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3238/Reviewer_WCy8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO) to address visual recognition under low-quality images. By operating in the frequency domain, PAC-FNO is able to learn the semantics of images in various resolutions and/or natural variations for challenging image recognition with a single model. The proposed PAC-FNO is capable of handling both low-resolution and input variations typically observed in low-quality images with a single model. It can also be attached to a downstream visual recognition model, which is beneficial for handling multiple input variations at once and minimizing the changes in the downstream model during fine-tuning. In the evaluation with four visual recognition models and seven datasets, the proposed PAC-FNO achieves excellent performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is organized well.\n2. Extensive experimental results are provided to illustrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Is there a more advanced choice for the SR baseline model used for comparison in your experimental setup? This will affect the fairness of the performance of your experiment?\n2. It can be found that in ViT-B16, PAC-FNO shows not very good results at all low resolutions compared to other methods. What caused this phenomenon to occur? Is your method also unfriendly to other Transformer methods?\n3. The ideal low-pass filter in the FNO block removes detailed image signals that play an important role in classification in the fine-grained dataset. Is this conclusion applicable to Transformer based image classification methods? More quantitative results should be provided to confirm the universality of the proposed method.\n4. The ablation experiments about the results of the zero-padding operation and the exclusion of the low pass filter need to be completed to explain the design of the AC-FNO block."
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3238/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756403996,
            "cdate": 1698756403996,
            "tmdate": 1699636271942,
            "mdate": 1699636271942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wZoQ8C2kiY",
                "forum": "Cf4FJGmHRQ",
                "replyto": "1Ae6eUADAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the encouraging remarks about our paper\u2019s extensive experimental results and organizing and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer\u2019s questions.\n\n> 1. Is there a more advanced choice for the SR baseline model used for comparison in your experimental setup? This will affect the fairness of the performance of your experiment?\n\n$\\to$ For fairness of experiment in SR baseline, we will report additional baseline equipped with the latest super-resolution model in Table 13 in Appendix F.3 of the revision that will be updated shortly around November 20th AoE.\n\n> 2. It can be found that in ViT-B16, PAC-FNO shows not very good results at all low resolutions compared to other methods. What caused this phenomenon to occur? Is your method also unfriendly to other Transformer methods?\n\n$\\to$ PAC-FNO generally performs well in low resolution but only worse than A-FNO. A-FNO is a model proposed as a token mixer for transformers, so it seems particularly friendly when combined with Transformer-based models. \n\n> 3. The ideal low-pass filter in the FNO block removes detailed image signals that play an important role in classification in the fine-grained dataset. Is this conclusion applicable to Transformer based image classification methods? More quantitative results should be provided to confirm the universality of the proposed method.\n\n$\\to$ Yes, high-frequency components play a more important role in image classification, especially fine-grained datasets. So, to see the effect of high frequency component removal for nuanced classification problem, we conduct  a fine-grained classification experiment with the ViT model. On the Oxford-IIIT Pets dataset, PAC-FNO, which captures the high-frequency components, performs better at all resolutions than FNO with an ideal low-pass filter.compared to FNO with an ideal low-pass filter. We add these results in Tables 16 and 17 in Appendix F.4 of the revision.\n\n| Oxford-IIIT Pets |  28  |  32  |  56  |  64  |  112 |  128 |  224 |\n|:----------------:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|        FNO       | 26.3 | 33.0 | 58.1 | 64.8 | 82.9 | 85.8 | 91.3 |\n|      PAC-FNO     | 40.3 | 46.2 | 69.0 | 72.5 | 86.8 | 89.2 | 92.2 |\n\n> 4. The ablation experiments about the results of the zero-padding operation and the exclusion of the low pass filter need to be completed to explain the design of the AC-FNO block. \n\n$\\to$ Zero-padding works to upscale low-resolution images that are smaller than the target resolution in the frequency domain to the target resolution. In other words, if there is no zero-padding operation, low-resolution images cannot be processed. \nThen, we provide the ablation experiments for low-pass filters in the first row in Table 21 in Appendix F.7 of the revision. When a low-pass filter is used, performance decreases in terms of accuracy because high-frequency components cannot be considered, but the performance decrease is large for input variation.\n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232272324,
                "cdate": 1700232272324,
                "tmdate": 1700232272324,
                "mdate": 1700232272324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T5hShyNZDJ",
                "forum": "Cf4FJGmHRQ",
                "replyto": "1Ae6eUADAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. In addition, we report OSRT (fine-tune) that combines super-resolution and fine-tuning methods. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model.  As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|              |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|  ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |  **58.9**   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                 |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n| ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |          |\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450313662,
                "cdate": 1700450313662,
                "tmdate": 1700460933070,
                "mdate": 1700460933070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eIQVK6qbNn",
                "forum": "Cf4FJGmHRQ",
                "replyto": "1Ae6eUADAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer WCy8,\n\nWe appreciate the reviewer\u2019s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review\u2019s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635285078,
                "cdate": 1700635285078,
                "tmdate": 1700635285078,
                "mdate": 1700635285078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vC3Jx4r6zF",
                "forum": "Cf4FJGmHRQ",
                "replyto": "eIQVK6qbNn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Reviewer_WCy8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Reviewer_WCy8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your time! My major concerns have already been addressed. I keep my original score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672339794,
                "cdate": 1700672339794,
                "tmdate": 1700672339794,
                "mdate": 1700672339794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Zfm29TuCw",
            "forum": "Cf4FJGmHRQ",
            "replyto": "Cf4FJGmHRQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3238/Reviewer_nwGj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3238/Reviewer_nwGj"
            ],
            "content": {
                "summary": {
                    "value": "This work has developed a neural network architecture for image recognition that is designed to address the influence of complex degradation factors. It aims to capture both low-frequency and high-frequency components to balance accuracy and generalization. The authors first propose to discard the low-pass filters in the existing FNO structure to retain all frequency components. Subsequently, a parallel structure is introduced to further enhance the utilization of frequency domain information. Finally, the authors design a two-stage training strategy to ensure performance stability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The overall paper has a clear logical structure, and the explanation of the methodology and the presentation of the constructed mechanisms are intuitive and easy to understand.\n2. The author provides a sufficiently detailed explanation for the motivation behind each component in PAC-FNO.\n3. The problem that this work aims to address holds a certain degree of practical application value."
                },
                "weaknesses": {
                    "value": "1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions)."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3238/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698938241615,
            "cdate": 1698938241615,
            "tmdate": 1699636271839,
            "mdate": 1699636271839,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dy08a0S7KB",
                "forum": "Cf4FJGmHRQ",
                "replyto": "1Zfm29TuCw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the encouraging remarks about our paper\u2019s clear methodology explanation and the valuable feedback from the reviewer. We hope that our responses will solve the reviewer\u2019 questions.\n\n> 1. The abandonment of the low-pass filter is one of the main innovations in this work. Although the author provides an explanation for the motivation behind this operation, it is still recommended that the author conduct ablative experiments to analyze the impact of low-frequency/high-frequency information on accuracy/generalization.\n\n$\\to$ Thank you for the suggestion! We provide an analysis of the impact of low and high-frequency information on accuracy/generalization through ablation experiments in Table 21 in Appendix F.7 of revision. Compared to PAC-FNO, accuracy and generalization decrease when using low-pass filter or high-pass filter. PAC-FNO with low pass filter show similar performance in ImageNet-1k compared with our PAC-FNO, but show a decrease in performance in terms of generalization in ImageNet-C/P Fog. On the other hand, when using a high-pass filter, it is expected to show good performance in ImageNet-C/P Fog, but it does not show good performance in terms of generalization because the performance is also poor in ImageNet-1k. Therefore, PAC-FNO, which uses both low-frequency and high-frequency components, only shows good performance in terms of accuracy/generalization. \n\n|           ImageNet-1k           |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 53.5 | 71.4 | 78.7 | 79.0 |\n| PAC-FNO (high pass filter) | 21.6 | 49.4 | 68.2 | 74.8 |\n|             PAC-FNO             | 58.9 | 74.5 | 80.2 | 81.5 |\n\n|         ImageNet-C/P Fog        |  32  |  64  |  128 |  224 |\n|:-------------------------------:|:----:|:----:|:----:|:----:|\n|  PAC-FNO (low pass filter) | 18.0 | 41.7 | 52.4 | 54.4 |\n| PAC-FNO (high pass filter) |  5.92  | 23.0 | 43.2 | 50.2 |\n|             PAC-FNO             | 25.4 | 48.2 | 60.1 | 62.8 |\n\n> 2. As for parallel architecture, the relevant experimental results have indeed proven its effectiveness. However, the explanation of parallel architecture in the method section appears somewhat lacking. It is hoped that the author can provide further analysis of the mechanism that enables it to be effective.\n\n$\\to$ The reason that it is effective is that the parallel configuration captures both low and high-frequency components while the serial architecture captures low-frequency components only (first paragraph in Sec. 3.2). Both low and high-frequency components must be captured to achieve good accuracy/generalization. This can be confirmed in Table 21 in Appendix F.7 in the revision. We visualize what frequency the parallel configuration and serial configuration capture in Figure 7 in Appendix F.8 of revision. Figure 7 shows that the parallel configuration more captures high frequencies than the serial configuration."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232162158,
                "cdate": 1700232162158,
                "tmdate": 1700232294017,
                "mdate": 1700232294017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rKWh4YjrIK",
                "forum": "Cf4FJGmHRQ",
                "replyto": "1Zfm29TuCw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 3. In terms of comparative experiments, the methods used by the author for comparison appear to be lacking in both quantity and novelty. The comprehensiveness of the complex scenarios considered by the author is commendable, but it is hoped that the author can still increase the comparison results with more advanced works to more effectively validate the superiority of the proposed method.\n\n$\\to$ We believe that advanced work to effectively validate the superiority of our method is an advanced super-resolution method, as super-resolution models are relatively old models. Therefore, we provide additional comparison to the method equipped with the latest super-resolution model in Appendix F.4 of the revision that will be updated shortly around November 20th AoE. Furthermore, we report additional baselines that combine super-resolution and fine-tuning methods in rows 5-6 in Table 13 in Appendix F.4 of the revised paper. That is, we fine-tune the pre-trained classification model with low-resolution images upscaled by a super-resolution model. This method showed worse performance than our PAC-FNO. \nIn addition, super-resolution methods are needed for each resolution. In other words, upscaling models of x8, x4, and x2 are needed to handle 28, 56, and 112 resolution, respectively. In contrast, our proposed PAC-FNO can handle images of all resolutions with an additional 3.65M network and shows good performance.\nIf the advanced works you intend are not super-resolution models, please let us know.\n\n> 4. The author mentions the advantages of this work in terms of efficiency, but it seems that no experimental analysis related to efficiency has been provided (such as FLOPs and runtime on data at different resolutions).\n\n$\\to$ We did not mention the efficiency as the advantage of this work. Instead, we stated \"efficacy of our neural operator-based mechanism\" (not the \u201cefficiency\u201d) in the last paragraph of Section 4.4 is that neural operator-based mechanism is suitable for real-world applications because it can handle a variety of resolutions without the process of resizing to the target resolution. But for curiosity, we also compare the efficiency of our method to the other methods in the following table:\n\n| ImageNet-1k |    Metrics   |    28   |   56   |   112   |  224  |\n|:-----------:|:------------:|:-------:|:------:|:-------:|:-----:|\n|    Resize   |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|  Fine-tune  |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.006  |  0.006 |  0.006  | 0.006 |\n|     DRLN    |    GFLOPs    |  180.66 | 412.05 | 1200.50 |  8.96 |\n|             | Runtimes (s) |  0.378  |  0.498 |  0.913  | 0.007 |\n|     DRPN    |    GFLOPs    | 1220.42 | 576.94 |  387.88 |  8.96 |\n|             | Runtimes (s) |  0.1532 |  0.164 |  0.171  | 0.007 |\n|     FNO     |    GFLOPs    |   9.78  |  9.78  |   9.78  |  9.78 |\n|             | Runtimes (s) |  0.016  |  0.016 |  0.016  | 0.016 |\n|     UNO     |    GFLOPs    |   9.10  |  9.10  |   9.10  |  9.10 |\n|             | Runtimes (s) |  0.018  |  0.018 |  0.018  | 0.018 |\n|     AFNO    |    GFLOPs    |   8.96  |  8.96  |   8.96  |  8.96 |\n|             | Runtimes (s) |  0.010  |  0.010 |  0.010  | 0.010 |\n|   PAC-FNO   |    GFLOPs    |   8.98  |  8.98  |   8.98  |  8.98 |\n|             | Runtimes (s) |  0.013  |  0.013 |  0.013  | 0.013 |\n\nAs a result, PAC-FNO showed the most efficiency in terms of FLOPs and runtimes except AFNO."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232172628,
                "cdate": 1700232172628,
                "tmdate": 1700232172628,
                "mdate": 1700232172628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jfU6XFka2r",
                "forum": "Cf4FJGmHRQ",
                "replyto": "1Zfm29TuCw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We provide an additional comparison to the method equipped with the latest super-resolution model. The results are in the following table. OSRT [1] is a state-of-the-art model in the super-resolution domain but it does not support $\\times$8 upscale. Therefore, we only use the $\\times$2 and $\\times$4 upscale models of OSRT. As a result, PAC-FNO shows better performance than OSRT and OSRT (fine-tune) methods. We also report these results in Table 13 in Appendix F.3 of the revision. \n\n|  Imagenet-1k  |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |  224 |\n|:-------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:----:|\n|               |     DBPN    |    Top-1 Acc (%)    |   40.7   |     -    |   68.2   |     -    |   79.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |   -  |\n|               |     DBPN    |    Top-1 Acc (%)    |   **60.8**   |     -    |   72.5   |     -    |   76.7   |     -    | 82.5 |\n|               | (Fine-tune) | # of Parameters (M) | 23.21 |     -    |   10.43   |     -    |   5.95   |     -    | - |\n| ConvNeXt-Tiny |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   61.4   |     -    |   75.4   |     -    | 82.5 |\n|               |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |   -  |\n|               |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   71.2   |     -    |   78.4   |     -    | 82.1 |\n|               | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |   -  |\n|               |   PAC-FNO   |    Top-1 Acc (%)    |   58.9   | **63.2** | **77.6** | **76.2** | **80.2** | **80.7** | 81.8 |\n|              |             | # of Parameters (M) |          |          |          |   3.65   |          |          |      |\n\n| Imagenet-C/P Fog |    Method   |        Metric       |    28    |    32    |    56    |    64    |    112   |    128   |    224   |\n|:----------------:|:-----------:|:-------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|                  |     DBPN    |    Top-1 Acc (%)    |   0.67   |     -    |   0.99   |     -    |   1.32   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |   23.21  |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|                  |     DBPN    |    Top-1 Acc (%)    |   21.8   |     -    |   42.3   |     -    |   56.8   |     -    |   61.0   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   10.43  |     -    |   5.95   |     -    |     -    |\n|   ConvNeXt-Tiny  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   19.4   |     -    |   37.9   |     -    |   58.4   |\n|                  |             | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.93  |     -    |     -    |\n|                  |     OSRT    |    Top-1 Acc (%)    |     -    |     -    |   42.3   |     -    |   56.4   |     -    |   59.4   |\n|                  | (Fine-tune) | # of Parameters (M) |     -    |     -    |   11.93  |     -    |   11.79  |     -    |     -    |\n|                  |   PAC-FNO   |    Top-1 Acc (%)    | **25.4** | **30.4** | **48.2** | **51.7** | **60.1** | **61.4** | **62.8** |\n|         `        |             | # of Parameters (M) |          |          |          |   3.65   |          |          |\n\n\n[1] Yu, Fanghua, et al. \"OSRT: Omnidirectional image super-resolution with distortion-aware transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450283436,
                "cdate": 1700450283436,
                "tmdate": 1700460901023,
                "mdate": 1700460901023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D9b1rmucXF",
                "forum": "Cf4FJGmHRQ",
                "replyto": "1Zfm29TuCw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3238/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer nwGj,\n\nWe appreciate the reviewer\u2019s time and effort in reviewing our manuscript and insightful comments. \n\nAs the closure of the discussion period is approaching, we would like to bring the review\u2019s attention and check if the reviewer could let us know whether the concerns or the misunderstanding have been addressed. \n\nIf this is the case, we would appreciate if you could adjust your rating accordingly. \n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3238/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635265322,
                "cdate": 1700635265322,
                "tmdate": 1700635265322,
                "mdate": 1700635265322,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]