[
    {
        "title": "Fair Attribute Classification via Distance Covariance"
    },
    {
        "review": {
            "id": "z1hLRvj5M0",
            "forum": "1JuMFjSkpD",
            "replyto": "1JuMFjSkpD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4736/Reviewer_zY1Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4736/Reviewer_zY1Z"
            ],
            "content": {
                "summary": {
                    "value": "In order to improve independence between a model's predictions and sensitive attributes, this paper utilizes empirical distance as a constraint.  This constrained problem is then optimized using the Lagrangian dual method to find a better trade-off between accuracy and\nfairness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Results show that the proposed approach can achieve good DP and EO at the same time while maintaining a good accuracy. Which are not commonly seen results.\n\n2. Theorem 4 provides the convergence of using empirical distance covariance to estimate population distance covariance.\n\n3. The proposed solution is technically sound.\n\n4. The presentation is clear."
                },
                "weaknesses": {
                    "value": "1. Section 3.3 claims that optimizing for Eq. (6) leads to an optimal case where both DP and EO are satisfied. This contradicts to the fact that (also mentioned in Section 3.3) \"It is only possible to achieve both DP and EO when the sensitive attributes Z are independent of the labels Y.\" Therefore, I do not think it justifies the claim that the proposed approach can achieve both DP and EO at the same time.\n\n2. Most of the baselines optimize for DP (only FSCL optimizes for EO and is primarily tailored for image datasets). More baseline methods specifically optimizing for EO should be included.\n\n3. More tabular data experiments should be conducted to confirm the finding. E.g. on datasets provided by [1].\n\n[1] Ding, Frances, Moritz Hardt, John Miller, and Ludwig Schmidt. \"Retiring adult: New datasets for fair machine learning.\" Advances in neural information processing systems 34 (2021): 6478-6490."
                },
                "questions": {
                    "value": "1. Can you discuss more about why the proposed approach can achieve good EO results? This finding is both interesting and suspicious to me. Given that \"It is only possible to achieve both DP and EO when the sensitive attributes Z are independent of the labels Y,\" it is not likely that good EO should be achieved when optimizing with the constraint  \u03d5(X) \u22a5 Z. E.g. can you also calculate the empirical distance covariance between Y and Z for your datasets? I would also suggest the authors to consider adding more baselines and experiment on more datasets to further confirm this finding (as discussed in my weaknesses).\n\n2. In Table 2, why are the results for EO and DP separated (and Acc for \u03b1=2 are different for Col 2 and Col 8)? Table 1 shows those results together."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Reviewer_zY1Z"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4736/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697484777097,
            "cdate": 1697484777097,
            "tmdate": 1699636455633,
            "mdate": 1699636455633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "q8uB67e3mL",
            "forum": "1JuMFjSkpD",
            "replyto": "1JuMFjSkpD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4736/Reviewer_uH68"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4736/Reviewer_uH68"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the (empirical) distance covariance is introduced as a fairness constraint term for achieving group fairness in fair classification tasks. During optimization, the Lagrange dual method is used, and automatic hyperparameter selection is implemented. The authors also provide an estimation of the difference between the empirical distance covariance and the population distance covariance. Experiments are conducted on the tabular dataset and the image dataset, respectively, to show the performance of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The introduction of distance covariance as a fairness constraint in the fair classification.\nS2. The paper provides some theoretical support for the computation of empirical distance covariance.\nS3. The authors conducted experiments on both tabular and image datasets, as well as experimental comparisons in scenarios with multiple sensitive attributes and unbalanced distribution of data across different subgroups."
                },
                "weaknesses": {
                    "value": "W1. The paper is relatively weak in describing and analyzing the mathematical properties of distance covariance. Although the paper mentions that distance covariance can measure linear and nonlinear correlations between two random vectors (predicted values and sensitive attributes), are there other metrics that can also capture nonlinear relationships between variables? Also, does distance covariance have any advantages over other measures? This is the starting point of why distance covariance is used as a constraint term, which is not elaborated in the article.\n\nW2. The explanation of the connection between DP and EO in Section 3.3 is not very clear. For example, \"the equation (7) suggests that the objective goes beyond achieving independence between the feature representation \u03c6_\u03b8(X) and the sensitive attribute Z.\" Is \u03c6_\u03b8(X) a feature representation or a prediction?\n\nW3. In Experiment 4.1, the trend between accuracy and \u2206EO demonstrated by the proposed method is quite different from the comparison method. Is there any analysis to explain this?\n\nW4. Does this method work equally well in scenarios where the sensitive attribute is a continuous variable?"
                },
                "questions": {
                    "value": "1) Explain the proposd connection between DP and EO.\n2) Does this method work equally well in scenarios where the sensitive attribute is a continuous variable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Reviewer_uH68"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4736/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697636485998,
            "cdate": 1697636485998,
            "tmdate": 1699636455532,
            "mdate": 1699636455532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ekFuZM5M2l",
            "forum": "1JuMFjSkpD",
            "replyto": "1JuMFjSkpD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4736/Reviewer_eHoC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4736/Reviewer_eHoC"
            ],
            "content": {
                "summary": {
                    "value": "This paper leveraged empirical distance covariance as an approximation of statistic independence between two random variables. They incorporated it as an equality constraint in the classification tasks and solved it via Lagrangian dual approach without the need of manually specifying the weight. The authors conducted numerical experiments using different datasets and machine learning tasks, showing competitive performance over existing methods. The authors consolidated the detailed properties of the selected estimation metric distance covariance and provided a math proof of the convergence of empirical distance covariance to the true distance covariance in terms of sample size."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-organized, well-written, and easy to follow. The numerical experiments demonstrated the superiority of the proposed empirical distance covariance over other approximation metrics in the literature."
                },
                "weaknesses": {
                    "value": "I am giving a weakly reject because (1) the novelty is limited (see the main argument 1 for details). (2) the theoretical contribution is not targeted at the conference audiences. Theorem 4 delivers limited insight into the convergence speed of the proposed approach to the optimizer in terms of sample size (see the main argument 2). (3) The numerical experiment results demonstrated the superior performance of the new metric that is able to capture independence in a more accurate way. However, the computation cost was not compared."
                },
                "questions": {
                    "value": "**Main arguments**\n\n1. The novelty of the proposed fair classification method came from two parts: (1) introduce an alternative independence approximation named distance covariance. For me, the first novelty is limited as this is an extension of previous work, e.g., mutual information (Kamishima et al., 2012), covariance (Zafar et al., 2017), or HGR coefficient ****(Mary et al., 2019) were token as the approximation metric. These works all added the empirical approximation to the original loss function as the regularization term. As the authors mentioned in the paper, the covariance only captures linear dependency between sensitive attributes and the predictions. MI and HGR are also nonlinear independence approximation metrics. The superiority of selecting distance covariance over other nonlinear metrics are not clear in terms of computational efforts and math property. In particular, Mutual Information is nonnegative measure, closely related to KL divergence measure, and can be well approximated by subsamples (Kamishima et al., 2012). (2) leverage Lagrangian primal-dual alternative optimization to automatically select the weight coefficient. This part of contribution is debatable. While the Lagrangian approach is an efficient way of iteratively updating both training parameters and the weight coefficient, it also lost the advantage of controlling the trade-off if the decision-maker does have the domain knowledge.\n2. The main theoretical contributions are the analysis of the properties of distance covariance and the convergence analysis of the empirical distance covariance. While the existence of bi-convexity renders lower effort in minimizing the distance covariance in the fair ML setting, the convergence analysis is not associated with the quality of the fair solution, i.e., how the convergence speed to a (Pareto) minimizer of the penalized training object is impacted by the sample size. \n3. It is not clear in the paper whether the distance covariance has non-negative property. This is related to the sign of Lagrangian multiplier $\\lambda$. It should not have any sign constraint for equality equality-constrained problem given in (2).\n\n**The paper has some imprecise parts, here are a few:**\n\n1. Section 4.2 P8: It is not clear how the trade-off curve is obtained if the Lagrangian multiplier is not under control. Fixing the Lagrangian multiplier or just randomly initialize the starting points? \n2. Figure 1 Right P8: a trade-off curve should only contain non-dominated solutions. For example, the left most blue dot is dominated by the second dot and should not included in the numerical result.\n3. Section 4.2.2: Not sure if I understand correctly, the experiment targeting predicting gender is odd and not aligned with any realistic applications.\n4. It is mentioned in the Related Work section that benchmark methods like HGR, MI, etc. are computationally challenging. So it is natural that readers are expecting a comparison of computation effort. \n\n**Minor Issues, no impact on the evaluation score:**\n\n1. P7 Section 4 the 1st Paragraph: \u201cThe criteria used to assess the performance of fairness are\u2026\u201d reference format Park et al. (2022) needs to be corrected.\n2. Section 4.2 P8: \u201cFurthermore, there may be a mistake in\nthe implementation of the function *dis* within the provided code that computes the corresponding\nprobability density function.\u201d If this is the case, I would remove FairDisCo from the comparison in Section 4.1 as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4736/Reviewer_eHoC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4736/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725736189,
            "cdate": 1698725736189,
            "tmdate": 1699636455452,
            "mdate": 1699636455452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "tbXu2j91N5",
            "forum": "1JuMFjSkpD",
            "replyto": "1JuMFjSkpD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4736/Reviewer_8JtY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4736/Reviewer_8JtY"
            ],
            "content": {
                "summary": {
                    "value": "This paper promotes the independence of the sensitive attribute and the predicted label to enhance fairness in classification by using (sample) distance covariance as a penalty term. The authors not only provide theoretical analysis on the convergence and sample complexity bounds for the estimation of distance covariance and mini-batch computation, but also numerical results on UCI tabular and image datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tDistance covariance seems to be a very interesting metric for the dependency between two random variables and an active research area. So this paper which connect the distance covariance with fairness notion is very timely.\n2.\tThis paper provides theoretical background, consistency and sample complexity bounds for the distance covariance\n3.\tSection 3.3 that connect DP and EO with the nature of dependency and distance covariance is well written.\n4.\tThe numerical results are abundant, including tabular and image data. The experiments on image data illustrate the scalability of the proposed algorithms, unlike common experiments only on small-scale tabular datasets."
                },
                "weaknesses": {
                    "value": "1.\tThe intuitive explanation for the distance covariance and its existing usages in statistics are not well explained.\n2.\tThere are some missing references for fairness interventions, which I encourage the authors to include and compare, for example, \na.\tLowy, A., Baharlouei, S., Pavan, R., Razaviyayn, M. and Beirami, A., 2021. A stochastic optimization framework for fair risk minimization. arXiv preprint arXiv:2102.12586.\nb.\tAlghamdi, W., Hsu, H., Jeong, H., Wang, H., Michalak, P., Asoodeh, S. and Calmon, F., 2022. Beyond Adult and COMPAS: Fair multi-class prediction via information projection. Advances in Neural Information Processing Systems, 35, pp.38747-38760.\n3.\tDespite that the distance covariance is interesting, the reason why it is potentially a better metrics than other information-theoretic quantities such as mutual information and the Renyi maximal correlation is unclear to me. Distance covariance, MI and the maximal correlation are all zero when two random variables are independent; however, the maximal correlation satisfies Renyi\u2019s 6 postulates for a good measure of dependency. It is encouraged that the authors spend more space to discuss the pros and cons regarding the dependency metrics, give illustrations on why one is better than the other and hopefully provide a simple numerical example.\n4.\tIn the experimental results (Table 1 and 2), it seems that the proposed results consistently have higher accuracy and lower fairness violation. However, the proposed result is not too different from other methods as most of them are in the Lagrangian form, i.e., CE loss plus fairness/ independence constrains. It is encouraged that the authors explain clearly why the proposed method could lead to a consistently better acc-fariness trade-off point than other methods."
                },
                "questions": {
                    "value": "Please refer to Weakness. I will consider raising the scores after the rebuttal period if the authors could address the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4736/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791486930,
            "cdate": 1698791486930,
            "tmdate": 1699636455372,
            "mdate": 1699636455372,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]