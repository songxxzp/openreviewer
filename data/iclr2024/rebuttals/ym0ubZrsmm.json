[
    {
        "title": "Image Background Serves as Good Proxy for Out-of-distribution Data"
    },
    {
        "review": {
            "id": "eQWSNjcYUE",
            "forum": "ym0ubZrsmm",
            "replyto": "ym0ubZrsmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_EtXA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_EtXA"
            ],
            "content": {
                "summary": {
                    "value": "The text addresses challenges in Out-of-distribution (OOD) detection, emphasizing the need for a unified perspective and the generation of compact boundaries between in-distribution (ID) and OOD data without explicit OOD samples. A general probabilistic framework is proposed to interpret various existing OOD detection methods, providing insights for future research. Concurrently, a Self-Supervised Sampling for OOD Detection (SSOD) model is introduced, which leverages natural OOD signals from ID data through convolution\u2019s local property, allowing for the joint optimization of OOD detection and ID classification in an end-to-end manner. Extensive experiments demonstrate that SSOD achieves state-of-the-art performance on large-scale benchmarks, significantly outperforming previous methods, and showing remarkable results on both standard and challenging OOD datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to understand.\n2. The author conducts extensive experiments for evaluation.\n3. The visualization is helpful to better understand SSOD."
                },
                "weaknesses": {
                    "value": "1. SSD [1] also uses a self-supervised algorithm for OOD detection; however, the article does not make any comparisons with SSD.\n2. As shown in Tables 2, 4, and 5, SSOD results in a decrease in the model's accuracy for ID classification, which is not permissible for OOD detection. OOD detection requires the model to identify OOD data without affecting the accuracy of ID classification.\n3. The results reported in Table 1 show that SSOD does not achieve state-of-the-art (SOTA) performance on all OOD datasets, and there are many works with better performance not compared in Table 1: React [2], Dice [3], Ash [4].\n4. The motivation is not convincing enough: I do not agree with the author's critique of the two-stage manner approach. Post-processing algorithms are actually more suitable for adapting to various pre-trained models, regardless of whether the model is trained with supervised learning or self-supervised learning.\n5. Table three indicates that SSOD performs poorly on hard OOD detection tasks, does this highlight a flaw in the algorithm: its inability to differentiate between ID and OOD with similar backgrounds? Take the following more realistic example of OOD input: if a network trained to distinguish between different types of apples (e.g., fuji, red, honey-crisp, etc.) is presented with a different fruit as OOD data (e.g., peach, plum, tomato, etc.), the background features could be similar for both ID and OOD data. Alternatively, consider a scenario like scene classification (indoor scene vs beach scene vs forest scene, just as a hypothetical example). In this case, there is no background as such, because the entire scene constitutes the 'foreground.' I am very curious about how SSOD would perform in such scenarios.\n6. I recommend the author conduct experiments about ViT architectures.\n\n[1]  Sehwag, Vikash, Mung Chiang, and Prateek Mittal. \"Ssd: A unified framework for self-supervised outlier detection.\"\u00a0ICLR 2021.\n\n[2] Sun, Yiyou, Chuan Guo, and Yixuan Li. \"React: Out-of-distribution detection with rectified activations.\"\u00a0Advances in Neural Information Processing Systems\u00a034 (2021): 144-157.\n\n[3] Sun, Yiyou, and Yixuan Li. \"Dice: Leveraging sparsification for out-of-distribution detection.\"\u00a0European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[4] Djurisic, Andrija, et al. \"Extremely Simple Activation Shaping for Out-of-Distribution Detection.\"\u00a0The Eleventh International Conference on Learning Representations. 2022."
                },
                "questions": {
                    "value": "see Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698314422590,
            "cdate": 1698314422590,
            "tmdate": 1699636838258,
            "mdate": 1699636838258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iViXdVfXwF",
                "forum": "ym0ubZrsmm",
                "replyto": "eQWSNjcYUE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "Thanks for your valuable suggestions. We detail our response below.\n- **Response to weakness 1**: We notice that the original manuscript of SSD only provides AUROC while ignoring the FPR95, which is more important in OOD detection. Nevertheless, we provide the comparison (AUROC) below. The CIFAR-10 is employed as the in-distribution data.\n| OOD data | SVHN | LSUN | iSUN | Texture | Places | Average |\n| --- | --- | --- | --- | --- | --- | --- |\n| SSD | 99.60 | 96.50 | None | 97.60 | 95.20 | 97.225 |\n| Ours (SSOD) | 99.44 | 99.11 | 98.16 | 99.59 | 98.42 | **98.944** |\n- **Response to weakness 2**: SSOD only introduces marginal influence to the ID ACC. As a researcher and a practical developer of online applications, we focus on the overall performance of the method instead of entangling in these imperceptible changes. We provide the influence that SSOD introduced on ID ACC below.\n| Method | ResNet-50 | SSOD (ResNet-50) | ResNet-101 | SSOD (ResNet-101) | MobileNet V3L | SSOD (MobileNet V3L) | ResNet-18 | SSOD (ResNet-18) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Dataset | ImageNet-1K | ImageNet-1K | ImageNet-1K | ImageNet-1K | ImageNet-1K | ImageNet-1K | CIFAR-10 | CIFAR-10 |\n| ID ACC | 76.13 | 76.09 (-0.04%) | 77.37 | 76.92 (-0.45%) | 74.04 | 73.94 (-0.1%) | 94.21 | 94.17 (-0.04%) |\n- **Response to weakness 3**: We evaluate our proposed SSOD on several benchmarks as shown in Table 1 and Table 2. The averaged results suggest that SSOD surpasses other methods by a large margin, i.e., **-6.28%** FPR95 on ImageNet and **-19.01%** FPR95 on CIFAR-10. Moreover, as a researcher in this field and to the best of our knowledge, our team doesn't believe there exists a method that performs absolutely better than all other existing methods on all benchmarks. Besides, we compare over 13 (MSP, MaDist, ODIN, GODIN, KLM, Energy, KNN, MOS, Fort, MCM, CSI, ReAct, ViM, and their variants) methods in our experiments, which is quite sufficient and enough to represent the current research situation. ReAct is shown in `Table 3`. SSOD achieves comparable performance on hard OOD detection as ReAct. More comparisons are demonstrated below. The ImageNet-1K is set as ID data and we compare the FPR95 of ReAct, Dice, and Ash. We will also attach this part to our revised manuscript. Thanks for your suggestion in filling the gap appearing in our experiments. Honestly, based on the results demonstrated in the original manuscript of Ash, it performs better than most current methods, and our SSOD also achieves top-ranked performance among them. **The following results are FPR95, lower is better.**\n| Dataset (ID=ImageNet-1K) | iNaturalist | SUN | Places | Texture | Average |\n| --- | --- | --- | --- | --- | --- |\n| ReAct | 20.38 | 24.20 | 33.85 | 47.30 | 31.43 |\n| Dice | 25.63 | 35.15 | 46.49 | 31.72 | 34.75 |\n| Ash | 14.21 | 22.08 | 33.45 | 21.17 | 22.73 |\n| Ours (SSOD) | 14.80 | 28.52 | 38.92 | 45.32 | 31.89 |\n \n   | Dataset (ID=CIFAR-10) | SVHN | LSUN | iSUN | Texture | Places | Average |\n   | --- | --- | --- | --- | --- | --- | --- |\n   | ReAct | 41.64 | 11.46 | 12.72 | 43.58 | 43.31 | 30.54 |\n   | Dice | 25.99 | 3.91 | 4.36 | 41.90 | 48.59 | 24.95 |\n   | Ours (SSOD) | 2.12 | 4.42 | 10.06 | 1.91 | 7.44 | 5.19 |\n- **Response to weakness 4**: I think that there exist some misunderstandings. We agree with you that both post-processing and pre-processing methods are great for tackling OOD detection, such as ODIN, GODIN, ReAct, and KNN. However, when concerning practical applications, we find two obstacles that these methods encounter. Firstly, we have no suitable pre-trained weights for some specific tasks such as face recognition. This makes it costly to perform these methods. Besides, the deployment is usually complex as we have to compute and store many intermediate results or features. Secondly, our proposed SSOD can better utilize the image data from the target scenario since it's end-to-end trainable, which is more suitable in some situations than the two-stage manner approach. Again, we are here to emphasize that the two-stage manner methods are quite great for promoting the development of OOD detection. **The motivation of SSOD is to inject more insights into this research direction and provide a more suitable technical proposal in some specific scenarios.** For example, in surveillance video processing, if we have some data from the target scenarios, our SSOD can learn their backgrounds as OOD characters, and therefore, reduce false alarms when nothing appears on the video screen."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699959122289,
                "cdate": 1699959122289,
                "tmdate": 1699959122289,
                "mdate": 1699959122289,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qLOIuXY0jw",
            "forum": "ym0ubZrsmm",
            "replyto": "ym0ubZrsmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_u8Y6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_u8Y6"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses challenges in Out-of-distribution (OOD) detection by:\n1. Proposing a unified probabilistic framework to understand existing methods.\n2. Introducing a new model, SSOD, that uses natural OOD proxy from in-distribution data without needing explicit OOD samples.\n3. Demonstrating that SSOD significantly outperforms previous methods on major benchmarks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is articulately composed and organized, facilitating a clear understanding of most sections.\n2. To my understanding, the method introduced is innovative.\n3. The study is underpinned by a comprehensive set of experiments."
                },
                "weaknesses": {
                    "value": "Major Points:\n\n1. **Concerns about the First Contribution:**\n\na. The derivation in Eqs 1-6 appears not helpful to the proposed method and Eq. 7 could be introduced more directly with \n\n$$P(w_i|x)=P(w_i,x\\in S_{ID}|x) = P(w_i|x\\in S_{ID},x) \\cdot P(x\\in S_{ID}|x), i=1,\\dots,M$$\n\nb. The analysis of prior methods seems not necessarily dependent on a probabilistic perspective, since $P(x\\in S_{ID}|x)$ is essentially a rephrasing of existing methods (in A.1).\n\nThus, the emphasis on the probabilistic viewpoint might not be as novel as suggested.\n\n2. **Impact of Confidence Threshold:** How does the confidence threshold, $\\gamma$, in Eq. 9 influence the performance? Could the authors elaborate on how they determined its value during experiments?\n\n3. **Bias in the Proposed Method:** The paper indicates that existing post-hoc methods are influenced by biases from pretrained models. Yet, as Table 3 reveals, the proposed technique doesn't perform optimally on ImageNet-O, which contains adversarial images for ImageNet. Given that the method's training relies on both the original dataset and model's intermediate predictions, is it possible the method still suffers from the bias?\n\n4. **Effect of OOD Training Target on ID ACC:** In the experiments, what is the ID ACC performance when $\\alpha=0$? Essentially, does including the OOD training target lead to a noticeable decrease in ID ACC?\n\n5. **Fairness of Comparison in Experiments:** In the experimentation section, multiple methods such as MSP, ODIN, ReAct, and many others are post-hoc techniques that work off a fixed pre-trained model. These methods don't adjust the training process, distinguishing them from the proposed approach. As a result, juxtaposing these techniques might not offer a balanced comparison. The majority of experiments may not necessarily highlight the superiority of the proposed method. It would be beneficial for the authors to contrast their strategy with other training-based techniques.\n\nMinor Points:\n1. **Table Formatting Issues:** In Tables 4 & 5, the shaded regions seem to obscure the lines, affecting clarity.\n\n2. **Discussion Location:** In the introduction, the authors state that \"various OOD methods can be analyzed, with main differences and key limitations clearly identified\". However, this discussion is relegated to the appendix. It would be more helpful if the main content were self-contained and inclusive of this analysis."
                },
                "questions": {
                    "value": "Please see the content in Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Reviewer_u8Y6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698460917899,
            "cdate": 1698460917899,
            "tmdate": 1699636838108,
            "mdate": 1699636838108,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uinT353Vci",
                "forum": "ym0ubZrsmm",
                "replyto": "qLOIuXY0jw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "Thanks for your valuable reviews. We detail our response below.\n- **Response to weakness 1 (a)**: The derivation from `Eq 1` to `Eq 6` is based on Bayes's theorem. Concretely, based on the total probability formula, we have $P(w_i|x)=P(w_i|x\\in S_{ID}, x)P(x\\in S_{ID}|x)+P(w_i|x\\in S_{OOD}, x)P(x\\in S_{OOD}|x)$. As illustrated in `Section 3.1`, $w_i$ indicates the ID categories, and $S_{ID}$ has no overlap with $S_{OOD}$. Therefore, the term $P(w_i|x\\in S_{OOD}, x)$ equals zero. This makes `Eq 7` hold conditionally.\n- **Response to weakness 1 (b)**: The probabilistic framework is vital for robust classification. As we can see from `Eq 7`, robust classification consists of conventional multi-category classification (ID factor) and OOD detection (OOD factor). **This perspective provides a unified view of open world recognition.** OOD detection is an important part of robust classification, and we show their connection clearly.\n- **Response to weakness 2**: The ablations on $\\gamma$ are provided in our response to **Reviewer #ajFn: Response to question 2**.\n- **Response to weakness 3**: Honestly, SSOD is quite effective but not perfect. The methodology of SSOD is to obtain OOD supervision from the CLS head. Therefore, if the CLS head fails to detect the target object of the ID dataset, its generated ID/OOD supervision is likely to be invalid. This tells us that SSOD suffers from the bias when the CLS head fails to identify the input images, i.e., fails to perform classification. We argue that it's hard to design a perfect method that can handle all scenarios. We present SSOD for injecting new thoughts into the sub-field of OOD syntheses.\n- **Response to weakness 4**: If $\\alpha$ is set to zero, SSOD degenerates into conventional classification, and the ID ACC equals that of the pre-trained models on ImageNet-1K. Marginal influence is introduced by SSOD on ID ACC, and we demonstrate the detailed results below. All pre-trained weights are released by Pytorch officially.\n| Method | ResNet-50 | SSOD (ResNet-50) | ResNet-101 | SSOD (ResNet-101) | MobileNet V3L | SSOD (MobileNet V3L) | ResNet-18 | SSOD (ResNet-18) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Dataset | ImageNet-1K | ImageNet-1K | ImageNet-1K | ImageNet-1K | ImageNet-1K | ImageNet-1K | CIFAR-10 | CIFAR-10 |\n| ID ACC | 76.13 | 76.09 (-0.04%) | 77.37 | 76.92 (-0.45%) | 74.04 | 73.94 (-0.1%) | 94.21 | 94.17 (-0.04%) |\n- **Response to weakness 5**: In fact, concern about fairness is difficult to mitigate. In the comparison part, we collect some mainstream methods such as MSP, ODIN, GODIN, and Energy, and the latest methods such as MOS, Fort, KNN, and MCM. These methods are developed with some individual designs and architectures, and therefore, they are trained, finetuned, or post-processed in different ways. **We strictly keep the experimental settings identical for all comparable methods.** Besides, the fairness is quite difficult to evaluate. For example, ODIN resorts to pre-processing and post-processing without training the models, while SSOD resorts to finetuning without pre-processing or post-processing. We can not force all methods to follow the same manner to solve OOD detection. Nevertheless, we keep the experimental settings and evaluation protocol identical for all comparable methods.\n- **Response to minor points 1**: This bug appears if the PDF is opened using Adobe and disappears if using Google Chrome. We will fix this point.\n- **Response to minor points 2**: Yes, we agree with you and we are organizing the manuscript carefully. The content of `Appendix A.1` is vital since it interprets the existing OOD methods from our probabilistic view. We will move this part to the main body."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699876701343,
                "cdate": 1699876701343,
                "tmdate": 1699876701343,
                "mdate": 1699876701343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o6H8WYDUki",
                "forum": "ym0ubZrsmm",
                "replyto": "uinT353Vci",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_u8Y6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_u8Y6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your detailed response. I appreciate the time and effort you've put into addressing many of my concerns. I have a couple of points that I believe could benefit from further clarification to enhance my understanding:\n\n**Regarding Weakness 1:**\n\na. I'd like to discuss the relationship between Eq. 7 and Eqs. 1-6. While it appears that Eq. 7 can be derived directly from Bayes's theorem, I'm curious about the specific necessity of Eqs. 1-6 in this context. \n\nb. I appreciate the efforts to provide a comprehensive perspective. However, I feel that Eq. 7, while insightful, essentially involves first determining if a sample is ID and then assessing the probability of it being an ID class. Eq. 7 does not necessarily deepen our understanding beyond the fundamental concept of OOD detection task. The proposed method actually does not reply on the unified view. The revisiting part in Appendix (A.1) uses the unified view, but the analyses can be done without the view. Could you shed more light on the pivotal role and benefits of adopting this unified view in your methodology?\n\n**Regarding Weakness 5:**\n\nAlthough the proposed method outperforms previous works, most of methods involved in the experiments are not directly comparable to the proposed method. Take Table 1 as an example. \n\n1. MSP, MaDist, ODIN, Energy, KLM, KNN (w/o), and MCM are post-hoc methods assuming the access to a fixed pre-trained model. The proposed method needs to modify the training process of the model, so it is not surprising that it shows better performance than post-hoc methods. \n\n2. MSP (CLIP-B/L), Fort (ViT-B/L), MCM (CLIP-B/L) are using different model architectures than others (using ResNet-50). This introduces more confounding factors.\n\nIf you would like to refer to other benchmarks like OpenOOD [1,2], I suggest you could more focus on the comparison between the proposed method and other training methods on the same model architecture, such as SSD (also mentioned by another reviewer), CIDER, and LogitNorm. But currently, these methods are not included in the experiments.\n\n\n[1] Yang, Jingkang, et al. \"Openood: Benchmarking generalized out-of-distribution detection.\" Advances in Neural Information Processing Systems 35 (2022): 32598-32611.\n[2] Zhang, Jingyang, et al. \"OpenOOD v1. 5: Enhanced Benchmark for Out-of-Distribution Detection.\" arXiv preprint arXiv:2306.09301 (2023)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700008689896,
                "cdate": 1700008689896,
                "tmdate": 1700008689896,
                "mdate": 1700008689896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fk1vuJCZCj",
                "forum": "ym0ubZrsmm",
                "replyto": "qLOIuXY0jw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_u8Y6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_u8Y6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I appreciate the time and effort you have put into addressing my concerns.\n\nHowever, the significance of the probabilistic view is still unclear. According to your reply, I feel you are actually addressing Open-Set Recognition (OSR) [1], a task highly related to OOD detection. Notably, on page 3 of this work [1], there is a definition of open-set recognition that includes a two-stage prediction process for each sample: (i) assessing whether the test sample belongs to any known classes, and (ii) determining the distribution over the known classes. There are also metrics for OSR that may address the bias you mention in evaluating ID ACC. The existence of previous works does diminish the novelty of the proposed probabilistic view.\n\nBesides, regarding the benchmarks, it is beneficial to focus more on comparing the proposed method with other training methods within the same model architecture. The current experimental setup cannot prove the advantage of the proposed method, as I discussed in the previous comments.\n\n[1] \"Open-Set Recognition: A Good Closed-Set Classifier is All You Need.\" ICLR 2022 Oral. https://openreview.net/forum?id=5hLP5JY9S2d"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532118530,
                "cdate": 1700532118530,
                "tmdate": 1700532202394,
                "mdate": 1700532202394,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x0mcJpZcV5",
            "forum": "ym0ubZrsmm",
            "replyto": "ym0ubZrsmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_f34F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_f34F"
            ],
            "content": {
                "summary": {
                    "value": "The article focuses on Out-of-distribution (OOD) detection and proposes a model called Self-supervised Sampling for OOD Detection (SSOD) that does not require explicit OOD data annotation, which is able to extract natural OOD signals from the background of ID images, end-to-end end-to-end training of OOD detection branches. Experiments are conducted on several large-scale OOD detection datasets to demonstrate the effectiveness and superiority of SSOD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe article proposes a general and reasonable probabilistic framework to understand the OOD detection problem, which can cover a wide range of existing OOD detection methods in an innovative way.\n2.\tThe article proposes an effective self-supervised sampling mechanism capable of extracting useful OOD signals from ID images, avoiding the difficulty and cost of collecting and labeling large amounts of OOD data.\n3.\tThe article provides a detailed description and derivation of the working principle and design of SSOD."
                },
                "weaknesses": {
                    "value": "1.\tThe article uses some uncritical and unreasonable assumptions in the derivation of the probabilistic framework, such as setting T as sM+1, ignoring the possible differences between sM+1 and T; treating P(wi|x) as P(wi|x\u2208SID,x), ignoring the possibility that x may belong to the OOD data.\n2.\tThe article uses a fixed and subjective threshold \u03b3 in the self-supervised sampling mechanism to determine whether an image block belongs to ID or OOD, which does not consider the possible differences and variations between different datasets, models, and categories.\n3.\tThere are some spelling mistakes, grammatical errors, and punctuation errors in the article; some irregular or inappropriate terms are used in the article, such as OOD-data-free model, OOD patch sampler, and so on."
                },
                "questions": {
                    "value": "1.\tThe derivation of formula (1) seems to lack a detailed explanation. Can a more complete mathematical derivation of this formula be provided?\n2.\tThere are some grammatical and spelling errors in the text, please fix them\n3.\tThere seems to be a subjective bias in the interpretation of the experimental results. Can more objective evidence be provided to support these interpretations?\n4.\tIn the first paragraph on page 1, the author mentions that \"OOD detection empowers the model trained on the closed image set to identify unknown data in the open world\". But there is no definition or difference between what is closed image set and open world. Please define and explain these two concepts clearly in the introduction.\n5.\tOn page 4, paragraph 5, the author mentioned \"Since the ImageNet-O mainly contains adversarial images, leading to the classifier's wrong prediction, SSOD reports higher FPR95 compared to the open world\". SSOD reports higher FPR95 compared to the best previous methods\", but no reason or mechanism is given as to why the adversarial images cause SSOD's performance to degrade."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Reviewer_f34F"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638748179,
            "cdate": 1698638748179,
            "tmdate": 1700614012908,
            "mdate": 1700614012908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9icEYoDcAr",
                "forum": "ym0ubZrsmm",
                "replyto": "x0mcJpZcV5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "Thanks for the valuable reviews from Reviewer **#f34F**. We detail the response as follows.\n- **Response to weakness 1**: I have to argue that using $T$ as $s_{M+1}$ is not an assumption, it is just an equivalent substitution. As depicted in `Eq 1`, both $s_i$ and $T$ are trainable parameters of neural networks, and what we do in `Eq 5` is just use $s_{M+1}$ to indicate $T$. Moreover, we argue that treating $P(w_i|x)$ as $P(w_i|x\\in S_{ID}, x)$ is not an assumption as well. Based on the bayes' theorem, we have:\n\n     $P(w_i|x)=P(w_i|x\\in S_{ID}, x) P( x \\in S_{ID}|x) + P(w_i|x\\in S_{OOD}, x) P(x\\in S_{OOD}|x)$.\n     In Section 3.1, we have highlighted that $w_i$ indicates the ID categories, and $S_{ID}$ has no overlap with $S_{OOD}$. Therefore, the term $P(w_i|x\\in S_{OOD}, x)$ equals zero.\n- **Response to weakness 2**: SSOD performs consistently well across whole benchmarks using $\\gamma$ as 0.95. We manually try several different values of $\\gamma$ on different benchmarks and find this parameter is stable. Due to the limited space, we didn't provide this part in ablation. Please refer to our response to **Reviewer #ajFn**, where we provide detailed ablations on $\\gamma$. The more detailed experimental results on these parameters will be attached in our revised Appendix.\n- **Response to weakness 3**: We will correct these errors in our revised manuscript.\n- **Response to question 1**: In Eq 1, $\\sigma(\\cdot)$ indicates the `Sigmoid` function, and $g_i(x)=-s_i+T$ is a score function. More concretely, $g_i(x)=-s_i(x)+T$, where both $s_i(x)$ and $T$ can be regarded as trainable neural networks. $s_i(x)$ changes with the input $x$. $T$ is the learnable bias term which is a constant. Therefore, we can use $T$ as $s_{M+1}$ since they are both unchanged with the input image $x$. The details about `Sigmoid` function can be found here: https://en.wikipedia.org/wiki/Sigmoid_function.\n- **Response to question 2**: We will correct these errors in our revised manuscript.\n- **Response to question 3**: We compare all methods based on valid experiments, besides, for those results from the original copy, we highlight them as well. For example, in `Section 4.2`, we detail the evaluation protocol and the experimental settings. All conclusions are drawn from the experimental results. **We admire all of these investigated methods which have promoted the development of OOD detection and robust classification**. Our team has no subjective judgment during the evaluation procedure. We will refine the description and look forward to your detailed reply.\n- **Response to question 4**: The closed image set indicates the image dataset with a fixed number of categories. For example, the ImageNet-1K is a closed image dataset with 1000 categories. For those objects not included within this scope, a conventional classification model trained on ImageNet-1K fails to identify them. The open world indicates the real world where any kind of image can appear. These terminologies usually appear in the field of **Open World Recognition**, and we will present a more detailed description. Besides, this survey [1] shows the overall view of OOD detection and robust classification, which may help to mitigate this concern.\n- **Response to question 5**: The failure of SSOD in adversarial images is led by incorrect supervision from the CLS head. Recall that the mechanism of SSOD is joint learning between the CLS head and the OOD head. Adversarial images lead to poor performance of the CLS head, and therefore, the OOD head encounters a performance drop as well. More concretely, in `Eq 9`, the poor CLS head will provide incorrect confidence $\\hat{y}_i^{HW}$ for the OOD head. \n\n[1] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized Out-of-Distribution Detection: A Survey, 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699849934602,
                "cdate": 1699849934602,
                "tmdate": 1699849934602,
                "mdate": 1699849934602,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BuOtPa3FGJ",
            "forum": "ym0ubZrsmm",
            "replyto": "ym0ubZrsmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_TJ6P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_TJ6P"
            ],
            "content": {
                "summary": {
                    "value": "- Authors tackle the problem of Out-Of-Distribution (OOD) detection in this work and show that the image background in In-Distribution (ID) datasets can act as a good proxy for OOD data, preventing the necessity to collect real/synthetic OOD data for training strong OOD detectors. \n- First, authors propose a general probabilistic framework that can explain existing OOD detection methods.\n- Next, using this interpretation, authors propose **S**elf-**S**upervised **O**OD **D**etection (SSOD) to exploit the natural OOD signals present in ID data. This prevents the need for collecting explicit real or synthetic OOD data making the pipeline more efficient.\n- With extensive experiments, authors show impressive results on several OOD benchmarks proving that image backgrounds of ID data can serve as a good proxy for OOD data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The probabilistic interpretation of OOD detection methods is very useful to advance future research. The factorized interpretation also helps tune each independent component accordingly to optimize ID or OOD performance depending on the downstream objective.\n- The paper is well written, and the math is easy to follow once derived on paper. \n- The experimental section supports all the claims made in the paper."
                },
                "weaknesses": {
                    "value": "I will summarize my concerns with this work under three broad sections.\n\n**Nomenclature**\n- Authors chose to proceed with the name SSOD for their work but the whole field of Semi-Supervised Object detection (SSOD) [1] already exists creating a bit of a confusion.\n- I recommend using SSOOD to avoid any confusion with an already established sub-field. \n\n**Presentation of results**\n- Authors claim that their first contribution is to provide a probabilistic interpretation of OOD, using which existing methods can be analyzed, but pushed the analysis section to the supplementary. In my opinion, if authors claim the probabilistic interpretation as an analysis tool, then it shouldn't be delegated to the supplementary.  \n\n**Motivation and intuition**\n- Authors show impressive results on several OOD benchmarks but the motivation that image background can serve as a good proxy for OOD data has some flaws.\n- First, using the penguin image example from Fig. 2, teaches the network to consider the background (in this case \"water\") as a signal for OOD. Now this will be ineffective in datasets constructed from comics or cartoons which is not one of the domains that authors evaluate their method on. This also explains why the scores are lower on OOD datasets constructed from SUN, Places because Imagenet is predominantly biased towards \"organisms\" and \"food\" with lower signals from indoor scenes and places. This raises the question \"Does this method work because of the choice of the OOD datasets used?\" This is partly explained by the results on \"Textures\" (I agree with the authors that textures has some overlap with patterns in the ID classes and the results are low, but I believe that is just part of the story). \n- Second, I think some additional analysis is required on the iNaturalist dataset (or maybe another toy setup on the subset of imagenet) which explains why the method works. If the OOD head is learning to detect any background patch as OOD, then what is the role of this in iNaturalist, where the background is usually water/trees/nature etc? From Imagenet, the network learnt to flag any background containing these regions as OOD, so does it ignore the ID category entirely and just focus on background? But that can't be true because the ID performance is also higher. The interplay between the CLS head and OOD head is very important to completely understand why image background is a good proxy and is missing from the paper. \n\n**I recommend authors to consider answering these questions for me to improve on my rating**\n\n[1] Liu, Yen-Cheng and Ma, Chih-Yao and He, Zijian and Kuo, Chia-Wen and Chen, Kan and Zhang, Peizhao and Wu, Bichen and Kira, Zsolt and Vajda, Peter, Unbiased Teacher for Semi-Supervised Object Detection, ICLR 2021."
                },
                "questions": {
                    "value": "**Questions**\n- In the abstract, authors mention they leverage \"local property of convolution\" for OOD. But it hasn't been mentioned anywhere else. Can you elaborate what they mean by this?\n- In eq. 9 the $y_i^{\\text{OOD}}$ is the label for detecting OOD patches right? In which case, patches with confidence lower than 5% ($1-\\gamma$) should have a label 1 and not 0 right?\n- In the 2nd sentence below Eq. 11, is it \"During inference\" or \"During training/inference\"? The second loss term in Eq. 11 is applied on a spatial map from what I understood, so why do we have to compute $P(x\\in \\mathcal{S}_{\\mathbb{ID}}|x)$ explicitly during training? We just need that during inference correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not foresee any immediate ethical concerns with this work."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680503500,
            "cdate": 1698680503500,
            "tmdate": 1699636837736,
            "mdate": 1699636837736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Of9tFO6kG5",
                "forum": "ym0ubZrsmm",
                "replyto": "BuOtPa3FGJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "The suggestions from Reviewer **#TJ6P** are quite valuable and well-judged. We provide our response in detail as follows.\n- **Response to nomenclature**: Thanks a lot for your suggestion and the better name. Honestly, we omit this point during the writing of our manuscript. We chose SSOD as the name of our method only because of its suitable meaning, i.e., self-supervised sampling for OOD detection. We want to emphasize the self-supervised manner and the OOD proxy sampling. Your suggested SSOOD is clearer and more suitable since it indicates the sub-field of OOD detection. We will modify the method name in our revised manuscript.\n- **Response to the presentation of results**: The probabilistic framework of OOD detection is one of our main contributions. We detail this part in `Section 3.1`, however, the interpretation of existing OOD methods from this view is attached in `Appendix A.1`. This is quite unsuitable. We are organizing our manuscript currently, and we plan to move a part of the related work and experimental setup to the Appendix and transfer the interpretation part from the Appendix to the main body. We have to admit that the organization of this manuscript is a little hurried. Thanks again for your suggestions for improving our manuscript.\n- **Response to motivation and intuition 1**: SSOOD explores the image background as the OOD proxies, providing OOD supervision during the training of the binary classification head. Sailing from this fundamental premise, SSOOD encounters failure most in the following two cases. Firstly, the target object of the input image carries similar characters as the background of ImageNet. Secondly, the learned backgrounds in ImageNet can not cover the background of input images, for example, different domains or styles. Obviously, you fully have the core of SSOOD, including its superiority and limitations. An effective way to close this gap is to train the SSOOD with sufficient and diverse images. In our in-house application, we pre-train the SSOOD on ImageNet-1K and continue to train it on several combined datasets collected from our practical scenarios. The data from the target scenario significantly guarantees the performance of outlier detection.\n- **Response to motivation and intuition 2**: This is quite an amazing problem. I am trying to figure out your concern from my limited understanding. Firstly, the training of robust classification consists of OOD detection (binary classification) and conventional multi-category classification as shown in `Eq 11`, and therefore, the classification accuracy gets marginal influence. Besides, within the loss of the OOD head, we collect both ID patches (`objects in ImageNet, labeled as 1`) and OOD patches (`backgrounds in ImageNet, labeled as 0`) to supervise the training, thus the model will not ignore the ID category. In ImageNet vs. iNaturalist, the backgrounds of ImageNet usually appear like some lakes, trees, or nature, and the OOD head treats this information as OOD characters. During the inference, the trained OOD head will assign lower confidence (`note: lower confidence from the OOD head indicates a higher probability that the input is OOD data`) to iNaturalist since it is similar to the background of ImageNet. I doubt that I only get a part of your problem, and I am looking forward to your reply and continuing this discussion. Again, quite admire your insights.\n- **Response to question 1**: The local property of convolution indicates the limited perceptive field of conventional neural networks. For example, using the ResNet-50 to extract the feature of an input image in the shape of 224x224x3, the yielded feature is in the shape of 2048x7x7, i.e., 7x7 patch features whose dimension is 2048. The local property indicates that the top-left (the first) feature patch only consists of local information from the top-left part of the image instead of global information. And that is the critical factor that SSOOD works. Still, we are extending SSOOD to self-attention-based models, and the key problem is how to extract pure background information since the self-attention mechanism fuses all image patches such as ViT and SwinTransformer.\n- **Response to question 2**: We design the OOD head as a binary classification head. Higher confidence outputted by the OOD head indicates a higher probability the input is ID data, i.e., **the ID data is labeled as 1 while the OOD data is labeled as 0 by the OOD head.** We term this head as OOD head may incur some misunderstanding. **Shortly, the output confidence of the OOD head reflects the probability that the input belongs to ID.** For example, if the output confidence is 0, then the input is OOD data; and if the output confidence is 1, then the input is quite likely ID data.\n- **Response to question 3**: Right. This term (OOD factor) is only needed during the inference phase."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699770755386,
                "cdate": 1699770755386,
                "tmdate": 1699770755386,
                "mdate": 1699770755386,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oXWFq2fMl0",
                "forum": "ym0ubZrsmm",
                "replyto": "Of9tFO6kG5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_TJ6P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_TJ6P"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author's comments"
                    },
                    "comment": {
                        "value": "I thank the author for their detailed response to all my concerns and overall I'm happy with the response and am willing to increase my score. Please see a few more comments below.\n**Clarification to Response to motivation and intuition 2**: The background information in iNaturalist and most animal classes in imagenet is very similar (trees, lakes, nature etc.). So if iNaturalist dataset is used as the OOD dataset, even if there are some classes overlapping between the ID dataset and OOD dataset, the model will learn to flag any image with nature in the background as OOD. Maybe this can be better seen by creating a subset within imagenet. Ideally we want the model to look at the foreground object within an image to decide if its ID or OOD. But in this case, SSOOD is heavily relying on the background. Hope that makes sense.\n\n**Response to question 1**: For ViTs maybe a masked attention can potentially be helpful as it avoids a patch to attend to everything else.\n\n**Response to question 2**: If its the OOD head we are talking about, then doesn't it make sense to assign label 1 to OOD and 0 to ID?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700063019715,
                "cdate": 1700063019715,
                "tmdate": 1700063019715,
                "mdate": 1700063019715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lcg72aTiTx",
                "forum": "ym0ubZrsmm",
                "replyto": "jkU357JG9G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_TJ6P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_TJ6P"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author's comments"
                    },
                    "comment": {
                        "value": "I thank the reviewer for their effort. I agree that more work is needed to understand why solely background helps detecting OOD samples and not the foreground object in the OOD images. Since this is extremely important to appreciate the contributions, I would like to keep my original rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695760963,
                "cdate": 1700695760963,
                "tmdate": 1700695760963,
                "mdate": 1700695760963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nQyUeatA5D",
            "forum": "ym0ubZrsmm",
            "replyto": "ym0ubZrsmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_ajFn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_ajFn"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on Out-of-Distribution (OOD) detection and introduces a unified probabilistic framework that divides the OOD detection problem into In-Distribution (ID) and OOD components. This framework presents an insightful overview of existing OOD detection methodologies and pinpoints their limitations (classifiers and features are often biased towards ID data). To address this challenge, the authors introduce Self-Supervised Sampling for OOD Detection (SSOD), utilizing image backgrounds as effective proxies for OOD data. The model employs separate ID and OOD heads, with the OOD head being self-trained through the utilization of confidence scores derived from the classification head. The results demonstrate that the proposed method significantly outperforms existing approaches by a substantial margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The motivation derived from the proposed general OOD detection framework seems both intuitive and solid. \n2. The paper is well-written and figures / tables are easy to follow.\n3. The proposed SSOD approach, while seemingly simple, demonstrates strong effectiveness.\n4. The experimental results are quite strong across various datasets."
                },
                "weaknesses": {
                    "value": "1. Given that SSOD necessitates pseudo-labels for each patch (like semantic segmentation), \nSince SSOD requires the pseudo-labels for each patch (like semantic segmentation), I presume that the training expenses could surpass those of conventional pre-training methods. Could the authors provide a computational comparison of SSOD with other baseline OOD detection models, as well as standard classification models (e.g., ResNet-18, ResNet-50, etc.)?\n\n2. It appears that the principal interpretation derived from the probabilistic framework (Appendix A.1) might be more aptly positioned between Sections 3.1 and 3.2. Currently, the main motivation behind SSOD doesn't seem to be adequately emphasized."
                },
                "questions": {
                    "value": "1. Could you specify the number of patches used in the SSOD model? More specifically, what are the dimensions (height H and width W) in the last feature map?\n\n2. Does the performance of SSOD show sensitivity to the classification confidence parameter gamma?\n\n3. While SSOD generally surpasses other baselines in performance, as seen in the tables, there are instances where other baselines demonstrate notably strong results (e.g., MOS in iNaturalist and KNN in Texture). What could be the main reason for these exceptional cases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7098/Reviewer_ajFn"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844435907,
            "cdate": 1698844435907,
            "tmdate": 1699636837607,
            "mdate": 1699636837607,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aEkwUgM3hN",
                "forum": "ym0ubZrsmm",
                "replyto": "nQyUeatA5D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "Thanks for the valuable suggestions from Reviewer **#ajFn**. We demonstrate the response below.\n- **Response to weakness 1**: We provide the parameters and MACs (Multiplication and Accumulation) of different models as follows. The evaluation code is from: https://github.com/Lyken17/pytorch-OpCounter, and all input images are in the shape of 224x224x3, and the number of classes is set to 1000. We will update the results of other methods ASAP.\n|  Model Name  | ResNet-18 | SSOD (ResNet-18) | ResNet-50 | SSOD (ResNet-50) |\n|  ----  | ----  |  ----  | ----  |  ----  |\n|  Params (M) | 11.69 | 12.40 | 25.56 | 28.19 |\n| MACs (G) | 1.82 | 1.93 | 4.14 | 4.57 |\n- **Response to weakness 2**: We will move a part of the content from Appendix A.1 to the main body of the manuscript, better illustrating our motivation.\n- **Response to question 1**: The dimensions of the feature map depend on the employed backbone and the input images. For example, using a 224x224x3 image and ResNet-50 backbone, the yielded feature map is in the shape of 2048x7x7, i.e., the feature channel is 2048 and the number of patches is 49. The number of employed patches is adaptive as demonstrated in Figure 2 and Eq 9. For instance, in Figure 2 (d), the patches in red/green are used as OOD/ID. The number of employed patches is about 45% of the overall image.\n- **Response to question 2**: SSOD is stable. In our experiments, we set the $\\gamma$ from 0.8 to 0.99 with a step of 0.05. Using the ImageNet vs. iNaturalist as the benchmark, and the ResNet-50 as the backbone. The detailed results are demonstrated below. Since the threshold may be different with the changing backbones, we don't detail this part for its marginal significance.\n| $\\gamma$ | 0.8 | 0.85 | 0.9 | 0.95 | 0.99 |\n| --- | --- | --- | --- | --- | --- |\n| FPR95 | 18.64 | 17.52 | 15.88 | 14.80 | 15.62 |\n- **Response to question 3**: The performance of different methods depends on both the distribution of the dataset and the parameter space of the model itself. To provide a convincing result, we test comparable methods on several benchmarks and compute the averaged performance. For example, in Table 1, the overall FPR95 and AUROC of SSOD are 1.89% and 91.26%, surpassing the following method with -6.28% FPR95 and +0.77% AUROC. SSOD reports competitive performance (however, poorer than KNN) on Texture, that is because Texture overlaps with ImageNet, we have demonstrated this part in Appendix A.5 (Failure Case Analyses). We suggest that KNN is a clustering-based method. Maybe in the Texture dataset, its yielded manifold and classification boundary are more compact than our discriminative SSOD, therefore, achieving the best FPR95 compared to current approaches."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699766104214,
                "cdate": 1699766104214,
                "tmdate": 1699766136128,
                "mdate": 1699766136128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2zQPqBobTz",
                "forum": "ym0ubZrsmm",
                "replyto": "aEkwUgM3hN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_ajFn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Reviewer_ajFn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response from the authors.\n\nConsidering the overall quality of the paper, I hold my original score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630664334,
                "cdate": 1700630664334,
                "tmdate": 1700630664334,
                "mdate": 1700630664334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w7p7jKJVma",
            "forum": "ym0ubZrsmm",
            "replyto": "ym0ubZrsmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_f65W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7098/Reviewer_f65W"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the task of out-of-distribution detection in image classification tasks. The authors identify two main challenges the existing methods are facing, namely the absence of a unified perspective to interpret existing techniques and the need for natural OOD supervision to enhance model robustness without the necessity of explicit OOD data collection. Towards this end, the authors introduce a novel probabilistic framework that provides a unified interpretation of many existing OOD detection methods and the SSOD model that efficiently leverages the CNNs properties (retain spatial information) to create a distinction between ID and OOD samples. The paper's contributions are substantiated through extensive experiments (both is terms of benchmarks, as well as proper evaluation w.r.t state-of-the-art methods). SSOD shows consistent improvements in OOD detection metrics over previous methods, highlighting the model's effectiveness in recognizing and handling OOD samples without the need for additional OOD data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality >> SSOD uses image backgrounds as natural proxies for OOD samples, which is a novel perspective in the field.\n\nQuality >> The best paper I read in a while, both in terms of problem statement/formulation and execution (exemplar experimental analysis).\n\nClarity >> The paper is well-articulated - clearly presents the research problem, the proposed solution, and the insights from the conducted experiments. Also, the authors seem to have made an effort to ensure that the concepts are accessible to anyone, regardless of their expertise in OOD detection.\n\nSignificance >> By proposing a general probabilistic framework, the paper unifies various existing approaches under a single interpretative lens, on top of SSOD that offers an evident advancement for OOD detection (relevant problem)."
                },
                "weaknesses": {
                    "value": "No fundamental flaws with the current submission, but just a suggestion to the authors - to include a more thorough discussion on the limitations of SSOD - what are the potential biases, the impact of background complexity on the model's performance, and scenarios where the model may not perform as expected - to start a discussion towards further improvements. Maybe within a dedicated section, tackling these aspects would be valuable."
                },
                "questions": {
                    "value": "No further questions, besides addressing the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7098/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699175471977,
            "cdate": 1699175471977,
            "tmdate": 1699636837492,
            "mdate": 1699636837492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KUuA4QOWof",
                "forum": "ym0ubZrsmm",
                "replyto": "w7p7jKJVma",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7098/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "Firstly, we are honored to receive such high praise from Reviewer **#f65W**, and thanks for the valuable suggestions. In our current manuscript, the limitation analyses on SSOD is attached in  `Appendix A.5` (Failure Case Analyses). And in our revised manuscript, we will extend the part **Conclusion and Discussion** to include more insights on broadening the scope of SSOD. We have noticed the potential of extending SSOD to transformer architectures, and the partly finished experiments surprised us. There are many details of generalizing SSOD to self-attention-based models. For instance, how to sample local background given the fact that self-attention fuses global information, and how to perform inference since the tokens of background patches are less than the whole images. All these works will be finished ASAP and demonstrated on Arxiv. **Besides, we here want to highlight that SSOD has been deployed in our in-house application.** The engineers highly praise the transferability and scalability of SSOD since it is quite easy to deploy as an end-to-end model."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7098/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699761680501,
                "cdate": 1699761680501,
                "tmdate": 1700034618030,
                "mdate": 1700034618030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]