[
    {
        "title": "On the Relation between Gradient Directions and Systematic Generalization"
    },
    {
        "review": {
            "id": "SxxOUFldH2",
            "forum": "P2AD80rFNu",
            "replyto": "P2AD80rFNu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7135/Reviewer_jCQf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7135/Reviewer_jCQf"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies systematic generalization problems from the perspective of gradient directions. Based on the analysis of gradients on different distributions, the paper points out a bias that the training gradient is less efficient than another alternative gradient (the gradient on the ground-truth data distribution for all samples). Such a bias is experimentally verified on many classical deep neural networks, like CNN, LSTM, ViT, etc.\n\nAlthough the idea of this paper is novel to me, the analysis and presentation of the paper are not good. It is hard for me to follow the paper, and I cannot see how the community can apply the proposed analysis to improve systematic generalization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See the summary part."
                },
                "weaknesses": {
                    "value": "Although studying the learning dynamics from the perspective of the gradient on different distributions (i.e., training, test, and overall) is novel, the paper lacks persuasive results and good presentations. It is hard for me to conclude the contribution of this paper. I have the following two main concerns. \n\nFirst, the systematic generalization (sys-gen) is not defined in this paper. What\u2019s the difference between sys-gen and OOD problem? Are there any constraints on the differences between training and test distributions? What are the differences and similarities between training and test distributions in this setting?\n\nSecond, it is hard to see any potential of the provided analysis. The paper proposes several concepts based on gradient directions and then proposes a metric named UDDR. How is the UDDR gap related to systematic generalization, and how could we improve the systematic generalization performance based on the findings?"
                },
                "questions": {
                    "value": "1. Alternative direction is not a good term in my opinion. I guess this direction means the direction of the ground-truth data distribution.\n\n2. In definition 3, \u201calternative gradient reduces the training loss\u201d. I think this type of claim is a little weird. Because in practice, learning can only reduce the training loss, while the gradient for all data is unobservable. Better to say \u201ctraining gradient reduces the alternative loss\u201d. Or just call this \"alternative gradient\" an \"oracle gradient\", which means the optimal but inaccessible correct gradient.\n\n3. Many of the concepts and definitions in this paper are based on vector inner produce, e.g., definition 3, 4, proposition 1, 2, etc. It is very hard to remember what they are discussing without a clear visualization. I think it would be helpful to visualize these concepts using a series of figures (similar to Figure 2) somewhere in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7135/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698471421135,
            "cdate": 1698471421135,
            "tmdate": 1699636844485,
            "mdate": 1699636844485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vgln0UiE24",
                "forum": "P2AD80rFNu",
                "replyto": "SxxOUFldH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "> Q1. Although studying the learning dynamics from the perspective of the gradient on different distributions\n(i.e., training, test, and overall) is novel, the paper lacks persuasive results and good presentations. It is\nhard for me to conclude the contribution of this paper. I have the following two main concerns.\n\nA1. The main contribution is the proposed formulation of treating reducible training loss as a resource and\ncompute efficiency.\n\n> Q2. First, the systematic generalization (sys-gen) is not defined in this paper. What\u2019s the difference\nbetween sys-gen and OOD problem? Are there any constraints on the differences between training and\ntest distributions? What are the differences and similarities between training and test distributions in this\nsetting?\n\nA2. In an OOD problem, some test samples have zero probability in training distribution. The sys-gen is a\ntype of OOD problem, and it has a new combination of seen factors in the test. Datasets in the experiment\nsections are examples. Since it has the new combination, the test samples are likely to be distant from the\nsupport of training distribution (which may not be true for all OOD problems). So, the training and the\nalternative gradients are not likely to have the same direction. This assumption on the difference in direction\nis important to apply the theorem. So, in this paper, the main characteristic of sys-gen is that the training\nand the test distributions have large differences.\n\n> Q3. Second, it is hard to see any potential of the provided analysis. The paper proposes several concepts\nbased on gradient directions and then proposes a metric named UDDR. How is the UDDR gap related to\nsystematic generalization, and how could we improve the systematic generalization performance based on\nthe findings?\n\nA3. UDDR is the efficiency of consuming training loss as a resource. When the efficiency is low, the test\nlosses are less reduced, so there is a bias not to achieve systematic generalization. This paper discusses\nstandard architectures and explains the difficulties for them. It provides a theoretical explanation of the\nbias, which is often empirically observed. It is a theoretical basis for why we should avoid using the standard\nnetworks. When we design new architecture or regularization for sys-gen, it says we should consider whether\nit avoids the bias.\n\nA4. Also, thank you for the writing and figure suggestions. We will improve them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310328599,
                "cdate": 1700310328599,
                "tmdate": 1700310328599,
                "mdate": 1700310328599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4bNFjaFF0r",
                "forum": "P2AD80rFNu",
                "replyto": "vgln0UiE24",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Reviewer_jCQf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Reviewer_jCQf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks very much for the response, I will keep my original score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372514129,
                "cdate": 1700372514129,
                "tmdate": 1700372514129,
                "mdate": 1700372514129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Na1yl5XiZa",
            "forum": "P2AD80rFNu",
            "replyto": "P2AD80rFNu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7135/Reviewer_ymSN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7135/Reviewer_ymSN"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a measure of out-of-distribution generalization bias based on a \\textit{total} and \\textit{training} gradient (cosine) similarity. After introducing definitions and lemmas, authors show (Theorem 1) that, letting alone degenerate case of zero gradients, the systematic generalization bias exists whenever  \\textit{total} and \\textit{training} gradients point in DIFFERENT direction, i.e., their cosine similarity is < 1. The claim is (partially) supported by experiments on the most popular deep learning architectures incl. transformers, LSTM and ResNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ Well written Introduction section\n+ Addressing a timely and needed research topic of systematic generalisation of deep learning models\n+ Thorough build up of definitions and lemmas leading to the main Theorem 1 and related Propositions and Corollaries"
                },
                "weaknesses": {
                    "value": "While claim of Theorem 1 seems almost trivial from Definitions 1-4 (bias), the most of the paper covers formalism of this relation. But it rather misses explaining why would such a \"local\" step-wise definition of systematic generalization bias (see bellow for what is meant by \"local\") capture a \"global\",i.e., after all gradient descent steps are done, o.o.d. generalization gap. In my opinion the paper in its current form has made interesting initial steps but has not succeeded in showing the proposed measure of bias is a \"good\" one (in a sense described bellow).\n\n- While experimental setup is described at length, the summary of experiments is only one paragraph long and lacking needed details. More over, Contributions, bullet 3 on page 2, it is claimed: \u201cExperiments validate the result and demonstrate a bias in standard deep learning models \u201d. \n\nWhat is meant by \u201cvalidate\u201d exactly? The fact that there is generalization gap present in the most of DL models is generally known. If authors want to show that UDDR gap is good measure of it then they should \"validate\" not only UDDR gap =>\"bad systematic generalization\", but as well a contraposition the statement, i.e.,  \"good o.o.d. generalization => low UDDR gap\". The experiments do not show any such example to my knowledge.\n\n- \u201cWe also discuss that systematic generalization requires a network decomposed to sub-networks, each with a seen test inputs. \u201d This is very interesting suggestion and possible research direction that would deserve a bit more comments perhaps. The section 4.2 only provides very brief and, to me, incomprehensible comments, however. Especially the Requirement on page 8, \u201c Systematic generalization requires that a model can be decomposed into sub- networks, each with seen test inputs. \u201d. For instance what is a \u201csubnetwork with seen test inputs?\u201d\n\n- Proposed method of DDR (and D()) measures step-wise bias, i.e., a \"local\" bias at a given step of a gradient descent. How does this local bias relate to a final, \"global\", systematic generalization gap? Can there be two different pathways leading to the same model/result? Why not?"
                },
                "questions": {
                    "value": "In addition to questions raised in \"Weaknesses\" section. \n\n- Contributions: Claim No. 3 on page 2: \u201cExperiments validate the result and demonstrate a bias in standard deep learning models \u201d. What is meant by \u201cvalidate\u201d exactly? The fact that there is generalization gap present in the most of DL models is generally known. If authors want to show that UDDR gap is good measure of it then they should \"validate\" not only UDDR gap =>\"bad systematic generalization\", but as well a contraposition the statement, i.e.,  \"good o.o.d. generalization => low UDDR gap\". The experiments do not show any such example to my knowledge.\n\n- What is \u201csin\u201d in Prop 1? Please add am explanatory note \u2026\n\n- Fig 4. UDDR of exactly what is depicted? It has three arguments \u2026 is if UDDR(test, train, all)?\n\n-(Q): Fig 4, 5 Did both networks converge to same or different optima? Could you add these results?\n\nMore generally, how does proposed approach deal with stochastic gradient descent with noisy gradient and possibly several paths leading to the same solution? The appendix treats this very briefly and does not answer the question in my opinion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7135/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668765515,
            "cdate": 1698668765515,
            "tmdate": 1699636844378,
            "mdate": 1699636844378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KrLdX1E8pp",
                "forum": "P2AD80rFNu",
                "replyto": "Na1yl5XiZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 1/2"
                    },
                    "comment": {
                        "value": "> Q1. While claim of Theorem 1 seems almost trivial from Definitions 1-4 (bias), the most of the paper\ncovers formalism of this relation.\n\nA1. The point is that definitions 3 and 4 may not cover all possible cases. So, it looks trivial because\nDefinition 4 contains only rare cases, so the biased cases (Definition 3) seem to dominate spontaneously.\nHowever, we need Proposition 1,2,3 to prove they are the only cases (Lemma 1), and the derivation is trivial.\n\n> Q2. Proposed method of DDR (and D()) measures step-wise bias, i.e., a \u201dlocal\u201d bias at a given step of\na gradient descent. How does this local bias relate to a final, \u201dglobal\u201d, systematic generalization gap? Can\nthere be two different pathways leading to the same model/result? Why not? How does proposed approach\ndeal with possibly several paths leading to the same solution?\n\nA2. The questions are about the relation between local and global behaviors. Multiple paths can lead to the\nsame solution. However, a path following training gradients is less likely to achieve a solution of systematic\ngeneralization. The point is that the theorem compares two efficiencies at the same point in the parameter\nspace. So, it fits to study whether a pathway can reduce enough test loss, but it does not fit to directly\ncompare different pathways.\n\nTwo different pathways can lead to the same model. Suppose different pathways P and Q. Q follows the\nalternative gradient at each step and is an oracle path. When the pathways deviate, P has less efficiency. If\nthe two pathways lead to the same result, they have the same final test loss. It means P will catch up with Q\nafter deviating. This is possible because the efficiency is compared between two gradients at the same point\nin the parameter space. Since P and Q are already apart, they are not at the same point, so the theorem\ndoes not apply.\n\nWe then look at the case that P follows the training pathway. This paper assumes that the training and the\nalternative gradients do not point in the same direction for standard architectures in systematic generalization\nproblems. The assumption and the theorem apply to each step in P. It means each step in P has a bias in\nefficiency, compared with an alternative gradient that leads to the final test loss of the Q pathway. So, the\nfinal test loss of the P pathway unlikely achieves that of the Q pathway. It also means that the P and Q\npathways have different final models. Also, the experiments show that the efficiency gap will likely increase\nduring training, suggesting the P is not likely to catch up.\n\nWe aim to develop a theory for local behavior. The gradient is defined locally, so developing a theory in\nlocal space is more natural. The training process may be unstable since a small difference may cause a large\ndifference after multiple steps, which makes it hard to develop a strict theory. In analogous, gradients are\nused to train models, though it does not guarantee global optimum.\n\n> Q3. While experimental setup is described at length, the summary of experiments is only one paragraph\nlong and lacking needed details.\n\nA3. Thank you, and we would like to improve the result summary. The purposes and settings of experiments\nare stated before it, so the summary mainly needs to check whether the results meet the expectations. Other\ndetails of the experiments are in Appendix B. We assume that the training and the alternative gradients have\ndifferent gradient directions in each training step. The result shows the assumption holds in the experiments.\n\n> Q4. More over, Contributions, bullet 3 on page 2, it is claimed: \u201cExperiments validate the result and\ndemonstrate a bias in standard deep learning models \u201d. What is meant by \u201cvalidate\u201d exactly?\n\nA4. \u201cValidate\u201d means providing common examples to check that the derived result is correct. It helps to\nimprove the confidence of the proofs, especially when it is long.\n\n> Q5. The fact that there is generalization gap present in the most of DL models is generally known. If\nauthors want to show that UDDR gap is good measure of it then they should \u201dvalidate\u201d not only UDDR\ngap => \u201dbad systematic generalization\u201d, but as well a contraposition the statement, i.e., \u201dgood o.o.d.\ngeneralization => low UDDR gap\u201d. The experiments do not show any such example to my knowledge.\n\nA5. The purpose of this paper is to provide a theoretical explanation of empirical observation that standard\ndeep learning models often do not generalize systematically. We mainly focus on local behaviors. It is a\ngood suggestion to test the contraposition statement. It is an additional support. In this study, we do not\nfind a standard architecture trained to achieve the generalization ability, so we do not have examples for the\ntest. We like to think how we can do it."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309269044,
                "cdate": 1700309269044,
                "tmdate": 1700309269044,
                "mdate": 1700309269044,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OnvMH3sibB",
                "forum": "P2AD80rFNu",
                "replyto": "Na1yl5XiZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 2/2"
                    },
                    "comment": {
                        "value": "> Q6. \u201cWe also discuss that systematic generalization requires a network decomposed to sub-networks,\neach with a seen test inputs.\u201d This is very interesting suggestion and possible research direction that would\ndeserve a bit more comments perhaps. The section 4.2 only provides very brief and, to me, incomprehensible\ncomments, however. Especially the Requirement on page 8, \u201cSystematic generalization requires that a model\ncan be decomposed into sub-networks, each with seen test inputs. \u201d. For instance what is a \u201csubnetwork\nwith seen test inputs?\u201d\n\nA6. An example of seen test input is that a subnetwork is a patch in a convolutional layer. Though the whole\ninput for a test sample is unseen, a patch may be seen. It may also be a part of a hidden representation.\nWe would like to add more comments in the discussion section.\n\n> Q7. What is \u201csin\u201d in Prop 1? Please add am explanatory note.\n\nQ7. sin is sine trigonometric function.\n\n> Q8. Fig 4. UDDR of exactly what is depicted? It has three arguments . . . is if UDDR(test, train, all)?\n\nA8. UDDR is a scalar value corresponding to the efficiency of consuming training loss. Figure 4 depicts\nUDDR values in each iteration (step) of the training.\n\n> Q9. Fig 4, 5 Did both networks converge to same or different optima? Could you add these results?\n\nA9. The following tables show the training and the test losses in the trained models in each experiment.\nIn each experiment, the losses have a significant difference, indicating the models do not achieve the generalization.\nIt also means the trained parameters differ from those that enable the generalization.\nNote that rows may have different datasets (e.g., image or text), so they are not comparable.\n\nTrain and test losses on trained models for experiments in Figure 4 and Figure 5.\n| Figure 4 | Train loss | Test loss |\n| ----------- | ----------- | ----------- |\n| Fully connected network | 0.11 $\\pm$ 0.01 | 2.33 $\\pm$ 0.08 |\n| Convolutional network   | 0.03 $\\pm$ 0.01 | 6.22 $\\pm$ 0.23 |\n| Residual Network        | 0.25 $\\pm$ 0.09 | 3.53 $\\pm$ 0.46 |\n| Vision Transformer      | 0.28 $\\pm$ 0.02 | 2.28 $\\pm$ 0.07 |\n| LSTM                    | 0.26 $\\pm$ 0.01 | 2.49 $\\pm$ 0.02 |\n| Transformer             | 0.19 $\\pm$ 0.01 | 3.01 $\\pm$ 0.10 |\n\n| Figure 5 | Train loss | Test loss |\n| ----------- | ----------- | ----------- |\n| Fully connected network | 0.01 $\\pm$ 0.00 | 7.19 $\\pm$ 0.66 |\n| Convolutional network   | 0.00 $\\pm$ 0.00 | 7.95 $\\pm$ 1.54 |\n| Residual Network        | 0.03 $\\pm$ 0.02 | 3.78 $\\pm$ 0.55 |\n| Vision Transformer      | 0.00 $\\pm$ 0.01 | 5.34 $\\pm$ 0.55 |\n| LSTM                    | 0.36 $\\pm$ 0.01 | 0.78 $\\pm$ 0.04 |\n| Transformer             | 0.41 $\\pm$ 0.01 | 0.85 $\\pm$ 0.03 |\n\n> Q10. More generally, how does proposed approach deal with stochastic gradient descent with noisy\ngradient? The appendix treats this very briefly and does not answer the question in my opinion.\n\nA10. Appendix C.1 discusses variants of gradients, and stochastic gradient can be regarded as one of them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309941416,
                "cdate": 1700309941416,
                "tmdate": 1700309941416,
                "mdate": 1700309941416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2fVYuwgbJ3",
                "forum": "P2AD80rFNu",
                "replyto": "Na1yl5XiZa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Reviewer_ymSN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Reviewer_ymSN"
                ],
                "content": {
                    "comment": {
                        "value": "I thank authors for their responses. My concerns have not been alleviated, I am afraid. The most pressing are Q2 and Q5. \nAs for Q2. \n\n> from A2: \"... a path following training gradients is less likely to achieve a solution of systematic generalization ...\". Where is it shown in the paper?\n\nI try to clarify Q2 a bit more. Consider convex non-symetrical problem (eigenvalues of Hessian are not equal). Then natural (preconditioned by inverse Hessian) vs. vanilla gradient descent, starting (by choice) and ending (by construction) at the same solution, will take different paths. Whatever oracle path is (could even be one of them) all solutions coincide and have same properties. Relatively natural and vanilla GD are inferior to each other as their efficiency biases to oracle path differ (due to preconditioning). But at the same time they are not less likely to achieve a solution of systematic generalization (claimed above). \n\nCould authors clarify this through prism of the paper?\n\nAd A5.)  Especially because the paper \"... provides a theoretical explanation...\" and since \"A =>B\" is equivalent to \"B' => A'\", where prime \" ' \" denotes logical negation, the contraposition mentioned in Q5 has to hold. Could authors show it in the \"convex\" settings, as the one above, perhaps?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389606920,
                "cdate": 1700389606920,
                "tmdate": 1700389668492,
                "mdate": 1700389668492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TAoerZT1lg",
                "forum": "P2AD80rFNu",
                "replyto": "Y4a3SVv2gJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Reviewer_ymSN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Reviewer_ymSN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for their responses. Unfortunately, they have not alleviated raised concerns. \n\nFor instance, the response to A5 regarding contraposition in the rebuttal above\"...the training and the alternative gradients do not deviate at any step.\" Considering GD and natural GD (gradient multiplied by inverse of Hessian) in the convex case, these two gradient directions do differ at every step (for non-symmetric Hessian) but still converge to same train and test error ...\n\nI keep my rating. Thank you."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733679102,
                "cdate": 1700733679102,
                "tmdate": 1700733679102,
                "mdate": 1700733679102,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g7zsfsbE1C",
            "forum": "P2AD80rFNu",
            "replyto": "P2AD80rFNu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7135/Reviewer_SN1k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7135/Reviewer_SN1k"
            ],
            "content": {
                "summary": {
                    "value": "This work explores how the alignment of the parameter update of a neural network followed by gradient descent with that of the gradient which aligns with the true (all data distribution) gradient. A number of cases are considered, which provable cover all possible outcomes, and it is shown that standard deep learning models are predominantly biased away from systematic generalisation. This argument is supported empirically for a range of practical models and tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "## Originality\nThe notion of treating the gradient update step as a resource which must be allocated is indeed interesting and an intuitive interpretation of events.\n\n## Quality\nThe experimental design of Section 3 does support the main claim of the theory that neural networks are inherently biased away from generalisation. In addition the experiments cover a wide range of models that are of practical interest to the community. This makes the results interesting and useful. The claims which are made from these experimental results also appear accurate. On the theory side, definitions are clearly stated and used consistently throughout and propositions are formal but also explained clearly.\n\n## Clarity\nThe work clearly describes the hypothesis. Figures are clear, interpretable and relate easily to the text.\n\n## Significance\nThis work aims to address an interesting problem, and importantly does so for a broad range of practically useful models. Thus, the proposed finding of explaining why this broad range of models fail to systematically generalise could be of high impact. I do have some concerns on whether the work fully realises its intended purpose which I discuss in the Weaknesses section below. This limits the potentially high impact and significance of this work."
                },
                "weaknesses": {
                    "value": "## Quality\nMy main issue with this work is that it does not seem to address systematic generalisation, but generalisation more broadly. A primary purpose of systematicity and compositional generalisation is to break the task into smaller pieces which are then learned (the modules specialise to the subtask) and composed later. By definition learning these smaller pieces is not the same as learning the entire data distribution. This seems at odds with this work's proposed theory that how far a network deviates from the all gradient direction is an indicator of systematicity. For example, learning a module which identifies the colour red, another which identifies cars and then learning to use both modules to identify red cars - as would be the case with neural module networks [1] - would follow extremely different gradient directions than if a similar network trained to directly identify all red cars. Put another way, to learn a disentangled representation [2] (a stronger condition than systematicity) you would need to follow a different gradient than if you learned the ground-truth mapping directly. Even from a linguistics perspective, seeing only a subset of data and this being used to learn something different from memorizing all of language is a foundational idea in systematicity [3]. Finally, more recent theory even explicitly makes the distinction between generalising because the network has seen enough data and generalising because the network decomposed the problem and learned a solution with an entirely different rank [4]. This last point is from a paper which is only a year old - and so open to debate - but this work would need to at least show how its definition of systematicity aligns with these prior notions. So for example the line \"The alternative gradient is computed from all data, leading to systematic generalization\" is just at odds with our notions of systematicity. This work, is of interest to generalisation broadly however, just not systematic generalisation as far as I can tell. Also the mention of \"seen test inputs\" is confusing and I don't know what this is meant to be referring to. But by definition test inputs are unseen.\n\n## Clarity\nOn the point of clarity, there are some statements in this work which do not make sense or appear out of context. Examples are \"Also, deep learning does not require many task-specific designs for specific tasks\", \"Some standard networks, such as ... work well in i.i.d. settings\", \"To keep the advantage, we  discuss whether standard deep learning models achieve systematic generalization\", \"The condition $\\Delta=0$ is rare to hold because it requires an equation to hold\" and \"Both (A) and (B) contain equal signs, which are generally difficult to hold\". Hopefully these examples will guide a general clean up of the writing.\n\nA few more quicker points and concerns on clarity are the following. The last paragraph of page 2 where the notation is introduced is also not clear and introduces more notation than necessary. Why are $u$ and $h$ defined here and why have $u$ if $x$ already denotes input vectors? Similarly, $D(f,u)$ is defined and includes a case for if $u=0$ where on the top of the same page it is stated that $u \\neq 0$. $\\Delta$ is overall unhelpful as it obscures comparison with the other uses of  the $D(\\cdot,\\cdot)$ function. Definition 3 and 4 could be merged with Propositions 1, 2 and 3 since the propositions follow immediately from the definitions. Proposition 2 and 3 could also just be combined since the two cases are practically identical. DDR is also only used for two of five cases and so appears to be another function needlessly defined which just obscures the comparison of various cases. Also, why not use UDDR from the beginning? The captions for Figure 3 and Table 1 should be improved and clearly state why we should care about these datasets, how they are used and why they relate to systematicity.\n\n[1] Andreas, Jacob, et al. \"Neural module networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. \\\n[2] Locatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\" international conference on machine learning. PMLR, 2019. \\\n[3] Hadley, Robert F. \"Systematicity in connectionist language learning.\" Mind & Language 9.3 (1994): 247-272. \\\n[4] Jarvis, Devon, et al. \"On The Specialization of Neural Modules.\" The Eleventh International Conference on Learning Representations. 2022."
                },
                "questions": {
                    "value": "I have asked a number of questions and raised some concerns in the Weaknesses section where they naturally came up. I do not currently have any further questions for this section but would appreciate if these early questions were addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7135/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7135/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7135/Reviewer_SN1k"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7135/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835075901,
            "cdate": 1698835075901,
            "tmdate": 1699636844264,
            "mdate": 1699636844264,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N75Dy6UN4o",
                "forum": "P2AD80rFNu",
                "replyto": "g7zsfsbE1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 1/2"
                    },
                    "comment": {
                        "value": "> ### Quality\n> Q1. My main issue with this work is that it does not seem to address systematic generalisation, but\ngeneralisation more broadly. A primary purpose of systematicity and compositional generalisation is to break\nthe task into smaller pieces which are then learned (the modules specialise to the subtask) and composed\nlater. This seems at odds with this work\u2019s proposed theory that how far a network deviates from the all\ngradient direction is an indicator of systematicity. ... This work, is of interest to generalisation broadly\nhowever, just not systematic generalisation as far as I can tell.\n\nA1. Thank you for the detailed explanation. We agree it is reasonable to require systematic generalization\nto learn internal modules or representations corresponding to the factors so that the gradient differs from\nthe one computed from all data. We have the following arguments.\n\n(a) **Training with all data achieves the \u201ceffect\u201d of systematic generalization.** The effect means\nthe model has the same outputs and loss as the one with systematicity. It is a necessary condition for the\nsystematic generalization. So, when a trained model has less test loss reduction, it does not achieve the\ngeneralization with systematicity (because the test loss is not low enough). Also, even if we only look at the\neffect, understanding the bias is still important because it would be great for standard models to achieve the\neffect.\n\n(b) **The theorem can also be applied to the cases with modules or representations.** We may\nthink about a setting with the supervision of the modules or representation. We can train a model to output\nthe prediction of the supervision. When all data are used in training, a model is encouraged to learn the\ncorrect output. So, it fits the notion of systematic generalization. It is normal training, so the theorem\napplies. The classification tasks in the experiment section correspond to such cases.\n\nAlternatively, we can consider supervision of the internal representation, though they are not usually available. It is not standard, so we do not focus on them. However, the theorem applies since the supervision\ncan be a part of the loss.\n\n(c) **Why do we focus on systematic generalization?** We focus on systematic generalization because it has\nnew factor combinations in test samples, so the training and the test distributions should have significant\ndifferences. So, it makes more sense to assume that the training and the alternative gradients do not point in\nthe same direction for standard architectures. Also, systematic generalization is an important topic. Despite\nthe distribution difference, systematic generalization is straightforward for humans, so it seems able to solve.\nHowever, it may not be true for other generalization types with such large differences.\n\n> Q3. Also the mention of \u201dseen test inputs\u201d is confusing and I don\u2019t know what this is meant to be referring\nto. But by definition test inputs are unseen.\n\nA3. It means the test input for each decomposed sub-network is seen. The whole input is unseen by\ndefinition. However, the input of a sub-network may be a part of the whole input, such as a convolutional\nlayer or attention. It may also be a part of a hidden representation. For example, a hidden variable is a\ncolor, and it is invariant to other factors, even in test data."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307290376,
                "cdate": 1700307290376,
                "tmdate": 1700307290376,
                "mdate": 1700307290376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bv5l1aUqS5",
                "forum": "P2AD80rFNu",
                "replyto": "g7zsfsbE1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 2/2"
                    },
                    "comment": {
                        "value": "> ### Clarity\n> Q4. On the point of clarity, there are some statements in this work which do not make sense or appear\nout of context. Examples are \u201dAlso, deep learning does not require many task-specific designs for specific\ntasks\u201d, \u201dSome standard networks, such as ... work well in i.i.d. settings\u201d, \u201dTo keep the advantage, we discuss\nwhether standard deep learning models achieve systematic generalization\u201d.\n\nA4. These statements explain why we focus on standard models. It is because they have advantages of\ngood performance and generality (apply to different tasks). So it is welcomed if they show systematic\ngeneralization ability.\n\n> Q5. \u201dThe condition \u2206 = 0 is rare to hold because it requires an equation to hold\u201d and \u201dBoth (A) and\n(B) contain equal signs, which are generally difficult to hold\u201d. Hopefully these examples will guide a general\nclean up of the writing.\n\nA5. These cases are considered for the strictness of the theory. We will consider how to make them clearer.\n\n> Q6. A few more quicker points and concerns on clarity are the following. The last paragraph of page 2\nwhere the notation is introduced is also not clear and introduces more notation than necessary. Why are u\nand h defined here and why have u if x already denotes input vectors?\n\nA6. u is a vector in parameter space, so it is different from x. We study gradient, so the input is in the\nparameter space. We agree that it is more clear to define h in Definition 1, and we will make an update.\n\n> Q7. Similarly, D(f,u) is defined and includes a case for if $u = 0$ where on the top of the same page it is\nstated that $u \\neq 0$.\n\nA7. Definition 1 assumes $u \\neq 0$, and $u = 0$ is the extension to the definition.\n\n> Q8. \u2206 is overall unhelpful as it obscures comparison with the other uses of the D(.,.) function.\n\nA8. \u2206 is used to compare with zero as a threshold to discuss different cases, and it is not compared with\nother uses of D(.,.) in the main paper. We will think about how to update it.\n\n> Q9. Definitions 3 and 4 could be merged with Propositions 1, 2, and 3 since the propositions follow\nimmediately from the definitions. Proposition 2 and 3 could also just be combined since the two cases are\npractically identical. DDR is also only used for two of five cases and so appears to be another function\nneedlessly defined which just obscures the comparison of various cases.\n\nA9. Thank you. We will think about how to revise the paper.\n\n> Q10. Also, why not use UDDR from the beginning?\n\nA10. When the two gradients both reduce training loss, it makes sense to compare their efficiency, and we\nuse DDR to compute the ratio. However, if one reduces training loss and the other does not, the ratio is\nnot important because the signs are different, i.e., reducing or increasing training loss. Also, DDR has a\nstraightforward interpretation, whereas UDDR may not.\n\n> Q11. The captions for Figure 3 and Table 1 should be improved and clearly state why we should care\nabout these datasets, how they are used and why they relate to systematicity.\n\nA11. Thank you. We may write in the following way.\n\nFigure 3: Examples of image data. Each sample has an image and two factors: foreground and background.\nThe image is the model input, and each factor is one of two classification outputs. Training data and test\ndata have different factor value combinations (e.g., [cat, grass]), and each factor value is seen in training\ndata. For example, the rightmost sample (d) with [bus, rock] combination is in test data, and the other\nthree samples (a,b,c) are in training data. A model needs to work on unseen samples by recombining factors\nlearned in training (systematic generalization), which makes learning more efficient and supports creativity.\n\nTable 1: Examples of text data. Each sample has a review (text) and two factors: category and ranking.\nSimilar to the image data example, it is a systematic generalization problem, which has the review as input\nand the factors as classification outputs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307628158,
                "cdate": 1700307628158,
                "tmdate": 1700307673281,
                "mdate": 1700307673281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PuapnJRLv7",
                "forum": "P2AD80rFNu",
                "replyto": "bv5l1aUqS5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Reviewer_SN1k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Reviewer_SN1k"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their rebuttal. I will await an updated draft to assess if the clarity issues raise in Q4 to Q11 are addressed.\n\nUnfortunately I am unconvinced by the authors discussion in A1. The authors seem to be arguing that if a network systematically generalizes it will behave as if it has seen the full data distribution (in other words it will generalize). Ignoring more specific point on whether the network has seen all possible pieces, I agree with this. If I am correct in this interpretation then there is still a major issues. This is because the authors are then using the gradient of the full distribution to justify that the network is systematically generalizing. But this is not true. Systematic generalization (again generously ignoring details) is a sufficient condition for the network to correctly classify unseen data. Classifying unseen data is a necessary but not sufficient condition for systematic generalization. Systematic generalization is not the only way to generalize to unseen data. It is the manner in which the generalization is achieved which matters and systematic generalization is a fundamentally different manner of generalization than aligning to the potentially very entangled relationship between features and labels in the ground-truth mapping.\n\nSupervising modules is indeed a way to systematically generalize, but only when the modules are allocated correctly. Thus, it once again cannot be taken for granted just because a module is used [1].\n\n[1] Bahdanau, Dzmitry, et al. \"Systematic generalization: what is required and can it be learned?.\" arXiv preprint arXiv:1811.12889 (2018). Finally, A3 raises other question. If I understand the implication being made here, the network sees pieces of the input during training and then must systematically generalize to the full input at test time? But this does make sense because then the notion of the network following the complete data distribution gradient would be impossible.\n\nUnfortunately, I will be keeping my rating the same for now."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677852331,
                "cdate": 1700677852331,
                "tmdate": 1700677852331,
                "mdate": 1700677852331,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R5JsCs3X8Q",
                "forum": "P2AD80rFNu",
                "replyto": "g7zsfsbE1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7135/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed comments. \n\n> A1(a). The authors seem to be arguing that if a network systematically generalizes it will behave as if\nit has seen the full data distribution (in other words it will generalize). Ignoring more specific point on\nwhether the network has seen all possible pieces, I agree with this. If I am correct in this interpretation then\nthere is still a major issues. This is because the authors are then using the gradient of the full distribution\nto justify that the network is systematically generalizing. But this is not true. Systematic generalization\n(again generously ignoring details) is a sufficient condition for the network to correctly classify unseen data.\nClassifying unseen data is a necessary but not sufficient condition for systematic generalization. Systematic\ngeneralization is not the only way to generalize to unseen data. It is the manner in which the generalization is\nachieved which matters and systematic generalization is a fundamentally different manner of generalization\nthan aligning to the potentially very entangled relationship between features and labels in the ground-truth\nmapping.\n\nWe would like to clarify our arguments and discuss the main concern\nin rebuttal A1(a). The point is that we argue something does not hold because its necessary condition does\nnot hold.\n\n(i) We like to clarify arguments by citing a comment.\n> Classifying unseen data is a necessary but not sufficient condition for systematic generalization.\n\nThe argument is that a model trained on training data does not reduce enough test loss (compared to a\nmodel trained on full distribution). So, it does not classify unseen data well. So, the necessary condition in\nthe cited comment does not hold. So, systematic generalization (including the details) is not achieved.\n\n(ii) Discuss the main concern.\n> The authors are then using the gradient of the full distribution to justify that the network is systematically\ngeneralizing.\n\nIn the rebuttal, we mention using the gradient of the full distribution to justify that the network (trained with the full distribution\ngradients) has the same final test loss as an ideal network that systematically generalizes (including the\ndetails), because they have the same test outputs. It is enough to justify the test loss because we argue\na necessary condition does not hold (the last comment).\n*(This answer has been modified after it was initially posted.)*\n\n> A1(b). Supervising modules is indeed a way to systematically generalize, but only when the modules are\nallocated correctly. Thus, it once again cannot be taken for granted just because a module is used [1].\n\nSupervision is ground-truth, so why may the modules not be allocated correctly?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7135/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729096000,
                "cdate": 1700729096000,
                "tmdate": 1700731752398,
                "mdate": 1700731752398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]