[
    {
        "title": "Large Language Models can $\\textit{Share}$ Images, Too!"
    },
    {
        "review": {
            "id": "9ruUr3UysY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_7gME"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_7gME"
            ],
            "forum": "fhJALInfQj",
            "replyto": "fhJALInfQj",
            "content": {
                "summary": {
                    "value": "This submission is primarily concerned with the \u201cimage sharing\u201d capability of LLM, to make it a more human-like chatbot. It proposes a two-stage mechanism for image sharing which is based on prompt-engineering."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The task is novel and critical for a chatbot application."
                },
                "weaknesses": {
                    "value": "The task needs to be better motivated:\n1.\tThough \u201csharing\u201d images is very well-motivated for today\u2019s chatbot applications. If it\u2019s a retrieval task over Internet images, it could bring user with complimentary information missing from language. However, if it\u2019s as shown in one of the expriments, where image generation conditioned on texts which are generated from LLM is shared, I\u2019m afraid there were nothing language could not deliver in the first place.   \n2.\tEven though there is a previous work, Zang et al., who proposes the PhotoChat dataset, it\u2019s still worth questioning if the \u201ctwo-stage\u201d framework is correct, or a reasonable approximation of how we should approach the capability. I found it unconvincing to me and is pending more justification.   \n\nThe method is plain and lacks comparison with alternatives:\n3.\tI doubt any readers who have played with modern LLM would be surprised/informed by this work that LLM can already predict whether to share images and provide image descriptions accordingly.   \n4.\tWhat about some other finetuning-based methods?   \n\n\nFollowing this, there are two critical scientific errors in this submission:\n5.\tIt\u2019s confusing to say \u201cwe present a restriction-based prompt by adding a Restrictions: token.\u201d This reads to me as if you introduced a new token to the vocabulary in a finetuning stage and might confuse others. In fact, it\u2019s just typing in the word \u201cRestrictions: \u201d According to [this link](https://platform.openai.com/tokenizer), this is even split into 4 tokens.  \n6.\tClaiming image-sharing an emergent capability isn\u2019t quite right, considering the authors are not observing the model sizes in a sufficiently fine-grained fashion as in Wei et al. Also the Figure 2 should be changed into a table. It\u2019s scientifically sloppy to plot the x-axis as it is now. Only comparable values should appear in x-axis of a line chart."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1137/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697334354766,
            "cdate": 1697334354766,
            "tmdate": 1699636040188,
            "mdate": 1699636040188,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "SwQmlhFN6S",
            "forum": "fhJALInfQj",
            "replyto": "fhJALInfQj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_x6ME"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_x6ME"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the ability of pure LLM-based dialogue system to understand when and where suitable images should be shared to enrich a conversation.\nThis is tackled through a two-stage system, which leverages particularly structured input prompt templates. In the first stage, the prompt template is designed to encourage the LLM to produce a ranking of dialogue utterances based on a LLM-designated confidence score. In a second stage, the authors aim for suitable positions to be replaced by detailed image descriptions, which later on may be converted to realistic-looking images via text-to-image generative models.\nFor their proposed prompt-template-based setup, various experiments are conducted, mainly aiming to compare different proprietary and open LLMs. On this basis, the authors draw the conclusions that image-sharing capacities may be of size-emerging nature, and that LLMs on their own can suitable judge where and what images to incorporate in a dialogue."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* I believe that the overall goal of separating the process of image generation and its definition and placement within a dialogue has merits, as it introduces a degree of modularity and interpretability (by operating entirely in the human-understandable text domain), which allows image generation to be done with any text-to-image model of choice. \n* To the best of my knowledge, this is also the first work that aims to tackle multi-modal dialogue from such an entirely modular perspective."
                },
                "weaknesses": {
                    "value": "I have several issues and questions with this work, which I have separated into those I find most important to have addressed, and those that are good to tackle to raise the overall quality of the work.\n\n__Larger issues__\n\n* Fundamentally, vision foundation models or fundamentally multimodal models are still required to generate the final images for a multi-modal dialogue. As such, the restriction to LLM-only strategies should be appropriately supported, as it otherwise appears like a fairly contrived research problem. Unfortunately, I do not find any comparison to existing multi-modal dialogue models or even a discussion on their limits (as listed e.g. Zhang et al. 2021 or Koh et al. 2023). It would be great if the authors could address this.\n\n* It's also difficult to determine the significance of the contribution.\n\n\t* For example, the authors introduce a new prompt template to tackle this problem. But limited details are missing, with no examples for full prompts provided, or additional details such as the potential use of in-context examples. Particularly without the latter, I find it hard to believe that the LLM naturally produces a suitable ranking of confidence-based candidate positions. Similarly, how can the authors guarantee that the produced confidence has any actual meaning?\n\t\n\t* A lot of claims are made without sufficient experimental backing. For example:\n\t\n\t\t* p.5: \"... a scaling law (...) also exists in the image-sharing turn prediction task.\". I'm not sure if the data-point and their size-relation allow for suitable conclusions over scaling laws, as the parameter sizes go from 6.7B by nearly two magnitudes to 175B, and then also includes same-sized variants trained differently.\n\t\t\n\t\t* For the human evaluation, what are the exact metrics used to measure turn relevance/speaker adequacy? What are the exact setups for the human evaluation studies, e.g. the number of participants, the experimental setup, what is the significance of the results, etc.? This impacts both Fig. 5 and Fig. 6, where results are in parts very close to each other (e.g. w/rank & w/o rank).\n\t\t\n\t\t* \"LLMs generate roughly 3.1 image-sharing turns (...), a reasonable count compared to 2.59 turns in multi-modal dialogue datasets\" - this is a difficult statement to make when looking at table 3, which shows large variances in the number counts (between 1.58 and 6.8).\n\t\t\n\t\t* The motivation and relevance of the diversity comparisons w.r.t. to the PhotoChat baseline: Is it not the case that PhotoChat only has one image per dialogue, while the LLM systems are allowed to generate multiple ones? This would throw off the direct comparability.\n\t\t\n\t\t* For 2.3, the authors simply mention a crucial initial prompt template ablation study, but provide no further details. However, numerical comparisons are important to understand the significance of the particularly proposed prompt template.\n\n\n__Smaller issues__\n\n* Connecting points in Fig. 2 implies a sequential relation between the tested models, which does not exist.\n\n* Why the consistent change between F1@All, F1@K and F1@1 across different plots (e.g. Fig.2 -> Fig.3 (right))?\n\n* Fig. 4 doesn't seem like the most suitable example, as the dialogue could be interpreted as concluded as well.\n\n* CLIPScore is somewhat lacking for meaningful text-to-image alignment, methods such as ImageReward may be more suitable.\n\n* Section 2.2 is redundant, and should be absorbed into 2.1, e.g. as a paragraph.\n\n* It would be great to provide an example of how the prompt template is filled in practice in the main paper.\n\n* The emoji in the 2.3 title (and generally when referencing their restriction-based template) is somewhat confusing, as it appears to imply \"no restrictions\".\n\n* Page 3: The change in indices for s_t -> (s_j)^(t-1)_1 makes it slightly harder to parse"
                },
                "questions": {
                    "value": "I am currently opting for rejection, as I don't believe the problem setting to be sufficiently motivated, and the experiments to provide sufficient evidence (or even a lack thereof). I am however willing to raise my score if the authors can address the larger issues mentioned in the previous section, particularly with respect to the importance and relevance of this problem (as vision-foundation models / multi-modal ones will still have to be queried inevitably), as well as the experimental significance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1137/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1137/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1137/Reviewer_x6ME"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1137/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936595695,
            "cdate": 1698936595695,
            "tmdate": 1699636040127,
            "mdate": 1699636040127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "DoEqgMyLeA",
            "forum": "fhJALInfQj",
            "replyto": "fhJALInfQj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_LPEG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_LPEG"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the image-sharing capability when using Large Language Models (LLMs), such as InstructGPT, ChatGPT, and GPT-4, in a zero-shot setting, without the help of visual foundation models. Inspired by the two-stage process of image-sharing in human dialogues, the paper proposes a two-stage framework that allows LLMs to predict potential image-sharing turns and generate related image descriptions using the proposed effective restriction-based prompt template. The paper uncovers the emergent image-sharing ability in zero-shot prompting, demonstrating the effectiveness of restriction-based prompts in both stages of the proposed framework.\n\nThe paper also creates a new dataset, and the paper plans to share the dataset together with the code after publication."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The studied problem/use case is very interesting.\n\n(2) The paper is well written and it is very easy to read/follow.\n\n(3) The proposed method has good intuitions."
                },
                "weaknesses": {
                    "value": "(1) It is not very clear why the studied setting should assume 'without the help of visual foundation models.', considering (a) it is so easy to access the visual foundation models these days (and the large multimodal model seems to be a potential future trend, e.g., GPT-4) and (b) many similar previous works (mentioned in the 2nd paragraph of the introduction section) use the visual foundation models.\n\n(2) A potential related concern is that all the baselines (except PhotoChat Zang et al. (2021)) in this paper seem to not be from previous works? Are there any baselines from previous papers can be compared here? Is it possible to compare the results in the paper to the results by the previous works mentioned in the 2nd paragraph of the introduction section, in some aligned setting?"
                },
                "questions": {
                    "value": "(1) Can you please justify why it is needed to assume 'without the help of visual foundation models.' (e.g., by some practical needs or real-world use cases)\n\n(2) Is it possible to compare the results in the paper to the results by the previous works mentioned in the 2nd paragraph of the introduction section, in some aligned setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1137/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699165270860,
            "cdate": 1699165270860,
            "tmdate": 1699636040057,
            "mdate": 1699636040057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Q89crLU6z0",
            "forum": "fhJALInfQj",
            "replyto": "fhJALInfQj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_aJto"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_aJto"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the image-sharing capability of Large Language Models (LLMs) in a zero-shot setting, without the help of visual foundation models. The problem involves two stages: (1) when to share and (2) what to share. The evaluate the method, the authors augmented the PhotoChat dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is technically sound. \n\nDoing visual understanding without any help of visual foundation models is interesting. \n\nThe proposed method is effective."
                },
                "weaknesses": {
                    "value": "The proposed method seems a bit straightforward and not that novel. \n\nAlthough the problem to understand vision without visual models is interesting, the image-sharing application itself is not that impressive."
                },
                "questions": {
                    "value": "Besides dialog, can the image-sharing capability be applied to other scenarios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1137/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699271350473,
            "cdate": 1699271350473,
            "tmdate": 1699636039981,
            "mdate": 1699636039981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "JUhcx7VPSS",
            "forum": "fhJALInfQj",
            "replyto": "fhJALInfQj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_m6aj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1137/Reviewer_m6aj"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the image-sharing capability of Large Language Models without the help of visual information.  To this end, they propose a two-stage framework: (1) predicting image-sharing turns and (2) generating image prompts, to mimic the human photo-sharing behavior. Specifically, they design a restriction-based prompt for unlocking the ability of image-sharing. Therefore, they conduct extensive experiments on PhotoChat++ (augmented by the PhotoChat), which show that LLMs achieve competitive zero-shot performance without additional training."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.  **Clear Motivation**. The motivation for unlocking the image-sharing capability in the frozen LLM is clear and interesting.\n\n2. **Contribution of the new dataset**. The photochat++ dataset may be meaningful for image-share ability if this dataset can be released."
                },
                "weaknesses": {
                    "value": "1. **The writing is bad.** The abstract section is so confusing to me that it has many undefined nouns, such as *What is image-sharing capability?* \"Inspired by the two-stage process of image sharing, what is the two-stage process?\" Figure 1 makes it hard to understand the key point of image-sharing ability. \n\n2. **The technical contribution is limited**. In the method section, it seems that you just use the open API and a prompt engineer to predict when to share and what to share. It can not provide more technical insight for the community.\n\n3. **The experiment is not enough**. Apart from LLM API, the open-source LLM models (such as Vicuna, llama v1, llama v2 [1]) also need to be evaluated.  It is important to compare the existing methods in photochat benchmark. \n\n4. **The necessity of the image-shared ability in the LLM**. With the growth of the Large Multimodal Models (LMM) (e.g. MiniGPT4[2], LLava[3], Openflamingo[4], Emu[5]), if it is necessary to unlock the image-shared ability in LLM, rather than directly use LMM to predict when to share and how to share.\n\n[1] https://github.com/lm-sys/FastChat\n\n[2] Zhu, Deyao, et al. \"Minigpt-4: Enhancing vision-language understanding with advanced large language models.\" \n\n[3] Liu, Haotian, et al. \"Visual instruction tuning.\" \n\n[4] Awadalla, Anas, et al. \"Openflamingo: An open-source framework for training large autoregressive vision-language models.\" \n\n[5] Sun Q, Yu Q, Cui Y, et al. Generative pretraining in multimodality."
                },
                "questions": {
                    "value": "As shown in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1137/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699395919193,
            "cdate": 1699395919193,
            "tmdate": 1699636039914,
            "mdate": 1699636039914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]