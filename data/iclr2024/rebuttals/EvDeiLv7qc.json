[
    {
        "title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning"
    },
    {
        "review": {
            "id": "NMOca3rJyj",
            "forum": "EvDeiLv7qc",
            "replyto": "EvDeiLv7qc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3621/Reviewer_Bdev"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3621/Reviewer_Bdev"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new Parameter Efficient Fine-Tuning (PEFT) method which uses Mixture-of-Experts (MoE) architecture. In particular, the authors target two pre-existing PEFT methods which are (IA)3 and LoRA, and replace the vectors in (IA)3 and LoRA components in LoRA with MoE modules - MoV (Mixture of Vectors) and MoLoRA (Mixture of LoRA) respectively. The authors show the effectiveness of the proposed methods by comparing the downstream task fine-tuning performance. MoV and MoLoRA outperforms (IA)3 and LoRA performance and show similar performance as full weight fine-tuning. The authors also add various ablation studies including different routing strategies, varying number of experts, and expert usage on different tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors show that a simple combination of two well-known methods (PEFT and MoE) can achieve better quality.\n- Overall, the proposed method is well described and the paper is easy to follow"
                },
                "weaknesses": {
                    "value": "- Training and inference efficiency analysis is missing. Table 6 briefly touched on the training of MoV case, but it would be good to have more comprehensive analysis such as memory consumption and training time. Also, LoRA has an advantage that the LoRA components can be added to the original weight and no additional inference cost is incurred. On the other hand, MoE might introduce some overheads. As this paper is about an efficient method, this kind of analysis will make the paper more comprehensive.\n- Even though MoV and MoLoRA use small fraction of the original model parameters, they are using quite more parameters than (IA)3 and LoRA. This is important because we are handling quite larger models these days and those information would be useful to consider. Therefore, the information about how much memory consumption and the number of parameters will increase (not full fine-tuning experiments) would be useful to have."
                },
                "questions": {
                    "value": "- 'efficiency side-effects': side-effect usually means negative, but here constant inference cost is a positive thing.\n- page 2, Contributions (3): 'higly' -> 'highly'\n- Do you have any insights why more number of experts doesn't always give better results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3621/Reviewer_Bdev",
                        "ICLR.cc/2024/Conference/Submission3621/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737359111,
            "cdate": 1698737359111,
            "tmdate": 1700606789860,
            "mdate": 1700606789860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ck0LmHthQ0",
                "forum": "EvDeiLv7qc",
                "replyto": "NMOca3rJyj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate **R Bdev** for their valuable feedback and their observations, noting that the \u201cproposed method is well described and the paper is easy to follow\u201d and that our proposed method \u201ccan achieve better quality.\u201d We acknowledge the concern raised for the preliminary analysis of the efficiency metric being reported for our proposed method, and we used the rebuttal period to run additional, more comprehensive benchmarking\n\n\nWe have now curated a comprehensive table detailing the efficiency metrics, which include parameter number, training and inference time, memory consumption, and overall performance for various methods applied to the T5-3B. By using these metrics, we aim to provide a more nuanced understanding of the performance-to-cost ratio, a crucial aspect of our study's focus on efficiency. This addition not only addresses the specific concerns raised but also enhances the overall comprehensiveness of our paper. We are confident that this addition substantially enhances the quality of our work. Furthermore, we will apply the recommended revisions addressing semantic and spelling errors in the final version of the manuscript. We respectfully request that the reviewer consider revising their evaluation in light of these improvements.\n\n\n| Method (Base model T5-XL 3B parameters) | # Params | %Decrease in Training Time w.r.t. Full FT | Increase in Inference w.r.t. Full FT (Summarization) | Peak Memory in GB (TPU v4-32) | Performance |\n| --------------------------------------- | -------- | ------------------------------------------------------------  | ------------------------------------------------------ | ----------------------------- | ----------- |\n| IA3     | 0.018%   | 38%  | 1x      | 7.88     | 52.90       |\n| MoV, 10 experts                         | 0.32%    | 31%      | 1.1x       | 10.44           | 59.93       |\n| MoV, 30 experts                         | 0.68%    | 27%     | 1.15x        | 10.62      | 60.61       |\n| LoRA, rank 4      | 0.30%    | 7%    | 1x      | 8.96        | 57.51       |\n| LoRA, rank 8    | 0.60%    | 4%      | 1x         | 9.01       | 58.89       |\n| LoRA, rank 16  | 1.20%    | 3% | 1x | 9.05  | 59.54       |\n\n\n**Training time and inference cost**: We evaluated $(IA)^3$, MoV, and LoRA for training and inference times. $(IA)^3$ trains the fastest but with the worst-performing method. MoV (10 and 30 experts) offers a balance of significantly higher training speed relative to full fine-tuning and higher performance, outperforming LoRA. In inference, $(IA)^3$ and LoRA add no extra time when their weights are integrated into the base model, while MoV increases it by only 1.1x to 1.15x depending on the number of experts. \n      \n**The number of fine-tuned parameters and memory consumption**: In the table, we provide the number of fine-tuned parameters (*# Params*) for each method.  In terms of updated parameters vs. performance trade-off, our MoV method outperforms others while balancing parameter count and performance effectively. Specifically, MoV with 30 experts surpasses LoRA rank 16 in performance while updating a smaller number of parameters.\n\nWe also include the maximum memory usage during training for each method.  Note that this memory usage is influenced by the total parameters, fine-tuned number of parameters, optimizer, batch size, and hardware. Notably, our MoV method at the 3B scale offers optimal performance without significant efficiency costs during training and inference. It also scales effectively to 11B, maintaining competitive fine-tuning performance, as shown in Figure 1 and Table 4. We plan to include this table and a detailed cost-benefit analysis in the final paper in response to reviewer feedback.\n\n> Do you have any insights why more number of experts doesn't always give better results?\n\nWe hypothesize that as the number of experts increases, each additional expert contributes less to overall performance improvement because the most significant gains are often achieved early on when key expertise areas are covered. Additional experts might overlap in their specialization, leading to diminishing returns; this notion is reflected in the routing probabilities of experts in Figure 5. The same empirical observations were illustrated in the following [1, 2].\n\n[1] Fedus, William, et al. \u201cSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.\u201d arXiv preprint arXiv:2101.03961 (2022).\n\n[2] Shen, Sheng, et al. \"Mixture-of-experts meets instruction tuning: A winning combination for large language models.\" arXiv preprint arXiv:2305.14705 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185216792,
                "cdate": 1700185216792,
                "tmdate": 1700317325529,
                "mdate": 1700317325529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zCHouJJDFe",
                "forum": "EvDeiLv7qc",
                "replyto": "3gWQagVRr3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3621/Reviewer_Bdev"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3621/Reviewer_Bdev"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing additional studies in the rebuttal.\nI think this improves the quality of the paper.\nI will change my rating to 6 based on that changes the authors are making."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606910643,
                "cdate": 1700606910643,
                "tmdate": 1700606910643,
                "mdate": 1700606910643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4M5qet7kFo",
            "forum": "EvDeiLv7qc",
            "replyto": "EvDeiLv7qc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3621/Reviewer_hesi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3621/Reviewer_hesi"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims at conducting efficient instruction tuning with language models equipped with mixture of expert layers, while to manage the large computational cost, the authors propose to consider each single copy of PEFT parameters as an expert instead of a copy of MLP, the common definition of experts, for better usage. Extensive experiments demonstrate the superiority of the proposed MOV and MOLORA architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors utilize extensive accuracy vs. compute figures to demonstrate the superior efficiency.\n- The authors emphasize the scalability of the proposed methods, which is important for large models.\n- The paper writing and architecture is clear with easy to read."
                },
                "weaknesses": {
                    "value": "- About expert specialization and load balancing: \n  - One of the fundamental reasons to utilize MoE-style architecture is expert specialization.\n  - In Sec. 3, the authors mention to apply load balance loss specifically for discrete merging. Is load balancing applied also for soft merging? And does soft merging rely on load balancing loss to prevent expert collapse? \n  - If not, how to specilize the abilities of each expert to obtain the results in Fig. 5 with simple token-level routing without any explicit control?\n  - If indeed we can get specialized experts, does the similar distribution shown in Fig. 5 suggest that the evaluated tasks still have connection with the several training tasks, otherwise it is hard to understand why a linear router can generate such a low entropy distribution on the evaluated tasks with a zero-shot setting?\n  - In other words, if the evaluated tasks differ from the training tasks a lot, the reasonable routing distribution should be close to uniform, especially for a linear router.\n- About parameter efficient fine tuning (PEFT):\n  - Another way to view this work is another way to increase amounts of PEFT parameters.\n  - But one perspective of current development of PEFT is that the amount of parameters is the most important, while how you deploy the parameters does not make much differ.\n  - In Tab. 1, comparing LoRA (r=4) & MoV-10, LoRA (r=8) & MoV-30 and LoRA (r=16) & MoV-60 (all pairs with similar learnable parameters), the performance gaps are 2.42%, 1.72% and 0.29%, greatly shrinking with respect to the number of parameters, doubting about the scalability of the proposed method. \n- Overall, \n  - From the perspective of MoE, this paper shares similar architecture with [1], and unfortunately still cannot answer the basic question including why the specialized architecture can benefit instruction tuning on novel tasks.\n  - From the perspective of PEFT, the methods can still not convince me that it is non-trivial to other PEFT methods by simply adding more parameters for better results.\n\n[1] Shen, Sheng, et al. \"Mixture-of-experts meets instruction tuning: A winning combination for large language models.\" *arXiv preprint arXiv:2305.14705* (2023)."
                },
                "questions": {
                    "value": "- Implementation details:\n  - Soft vs. discrete merging: There are two things to decide, including how many experts to choose and how to combine these experts. According to the equation in Sec. 2.2, does soft merging suggest that you first ensemble different expert weights according to the router output and then conduct forward propagation for only once? If so, does this manner only apply for (IA)3 or for LoRA with discrete merging also, since the previous manner is different from our common modeling of MoE architecture? Would you mind further explain how you conduct this ablation?\n  - What is the \"average median score\", the main evaluation metric used by the authors, first mentioned in the first line of Page 6?\n\n[1] Shen, Sheng, et al. \"Mixture-of-experts meets instruction tuning: A winning combination for large language models.\" *arXiv preprint arXiv:2305.14705* (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738345306,
            "cdate": 1698738345306,
            "tmdate": 1699636317506,
            "mdate": 1699636317506,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YL1544d29A",
                "forum": "EvDeiLv7qc",
                "replyto": "4M5qet7kFo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank **R hesi** for their detailed review. We are particularly flattered that they mentioned we \u201cutilize extensive accuracy vs. compute figures to demonstrate the superior efficiency.\u201d Also, we appreciate them noting the \u201cscalability of the proposed methods, which is important for large models.\u201d We address their remaining concerns below:\n\n> In Sec. 3, the authors mention to apply load balance loss specifically for discrete merging. Is load balancing applied also for soft merging? And does soft merging rely on load balancing loss to prevent expert collapse? If not, how to specilize the abilities of each expert to obtain the results in Fig. 5 with simple token-level routing without any explicit control?\n\nThanks for pointing out an unclear description. Our soft merging does not rely on any external load-balancing loss. The main reason that simple soft merging achieves expert specialization is the distinct task and dataset composition in the training, which is characterized by instruction tuning. Since we start with a pre-trained model, token representations are already diversified from each other based on different tasks and datasets, enabling routing to be specialized.\n\nIn our experiments, we found that load balancing loss decreases performance in both soft and discrete routing cases. We attribute this to two potential factors: (1) highly different data sizes for each dataset in our instruction tuning mixture, and (2) the misalignment between pretraining loss and fine-tuning loss when we add an auxiliary load balancing loss. This is similar to the experimental result shown in [1; Section 4.1, FLAN-ST_base].\n\n\n> If indeed we can get specialized experts, does the similar distribution shown in Fig. 5 suggest that the evaluated tasks still have connection with the several training tasks, otherwise it is hard to understand why a linear router can generate such a low entropy distribution on the evaluated tasks with a zero-shot setting?\n\nThanks for giving us the opportunity to highlight this important point. Indeed, there are cross-task relationships between multiple training tasks and held-out evaluation tasks. For example, [2] has shown that question-answering tasks such as \u201ccosmos_qa\u201d and \u201cquail\u201d have strong cross-task transfer to NLI tasks (e.g., super glue cb, super glue rte).\n\nTo validate this, during the rebuttal period, we fine-tuned task-specific $(IA)^3$ vectors for each task category in training and evaluated them for held-out tasks. As shown in the table below, different training tasks perform best on different held-out tasks. For example, the multiple-choice_qa task is most beneficial for super_glue_cb and super_glue_copa. Conversely, paraphrase and structure_to_text super_glue_rte tasks work best on super_glue_wsc.\n\n| Task     | (T-few) QA Closed Book | (T-few)  Paraphrase Task | (T-few)  QA Extractive | (T-few) QA Multiple Choice | (T-few) Sentiment | (T-few) Structure to Text | (T-few)  Summarization | (T-few)  Topic Classification |\n| ------------------------------------- | -------------------------------------- | --------------------------------------- | ------------------------------------- | ------------------------------------------ | --------------------------------- | -------------------------------------------- | ------------------------------------ | ------------------------------------------------ |\n| NLI: super_glue - cb                  | 43.8    | 51.6                                  | 42.8    | 53.3  | 40.1       | 35.5     | 33.5   | 40.1   |\n| NLI: super_glue - rte                 | 57.0    | 59.8        | 52.9        | 55.3         | 53.6                            | 47.2     | 47.9   | 47.9                    |\n| Coreference: super_glue - wsc.fixed   | 45.0   | 62.4    | 40.2    | 43.8     | 43.4   | 62.3      | 61.4       | 55.9           |\n| Coreference: winogrande - winogrande.xl | 49.5 | 50.1     | 51.5      | 50.9   | 49.8    | 50.1     | 49.8     | 50.2 |\n| Word sense disambiguation: super_glue - wic | 51.5  | 50.7  | 50.7   | 50.5     | 51.3                            | 51.3   | 50.4    | 49.2                     |\n| Sentence completion: hellaswag        | 24.1 | 25.6      | 26.4   | 27.3       | 25.3 | 25.4     | 24.7      | 27.2   |\n| Sentence completion: super_glue - copa | 49.6  | 54.5    | 51.8   | 66.9   | 57.5      | 55.1   | 53.8         | 51.9                       |\n\nAs validated by our experimental results, MoE-style architecture better exploits such latent relationships with expert specialization compared to other parameter-efficient fine-tuning methods $(IA)^3$, and LoRA, resulting in higher performance with similar efficiency."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251968722,
                "cdate": 1700251968722,
                "tmdate": 1700252164634,
                "mdate": 1700252164634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y3C3ukdnbz",
                "forum": "EvDeiLv7qc",
                "replyto": "4M5qet7kFo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3621/Reviewer_hesi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3621/Reviewer_hesi"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. However, there are several questions raised in my initial review that the authors do not response directly.\n\n**1. About the usage of loading balancing loss.**\n\nThe authors specifically claim that they do not adopt loading balancing loss, suggesting that the learnt experts can not be **sparsely** activated during training, otherwise according to [1], even initialization with a pre-trained model would still meet the risk of the expert degradation problem.\n\n**2. About the performance ceiling for fully fine-tuning.** \n\nIf this so called **performance ceiling** does exist and 1) it is close with the performance of full fine-tuning, how to explain why in Table 1 MoLORA-15 (rank 4) exceeds the T0-3B baseline? 2) If this ceiling is way higher than the performance of full fine-tuning (and also MoLORA), then the authors' response do not make sense any more.\n\n**3. About the novelty of the architecture.**\n\nInstead of a novel architecture, the proposed method is just an another implementation of the tranditional MoE framework. The tranditional MoE is defined in the Equation (MoE) on top of Page 4 in the main paper, and there are lots of different implementation for the $E_i(\\cdot)$ function, including 1) MLPs, 2) IAs and 3) LoRAs.\n\n\nTherefore, I do agree with Reviewer EZ7y that MoE is utilized in this paper only because the authors insist on using it without a solid research motivation, and as shown in the experiment tables in the authors' reply to Reviewer EZ7y, the less than **2.0%** improvement is also far \naway from **\"Pushing Mixture of Experts to the Limit\"** in the paper title. Thus, my score stands.\n\n[1] Wu, Lemeng, et al. \"Residual mixture of experts.\" arXiv preprint arXiv:2204.09636 (2022)."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729505458,
                "cdate": 1700729505458,
                "tmdate": 1700729630381,
                "mdate": 1700729630381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TZbLxH45fi",
            "forum": "EvDeiLv7qc",
            "replyto": "EvDeiLv7qc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3621/Reviewer_EZ7y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3621/Reviewer_EZ7y"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a parameter-efficient Mixture of Experts (MoE) training approach. By combining MoE architecture with lightweight experts, the paper introduces Mixture of Vectors (MoV) and Mixture of LORA (MoLORA), optimized for parameter efficiency training.  In particular, each expert is replaced with a lightweight PEFT adapter such as $(IA)^3$ vectors or LORA adapters. The proposed MoV and MoLORA, are highly efficient in terms of parameters. By updating less than 1% of the model\u2019s parameters, MoV and MoLORA consistently maintain higher performance compared to standard PEFTs and achieve comparable results with full fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written, presenting a clear and sound approach. The subject matter is highly relevant to the ML community, addressing a critical challenge: while MoEs holds great potential, its practical application has been notably hindered by prohibitive computational expenses and training instabilities, rendering it inaccessible to many researchers. Showing that parameter-efficient methods such as $(IA)^3$ or LORA can substantially improve the feasibility and effectiveness of training MoEs is a significant and valuable contribution to the field.\n\nThe authors have provided extensive ablation studies, systematically evaluating the effectiveness of their proposed MoV and MoLORA approaches against parameter-efficient fine-tuning (PEFT) strategies, and full fine-tuning. The evaluations cover multiple model sizes from the T5-family, adapter types, the number of experts, and routing mechanisms."
                },
                "weaknesses": {
                    "value": "- All experiments in this paper have been conducted exclusively on T5 model family (encode-decoder architecture). Showing that the proposed light-weight MoEs additionally works for decoder-only architectures would significantly strengthen the paper\u2019s contributions and findings.\n\n- The zero-shot evaluation tasks considered in this paper are mainly classification/multiple choice selection tasks and require generating a single token. The paper does not clearly articulate the adaptability of the proposed method to more complex tasks, such as summarization, translation, or coding, which necessitate the auto-regressive generation of longer sequences. It would be beneficial to explore and elucidate how the experts and routers operate in such long-seq generation scenarios."
                },
                "questions": {
                    "value": "The authors show that token routing performs better than sentence routing across various model sizes. An alternative input to the router could be the representation derived from the last encoder layer of T5. How does this encoded representation perform in comparison? This could also offer considerable computational advantages, particularly in situations involving a large number of experts, as it would allow for the pre-computation of the weighted average of experts immediately following the generation of the encoder representation. On the other hand, it would be interesting to see if adding or concatenating the encoder's representation to the representation of tokens further improve performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789347586,
            "cdate": 1698789347586,
            "tmdate": 1699636317370,
            "mdate": 1699636317370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xkTusJaMir",
                "forum": "EvDeiLv7qc",
                "replyto": "TZbLxH45fi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank **R EZ7y** for their very positive review of our work, noting that our paper is \u201cwell-written, presenting a clear and sound approach\u201d and emphasizing strongly the importance of the research problem. We greatly appreciate their claim that our proposed work is a \u201csignificant and valuable contribution to the field.\u201d We conducted an additional experiment during the rebuttal period to evaluate our method on additional summarization tasks.\n\n>Regarding the use of the T5 model family in our experiments and extending to decoder-only architectures:\n\nWe thank **R EZ7y** for suggesting an additional experimental axis to further evaluate our proposed method. We note that our choice to focus on the T5 model family was partly dictated by how extensive and expensive our experiments were. We trained 100+ models, the majority of them on 3B and 11B scales, including preliminary experiments, final model runs, ablations, and analysis. T5 is also heavily used in prior work [1, 2], which allows for a clean comparison.\n\nOur primary focus in the given work is to show the effectiveness of our MoE-style parameter-efficient architecture and propose an efficient alternative to the full fine-tuning baseline for instruction tuning. However, we agree that an important direction of future work is extending these results to other architectures.\n\n> Regarding our evaluation setup and evaluating our proposed methods on more complex auto-regressive generation tasks:\n\nWe clarify that we chose these tasks to fairly compare with T0 by mirroring its exact fine-tuning and evaluation setup, thereby showcasing our method's relative efficacy against a key benchmark in instruction-based tuning. These tasks are the comprehensive set of prompt instructions from the **(P3), so we also avoid any bias introduced by selecting a subset that favors our method**. We follow the same procedure as [4] where each task is converted into the format provided by the templates [1]\n\nHowever, to validate our methods\u2019 efficacy in an evaluation setup that includes auto-regressive generation, we conduct an additional evaluation during the rebuttal period on a summarization dataset, namely SAMSum [3]. Below, we compare our MoV and MoLoRA with PEFT baselines ($(IA)^3$, LoRA) and full fine-tuning using the T5-3B base model. Note that this is not a held-out task, given that its training split is included in the P3 dataset. However, we believe it may be a valuable data point for the generalization of our methods in-distribution. We calculate the rouge scores for each model.\n\nThe below results confirm that MoV and MoLoRA achieve very competitive results with full fine-tuning in a more complex generative task, similar to the trend we showed in the paper\u2019s main evaluation setting.\n\n| Base T5 3B model     | Rouge1 | Rouge2 | RougeL | Avg   |\n| -------------------- | ------ | ------ | ------ | ----- |\n| $(IA)^3$   | 43.6   | 21.4   | 36.8   | 33.9  |\n| MoV, 10 experts      | 47.8   | 24.8   | 40.3   | 37.6  |\n| MoV, 30 experts      | 48.5   | 25.3   | 41.1   | 38.3  |\n| LoRA, rank 4         | 46.1   | 22.4   | 38.0   | 35.5  |\n| LoRA, rank 8         | 46.2   | 22.5   | 38.1   | 35.6  |\n| LoRA, rank 16        | 46.3   | 23.0   | 38.5   | 35.9  |\n| MoLoRA, 2 experts, rank 4 | 47.9 | 24.8 | 40.1 | 37.6  |\n| Full fine-tuning (T0-3B) | 48.6 | 25.8 | 41.2 | 38.5  |\n\n>Regarding further exploration of routing input, particularly with the use of representation derived from the last encoder layer of T5:\n\nIn this work, we limit our exploration in terms of routers\u2019 input by only comparing pre-computed sentence embeddings and token representation. We are heartened that **R EZ7y** acknowledges we have already conducted extensive experiments (doubling the number of variants) to compare token routing to sentence routing. Our results show token routing achieves better performance since routers can exploit similarities between tokens for expert decisions, offering a better generalization. This is at least a preliminary signal that a higher degree of inductive bias for task datasets (the representation derived from the last encoder layer would take this even further) is not necessarily beneficial, as one can acquire a diverse set of task knowledge directly from the hidden representations of tokens. We leave further exploration to future work.\n\n[1] Sanh, Victor, et al. \u201cMultitask Prompted Training Enables Zero-Shot Task Generalization\u201d arXiv preprint arXiv:2110.08207 (2022).\n\n[2] Liu, Haokun, et al. \"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.\" Advances in Neural Information Processing Systems 35 (2022): 1950-1965.\n\n[3] Gliwa, Bogdan, et al. \"SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization.\" arXiv preprint arXiv:1911.12237 (2019).\n\n[4] Colin, Raffel, et al. \u201cExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\u201d arXiv preprint arXiv:1910.10683 (2020)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143683756,
                "cdate": 1700143683756,
                "tmdate": 1700143683756,
                "mdate": 1700143683756,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7BCl9YahvP",
                "forum": "EvDeiLv7qc",
                "replyto": "xkTusJaMir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3621/Reviewer_EZ7y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3621/Reviewer_EZ7y"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their responses. I am happy with the contributions of the paper and will keep the rating of 8."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667755337,
                "cdate": 1700667755337,
                "tmdate": 1700667755337,
                "mdate": 1700667755337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f4nrMNYF6j",
            "forum": "EvDeiLv7qc",
            "replyto": "EvDeiLv7qc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3621/Reviewer_s33v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3621/Reviewer_s33v"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses how to leverage MoEs for instruction fine-tuning and presents MoV and MoLORA. The use of MoE in fine-tuning leverages the fact that conditional computation is efficient while avoiding the disadvantage that MoE requires relatively large storage by combining MoE with (IA)^3 and LoRa. In-depth experimental studies demonstrate that the proposed method achieves competitive results with full fine-tuning, only 1% of the parameters involved, and no limitation to the model scale, and can also adapt the model to unseen tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tLeverage the advantages and avoid the disadvantages of the MoE structure when combined with PEFT methods.\n2.\tIn-depth ablation study demonstrates the capabilities and scalabilities of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tMoE takes advantage of the inherent heterogeneity of the data, allowing different parameters to handle different distributions in the data. However, fine-tuning usually focuses on one or a few downstream tasks. In this case, the motivation for using ten or even dozens of experts for learning requires further justification.\n2.\tLoRa exploits the inherent low-rank properties of large models. However, multiple parallel LoRa matrices are mathematically equivalent to higher rank LoRa. The effectiveness of MoLoRa proposed in this article cannot be proven without comparison with higher-rank lora."
                },
                "questions": {
                    "value": "The article mentioned that the larger the batch size, the easier it is for MoE to collapse to an expert. However, the difference between fine-tuning and pretrain is that the model has entered a relatively stable and better-performing landscape before training begins. Why under such conditions, the more common gradient direction brought by a larger batchsize will still cause a certain expert to dominate the gradient descent direction of the parameter space?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3621/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3621/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3621/Reviewer_s33v"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3621/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848565863,
            "cdate": 1698848565863,
            "tmdate": 1700779286161,
            "mdate": 1700779286161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xrN2kexsWX",
                "forum": "EvDeiLv7qc",
                "replyto": "f4nrMNYF6j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3621/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3621/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank **R s33v** for their positive feedback and for highlighting the \u201cin-depth ablation study\u201d that shows the \u201ccapabilities and scalabilities'' of our proposed method of combining PEFT with MoEs. We also express gratitude towards them for emphasizing positively that our approach, \u201cleverage the advantages and avoid the disadvantages of the MoE structure when combined with PEFT methods.\u201d We address some of the individual concerns below:\n\n> MoE takes advantage of the inherent heterogeneity of the data, allowing different parameters to handle different distributions in the data. However, fine-tuning usually focuses on one or a few downstream tasks. In this case, the motivation for using ten or even dozens of experts for learning requires further justification.\n\n**R s33v** is indeed correct that if there were only one or a few downstream tasks, we wouldn\u2019t use as many experts. However, we are interested in the multi-task fine-tuning setting that has led to recent breakthroughs in NLP [1], where the fine-tuning dataset is deliberately structured to represent a **diverse and large** set of tasks, with a high degree of heterogeneity in the data. Specifically in our experimental setup, the P3 dataset we use for fine-tuning **contains 62 datasets, each containing data points with 5 to 20 unique prompt templates, where there are 8 distinct tasks** such as paraphrasing, QA closed book, QA extractive, QA multiple-choice, sentiment, summarization, topic classification, and word-sense disambiguation. Hence, multiple experts are more appropriate here to gain from the benefits of modular specialization given the diversity in the number of datasets, prompts, and tasks.\n\n> The article mentioned that the larger the batch size, the easier it is for MoE to collapse to an expert. However, the difference between fine-tuning and pretrain is that the model has entered a relatively stable and better-performing landscape before training begins. Why under such conditions, the more common gradient direction brought by a larger batch size will still cause a certain expert to dominate the gradient descent direction of the parameter space?\n\nWe clarify to **R s33v** that our setting differs from \u201ctraditional\u201d finetuning where all weights have already been calibrated during pretraining. Even though the model is indeed pre-trained, both the router layers and PEFT experts are **randomly initialized before finetuning** which increases instability during finetuning relative to updating all pre-trained weights.\n\nOur finding that larger batch sizes lead to the learning collapse in MoE is consistent with other findings that MoE models benefit from smaller batch sizes due to the instability of training and fine-tuning MoE-style architectures using large batch sizes [2, 3]. We highlight this finding to contribute to the understanding of limitations for MoE approaches. \n\n[1] Sanh, Victor, et al. \u201cMultitask Prompted Training Enables Zero-Shot Task Generalization\u201d arXiv preprint arXiv:2110.08207 (2022).\n[2] Zoph, Barret, et al. \u201cST-MoE: Designing Stable and Transferable Sparse Expert Models.\u201d arXiv preprint arXiv:2202.08906 (2022).\n[3] Shen, Sheng, et al. \"Mixture-of-experts meets instruction tuning: A winning combination for large language models.\" arXiv preprint arXiv:2305.14705 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3621/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092437065,
                "cdate": 1700092437065,
                "tmdate": 1700092437065,
                "mdate": 1700092437065,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]