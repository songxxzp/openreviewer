[
    {
        "title": "Are LLMs Aware that Some Questions are not Open-ended?"
    },
    {
        "review": {
            "id": "VMRvFOYHsk",
            "forum": "YIWe2amtrV",
            "replyto": "YIWe2amtrV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7732/Reviewer_BHia"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7732/Reviewer_BHia"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a temperature sampling FT technique (QAT) to improve the performance of chat-oriented LLMs on open-ended questions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper has the following strengths:\n\n1. QAT is a fairly simple procedure that could have benefits beyond just open-endedness of questions posed to the chatbot.\n2. QAT shows improvements over non-QAT in a variety of LLMs (Table 1)"
                },
                "weaknesses": {
                    "value": "The paper has several weaknesses.\n\n1. The labelling of open-endedness itself comes from GPT-4, thus invalidating one of the core propositions that LLMs are not very good at identifiying open-ended questions.\n\n2. The prompt used to GPT classifies is simplistic and would result in classification based on the overall topic and appearance rather than factual open-endedness. As an example, consider a question that is open-ended by seemingly about science (eg. concerned with origin of life or questions about the universe). These will likely be classified as Highly Accurate by the prompt.\n\n3. Since the final evaluation is also done via GPT-4 it collapses the evaluation function with the labelling function."
                },
                "questions": {
                    "value": "1. Can human evaluation be used for at least a subset of the queries to separate the evaluation function from the labelling function?\n2. Can the authors measure the efficacy of their prompt?\n3. Typo: s/hiddent/hidden/ in Algorithm 1"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7732/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7732/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7732/Reviewer_BHia"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697494296276,
            "cdate": 1697494296276,
            "tmdate": 1699636943354,
            "mdate": 1699636943354,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "PY4amGqsFi",
            "forum": "YIWe2amtrV",
            "replyto": "YIWe2amtrV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7732/Reviewer_iU6n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7732/Reviewer_iU6n"
            ],
            "content": {
                "summary": {
                    "value": "This paper evaluates the \u201cquestion awareness\u201d ability of large language models, i.e., whether the model can realize when a deterministic answer to a question is needed. The paper also proposes Question Awareness Temperature (QAT) sampling method, which includes a continual finetuning phase and a temperature tuning inference phase. QAT has shown some improvements against automatic evaluation metrics, but no human evaluation is done yet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper calls attention to the ability of question awareness, which is useful in detecting when \u201challucination\u201d or creativity is needed, and when factuality is more important when answering user queries.\n- This paper use average Kurtosis as a metric to evaluate the \u201cdeterminacy\u201d of the answer. I am not fully convinced of whether this metric is reliable and whether it truly reflects determinacy, but it is an interesting choice.\n- The paper introduced QAT, an adaptive way to tune the temperature parameter to adjust the output distribution."
                },
                "weaknesses": {
                    "value": "- Better definition of question awareness/determinacy: Why does average kurtosis reflect determinacy? Is determinacy equivalent to \u201ccertainty\u201d of the model? How do these two concepts link together? I am not convinced by the claim in section 2.4 that LLMs have fundamental question awareness on some scenarios, maybe these tasks are indirectly or directly presented in their training data, and thus not necessarily mean that they know that they need to choose more deterministically.\n- QAT has a phase of continual fine-tuning prior to the temperature tuning phase, so I would love to see a comparison with baselines like simply tuning the temperature by hand. How does the cost differ? Which one performs better? Without a comprehensive comparison, and human evaluations for open-ended writing tasks, I don\u2019t know whether QAT truly improves the generation."
                },
                "questions": {
                    "value": "- To clarify, in section 4.3, TRU/SMW/WDK are from ShareGPT dataset, right? Is there any additional effort done by the authors regarding definition of \u201chard\u201d questions about commonsense or \u201cdiverse\u201d in SMW questions?\n- What sampling method is used for section 2? Multiple parameters like temperature can be tuned.\n- Llama 2 is validated on GSM8K, do you think this might be affecting the average Kurtosis number? Is there a reason we choose to include GSM8K?\n- In section 4.3, \u201cQuestion Awareness Evaluation Dataset\u201d, open-ended tasks should be CCR/DSC/SUG and non-open-ended ones should be TRU/SMW/WDK right?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7732/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7732/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7732/Reviewer_iU6n"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698487299991,
            "cdate": 1698487299991,
            "tmdate": 1699636943217,
            "mdate": 1699636943217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "iLeBsbOzwl",
            "forum": "YIWe2amtrV",
            "replyto": "YIWe2amtrV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7732/Reviewer_NsHB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7732/Reviewer_NsHB"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the exploration of large language models (LLMs) and their sensitivity to questions with potential societal implications. Specifically, it investigates whether these models recognize the potential harm and ethical considerations associated with certain questions and, if so, how they address such queries. By establishing a set of criteria and benchmarks, the authors systematically assess the responses of LLMs. The research illuminates the intricate balance between providing information and navigating societal and ethical boundaries, offering insights for the design and evaluation of future LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tNovel Focus: The paper tackles the \"question awareness\" in LLMs, exploring their ability to discern between open-ended and non-open-ended questions.\n\n2.\tMethodological Contribution: The introduction of the Question Awareness Temperature (QAT) sampling method is novel."
                },
                "weaknesses": {
                    "value": "1. The experimental setting is not clear. \n\n2. Some benchmarks are missing for evaluation.\n\n3. Some important implementation details are missing."
                },
                "questions": {
                    "value": "1. Experiments: Is the baseline continuously pre-trained on the 3.5k data?  It seems authors directly compared with llama, not continue-pre-trained llama. \n\n2. Benchmark Limitation: The evaluation should include some factual QA benchmarks.\n\n3. Missing Details in the Paper:\n -- The prompts used in GPT-4 to evaluate the outputs.\n -- The criteria for using GPT-4 to assess the outputs.\n -- The absence of a human study on open-ended generation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698883140581,
            "cdate": 1698883140581,
            "tmdate": 1699636943096,
            "mdate": 1699636943096,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]