[
    {
        "title": "Analysis of Task Transferability in Large Pre-trained Classifiers"
    },
    {
        "review": {
            "id": "xrVkwgmccb",
            "forum": "HCMmC8DETj",
            "replyto": "HCMmC8DETj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1465/Reviewer_nX5m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1465/Reviewer_nX5m"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a theoretical framework for understanding transfer learning. The authors decompose the transfer learning loss into three parts, namely the losses incurred due to the prior transform, feature transform, and distribution mismatch. Building on top of this decomposition, the authors provide a loss upper bound on the transfer learning, and empirically demonstrate that this bound is close to what happens in the real world."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of decomposing the transfer learning loss into the three parts is novel and interesting.\n- The paper is well-written with the motivation for each part of the decomposition clearly stated.\n-  The empirical verification of the bound is nice."
                },
                "weaknesses": {
                    "value": "- The current framework doesn\u2019t incorporate the effect of network architecture, which seems to be quite important in the real world.\n- It\u2019s unclear what\u2019s something that is constructive out of this analysis. The authors don\u2019t provide any suggestion on how to improve the transfer learning performance based on the current analysis framework.\n- It would be good to provide some outline of the key steps of the theoretical derivation. Given that this is mainly a theory paper, it would be good to highlight the most non-trivial / novel part of the analysis."
                },
                "questions": {
                    "value": "- How do you think of the relative importance of these different types of mismatch? Is there any way to directly verify them in the real world?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1465/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698465418198,
            "cdate": 1698465418198,
            "tmdate": 1699636075491,
            "mdate": 1699636075491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "F5Xus2vEQe",
            "forum": "HCMmC8DETj",
            "replyto": "HCMmC8DETj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1465/Reviewer_Zr88"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1465/Reviewer_Zr88"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the domain of transfer learning, specifically the relationship between performance on a source task and its consequent performance on a target task after linear fine-tuning. The authors aim to understand the transferability of large pre-trained models when applied to downstream classification tasks. Their claimed contributions include:\n\n1. Introduction of the **Task Transfer Analysis** approach, which alters the distribution and classifier of the source task to mirror those of the target task.\n2. Proposal of an **upper bound on transferability**. This is a composite measure derived from:\n   - The conditional entropy between the label distributions of the two tasks.\n   - The weighted loss of the source classifier on the source task.\n   - The Wasserstein distance between the modified source and the target distributions.\n3. Formulation of an **optimization problem** designed to minimize the aforementioned bound to determine transferability.\n\nThe empirical results, obtained from top-tier pre-trained models, suggest that this proposed upper bound can provide a reasonable estimate of transferability across various datasets. A key takeaway from the study is : the more seamlessly the source task can be modified to match the target, the better the transferability.\n\nFuture research, as suggested by the authors, may involve extending this analysis to encompass full fine-tuning and exploring alternate loss functions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The work introduces a new \"Task Transfer Analysis\" approach that aims to transform both the distribution and the classifier of the source task to resemble those of the target task.\n\n2. The paper proposes an upper bound on transferability, allowing for a more quantifiable understanding of how one task relates to another in the context of transfer learning. This upper bound considers the Wasserstein distance, conditional entropy, and the weighted loss, offering a multi-faceted perspective on transferability.\n\n3. The authors further introduce an optimization problem designed to minimize the proposed bound, providing a practical tool or method for estimating transferability.\n\n4. The paper covers many datasets and pre-trained models. This broad coverage enhances the generalizability of the findings and ensures they're applicable across diverse contexts."
                },
                "weaknesses": {
                    "value": "1. **Scope Limitation**: A glaring limitation of this study lies in its exclusive focus on linear fine-tuning. The practical implications of linear fine-tuning in transfer learning are limited. If the overarching goal is to gauge performance, why not simply engage in the linear fine-tuning itself? When distilled to its essence, with features in hand, linear fine-tuning is fundamentally a softmax regression challenge\u2014a territory that has been well-studied in literature with efficient second-order solutions at its disposal. The potential value of this paper could have been augmented significantly had it considered a more comprehensive exploration of general fine-tuning, or what the authors label as \"full fine-tuning\".\n\n2. **Insufficient Comparative Analysis**: The paper exhibits a noticeable deficiency in terms of benchmarking against established literature. While there's frequent phrases against score-based transferability estimation, the main content conspicuously lacks any substantive comparative evaluations. The only claim that their approach offers insights unattainable through SbTE methods feels unsubstantiated, especially considering existing studies such as \"Representation similarity analysis for efficient task taxonomy & transfer learning\" and \"Duality Diagram Similarity: A Generic Framework for Initialization Selection in Task Transfer Learning\". These studies have delved deep into elucidating transferability using both source and target tasks, casting doubt on the novelty of the authors' task transfer analysis. (The authors cited these two papers, but failed to understand the methods.)\n\n3. **Inefficiency and Instability Concerns**: Turning our attention to Figure 2, it is evident that the chosen subset of source data can have significant influence over the upper bound. Given that state-of-the-art pre-trained models often draw upon vast datasets of pre-training images, often running into millions, the method proposed by the authors appears neither resource-efficient nor consistently stable. Such a scalability challenge hinders the broader applicability of their approach in real-world scenarios where data volume is massive.\n\nIn summary, while the paper takes an interesting approach to task transferability analysis, it seems to be weak by its limited scope, a lack of rigorous comparative assessment, and potential challenges in scalability and stability. Expanding the study's scope to encompass broader fine-tuning techniques and integrating more robust comparative metrics would substantially improve its academic and practical significance."
                },
                "questions": {
                    "value": "1. **Regarding Figures 2 and 4:**\n   - In both Figure 2 and Figure 4, the legends indicate the presence of three components: \"Reweighted Source Loss\", \"Label Mismatch\", and \"Distribution Mismatch\". However, upon inspection, there seems to be no visible representation for \"Label Mismatch\", i.e. the orange segment. Can the authors clarify this discrepancy?\n\n2. **Concerning Figure 4:**\n   - It is noticeable that nearly all the models in Figure 4 exhibit almost identical target losses. This is counterintuitive given the presumption that models with varying architectures and training methods should manifest disparate downstream transfer performances. Could the authors provide an explanation or rationale behind this observation?\n\n3. **On the Paper's Textual Content:**\n   - There appears to be a repetitive phrasing in the sentence, \"Our approach works by transforming the source distribution (and the classifier of the source task) **by transforming**.\" The consecutive use of \"by transforming\" seems redundant. Is it a typo?\n\n4. **Regarding Terminology:**\n   - The chosen abbreviation for \"score-based transferability estimation\" seems unconventional. Could the authors justify this choice or consider a more intuitive abbreviation?\n\nWhile the paper presents valuable insights on task transferability, it would benefit from addressing the above queries to ensure clarity and coherence for readers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1465/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698494932977,
            "cdate": 1698494932977,
            "tmdate": 1699636075417,
            "mdate": 1699636075417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "UkrJ7wLBeq",
            "forum": "HCMmC8DETj",
            "replyto": "HCMmC8DETj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1465/Reviewer_2KtK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1465/Reviewer_2KtK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to estimate the transferability of a pre-trained classifier for a given task without directly conducting transfer learning. Specifically, it derives an upper bound on the observed loss of the model after linear fine-tuning by solving the proposed optimization problem. This optimization problem includes prior transformation, label transformation, and feature transformation, incorporating a term that accounts for the mismatch between the data distribution after applying their combined transformations and the target distribution. The authors demonstrate the proximity of the proposed upper bound to the target loss through simple experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is overall clearly clarified and well organized.\n2. This paper provides both theoretical and empirical analyses."
                },
                "weaknesses": {
                    "value": "1. The practicality of the proposed method is limited:\n - Only linear fine-tuning is considered.\n - For the proposed method, not only pre-trained models but also supervised pre-training data are required.\n - When the pre-training dataset is not clearly defined in terms of the classification task (such as the vision-language pre-training dataset), the application of the proposed method is not straightforward.\n2. The proposed method is not robust. In other words, depending on the selection of source classes, significantly different estimations of transferability can be made for the same pre-trained model. For instance, as demonstrated in Section 4.2, considering a higher number of source classes than downstream classes leads to an increase in the estimated loss upper bound. In addition, sampling challenging classes among the source classes leads to a higher estimated upper bound."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1465/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1465/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1465/Reviewer_2KtK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1465/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675965381,
            "cdate": 1698675965381,
            "tmdate": 1699636075274,
            "mdate": 1699636075274,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "61K6rNZzzc",
            "forum": "HCMmC8DETj",
            "replyto": "HCMmC8DETj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1465/Reviewer_GYU6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1465/Reviewer_GYU6"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the transferability of pre-trained models on downstream classification tasks after linear fine-tuning. It transforms the distribution (and classifier) of the source task to produce a new distribution (and classifier) similar to that of the target task. Based on this, it proposes an upper bound on transferability composed of the Wasserstein distance between the transformed source and the target distributions, the conditional entropy between the label distributions of the two tasks, and the weighted loss of the source classifier on the source task. It then proposes an optimization problem that minimizes the proposed bound to estimate transferability. Experiments on some image or sentence classification tasks with different pre-trained models show that the proposed upper bound can estimate the performance on the target tasks after linear fine-tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: This idea of considering relatedness between the source and target tasks for transferability makes sense. The general framework designs of prior transformation, label transformation, feature transformation and distribution matching seem reasonable. \n\nS2: The method is evaluated on various pre-trained models on both image and sentence classification tasks."
                },
                "weaknesses": {
                    "value": "W1: The aim of the paper seems to estimate the bound of target performance after learning a linear classifier with a fixed pre-trained network, and the estimation needs source data, target data and the pre-trained model. I have some concerns about the problem setting. (1) Why don\u2019t we directly train a linear classifier with the target data to get the performance (since we already have target data and training a linear classifier does not have a high cost)? What is the point of estimating the bound? (2) The requirement of source data prevents practical usage of this setting, since many pre-trained models do not provide their original training data with them. (3) This paper focuses on a fixed pre-trained model, which still has a gap between more common practices such as fine-tuning or parameter-efficient fine-tuning. (4) The problem here can only be used in classification tasks. \n\nW2: The general estimation framework makes sense to me, but the technical designs of each step are not novel or insightful enough to me. The designs of prior transform, label transform, and feature transform are straightforward, and the Wasserstein distance has already been well studied in related topics such as distribution shifts and domain adaptation. \n\nW3: The experimental part is hard to follow in some ways. (1) Why should we select the classes from the source label set for estimation instead of using the whole label set? If the label or semantic shift matters for transferability, why do random selections and semantic selections have similar results? (2) The method contains four steps of prior transformation, label transformation, feature transformation, and distribution matching, but there are no direct ablation studies to evaluate these four parts. (3) In Section 4.4, ImageNet is used as the source task, but some of the pre-trained models, such as CLIP, are not pre-trained with ImageNet. (4) In Figures 2 and 4, there are no bars corresponding to \u2018Label Mismatch\u2019. (5) It is claimed that the estimation of the proposed method has a small gap with the target loss. However, there are no other reference results to compare with, so how can we define \u2018a small gap\u2019?"
                },
                "questions": {
                    "value": "Could the method in this paper provide more accurate model selection results compared with existing score-based transferability estimation (SbTE) methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1465/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1465/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1465/Reviewer_GYU6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1465/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699183725914,
            "cdate": 1699183725914,
            "tmdate": 1699636075202,
            "mdate": 1699636075202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]