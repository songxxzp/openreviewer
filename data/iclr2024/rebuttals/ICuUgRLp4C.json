[
    {
        "title": "Learning High-Order Relationships of Brain Regions"
    },
    {
        "review": {
            "id": "2jCzZZg8SX",
            "forum": "ICuUgRLp4C",
            "replyto": "ICuUgRLp4C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8333/Reviewer_Z7u6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8333/Reviewer_Z7u6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method named HYBRID for extracting maximally informative and minimally redundant high-order relationships from fMRI data. The authors argue that most current methods fail to accurately characterize interactions among brain regions because they only focus on pairwise connections and overlook high-order relationships. HYBRID addresses this limitation by constructing a hypergraph where hyperedges represent high-order relationships and their weights represent the strengths of those relationships. The authors demonstrate the effectiveness of HYBRID through comprehensive experiments, outperforming the state-of-the-art predictive model by an average of 12.1%. The contributions of this paper include a novel method for extracting high-order relationships from fMRI data and a comprehensive evaluation of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The paper proposes a novel method named HYBRID for extracting maximally informative and minimally redundant high-order relationships from fMRI data. This is a significant contribution as most current methods focus on pairwise connections and overlook high-order relationships. HYBRID addresses this limitation by constructing a hypergraph where hyperedges represent high-order relationships and their weights represent the strengths of those relationships. The proposed method is original and creative, and the authors provide a comprehensive evaluation of the proposed method, demonstrating its effectiveness through experiments.\n\nQuality & clarity: The paper presents a clear problem formulation, a detailed description of the proposed method, and a comprehensive evaluation of the proposed method. The authors provide theoretical guarantees for the proposed method and demonstrate its effectiveness through experiments. The paper is well-written, with clear and concise language, making it easy to understand.\n\nSignificance: The paper addresses an important problem in neuroscience and machine learning. Discovering reliable and informative interactions among brain regions from fMRI signals is essential in neuroscientific predictions of cognition. Most of the current methods fail to accurately characterize those interactions because they only focus on pairwise connections and overlook the high-order relationships of brain regions. The proposed method addresses this limitation and provides a new approach for extracting high-order relationships from fMRI data."
                },
                "weaknesses": {
                    "value": "The matrix notation in SEction 4 is confusing:\nSection 4.1, first line: $X = [X_1, X_2, \\dots, X_N]$ should be $X = [X_1, X_2, \\dots, X_N]^T$ (i.e., a transpose operator should be inserted).\nEq. (4): a transpose operator should be inserted after the square brackets."
                },
                "questions": {
                    "value": "Can you briefly explain how to choose the number of hyperedges?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762057860,
            "cdate": 1698762057860,
            "tmdate": 1699637036634,
            "mdate": 1699637036634,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oBqCiLmtvL",
                "forum": "ICuUgRLp4C",
                "replyto": "2jCzZZg8SX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Authors"
                    },
                    "comment": {
                        "value": "We would like to address your concerns and questions below\n## Weakness\n\n### C1\n\nA transpose operator should be inserted.\n\n### RC1\n\nThanks for pointing this out and sorry for the confusion caused by the notation. We have inserted the transpose operator and gone through all the notations to make them more consistent and clear.\n\n### Questions\n### Q1\n\nCan you briefly explain how to choose the number of hyperedges?\n\n### RQ1\n\nSure! As stated in Section 4.1 (hyperedges construction), the number of hyperedges $K$ is a pre-defined hyperparameter. We choose $K=32$ based on the discussion and ablation studies that have been included in Appendix E in the submission. Basically, the performances improve as the $K$ goes larger, and becomes saturated after $K=32$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700026872832,
                "cdate": 1700026872832,
                "tmdate": 1700026872832,
                "mdate": 1700026872832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qcLMyTGP44",
            "forum": "ICuUgRLp4C",
            "replyto": "ICuUgRLp4C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8333/Reviewer_yeBt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8333/Reviewer_yeBt"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a principle to learn high-order relationships of brain regions -- high-order relationships should be maximally informative and minimally redundant (MIMR), and a method called Hypergraph of Brain Regions via multi-head Drop- bottleneck (HyBRiD) to learn such relationships from fMRI data. HyBRiD includes a constructor to identify hyperedge structures, and a weighter to compute a weight for each hyperedge. The results show that HyBRiD outperformed 8 baseline methods in 7 out of 8 fMRI datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well written with clear organization, detailed theoretical explanation, and comprehensive empirical evaluation. \n\n2. The proposed method is intuitively simple yet effective, and could be potentially applied to learn high-order relationships of brain regions with respect to different prediction targets. \n\n3. The neuroimaging experiments were comprehensive. A large sample size (8 datasets with 11875 subjects) was used to evaluate the models. The proposed method HyBRiD was compared to 3 types of baseline methods including 8 methods. HyBRiD outperformed 8 baseline methods in 7 out of 8 datasets. \n\n4. The hyperedge profile analysis indicates interactions of multiple brain regions are more important in cognition tasks. The region importance reveals reasonable task-related brain regions under different conditions."
                },
                "weaknesses": {
                    "value": "1. The authors mentioned that \"Due to the data scarcity, training on individual datasets would result in serious overfitting.\" Each individual dataset includes at least 1000 subjects but the model performance is still not ideal. Can the model be applied to a dataset with less samples? Most clinical datasets are relatively small with < 1000 subjects. Is it applicable to apply the model on datasets with fewer samples? \n\n2. It would be helpful if the authors could discuss potential reasons why HyBRiD failed at Rest 1 dataset. \n\n3. Region importance. What about region importance for resting state data? If I understand it correctly, region importance is a metric for nodes. What about edges? Can you show how edges are connected under different conditions? \n\n4. I appreciate that the authors include the code in supplemental material, but a README file should be also included to explain how to replicate the results. \n\n5. The notations in Section 5.1 Metric are not very clear to me. \n\n\ti. The input of CPM should include the prediction target Y, right? Maybe use CPM(E, Y)? \n\n\tii. What is the dimension of E? Is E equivalent to H+$\\mathbf{w}$ in HyBRiD? \n\n\tiii. CPM is evaluated on each model separately, right? If so, I think Eq. 13 should be defined separately for HyBRiD.\n\n6. Minor: \n\n\ti. Typos: Section 2.1 \"Inupt\" should be \"Input\"; Section 5.2 \"conducte\" should be \"conduct\"; Figure 7: \"grpahical\" should be \"graphical\".\n\n\tii. CPM should be defined in the abstract and introduction upon its first occurrence.\n\n\tiii. In Section 5.1 Dataset, RS (resting state) should be defined.\n\n\tiv. In Table 5, $\\beta = 0.2$ instead of $0.3$ to be consistent with Section E.3?"
                },
                "questions": {
                    "value": "1. Does the model generalize well across conditions or out-of-sample data? For example, if a model is trained on resting state data, can it be applied to predict task data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8333/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8333/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8333/Reviewer_yeBt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812251391,
            "cdate": 1698812251391,
            "tmdate": 1699637036493,
            "mdate": 1699637036493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zY2rwnLT1o",
                "forum": "ICuUgRLp4C",
                "replyto": "qcLMyTGP44",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Authors - Major"
                    },
                    "comment": {
                        "value": "We would like to address your major concerns in this thread.\n\n### C1\n\nThe reviewer wondered if the model could be applied to a dataset with fewer samples. \n\n### RC1\n\nWhile we do think training on a small dataset is an interesting question, we argue that both our model and the state-of-the-art one are unable to solve it, and finding out the solution to such a problem is out of the scope.\n\nTo show it, we conducted experiments of training on individual datasets for both our model and BrainNetTF. The $r$ values:\n\n|            | SST 1  | EN-back 1 | MID 1 | Rest 1 |\n| ---------- | ------ | --------- | ----- | ------ |\n| BrainNetTF | -0.029 | 0.091     | 0.075 | 0.081  |\n| HyBRiD     | 0.103  | 0.085     | 0.002 | 0.060  |\n\nThe $P$-values corresponds to these $r$ values:\n\n|            | SST 1 | EN-back 1 | MID 1 | Rest 1 |\n| ---------- | ----- | --------- | ----- | ------ |\n| BrainNetTF | 0.650 | 0.152     | 0.235 | 0.204  |\n| HyBRiD     | 0.105 | 0.178     | 0.975 | 0.341  |\n\nFrom this table, we can see all of the $P$-values> 0.05. Therefore, none of the existing models obtain **meaningful** results in this situation.\n\nHowever, our work implies that if you have small fMRI datasets, you can usually **combine** them for a better result. We hope this could be a good solution to the small datasets issue.\n\n### C2\n\nShould discuss potential reasons why HyBRiD failed at Rest 1 dataset.\n\n### RC2\n\nThanks for your suggestion. We inspect the performance on both the training, validation and test datasets. We summarize the performances of our model and the SOTA one in the table below\n\n| model      | train | validation | test  |\n| ---------- | ----- | ---------- | ----- |\n| BrainNetTF | 0.967 | 0.484      | 0.334 |\n| HyBRiD     | 0.984 | 0.491      | 0.223 |\n\nWe find our model outperforms BrainNetTF on both training and validation datasets but fails on test datasets. Therefore, given that Rest 1 is the **most noisy** dataset, we argue that the failure is because\n\n1. Our model is more **expressive** than BrainNetTF, so it *overfits* the training dataset more than BrainNetTF.\n2. The validation score is higher, indicating the failure reason is the **discrepancy** between the training dataset and the test dataset.\n\nAdditionally, Note that although HyBRiD is not the best on Rest 1 dataset, it is still the **second-best** one.\n\n### C3\n\nThe reviewer would like to see region importance for resting state and individual hyperedges.\n\n### RC3\n\nWe did not visualize it in the original submission because different from task states, where specific brain regions are activated in response to particular tasks, brain activities during resting states, are not driven by external tasks, leading to more **diffuse and less predictable** patterns of activation. This makes it harder to interpret. However, we appreciate the reviewer\u2019s interest and we have included the visualizations and interpretations of *resting-state region importance* and *individual hyperedges* in **Appendix G** in the updated version. We would like to do more analysis and interpretations in the future.\n\n### Q1\n\nDoes the model generalize well across conditions or out-of-sample data? For example, if a model is trained on resting state data, can it be applied to predict task data?\n\n### RQ1\n\nThanks for your interest in the model's generalizability. In our experience, training on resting-state and generalizing it on task-state is difficult because resting-state data is more **noisy**. However, we do think it is a good idea so instead, we train our model on task-state data and generalize it one resting-state data.\n\n|        | Training on all states, predicting on resting-state | Training on task-states, predicting on resting-state |\n| ------ | --------------------------------------------------- | ---------------------------------------------------- |\n| Rest 1 | 0.223                                               | 0.253                                                |\n| Rest 2 | 0.730                                               | 0.475                                                |\n\nThe tables illustrate that our model demonstrates **a certain level of generalization**. For Rest 1, it is even better than training on Rest 1. We argue that it is because the model easily overfits on Rest 1 dataset, which is consistent with our response for **RC2.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700027701078,
                "cdate": 1700027701078,
                "tmdate": 1700027803296,
                "mdate": 1700027803296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a7cai2oKPy",
                "forum": "ICuUgRLp4C",
                "replyto": "qcLMyTGP44",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official comments by Authors - Minor"
                    },
                    "comment": {
                        "value": "We would like to address your minor concerns in this thread.\n\n### C4\n\nAdd a README file to explain how to replicate the results.\n\n### RC4\n\nThanks for the suggestion! We created an anonymous Github link https://anonymous.4open.science/r/HyBRiD-A7CC. Note that the code may be a little bit messy because we haven\u2019t had time to organize and refactor it. We will do this as soon as possible in the future.\n\n### C5\n\nNotations in Section 5.1 are not very clear.\n\n### RC5\n\ni) I agree that CPM(E, Y) is more accurate and Eq. 13 has been updated.\n\nii) $E$ denotes traditional pairwise edge weights, and it is a vector of length $K_p$, which is the total number of pairwise edges. We made it clearer by changing the notation $E$ to $\\boldsymbol{W}_p$.\n\niii) We made it clearer by changing $\\boldsymbol{w}$ to  $\\boldsymbol{w}_h$ for Eq.13. And made $\\boldsymbol{w}_h = \\boldsymbol{w}$ as a special case when evaluating on our model. \n\nYou can check Eq.12 and Eq.13 in the updated version.\n\n### C6\n\ni. Typos: Section 2.1 \"Inupt\" should be \"Input\"; Section 5.2 \"conducte\" should be \"conduct\"; Figure 7: \"grpahical\" should be \"graphical\".\n\nii. CPM should be defined in the abstract and introduction upon its first occurrence.\n\niii. In Section 5.1 Dataset, RS (resting state) should be defined.\n\niv. In Table 5, $\\beta=0.2$ instead of 0.3 to be consistent with Section E.3? \n\n### RC6\n\ni) Thanks for pointing out. Typo fixed.\n\nii) Thanks for the suggestion. We acknowledge the importance of defining 'CPM' upon its first appearance in the text. However, due to the constraints of the abstract and introduction sections, providing a full definition is challenging. We have, therefore, incorporated a concise explanation, stating that CPM is the evaluation protocol for our method, which gives readers an adequate initial background. In addition, we also refer to the appendix. We have updated:\n\nIn abstract\n\n> Our model outperforms the state-of-the-art predictive model by an average of 12.1%, regarding the quality of hyperedges measured by CPM, a standard protocol for studying brain connections.\n\nIn introduction\n\n> We quantitatively evaluate our approach by a commonly used protocol\n>\n>\n> for studying brain connections, CPM (Shen et al., 2017) (Appendix A), and show that\u2026\n\niii) Sorry for the confusion. We have modified it to\n\n> the resting state where the brain is not engaged in any activity (Rest)\n\niv) Thanks for pointing out. Typo fixed."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700027732316,
                "cdate": 1700027732316,
                "tmdate": 1700027791341,
                "mdate": 1700027791341,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1BoUFvOO3j",
            "forum": "ICuUgRLp4C",
            "replyto": "ICuUgRLp4C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8333/Reviewer_eSJE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8333/Reviewer_eSJE"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts at capturing multivariate relationships among a set or random variables as would be captured by edges in a hypergraph representation. For that, the paper constructs a differentiable regression model that first applies a set of learnable square (for the number of regions) linear projection with subsequent thresholding of the output - a mask, then applies an MLP to compute a scalar value (weight) for each of the masked subsets of the nodes. The weights are used to produce a scalar value (after an inner product with the output weight vector). This model is trained in a regularized regression manner and the produces features are evaluated in an acceptable feature selection evaluation pipeline using predictive strengths of the features as final evaluations. The approach is applied to a subset of the ABCD dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper contains an interesting approach to model building, where an encoder builds clustering - a representation interpretable to human experts. A glass-layer with the partition clearly visible. Potentially, a rewrite of the paper could focus on this part, instead of the unsubstantiated claims about capturing high-order interations"
                },
                "weaknesses": {
                    "value": "1.  **The positioning of the paper is a problem.** The assertion that it captures high-order interaction is not substantiated, even though the feature selection model's entire motivation hinges on this claim. Certainly, the title emphasizes high-order interaction. However, the exact type of high-order interaction that the proposed model captures remains ambiguous. I would suggest considering the following papers, which were mistakenly overlooked. These papers seek to formally define what is being captured before attempting to estimate the interactions:\n    -   Rosas FE, Mediano PA, Gastpar M, Jensen HJ. [Quantifying high-order interdependencies via multivariate extensions of the mutual information](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.100.032305). Physical Review E. 2019 Sep 13;100(3):032305.\n    -   Varley TF, Pope M, Faskowitz J, Sporns O. [Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex](https://www.nature.com/articles/s42003-023-04843-w). Communications biology. 2023 Apr 24;6(1):451.\n    -   Santoro A, Battiston F, Petri G, Amico E. [Higher-order organization of multivariate time series](https://www.nature.com/articles/s41567-022-01852-0). Nature Physics. 2023 Feb;19(2):221-9.\n2.  **The clarity of the writing, with regard to the implementation**, is also inadequate in other sections. If we are estimating a hypergraph, then the edges, or node clusters, should form a cover rather than a partition. However, the regularization of the Mean Squared Error (MSE) used in Equation 11, as well as a preceding statement, both confirm the need for the edges to be disjoint, thereby suggesting a partition. This leads us back to the issue of positioning as it means that what is proposed is a clustering algorithm or the task of finding a partition. It's worth noting that node partitioning can still be conducted to recover high-order interactions, as has been exemplified in the neural imaging context, for instance, here:\n    -   Plis SM, Sui J, Lane T, Roy S, Clark VP, Potluru VK, Huster RJ, Michael A, Sponheim SR, Weisend MP, Calhoun VD. [High-order interactions observed in multi-task intrinsic networks are dominant indicators of aberrant brain function in schizophrenia.](https://www.sciencedirect.com/science/article/pii/S1053811913007970) NeuroImage. 2014 Nov 15;102:35-48.\n3.  Overall, **the approach feels like an ad hoc method** for grouping the feature vectors via their predictive potential for a dependent variable. Even the input features are correlation coefficients. That is, the initial input matrix for each subject is the correlation matrix of which the goal is to subselect rows (or columns, which is equivalent due to symmetry) into K different groups.\n4.  **Comparisons in Table 1 are highly problematic** as well.\n    1.  If the goal is to find high-order relations what does predictive quality of representations has to do with it? Table 1 in my opinion does not belong in a paper on high-order relations.\n    2.  However, if the paper would be rewritten to focus on feature grouping and clustering, this approach may potentially work although not without changes. In this case, the proposed model needs to be compared with other approaches that do clustering or partition of random variables. For example, it seems appropriate to consider comparison with Deep Clustering.\n5.  **Results are confusing.** They do not show individual \"hyperedges\" and analyze the ROIs that have grouped together and explain high-order interactions that grouped them. If the regularization enforces the partition, why are so many clusters overlap per my interpretation of Figures 5 and 4c?"
                },
                "questions": {
                    "value": "Note, I do not think answering my question below can change the problems with the way it is written which lead to experiments not supporting the claims. My comments below are to help future clarity of the work:\n\n1.  The abstract states that CRM measures quality of hyperedges, however, CRM looks like a feature selection protocol that also has recommendations for assessing predictivity of the features. This is a disconnect between what is claimed and what is presented.\n2.  The beginning of the second paragraph of the Introduction needs references to support the claim. The last phrase of that paragraph needs a rewrite \"of the intricate behind brain regions\"\n3.  Section 2.1 \"Inupt\" - Do you mean Input?\n4.  Section 2.2 describes feature selection - why is this tied to high-order interactions. Confusing.\n5.  Section 4.1 what do you mean by \"N-dimensional shallow embedding layer parameterized by\". It would be best if all operations and parameters were clearly defined. I assume this is a linear transformation but that is a guess.\n6.  Hyperedge weighting. MLP is not a sufficient description of the used model. Please also mention the activation function.\n7.  Baselines: \"standard\" method mentioned there is unclear. What is standard? How is it defined?\n8.  Why the model is compared with arbitrary models that solve problems different from the proposed method? How were models shown in the comparison selected?\n9.  Ktena, Li, and Kan papers do not construct hypergraphs and yet used as such in the paper for comparisons. Confusing.\n10. Consider fixing capitalization in your bib file. To do that, you can go over the cited papers in your .bib file and put all words you want to preserve capitalization of in additional curly braces. Like {fMRI}."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698894068048,
            "cdate": 1698894068048,
            "tmdate": 1699637036373,
            "mdate": 1699637036373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WnPH6v3Xs3",
                "forum": "ICuUgRLp4C",
                "replyto": "1BoUFvOO3j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Authors - clarify misunderstandings"
                    },
                    "comment": {
                        "value": "We believe there is a fundamental misunderstanding between the reviewer and us, which will be described below. In addition, we have also updated the paper to avoid other potential confusion. We would like to resolve the misunderstanding in this thread.\n\n### C2\nThe proposed method learns a partition, rather than a cover.\n\n### RC2\nWe would like to politely point out that the claim is incorrect. Our method indeed learns a **cover**, rather than a partition.\n\nWe guess the reviewer obtained such a conclusion because the statement in the submission\n\n> The equality holds if and only if nodes are independent and hyperedges do not overlap\n\nHowever, this is a fundamental misunderstanding of our **optimization objective**. This sentence just stated the condition in which equality holds in this inequality.\n\nSpecifically, Eq.10 shows\n\n$$\nI(H; X) \\leq I_{\\text{upper}}(H;X)\n$$\n\nwhere $I_\\text{upper}(H;X)$ corresponds the right-hand-side of Eq.10. We optimize $I_\\text{upper}(H;X)$ as a surrogate objective of  $I(H;X)$. In other words, we minimize $I(H;X)$ through minimizing $I_\\text{upper}(H;X)$. This is a *common technique* in an abundance of works (e.g. in ELBO [1][2], in IB [3][4], and others [5][6]). Hence, the objective does not indicate partition.\n\nOn the contrary, if one wants to force the partition, he/she should optimize the **tightness** instead\n\n$$\nT(H, X) = I_{\\text{upper}}(H;X) - I(H; X)\n$$\n\nwhere $T(H,X)$ denotes the tightness and it is minimized when hyperedges do not overlap. And minimizing the tightness is not what we are doing in this work.\n\nFurthermore, the MSE term is **not for regularization**, it is the lower bound of informativeness in Eq.9, where $Y$ is the cognition score (defined in Section 2), which ensures the predictive performance of our high-order relations. The $I_{\\text{upper}}$ actually corresponds to $\\sum_{k=1}^K \\sum_{i=1}^N \\mathbb{H}[X_i](1-p_{\\theta, i}^k)$ in Eq,10 & Eq.11, where one can find there is no force to partition at all.\n\nThe property that a node can be connected by **any number** (0 to $K$) of hyperedges is attributed to our masking mechanism. Each hyperedge is generated by a learnable mask independently, which is free to connect to the same node.\n\n[1] Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. *arXiv preprint arXiv:1312.6114*.\n\n[2] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. *Advances in neural information processing systems*, *33*, 6840-6851.\n\n[3] Kim, J., Kim, M., Woo, D., & Kim, G. (2021). Drop-bottleneck: Learning discrete compressed representation for noise-robust exploration. *arXiv preprint arXiv:2103.12300*\n\n[4] Wu, T., Ren, H., Li, P., & Leskovec, J. (2020). Graph information bottleneck. *Advances in Neural Information Processing Systems*, *33*, 20437-20448.\n\n[5] Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., & Bengio, Y. (2018). Learning deep representations by mutual information estimation and maximization. *arXiv preprint arXiv:1808.06670*.\n\n[6] Veli\u010dkovi\u0107, P., Fedus, W., Hamilton, W. L., Li\u00f2, P., Bengio, Y., & Hjelm, R. D. (2018). Deep graph infomax. *arXiv preprint arXiv:1809.10341*.\n\n### C4.2 \n\nThe proposed method should be compared with Deep Clustering if it will be rewritten to focus on feature grouping.\n\n### RC4.2\n\nThis concern is also a result of the misunderstanding of the model objective. Even if our task might be related to Deep Clustering, it differs a lot from the following aspects.\n\n1. Our methods can learn a cover, which means a node can be connected to multiple hyperedges at the same time. However, most clustering methods usually assume each node only belongs to one cluster.\n2. Most Deep Clustering methods usually need the supervision of cluster labels or semi-supervision signals of \u201cmust-links\u201d and \u201ccannot-links\u201d [7].  However, we don\u2019t have **such labels** besides the prediction labels. \n3. Deep Clustering methods don\u2019t characterize the clusters with a **succinct summary**. However, our methods compute a weight for each hyperedge, which is **informative** toward cognition.\n4. In contrast to clustering methods, which usually assign each node to a specific cluster, our approach selectively **prunes** nodes that are not crucial for cognition. For example, for the EN-back condition, only 64 out of 164 regions are connected by at least one hyperedge.\n\n[7] Ren, Y., Pu, J., Yang, Z., Xu, J., Li, G., Pu, X., ... & He, L. (2022). Deep clustering: A comprehensive survey. *arXiv preprint arXiv:2210.04142*\n\n### C5.2\n\nClusters overlap with each other, which contradicts the regularization\n\n### RC5.2\n\n This is also a result of the misunderstanding in Q2. Our approach does not incorporate any regularization term to enforce the partition according to Eq.11 in the submission, so hyperedges are free to overlap with each other as long as the **overlap enhances the predictive performance**."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028859488,
                "cdate": 1700028859488,
                "tmdate": 1700068378115,
                "mdate": 1700068378115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rcKKJucAZx",
                "forum": "ICuUgRLp4C",
                "replyto": "1BoUFvOO3j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Authors - other concerns"
                    },
                    "comment": {
                        "value": "We would like to address the reviewer's concerns in this thread.\n\n## Weakness\n### C1\n\nThe author needs to clearly and formally define what type of high-order relations the proposed model captures.\n\n### RC1\n\nWe would like to politely point out that the high-order relations have been **intuitively** defined in the second paragraph of the Introduction (maximally informative and minimally redundant, MIMR) and **formally** defined in Equation 10 (IB principle). \n\nSpecifically, our high-order relations are those that are most **predictive**, yet contain the **least** redundant information towards cognition. We have made the definition more clear and explicit in the updated version.\n\nThe definition of ours is different from that in the three papers, where they consider nodes in a high-order relation should be collectively correlated in terms of co-fluctuations or some metrics like O-information.\n\nHowever, we appreciate the references you provided. We have included them and discussed the difference between their methods and ours in **Related Work (high-order relationships in fMRI)** in the updated version: Some of them are not scalable to high-order relations of a large degree, while some of them are inconsistent with our MIMR objective.\n\n### C3\n\nThe approach feels like an ad hoc method for grouping the feature vectors.\n\n### RC3\n\nWe would like to point out that our method is beyond feature grouping because \n\n1. A node is free to be connected by different hyperedges at the same time, which makes it different from grouping or clustering. \n2. Our method summarizes each hyperedge into a scalar, which is itself informative without the original features.\n3. Our method also shows consistent predictive performance improvements other than selecting significant feature groups.\n4. Our objective (MIMR) is based on a widely used principle and can be generalized on various tasks and various inputs.\n\nAlthough in practice we choose the Pearson correlation as our input features, our method can generalize on other features (e.g. time signals). Our choice of Pearson correlation coefficients as input features is intended to enable a **direct comparison** with the state-of-the-art method [9], and they use Pearson correlations as their node features.\n\n[9] Kan, Xuan, et al. \"Brain network transformer.\" *Advances in Neural Information Processing Systems* 35 (2022): 25586-25599\n\n### C4.1\n\nPredictive quality has nothing to do with discovering high-order relations.\n\n### RC4.1\n\nThis concern is a result of the misunderstanding of the model objective, as explained in our responses to Q1 and Q2, the high-order relations we want to discover in this work are the most **predictive** ones with minimal redundancy. \n\n### C5.1\n\nThe author does not show individual hyperedges.\n\n### R5.1\n\nWe focus on the region importance instead of individual hyperedges because we believe statistics make it clearer to present information. However, we appreciate your interest in individual hyperedges so we have included visualization and analysis of the most significant hyperedge under the EN-back condition in Appendix G."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029008284,
                "cdate": 1700029008284,
                "tmdate": 1700068292287,
                "mdate": 1700068292287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gA7GRFf7gV",
            "forum": "ICuUgRLp4C",
            "replyto": "ICuUgRLp4C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8333/Reviewer_JozW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8333/Reviewer_JozW"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a hypergraph inference method based on optimizing the predictive power (in the form of mutual information) of the selected (\"connected by hyperedge\") graph node towards certain labels and the redundancy term. The effectiveness of the inference method was evaluated based on an fMRI condition classification task with comparison to pair-wise connectivity estimation and connectivity-based cognition prediction methods and achieved superior performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Eq. 9 and 10 provide a useful solution for the MRMR-like feature selection problem."
                },
                "weaknesses": {
                    "value": "Formulating the hyperedge inference problem into a feature selection (MRMR-like) problem is interesting, yet not valid, at least in the context of functional connectivity analysis. Regions that are predictive together towards a certain cognitive condition do not imply that they are functionally connected. It's actually easy to construct a case where two regions have very similar fMRI signals, indicating potential strong functional connectivity, but will not be considered as \"hyperedge connected\" in the presented model as their information is redundant for the prediction."
                },
                "questions": {
                    "value": "1) How is the p-value calculated for the hyperedge in Fig. 4?\n2) It is recommended to discuss how the DimReduction MLP could be trained with a large number of nodes.\n3) Are the linear-head Fl shared across hyperedges or trained separately for each hyperedge?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699598957902,
            "cdate": 1699598957902,
            "tmdate": 1699637036270,
            "mdate": 1699637036270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nBmcp49PrN",
                "forum": "ICuUgRLp4C",
                "replyto": "gA7GRFf7gV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Offical comments by Authors"
                    },
                    "comment": {
                        "value": "The reviewer stated \n\n> The effectiveness of the inference method was evaluated based on an fMRI condition classification task with comparison\u2026\n\nWe would like to politely point out that our method is not evaluated based on the fMRI condition classification task. Instead, as mentioned in Section 2 and Section 5.1 (Dataset) in the submission, our method is evaluated based on cognition score prediction, which is a regression task.\n\n## Weaknesses\n### C1\nIt is not valid to infer functional connectivity according to the predictive performance.\n\n### RC1\n\nWe believe that there is a misunderstanding on the motivation. Traditional methods [1][2] typically define functional connectivity based on **similarity or correlation** (like Pearson correlation, and mutual information). However, this kind of connectivity is **not informative** toward a neurological outcome (see the performance of standard pairwise baseline in Table 1 as evidence) and might fall short in providing insights relevant to the outcome. As a result, it is crucial to learn these connections taking the outcome into account like what has been done in [3][4]. Therefore, our work does not seek to identify the traditional connections.\n\nInstead, as mentioned in 3rd paragraph of the submission, the objective of our method is to discover high-order connections that are **predictive** toward a cognition score (a part of our **MIMR objective**). And for regions that are connected by a predictive high-order relation, the relations between these regions can be highly non-linear and complex, and far beyond any existing manually-designed metrics (e.g. Pearson correlation, mutual information).\n\n[1] Shen, X., Finn, E. S., Scheinost, D., Rosenberg, M. D., Chun, M. M., Papademetris, X., & Constable, R. T. (2017). Using connectome-based predictive modeling to predict individual behavior from brain connectivity.\u00a0*nature protocols*,\u00a0*12*(3), 506-518.\n\n[2] Boyle, R., Connaughton, M., McGlinchey, E., Knight, S. P., De Looze, C., Carey, D., ... & Whelan, R. (2023). Connectome\u2010based predictive modeling of cognitive reserve using task\u2010based functional connectivity.\u00a0*European Journal of Neuroscience*,\u00a0*57*(3), 490-510.\n\n[3] Kawahara, J., Brown, C. J., Miller, S. P., Booth, B. G., Chau, V., Grunau, R. E., ... & Hamarneh, G. (2017). BrainNetCNN: Convolutional neural networks for brain networks; towards predicting neurodevelopment.\u00a0*NeuroImage*,\u00a0*146*, 1038-1049.\n\n[4] Mahmood, U., Fu, Z., Calhoun, V. D., & Plis, S. (2021). A deep learning model for data-driven discovery of functional connectivity.\u00a0*Algorithms*,\u00a0*14*(3), 75.\n\n\n## Questions\n### Q1\n\nHow is the p-value calculated for the hyperedge in Fig. 4?\n\n### RQ1\n\nAs mentioned in Section 5.3\n\n> CPM conducts a significance test on pairwise edges and hyperedges internally based on a linear regression model, and thus we can obtain a P-value for each hyperedge from the significance test.\n> \n\nAt a high level, CPM internally calculates a correlation coefficient for each hyperedge with the cognition score. Under a classical statistical hypothesis test framework, the coefficient follows a distribution. The p-value is obtained from the distribution. For details, we have added a paragraph about it in Appendix B.\n\n### Q2\n\nHow the DimReduction MLP could be trained with a large number of nodes.\n\n### RQ2\n\nWe apply DimReduction MLP independently on each hyperedge. As specified in Eq. 6 in the submission, for a hyperedge we first average features of nodes in that hyperedge by ${\\boldsymbol{m}^k}^T \\boldsymbol{h}^k$ and obtain the hyperedge features in $\\mathbb{R}^{d}$. After that, the DimReduction MLP compresses the dimensionality from $d$ to $1$ as a scalar weight for the hyperedge. The parameters of the DimReduction MLP are shared for every hyperedge and every dataset.\n\n### Q3\n\nAre the linear-head Fl shared across hyperedges or trained separately for each hyperedge?\n\n### RQ3\n\nAs specified in Eq.7, the inputs of the linear-head $\\mathcal{F}_l$ are scalar weights of all hyperedges. Consequently, the notion of shared vs. separate linear-head functions in terms of hyperedges does not apply in this context regarding hyperedges. However, $\\mathcal{F}_l$ is shared across all datasets."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700026306247,
                "cdate": 1700026306247,
                "tmdate": 1700026907415,
                "mdate": 1700026907415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]