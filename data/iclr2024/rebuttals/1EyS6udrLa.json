[
    {
        "title": "Towards Bringing Advanced Restoration Networks into Self-Supervised Image Denoising"
    },
    {
        "review": {
            "id": "XSDy7kJ1UM",
            "forum": "1EyS6udrLa",
            "replyto": "1EyS6udrLa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission644/Reviewer_jHRD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission644/Reviewer_jHRD"
            ],
            "content": {
                "summary": {
                    "value": "The main idea of this article is to enhance the performance of self-supervised image denoising (SSID) by integrating advanced image restoration network designs into SSID methods. Traditional SSID approaches often rely on simple neural network architectures, which have become outdated in light of recent developments in image restoration networks. The authors aim to bridge this gap by adapting advanced network architectures and attention mechanisms from image restoration to SSID. Experimental results demonstrate performance improvements on synthetic and real-world noisy images."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper delves into the relationship between network architectures and SSID methods.\n2. The paper proposes several techniques to improve the network architecture of SSID.\n2. The paper demonstrates the achievement of more advanced and improved results in the field of SSID."
                },
                "weaknesses": {
                    "value": "1. The paper appears to lack significant innovation as the consensus in the field is that using more advanced and complex network architectures to enhance performance is well-established. \n2. While the paper introduces some modifications to network structures to make them suitable for SSID, these changes are relatively minor, such as the introduction of channel attention and other attention mechanisms.\n3. It appears that the network modifications may not yield as significant improvements as directly increasing the network parameters would. It would be informative to investigate the results of directly increasing the parameters of the original U-Net network. Ultimately, as previously mentioned, whether through improving the network or increasing parameters, the performance improvements seem somewhat expected in the context of prior neural network-based work."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission644/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission644/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission644/Reviewer_jHRD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698598304518,
            "cdate": 1698598304518,
            "tmdate": 1699635991877,
            "mdate": 1699635991877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "BTAVFRnwEh",
            "forum": "1EyS6udrLa",
            "replyto": "1EyS6udrLa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission644/Reviewer_iVws"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission644/Reviewer_iVws"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to adapt transformer based models for self-supervised image denoising. To achieve the blind spot effect, the authors start by analyzing the previous blind spot mechanism in detail. Inspired by the mechanism in previous works, the authors proposed the blind spot window attention by adjusting the masks. The channel attention, downsampling, and upsampling blocks are difficult to deal with. And it is quite likely that information leaks will happen in the network. To avoid that, the authors simply remove the channel attention, downsampling, and upsampling operations in the network. Experiments are conducted on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to understand.\n\n2. At the beginning of the paper, the paper talks about how the blind-spot mechanism could be achieved by analyzing the four blind-spot networks. And this analysis leads naturally to the extension of window self-attention.\n\n3. The authors conducted various experiments and provided sufficient experimental results to support the effectiveness of the proposed mechanism."
                },
                "weaknesses": {
                    "value": "1. The main concern is the novelty of this paper. There are mainly three parts in a transformer network that might influence the blind-spot mechanism. The first one is window self-attention and it is adapted to  blind-spot networks naturally. The other two operations includes channel attention, downsampling and upsampling. Yet, the possible information leak is only solved by discarding those components. This is quite a brute-force approach.\n\n2. The biggest of removing the downsampling and upsampling operations enforce that all operation is done on the same resolution as the input image. Yet, this corresponds to a significant increase of the computation. Thus, it becomes questionable whether it is good to have a lightly improved performance by introducing too much computation."
                },
                "questions": {
                    "value": "1. In network such as HAT, and swinir. there are 3x3 convolutions in the network. How the blind-spot mechanism of those 3x3 convolution kernels are guaranteed.\n\n2. Are there experiments  on synthetic dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission644/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission644/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission644/Reviewer_iVws"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796279386,
            "cdate": 1698796279386,
            "tmdate": 1699635991793,
            "mdate": 1699635991793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "E81aqGHczX",
            "forum": "1EyS6udrLa",
            "replyto": "1EyS6udrLa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission644/Reviewer_uBSH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission644/Reviewer_uBSH"
            ],
            "content": {
                "summary": {
                    "value": "This work investigated how to incorporate transformers into blind-spot based self-supervised image denoising (SSID) by adjusting window attention and not using channel attention. This work investigated various SSID methods with different blind-spot strategies and demonstrated its great denoising performance in both synthetic and real denoising tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Reporting state-of-the-art performance in popular synthetic and real benchmarks is promising.\nConsidering different blind-spot strategies in self-supervised denoising is neat."
                },
                "weaknesses": {
                    "value": "Unlike the claim in this manuscript, self-supervised image denoising with transformers has been investigated in multiple works. It is unclear if the proposed method is novel over these prior works.\nThe motivation of this work (\"We notice the lack of research on backbone architectures in SSID and suggest adapting the advanced designs in restoration networks into SSID\") seems unclear - why transformers should work better than CNNs in denoising?"
                },
                "questions": {
                    "value": "Q1. here are prior works on self-supervised denoising with transformers. Please clarify the novelty of the proposed method over them as well as demonstrate its superiority to them.\n- Young-Joo Han, Ha-Jin Yu, SS-BSN: Attentive Blind-Spot Network for Self-Supervised Denoising with Nonlocal Self-Similarity, IJCAI 2023\n- X Liu et al., DnT: Learning Unsupervised Denoising Transformer from Single Noisy Image, IPMV 2022 (doi: 10.1145/3529446.3529455)\n- LG-BPN (Wang et al., 2023), which was cited in the manuscript, but need more explanation.\n\nQ2. the motivation of this work is simply mentioning \"we notice the lack of research on backbone architectures in SSID and suggest adapting the advanced designs in restoration networks into SSID,\" but it is unclear why transformers should work better than CNNs in denoising. CNN still works well in very recent works like the following works, so strong justification of using transformers in denoising may be needed.\n- J Li et al., Spatially Adaptive Self-Supervised Learning for Real-World Image Denoising, CVPR 2023\n- D Zhang et al., MM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask based on Blind-Spot Network, CVPRW 2023\n- Y Zou et al., Iterative Denoiser and Noise Estimator for Self-Supervised Image Denoising, ICCV 2023\n- J Wang et al., Noise2Info: Noisy Image to Information of Noise for Self-Supervised Image Denoising, ICCV 2023\n\nQ3. The proposed method may have limited extension capability. Recent works on self-supervised single image denoising now uses very simple and lightweight CNN networks (not even U-net) to achieve remarkable results (see the below works) while it is unclear transformer-based networks can achieve similar performance in denoising considering that transformers usually require larger dataset than CNNs. I am afraid that this work goes into the opposite direction of using heavier network for denoising. Please discuss.\n- Y Mansour and R Heckel, Zero-Shot Noise2Noise: Efficient Image Denoising without any Data, CVPR 2023\n- J Lequyer et al., A fast blind zero-shot denoiser, Nature Machine Intelligence, 2022.\n\nQ4. Tables 5 and 6 look promising, but there are a number of other information needed such as training time and network size for fair comparisons. Please report."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809659309,
            "cdate": 1698809659309,
            "tmdate": 1699635991681,
            "mdate": 1699635991681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "gfxXusG97i",
            "forum": "1EyS6udrLa",
            "replyto": "1EyS6udrLa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission644/Reviewer_xn1Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission644/Reviewer_xn1Q"
            ],
            "content": {
                "summary": {
                    "value": "The paper address self-supervised denoising task, where advanced restoration network designs ( SwinIR, Restormer, NAFNet, and HAT)  are incorporated into blind-spot self-supervised image denoising (SSID) networks. This paper introduce a series of approaches to adapt restoration networks into various blind-spot ones, where they suggest effective adjustment for window attention to mimic the convolution layers in BSN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper conducts the study where networks like SwinIR, Restormer, NAFNet, and HAT can be adapted to BSN networks to perform SSID task \n- The paper discusses why channel wise is attention is not beneficial and how it can leaks blind-spot information\n- The paper introduced efficient masking strategy to adapt the restoration networks to blind-spot ones\n- THe paper performs extensive experiments to on both synthetic and real-world RGB noisy images demonstrate the proposed\nmethods substantially enhance SSID performance"
                },
                "weaknesses": {
                    "value": "- Can authors explain if the proposed can be applicable for both signal depdent and signal independent noises\n- Can authors explain if the proposed method can handle other types noises like Poisson, and other types of degradations like chromatic aberration, and jpeg compression etc.\n- Can authors explain if the proposed method of experiments can be extended other image restoration tasks like deblurring, adverse weather removal, inpainting\n- Can authors show some visualizations of attentions and masks for SSID task and supervised image denoising tasks for atleast one method like SwinIR or Restormer. It would be really helpful for reader to understand how the attentions are being adapted for blind spot ones.\n- Can the proposed method experiments be extended to dynamic windowed attentions, or dynamic deformable kernel attentions"
                },
                "questions": {
                    "value": "Please refer weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission644/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699133204373,
            "cdate": 1699133204373,
            "tmdate": 1699635991608,
            "mdate": 1699635991608,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]