[
    {
        "title": "Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning"
    },
    {
        "review": {
            "id": "5nAW4xXSAz",
            "forum": "dIynM7bHCG",
            "replyto": "dIynM7bHCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_mbZH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_mbZH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DOM2, a decentralized training and execution Offline MARL approach using diffusion policies. Their algorithm combines CQL (critic) and DPM-Solver (actor) as well as data augmentation to increase the dataset size with good trajectories. DOM2 allows for better generalization to slightly shifted environments as well as improved data efficiency. DOM2 is evaluated on MPE and HalfCheetah-v2 and shows improved performance across all datasets, and even shows good performance on random datasets where all other previous methods fail."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The experimental results are impressive and it is especially surprising to see DOM2 perform well in random datasets across all tasks, which is challenging for conservatism-based offline RL algorithms.\n\n2. While DOM2 uses decentralized training without any consideration of the non-stationarity in MARL (see Weaknesses), I find it interesting that decentralized training is able to perform well across all datasets and even exhibit multi-modal behavior."
                },
                "weaknesses": {
                    "value": "1. Since the policy training loss in Eq. 5 is decentralized, there is no guarantee that DOM2 maximizes the global Q-function for the overall Dec-POMDP. The $Q(o_j, a_j)$ is just an individual utility function which ignores the non-stationarity of the environment due to the other agents\u2019 policies being updated during training. Since many environments require policy dependence to be considered (see e.g. [1]), there should be some clear insight as to (a) what kinds of environments and (b) what specific aspect of DOM2/diffusion models in general can allow for the community to consider decentralized training.\n\n2. The paper appears to be a relatively simple combination and application of existing work, namely CQL for the critic loss, DPM Solver for the diffusion policy. This is not a problem in and of itself but there is not insight or deeper analysis of the specific properties of DOM2/diffusion policies which makes it suitable for offline MARL. \n\n3. While the results on MAMuJoCo and MPE are impressive, DOM2 should be tested on more complex environments requiring agents to coordinate at a higher level e.g. Google Research Football, SMACv2.\n\n4. The main contribution or key insight of DOM2 from the diffusion model perspective is not clear. The analysis below Algorithm 2 only refers to the architectural differences compared to other algorithms.\n\n5. The claim that DOM2 is ultra data efficient is not convincing to me as data augmentation is included in the DOM2 algorithm but it seems the same augmentation technique could be used for other baselines.\n\n[1] Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning (Fu et, al. ICML 2022)"
                },
                "questions": {
                    "value": "1. If a Dec-POMDP is considered, why does the reward $r_j^t$ index on agent ID? Are different reward values given for each agent in the environment during the experiments as well?\n\n2. Is there any insight regarding why decentralized training is enough for DOM2 perform well? For instance, it could be the case that (1) the environments considered are too simple or (2) some specific property of using diffusion policies make it such that there is some implicit dependence among policies or (3) continuous control environments in practice require less  dependence among policies. \n\n3. If the critic is trained using the CQL loss, then it seems that the Q values will just be conservative to OOD actions. This means that in order to produce policy diversity, the dataset must already contain the diverse behavior. Is my understanding correct here? I also considered the possibility that data augmentation helps with behavior diversity but Figure 6 suggests that it is not crucial. \n\n4. As far as I can tell, the data augmentation technique seems orthogonal to the DOM2 algorithm itself. If that is the case, shouldn\u2019t all baselines also include the data augmentation technique? Is it really possible to say DOM2 is \u201cultra\u201d data efficient without a fair comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697959476291,
            "cdate": 1697959476291,
            "tmdate": 1699636170270,
            "mdate": 1699636170270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v8QOLJGBVv",
                "forum": "dIynM7bHCG",
                "replyto": "5nAW4xXSAz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer mbZH"
                    },
                    "comment": {
                        "value": "Thanks for your time and effort in reviewing our paper! Please find our responses to your comments below. We will be happy to answer any further questions you may have.\n\nW1: Since the policy training loss in Eq. 5 is decentralized, there is no guarantee that DOM2 maximizes the global Q-function for the overall Dec-POMDP. The is just an individual utility function which ignores the non-stationarity of the environment due to the other agents\u2019 policies being updated during training. Since many environments require policy dependence to be considered (see e.g. [1]), there should be some clear insight as to (a) what kinds of environments and (b) what specific aspect of DOM2/diffusion models in general can allow for the community to consider decentralized training.\n\nA1: Thank you very much. In the task that the observation of each agent is dependent with other agents (e.g., observation includes other agents' information), a fully-decentralized algorithm can be applied. Besides, if the selection of the action is not highly influenced by other agents states, e.g., MAMuJoCo 2-agent halfcheetah, a fully-decentralized algorithm can be used to solve the problem, which is irrelevant to the selection of the diffusion model.\n\nW2: The paper appears to be a relatively simple combination and application of existing work, namely CQL for the critic loss, DPM Solver for the diffusion policy. This is not a problem in and of itself but there is not insight or deeper analysis of the specific properties of DOM2/diffusion policies which makes it suitable for offline MARL.\n\nA2: **We emphasize that DOM2 can train a diffusion-based policy with better performance and high diversity.** A policy with high diversity is easy to generalize better when the environment changes. Deeper analysis is a good extension to discover in the future. \n\nW3: While the results on MAMuJoCo and MPE are impressive, DOM2 should be tested on more complex environments requiring agents to coordinate at a higher level e.g. Google Research Football, SMACv2.\n\nA3: Thanks for your advice and more experiments should be considered in the future.\n\nW4: The main contribution or key insight of DOM2 from the diffusion model perspective is not clear. The analysis below Algorithm 2 only refers to the architectural differences compared to other algorithms.\n\nA4: **Our proposed DOM2 incorporates diffusion into action generation to establish a highly-diverse policy**, which is the key insight as our contribution. More comparisons beyond the architectural differences are good extensions, which should be considered in the future.\n\nW5: The claim that DOM2 is ultra data efficient is not convincing to me as data augmentation is included in the DOM2 algorithm but it seems the same augmentation technique could be used for other baselines.\n\nA5: The reason is that it corresponds to a non-uniform data sampling strategy and other algorithms can obtain this technique without considering the algorithm itself. It is a simple but efficient method and for more data-augmented methods, it is a good extension to discover.\n\n[1] Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning (Fu et, al. ICML 2022)\n\nQ1: If a Dec-POMDP is considered, why does the reward index on agent ID? Are different reward values given for each agent in the environment during the experiments as well?\n\nA1: The reward values for each agent is related to the actual environment and task. For instance, in the particles environment, after each agent decides the action, the agent can gain an individual reward and the overall reward for these agents is the sum of the individual rewards. In MAMuJoCo environment, each agent can only gain an overall reward after taking actions by each agent. There does no exist an individual reward for each agent in this environment. \n\nQ2: Is there any insight regarding why decentralized training is enough for DOM2 perform well? For instance, it could be the case that (1) the environments considered are too simple or (2) some specific property of using diffusion policies make it such that there is some implicit dependence among policies or (3) continuous control environments in practice require less dependence among policies.\n\nA2: The reason is that the observation of each agent consists of the information of other agents, so a fully-decentralized offline MARL algorithm is applicable. Another possible reason is that the influence of other agents' states or actions are limited for each agent to take actions. Our algorithm in fully decentralized setting can solve a class of tasks in the MPE and MAMuJoCo environments."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454227273,
                "cdate": 1700454227273,
                "tmdate": 1700454227273,
                "mdate": 1700454227273,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FiZPOoIfQm",
            "forum": "dIynM7bHCG",
            "replyto": "dIynM7bHCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
            ],
            "content": {
                "summary": {
                    "value": "Recent works in offline Reinforcement Learning (RL) rely on conservatism. The paper presents Diffusion Offline Multi-agent Model (DOM2), which improves policy design and diversity using a diffusion model. DOM2 utilizes a diffusion model in the policy network and makes use of a trajectory-based data augmentation scheme during offline training. The data augmentation technique trains DOM2 agents on a replay buffer wherein trajectories with higher rewards are duplicated. Ablation studies and experiments demonstrate the empirical effectiveness of proposed design choices."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is well-written and organized.\n* Empirical evaluation provided by the authors is comprehensive."
                },
                "weaknesses": {
                    "value": "* **Claims on Conservatism:** My main concern is the claims on conservatism used within the DOM2 model. Authors argue that different from prior works they do not rely on conservatism for policy design. The paper also states that learning policies and values with conservatism is inefficient. However, DOM2 model significantly relies on Conservative Q Learning (CQL) to train Q values and hence the policy $\\pi$. Furthermore, CQL forms the key part of DOM2 as it is the only ingredient used for policy improvement. This can be validated from ablation studies wherein the conservative policy improvement scheme contributes to most gains in the performance of DOM2. Thus, claims on conservatism severely contradict the paper's central idea.\n* **Data Augmentation:** The proposed data augmentation scheme prioritizes highly rewarding trajectories while downweighing lower ones. DOM2 agents, thus, have access to privileged data samples rather than augmented samples as the datapoints itself have not been modified in any way (eg- shifting, scaling, transformed, etc.). In this view, the training process appears to be biased resulting in a near-expert dataset for DOM2 agents and sub-optimal dataset for baseline agents. Note that other baselines do not have access to privileged data samples but only the orignal dataset. This leads DOM2 to outperform prior methods as a result of dataset selection and not algorithmic modifications.\n* **Choice of Baselines:** While the empirical evaluation provided in the paper is comprehensive, authors only compare DOM2 to multi-agent baselines. It would be worthwhile to consider other offline RL algorithms in multi-agent settings which have demonstrated cutting edge performance. The paper could compare DOM2 to independent IQL learners [1] or BRAC agents [2] in the multi-agent setting. Similarly, authors could assess the choice of policy improvement scheme using a different offline RL algorithm such as BEAR [3]. This would help validate the claims of conservatism and evaluate the importance of CQL during training.\n* **Differences from Prior Work:** I struggle to understand the central contribution of DOM2 within the offline RL literature. Using diffusion models for learning policies is a common practice in offline RL. In addition, the data augmentation scheme corresponds to a top-k sampling strategy wherein trajectories with higher rewards are sampled. It is thus unclear as to what is the novel contribution of DOM2 within multi-agent offline RL literature. It would be worthwhile if authors could highlight the differences between DOM2 and recent algorithms such as Diffuser [4], EDP [5], MADIFF [6], OMAC [7] and OMIGA [8] explicitly. Additionally, authors could discuss the benefits or design choices which are not found in standard multi-agent learning algorithms.\n\n[1]. Kostrikov et. al., \"Offline Reinforcement Learning with Implicit Q-Learning\", ICLR 2022.  \n[2]. Wu et. al., \"Behavior Regularized Offline Reinforcement Learning\", arxiv 2019.  \n[3]. Kumar et. al., \"Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction\", NeurIPS 2019.  \n[4]. Janner et. al., \"Planning with Diffusion for Flexible Behavior Synthesis\", ICML 2022.  \n[5]. Kang et. al., \"Efficient Diffusion Policies for Offline Reinforcement Learning\", arxiv 2023.  \n[6]. Zhu et. al., \"MADIFF: Offline Multi-agent Learning with Diffusion Models\", arxiv 2023.  \n[7]. Wang et. al., \"Offline Multi-Agent Reinforcement Learning with Coupled Value Factorization\", AAMAS 2023.  \n[8]. Wang et. al., \"Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization\", arxiv 2023."
                },
                "questions": {
                    "value": "* Why is learning policies and value functions with conservatism inefficient? Can you please explain the reliance of DOM2 on CQL for conservatism?\n* Does trajectory-based augmentation provide high-quality samples only to DOM2? What if other baselines are trained with a similar scheme? Were any samples modified/augmented using shifting, scaling , etc. during training?\n* How does DOM2 compare with other offline multi-agent RL baselines such as IQL or BRAC? How effective is the usage of CQL for policy improvement? Can the policy improvement scheme be replaced/compared with another offline RL algorithm such as BEAR?\n* How is DOM2 different from Diffuser [4], EDP [5], MADIFF [6], OMAC [7] and OMIGA [8]? Can you please discuss some recent related works comparing DOM2 with offline RL and multi-agent RL literature?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2375/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2375/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678683095,
            "cdate": 1698678683095,
            "tmdate": 1699636170095,
            "mdate": 1699636170095,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X6NURbEYu9",
                "forum": "dIynM7bHCG",
                "replyto": "FiZPOoIfQm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer SRZp"
                    },
                    "comment": {
                        "value": "Thanks for your time and effort in reviewing our paper! Please find our responses to your comments below. We will be happy to answer any further questions you may have.\n\nW1: Claims on Conservatism: My main concern is the claims on conservatism used within the DOM2 model. Authors argue that different from prior works they do not rely on conservatism for policy design. The paper also states that learning policies and values with conservatism is inefficient. However, DOM2 model significantly relies on Conservative Q Learning (CQL) to train Q values and hence the policy. Furthermore, CQL forms the key part of DOM2 as it is the only ingredient used for policy improvement. This can be validated from ablation studies wherein the conservative policy improvement scheme contributes to most gains in the performance of DOM2. Thus, claims on conservatism severely contradict the paper's central idea.\n\nA1: **We emphasize that our DOM2 algorithm is beyond conservatism due to the reason that DOM2 can train a policy with better performance and high diversity** compared to the conservatism-based methods, e.g., MA-CQL and OMAR, which means that such a diverse policy has superior generalization ability under the change of environments. In the motivating examples, we have shown that DOM2 can find more solutions compared to other algorithms (In Table 8 and Figure 9 of our paper), which confirms our justification beyond conservatism. Results show that DOM2 has nearly state-of-the-art performance under the standard and shifted environments, which also means that DOM2 is an efficient algorithm beyond conservatism.\n\nW2: Data Augmentation: The proposed data augmentation scheme prioritizes highly rewarding trajectories while downweighing lower ones. DOM2 agents, thus, have access to privileged data samples rather than augmented samples as the datapoints itself have not been modified in any way (eg- shifting, scaling, transformed, etc.). In this view, the training process appears to be biased resulting in a near-expert dataset for DOM2 agents and sub-optimal dataset for baseline agents. Note that other baselines do not have access to privileged data samples but only the orignal dataset. This leads DOM2 to outperform prior methods as a result of dataset selection and not algorithmic modifications.\n\nA2: **We emphasize that the quality of a dataset is not determined by the trajectory rewards.** The most important factor for a dataset is the behavior policy. The generation of the offline data is determined by the behavior policy and the quality of the data is highly related to the performance of the behavior policy. Our data augmentation method is a simple but efficient method by duplicating the trajectories with higher returns, which corresponds to a non-uniform sampling strategy. No evidence shows that our data augmentation method privileges the high-reward trajectories to enable the transformation of a dataset from low-quality to high-quality.\n\nW3: Choice of Baselines: While the empirical evaluation provided in the paper is comprehensive, authors only compare DOM2 to multi-agent baselines. It would be worthwhile to consider other offline RL algorithms in multi-agent settings which have demonstrated cutting edge performance. The paper could compare DOM2 to independent IQL learners [1] or BRAC agents [2] in the multi-agent setting. Similarly, authors could assess the choice of policy improvement scheme using a different offline RL algorithm such as BEAR [3]. This would help validate the claims of conservatism and evaluate the importance of CQL during training.\n\nW4: Differences from Prior Work: I struggle to understand the central contribution of DOM2 within the offline RL literature. Using diffusion models for learning policies is a common practice in offline RL. In addition, the data augmentation scheme corresponds to a top-k sampling strategy wherein trajectories with higher rewards are sampled. It is thus unclear as to what is the novel contribution of DOM2 within multi-agent offline RL literature. It would be worthwhile if authors could highlight the differences between DOM2 and recent algorithms such as Diffuser [4], EDP [5], MADIFF [6], OMAC [7] and OMIGA [8] explicitly. Additionally, authors could discuss the benefits or design choices which are not found in standard multi-agent learning algorithms.\n\nA3/A4: These comparisons are beneficial to justify the superior performance of DOM2 algorithm and state that our algorithm is an efficient algorithm beyond conservatism to find optimal solutions with diversity. Due to the time limit, we have finished the comparison between DOM2 and MA-IQL (a fully-decentralized multi-agent version of IQL, the same later), MA-BRAC, MA-Diffusion-QL, OMAC and OMIGA. More results will be shown later. The results show that DOM2 has state-of-the-art performance compared with the mentioned algorithms."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452699550,
                "cdate": 1700452699550,
                "tmdate": 1700452699550,
                "mdate": 1700452699550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2wsJVgS2Si",
                "forum": "dIynM7bHCG",
                "replyto": "GzMT93xuQZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I thank the authors for responding to my comments. After going through other reviews and responses from authors, my concerns remain unaddressed-\n\n* **Claims on Conservatism:** Authors state that DOM2 is beyond conservative methods as a result of its improved performance. Algorithmically, DOM2 still uses the conservative regularization of CQL for policy improvement which makes it a conservative algorithm. The fact that DOM2 identifies more solutions than other methods does not remove it from the bracket of conservative algorithms. Furthermore, identifying diverse solutions does not necessarily correlate with generalization. DOM2 identifies more solutions but conservative methods have shown decent performance in Out-Of Distribution (OOD) settings. Lastly, improvement in results can be observed as a primary consequence of the CQL algorithm from ablations. I encourage the authors to revisit their claims and assess their soundness.\n* **Data Augmentation:** Authors state that the quality of the dataset is not determined by trajectory rewards. It is also stated that the quality of the dataset is related to the performance of behavior policy. I find these statements to be contradictory and confusing. The data augmentation technique uses rewards as a scoring function to select high-quality samples. These samples are then duplicated in the dataset. In my view, this does not conform to a data augmentation technique since none of the data samples are augmented/manipulated in any way. In fact, the technique does not change the distribution of data. It only provides DOM2 with frequent access to high-reward samples. What happens if we do this selective sampling for other baselines? How does the augmentation strategy lead to a change in data distribution in order to grow the dataset? These questions remain unanswered since the technique does not alter data samples. Authors could revise their augmentation strategy or categorize it as a separate design choice for future work.\n* **Differences from Prior Work:** Authors have provided a comprehensive evaluation and discussion of DOM2 in comparison to other works. However, it still remains unclear as to how DOM2 contributes to a new algorithm in principle. Each design choice adopted by DOM2 is a common technique used by offline and multi-agent RL algorithms. A combination of these strategies does not provide new insights into the training of multiple agents. Authors could provide new insights into the generation process of actions of agents or discuss how their implementation scales with the growing number of agents."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509728107,
                "cdate": 1700509728107,
                "tmdate": 1700509728107,
                "mdate": 1700509728107,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O3vvNxENrn",
                "forum": "dIynM7bHCG",
                "replyto": "fKrQKMWdow",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
                ],
                "content": {
                    "title": {
                        "value": "Response to Follow Up"
                    },
                    "comment": {
                        "value": "I thank the authors for their follow up response. Unfortunately, my concerns remain unadressed. I encourage the authors to revisit their reasoning behind claims on conservatism and the data augmentation strategy. Specifically, it would be worthwhile to answer the following questions, _How is DOM2 algorithmically beyond conservatism when it uses a conservative policy at its core?_ and _Why is the proposed strategy a data augmentation strategy when the data distribution remains unchanged?_. Given the responses from authors and opinions of other reviewers, I will keep my score as is for now. I thank the authors for their efforts."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677373532,
                "cdate": 1700677373532,
                "tmdate": 1700677373532,
                "mdate": 1700677373532,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sR7ngPC4bh",
            "forum": "dIynM7bHCG",
            "replyto": "dIynM7bHCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_pMHc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_pMHc"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Diffusion Offline Multi-agent Model (DOM2), an offline MARL algorithm that is based on diffusion policy. DOM2 first augments the dataset by replicating the high-return trajectories. Then, each agent is trained independently by the Diffusion-QL-style learning method while using CQL loss for the critic. In the experiments, DOM2 outperforms the baselines in diverse MARL benchmarks including standard and shifted environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The empirical performance of the proposed DOM2 is strong. It outperforms the baselines in various domains and in shifted environment setting."
                },
                "weaknesses": {
                    "value": "1. While the paper claims \"beyond conservatism\", it still relies on conservatism both in value learning and policy learning, both in critic learning (i.e. using CQL loss) and actor learning (i.e. using BC loss).\n2. The novelty is limited. It seems the proposed DOM2 is a straightforward extension of Diffusion-QL (Wang et al., 2023) with additional data augmentation. Since the overall training is done in a fully decentralized way, it seems there is no additional/special consideration in the algorithm for the 'multi-agent' setting. 'Why diffusion model for multi-agent RL' is not well-motivated in the paper.\n3. Given that each agent is trained independently (decentralized training, rather than centralized training), it may be suboptimal even in a very simple domain (e.g., like OMAR in XOR-game as described in [1]). Can DOM2 solve simple XOR-game-like domains?\n4. The proposed data augmentation (section 4.3) does not seem doing actual data augmentation. It is not generating novel data samples, but rather just replicating the existing data samples in the dataset. It just corresponds to changing the 'data sampling distribution' (uniform -> non-uniform depending on the trajectory return).\n\n[1] Matsunaga et al., AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation, NeurIPS 2023"
                },
                "questions": {
                    "value": "Please see the weaknesses section above.\n- What is the difference between DOM2 with Diffusion-QL, except for the data augmentation? Also, could you elaborate on the core contribution of DOM2 to solve 'multi-agent' RL?\n- Why does DOM2 show better generalization performance than other baselines? Is it due to using diffusion policy, or from other factors?\n- I am also curious about the offline single-agent RL performance of DOM2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699297567712,
            "cdate": 1699297567712,
            "tmdate": 1699636169946,
            "mdate": 1699636169946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uNiZNZEDKv",
                "forum": "dIynM7bHCG",
                "replyto": "sR7ngPC4bh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer pMHc"
                    },
                    "comment": {
                        "value": "Thanks for your time and effort in reviewing our paper! Please find our responses to your comments below. We will be happy to answer any further questions you may have.\n\nW1: While the paper claims \"beyond conservatism\", it still relies on conservatism both in value learning and policy learning, both in critic learning (i.e. using CQL loss) and actor learning (i.e. using BC loss).\n\nA1: **We emphasize that our DOM2 algorithm is beyond conservatism due to the reason that DOM2 can train a policy with better performance and high diversity** compared to the conservatism-based methods, e.g., MA-CQL and OMAR, which means that such a diverse policy has superior generalization ability under the change of environments. In the motivating examples, we have shown that DOM2 can find more solutions compared to other algorithms (In Table 8 and Figure 9 of our paper), which confirms our justification beyond conservatism. Results show that DOM2 has nearly state-of-the-art performance under the standard and shifted environments, which also means that DOM2 is an efficient algorithm beyond conservatism.\n\nW2Q1: The novelty is limited. It seems the proposed DOM2 is a straightforward extension of Diffusion-QL (Wang et al., 2023) with additional data augmentation. Since the overall training is done in a fully decentralized way, it seems there is no additional/special consideration in the algorithm for the 'multi-agent' setting. 'Why diffusion model for multi-agent RL' is not well-motivated in the paper. What is the difference between DOM2 with Diffusion-QL, except for the data augmentation? Also, could you elaborate on the core contribution of DOM2 to solve 'multi-agent' RL?\n\nA2: **We emphasize that DOM2 is not a direct extension from the single-agent Diffusion-QL algorithm into a fully-decentralized version.** The most critical difference between DOM2 and the Diffusion-QL algorithm has these aspects without considering the contribution of data augmentation. DOM2 uses the dpm-solver for sampling acceleration. Besides, DOM2 uses a multi-layer residual network alike the U-Net architecture in the noise network compared with the multi-layer perceptron as the noise network in Diffsuion-QL algorithm. Table below shows the performance of DOM2 and the extension of Diffusion-QL as a fully decentralized version in offline MARL. Results show that DOM2 has better performance, which justifies our statement that it is not a direct extension without any difference.\n\n|  Predator Prey  | Random  | Medium Replay | Medium | Expert |\n|  ----  | ---- | ---- | ---- | ---- |\n| MA-Diffusion-QL | 82.2$\\pm$22.6 | 83.9$\\pm$18.4 | 117.1$\\pm$45.0 | 224.6$\\pm$29.5 |\n| DOM2 | **208.7$\\pm$57.3** | **150.5$\\pm$23.9** | **155.8$\\pm$48.1** | **259.1$\\pm$22.8** |\n\nDirectly extending the algorithm from single-agent setting into a fully decentralized multi-agent setting is not trivial and easy to fail. A simple extension in a fully decentralized algorithm, e.g., MA-CQL, is not optimal. Considering the fact, DOM2 is a successful algorithm to find the optimal solution compared to other conservatism-based algorithms. Additionally, DOM2 algorithm benefits in policy diversity, which is important for a multi-agent algorithm to guarantee the generalization ability. Results in the standard and shifted environments show that DOM2 algorithm has state-of-the-art performance with better generalization ability.\n\nW3: Given that each agent is trained independently (decentralized training, rather than centralized training), it may be suboptimal even in a very simple domain (e.g., like OMAR in XOR-game as described in [1]). Can DOM2 solve simple XOR-game-like domains?\n\n[1] Matsunaga et al., AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation, NeurIPS 2023\n\nA3: In XOR-game described in [1], **any fully-decentralized algorithm is impossible to finish the task if the observation for each agent is only the choice of itself.** It is not the reason to refute that DOM2 is not a useful offline MARL algorithm. A fully-decentralized algorithm may be impossible to solve such game tasks. However, these algorithms can solve a class of problems, e.g., tasks in the particles, Multi-agent MuJoCo and other environments. Our results show that DOM2 can successfully solve the MPE and MAMuJoCo tasks with the optimal performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452321826,
                "cdate": 1700452321826,
                "tmdate": 1700452321826,
                "mdate": 1700452321826,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DyGf4jVH0r",
            "forum": "dIynM7bHCG",
            "replyto": "dIynM7bHCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_tEv5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_tEv5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to incorporate diffusion-based policy into multi-agent offline reinforcement learning, which is a straightforward extension of the diffusion-based policy from single-agent setting into the multi-agent counterpart. Most of the techniques are known to the community, but empirical results are quite strong."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Very strong empirical results."
                },
                "weaknesses": {
                    "value": "* I don\u2019t find any new insights from this paper. Most of the techniques are from the existing work, and I don\u2019t get any intuitions on why we should do that."
                },
                "questions": {
                    "value": "* What are the unique hardnesses of the multi-agent setting, compared with the single-agent setting? I feel there are no differences between the algorithm for single-agent setting and multi-agent setting, except that authors replace the state with the observation that can contain other agents\u2019 information."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699370692181,
            "cdate": 1699370692181,
            "tmdate": 1699636169880,
            "mdate": 1699636169880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sGIsMzyItt",
                "forum": "dIynM7bHCG",
                "replyto": "DyGf4jVH0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer tEv5"
                    },
                    "comment": {
                        "value": "Thanks for your time and effort in reviewing our paper! Please find our responses to your comments below. We will be happy to answer any further questions you may have.\n\nW1: I don\u2019t find any new insights from this paper. Most of the techniques are from the existing work, and I don\u2019t get any intuitions on why we should do that.\n\nA1: Our main contributions include **(i) we incorporate diffusion models into offline MARL problem, and (ii) our proposed DOM2 algorithm achieves state-of-the-art performance with robustness and data efficiency.** In the standard and shifted environments, DOM2 achieves the state-of-the-art performance in nearly all tasks, which shows the superior performance and generalization ability. Moreover, DOM2 achieves the same performance of the SOTA baselines using only $5\\%$ data, making it a highly appealing solution for scenarios where data is scarce. Besides, DOM2 possesses state-of-the-art performance in the random dataset of all tasks, which shows that our algorithm is ultra-data-efficient in low-quality data.\n\nQ1: What are the unique hardnesses of the multi-agent setting, compared with the single-agent setting? I feel there are no differences between the algorithm for single-agent setting and multi-agent setting, except that authors replace the state with the observation that can contain other agents\u2019 information.\n\nA1: **DOM2 is not a direct extension from the single-agent Diffusion-QL algorithm into a fully-decentralized version.** The difference is that we use a faster dpm-solver to accelerate action sampling. To improve the ability of the noise network, we replace the architecture of the noise network from a simple MLP to a multi-layer residual network. Moreover, we present a trajectory-based data augmentation mechanism to improve the probability of sampling data with higher trajectory returns.  \n\nTable below shows the performance of DOM2 and the extension of Diffusion-QL as a fully decentralized version in offline MARL. Results show that DOM2 has better performance, which justifies our statement that it is not a direct extension without any difference.\n\n|  Predator Prey  | Random  | Medium Replay | Medium | Expert |\n|  ----  | ---- | ---- | ---- | ---- |\n| MA-Diffusion-QL | 82.2$\\pm$22.6 | 83.9$\\pm$18.4 | 117.1$\\pm$45.0 | 224.6$\\pm$29.5 |\n| DOM2 | **208.7$\\pm$57.3** | **150.5$\\pm$23.9** | **155.8$\\pm$48.1** | **259.1$\\pm$22.8** |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451920981,
                "cdate": 1700451920981,
                "tmdate": 1700451920981,
                "mdate": 1700451920981,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I6tceIrlF1",
            "forum": "dIynM7bHCG",
            "replyto": "dIynM7bHCG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_dHvd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2375/Reviewer_dHvd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DOM2, which applies the Diffusion QL to cooperative multiagent settings following the independent learning paradigm. Extensive experiments on multi-agent particle and multi-agent MuJoCo environments show the superiority of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The writing of the paper is clear.\n* Extensive experiments on multi-agent particle and multi-agent MuJoCo environments are conducted."
                },
                "weaknesses": {
                    "value": "* **The contribution of the paper is minor.** The proposed method DOM2 seems to be a simple application of Diffusion QL [1] to cooperative MARL. Besides, DOM2 just follows the independent learning paradigm (more like single-agent learning problems). \n* There are very few differences between DOM2 and Diffusion QL [1]. Replacing the DDPM-based diffusion policy with a faster first-order DPM-Solver should not be the main contribution of the paper. \n* **The proposed method DOM2 has little to do with multiagent.** Since the conservatism-based approaches in single-agent RL have limitations, why not directly apply the diffusion-based method to single-agent domains? As the proposed DOM2 is a decentralized training and execution framework (i.e., independent learner), evaluating the method in the single-agent domain is more straightforward.\n  * MA-DIFF (Zhu et al., 2023) have done some special designs to apply diffusion models to MARL under the CTDE paradigm, while DOM2  is a straightforward application of Diffusion QL [1].\n* The description of the motivating example shown in Figure 1 is not clear. \n* Since MA-SfBC (the extension of the single agent diffusion-based policy SfBC) is compared, MA-Diffusion QL (the extension of the single agent Diffusion QL) should also be compared.\n\n\nReferences\n\n* [1] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning."
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2375/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2375/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2375/Reviewer_dHvd"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2375/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699864763543,
            "cdate": 1699864763543,
            "tmdate": 1699864763543,
            "mdate": 1699864763543,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ek3ucgC5oE",
                "forum": "dIynM7bHCG",
                "replyto": "I6tceIrlF1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2375/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer dHvd"
                    },
                    "comment": {
                        "value": "Thanks for your time and effort in reviewing our paper! Please find our responses to your comments below. We will be happy to answer any further questions you may have.\n\nW1: The contribution of the paper is minor. The proposed method DOM2 seems to be a simple application of Diffusion QL [1] to cooperative MARL. Besides, DOM2 just follows the independent learning paradigm (more like single-agent learning problems). There are very few differences between DOM2 and Diffusion QL [1]. Replacing the DDPM-based diffusion policy with a faster first-order DPM-Solver should not be the main contribution of the paper.\n\nA1: **DOM2 is not a direct extension from the single-agent Diffusion-QL algorithm into a fully-decentralized version.** The difference is that we use a faster dpm-solver to accelerate action sampling. To improve the ability of the noise network, we replace the architecture of the noise network from a simple MLP to a multi-layer residual network. Moreover, we present a trajectory-based data augmentation mechanism to improve the probability of sampling data with higher trajectory returns.  \n\nTable below shows the performance of DOM2 and the extension of Diffusion-QL as a fully decentralized version in offline MARL. Results show that DOM2 has better performance, which justifies our statement that it is not a direct extension without any difference.\n\n|  Predator Prey  | Random  | Medium Replay | Medium | Expert |\n|  ----  | ---- | ---- | ---- | ---- |\n| MA-Diffusion-QL | 82.2$\\pm$22.6 | 83.9$\\pm$18.4 | 117.1$\\pm$45.0 | 224.6$\\pm$29.5 |\n| DOM2 | **208.7$\\pm$57.3** | **150.5$\\pm$23.9** | **155.8$\\pm$48.1** | **259.1$\\pm$22.8** |\n\nW2: The proposed method DOM2 has little to do with multiagent. Since the conservatism-based approaches in single-agent RL have limitations, why not directly apply the diffusion-based method to single-agent domains? As the proposed DOM2 is a decentralized training and execution framework (i.e., independent learner), evaluating the method in the single-agent domain is more straightforward.\n\nMA-DIFF (Zhu et al., 2023) have done some special designs to apply diffusion models to MARL under the CTDE paradigm, while DOM2 is a straightforward application of Diffusion QL [1].\nThe description of the motivating example shown in Figure 1 is not clear.\nSince MA-SfBC (the extension of the single agent diffusion-based policy SfBC) is compared, MA-Diffusion QL (the extension of the single agent Diffusion QL) should also be compared.\n\nReferences:\n\n[1] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning.\n\nA2: As we have mentioned, DOM2 is not a direct extension of Diffusion-QL for the reasons that we mention in the answer of weakness 1. Our DOM2 is implemented as a fully decentralized algorithm and a CTDE-based algorithm is also available. It is a good extension to discover in the future.\n\nThe comparison of DOM2 and MADIFF is as follows. DOM2 is a fully-decentralized offline MARL algorithm using the dpm-solver to generate action given the observation at a timestep. MADIFF is a centralized or CTDE-based algorithm to utilize the decision diffuser to generate actions by generating the trajectories for all agents based on conditional diffusion and predicting the actions using the inverse dynamical model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451727132,
                "cdate": 1700451727132,
                "tmdate": 1700451727132,
                "mdate": 1700451727132,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]