[
    {
        "title": "Classification with Conceptual Safeguards"
    },
    {
        "review": {
            "id": "WWPCitXMwH",
            "forum": "t8cBsT9mcg",
            "replyto": "t8cBsT9mcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2858/Reviewer_1z66"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2858/Reviewer_1z66"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a perspective on selective classification within deep learning using concepts. It suggests a strategy for balancing accuracy and coverage by abstaining from predictions in situations where errors can be costly. The proposed approach involves creating a concept bottleneck model, enabling the front-end model to use soft concepts and improving coverage and performance through concept confirmation. The paper presents techniques for handling uncertainty and pinpointing concepts for confirmation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper exhibits good clarity in its articulation, with ideas clearly presented and structured in an organized manner. Exploring the integration of user feedback into ML models to enhance accuracy and ensure broad coverage is intriguing and holds significance for ML models in real life usage. Furthermore, the paper touches interpretability in machine learning, which is an important aspect for ML models in real life."
                },
                "weaknesses": {
                    "value": "1. The abstract should be expanded to encompass key concepts that effectively summarize the paper's contributions. In the introduction, the authors emphasize the significance of interpretability and the challenges it poses in achieving high accuracy. By including these vital points in the abstract, the paper can provide a more comprehensive overview of its content and contributions.\n\n2. Regarding the abstention process, it appears to be based on a prediction probability threshold, where if the probability is lower than the threshold, the prediction is abstained? How does it different from a decision threshold used by the models? Can authors clarify that?\n\n3. In the results and discussion section, there's limited exploration and commentary on the impact of the solution on system accuracy, as seen in Table 2. Notably, the confirmation budget appears to have a limited effect on datasets like \"noisyconcepts25\" and \"warbler\" compared to others. The paper can delve into the reasons behind this discrepancy.\n\n4. In real-world applications of this solution, questions about the ease of concept approval and handling conflicting user feedback arise. While these aspects may be considered out of scope, addressing them would be beneficial for evaluating the practicality of implementing this approach in real-world scenarios. This is particularly important when considering the potential challenges of user feedback and conflicting inputs in such applications.\n\nMinor things:\nPage 4, confirm. we \u2014> replace . with comma\nSection 4.2, Table Table 2 \u2014> Table 2\nShouldn\u2019t Table 2 rather be labelled as Figure 2?"
                },
                "questions": {
                    "value": "stated above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2858/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2858/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2858/Reviewer_1z66"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418145404,
            "cdate": 1698418145404,
            "tmdate": 1699636229482,
            "mdate": 1699636229482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cVsBN3ajN5",
                "forum": "t8cBsT9mcg",
                "replyto": "WWPCitXMwH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback and thoughtful comments! We address your comments and questions below. \n\n> Regarding the abstention process, it appears to be based on a prediction probability threshold, where if the probability is lower than the threshold, the prediction is abstained? How does it different from a decision threshold used by the models?\n\nThe prediction probability threshold $\\tau$ functions similarly to a decision threshold in that it maps the raw output of a model to a set of outcomes. In the traditional binary classification task, a decision threshold would map a score to one of two classes (0 or 1). In our setting, $\\tau$ is used to map a predicted probability to one of three classes (0, 1, or abstain).\n\nThis extension is particularly significant in our setup, where we apply the abstention threshold to predicted probabilities from a calibrated classifier. In this case, the value of $\\tau$ can be set so that the model abstains from a decision when the prediction probability does not meet the established confidence level. More generally,  it can be tuned to control for the coverage or selective risk (similarly to how a decision threshold can be tuned to control for TPR/FPR).\n\n\n> The abstract should be expanded to encompass key concepts that effectively summarize the paper's contributions . . . [such as] the significance of interpretability and the challenges it poses in achieving high accuracy\n\nThank you for bringing this to our attention. We agree! We plan to revise the abstract more clearly following the discussion with the reviewers. \n\n> In the results and discussion section, there's limited exploration and commentary on the impact of the solution on system accuracy, as seen in Table 2. Notably, the confirmation budget appears to have a limited effect on datasets like \"noisyconcepts25\" and \"warbler\" compared to others.\n\nThank you for bringing up this point! \n\nTo be clear, this is indeed the correct interpretation of our results \u2013 i.e., there is a limited gain from confirmation on \"noisyconcepts25\" and \"warbler\" datasets. In practice, this can arise for several reasons. \n\n- The concept detectors perform well enough without confirmation to achieve high accuracy (this is the case for the \"noisyconcepts25\" and \"warbler\" datasets).\n- The dataset contains a large number of instances where confirmation cannot lead to gains. This can happen when, for example, instances for which we confirm concepts are inherently uncertain \u2013 i.e. so that $\\textrm{Pr}(y \\mid c)$ is large even when we know ground truth $c$.\n- The confirmation policy is not working well \u2013 i.e., it is somehow selecting the wrong instances for confirmation. This is not the case for the datasets that you listed. If this were the case, for example, we would expect a large gain in accuracy once we increase the confirmation budget from 75% to 100%.\n\nWe\u2019ve added these points to our discussion in Section 4.\n\n> In real-world applications of this solution, questions about the ease of concept approval and handling conflicting user feedback arise. While these aspects may be considered out of scope, addressing them would be beneficial for evaluating the practicality of implementing this approach in real-world scenarios. \n\nThank you for bringing up this point.\n\nIn our approach, we primarily focus on what we term \"deterministic concepts.\" These are concepts where there is a high degree of consensus among informed individuals. For instance, in medical imaging, the identification of a bone spur on an X-ray is generally agreed upon by radiologists. Among non-expert users, identifying that a bird has a red wing (rather than a black wing) is likely to lead to consensus. These examples represent areas where expertise leads to a high degree of agreement. It's crucial to note that it's not necessary for every human to agree; rather, the consensus should exist among those with relevant expertise. For example, in medical applications, we would rely on the agreement among medical professionals. By focusing on such concepts, we can set appropriate controls for concept selection and confirmation, effectively managing the costs and complexities involved. We plan to include a comprehensive list and categorization of these concepts in our paper to enhance clarity and facilitate a more practical evaluation of our approach in real-world scenarios.\n\n\n> Minor things\n\nThank you for pointing these out! We\u2019ve made corrections in the updated version of the paper according to your comments. We\u2019ve left Table 2 as a Table of figures rather than a Figure, though we appreciate your comment that this may be confusing."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248679267,
                "cdate": 1700248679267,
                "tmdate": 1700248679267,
                "mdate": 1700248679267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PClmqGeQy8",
            "forum": "t8cBsT9mcg",
            "replyto": "t8cBsT9mcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2858/Reviewer_msQs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2858/Reviewer_msQs"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a classification system that uses the combined approaches of a conceptual bottleneck and abstaining outputs to increase the reliability of models. The conceptual bottleneck approach trains a classification model for each concept identified in the training data. The end-model is a classifier that uses the presence or absence of concepts to make the target classification. The abstain mechanism allows the end-model to abstain from prediction.  When the model is uncertain about the presence of a concept, it may query the user for confirmation, thereby increasing trust and performance. Concept uncertainies are propagated through the end-model by using concept identification model scores as probabilities and sampling over potential concept vectors. This also improves performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The three strengths of the proposed approach are a functional abstaining method, requests for confirmation, and uncertainty propagation. Together these methods raise a classification model to something that is more intelligent, capable of some corrective action when faced with unusual inputs."
                },
                "weaknesses": {
                    "value": "1. The uncertainty propagation methodology doesn't seem computationally efficient.\n2. The performance of the default classifier (always predict majority class, uniformly randomly abstain) ought to be included in Table 2. The default performance ought to always be presented when using accuracy as a performance metric."
                },
                "questions": {
                    "value": "Can a deeper analysis of the consequences of abstaining be provided? Abstaining almost always improves average performance on the remaining predictions. Reporting the average is almost illusory, since those non-abstain predictions would have been correct or incorrect regardless. Rather, there is a real cost associated with refusing to provide an answer. The benefit is that the model reduces risk of error, but the costs are application dependent. How can we think about these costs in a constructive manner?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705738834,
            "cdate": 1698705738834,
            "tmdate": 1699636229413,
            "mdate": 1699636229413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "STN8AjapT8",
                "forum": "t8cBsT9mcg",
                "replyto": "PClmqGeQy8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and thoughtful comments! We address your comments and questions below and included an updated version with additional experiments. \n\n> The performance of the default classifier (always predict majority class, uniformly randomly abstain) ought to be included in Table 2\n\nThank you for pointing this out! We agree. In response, we have included the coverage-accuracy curve of a baseline model that employs uniform random abstention in Table 2's results. This baseline is designated as \"Random\" in the table's legend and is represented by a dark gray line in the corresponding plots. Additionally, we intend to incorporate a baseline that predicts the majority class into the supplementary results on multiclass datasets presented in Appendix A.3 in our next revision.\n\n> The uncertainty propagation methodology doesn't seem computationally efficient.\n\nThis is true \u2013 the uncertainty propagation method presented in section 3.1 requires $2^m$ calls to the front-end model, where $m$ is the number of concepts. However, in practice, Monte Carlo sampling can be used whenever the number of concepts is larger than is computationally feasible for exact calculation. In our experiments, we found that the Monte Carlo estimation converged quickly to the true value for larger datasets, and were able to run experiments across all datasets in a matter of minutes using a single GPU. Thank you for pointing out that this may not be clear on an initial read \u2013 we\u2019ve updated section 4.1 accordingly.\n\n> There is a real cost associated with refusing to provide an answer. The benefit is that the model reduces risk of error, but the costs are application-dependent. How can we think about these costs in a constructive manner?\n\nThis is a great point and actually part of the reason why we focus on \"automation\" as a motivating class of applications \u2013 i.e., applications where we would build a deep learning model to automate a routine task that a human expert can perform, like detecting pneumonia from a chest x-ray or melanoma from an image of a skin lesion.\n\nIn these applications, there is a constructive role for abstention for two reasons. First, accuracy is a major bottleneck to deploying any predictive model - i.e., even a model that achieves a 10% error rate may not be accurate enough to deploy in a medical setting. Second, there is a clear baseline option for what to do when we abstain. If we did not use a predictive model, then we can continue to have medical professionals perform routine yet important tasks.\n\nWith this in mind, we think that the right framing is to associate a \u201cgain\u201d with any prediction rather than a \u201ccost\u201d with an abstention. Put differently, using existing methods, we may not be able to automate at all. Using abstention, however, we can automate at least some of the instances. This is also the broader case that surrounds some recent work on automation - see, e.g., Feng, Jean, et al. \"Selective prediction-set models with coverage guarantees.\" Biometrics 79.2 (2023): 811-825. \n\nIn our manuscript, we follow a \u201cbounded risk\u201d setup for selective classification where practitioners seek to build a system that will maximize coverage while ensuring that its performance exceeds a desired level of accuracy. In other words, we seek to minimize the costs associated with abstention. In comparison to traditional selective classification, the one benefit of our approach is that we can \u201cconfirm\u201d to further improve coverage.  An alternative setup for selective classification is to set fixed costs associated with misclassification and with abstention and to minimize their additive costs. We think this is also a valid setup, but note that in many applications it is difficult to specify these quantities.\n\nWe hope that this provides some answer to your question. We plan to express this more clearly in our manuscript and are happy to discuss more if needed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248499485,
                "cdate": 1700248499485,
                "tmdate": 1700248499485,
                "mdate": 1700248499485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "009PUVaWso",
                "forum": "t8cBsT9mcg",
                "replyto": "STN8AjapT8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2858/Reviewer_msQs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2858/Reviewer_msQs"
                ],
                "content": {
                    "title": {
                        "value": "Author responses"
                    },
                    "comment": {
                        "value": "I thank the authors for their responses and hard work. I will stand by my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665391441,
                "cdate": 1700665391441,
                "tmdate": 1700665391441,
                "mdate": 1700665391441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uVUSBFkXNO",
            "forum": "t8cBsT9mcg",
            "replyto": "t8cBsT9mcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2858/Reviewer_3F5i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2858/Reviewer_3F5i"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose the use of a concept bottleneck model as input for selective classification. Moreover, they propose a greedy algorithm to select concepts to be confirmed by human experts, with the objective to increase the coverage of the selective classifier while guaranteeing a minimum accuracy level of the selective classifier. They evaluate their method with competitive baselines using both synthetic and real datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The work appears to be the first to use concept bottleneck models to capture the uncertainty of the entire model for selective classification. Moreover, the idea of getting human feedback to confirm concepts to improve selective classification is quite interesting and adds to the increasing literature of human-in-the-loop algorithms. \n\nThe paper is very well organized, has a clear structure, and is nicely written. The authors clearly state their contributions as well as the assumptions of their method. They also provide a detailed description of the experimental setup and provide the code for reproducibility in an anonymized repo. The experimental evaluation seems comprehensive including experiments with on both synthetic and real datasets, as well as a  robustness analysis under violations of the Assumptions 1 and 2."
                },
                "weaknesses": {
                    "value": "Even though the meaning of coverage might be clear to experts in selective classification, it might be helpful to include a high level definition of coverage in the introduction, so that it is clear for a broader ML audience.  \n\nIn Proposition 4, the authors assume a perfectly calibrated predictor. However, in practice, perfect calibration is impossible. As a results, it would be useful to include theoretical results that complement proposition 4 that account for the calibration error a classifier.     \n\nStyle/Typos:\n1. Figure 3 has no caption.\n2. The style of citations and captions of tables and figures does not follow the ICRL author instructions."
                },
                "questions": {
                    "value": "1. Assuming that there is (small) calibration error of the predictions of the classifier, how would the results of proposition 4 change? \n2. It seems that abstention happens when $\\bar{y}_i = \\tau$. Could one also assume that abstention happens when $\\bar{y}_i \\in( \\tau_1, \\tau_2)$, that is when the prediction of the classifier is within some range? How could this affect the results of proposition 4, as well as the accuracy guarantees assuming a not perfectly calibrated classifier?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2858/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2858/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2858/Reviewer_3F5i"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826817803,
            "cdate": 1698826817803,
            "tmdate": 1699636229339,
            "mdate": 1699636229339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tpncigkDFu",
                "forum": "t8cBsT9mcg",
                "replyto": "uVUSBFkXNO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and feedback! We address your comments and questions individually below.\n\n> It would be useful to include theoretical results that complement proposition 4 that account for the calibration error a classifier . . . Assuming that there is (small) calibration error of the predictions of the classifier, how would the results of proposition 4 change?\n\nThis is an important point. In short, you are right that Proposition 4 assumes that the front-end model will output calibrated probability predictions. In practice, this is a reasonable assumption as we can usually build such models using standard post-hoc calibration techniques. In the event that the model is not calibrated, then Proposition 4 will hold only given the degree of calibration. For example, given a miscalibrated model such that $|Pr(y = 1 | p = t) - t| = \\epsilon > 0$ for some error rate $\\epsilon$, the accuracy in Proposition 4 is guaranteed to be $1 - \\tau - \\epsilon$ instead of $1 - \\tau$. \n\nWe can address this case by setting $\\tau$ using a more sophisticated method that works with \u201cconfidence scores\u201d that may or may not be calibrated. One such example is the \u201cSelection with Guaranteed Risk Control\u201d algorithm by Geifman and El-Yaniv [1], which can set threshold $\\tau$ so that the accuracy of the resulting system will exceed accuracy $r*$ with probability $1-\\delta$.\n\nIn both cases, we can obtain theoretical guarantees on the selective risk of the system under the standard PAC model. These guarantees would let us say that the accuracy of our system will surpass a user-specified threshold on accuracy when we set the abstention threshold to $\\tau$. \n\nWe\u2019ve included this discussion in our revision.\n\n\n> It seems that abstention happens when $\\bar{y}_i = \\tau$. \nCould one also assume that abstention happens when $\\bar{y}_i = (\\tau_1, \\tau_2)$?  That is when the prediction of the classifier is within some range?\n\nThis is actually the correct assumption. To be clear, given \\bar{y}_i we would operate as follows:\n\n- $\\bar{y}_i > \\tau \\implies$ predict ($\\hat{y}_i = 0$)\n- $\\bar{y}_i \\in (\\tau, 1-\\tau) \\implies$ abstain ($\\hat{y}_i = \\perp$)\n- $\\bar{y}_i < \\tau \\implies$ predict ($\\hat{y}_i = 1$)\n\n\n> It might be helpful to include a high-level definition of coverage in the introduction\n\nThanks for raising this point. We use coverage to denote the proportion of samples for which a predictor outputs a prediction rather than abstaining. In other words, given a model $h$, we define coverage as \n$\\mathrm{Coverage}(h) := \\mathrm{Pr}(\\hat{y}\\neq\\perp)$. In our problem setting, we seek to improve coverage by maximizing the proportion of samples on which the system can safely predict. We\u2019ve added these notes to the introduction.\n\n\n> Figure 3 has no caption.\n\nThanks for catching this! It looks like a compilation error. We\u2019ve fixed this in the latest manuscript.\n\n> Citation and Caption Style\n\nThanks for catching this as well! We\u2019ve adapted in the latest version.\n\n[1] Y Geifman, R El-Yaniv, Selective Classification for Deep Neural Networks NeurIPS 2017"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248395423,
                "cdate": 1700248395423,
                "tmdate": 1700262069294,
                "mdate": 1700262069294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rmMrLAR4sV",
                "forum": "t8cBsT9mcg",
                "replyto": "tpncigkDFu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2858/Reviewer_3F5i"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2858/Reviewer_3F5i"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for taking the time to answer my questions and revise the manuscript to address any issues."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657155173,
                "cdate": 1700657155173,
                "tmdate": 1700657155173,
                "mdate": 1700657155173,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VWmurjxnPM",
            "forum": "t8cBsT9mcg",
            "replyto": "t8cBsT9mcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2858/Reviewer_wCU7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2858/Reviewer_wCU7"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an approach to do selective classification in deep learning with concepts, by constructing a concept bottleneck model where the front end model can make predictions given soft concepts and leverage concept confirmations to improve coverage and performance under abstention."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors provide a good motivation and introduction. Authors also provide emperical validations on multiple datasets. The problem statement is very relevant to practical problems and provide an insight into how to automate classification tasks by making it safe and interpretable."
                },
                "weaknesses": {
                    "value": "The writing and flow could be improved better, some of them are raised in questions below. Table 1 is referenced in Section 1, however what the columns means is defined only in Section 2, which makes it harder to read the table meaning. \n\nIt would also be better to provide more details in the evaluation dataset around what each datasets means, and some statistics around it.\n\nIn my opinion the paper lacks novelty in terms of the innovation, and answers to the questions raised would help to understand better. Its not very clear about dataset statistics and how it changes and aligns with the interpretations that are presented."
                },
                "questions": {
                    "value": "In the introduction its mentioned the front model can make predictions given soft concepts, however later in the text its mentioned in Section 2 under: `Propagating Concept Uncertaininty` its mentioned the front-end model requires hard concepts as inputs, which is not very clear?\n\nIn Introduction, its not very clear why the two objectives would conflict with other, if there are papers to cite that would help to make the claim stronger?\n\nHow does the choice of models to more complex architectures change the performance of the system?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2858/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2858/Reviewer_wCU7",
                        "ICLR.cc/2024/Conference/Submission2858/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2858/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698909360906,
            "cdate": 1698909360906,
            "tmdate": 1700690862649,
            "mdate": 1700690862649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9JWt6zc9jp",
                "forum": "t8cBsT9mcg",
                "replyto": "VWmurjxnPM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2858/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and feedback! We respond to your comments below and have included an updated version of the paper with additional experiments.\n\n> How does the choice of models to more complex architectures change the performance of the system?\n\nThis is a good question. Based on your question, we ran ablation studies where we swapped the logistic regression concept detectors for multi-layer perceptron concept detectors and have included these results in the updated version of the paper under Appendix A.2. We found that using a more complex model can improve baseline performance, though improvements are task-dependent, and usually small compared to strategies like improving the confirmation policy or increasing the confirmation budget. For example, using an MLP instead of logistic regression improves performance across all strategies on the \"flycatcher\" dataset, with the greatest gains seen in the Baseline strategy. However, this improvement is small compared to increasing the confirmation budget from even 10\\% to 20\\%. Stepping back, one of the benefits of our proposed approach is that it provides some flexibility over the architecture used for concept detectors and the front-end model. \n\n> In the introduction it\u2019s mentioned the front model can make predictions given soft concepts, however later . . . [it\u2019s] mentioned the front-end model requires hard concepts as inputs, which is not very clear?\n\nWe can see how this may have been unclear and have updated Figure 1 and Section 2 to clarify this point. To be clear, traditional concept bottleneck models are designed to use hard concepts, and in conceptual safeguards, the front-end model is still trained on hard labels. One key innovation of conceptual safeguards is the ability to use soft labels at inference time using the uncertainty propagation technique in Section 3.  \n\n> In Introduction, its not very clear why the two objectives would conflict with other, if there are papers to cite that would help to make the claim stronger?\n\nThank you for bringing this to our attention. We are specifically discussing this conflict in the context of concept bottleneck models. In this case, we can substantially improve interpretability over traditional deep learning models because we can show how the model makes its predictions in terms of concepts. At the same time, this architecture leads to a loss of accuracy since datasets may not have a large number of useful concepts [see 1,2]. We\u2019ve rephrased the introduction and added these citations to make this more clear. \n\n> It would also be better to provide more details in the evaluation dataset around what each datasets means, and some statistics around it.\n\nWe included details on the datasets, including the source of the dataset, the task outcome description, the number of samples, the number of concepts, and the number of positive samples in Appendix A.1. Could you take a look and let us know if there anything else you\u2019d like to see?\n\n> Table 1 is referenced in Section 1, however what the columns means is defined only in Section 2\n\nThank you for raising this point \u2013 we\u2019ve adjusted the flow of the text to make this easier to read. \n\n[1] M Yuksekgonul et al. Post-hoc concept bottleneck models\n\n[2] C Yeh et al. On completeness-aware concept-based explanations in deep neural networks"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248288142,
                "cdate": 1700248288142,
                "tmdate": 1700249987184,
                "mdate": 1700249987184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oF58s5HEnM",
                "forum": "t8cBsT9mcg",
                "replyto": "VWmurjxnPM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2858/Reviewer_wCU7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2858/Reviewer_wCU7"
                ],
                "content": {
                    "title": {
                        "value": "Changing my assessment to accept"
                    },
                    "comment": {
                        "value": "Thanks authors for providing a detailed review. Having read the response to my questions and to other questions. I will udpate my assessment !"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2858/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690847001,
                "cdate": 1700690847001,
                "tmdate": 1700690847001,
                "mdate": 1700690847001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]