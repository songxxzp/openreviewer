[
    {
        "title": "W\u00fcrstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models"
    },
    {
        "review": {
            "id": "7wpwhMqeGQ",
            "forum": "gU58d5QeGv",
            "replyto": "gU58d5QeGv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
            ],
            "content": {
                "summary": {
                    "value": "This study presents an architecture designed for efficient text-to-image generation. The first text-conditional LDM produces a low-resolution latent map (Stage C), which is used for the second LDM for a high-resolution latent map (Stage B). This map is fed into a VQGAN-based decoder to produce a final image (Stage A), as performed in other LDM and SD models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The key distinction of this work from previous LDM and SD lies in the introduction of a two-stage latent diffusion process, facilitated by the Semantic Compressor. The authors argue that the additional guidance from low-resolution latent maps (Stage C) can help yield good results under a smaller training budget, compared to the conventional LDM framework's Stage B and Stage A.\n- I appreciate the efforts put into designing and training the Semantic Compressor and Stage C. This appears to be far from straightforward, representing methodological and empirical contributions."
                },
                "weaknesses": {
                    "value": "- I'm uncertain about the inference efficiency of this approach, as it appears to add an \"extra\" computation (Stage C) on top of the conventional LDM and SD (Stage B and Stage A). In particular, how could the proposed method achieve better inference time than SD-v2.1 in Figure 4? A detailed computational comparison would be beneficial for different components in the system (the text encoder, LDM(s), and image decoder) instead of just an overall process. \n- I think the Baseline LDM (trained for 25,000 GPU-hours (same as Stage C)) needs to be trained for GPU hours of Stage B + Stage C, given that both stages contribute to the final latent representation of the proposed method. More importantly, a baseline with the same architecture of the upper part in Stage B, Figure 3 (i.e., a conventional LDM obtained by just removing Stage C and the below part of Stage B, Figure 3) seems necessary to show the benefit of the proposed approach.\n- The parameter values in Table 2 might confuse readers due to inconsistencies in their presentation. For some models, like LDM, the table seems to consider all the parameters, including the text encoder. Yet, for other models such as the proposed method and SD, only the diffusion parameters are listed. I strongly suggest presenting the \"total\" parameters (because several components work together for a single text-to-image system) or, preferably, detailing both the \"total\" and diffusion parameters separately.\n- The popular MS-COCO benchmark has been conducted at the resolution of 256x256. Why did the authors change the resolution for IS in Table 2? In my experience, the resolution affects the metric scores. Furthermore, for some models (LDM, DALL-E, CogView), the IS results at 256x256 were reported. I also highly recommend including CLIP score.\n- I think the description \u201cBy conditioning Stage B on low-dimensional latent representations, we can effectively decode images from a 16x24x24 latent space to a resolution of 3x1024x1024, resulting in a total spatial compression of 42:1\u201d in page 5 looks incorrect or overclaimed, because Stage B also takes a high-resolution latent map, 4x256x256, as input.\n- The behavior of the proposed model seems less explored. The representative analysis with different classifier-free guidance scales to show the tradeoff between FID-CLIP score [SD, GLIDE, Imagen] is missing. Furthermore, it would be interesting to analyze the tradeoff between the number of sampling steps and generation quality.\n- Minors: The paper is fairly easy to follow, but I think a careful proofreading is necessary: many typos exist.\n  - x -> \u00d7 (in many parts)\n  - In stage B, we utilize a -> Stage B\n  - Inception Score (IC) -> IS"
                },
                "questions": {
                    "value": "Please refer to the Weaknesses in the above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4208/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4208/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698568355891,
            "cdate": 1698568355891,
            "tmdate": 1700282070717,
            "mdate": 1700282070717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2GKmqhIA7u",
                "forum": "gU58d5QeGv",
                "replyto": "7wpwhMqeGQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review. We uploaded a revision based on your feedback.\n\n> [...]  How could the proposed method achieve better inference time than SD-v2.1 in Figure 4? [...]\n\nThe compute to process a sampling step scales quadratically with resolution. Stage C, operates on very low resolution, making sampling steps much more efficient. Stage B, is strongly guided by the latents of Stage C (see Appendix E) and thus needs a much lower number of diffusion steps (12, in our experiments) to produce high-fidelity results.\nHowever, we acknowledge that this should be communicated better. To facilitate an easier understanding of this, we made the following changes to the paper:\n\nFirst, we provide a more detailed plot on the inference-time, splitting up the time spent on the different inference stages in Fig. 4. Second, we overhauled Tab. 2, to show the properties of the individual Stages B and C, instead of only showing Stage C (more on this later).\n\n>I think the Baseline LDM [...]  needs to be trained for GPU hours of Stage B + Stage C [...]\n\nWe also discussed this internally before submission, as both models are diffusion models. We eventually decided to train our baseline with Stage C\u2019s budget, as Stage C is the image generator (see Appendix E)\nTraining a second baseline is not possible for us, as the 36K GPU-hours is outside our available budget and would require approx. 3 weeks to complete, not including evaluation.\nAs a compromise, we are in the process of fine-tuning Baseline LDM for 11K  additional gpu-hours to match the compute of Stage B+C training. We plan to add this model as a second baseline to the camera-ready version, as the results will unfortunately not be ready in the discussion period.\n\n> The parameter values in Tab. 2 might confuse readers due to inconsistencies in their presentation [...]\n\nThank you for pointing this out. Based on your feedback, we added CLIP-Score as well as separating total and generative parameters. We list Stage B and C as requested.\n\n> I think the description [...] in page 5 looks incorrect or overclaimed, [...]\n\nWe think the core of this misunderstanding lies in the question of how Stage B and C interact. Intuitively, Stage C conditioning Stage B suggests that Stage B is generating the image while Stage C is merely guiding this process, or both stages forming a complex generative cascade. However, Stage B should be viewed as a latent super resolution model (employing denoising for the decoding process), not as a generative model. Stage B samples 4x256x256 latents, which  are initialized to random Gaussian noise. The semantic compressor, on the other hand, whose image representation lies at a 16x24x24 space, is not noised during sampling of Stage B. Effectively, the compressed 16x24x24 latents are passed as a conditioning to Stage B to be upsampled to 4x256x256 latents, which are then decoded to 3x1024x1024 by Stage A. This results in an image in pixel space which is 42 times larger that the latent representation of the same image in Stage C. In Appendix E we provide further empirical evidence for this, by decoding the image directly from Stage C\u2019s 16x24x24 latents using a simple convolutional network without diffusion, which, as we show, directly yields a lower-resolution version of the same image as when decoding the latents with Stage B and Stage A, demonstrating that Stage B indeed only acts as a superresolution upsampler for the encoded image.\n\n> [...] Why did the authors change the resolution for IS in Tab. 2? [...]  I also highly recommend including CLIP score.\n\nConcerning the CLIP-Score, we agree that this is a missing piece in our automated evaluation, we added it to Tab. 2 and more results can be found in Appendix B of the revised manuscript. We found only minor changes in CLIP score, especially between the SD models and W\u00fcrstchen.\nConcerning IS-Score, the generated images used to compute the IS are the same images used to compute the FID-score. We think this is a misunderstanding, as a matter of fact, we used the code of CogView in particular for computing IS to make reproducibility more straightforward: https://tinyurl.com/f8mn9f2a \nWe write 299x299 since In line 36 of the linked, public source-file, images are resized to 299x299 pixels, hence ensuring all images are evaluated at the same resolution (which is, in fact, the fixed-size input resolution of the used InceptionV3 model). Because of this implementation quirk, we decided to report the \u201ctrue\u201d input resolution of the Inception model in the paper, to enhance clarity and reproducibility. \n\n> The behavior of the proposed model seems less explored. [...]\n\nWe agree that a more in-depth analysis of hyperparameters is valuable. We hence addressed this in two ways: We added Appendix I containing FID-CLIP plots for classifier-free-guidance and sampling steps. We also added Appendix H, which is a more detailed analysis on the generative qualities of W\u00fcrstchen based on the results of Sec. 4.2."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233591929,
                "cdate": 1700233591929,
                "tmdate": 1700233591929,
                "mdate": 1700233591929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YXOhVnSNLU",
                "forum": "gU58d5QeGv",
                "replyto": "2GKmqhIA7u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ],
                "content": {
                    "title": {
                        "value": "Post-Rebuttal Review (1/N)"
                    },
                    "comment": {
                        "value": "Dear Authors, thank you for your time and effort in preparing the rebuttal. \n\nI believe the core value of this work does not lie in W\u00fcrstchen outperforming SD models, but rather in its ability to produce satisfactory images with reduced training resources, making it publicly accessible to researchers. The open-source contribution is significant and should deserve proper recognition. I am also thankful for and satisfied with the authors' rebuttal, and as a result, I am happy to increase my evaluation score from 6 to 8.\n\nBelow are my responses and further questions, listed in order of importance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282177856,
                "cdate": 1700282177856,
                "tmdate": 1700282177856,
                "mdate": 1700282177856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e4uDwQ5X3F",
                "forum": "gU58d5QeGv",
                "replyto": "7wpwhMqeGQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ],
                "content": {
                    "title": {
                        "value": "Post-Rebuttal Review (2/N)"
                    },
                    "comment": {
                        "value": ">Stage B should be viewed as a latent super resolution model (employing denoising for the decoding process), not as a generative model. Stage B samples 4x256x256 latents, which are initialized to random Gaussian noise. The semantic compressor, on the other hand, whose image representation lies at a 16x24x24 space, is not noised during sampling of Stage B. Effectively, the compressed 16x24x24 latents are passed as a conditioning to Stage B to be upsampled to 4x256x256 latents, which are then decoded to 3x1024x1024 by Stage A. This results in an image in pixel space which is 42 times larger that the latent representation of the same image in Stage C.\n\nI believe the primary difference between the authors' perspectives and mine lies in the interpretation of the inputs and the role of Stage B. In my view, the main input is the 4x256x256 latent code, to which diffusion noise is added and reverse denoising loss is applied, as illustrated in Fig 3. Text and image-embedding vectors serve as conditioning elements that guide this denoising process.\n\n--- \n>Stage B samples 4x256x256 latents, which are initialized to random Gaussian noise.\n\nAs far as I know, it\u2019s a conventional reverse denoising process. The SD UNets also take latent vectors sampled from noise as input: 64x64 latents for v1 and v2-base, 96x96 for v2, and 128x128 for SDXL, all under the guidance of only text embedding.\n\nIn my view, Stage B of W\u00fcrstchen expands this process by incorporating additional image conditioning. Stage B eases the training burden of handling 256x256 latents by utilizing extra image information. In this regard, I believe that a 1024/24=42x compression ratio might be an overstatement, and a more realistic consideration would be a 1024/256=4x compression ratio, as indicated in the caption of Fig 3 (Stage B is trained as a diffusion model inside Stage A\u2019s latent space, which is a f4-reduction ratio).\n\n--- \n>In Appendix E we provide further empirical evidence for this, by decoding the image directly from Stage C\u2019s 16x24x24 latents using a simple convolutional network without diffusion, which, as we show, directly yields a lower-resolution version of the same image as when decoding the latents with Stage B and Stage A, demonstrating that Stage B indeed only acts as a superresolution upsampler for the encoded image.\n\nThank you for the additional experiments, and I agree that Stage B serves as a latent super-resolution module.\n\nOne point to comment on, though, is that the results of Appendix E are from Stage B after it was **already trained to utilize (*) Stage C embedding**. I think that if Stage B were trained solely with text conditioning, as is the case with SD models, and **without the Stage C conditioning, it would likely be workable too**, as demonstrated by the many successes of SD/LDM models.\n- (*) I\u2019ve reviewed the description stating: \"Since the conditioning on the Semantic Compressor is randomly dropped during Stage B training, we also evaluate Stage B without the image condition of Stage C\". However, this impact largely depends on the dropping ratio. If it is designed to heavily rely on Stage C conditioning, then Stage B is naturally trained to recognize Stage C conditioning."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282398950,
                "cdate": 1700282398950,
                "tmdate": 1700283281688,
                "mdate": 1700283281688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Xp4ApnCm6",
                "forum": "gU58d5QeGv",
                "replyto": "7wpwhMqeGQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ],
                "content": {
                    "title": {
                        "value": "Post-Rebuttal Review (3/N)"
                    },
                    "comment": {
                        "value": "> The behavior of the proposed model\n\nThank you for sincerely addressing this point and for the considerable additional effort. I also share the view that using FID to evaluate text-to-image models is not ideal, as it does not adequately account for image-text alignment and may not accurately reflect visual preferences. Fig 24(a) also demonstrates the potential of using W\u00fcrstchen with a reduced number of sampling steps. I appreciate the detailed discussion provided in Fig 24(b).\n\n---\n>parameter values in Tab. 2\n\nThank you for addressing this point. However, I have a further question. Upon checking the open-sourced W\u00fcrstchen, I noticed that it contains more than 3 billion parameters. Could you provide some clarification or comments on this?\n- W\u00fcrstchen in total: ~3110714522 = 3.1B model (reported as 2.4B in Table 2)~ -> 2.7B (revised after the below discussion)\n  - Stage A: vqgan.up_blocks 15704526 + vqgan.out_blocks 2316\n  - Stage B: decoder 1055365280 + ~its text encoder 352984064~\n  - Stage C: prior 993636896 + its text encoder 693021440\n- SDXL in total: 3434.7M = 3.4B model (reported as 2.8B in Table 2)\n  - UNet 2567.5M + Text Encoder-1 123.1M + Text Encoder-2 694.7M + Image Decoder 49.5M\n- cf. SD 1.4 (1.1B) and SD 2.1 (1.3B) in Table 2: the sum of UNet + Text Encoder + Image Decoder\n\n---\n>inference time in Figure 4\n\nI appreciate the clarification, and now I understand how W\u00fcrstchen achieves speedups by dividing the sampling burdens between the low-dimensional Stage C and the high-dimensional Stage B (enabling fewer steps in Stage B). However, I would like to comment that, based on my experience, 30 steps of SD models are often sufficient to generate satisfactory images.\n\n---\n> I think the Baseline LDM [...] needs to be trained for GPU hours of Stage B + Stage C [...]\n\nThank you for sharing the background story and the promising alternative. I look forward to seeing the new baseline in a future version.\n\n---\n> Why did the authors change the resolution for IS in Tab. 2? [...] I also highly recommend including CLIP score.\n\nThanks for clarifying this point and adding CLIP scores. Writing 299x299 resolution for IS is understandable."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283163435,
                "cdate": 1700283163435,
                "tmdate": 1700586473820,
                "mdate": 1700586473820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7RUXsRsNUk",
                "forum": "gU58d5QeGv",
                "replyto": "DYwakiEwJ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ],
                "content": {
                    "comment": {
                        "value": ">updated Table 2 ... However, our current model architecture, as presented in the paper, is designed to have only a single text encoder that is shared by both stages. This is also reflected in the parameter count (2.7B parameters vs. 3B.).\n\nThanks for the clarification. I have accordingly revised the parameter count in my earlier comment.\n\n>Appendix J: what is more surprising to us is that dropping Stage B\u2019s text-conditioning on the trained model **without adaption** is not significantly impacting generative performance. Essentially, it seems like Stage B has implicitly learned to ignore text-conditioning as the information is already contained and in the Stage C latents.\n\nI appreciate the extra effort and interesting results. Both quantitative and qualitative results seem enough to show that Stage B\u2019s text encoder is removable.\n\n---\nOverall, I believe the introduction of Stage C for distributing sampling burdens, along with thorough (and enhanced) analyses and open-source contribution, will be advantageous to the relevant research community."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589553995,
                "cdate": 1700589553995,
                "tmdate": 1700589553995,
                "mdate": 1700589553995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KsQL14YXIH",
            "forum": "gU58d5QeGv",
            "replyto": "gU58d5QeGv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4208/Reviewer_ix2S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4208/Reviewer_ix2S"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new text-to-image diffusion model architecture in which a base diffusion model is conditioned on a highly compressed 2D latent space obtained from a second diffusion model. Concretely, the \"main\" diffusion model denoises a higher-resolution image (e.g., 256x256 latent or pixel space) but is being conditioned on 24x24 feature map of the image that is to be generated. The 24x24 feature map is obtained by another diffusion model that is trained on that feature space. The resulting model is faster to train and faster to sample from, since both training and sampling of the 24x24 diffusion model is cheap and the large diffusion model at higher resolution benefits from the additional conditioning of the first diffusion model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The model architecture seems novel and based on the evaluation it seems to be faster to sample from while also being faster to train than other baseline models.\n\nThe paper builds on top of the latent diffusion architecture and outperforms similarly sized LDMs (and even Stable Diffusion 1 and 2) based on quantitative metrics and human user studies. Importantly, it does so while being faster to train and faster to sample from.\nThe evaluation is well done and compares against several strong baselines, performs severfal human user studies, and also highlights some weaknesses of the current model compared to other models (e.g. fewer high-frequency details).\n\nFurthermore, the model and code to reproduce will be released."
                },
                "weaknesses": {
                    "value": "The approach is only tested on latent diffusion models. While there is no reason to believe it wouldn't work on pixel diffusion models it would be nice to verify this."
                },
                "questions": {
                    "value": "Since the Semantic Compressor is one of the main novelties I wonder if you tested other feature extractors (e.g., could also use CLIP or Dino) and how that would affect training and quality. Or by simply training an autoencoder with strong compression rate instead of using a pretrained feature extractor?\nAlso, did you try other model architectures for the Stage C model (e.g., transformer based models) instead of only the ConvNext blocks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799110557,
            "cdate": 1698799110557,
            "tmdate": 1699636387798,
            "mdate": 1699636387798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b5eXLvWVtG",
                "forum": "gU58d5QeGv",
                "replyto": "KsQL14YXIH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> *\"The approach is only tested on latent diffusion models. While there is no reason to believe it wouldn't work on pixel diffusion models it would be nice to verify this.\"*\n\nWe agree with the reviewer that this would be interesting. Indeed, we have attempted a pixel-wise Stage B during early development. In these early trials, we found training to be slower to converge, and less efficient, which is why we ultimately decided to move on to latent diffusion models. However, a very recent publication has done something very similar with pixel diffusion models independently of us with their Matryoshka Diffusion Models (MDM) [1]. While MDM  models have a slightly different architecture, the core idea is very similar to our approach and their results as well as ours indicate that W\u00fcrstchen could be used in pixel-space. We added a reference to this work in our related work section in the revised manuscript.\n\n\n[1] Matryoshka Diffusion Models Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, Navdeep Jaitly, https://arxiv.org/abs/2310.15111, 23rd October 2023\n\n\n\n> *\"Since the Semantic Compressor is one of the main novelties, I wonder if you tested other feature extractors (e.g., could also use CLIP or Dino) and how that would affect training and quality. [...] Did you try other model architectures for the Stage C model (e.g., transformer based models) instead of only the ConvNext blocks?\"*\n\nInterestingly, we found that lower capacity semantic compressors did not impact the quality of the reconstruction negatively during our early exploration. For this reason, we switched from EfficientNetV2 L (120M parameters) to EfficientNetV2 S (22M parameters) to improve training efficiency in the early stages of development.\nDue to the considerable cost of training W\u00fcrstchen (approx.80.000-100.000$ per full training based on current AWS pricing), we were unable to study the effects on semantic compressor backbones and capacity thoroughly. For the same reason, we did not experiment with the Stage C architecture.\nWe hope to explore these avenues in future works."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233049802,
                "cdate": 1700233049802,
                "tmdate": 1700233134214,
                "mdate": 1700233134214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tq2Za8aPzx",
            "forum": "gU58d5QeGv",
            "replyto": "gU58d5QeGv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4208/Reviewer_b5h8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4208/Reviewer_b5h8"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new latent representation for images that can serve as compact semantic guidance for the current denoising diffusion process. Specifically, the proposed Wurstchen framework employs three stages of decoupling text-conditional image generation from high-resolution spaces. This supports an efficient optimization, which significantly reduces computational requirements for large-scale training. This architecture also enables faster inference."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ This paper is well-written and easy to follow.\n+ The field of efficient training is less discussed than inference, which makes this draft more valuable.\n+ The Wurstchen framework can reduce ~9X GPU training hours yet maintain competitive T2I performance.\n+ They provide comprehensive qualitative examples in the supplementary. The released code and checkpoint can benefit generative AI research."
                },
                "weaknesses": {
                    "value": "I am satisfied with the current draft. As it targets robust latent visual representations, there should be a detailed analysis (e.g., the quality of the latent features / the distribution of the compression space). This can make its claim more convincing."
                },
                "questions": {
                    "value": "Please see the Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Wurstchen is trained on LAION-5B, which may contain potentially harmful data and influence the trained T2I model."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699044710623,
            "cdate": 1699044710623,
            "tmdate": 1699636387714,
            "mdate": 1699636387714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "81TQu7eK0t",
                "forum": "gU58d5QeGv",
                "replyto": "Tq2Za8aPzx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are very glad about this very positive assessment of our work. Indeed, we also perceive the strengths the reviewer identified as the major contributions to our field. We have uploaded an updated version of our paper with additional content based on your review. \n\n\nWe also agree that an in-depth analysis of the compressed latent space would be insightful. Yet, as we enforce a normalized latent space (mean of 0, standard deviation of 1) of Stage C analyzing the distributions of the latent representation directly will not yield meaningful results. \nHowever, we expanded our analysis on the generative qualities further by analyzing categories and difficulty groups of partiprompts individually (Appendix H), to provide a more nuanced picture of the generative qualities of our model.\nIn a similar vein, we added FID-CLIP-score plots to characterize the model's behavior at different classifier-free-guidance scales and number of sampling steps (Appendix I). We also added collages for randomly sampled images for both ablation studies.\n\n\n\n\nWe also noted that this reviewer has flagged us for an ethics review due to the use of LAION-5B, and, since we spend significant effort into mitigating ethical risks that might have been introduced by the use of this dataset, we would like to take the chance to briefly comment on the measures we introduced in our training process to address potentially harmful impacts. We have also detailed these steps in the revised manuscript in Appendix G.\nThe version of LAION-5B available to the authors was aggressively de-duplicated and pre-filtered for harmful, NSFW and watermarked content using binary image-classifiers (watermark filtering), clip models (NSFW, aesthetic properties) and black-lists for URLs and words, reducing the raw dataset down to 699M images (12.06 % of the original dataset). \nHowever, since we found out that there was still a non-negligible image portion that contained images that fall within the NSFW category, during training, we applied an even more aggressive filter threshold on the NSFW and aesthetic properties, and also dropped all images of low resolution (smaller than 512x512 px).\nThe combination of these filters further reduced the number of unique image-text pairs to 103 million, which is approx. 1.78% of LAION-5B.\nWe acknowledge that this filtering is based on automated algorithms and due to the size of the dataset, we cannot guarantee the absence of false negatives. However, we think it is important to mention that we are aware of ethical concerns surrounding the LAION-5B dataset, and we tried to mitigate them as much as possible. \n\n\nFinally, we also want to mention that we think our increase in data efficiency (our model was trained on only 103M unique image-text-pairs) is an important step to making text-to-image model training more ethical, as the curation of datasets becomes significantly easier at smaller scales.\nVery recent publications like CommonCanvas [1] demonstrate that data at this scale is not far away from reaching a size that can be curated to a much higher standard.\n\n[1] CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images\nAaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, Volodymyr Kuleshov, https://arxiv.org/abs/2310.16825, 25th October 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232928198,
                "cdate": 1700232928198,
                "tmdate": 1700232928198,
                "mdate": 1700232928198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "62aSQMjc3L",
                "forum": "gU58d5QeGv",
                "replyto": "81TQu7eK0t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_b5h8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_b5h8"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the clarification, which resolves my concerns."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503899647,
                "cdate": 1700503899647,
                "tmdate": 1700503899647,
                "mdate": 1700503899647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QE5sUOsWuv",
            "forum": "gU58d5QeGv",
            "replyto": "gU58d5QeGv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4208/Reviewer_CAkw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4208/Reviewer_CAkw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an efficient architecture for large-scale text-to-image diffusion models. It presents a novel text-to-image generation model that utilizes a three-stage process for improved efficiency and superior output quality. With its unique ability to separate text-conditional generation from high-resolution projection, this model demonstrates superior performance over existing models, requiring fewer computational resources without compromising image quality. Evaluations with both automated metrics and human assessments substantiate its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) This study tackles an important topic of reducing the computational cost of text-to-image diffusion models.\n(2) The method introduced in the study is both innovative and efficient, offering clear results and validating its effectiveness through extensive evaluations.\n(3) The paper is well written, and one can quickly grasp the main idea and technical designs."
                },
                "weaknesses": {
                    "value": "(1) Ablation study is missing. An understanding of the impact of different model components on the final results is desired.\n(2) For automatic evaluation metrics in Section 4.1, only FID and Inception score are evaluated, and there are no metrics evaluating how well the generated images are aligned with the input text instructions, such as CLIPScore.\n(3) The paper does not elaborate on the possible limitations or potential failure cases of the proposed method. Could the authors clarify this aspect?"
                },
                "questions": {
                    "value": "Please refer to the weakness section. I expect the authors to clarify the questions about the ablation study and evaluation metrics in the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4208/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699260231681,
            "cdate": 1699260231681,
            "tmdate": 1699636387648,
            "mdate": 1699636387648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dODw9m8lft",
                "forum": "gU58d5QeGv",
                "replyto": "QE5sUOsWuv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4208/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We want to address all three points highlighted in the weakness section. We uploaded a new version of the paper with additional content based on your suggestions.\n\n\n> *(1)  Ablation study is missing. An understanding of the impact of different model components on the final results is desired.*\n\nAs suggested by the reviewer, we expand on the investigation of the different model components in the revised manuscript (due to length constraints in the appendices) and added **three** ablation experiments.  \n- We ablate classifier-free-guidance and the number of sampling steps with respect to text-alignment and image quality in Appendix I.\n- We show that Stage C can be considered the image generator, while Stage B acts as a latent super resolution model in Appendix E. \n- We performed an ablation study investigating the effect of dropping text-conditioning on Stage B and Stage C, respectively, in Appendix J. \n\nWhile, arguably, more ablation experiments could always be performed, we want to highlight that we undertook **significant effort** in the rebuttal period following this criticism by the reviewer.\n\nMore extensive studies, for example on the capacity of individual stages and the semantic compressor, would require multiple full training runs of the model, costing roughly 100.000$ (based on current AWS pricing) per full training, which would exceed our available compute budget by a significant margin.\n\n\n\n> *(2) [...] there are no metrics evaluating how well the generated images are aligned with the input text instructions*\n\nFollowing the recommendation of the reviewer, we provide MS-COCO CLIP-Score in Table 2 and on two additional datasets in Appendix B. We agree that an automated metric should be added showing the alignment of prompt and image directly. Technically, our third metric, the Pick-Score, considers prompts in the automated evaluation. However, Pick-Score also mixes general aesthetic qualities with text-alignment, making it insufficient for this purpose. \n\n\n\n> *(3) The paper does not elaborate on the possible limitations or potential failure cases of the proposed method.*\n\nFollowing this comment, we performed an in-depth investigation of our human evaluation experiment on the partiprompts dataset to identify failure patterns, both regarding the image category and the challenge class (see Appendix H of the revised supplementary material). \nWe indeed observe that W\u00fcrstchen is underperforming compared to Stable Diffusion 2.1 in some aspects, specifically in tasks involving prompts that ask for writing/text, symbols and prompts requesting specific quantities of objects. In other words, tasks where the composition of patterns and objects in relation to each other is particularly important. \nWe attribute this to the fact that Stage C operated on a very high compression, making fine-grained composition more challenging compared to Stable Diffusion 2.1. Stage B could correct this but is shown to primarily act as a super-resolution model, which we demonstrate in a small study in Appendix E.\nWe also briefly elaborate in Section 4.2 and Appendix I on the fact that the heavy filtering of the data (see Appendix G) results in a characteristic style for the generated images, which may not be desirable for a general image generator."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4208/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232882501,
                "cdate": 1700232882501,
                "tmdate": 1700552128271,
                "mdate": 1700552128271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]