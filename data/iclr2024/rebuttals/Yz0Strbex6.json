[
    {
        "title": "A Note on Some Statistical Properties of Signature Transform Under Stochastic Integrals"
    },
    {
        "review": {
            "id": "ZH90hKhih0",
            "forum": "Yz0Strbex6",
            "replyto": "Yz0Strbex6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5769/Reviewer_otaH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5769/Reviewer_otaH"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates signature transforms of time series data using a Lasso regression framework:\n\nFor time series resembling Brownian motion with weak correlations, Lasso regression aligns better with Ito integrals.\n For mean-reverting processes, Stratonovich integrals are more consistent. \n\nThe paper supports the theoretical findings with numerical experiments on synthetic data like Brownian motion and OU processes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is very well written, with a great review for signatures. \n\nThe propositions and the numerical results are well presented."
                },
                "weaknesses": {
                    "value": "The paper is mainly concerned with the whether Ito or Stratonovic signatures to combined with LASSO will result in a higher statistical consistency. However, the investigation is limited to very specific examples (such as Brownian motions and OU processes). It would have been a stronger paper if it provided a more detailed guideline as to when each of these should be applied. Even an extensive empirical study leading to an intuitively appealing empirically supported guideline would have made the paper stronger."
                },
                "questions": {
                    "value": "While it is obvious from Figure 1 that Ito signatures are more consistent, the conclusion that Stratonovic signatures are more consistent can not be drawn from Figure 2 (as far as I can see).  Is there a theoretical explanation as to why this is happening?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5769/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673375552,
            "cdate": 1698673375552,
            "tmdate": 1699636606047,
            "mdate": 1699636606047,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xc6Fov6GDY",
                "forum": "Yz0Strbex6",
                "replyto": "ZH90hKhih0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer otaH's Comments"
                    },
                    "comment": {
                        "value": "> ***Strengths.***\n\nWe thank the reviewer for appraising the writing and presentation of our paper. \n\n> ***Weaknesses: Detailed Guideline and Empirical Study***\n\nWe thank the reviewer for the comment. While our theoretical work is based on the Brownian motion and the OU process, our numerical studies also investigate the random walk (Figure 1b), AR(1) process (Figure 2b), ARIMA process (Appendix J), and time-augmented process (Appendix H). These cover many commonly used models of time series. Our main conclusions (\"Our study shows that, for processes and time series that are closer to Brownian motion or random walk with weaker inter-dimensional correlations, the Lasso regression is more consistent for their signatures defined by It\u00f4 integrals; for mean reverting processes and time series, their signatures defined by Stratonovich integrals have more consistency in the Lasso regression.\") in the Abstract of our paper are also based on the analysis of these results. \n\nWe agree with the reviewer that conducting an empirical study would be helpful. However, we would like to point out that, with all due respect, empirical studies would not allow us to compare consistency across different definitions of the signature because it is impossible to measure consistency without knowing the true data generating process. In other words, the consistency has to be verified on a simulated dataset where the ground truth of the set of true features is known. In fact, most classical literature on Lasso consistency, including works like [1,2,3], did not incorporate empirical studies. Therefore, following this strand of literature, we focused on theoretical analysis and numerical studies.\n\n> ***Questions: Conclusion of Figure 2***\n\nWe thank the reviewer's comment. We would like to clarify that in Figure 2, we did not conclude that Stratonovich signatures are consistently superior in all cases. Instead, our conclusion is that Stratonovich signatures are more consistent *when the process is sufficiently mean reverting*. In particular, in the paragraph following Figure 2, we stated the following observations:\n\n- \"First, the  It\u00f4 signature reaches the highest consistency rate when $\\kappa$ and $1-\\phi$ approach $0$, which correspond respectively to a Brownian motion and a random walk.\" This means that, when $\\kappa$ is small, the result is similar to a Brownian motion, and thus, It\u00f4 signatures are more consistent (the result of Figure 1). This can be observed in Figure 2 that the solid lines (It\u00f4) are higher when $\\kappa$ is small.\n- \"Second, when the process is sufficiently mean reverting,  Stratonovich signatures have higher consistency rates than It\u00f4 signatures.\" This means that, when $\\kappa$ is large (\"sufficiently mean reverting\"), Stratonovich signatures are more consistent. This can be observed in Figure 2 that the dashed lines (Stratonovich) are higher when $\\kappa$ is large. \n\n\nTheoretically, this result can be attributed to the fact that, for the OU process, the collinearity between It\u00f4 signatures increases as $\\kappa$ increases, leading to lower consistency. In contrast, the collinearity between Stratonovich signatures generally remains stable across different values of $\\kappa$, which leads to their consistent performance. This theoretical explanation is included in Appendix D. \n\n**References:**\n\n[1] Zhao, P., and B. Yu, 2006, On model selection consistency of Lasso, The Journal of Machine Learning Research, 7, 2541\u20132563.\n\n[2] Bickel, P. J., Y. Ritov, and A. B. Tsybakov, 2009, Simultaneous analysis of Lasso and Dantzig selector, The Annals of Statistics, 1705\u20131732.\n\n[3] Wainwright, M. J., 2009, Sharp thresholds for high-dimensional and noisy sparsity recovery using $l_1$-constrained quadratic programming (Lasso), IEEE Transactions on Information Theory 55, 2183\u20132202."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319211031,
                "cdate": 1700319211031,
                "tmdate": 1700319709394,
                "mdate": 1700319709394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I6e7XV84ER",
                "forum": "Yz0Strbex6",
                "replyto": "Xc6Fov6GDY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_otaH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_otaH"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your response. I read it carefully and I do not think that my comments are adequately addressed. I am keeping my rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360051848,
                "cdate": 1700360051848,
                "tmdate": 1700360051848,
                "mdate": 1700360051848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kfGQmtZTkR",
            "forum": "Yz0Strbex6",
            "replyto": "Yz0Strbex6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
            ],
            "content": {
                "summary": {
                    "value": "Signature transformation transforms a time series $\\mathbf{X}\\_n$ into point statistics $ S(\\mathbf{X}\\_n)\\_T^{\\ldots}$. In this paper, the authors study relationship between a random process (Brownian motion, random walk, OU process, and AR(1) model) underlying $\\mathbf{X}\\_n$, an integral method (It\u00f4  or Stratonovich) for defining the signature transformation, and result of signature-based Lasso: they discuss consistency properties of \u201cestimate of sign of parameter\u201d when performing Lasso regression for $\\\\{(S(\\mathbf{X}\\_n)\\_T^{\\ldots}, y\\_n)\\\\}\\_n$ with $y\\_n$ generated from a linear model with noise based on $ S(\\mathbf{X}\\_n)\\_T^{\\ldots}$. Through this study, the authors state \u201cOur study shows \u2026 the Lasso regression\u201d and a more generic fact \u201cOur findings highlight \u2026 and machine learning\u201d (both of which are written in abstract)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I have understood that the authors claim \u201cOur study shows \u2026 the Lasso regression\u201d (claim1) and a more generic fact \u201cOur findings highlight \u2026 and machine learning\u201d (claim2), both of which are written in Abstract. I have focused on verifying claim 1 only since claim 2 is too generic and trivial (if this is wrong, please let me know what problems have arisen due to existing studies\u2019 indifference to the integral method). According to this standpoint, I have wrote following comments.\n\n1. The paper is well-written (presentation is good).\n2. Propositions and Theorems are correct.\n3. Feature selection techniques may be a good option to improve regression based on signature transformations. This is because users will want to use somewhat large $K$ since an appropriate value of $K$ is not known in advance, but in that case the number $\\frac{d^{K+1}-1}{d-1}$ of predictors becomes large (when $d\\neq 1$).\n4. Discussion between Proposition 7 and Example 1 supports claim 1 to some extent.\n5. Program codes are reliable."
                },
                "weaknesses": {
                    "value": "6. I have thought that the sentence \u201cGiven the successful application \u2026 time series data\u201d expresses the motivation for this study. However, this motivation is too abstract. I would like to see a concrete description as to why discussion to support claim 1 is demanded.\n7. Most of the related studies seem to be published by the same research group. I am concerned about this point. For example, all the application paper cited in Section 1 (except for the arXiv paper (Arribas, 2018)) were published by the same research group. I cannot know if unrelated researchers recognize the usefulness of signature-based techniques in their applications. Please tell me about some interesting applications that unrelated researchers have done. Also, I think that the authors should cite such papers as well. I think that this is needed for an ICLR paper. This comment has nothing to do with whether the authors of this paper belong to that research group.\n8. In recent years, \u201cseries-to-point non-linear (neural-network-based) regression\u201d, which is an end-to-end regression in the setting of this paper, has been well studied. For example, studies referred in Section 3.3 of \u201cAhmed, S., Nielsen, I. E., Tripathi, A., Siddiqui, S., Ramachandran, R. P., & Rasool, G. (2023). Transformers in time-series analysis: A tutorial. Circuits, Systems, and Signal Processing, 42(12), 7433-7466\u201d. Is there a practical advantage to doing \u201cpoint-to-point linear regression via signature transformation\u201d? (I may have missed a cited paper that makes such a comparison. I'm sorry if that's the case. Please tell me that paper.) Considering readers of ICLR, it is worth mentioning the comparison between such techniques and signature-based ones. Also, the input dimension $\\frac{d^{K+1}-1}{d-1}$ of \u201cpoint-to-point linear regression via signature transformation\u201d can be larger than the input dimension $d T$ of \u201cend-to-end regression\u201d. Is this a negative factor of \u201cpoint-to-point linear regression via signature transformation\u201d?\n9. I would like to ask for additional explanation of interpretations of signature $ S(\\mathbf{X}\\_n)\\_T^{i\\_1,\\ldots,i\\_k}$ for each combination of $i\\_1,\\ldots,i\\_k$. There are generally two main types of evaluation strategies of Lasso: regression error or parameter estimate error. This research focuses on the latter, which means emphasizing the interpretability of the parameter estimate itself. For that position, the interpretability of signatures should be important.\n10. I could not understand the meaning of the experimental comparisons for Figures 1 and 2 to support claim1. When integral methods are different, the generated data (in particular, $\\\\{y\\_n\\\\}$) are different, and data analysis methods (the sets of features used for Lasso regression) are also different. For example, for data analysis in real situation, only one data is given, so analysis with different generated data is useless. In particular, since the comparison uses different data, I think that there is a gap between the experimental results and claim 1. Contrary, if the authors want to focus their discussion on the influence of the integral method on the distribution properties of the signature, it would be natural to consider with a single data analysis method. Please tell me more specifically what the authors want to claim with the experimental comparisons for Figures 1 and 2, and in what situations that claim is useful.\n11. Another problem is that consideration and explanation of reasons for the experimental results are not sufficiently described. (I imagine that multicollinearity among the explanatory variables $ S(\\mathbf{X})\\_T^{\\ldots}$ influences the experimental results; is it right?)\n12. Regarding \u201cOther feature selection techniques\u201d in Section 5. The ridge regression is not used for feature selection typically. Rather, it will be better to cite, for example, bridge regression (Frank, I. E., and Friedman, J. H. (1993), \u201cA Statistical View of Some Chemometrics Regression Tools,\u201d Technometrics, 35, 109\u2013135).\n13. $\\\\{\\epsilon\\_n\\\\}\\_n$ in (3) requires 0-mean assumption at least (otherwise $\\tilde{\\beta}\\_0$ will be biased)."
                },
                "questions": {
                    "value": "I also wrote questions in the previous item. Please respond to comments 6--13. I will raise my rating if I receive satisfactory responses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5769/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5769/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5769/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699159144052,
            "cdate": 1699159144052,
            "tmdate": 1699636605932,
            "mdate": 1699636605932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nXzrVpmLZq",
                "forum": "Yz0Strbex6",
                "replyto": "kfGQmtZTkR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer weWX's Comments"
                    },
                    "comment": {
                        "value": "> ***Strengths.***\n\nWe thank the reviewer for appraising the writing, rigor, and programming of our paper. \n\nWhile Claim 2 may appear to be generic or trivial, we would like to clarify that we believe it actually constitutes an important contribution from our work. To the best of our knowledge, no existing literature explicitly compares the differences in statistical performance between different signature definitions. In particular, most machine learning studies related to signatures use the default (Stratonovich) signatures directly without a comparative analysis with It\u00f4 signatures. We are the first to highlight the importance of choosing an appropriate definition of signatures both from a theoretical perspective and using numerical examples. We believe that the literature can achieve improved performance by adopting a more suitable signature definition for each of their specific application. Therefore, we consider the claim \"choosing appropriate definitions of signatures and stochastic models in statistical inference and machine learning\" (Claim 2) to be important and has often been overlooked in the past. \n\n\n> ***Weakness 6: Motivation***\n\nWe appreciate the reviewer's feedback and we agree wholeheartedly with the reviewer that establishing the motivation for our research is important. In the revised manuscript, we have modified the paragraph to better convey the motivation behind our work.\n\nWe believe that discussion to support Claim 1 is necessary for three key reasons. First, to the best of our knowledge, most empirical literature on signatures uses signatures directly without considering their underlying statistical properties. Second, the probabilistic foundation of empirical research using signatures, the universal nonlinearity, has been explored under different definitions of signatures. However, the associated statistical properties for different definitions of signatures have not been examined. Third, to our knowledge, most empirical research uses a default definition of signatures regardless of the specific scenarios under investigation. However, using an inappropriate signature definition may lead to suboptimal performance, as highlighted by our work. Together, these three reasons motivate us to study the statistical performance of signatures.  \n\n> ***Weakness 7: Literature***\n\nWe thank the reviewer's comment, and we fully agree with the reviewer that it is important to provide a more diverse set of references. In the revised manuscript, we have included references to research from a much wider set of research groups. For example, [1] makes a broad survey of signature applications and demonstrates the superior performance of signatures using diverse datasets including a drawing dataset (Quick, Draw!), a sound dataset (Urban Sound), and smartphone action sensory dataset (MotionSense); [2] uses the signature transform to extract speak-and-pause patterns and achieve superior results on three publicly available datasets; [3,4,5,6] explore the applications of the signature method in finance. \n\n> ***Weakness 8: Series-to-Point Regression***\n\nWe appreciate the reviewer's insightful question and agree that comparing neural-network-based regression with the signature-based model can make our paper more appealing to readers of ICLR. \n\n**Practical advantages of signature transform.** We believe that the signature transform has two practical advantages. First, it enhances computational efficiency in signature-based models because it only requires training a linear model; second, the linear model allows for interpretability. Essentially, the signature transform can be understood as a powerful method of feature engineering with sound theoretical properties. We have incorporated this comparison into Section 1 of the revised manuscript and have provided an interpretation of signatures in Appendix A. \n\n**Comparison between input dimensions.** We acknowledge the reviewer's concern about potentially higher input dimensions for the signature-based model ($\\frac{d^{K+1}-1}{d-1}$) compared to the neural-network-based model ($dT$). However, we do not think this is a negative factor for the signature-based model for two main reasons. \n\nFirst, signatures are computed using the original $dT$ input data. Therefore, $dT$ can also be regarded as the input dimension for the signature-based model. On the other hand, neural-network-based models typically involve a large number of parameters to be trained, significantly increasing their overall dimensions. In fact, as mentioned above, the signature transform itself is a feature engineering step that allows the subsequent step to use a simple linear model.\n\nSecond, $\\frac{d^{K+1}-1}{d-1}$ is not a concern in practice because, it has been documented that including signatures up to a small order $K$ usually suffices to achieve good performances [7,8] (see Section 2.2 of our paper)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317815269,
                "cdate": 1700317815269,
                "tmdate": 1700317815269,
                "mdate": 1700317815269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nr5pNxLBvm",
                "forum": "Yz0Strbex6",
                "replyto": "GEy1YQGk9H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response: 2nd comment"
                    },
                    "comment": {
                        "value": "**Weaknesses 7 and 11-13 and regarding claim 2 and \"Comparison between input dimensions\" in Weakness 8:**\n\nThank you for your responses.\nI am satisfied with these responses.\n\n**Weakness 6 and 10:**\n\nThank you for your responses.\nHowever, the revised description on the motivation remains abstract as that to support claim 1.\nWhat's the point of having high sign congruency rate? I do not recognize this point.\nFor example, consider an extreme example where data are generated with $S(\\mathbf{X})\\_T^{1}= S(\\mathbf{X})\\_T^{2}$, $\\beta\\_1=1$, and $\\beta\\_2=0$.\nUnder this example, difference between an estimate with $\\tilde{\\beta}\\_1=1, \\tilde{\\beta}\\_2=0$ and an estimate with $\\tilde{\\beta}\\_1=0, \\tilde{\\beta}\\_2=1$ is meaningless.\nThis example is too extreme, but I think that it has small significance to compare sign consistency rates under presence of multicollinearity.\n\nFor XdRo's Comments \"Weakness 2\", the authors replied \"We choose sign consistency as the starting point of our research because it is a basic and well-accepted metric for studying Lasso [4,5,6].\"\nHowever, in [4,5,6], the dependence of the consistency rate of the Lasso estimator on the number of training samples was only discussed (there, the data distribution and method were the same).\n\nI understand your response, but I still don't see any significance in comparing sign consistency rates for different data distributions or different methods.\nYou may provide additional responses, but I will probably not change my opinion on this point.\n\nThe current paper moves the discussion from the multicollinearity issue to a comparison of sign consistency rates.\nWe agree that the multicollinearity issue could be beneficial to claim 2.\nHow about moving the discussion from the multicollinearity issue to another topic?\nFor example, it might be useful if an argument such as \"to achieve the same level of prediction performance, $K$ can be smaller (more computationally efficient) when using an integral method such that the signatures have lower collinearity\" were provided along with real-world data experiments.\n(I am not suggesting that this submission should be rewritten to such discussions. In the case where this submission is rejected, please refer to it.)\n\n**\"Comparison between neural-network-based regression with the signature-based\" in Weakness 8:**\n\nI agree with the two points you mentioned as advantages of signature transformation-based point-to-point regression.\nHowever, I am more interested in whether that point-to-point regression can outperform neural-network-based series-to-point regression regarding the prediction performance.\nThis is because the two points mentioned by the authors are not often prioritized over predictive performance in practice in my sense.\nI do not require the introduction of experimental studies showing that signature-based regression significantly outperforms series-to-point regression with respect to prediction performance.\nI just want to be sure that the former can give a prediction performance comparable to the latter.\n\nAlso, the paper (Levin et al., 2016) (its initial version is published in 2013) is old.\nThe methodology of series-to-point regression has made significant progress in the last five years.\n\nI will comment again with more specificity:\nPlease cite studies that compare a series-to-point regression methods proposed after 2018 and signature transformation-based regression methods with respect to predictive performance.\n\n**Weakness 9:**\n\nIs there a difference between Ito integral and Stratonovich integral?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393138864,
                "cdate": 1700393138864,
                "tmdate": 1700393138864,
                "mdate": 1700393138864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oxg1pNqmFe",
                "forum": "Yz0Strbex6",
                "replyto": "akpjh2orpa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response: 3rd comment"
                    },
                    "comment": {
                        "value": "**\"Comparison between neural-network-based regression with the signature-based\" in Weakness 8**\n\nI am satisfied with the results of (Bleistein et al., 2023) and with citing it in your paper.\nSorry, I have overlooked it.\nHowever, I think that it may be better to cite such paper at 'the end of the 1st paragraph in Section 1' or around 'the sentence \"The nonlinearity property $\\ldots$ nonlinear methods (Ahmed et al., 2023).\" in the 2nd paragraph in Section 1'.\n\n**Weakness 9**\n\nThank you for your responses. I am satisfied with the response."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407623684,
                "cdate": 1700407623684,
                "tmdate": 1700407623684,
                "mdate": 1700407623684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0FBLGbv1vI",
                "forum": "Yz0Strbex6",
                "replyto": "kfGQmtZTkR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                ],
                "content": {
                    "title": {
                        "value": "Sorry 3rd comment was sloppy: 4th comment"
                    },
                    "comment": {
                        "value": "**\"Comparison between neural-network-based regression with the signature-based\" in Weakness 8.**\n\nSorry, the previous comment was inaccurate.\nI used 'the end of the 1st paragraph in Section 1' to imply the inside of \"including handwriting recognition $\\ldots$; Futter et al., 2023; Lemahieu et al., 2023)\".\nThe authors have increased the citation in the section regarding \"comprehensive reviews\".\nCiting it there is inappropriate.\nMy comment was sloppy. Sorry.\nI thought that it may be better to cite (Bleistein et al., 2023) in '\"including $\\ldots$\" of the 1st paragraph in Section 1' or around 'the sentence \"The nonlinearity property nonlinear methods (Ahmed et al., 2023).\" in the 2nd paragraph in Section 1'.\nMoreover, I think that multiple papers showing advantages over end-to-end neural-network-based, including (Bleistein et al., 2023), (if exists) should be cited around 'the sentence \"The nonlinearity property nonlinear methods (Ahmed et al., 2023).\" in the 2nd paragraph in Section 1'."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409564980,
                "cdate": 1700409564980,
                "tmdate": 1700410646377,
                "mdate": 1700410646377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zGeKJ0WRqI",
                "forum": "Yz0Strbex6",
                "replyto": "FyQC5wKy0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_weWX"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response: 5th (final) comment"
                    },
                    "comment": {
                        "value": "I get satisfied with the responses to all comments except \"Weaknesses 6 and 10\".\nRegarding \"Weaknesses 6 and 10\", I will share both the authors' attempts and my opinion with the area chair.\nGood luck."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411973386,
                "cdate": 1700411973386,
                "tmdate": 1700411973386,
                "mdate": 1700411973386,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0iNjWhBxAC",
            "forum": "Yz0Strbex6",
            "replyto": "Yz0Strbex6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5769/Reviewer_XdRo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5769/Reviewer_XdRo"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an analysis of signature transforms applied to Lasso regression of synthetic continuous time series data. Signature transforms are known for their ability to linearize the problem of feature selection. \n\nThe main contribution of the paper is the exploration of the correlations of these transforms when applied to Brownian motions and Ornstein-Uhlenbeck processes. This is done for signatures defined by Ito and Stratonovich integrals. They achieve this by analyzing the signatures' definitions in the context of different stochastic integrals and directly manipulating the resulting expressions. Then, using the Irrepresentable Condition (which, as proved in [Zhao & Yu '06], is almost equivalent to sign consistency of the Lasso estimator), study the consistency of the Lasso estimator when applied to the inference of labels generated from the signature transforms of these stochastic processes.\n\nThese results are then contrasted with numerical simulations that compare the performance of this signature-based regression under both Ito and Stratonovich integral definitions. Their findings suggest concrete settings under which signature transforms defined over one or the other integral definitions should be preferable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is structured in a clear way and is well written, which makes the results easy to understand. While the results are mainly theoretical, the authors contrast them with numerical simulations which allows them to strengthen their conclusions. Also, Proposition 7 an easy-to-check condition to establish the sign consistency of the Lasso estimator studied. Finally, the discussion regarding the preference for different integral definitions in signature transforms is interesting and could provide insights that could be relevant for further research in the field."
                },
                "weaknesses": {
                    "value": "There are two main issues that prevent me from recommending the acceptance of the work:\n\n- It is not entirely clear to me that Lasso regression combined with signature transforms constitutes a widespread methodology used by numerous researchers/practitioners. The references listed do not seem to be enough to support this. Furthermore, in the introduction it is claimed that, on several important Machine Learning problems, this methodology yields state-of-the-art results. But many of the references provided to support this are rather old for the standards of the field. Thus, it is not clear that this is indeed the case. All lead me to think that, although the paper has clear merits, it could maybe be a better fit for a more specialized venue than ICLR. However, if the authors could provide further references to change this opinion, I would be willing to upgrade my evaluation.\n\n- The second one being that consistency does not appear to be the most relevant performance metric for this kind of problems. First because it only describes the large sample limit of the estimator and does not give insights about its performance for, the usually more realistic, finite sample scenarios. But also because, as discussed by the authors themselves, this measure can be too restrictive. It is true that the authors go on to explore other performance metrics in the appendices; but they do so only in a numerical way. Finally, sign consistency cannot be defined in a misspecified setting."
                },
                "questions": {
                    "value": "In this section I list some minor questions and suggestions aside from the major ones expressed in the \"Weaknesses\" section.\n\n- Although the results presented rely on the Irrepresentable condition, little is discussed about it. It is also stated that it \"is almost a necessary and sufficient condition for Lasso estimator to be sign consistent\". But the meaning of \"almost\" is never explained. It is my opinion that, if some further discussion were added on this respect, the results could be better appreciated.\n- Is the bound in Proposition 7 expected to be tight under certain regime? When is it loose? It would be good if some details about this could be added.\n- In the discussion of the main results it is said that, because experimentally good regression results are obtained for fairly small K, then this bound \"can be fairly easy to verify\". I think this phrase is somewhat vague.\n- I think it would be good if some heuristic interpretation of why different performances for signatures defined with Ito and Stratonovich integrals are observed.\n\n[following discussion with the authors I raise my rating]"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5769/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5769/Reviewer_XdRo",
                        "ICLR.cc/2024/Conference/Submission5769/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5769/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699472241411,
            "cdate": 1699472241411,
            "tmdate": 1700658879837,
            "mdate": 1700658879837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VenuHBLHXd",
                "forum": "Yz0Strbex6",
                "replyto": "0iNjWhBxAC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XdRo's Comments"
                    },
                    "comment": {
                        "value": "> ***Strengths.***\n\nWe thank the reviewer for appraising the contribution and writing of our paper. \n\n> ***Weakness 1: References***\n\nWe appreciate the reviewer's feedback. In the revised manuscript, we have included more related literature which provides evidence that the signature has been increasingly adopted in various tasks in machine learning. First, we have incorporated additional literature on the combination of Lasso with signature transforms in the paragraph \"Consistency of Lasso\" in Section 1. Second, we have included more recent papers when discussing the applications of signatures in various fields in the first paragraph of Section 1. Third, to illustrate the alignment of our paper with ICLR, we have included several signature-related references that are accepted by top computer science conferences, such as [1] at ICLR, [2] at NeurIPS, and [3] at CVPR. We hope that these additional discussions of related literature enhance the completeness of our manuscript and help address the reviewer's concerns.\n\n> ***Weakness 2: Performance Metric***\n\nWe thank the reviewer for this comment. We choose sign consistency as the starting point of our research because it is a basic and well-accepted metric for studying Lasso [4,5,6]. In addition, although our theoretical results are based on large sample limits, the finite sample numerical results presented in our paper align with these theoretical results, implying that the insights gained from the large sample analysis can be informative in finite sample scenarios as well. We also believe, with all due respect, it is fairly standard in the literature to use large sample asymptotics for theoretical foundations and verify its practical effectiveness in finite sample using simulation.\n\nIn the meantime, we completely agree with the reviewer that exploring theoretical results in finite sample settings and considering alternative metrics beyond consistency is valuable. Other metrics, such as the out-of-sample $R^2$, may also alleviate the reviewer's concern that sign consistency is undefined in a misspecified setting. Our paper is an initial exploration of the statistical properties related to signature selection using Lasso, which does not exist in the literature. Therefore we choose consistency as the starting point. We would like to conduct more in-depth research on the finite sample case and other metrics in our future work. \n\n> ***Question 1: Irrepresentable Condition***\n\nWe thank the reviewer for the feedback. In our revised manuscript, we have added more explanations regarding the irrepresentable condition and clarified the meaning of \"almost\" in Appendix C. In particular, we introduce different versions of sign consistency and irrepresentable condition, and explain their relationship. In fact, we borrow the word \"almost\" precisely from the classical reference of [4] in this literature.\n\n> ***Question 2: Tightness of Bound***\n\nWe appreciate the reviewer's insightful question. In Appendix E of our revised manuscript, we investigate a specific scenario where the inter-dimensional correlations of the Brownian motion, denoted as $\\rho = \\rho_{ij}$, are the same. In this setup, we prove that the bound given by Proposition 7 is tight when $\\rho<0$, and is loose when $\\rho>0$. This is a very helpful supplement to our manuscript, so we really appreciate the reviewer for raising this question. \n\n> ***Question 3: Vague Phrase***\n\nWe thank the reviewer's comment. We have clarified this discussion in our revised manuscript. Because $K$ is usually small, $\\max_{0\\leq k \\leq K} \\{ \\sharp A_k^* \\}$ is also not expected to be large. Therefore, the upper bound given by Equation (12) in our main article, $\\frac{1}{2\\max_{0\\leq k \\leq K} \\{\\sharp A_k^* \\} -1}$, should not be too small, i.e., it can be fairly easy to satisfy. \n\n> ***Question 4: Heuristic Interpretation***\n\nWe thank the reviewer's feedback, and we agree with the reviewer wholeheartedly that adding heuristic interpretations can improve our manuscript. In fact, the different performances of It\u00f4 and Stratonovich signatures can be intuitively attributed to the difference in the definitions of It\u00f4 and Stratonovich integrals. In our revised manuscript, we have added these explanations at the end of Section 4.1."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316330044,
                "cdate": 1700316330044,
                "tmdate": 1700316330044,
                "mdate": 1700316330044,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "26xFwUOh0i",
                "forum": "Yz0Strbex6",
                "replyto": "0iNjWhBxAC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[1] Lee, J., J. Jeon, S. yon Jhin, J. Hyeong, J. Kim, M. Jo, K. Seungji, and N. Park, 2022, LORD: Lower-dimensional embedding of log-signature in neural rough differential equations, ICLR.\n\n[2] Salvi, C., M. Lemercier, C. Liu, B. Horvath, T. Damoulas, and T. Lyons, 2021, Higher order kernel mean embeddings to capture filtrations of stochastic processes, NeurIPS.\n\n[3] Ibrahim, M. R., and T. Lyons, 2022, ImageSig: A signature transform for ultra-lightweight image recognition, CVPR. \n\n[4] Zhao, P., and B. Yu, 2006, On model selection consistency of Lasso, The Journal of Machine Learning Research, 7, 2541\u20132563.\n\n[5] Bickel, P. J., Y. Ritov, and A. B. Tsybakov, 2009, Simultaneous analysis of Lasso and Dantzig selector, The Annals of Statistics, 1705\u20131732.\n\n[6] Wainwright, M. J., 2009, Sharp thresholds for high-dimensional and noisy sparsity recovery using $l_1$-constrained quadratic programming (Lasso), IEEE Transactions on Information Theory 55, 2183\u20132202."
                    },
                    "title": {
                        "value": "References"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316497432,
                "cdate": 1700316497432,
                "tmdate": 1700316532151,
                "mdate": 1700316532151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wzBj6Bx6oc",
                "forum": "Yz0Strbex6",
                "replyto": "26xFwUOh0i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_XdRo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_XdRo"
                ],
                "content": {
                    "title": {
                        "value": "On the Authors' response"
                    },
                    "comment": {
                        "value": "Thank you very much for your reply to my comments. Although you have provided some further citations to back the fact that signature transforms are currently being proposed for certain tasks, these references seem to address different models that do not include Lasso regression. Therefore, I still do not see if the theoretical study of this model would be a problem that could have a deep impact within the ICLR community. I will therefore keep the previous rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504944488,
                "cdate": 1700504944488,
                "tmdate": 1700504944488,
                "mdate": 1700504944488,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VKPyxfbDZM",
                "forum": "Yz0Strbex6",
                "replyto": "G9cwJUgU6i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_XdRo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Reviewer_XdRo"
                ],
                "content": {
                    "title": {
                        "value": "On the Authors' follow up"
                    },
                    "comment": {
                        "value": "Thank you for your response. My comment regarded the references [1-3] provided in your first response. But I see that you have a revised version (not uploaded yet) where there is a stronger motivation for the study of this model. I am now more convinced that this is indeed a methodology that is capturing more attention in the community. I will therefore raise my rating."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658783041,
                "cdate": 1700658783041,
                "tmdate": 1700658783041,
                "mdate": 1700658783041,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6SNLNXzJgR",
            "forum": "Yz0Strbex6",
            "replyto": "Yz0Strbex6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5769/Reviewer_PPAS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5769/Reviewer_PPAS"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the Lasso consistency of signature transformation of time series. A signature of order k for a d dimensional time series is defined as a path integral of the time series over a sequence of indices $i_1, i_2, \\cdots, i_k$. Signatures are powerful tools in machine learning.  The universal non-linearity property states that any continuous function of the time series may be approximated arbitrarily well by a linear function of its signature.\nThis paper studies the consistency issue of Lasso for signature transforms.  Feature selection with lasso regression has been studied extensively in the literature. Consistency is an important metric for out-of-sample model performance. This paper determines which signature gives a more lasso consistency of a given time series. Particularly, Ito integrals are more suitable for time series  close to\nBrownian motion or random walk; whereas Stratonovich integrals have more consistency for mean reverting processes."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is a solid paper that provides both a theoretical and numerical study of the lasso consistency of different signatures. The paper rigorously defines and analyzes the consistency of Lasso regression in signature transformations. Given the useful properties of signature transformations in feature selection, this is a nice result determining which signature transform provides better consistency for Lasso regression.\n\nThe paper reads well and seems correct, but I did not check all the proofs."
                },
                "weaknesses": {
                    "value": "I do not see any major weakness in the paper. I think the paper is a bit compressed, which probably makes it harder to read for a broader set of readers. It would be nice if the authors gave more explanations of their results in the main body of the paper."
                },
                "questions": {
                    "value": "What is the domain of $\\beta$'s values in (3)? How does this equation relate to the feature selection problem where one selects a subset of features from a the larger original collection of the features?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5769/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5769/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5769/Reviewer_PPAS"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5769/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699546340775,
            "cdate": 1699546340775,
            "tmdate": 1699636605735,
            "mdate": 1699636605735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jd7oD8m9kF",
                "forum": "Yz0Strbex6",
                "replyto": "6SNLNXzJgR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5769/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PPAS's Comments"
                    },
                    "comment": {
                        "value": "> ***Strengths.***\n\nWe thank the reviewer for appraising the soundness, rigor, and contribution of our paper. \n\n> ***Weaknesses: More Explanations***\n\nWe thank the reviewer for the suggestion to help us increase the readability of our paper. We have provided additional explanations for our results within the main body of our paper. For example, we have discussed the relationship with feature selection in Section 2.2 and offered heuristic interpretations for different performances of It\u00f4 and Stratonovich signatures in Section 4.1. We have added these explanations while adhering to the constraint of maintaining the paper within its nine-page limit. \n\n> ***Questions: Domain of $\\beta$ and Relationship with Feature Selection***\n\nWe thank the reviewer for the question. \n\n**Domain of $\\beta$'s values:** The parameter $\\beta$ may take any real number. The linear expression (3) is based on universal nonlinearity of signatures, which is proven in previous work such as [1]. These proofs do not impose any restrictions on the domain of $\\beta$ values.  In particular, if the signature does not contribute to explaining the target variable $y_n$, the corresponding $\\beta$ is zero.\n\n**Relationship with feature selection.** Equation (3) is a linear model with signatures as features, and only signatures with nonzero beta coefficients can explain the target variable, $y_n$. Therefore, we expect to select signatures with nonzero beta coefficients from all signatures included in the linear model (3). This is essentially a feature selection problem, where signatures serve as features, and our goal is to select the subset of signatures with nonzero beta coefficients from all features. This is also the reason why the literature uses feature selection methods, such as the Lasso, to select signatures. We have added more explanation to clarify this relationship in Section 2.2 of the revised manuscript. \n\n**Reference:**\n\n[1] Levin, D., T. Lyons, and H. Ni, 2016, Learning from the past, predicting the statistics for the future, learning an evolving system, arXiv preprint arXiv:1309.0260."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314447134,
                "cdate": 1700314447134,
                "tmdate": 1700319452964,
                "mdate": 1700319452964,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]