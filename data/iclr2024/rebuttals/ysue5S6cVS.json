[
    {
        "title": "Confidence-driven Sampling for Backdoor Attacks"
    },
    {
        "review": {
            "id": "2HbKjuSkru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6110/Reviewer_ycqn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6110/Reviewer_ycqn"
            ],
            "forum": "ysue5S6cVS",
            "replyto": "ysue5S6cVS",
            "content": {
                "summary": {
                    "value": "Traditional approaches proposed to mount backdoor attacks inject triggers into training samples selected at random. This paper presents a new approach for selecting the training samples to poison in order to launch a more effective backdoor attack. To that end, the authors select samples that are close to the decision boundary between different classes. These samples are the ones that have a low confidence (measured as the probability of a particular class). \n\nThe authors test their approach against 2 baselines: a random baseline, and a baseline where poisoned samples are chosen based on how early they are forgotten during training (FUS). For each of these choices, the authors implement several backdoor attacks, including BadNets, Blend and Adaptive Attacks. The attacks are evaluated when no defenses are employed, and when several defenses are used, such as SS, STRIP, ABL and NC.\n\nThe results on CIFAR10 and CIFAR100 show that selecting the samples to poison is more effective than poisoning random samples. Furthermore, several attacks that are not effective (when defenses are employed) with random sampling become effective when the samples to-poison are chosen. \n\nFinally, the authors investigate the choice of close-to-boundary and far-from-boundary samples to poison to validate that poisoning samples close to the decision boundary is more effective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper presents a new approach for selecting the samples to be poisoned during a backdoor attack, as opposed to random sampling. the results of the paper show that a good selection is more effective than random selection\n- the paper evaluates several backdoor attacks and defenses when random and targeted sampling are used"
                },
                "weaknesses": {
                    "value": "- the paper considers 2 small datasets: cifar10 and cifar100. it would be great to evaluate on larger datasets, such as imagenet\n- the paper evaluates defenses that rely on some notion of outlier detection, and as such, the selection mechanism is effective. the authors however do not evaluate defenses that are not based on outlier detection, such as [1]\n\n\n[1] Provable Guarantees against Data Poisoning Using Self-Expansion and Compatibility, Jin et al., 2021"
                },
                "questions": {
                    "value": "- FUS is not a method proposed for selecting poisoned samples. how do you do the selection for this technique?\n- can you evaluate your method on larger datasets such as imagenet?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6110/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697420962258,
            "cdate": 1697420962258,
            "tmdate": 1699636660350,
            "mdate": 1699636660350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RCgy2pdD7f",
                "forum": "ysue5S6cVS",
                "replyto": "2HbKjuSkru",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer ycqn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising these concerns. We will address these concerns by discussing the following questions: (1) evaluation on larger datasets such as ImageNet; (2) evaluation on defenses that do not depend on outlier detection; (3) clarification for FUS.\n\n**Q1: Evaluation on larger datasets such as ImageNet.**\n\nWe thank the reviewer for this suggestion. We conduct experiments on Tiny-ImageNet. Success rates of random sampling and CBS combined with 3 types of attacks and against various defenses are shown in the following tables. These results demonstrate that CBS is applicable to larger datasets and consistently improves the stealthiness of different attacks.\n\nPerformance on Type I backdoor attacks:\n|               | Attacks        | random | CBS  |\n|---------------|----------------|--------|------|\n| No defenses   | BadNet         | 89.5   | 83.1 |\n|               | Blended        | 83.4   | 81.6 |\n|               | Adaptive-Blend | 67.2   | 66.2 |\n|               | Adaptive-Patch | 84.5   | 81.7 |\n| SS            | BadNet         | 0.4    | 18.5 |\n|               | Blended        | 37.2   | 46.3 |\n|               | Adaptive-Blend | 59.4   | 65.1 |\n|               | Adaptive-Patch | 75.3   | 78.5 |\n| Strip         | BadNet         | 0.6    | 12.2 |\n|               | Blended        | 46.2   | 54.6 |\n|               | Adaptive-Blend | 58.4   | 63.3 |\n|               | Adaptive-Patch | 69.5   | 72.8 |\n\nPerformance on Type II backdoor attacks:\n|               | Attacks        | random | CBS  |\n|---------------|----------------|--------|------|\n| No defenses   | Hidden-trigger | 59.7   | 54.3 |\n| NC            | Hidden-trigger | 5.3    | 11.5 |\n| FP            | Hidden-trigger | 8.4    | 12.1 |\n| ABL           | Hidden-trigger | 1.8    | 4.2  |\n\nPerformance on Type III backdoor attacks:\n|               | Attacks        | random | CBS  |\n|----------------|---------|-------|------|\n| No defenses | WaNet | 98.5 | 96.1 |\n|             | LiRA  | 99.3 | 96.4 |\n|             | WB    | 98.2 | 92.7 |\n| NC          | WaNet | 5.4  | 10.7 |\n|             | LiRA  | 6.3  | 10.2 |\n|             | WB    | 9.6  | 15.1 |\n| FP          | WaNet | 9.5  | 13.6 |\n|             | LiRA  | 8.7  | 13.8 |\n|             | WB    | 10.2 | 16.5 |\n\n\n**Q2: Evaluatuion on defenses that do not depend on outlier detection.**\n\nWe would like to politely point out that we evaluate defenses that do not depend on outlier detections such as Neural Cleanse[3], Fine Pruning[4], Anti-Backdoor Learning[5], in this work. We present partial results in the main text Table 1-3 and full results in the Appendix Table 4-7. According to these results, our method can also effectively improve the attacks' resistance against non-outlier-detection defenses. We are grateful for the defense in [1] pointed out by the reviewer. We notice that the authors published an updated version in [2] so we follow the latest paper. In detail, we test on CIFAR-10 with model ResNet18, and evaluate the performance of attacks BadNet and Clean-label attack. We compare our method with random sampling. We present both success rate (ASR) and clean accuracy (Clean Acc) in the following Table. We include another defense Neural Cleanse for a convenient comparison. The results show that under this powerful defense, our CBS can still significantly improve the stealthiness of backbone methods.\n\n| Attacks |      | Random |       | CBS   |       |\n|---------|------|--------|-------|-------|-------|\n|         |      | ASR    | Clean Acc | ASR  | Clean Acc |\n| ISPL+B  | BadNet | 0.3    | 93.1    | 13.1 | 93.3      |\n|         | LC     | 0.9    | 92.2    | 10.3 | 92.4      |\n| NC      | BadNet | 1.1    | 93.5    | 24.6 | 93.1      |\n|         | LC     | 8.9    | 92.7    | 12.6 | 92.6      |\n\n**Q3: Clarification for FUS**\n\nAs mentioned in the Introduction of the original paper, FUS [2] is a method to improve the efficiency of poisoning by a rational selection of poisoned samples. We follow the strategy in Sections 3.3, 4.1 and 4.2 in the original paper where they compare FUS with random sampling for fixed poison rates (which is referred to as mixing ratio in the original paper). In this case, we can compare it with random sampling and CBS for fixed poisoned rates. \n\n**Additional comments:**\nWe follow the reviewer's suggestion and conduct experiments on Tiny-ImageNet.\n\nWe hope our response can address your concerns. We are grateful for your valuable reviews and look forward to further feedback.\n\n**References**\n\n[1] Provable Guarantees against Data Poisoning Using Self-Expansion and Compatibility\n\n[2] Data-Ef\ufb01cient Backdoor Attacks\n\n[3] Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks\n\n[4] Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks\n\n[5] Anti-Backdoor Learning: Training Clean Models on Poisoned Data"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090222515,
                "cdate": 1700090222515,
                "tmdate": 1700090222515,
                "mdate": 1700090222515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p2C8C0aWce",
                "forum": "ysue5S6cVS",
                "replyto": "2HbKjuSkru",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "We are grateful for your valuable comments. We hope that our responses have addressed your concerns. If you have any further concerns, please let us know. We are looking forward to hearing from you."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413153130,
                "cdate": 1700413153130,
                "tmdate": 1700413153130,
                "mdate": 1700413153130,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0k458bUO4D",
                "forum": "ysue5S6cVS",
                "replyto": "p2C8C0aWce",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Reviewer_ycqn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Reviewer_ycqn"
                ],
                "content": {
                    "comment": {
                        "value": "Yes, thank you for running the additional experiments and for clarifying the use of FUS!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416309855,
                "cdate": 1700416309855,
                "tmdate": 1700416309855,
                "mdate": 1700416309855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xathwyGeSC",
            "forum": "ysue5S6cVS",
            "replyto": "ysue5S6cVS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6110/Reviewer_gKjE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6110/Reviewer_gKjE"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new sampling procedure for backdoor attacks. Standard backdoor attacks poison (i.e., insert a backdoor trigger on) *random* examples from the training set. The authors claim that strategically choosing which training examples to poison can improve on the efficacy of a variety of attacks.  In particular, the authors choose to poison training examples on which a surrogate model exhibits low confidence. Then, the authors provide strong experimental evidence that this sampling scheme improves existing attacks. Additionally, the authors provide intuition for why their method might work on neural networks by analyzing an SVM model on a toy dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the authors propose a simple method with few hyperparameters that reliably gives good experimental results\n- the proposed method can be combined with existing attacks\n- the choice of backdoor attacks in the evaluation is thorough\n- the writing is clear and concise"
                },
                "weaknesses": {
                    "value": "My main concern is about how practical the idea is. In particular, the adversary is assumed to have:\n- *edit* access to the entire train set\n- access to a surrogate model that is similar to the model that is being backdoored.\n\nFor example, suppose I'm an (adversarial) user of some social media platform P. I know P will train *some* model on the data of its users (including my own data). I could execute a backdoor attack by simply inserting a backdoor trigger in each of my datapoints (e.g., images). However, to execute the proposed attack, I would need the ability to insert data into arbitrary users' profiles---this already makes the attack a lot harder to execute. Additionally, I would need access to a model that acts similarly to the model P will train. Because of this, now I need knowledge of the model P aims to train---in my view, another significant challenge in practice.\n\nAs another example in the same vein, consider the setup in [1]. There, the authors poison *expired* web-pages. The above two challenge persists in this setup.\n\nAdditional weakness:\n- Evaluated defenses: it makes (intuitive) sense to me that the proposed attack works well against outlier-based defenses. However, it is unclear to me whether this will translate to defenses based on model behavior like [2] and [3].\n\n\n[1] Carlini, Nicholas, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. \"Poisoning web-scale training datasets is practical.\" arXiv preprint arXiv:2302.10149 (2023).\n\n[2] Jin, Charles, Melinda Sun, and Martin Rinard. \"Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks.\" In The Eleventh International Conference on Learning Representations. 2022.\n\n[3] Khaddaj, Alaa, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, and Aleksander Madry. \"Rethinking Backdoor Attacks.\" (2023). https://arxiv.org/abs/2307.10163"
                },
                "questions": {
                    "value": "- in Table 1, why does your method reduce the efficacy of the attack in the absence of a defense?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6110/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698427354541,
            "cdate": 1698427354541,
            "tmdate": 1699636660164,
            "mdate": 1699636660164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xCvOh9eMyK",
                "forum": "ysue5S6cVS",
                "replyto": "xathwyGeSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer gKjE (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising these concerns. We address these concerns by discussing the following questions: (1) Does the attacker have \"edit access\" to the entire training set? Is this assumption practical? (2) Does the attacker have access to the surrogate model? Is this scenario practical? (3) discuss the defenses mentioned by the reviewer; (4) Why does the performance drop in the absence of defenses in Table 1?\n\n**Q1: Does the attacker have \"edit access\" to the entire training set? Is this assumption practical?**\n\nWe acknowledge that there exist some practical scenarios when the attacker has edit access to the entire training set. For example, the attacker can be a model provider or a training platform provider as described in scenarios 1 and 2 in [1]. Under these scenarios, the attacker have the capacity to modify any data in the training set as he wishes. Therefore, our method can be easily adapted for selection to improve the stealthiness of the backdoor. \n \nMoreover, our work does not necessarily assume the attacker has full edit access to the training set. \nWhen only allowed to insert triggers to part of the training data, the attacker can still select samples from his own data via our proposed method while keeping other users' data clean if the attacker is concerned about the stealthiness of the backdoor. To validate the feasibility of our proposed method under this scenario, we conducted an additional experiment on CIFAR-10. We allocated 10% of the training set as the attacker's private data, and the attacker is not allowed to perturb the rest 90%. We assume that the attacker selects 10% from his own data to insert triggers. To apply our method, we train a ResNet18 as the surrogate model and select samples from the private data. We evaluated the poisoning effect on both ResNet18 and VGG16. We considered the BadNet attack and defenses such as Spectral Signature(SS), Strip, and Neural Cleanse(NC). Our results (success rates), detailed in the following Table, suggest that our approach can indeed enhance the stealth of backbone attacks, even when the full training set is not accessible.   \n\n| Defenses    | Model   | Random | CBS  |\n|-------------|---------|--------|------|\n| No defenses | ResNet18| 99.9   | 93.7 |\n|             | VGG16   | 99.8   | 95.2 |\n| SS          | ResNet18| 1.2    | 15.7 |\n|             | VGG16   | 3.5    | 10.4 |\n| NC          | ResNet18| 2.7    | 14.4 |\n|             | VGG16   | 3.6    | 12.8 |\n| Strip       | ResNet18| 1.5    | 13.2 |\n|             | VGG16   | 1.3    | 10.9 |\n\n**Q2: Does the attacker have access to the surrogate model? Is this scenario practical?**\n\n**First**, we would like to clarify that there exist some practical scenarios where surrogate models are available. When the attacker is a model provider or a training platform provider, he has access to the model information and thus can easily utilize a surrogate model for selection via our proposed method.\n\n**Second**, we would like to highlight that it is a common assumption to have a surrogate model. The previous works in [2][3][4] also consider poisoning attacks in the same setting to leverage a surrogate model for generating poisoning samples. Notably, the architecture or parameters of the surrogate model do not need to be the same as the victim model. In our work, in Table 1 and 2, we present experiments where poisons are generated from a ResNet18 model, and we test the poisoning performance on the victim model VGG16. These results show that poisons can be transferred to different models. Existing work such as [7] also provides empirical evidence for the effectiveness of surrogate models under the black-box setting. In Tabel 4 in [7], poisons are generated from the surrogate model ResNet18 and transferred to MonileNet-V2, VGG11. The poisons are still effective. Therefore, we think the usage of surrogate models is practical."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700089512800,
                "cdate": 1700089512800,
                "tmdate": 1700101107426,
                "mdate": 1700101107426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XQOZAALD06",
                "forum": "ysue5S6cVS",
                "replyto": "xathwyGeSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer gKjE (2/2)"
                    },
                    "comment": {
                        "value": "**Q3: Discuss the defenses mentioned by the reviewer.**\n\nWe are grateful for the defenses highlighted by the reviewer. To begin, we would like to clarify that our evaluation includes not only outlier-based defenses but also non-outlier-based ones such as NC, ABL, and FP. While we present a portion of these results in the main paper (Tables 1-3), comprehensive details can be found in the Appendix (Tables 4-7). According to these results, our method can improve the attacks\u2019 resistance against both types of defenses, and we do not observe a drop in performance against non-outlier-detection defenses.\n\nRegarding the defense method mentioned in [5], we encountered significant challenges in implementing it due to its complexity. The method requires training an extensive number of models (approximately 100,000) on randomly selected 50% subsets of the poisoned dataset. This process is exceedingly time-intensive. Thus, we focus our experimental efforts on the defense proposed in [6]. In detail, we test on CIFAR-10 with model ResNet18, and evaluate the performance of attacks BadNet and Clean-label attack. We compare our method with random sampling. We present both success rate (ASR) and clean accuracy (Clean Acc) in the following Table. We include another defense Neural Cleanse for a convenient comparison. The results show that under this powerful defense, our CBS can still significantly improve the stealthiness of backbone methods.\n\n| Attacks |      | Random |       | CBS   |       |\n|---------|------|--------|-------|-------|-------|\n|         |      | ASR    | Clean Acc | ASR  | Clean Acc |\n| ISPL+B  | BadNet | 0.3    | 93.1    | 13.1 | 93.3      |\n|         | LC     | 0.9    | 92.2    | 10.3 | 92.4      |\n| NC      | BadNet | 1.1    | 93.5    | 24.6 | 93.1      |\n|         | LC     | 8.9    | 92.7    | 12.6 | 92.6      |\n\n**Q4: Why does the performance drop in the absence of defenses in Table 1?**\n\nIntuitively, our method selects samples close to the boundary, and this targeted selection results in a more constrained selection space compared to random sampling, which may lead to a slightly reduced performance in the absence of defenses. For instance, when high-confidence samples for their true classes are used as poisoning samples, there is a significant shift in the decision boundary, typically leading to a higher success rate. Moreover, in Section 4.3, we theoretically illustrate that boundary samples (with lower confidence) have a smaller success rate than samples that are further from the boundary (with higher confidence), which indicates that our methods slightly compromise the performance without defenses. Our analysis and experimental results show that there exists a trade-off between clean performance and stealthiness, and our method is stronger against defenses.\n\nWe hope our responses can address the reviewer's concerns. We are grateful for the insightful reviews and look forward to further interactions. \n\n**References**\n\n[1] Backdoor Learning: A Survey\n\n[2] Hidden Trigger Backdoor Attacks\n\n[3] Label-Consistent Backdoor Attacks\n\n[4] Backdoor Attack with Imperceptible Input and Latent Modification\n\n[5] Rethinking Backdoor Attacks\n\n[6] Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks\n\n[7] Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700089563638,
                "cdate": 1700089563638,
                "tmdate": 1700089563638,
                "mdate": 1700089563638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "csJAWbdBt5",
                "forum": "ysue5S6cVS",
                "replyto": "xathwyGeSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "We are grateful for your valuable comments. We hope that our responses have addressed your concerns. If you have any further concerns, please let us know. We are looking forward to hearing from you."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413136524,
                "cdate": 1700413136524,
                "tmdate": 1700413136524,
                "mdate": 1700413136524,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CcKBPT1Yz1",
                "forum": "ysue5S6cVS",
                "replyto": "xathwyGeSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder"
                    },
                    "comment": {
                        "value": "We appreciate your reviews. We hope that our responses have adequately addressed your concerns. As the deadline for open discussion nears, we kindly remind you to share any additional feedback you may have.  We are keen to engage in further discussion."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594343124,
                "cdate": 1700594343124,
                "tmdate": 1700594343124,
                "mdate": 1700594343124,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pMopuDj5rI",
                "forum": "ysue5S6cVS",
                "replyto": "CcKBPT1Yz1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Reviewer_gKjE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Reviewer_gKjE"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I appreciate the authors' rebuttal. I find the experiment targeting only 10% of the training data very promising. However, I still believe the proposed attack introduces significant challenges in practice and thus keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681231391,
                "cdate": 1700681231391,
                "tmdate": 1700681231391,
                "mdate": 1700681231391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B1gcRTxllM",
            "forum": "ysue5S6cVS",
            "replyto": "ysue5S6cVS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6110/Reviewer_okLZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6110/Reviewer_okLZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the strategies of choosing samples to inject triggers for training backdoor models. To improve the robustness against backdoor defenses, the paper proposes a confidence-driven sampling strategy for backdoor attacks. Specifically, the proposed method chooses samples with lower confidence scores to inject triggers, making the backdoored models difficult to be defended. Extensive experiments show the effectiveness of proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "To improve the robustness against backdoor defenses, the paper proposes a simple and effective sampling choosing strategy for backdoor attacks. Also, the proposed method can be integrated into various backdoor attacks to improve the robustness. The paper analyzes the proposed method theoretically, indicating the rationality of proposed method."
                },
                "weaknesses": {
                    "value": "The method is not efficient because it needs to train a suggorate model (e.g. ResNet18) to check the confidence scores. In the paper, the used datasets are two small-scale datasets including CIFAR10 and CIFAR100. Is it time-consuming when traing a suggorate model on a large dataset e.g. image-net?\n\nIn the experiments (Table 1, 2 and 3), the used backdoor defenses are not up-to-date. It is better to compare with some recent backdoor attacks e.g. i-bau [1]. \n\n[1] Zeng, Yi, et al. \"Adversarial Unlearning of Backdoors via Implicit Hypergradient.\" International Conference on Learning Representations. 2021."
                },
                "questions": {
                    "value": "Does the proposed method perform better against defenses depending on outlier detection than other backdoor defense methods?\n\nIn Figure 1, the paper shows the visualizations of two dirty-label attacks. Is there same observations for clean-label attacks?\n\nIn Table 1, 2 and 3, the proposed method achieves lower ASRs compared to Random and FUS under the condition of \"No Defenses\". Could the authors provide more explanations about the results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6110/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6110/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6110/Reviewer_okLZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6110/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698620817657,
            "cdate": 1698620817657,
            "tmdate": 1699636660028,
            "mdate": 1699636660028,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Gotm5c5dY",
                "forum": "ysue5S6cVS",
                "replyto": "B1gcRTxllM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer okLZ (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising these concerns. To address these concerns, we will discuss the following questions: (1) Is the proposed method inefficient, especially for training surrogate models on large datasets; (2) Evaluations on some recent backdoor attacks e.g. i-bau; (3) Does the proposed method perform better against defenses depending on outlier detection than other backdoor defense methods; (4) Are there same observations for clean-label attacks as for dirty-label attacks in Figure 1; and (5) Why the performance drop when no defenses.\n\n**Q1: Is the proposed inefficient, especially for training surrogate models on large datasets?**\n\nWe would like to clarify that the proposed method does not suffer much from the time efficiency problem. **First**, we do not need to train the surrogate model till the end, because shorter training epochs are sufficient for selection and have the added benefit of reducing the risk of the model overfitting to the training set which may reduce the poisoning effect. As mentioned in Section 5.1, we only train the surrogate model for 60 epochs. We provide an ablation study about training epochs for the surrogate model in the CIFAR-10 dataset and ResNet18 model in Table below. \nEpochs|20|60|80|100\n--------|----|--|---|-----\nNo defense|94.4|93.6|93.2|92.1\nSS|14.6|23.2|22.9|23.6\nStrip|12.1|26.2|26.7|25.9\nNC|13.5|24.6|24.2|24.8\nABL|10.8|31.3|31.5|30.7\n\nAccording to this table, when the surrogate model is trained for few epochs(20), we can not have a good estimation for boundary thus the performance is much decreased; when we train for more epochs(80), the performance is similar to 60 epochs.  **Second**, many existing attacking methods such as hidden-trigger[2] and clean-label[3], utilize a surrogate model. When applying the proposed method to these attacks, we can directly leverage surrogate models of these attacks for selection, thus avoiding inefficiency. Moreover, for some large datasets such as ImageNet, there exist pre-trained models that can be directly used by the proposed method. We conduct experiments on Tiny-ImageNet where the surrogate model is trained for 60 epochs within 2 hours. The results are shown in Q1 in the response for Reviewer ycqn, and our method can consistently improve the attack's resistance against various defenses.\n\n**Q2: Evaluations on some recent backdoor attacks e.g. i-bau.**\n\nWe thank the reviewer for suggesting the defense, I-BAU[1]. We compare our method (CBS) and random sampling (Random) on the CIFAR-10 dataset and ResNet18 model. We test various attacks including BadNet, Blend, WaNet and Hidden-trigger. We present both success rate(ASR) and clean accuracy(Clean Acc, accuracy on samples without trigger) in the following Table. \n\n| Attacks        | Random |     | CBS  |     |\n|----------------|--------|-----|------|-----|\n|                | ASR    | Clean Acc | ASR | Clean Acc |\n| **I-BAU**      |        |     |      |     |\n| BadNet         | 2.3    | 91.3 | 9.5  | 91.4 |\n| Blend          | 3.2    | 90.7 | 8.6  | 90.5 |\n| WaNet          | 1.7    | 90.5 | 8.2  | 90.6 |\n| Hidden trigger | 0.8    | 90.6 | 8.1  | 90.4 |\n| **NC**         |        |     |      |     |\n| BadNet         | 1.1    | 93.5 | 24.6 | 93.1 |\n| Blend          | 82.5   | 93.7 | 81.7 | 94.0 |\n| WaNet          | 8.9    | 94.1 | 13.4 | 93.8 |\n| Hidden trigger | 6.3    | 92.7 | 8.7  | 93.5 |    \n\nWe include another defense Neural Cleanse[7] for convenient comparison. We notice that I-BAU is much more powerful than other defenses such as Neural Cleanse[6] but compromises the clean performance. However, according to the results, our method can still improve the stealthiness of the backbone methods under this powerful defense."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088797918,
                "cdate": 1700088797918,
                "tmdate": 1700088797918,
                "mdate": 1700088797918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5IUzti3Y0S",
                "forum": "ysue5S6cVS",
                "replyto": "B1gcRTxllM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer okLZ (2/2)"
                    },
                    "comment": {
                        "value": "**Q3. Does the proposed method perform better against defenses depending on outlier detection than other backdoor defense methods?**\n\nIn this work, we consider various defenses including outlier-detection methods like Spectral Signature[4], Activation Clustering[5], SPECTRE[6], and other methods like Neural Cleanse[7], Fine Pruning[8], and Anti-Backdoor Learning[9]. We present part of the results in the main paper Table 1-3 and full results in Appendix Table 4-7. According to these results, our method can improve the attacks' resistance against both types of defenses, and we do not observe a drop in performance against non-outlier-detection defenses. For instance, in Table 1, BadNet+CBS can achieve a success rate of 24.6% against NC which is not an outlier-detection defense, while achieving 23.7% against STRIP which is an outlier-detection defense.\n\n**Q4: Are there same observations for clean-label attacks as for dirty-label attacks in Figure 1?**\n\nYes, we have similar observations for clean-label attacks such as Clean-label attack in [3]. We will include visualizations for Clean-label attacks in the appendix. Existing work [9] provides additional evidence. As shown in the introduction and Figure 1 in [10], poison and clean samples consistently form two separate clusters in the latent space, even for the Clean-label attack[3]. \n\n**Q5:Why the performance drop when no defenses?**\n\nIntuitively, our method selects samples close to the boundary, and this targeted selection results in a more constrained selection space compared to random sampling, which may lead to a slightly reduced performance in the absence of defenses. For instance, when high-confidence samples for their true classes are used as poisoning samples, there is a significant shift in the decision boundary, typically leading to a higher success rate. Moreover, in Section 4.3, we theoretically illustrate that boundary samples (with lower confidence) have a smaller success rate than samples that are further from the boundary (with higher confidence), which indicates that our methods slightly compromise the performance without defenses. Our analysis and experimental results show that there exists a trade-off between clean performance and stealthiness, and our method is stronger against defenses.   \n\nWe hope our responses can address the reviewer's concerns. We are grateful for the insightful reviews and look forward to further interactions.\n\n**References**\n\n[1] Adversarial Unlearning of Backdoors via Implicit Hypergradient\n\n[2] Hidden Trigger Backdoor Attacks\n\n[3] Label-consistent backdoor attacks.\n\n[4] Spectral Signatures in Backdoor Attacks\n\n[5] Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering\n\n[6] SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics\n\n[7] Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks\n\n[8] Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks\n\n[9] Anti-Backdoor Learning: Training Clean Models on Poisoned Data\n\n[10] Revisiting the Assumption of Latent Separability for Backdoor Defenses"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088826802,
                "cdate": 1700088826802,
                "tmdate": 1700089668411,
                "mdate": 1700089668411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NPAztBWqUU",
                "forum": "ysue5S6cVS",
                "replyto": "B1gcRTxllM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "We are grateful for your valuable comments. We hope that our responses have addressed your concerns. If you have any further concerns, please let us know. We are looking forward to hearing from you."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413119609,
                "cdate": 1700413119609,
                "tmdate": 1700413119609,
                "mdate": 1700413119609,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BHKdeWq94Q",
                "forum": "ysue5S6cVS",
                "replyto": "NPAztBWqUU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6110/Reviewer_okLZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6110/Reviewer_okLZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal. Most of concerns have been addressed and I will keep the score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6110/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710991152,
                "cdate": 1700710991152,
                "tmdate": 1700710991152,
                "mdate": 1700710991152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]