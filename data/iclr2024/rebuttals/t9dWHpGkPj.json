[
    {
        "title": "Language Model Inversion"
    },
    {
        "review": {
            "id": "Kg64HS6AjW",
            "forum": "t9dWHpGkPj",
            "replyto": "t9dWHpGkPj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2723/Reviewer_scV6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2723/Reviewer_scV6"
            ],
            "content": {
                "summary": {
                    "value": "The paper trains an encoder-decoder model to map an embedding back to the originating text. The problem and the method are very similar to [1]. The main differences seem to be\n\n(1) The paper is motivated in the context of LLMs (extract underlying prompts), whereas [1] is in the context of retrievers (extract paragraphs in the knowledge base).\n\n(2) In [1], the embedding is a low-dimensional text embedding of a retriever. Here, the embedding is the conditional token logits given the prompt, which are unrolled and transformed to be treated as encoder inputs. \n\n(3) This assumes access to LLM probabilities, which may not be entirely available. The paper proposes an estimator based on bisection that can be done with API calls, but this also assumes at least top-1 logit is exposed. \n\n(4) A critical component in [1] is the iterative refinement, which is not considered in this paper. \n\n[1] Text Embeddings Reveal (Almost) As Much As Text (to appear at EMNLP)"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Model inversion is a relatively new interesting problem.\n- The proposed method is shown to be more effective than more naive versions (specifically, not using the token logits/Monte Carlo estimator for probabilities)."
                },
                "weaknesses": {
                    "value": "- Given the high similarity between [1] and this submission, every nook and cranny of their differences must be disclosed and analyzed to make the submission's contributions clear. I found comparisons lacking with many unanswered questions like: why not use the iterative version, which is shown to be extremely useful in [1]? \n- The bisection-based estimator seems effective, but I find it a bit abrupt and lacking an explanation.\n- The method does not seem to consider the setting in which the model probabilities are not available at all (i.e., generation-only APIs), which is maybe the most relevant setting for extracting prompts given that the most powerful LLMs are private."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698603891939,
            "cdate": 1698603891939,
            "tmdate": 1699636214579,
            "mdate": 1699636214579,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LIsn8DcIju",
                "forum": "t9dWHpGkPj",
                "replyto": "Kg64HS6AjW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer scV6"
                    },
                    "comment": {
                        "value": ">  The paper proposes an estimator based on bisection that can be done with API calls, but this also assumes at least top-1 logit is exposed.\n\nTo clarify, we do not assume the top-1 logit is exposed, only the argmax output (which is given in all APIs we know of). Our only assumption is that we can provide a \"logit bias\" argument. We do not require the API to return logits or log probabilities.\n\n> Given the high similarity between [1] and this submission, every nook and cranny of their differences must be disclosed and analyzed to make the submission's contributions clear. I found comparisons lacking with many unanswered questions like: why not use the iterative version, which is shown to be extremely useful in [1]?\n\nWe agree that the method here is not novel. We apply ideas from past inversion work including [1] to a different application domain (inverting LM outputs). The contributions are developing a new architecture, a dataset of LM prompts, new baselines, and an algorithm for making this practical to run in an API setting with logit-bias.\n\nWe tested the iterative refinement approach in [1] but unfortunately did not find it worked when the input is LM logits. We have added these negative results to the appendix, along with a small experiment explaining why our corrector module might not work recursively (due to a narrow range of BLEU scores in the training distribution).\n\n> The bisection-based estimator seems effective, but I find it a bit abrupt and lacking an explanation.\n\nWe\u2019ve completely rewritten Section 5 in an effort to make our logit recovery algorithm more clear, and smooth the transition between sections. We plan to release our algorithm as a Python package to allow others to recover full probability vectors from argmax-only API access.\n\n> The method does not seem to consider the setting in which the model probabilities are not available at all (i.e., generation-only APIs), which is maybe the most relevant setting for extracting prompts given that the most powerful LLMs are private.\n\nSorry for the confusion. We *do* consider this baseline \u2013 \u201cSample Inverter\u201d is a T5 encoder-decoder trained to directly predict a prompt from LLAMA-2 Chat outputs. We have edited the paper to make this more clear in both the theoretical discussion (Section 3.2) and presentation of results (Section 7).\n\nAdditionally, the algorithm in Section 5 (originally known as \u201cDistribution Extraction\u201d) is proposed for precisely this reason: it enables logits access to models when APIs only provide argmax directly, by use of the logit bias argument. (As of November 2023, logit bias and argmax output are available in OpenAI, Anthropic, Cohere APIs.)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677945858,
                "cdate": 1700677945858,
                "tmdate": 1700680264290,
                "mdate": 1700680264290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "57WErcD2ob",
            "forum": "t9dWHpGkPj",
            "replyto": "t9dWHpGkPj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2723/Reviewer_YYeq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2723/Reviewer_YYeq"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the concept of inversion from language model outputs. The main goal is to retrieve the hidden prompts from language model systems, even without direct access to model output distributions. The authors have presented various techniques and experiments to substantiate their findings and have approached the problem from both attack and defense perspectives."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The paper delves into a novel area, focusing on inverting language model outputs, which is not extensively studied before. This introduces a new perspective in understanding language models and their vulnerabilities.\n\nQuality: The experiments are comprehensive and cover a range of scenarios, making the results more robust.\n\nSignificance: The findings could have implications for the safety and reliability of using large language models in real-world applications.\n\nClarity: The paper is generally well-structured with clear sections detailing the problem, methodology, experiments, and conclusions."
                },
                "weaknesses": {
                    "value": "The descriptions of the algorithm and the results, e.g. Algorithm 1 and Figure 2, are not very clear. It's hard to fully grasp the methodology and the results without a more detailed explanation.\n\nWhile the paper introduces a novel concept, the execution in terms of explaining the methodology and results could be enhanced. The paper would benefit from more detailed and clearer descriptions."
                },
                "questions": {
                    "value": "Can the authors provide a more detailed explanation of Algorithm 1 and its workings? How were the models chosen for the experiments, and can the authors provide a more in-depth description of these models?\n\nHow do the results in Figure 2 correlate with the methodology described? It would be helpful to have a clearer description or perhaps an example to illustrate the findings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2723/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2723/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2723/Reviewer_YYeq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713595732,
            "cdate": 1698713595732,
            "tmdate": 1700681244782,
            "mdate": 1700681244782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PihNz1lh2c",
                "forum": "t9dWHpGkPj",
                "replyto": "57WErcD2ob",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer YYeq"
                    },
                    "comment": {
                        "value": "> The descriptions of the algorithm and the results... are not very clear. It's hard to fully grasp the methodology and the results without a more detailed explanation.\n\nThanks for the suggestion. We have renamed the algorithm to \u201cAPI-Based Logits Extraction\u201d, but are open to suggestions for clearer terminology here. We rewrote the pseudocode and the explanatory text in Section 5.\n\n> While the paper introduces a novel concept, the execution in terms of explaining the methodology and results could be enhanced. The paper would benefit from more detailed and clearer descriptions.\n\nThank you for this recommendation. We\u2019ve completely written Section 5 (the section about stealing logits via API) and changed our description of the method, as well as added more details interspersed throughout the methods and results sections.\n\n> Can the authors provide a more detailed explanation of Algorithm 1 and its workings?\n\nWe have completely rewritten Section 5 and the pseudocode shown in Algorithm 1. Please indicate any further points of confusion so that we can continue to improve our explanations.\n\n> How were the models chosen for the experiments, and can the authors provide a more in-depth description of these models?\n\nWe chose the 7B parameter versions of LLAMA-2, and consider both the chat and base checkpoints. We chose LLAMA-2 as it was the among most powerful base language models available at the time of submission. We have clarified this point in experimental design.\n\nFor the ablations in the appendix (previously Section 9), we use GPT-2 instead of LLAMA due to computational constraints; these models require ~20x less compute to run.\n\n> How do the results in Figure 2 correlate with the methodology described? It would be helpful to have a clearer description or perhaps an example to illustrate the findings. \n\nWe rewrote the explanations and captions to better explain Figure 2. In particular, we explain these sampling-as-a-defense experiments differently in Section 8; please let us know if anything is still unclear."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677795546,
                "cdate": 1700677795546,
                "tmdate": 1700709033994,
                "mdate": 1700709033994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2FXwiHvhWh",
                "forum": "t9dWHpGkPj",
                "replyto": "PihNz1lh2c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2723/Reviewer_YYeq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2723/Reviewer_YYeq"
                ],
                "content": {
                    "comment": {
                        "value": "I am pleased to see the improvements made in response to the comments on clarity and methodology. I believe these changes will greatly benefit the understanding and applicability of your research. It looks much much better now. However, regarding the algorithm's name change to \u201cAPI-Based Logits Extraction\u201d, as it wasn't a point raised in my review, I cannot comment on its suitability or effectiveness.\n\nI hope this clarification helps, and I look forward to reviewing the updated manuscript with the implemented changes."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681171773,
                "cdate": 1700681171773,
                "tmdate": 1700681171773,
                "mdate": 1700681171773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ymIe9SjGn8",
                "forum": "t9dWHpGkPj",
                "replyto": "57WErcD2ob",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification and continued help. Hopefully the new name makes the purpose of our algorithm clearer to readers nonetheless... Please let us know if there are any improvements you recommend after reading the updated manuscript!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709241477,
                "cdate": 1700709241477,
                "tmdate": 1700710242866,
                "mdate": 1700710242866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lpRd3AbHUH",
            "forum": "t9dWHpGkPj",
            "replyto": "t9dWHpGkPj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2723/Reviewer_Cmp4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2723/Reviewer_Cmp4"
            ],
            "content": {
                "summary": {
                    "value": "This work tries to reverse engineer the prompt given to a language model solely using its outputs. In the case of access to the distribution over next tokens (given a particular prefix), they transform the logits of this distribution to a sequence of embeddings, feed these embeddings to a pretrained language model, and use the model\u2019s decoder in order to predict the prompt that lead to that distribution. They provide a number of empirical results that show the effectiveness of their proposed approach and explore methods for defending against prompt inversion. The further perform ablation studies to understand which components of their approach are necessary for its success"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The problem that the authors try to solve is quite interesting and relevant, given the large usage of prompt-based methods and the recent prevalence of \u201clanguage models as a service.\u201d This is the first work that I know of that tries to recover the prompt from output probabilities\n* The work explores methods for defending against prompt inversion, which is likely of large interest to those that provide \u201clanguage models as a service\u201d\n* Empirical results appear to be strong"
                },
                "weaknesses": {
                    "value": "* The method seems rather ad-hoc and not theoretically motivated. In the case of access to the conditional probability distribution, the method boils down to feeding (a transformed version of) the distribution back to a language model.\n* The formal methodology is difficult to understand. For example, it\u2019s unclear how the prompt is actually decoded from the embedded output probability distribution, i.e., what happens after their proposed encoding; I don\u2019t understand what is going on in section 5. What does it mean to \u201ccontrol one logit?\u201d"
                },
                "questions": {
                    "value": "* In the beginning of section 4 where it\u2019s stated that you \u201ctrain on samples from the conditional model\u201d, what is \u201cthe conditional model\u201d?\n* Given that the vector v was projected from R^d to R^v (as part of the final linear layer of the generation model), the argument that projecting it back to R^d would lead to a loss of information feels a bit strange\n* Where is the theoretical discussion/experiments corresponding to inversion when only the text output is available? The methodology discussed in section 5 still assumes access to probabilities from the model, which are often not available in \u201cmodels as a service\u201d platforms\n* In section 4, it mentions that there is a \u201cfixed-length input sequence of 42 words.\u201d Is this in reference to the sequence of _embeddings_ that are fed to the model? Or is this alluding to how the embeddings are decoded into the expected input? \n* How does the ability to reconstruct text change as a function of the pretrained model used to construct the distribution p(x | v)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699019077455,
            "cdate": 1699019077455,
            "tmdate": 1699636214423,
            "mdate": 1699636214423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "frKWZDxylq",
                "forum": "t9dWHpGkPj",
                "replyto": "lpRd3AbHUH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer Cmp4"
                    },
                    "comment": {
                        "value": "> Method is ad-hoc and not theoretically motivated.\n\nWe have added a new Subsection 3.1 (\u201cLogits Contain Residual Information\u201d) providing empirical and theoretical motivation why language model logits may reveal information about past tokens.\n\nIn terms of the method, we feel we are following standard practice in modern deep learning using Transformers. The main architectural challenge is how to feed in a high-dimensional vector (32,000 numbers) into a transformer. Main contributions were collecting a large dataset of prompts, extracting next-token probability distributions, and demonstrating that inversion is possible and effective in a variety of settings. We are not sure what would make a deep learning approach more theoretically motivated.\n\n> The formal methodology is difficult to understand. For example, it\u2019s unclear how the prompt is actually decoded from the embedded output probability distribution, i.e., what happens after their proposed encoding.\n\nWe havve tried to clarify this subsection of the paper; please let us know what you still find confusing. The output vector is provided to the encoder of an encoder-decoder language model, where the decoder autoregressively predicts the prompt. This approach is roughly standard in most conditional NLP systems.\n\n> I don\u2019t understand what is going on in section 5. What does it mean to \u201ccontrol one logit?\u201d\n\nWe consider the most common setting of LLM APIs, where we can provide a \u201clogit bias\u201d argument that upweights or downweights the relative importance of a token in the output. This feature is key to our algorithm. We have completely rewritten Section 5 and Algorithm 1 to be clearer and more contiguous with the rest of the paper, which may alleviate your concern.\n\n> In the beginning of section 4 where it\u2019s stated that you \u201ctrain on samples from the conditional model\u201d, what is \u201cthe conditional model\u201d?\n\nThis line was simply meant to indicate that we train the inversion model on output from a (conditional) language model. It was not a critical point, and we\u2019ve removed the statement entirely.\n\n> Given that the vector v was projected from R^d to R^v (as part of the final linear layer of the generation model), the argument that projecting it back to R^d would lead to a loss of information feels a bit strange\n\nThis is a small notational inconsistency. The original dimensionality d is that of the victim language model (4096 for LLAMA) while the final dimensionality is the input embedding of the inverter language model (typically 768 for T5). So in this case, projecting directly from LLAMA to T5 space would lead to a loss of dimensionality with a factor of 4096/768\u22485.3.\n\n> Where is the theoretical discussion/experiments corresponding to inversion when only the text output is available? The methodology discussed in section 5 still assumes access to probabilities from the model, which are often not available in \u201cmodels as a service\u201d platforms.\n\nWe did consider this baseline, sorry for not making it more clear. For Llama2-chat, we train a model to predict the prompt input given language model output samples . This is shown in Table 1 as \u201cSample Inverter\u201d. We add an additional paragraph of theoretical discussion in this matter to Section 3.2 (\u201cInverting from outputs\u201d).\n\n> In section 4, it mentions that there is a \u201cfixed-length input sequence of 42 words.\u201d Is this in reference to the sequence of embeddings that are fed to the model? Or is this alluding to how the embeddings are decoded into the expected input?\n\nThis is the sequence of embeddings that is fed into the encoder of the inversion model.\n\n> How does the ability to reconstruct text change as a function of the pretrained model used to construct the distribution p(x | v)?\n\nWe consider this question through many ablations shown in Table 9. Notably, we conclude that we could have seen greatly improved inversion performance if we had trained an inverter with more parameters."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677673167,
                "cdate": 1700677673167,
                "tmdate": 1700680178581,
                "mdate": 1700680178581,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "S4Wtj5eW9r",
            "forum": "t9dWHpGkPj",
            "replyto": "t9dWHpGkPj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2723/Reviewer_xaWT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2723/Reviewer_xaWT"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on extracting the prompt used in LM generation given access to the probability distribution of the token following the prompt. An \u201cinverter\u201d model is trained on pairs of distribution of distributions and prompts for this purpose. The dataset used for training is a meta dataset comprising a collections of various prompt/instruction datasets. A distribution extraction algorithm is also experimented with in case the LM API only provides access to top-k tokens and their logprobs instead of a full distribution. This approach is compared to several reasonable baselines on the task of inverting prompts of a LLaMA-2 (2B) model and training a much smaller model like T-5. Ablation analysis and other analysis around variation with inverter size, out-of-domain prompts etc. is also reported."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2013 The paper tackles an interesting problem with a sound approach.\n\n\u2013 The paper is well-organized and the presentation is clear.\n\n\u2013 The baselines are reasonable and well-designed and the approach in general outperforms these baselines.\n\n\u2013 The quantitative analysis is sound and substantive. Ablation studies and analysis on the impact of model size, out of domain prompts, distribution extraction, using partial distribution etc. provide great insight into the results. For example, the finding that whole distribution is important for good performance and losing even the tail of distribution causes a drop in performance is very interesting and provides insights into behavior of LMs in general. I am surprised however that using unnormalized logits over logprobs causes a drop in performance as well because unnormalized logits contain more information than post softmax logprobs."
                },
                "weaknesses": {
                    "value": "\u2013 While the results show a significantly better ability to recover prompts than the baseline, the absolute numbers are fairly low \u2013 the exact match scores are discouraging and the BLEU scores are also not very high. So I am not convinced if this is a serious concern for security in terms of prompt leakage. For example, looking at the qualitative examples, the inverter struggles with proper names which are often the important target tokens where security vulnerability is concerned.\n\n\u2013 Similar to above, the performance is significantly much worse on out-of-domain prompt datasets. This indicates an inclination of the inverter model to yield a large number of false positives. I am not entirely convinced whether this is a reliable attack on LM services.\n\n\u2013 Following the two points above, a more pointed analysis on the recoverability of sensitive information instead of recovering general purpose prompts might help establish the significance of the proposed attack more clearly. For example, can the proposed approach differentiate between or recover mildly different prompts that result in similar responses? Conversely, how good is the model at identifying seemingly benign prompt injection attacks? How good is the attack at exploiting specific sensitive information? Overall, I think the attack would be more serious if the exact match numbers are higher or exact match over sensitive tokens (like named entities) is high. Recovering paraphrased general purpose prompts while certainly interesting in its own right, doesn\u2019t seem like a convincing threat.\n\n\u2013 I am not entirely sure about this but the presentation gives me an impression that it only considers \u201cfirst token\u201d distribution after the prompt. This seems limited \u2013 would using token distributions at multiple positions improve the attack?\n\n\u2013 The distribution extraction from access to argmax/top-k logits is very expensive, requiring the number of API calls equal to the size of vocabulary at least. This is likely a concern if multiple positions other than the first token are used."
                },
                "questions": {
                    "value": "Please address points in the review above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699126241449,
            "cdate": 1699126241449,
            "tmdate": 1699636214348,
            "mdate": 1699636214348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sCaKVQU1rY",
                "forum": "t9dWHpGkPj",
                "replyto": "S4Wtj5eW9r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer xaWT"
                    },
                    "comment": {
                        "value": "Thanks for the comments and suggestions.\n\n> the absolute numbers are fairly low \u2013 the exact match scores are discouraging and the BLEU scores are also not very high. So I am not convinced if this is a serious concern for security in terms of prompt leakage.\n\nWe view this work as a proof-of concept, to show that prompts do indeed leak through LM outputs, which had not been known before. Bigger models and more training would likely improve absolute numbers. The work, as is, improves upon the best jailbreaking prompts in most settings.\n\nIn addition, we include new experiments that significantly improve both exact match and BLEU score for all out-of-distribution \nsettings. Please see the general response for more detail.\n\n> A more pointed analysis on the recoverability of sensitive information instead of recovering general purpose prompts might help establish the significance of the proposed attack more clearly.\n\nTo answer this question, we created a new dataset of prompts that include private data such as names, birthdays, and nationalities. Our models do a good job at reconstructing categorical national information (80% accuracy at countries and 50% on nationalities) while they struggle with exact numbers such as dates. Clearer analysis and examples are available in the new appendix section \"Private Prompt Dataset\" and the new Table 11.\n\n> Would using token distributions at multiple positions improve the attack?\n\nInverting a prompt using probability outputs from multiple positions is an interesting suggestion. Since this setting would allow us to model the prompt with strictly more information, it would likely improve inversion performance. Although this is easy to test with local models, our threat model is the one represented by standard APIs: a single API call provides probabilities for the next token at a single position.\n\n> Distribution extraction from access to argmax/top-k logits is very expensive.\n\nWe agree it has some cost and requires lots of calls, but not that it is \"very expensive\". Extracting the full logit distribution with argmax-only access requires a number of API calls proportional to the vocabulary size V.  For example, in the case of GPT-3.5 Instruct Turbo, using November 2023 pricing, extracting a single prompt can cost as low as $7 (`16 tokens * 3 calls per token * 100k tokens * $.0015 / 1k tokens) = $7.20`). Given the current value of these systems, this cost is not nothing, but it is also not that expensive compared to the engineering cost required to find the most effective prompt for some use cases."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677576994,
                "cdate": 1700677576994,
                "tmdate": 1700680123174,
                "mdate": 1700680123174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]