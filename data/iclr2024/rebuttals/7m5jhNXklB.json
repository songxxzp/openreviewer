[
    {
        "title": "VTruST : Controllable value function based subset selection for Data-Centric Trustworthy AI"
    },
    {
        "review": {
            "id": "F6FcmEYiS6",
            "forum": "7m5jhNXklB",
            "replyto": "7m5jhNXklB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2042/Reviewer_FQcd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2042/Reviewer_FQcd"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to construct high quality datasets for efficient training of trustworthy models. The authors propose a controllable framework for data-centric trustworthy AI, which allow users to control the trade-offs between different trustworthiness metrics of the constructed training datasets. They pose the training data valuation and subset selection problem as an online sparse approximation formulation and propose an online-version of the OMP algorithm for solving this problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper presents a single unified framework for balancing between the multiple value functions.\n\n- The authors formulate the value functions for multiple metrics including fairness, robustness, and accuracy.\n\n- The proposed framework can provide data-centric explanations behind its performance."
                },
                "weaknesses": {
                    "value": "- The goal of the proposed framework is to obtain a subset of training datapoints suited for trustworthiness metrics. However, the authors aim to augment the datapoints when defining the robust value function. The two objectives appear to be in conflict. The relationship between data selection and data augmentation within the proposed framework for robustness needs further elaboration.\n\n- To ensure trustworthiness, the authors propose to train the model using a subset of the training data points. Does the trustworthy model have lower accuracy compared to the model trained on the original dataset? It would be interesting if the authors could theoretically analyze the impact of the data selection on the accuracy of the trained model.\n\n- In their experiment, the authors evaluate the proposed framework's performance in terms of fairness and robustness independently. How does the framework perform when both fairness and robustness are considered together?\n\n- The efficiency analysis of the proposed framework is missing. The authors aim to construct high quality datasets for efficient training of trustworthy models. However, they neither analyze the framework\u2019s complexity nor provide its running time."
                },
                "questions": {
                    "value": "See above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702538295,
            "cdate": 1698702538295,
            "tmdate": 1699636135784,
            "mdate": 1699636135784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uT8v0NBhOB",
                "forum": "7m5jhNXklB",
                "replyto": "F6FcmEYiS6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2042/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful feedback and good questions! We address them below.\n\n> The relationship between data selection and data augmentation within the proposed framework for robustness needs further elaboration.\n\nIt is a well-known fact that augmentation of training data leads to a robust model [1]. However, it also leads to a larger training dataset with possibly unnecessary datapoints. The data subset selection removes the redundant data points thereby improving the dataset quality and data-centric interpretability as described in the paper. We have also added a discussion for the same under Section 3.2 - second paragraph.\n\n[1] Wang, Haotao, et al. \"Augmax: Adversarial composition of random augmentations for robust training.\" NeurIPS 2021.\n\n> Does the trustworthy model have lower accuracy compared to the model trained on the original dataset?\n\nThe trustworthy model can be adapted to have a better accuracy/error rate compared to the model trained on the original dataset. We can vary the parameter $\\lambda$ to achieve a balance between accuracy and fairness/robustness. We cite examples of instances where the trustworthy model performs better than the model trained on the original dataset.\n\nFigure 1(b) $\\lambda=0.5$ ,  VTrust achieves Label Robust ER = 0.19 and EO Disparity=0.11, which are better than the Wholedata-ST with Label Robust ER = 0.2 and EO Disparity = 0.19.\n\nFigure 1(c) $\\lambda=0.5$ , VTrust achieves Feature Robust ER = 0.2 and EO Disparity=0.102, which are better than the Wholedata-ST with Feature Robust ER = 0.213 and EO Disparity = 0.16.\n\n\n> How does the framework perform when both fairness and robustness are considered together?\n\nWe have added some results in Section 3.1 for the pareto curves in Figure 1, where we varied the $\\lambda$ to show the tradeoff between fairness and robustness (label robust using label flipping and feature robust using SMOTE-generated samples). We can observe that the trend is followed as expected and also performs better than Wholedata-ST in terms of all pairwise metrics. We shall add the results of other baselines in the final version.\n\n> Analyze the framework\u2019s complexity\n\nVTruST has a complexity of $\\mathcal{O}(kM(N-k))$ where $k$ = subset size, $M$ = size of the validation set, $N$ = size of training set, $M << N$. We had provided this complexity in Section 2.3 before \u201cConditions for Optimality of the Selected Subset\u201d."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470437896,
                "cdate": 1700470437896,
                "tmdate": 1700629950766,
                "mdate": 1700629950766,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PqcuE0jwEI",
            "forum": "7m5jhNXklB",
            "replyto": "7m5jhNXklB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2042/Reviewer_ohGL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2042/Reviewer_ohGL"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a controllable data selection framework that enables control of the tradeoffs between accuracy and trustworthiness metrics, which are fairness and robustness. \nThe trustworthiness metrics are incorporated into value functions by measuring the change of the metrics over each epoch and convexly combined with the validation loss. For efficiency, the data selection is done online with the iterative OMP algorithm, where data are chosen in a greedy manner, and the incoming data is decided to be added on the go by comparing with the already selected subset. In the case of the reached budget, incoming data can replace the already chosen one as well only if the incoming data can improve the online sparse approximation value. The authors also provide conditions for the selection of an optimal set of data. This work is said to be the first to attempt the problem of online sparse approximation for data selection. The experiments show good performance for fairness and robustness cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The method is intuitive and makes sense. \n+ It considers important trustworthiness metrics for the value function (generalization, fairness and robustness).\n+ The authors show good performance of VTruST."
                },
                "weaknesses": {
                    "value": "- No comparison to other more up-to-date (when considering the ICLR submission deadline) data valuation metrics such as Beta Shapley [Kwon, 2021], DAVINZ [Wu, 2022], Data OOB [Kwon, 2023], LAVA [Just, 2023], or CG-score [Nohyun, 2023], which are known to provide more robust values and are efficient to compute.\n\n- The fairness baselines also seem not recent (<= 2021).\n\n- If we are combining different value functions as a convex combination of each, how do they work out together? Are there not cancelling each other's effect?\n\n- The approach for robustness seems to be working only for image datasets."
                },
                "questions": {
                    "value": "- For augmentations that are used, it seems to be quite heuristic. Are there no better ways for robustness?\n\n- How do you apply augmentation to tabular data? \n\n- From the abstract and introduction, it seemed that both fairness and robustness metrics would be combined to achieve both desirable properties, but the main paper only provides the cases when they are separate.\n\n**Minor:** \n\n- Variables are often defined later than introduced, which are hard to trace:\n\n  + what is z0 and z1?\n  + and y0 and y1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2042/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2042/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2042/Reviewer_ohGL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822878525,
            "cdate": 1698822878525,
            "tmdate": 1699636135699,
            "mdate": 1699636135699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "98CKh7XxTg",
                "forum": "7m5jhNXklB",
                "replyto": "PqcuE0jwEI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2042/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful feedback and good questions! We address them below.\n\nQuestions:\n\n> Are there no better ways for robustness?  \n\nMany notions of robustness (e.g. adversarial robustness, etc.), and their tradeoffs with accuracy have been studied in the literature. We follow the notion of robustness defined in [1], which is not adversarial but based on random perturbations leading to image corruptions. These notions of robustness have also been followed in many subsequent papers, e.g. Augmax [2].\n\n[1] Hendrycks, Dan, and Thomas Dietterich. \"Benchmarking neural network robustness to common corruptions and perturbations.\"ICLR 2019.\n\n[2] Wang, Haotao, et al. \"Augmax: Adversarial composition of random augmentations for robust training.\" NeurIPS 2021.\n\n\n> How do you apply augmentation to tabular data? \n\nIn case of tabular data, we showed the tradeoff between Robustness and Fairness using the label-flipping experiment taken from SSFR [3]. However, in the revised document, we have also introduced feature-robustness using SMOTE [4] for augmenting samples.\n\nWe show the tradeoff between error rate, fairness, label robustness, and feature robustness in Figure 1 of the revised document. These experiments show the generality of our framework and also motivate the problems of studying tradeoffs between the various trustworthiness metrics.\n\n[3] Roh, Yuji, et al. \"Sample selection for fair and robust training.\" NeurIPS 2021.\n\n[4] Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002).\n\n\n\n> but the main paper only provides the cases when they are separate.\n\nIn the revised version, we have provided pairwise tradeoffs between various trustworthiness metrics in Section 3.1, since the pairwise tradeoffs are easy to visualize. However, our method can also be used to improve multiple trustworthiness metrics at the same time, whenever it is possible from the problem point of view. For example,\n\nFigure 1(b) $\\lambda=0.5$ , VTrust achieves Label Robust ER = 0.19 and EO Disparity=0.11, which are better than the Wholedata-ST with Label Robust ER = 0.2 and EO Disparity = 0.19.\n\nFigure 1(c) $\\lambda=0.5$ , VTrust achieves Feature Robust ER = 0.202 and EO Disparity=0.102, which are better than the Wholedata-ST with Feature Robust ER = 0.213 and EODisparity = 0.16.\n\n\n\n\n***Weaknesses:*** We would also like to rebut some reviewer comments in the weakness section.\n\n\n> No comparison to other more up-to-date (when considering the ICLR submission deadline) data valuation metrics such as Beta Shapley [Kwon, 2021], DAVINZ [Wu, 2022], Data OOB [Kwon, 2023], LAVA [Just, 2023], or CG-score [Nohyun, 2023], which are known to provide more robust values and are efficient to compute.\n\nThe reviewers mention the latest data valuation methods, which we have not compared with. We would like to point out that the methods (from 2023) focus on efficiency and are not specific to the predictive task. On the other hand, our framework is designed to be targeted towards one or more tasks using the value function metrics. Hence, we feel that the comparison with these methods would probably be unfair. However, for completeness, we provide the below comparison with CG-Score. As can be seen, our method outperforms the latest baselines as well in terms of Standard Accuracy (SA) and Robust Accuracy (RA).\n\n## CG-score vs VTruST on CIFAR10\n\nSubset size |    CG Score       |        VTruST\n***\n20% - SA, RA       |   84.11, 77.31    |    **92.25, 85.54**\n\n40% - SA, RA       |   89.13, 83.25    |    **94.74, 88.23**\n\n60% - SA, RA       |   92.56, 87.13    |    **94.77, 89.21**\n\n\n> The fairness baselines also seem not recent (<=2021)\n\nWe report a comparison with a recent paper -  FairDRO [1] using their metric DCA (Difference of Conditional Accuracy) below. The lower the value, the better is the fairness.\n\n## COMPAS\n\nMetrics  |    VTruST   |    FairDRO\n***\n\nER         |     **0.34**       |     0.43 \n \nDCA      |     **0.035**     |     0.04\n\n\n## Adult Census\n\nMetrics    |    VTruST   |   FairDRO\n***\n\nER           |      **0.18**      |      0.21\n\nDCA        |     **0.015**     |      0.02\n\n\nAs we can see, our method outperforms this recent baseline in both ER as well as DCA. Since the new method follows a different metric (DCA) compared to the existing baselines (EODisparity), we shall add this result in the appendix of the final version of our paper (due to space constraints).\n\n[1] Jung, Sangwon, et al. \"Re-weighting Based Group Fairness Regularization via Classwise Robust Optimization.\" ICLR 2023\n\n***Minor:***\n\nWe revised it in the updated version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470218568,
                "cdate": 1700470218568,
                "tmdate": 1700470809981,
                "mdate": 1700470809981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rfBZOSnpxo",
                "forum": "7m5jhNXklB",
                "replyto": "98CKh7XxTg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2042/Reviewer_ohGL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2042/Reviewer_ohGL"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response and additional results.\n\nMaybe it is unclear to me. However, your robustness notion is defined based on image robustness, then how does it apply to non-image datasets? Also, the augmentations that your provided for tabular datasets are used to address imbalance/fairness not robustness of the features. What are data augmentations you can use to tabular data for improved robustness?\n\nEven though the data valuation methods are not explicitly designed to address your defined tasks. However, it does not mean that they cannot be used to address these issues and that their selected subsets will be necessarily worse. CG-score does not utilize the validation data as compared to VTrust, which might be the reason for lower score.\n\nSince your method also uses epochs for selecting data, how is that the complexity your provided does not depend on that? Can you please elaborate? Thank you for your time!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645605671,
                "cdate": 1700645605671,
                "tmdate": 1700645605671,
                "mdate": 1700645605671,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1c5ik44hGF",
            "forum": "7m5jhNXklB",
            "replyto": "7m5jhNXklB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2042/Reviewer_7pAq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2042/Reviewer_7pAq"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a data-centric approach called VTruST for achieving trustworthy AI. It introduces a controllable data valuation framework with multiple trustworthiness metrics. The framework allows users to control the balance between these trustworthiness metrics. The problem of training data valuation and selection is formulated as an online sparse approximation problem, which is solved using a novel online version of OMP. The proposed method outperforms state-of-the-art baselines regarding fairness and robustness by using subsets of the original dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provides a unified framework for balancing between multiple trustworthiness metrics.\n2. The proposed method achieves better performance in terms of fairness and robustness compared to the baselines.\n3. The online sparse approximation algorithm efficiently selects subsets of training data based on the specified value functions.\n4. The paper includes theoretical derivations of the conditions used in the proposed method."
                },
                "weaknesses": {
                    "value": "1. The value functions of \"value\" and \"robustness\" are the same, except that one is the original data and the other is the augmented data. Why are these two metrics that require a trade-off?\n2. Here, you only control the weights of multiple objectives through $\\lambda$, and users can only adjust the weights. When users need to reset the weights, does your algorithm need to be rerun?\n3. Why didn't you compare with other method baselines in your robustness experiments? Additionally, in your experimental results, the two metrics do not have a trade-off relationship. Is it reasonable to set the objectives this way?"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698891100192,
            "cdate": 1698891100192,
            "tmdate": 1699636135617,
            "mdate": 1699636135617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eKZgVbtF3S",
                "forum": "7m5jhNXklB",
                "replyto": "1c5ik44hGF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2042/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful feedback and good questions! We address them below.\n\n\n> Why are these two metrics that require a trade-off?\n\nMany notions of robustness (e.g. adversarial robustness, etc.), and their tradeoffs with accuracy have been studied in the literature. We follow the notion of robustness defined in Hendryks et a.  [1], which is not adversarial but based on random perturbations leading to image corruptions. We use the same technique to generate the augmented dataset $\\mathcal{D}_a$. The value functions are defined as loss on validation datasets. Hence, they are different for accuracy (\u201cvalue\u201d according to reviewer) which uses $\\mathcal{D}\u2019$ and \u201crobustness\u201d which uses $\\mathcal{D}'_a$ (see Section 2.2). These value functions guide the selection of important training datapoints, which we find are different from each other. Hence, we report the tradeoff between these two value functions.\n\n[1] Hendrycks, Dan, and Thomas Dietterich. \"Benchmarking neural network robustness to common corruptions and perturbations.\"ICLR 2019.\n\n> When users need to reset the weights, does your algorithm need to be rerun?\n\nYes, the current algorithm needs to be re-run for different values of $\\lambda$. However, $\\lambda$ only affects the value function and not the features $X_i$ which are dependent on the training run. Hence, for each training run, we can perform subset set-selection for a grid of $\\lambda$ in parallel, without additional inference/parameter update cost.\n\n> Why didn't you compare with other method baselines in your robustness experiments?\n\nWe compared the image robustness algorithm with state-of-the-art AugMax (NeurIPS 2021) and SSR (NeurIPS 2021) in Table 5. We also compared our method with latest data valuation method [2] as a response to the second reviewer\u2019s comment and found ours to outperform the former method. We will be happy to compare with any suggested baselines. Besides, we have added some results for the robustness on tabular data in the revised document.\n\n[2] Nohyun, Ki, Hoyong Choi, and Hye Won Chung. \"Data valuation without training of a model.\" ICLR 2023.\n\n> Compare with other method baselines in your robustness experiments; The two metrics do not have a trade-off relationship\n\nWhile the metrics reported initially on the CIFAR10 showed a weaker tradeoff relationship, that might be a property of the dataset which already contains many images per class. We have added a figure (in Figure 3 of the revised document), which shows the tradeoff more clearly. We thank the reviewer for the comment. We shall report the tradeoff on TinyImagenet, shortly."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469055946,
                "cdate": 1700469055946,
                "tmdate": 1700469055946,
                "mdate": 1700469055946,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]