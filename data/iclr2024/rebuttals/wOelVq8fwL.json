[
    {
        "title": "Adapting LLM Agents Through Communication"
    },
    {
        "review": {
            "id": "uVRbECxGkf",
            "forum": "wOelVq8fwL",
            "replyto": "wOelVq8fwL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1240/Reviewer_7WiZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1240/Reviewer_7WiZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new way of adapting LLMs to new tasks through learning from communication. The authors introduce three communication patterns: Monologue with the LLM interact with the environment and learn from the system provided reward; Dialogue with two LLMs play different roles and the student LLM learn from the teacher's actions; and Analogue with a teacher model provide feedback and reward for student agent's actions. With the different communication patterns, the authors propose to tune the model with both language modeling loss and PPO. Experimental results on three different benchmarks demonstrate the effectiveness of LTC, with additional discussion and ablation validate the design choices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper introduces a new paradigm of adapting LLMs to downstream tasks that is different from instruction tuning and or prompting. The authors designs three communication patterns considering interaction with the environment and use of multiple LLM agents. While RL with environment and learn from stronger teacher LLMs are things that have been explored in previous works. The paper summarizes these into the three categories, and has additional design to better orchestrate the different components.\n2. Experimental results on three different benchmark datasets shows that the proposed learning from communication method can achieve better performance than direct instruction tuning."
                },
                "weaknesses": {
                    "value": "1. The authors proposes three different communication patterns which is nice, however, the experiments only study one pattern for each task. This makes it unclear on what are the pros and cons comparing these three communication patterns, whether each of them could generalize across different tasks and how to choose the right communication pattern. I feel it would be great to have more thorough comparison among the three patterns, and just with the baseline instruction tuning and prompting.\n2. The experiment is only done with the 7B model. It is not clear whether the method could apply to smaller models, and more importantly whether it could scale up and how much improvement it could bring to models of larger size."
                },
                "questions": {
                    "value": "1. Have you tried the three patterns across tasks, or they are only tested on the specific task?\n2. How many instruction data by GPT 3/4 (as mentioned in section 4.2) are used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1240/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1240/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1240/Reviewer_7WiZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829974527,
            "cdate": 1698829974527,
            "tmdate": 1699636050664,
            "mdate": 1699636050664,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i65gQ01BuH",
                "forum": "wOelVq8fwL",
                "replyto": "uVRbECxGkf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response to Reviewer 7WiZ"
                    },
                    "comment": {
                        "value": "Thank you very much for the constructive comments. We just open-sourced our code in an anonymous Github repo (https://github.com/AIAnonymous/LTC) to help the research community further explore this interesting topic. We'll de-anonymize the repo after the review process.\n1. Communication patterns across Tasks\n\n    To make the differences among patterns more clear, we reformulate the communication patterns as follows:\n    - (1) **Single-agent Monologue** allows a single agent to generate trajectories in a self-talk style and get interactive feedback from environments;\u00a0\n    - (2) **Multi-agent Dialogue** enables the multiple agents to interact with each other and external tools, and utilizes the reward signals provided by the environments;\u00a0\n    - (3) **Teacher-student Dialogue** is a variant of multi-agent dialog in which the teacher agent can provide not only linguistic feedback but also non-linguistic reward signals.\n\n    Different patterns are applied to tasks based on their difficulty and characteristics. In simulated Alfworld environments, the tasks are relatively easy. LLM agents can explore these tasks effectively. For instance, an agent trained with our single-agent Monologue communication achieves a 91% overall success rate. This rate even surpasses GPT-4's success rate of 89.6%. Therefore, there's no need to use multi-agent patterns with GPT-4 as a teacher in this scenario.\n\n    However, as shown in the following table, the situation is different for other tasks. Using only the single-agent Monologue patterns to train the LTC agent shows limited improvements. In these cases, the GPT-4 agent performs much better than our Llama-7B agent. To address this, we designed multi-agent Dialogue and teacher-student Dialogue patterns. These are specifically for HotpotQA and GSM8k tasks. By leveraging GPT-4 agents in the loop, we aim to further improve our agents' training.\n\n    | Method \\ Task\u00a0 \u00a0 \u00a0 |  GSM8K \u00a0 |  HotpotQA\u00a0 |  Alfworld |\n    |--------------|--------|------------|-----|\n    | GPT-4 prompting | 87.1 \u00a0 | 55.4  | 89.6 |\n    | Llama-7B-Tuning | 37.7 \u00a0 | \u00a028.2  | 78.4 |\n    | Llama-7B-LTC (Monologue) | 39.6 \u00a0 | 31.0 \u00a0 | 91.0 |\n    | Llama-7B-LTC (Dialogue)\u00a0 \u00a0 | 41.3 |  33.2\u00a0 | - |\n\n\n2. **Scaling up for smaller and larger LLMs**\n\n    To further investigate the scaling-up effectiveness of our method, we conduct extra experiments based on smaller (1B) and larger (13B) models on HotpotQA, the results are shown below:\u00a0\n\n    | Model \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | Method \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | EM score |\n    |----------------------|----------------------------------------------------|----------|\n    | PaLM-540B \u00a0 \u00a0 \u00a0 | CoT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 29.4 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | CoT-SC \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 33.4 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | ReAct\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 27.4 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | ReAct \u2192 CoT-SC \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 35.1 \u00a0 \u00a0 |\n    | GPT3-175B\u00a0 \u00a0 \u00a0 | ReAct\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 30.8 \u00a0 \u00a0 |\n    | PaLM-62B\u00a0 \u00a0 \u00a0 \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 32.6 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | CoT-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 25.2 \u00a0 \u00a0 |\n    | PaLM-8B\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 25.0 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | CoT-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 14.0 \u00a0 \u00a0 |\n    | Llama-1B[*](https://github.com/jzhang38/TinyLlama) \u00a0 \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 12.0 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | LTC\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 15.4 \u00a0 \u00a0 |\n    | Llama-7B\u00a0 \u00a0 \u00a0 \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 28.2 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | LTC\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 33.2 \u00a0 \u00a0 |\n    | Llama2-13B \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 33.8 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | LTC\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | **35.8** |\n\n\n    It shows that our method with a Llama-2-13B can even surpass the huge PaLM-540B by 0.7%, which is 41x larger. The tiny Llama model with 1B parameters can also benefit from our method, outperforming its tuning baseline. The improvement of our method is consistent when scaling up.\u00a0\n\n\n3. **Instruction data**\n\n    For Alfworld datasets, we collect 3.5k ReAct trails on its training set and only use the 1.9k successful trails as the instruction data to fine-tune the agents. For HotpotQA, we use 3.5k successful ReAct trail data from the training set. For GSM8k, we use 7.5k examples with CoT as the instruction fine-tuning data, the same scale as its own training data. We just open-sourced our code in an anonymous Github repo (https://github.com/AIAnonymous/LTC) to help the research community further explore this interesting topic. The instruction finetuning data are also included in this repo for reproducibility."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702673042,
                "cdate": 1700702673042,
                "tmdate": 1700705163359,
                "mdate": 1700705163359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C7qOfATeRG",
            "forum": "wOelVq8fwL",
            "replyto": "wOelVq8fwL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1240/Reviewer_SmMH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1240/Reviewer_SmMH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for adapting LLMs to target domains via RL in domain-specific environments, through distillation from oracle feedback (linguistic and scalar rewards). Evaluation is performed on three NLP tasks (grounded instruction following, multi-hop question answering, and math story reasoning). Results show that the proposed method, which affords exploration during learning with oracle feedback, improves over methods that finetune models only with static domain-specific training data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper explores several different tasks and compares against existing methods. It shows relatively strong results showing that exploration and language-like feedback can improve LLMs on domain specific tasks."
                },
                "weaknesses": {
                    "value": "Concern about experimental setup:\n* The ablation in Fig 5 doesn't seem to disentangle the contributions of exploration and \"communication\". In particular, performing exploration and receiving non-linguistic feedback (e.g., in the environment) doesn't seem to be evaluated. And on the other hand, training with static instruction-tuning data augmented with \"communication\" traces sampled using the same method as LTC (without sampling agent actions, but using the static demonstrations instead) is not evaluated.\n\nWriting feedback: I found the paper was relatively confusing in the terminology used.\n* I would suggest renaming the approach away from \"through communication\". This phrase is very vague, and there really isn't any true communication happening here at all: the learning setting is to just prompt the model(s) to replicate what a dialogue might look like in the domain. It is much more reminiscent of work on distillation and learning from oracles, like DAgger (as briefly mentioned in a footnote on page 7).\n* Framing this as continual learning also seems wrong. Fine-tuning with LTC is performed on some held-out set of examples from these datasets' training sets, right? So how is this continual?\n* Some of the language around LLMs is too anthropomorphizing. E.g., \"human-like\" LLM agents, \"the agent's brain\".\n* Minor nitpick, but the whole learning setup is RL, not just the training phase, as the exploration part of the proposed approach is certainly part of a general RL framework.\n* There seems to be a bug in Fig 7 with the \"question\".\n\nOne recurring issue with the clarity is that some terminology used is either overly ambiguous, or overly specific. For example:\n* \"generate trajectories in a self-talk style\"\n* references to \"masks\" in the introduction\n* \"decision-making\" as ALFWorld's task -- more precisely, the task is grounded instruction following\n* References to a value and a log-prob list in Section 3.1; this is underspecified and seems somewhat irrelevant without having introduced PPO in depth\n* \"Analogue\" as the third communication pattern -- why does it have this name?\n* \"action tokens\" in 4.2\n* In the experimental section this is particularly confusing. As a general point, some existing methods are introduced and compared against without intuition on why the experiment is performed (unless the reader is very familiar with the existing approaches). E.g., what data are ReAct-Tuning and CoT-Tuning trained on? There's a self-reference to Section 4.2.\n\nDistinguishing from some existing work would be useful. In particular, work on continual learning for language tasks such as:\n* Gao et al. 2022 (\"Simulating Bandit Learning from User Feedback for Extractive Question Answering\"), and 2023 (\"Continually Improving Extractive QA via Human Feedback\")\n* Kojima et al. 2021 (\"Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior\")\n* More recent work on RLAIF (e.g., Lee et al. 2023, or self-instruct, Yang et al. 2023)"
                },
                "questions": {
                    "value": "* The points on the righthand side of Figure 1 seem somewhat arbitrary. ICL is less efficient at inference time, but LTC requires a lot more compute for finetuning.\n* How are rewards derived? Is this different in each environment?\n* Did you evaluate the quality of the teacher agents? How often are they making mistakes?\n* Why not perform a 3x3 experiment combining the three datasets and the three \"communication\" patterns?\n* What are the actual steps for training? It seems the model starts as LLaMA-7B, then is instruction-tuned on a domain-general dataset, then instruction fine-tuned with data sampled from GPT-3/4, and _then_ LTC is applied? \n* What is the stopping condition for LTC training? Just when the training data has ran out?\n* Why are the results in Table 1 very low precision?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871969651,
            "cdate": 1698871969651,
            "tmdate": 1699636050589,
            "mdate": 1699636050589,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2S4pczf6oH",
                "forum": "wOelVq8fwL",
                "replyto": "C7qOfATeRG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response to Reviewer SmMH (1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for the constructive comments. We have revised our draft based on your suggestions. We just open-sourced our code in an anonymous Github repo (https://github.com/AIAnonymous/LTC) to help the research community further explore this interesting topic. We'll de-anonymize the repo after the review process.\n\n**Ambiguous terminology**\n\nThese comments are very helpful in our revision, now we have updated our draft based on your suggestions. The core change is the definitions of the communication patterns. To make it more clear,  we reformulate the communication patterns as follows:\n- (1) **Single-agent Monologue** allows a single agent to generate trajectories in a self-talk style and get interactive feedback from environments;\u00a0\n- (2) **Multi-agent Dialogue** enables the multiple agents to interact with each other and external tools, and utilizes the reward signals provided by the environments;\u00a0\n- (3) **Teacher-student Dialogue** is a variant of multi-agent dialog in which the teacher agent can provide not only linguistic feedback but also non-linguistic reward signals.\n\n**Distinguishing from continual learning**\n\nYes, we should not frame our methods as continual learning, so we rewrite all parts in the paper that mention continual learning to avoid misleading. With our new formulation of the communication patterns, our current paper focuses on training the LLM agents for single-agent and multi-agent environments. \n\nExperimental setup: performing exploration and receiving non-linguistic feedback\nWith our new formulation of the communication patterns, our work mainly targets on studying agent adaptation algorithms in various environments (i.e., single/multi-agent setup). We think the reviewer wants to ask 1. the comparison of \u201csingle-agent\u201d v.s. \u201cmulti-agent\u201d setup in one particular task. 2. reinforce the learning algorithm with environment reward v.s. (environment reward + feedback signals from other agents).\n\nFor \u201csingle-agent\u201d v.s. \u201cmulti-agent\u201d, we add the experiments on HotpotQA (in Table 2) to show the the comparison between single-agent monologue and multi-agent dialogue, which shows that using Multi-agent Dialogue can further improve the performance:\u2028\n| Method\u00a0 \u00a0 \u00a0 |  EM score\u00a0 |\n|--------------|--------|\n| Llama-7B-Tuning\u00a0 | \u00a028.2  | \n| Llama-7B-LTC (Single-agent Monologue) | 31.0 \u00a0 | \n| Llama-7B-LTC (Multi-agent Dialogue)\u00a0  |  **33.2**\u00a0 | \n\nFor environment reward v.s. (environment reward + feedback signals from other agents), we add the experiments on GSM8k (in Table 3) to show the comparison between single-agent monologue and teacher-student dialogue, which shows that using teacher-student Dialogue can further improve the performance by providing the direct reward signals from the GPT-4 agent:\u2028\n| Method\u00a0 \u00a0 \u00a0 |  Acc\u00a0 |\n|--------------|--------|\n| Llama-7B-Tuning\u00a0 | \u00a037.7  | \n| Llama-7B-LTC (Single-agent Monologue) | 39.6 \u00a0 | \n| Llama-7B-LTC (Teacher-student Dialogue)\u00a0  |  **41.3**\u00a0 | \n\n\n**Experimental setup: training with static instruction-tuning data**\n\nWe want to confirm that you are suggesting two more baselines: 1. SFT on (Instruction-tuning dataset + static few-shot dataset); 2. SFT on (instruction-tuning dataset + communication traces).\u00a0\nFor the 1st baseline, we don\u2019t see the motivation of adding \u201cstatic demonstration\u201d in the prompt of the training set, since our method does not require few-shot examples as prompt.\nFor the 2nd baseline, we present the comparison results in Figure 5, the \u201cw/o PPO\u201d curve can be an example of SFT on communication traces since it uses the exploration traces as the training data and only does SFT on the text part of the data. Our takeaway from the Fig 5 is that what really matters is the communication traces generated by the agent itself, which is better than the SFT data generated by other powerful LLMs."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714919699,
                "cdate": 1700714919699,
                "tmdate": 1700714919699,
                "mdate": 1700714919699,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KYekvsyZav",
                "forum": "wOelVq8fwL",
                "replyto": "C7qOfATeRG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response to Reviewer SmMH (2/2)"
                    },
                    "comment": {
                        "value": "**Rename the approach**\n\nWe appreciate the reviewer's suggestion. In our multi-agent framework, we attribute performance improvement primarily to effective communication, where multiple agents exchange information throughout the process without manual intervention. This is evident in our study, where the student agent, by communicating with teacher agents or the environment, learns more effectively than when limited to static training data.\n\nTo further illustrate the role of communication, we introduced the Chameleon game environment in our experiments. Detailed in section 4.2 of our updated paper, the Chameleon game involves two roles: chameleon and non-chameleon. In our setup, all players are played by the Llama2-7B model during training, but in testing, one player is randomly selected to be played by our trained agent, with the others played by GPT-4. The table below showcases the winning rates of our trained agent against GPT-4 players:\n\n| Method \\ #players \u00a0 \u00a0 \u00a0 | n=3\u00a0 \u00a0 | n=4\u00a0 \u00a0 | n=5\u00a0 \u00a0 |\u00a0 overall |\n|--------------|--------|--------|--------|---------|\n\u00a0\u00a0\u00a0| Tuning | 20.8 \u00a0 | 20.3 \u00a0 | 23.8 \u00a0 | 21.9\u00a0 \u00a0 |\n| LTC\u00a0(multi-agent dialogue) \u00a0 | **22.9** | **23.4** | **27.5** | **25.0**\u00a0 |\n\nThe results show that our LTC method surpasses instruction tuning baselines by 3.1% in overall winning rates. This underscores the importance of communication in the Chameleon game, which demands bluffing, deduction, and strategic interaction between agents and their environment for success.\n\n\n**The training phase**\n\nSure, we\u2019ve renamed the Training Phase as the Updating Phase in the text and figures. Now the iterative training is a standard reinforcement learning pipeline, which has the exploration phase and the updating phase.\n\n\n**The bug in Figure 7**\n\nThanks for reminding us, the question text is forgotten to replace when using the template of Fig3, we\u2019ve updated it with the correct question text.\n\n\n**The righthand side of Figure 1**\n\nYes, the original Figure 1 does not properly show our core ideas. So we\u2019ve removed the original Figure 1 and changed it to the current Figure 1 with a more clear diagram to show our LTC could process both single-agent and multi-agent environments, which is the core of our method. \n\n\n**Experiments combining the datasets and patterns**\n\nDifferent patterns are applied to tasks based on their difficulty and characteristics. In simulated Alfworld environments, the tasks are relatively easy. LLM agents can explore these tasks effectively. For instance, an agent trained with our single-agent Monologue communication achieves a 91% overall success rate. This rate even surpasses GPT-4's success rate of 89.6%. Therefore, there's no need to use multi-agent patterns with GPT-4 as a teacher in this scenario.\nHowever, as shown in the following table, the situation is different for other tasks. Using only the single-agent Monologue patterns to train the LTC agent shows limited improvements. In these cases, the GPT-4 agent performs much better than our Llama-7B agent. To address this, we designed multi-agent Dialogue and teacher-student Dialogue patterns. These are specifically for HotpotQA and GSM8k tasks. By leveraging GPT-4 agents in the loop, we aim to further improve our agents' training.\n\n| Method \\ Task\u00a0 \u00a0 \u00a0 |  GSM8K \u00a0 |  HotpotQA\u00a0 |  Alfworld |\n|--------------|--------|------------|-----|\n| GPT-4 prompting | 87.1 \u00a0 | 55.4  | 89.6 |\n| Llama-7B-Tuning | 37.7 \u00a0 | \u00a028.2  | 78.4 |\n| Llama-7B-LTC (Monologue) | 39.6 \u00a0 | 31.0 \u00a0 | 91.0 |\n| Llama-7B-LTC (Dialogue)\u00a0 \u00a0 | 41.3 |  33.2\u00a0 | - |\n\n**Reward**\n\nFor the single-agent monologue and multi-agent dialogue patterns, the rewards are provided by the environments. For the teacher-student dialogue pattern, the rewards come from both the environment and the teacher agent.\n\n**The actual steps for training**\n\nThere are only two steps starting from the pre-trained LLMs(I.e, Llama): \n- 1. We use the collected task-specific instruction fine-tuning data to fine-tune the Llama to help it learn the instruction-following ability (like ReAct/CoT). And the output models are our Llama-Tuning baseline in our experiments.\n- 2. We put the fine-tuned agents in our communication patterns to explore the environments and collected the traces for training iteratively.\n\n**Stopping condition**\n\nIn our experiments, we set the stopping condition as a max iteration number, and this number is chosen considering our computation resource and the performance of the model. This number could theoretically be increased to infinity in the case of online learning, though it is unclear if that would have unintended side effects.\n\n**low-precision in Table 1**\n\nThe top half of the Table is obtained from Table 3 of the ReAct Paper, which uses the same precision, so we follow their settings for a fair comparison."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717781932,
                "cdate": 1700717781932,
                "tmdate": 1700717781932,
                "mdate": 1700717781932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e5smMdHsSz",
            "forum": "wOelVq8fwL",
            "replyto": "wOelVq8fwL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1240/Reviewer_2FJ7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1240/Reviewer_2FJ7"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new training paradigm called Learning through Communication (LTC), which enables large language model (LLM) agents to adapt to new tasks through interaction. The major contributions include the LTC method itself, the introduction of task-specific communication patterns (Monologue, Dialogue, Analogue), and empirical evidence that LTC outperforms instruction-tuning baselines on decision-making, knowledge-intensive reasoning, and numerical reasoning tasks. The paper demonstrates LTC's effectiveness and efficiency, with significant gains in success rates and accuracy, along with reduced token usage during inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Novelty of the Learning Method: The proposed Learning through Communication (LTC) framework is a commendable advancement. It is an inventive approach that enables language models to dynamically adapt to new tasks through iterative interactions. The methodology is well-conceived, blending language modeling with reinforcement learning objectives in a manner that is both theoretically sound and practically viable.\n\nComprehensive Evaluation: The authors have conducted a rigorous empirical evaluation of the LTC framework across a variety of tasks and datasets. The breadth of the evaluation\u2014spanning decision-making, knowledge-intensive reasoning, and numerical reasoning\u2014is impressive. This comprehensive testing not only demonstrates the applicability of LTC to a wide range of tasks but also provides a convincing argument for its efficacy compared to existing baselines."
                },
                "weaknesses": {
                    "value": "Model Comparisons: The LTC method's performance is compared to that of models with and without tuning. However, the paper states that the combined method of ReAct and CoT-SC surpasses LTC by 1.9%. This suggests that while LTC has strengths, there may be specific configurations of existing methods that outperform it, which could be a point of concern regarding the robustness and superiority of LTC\u200b1\u200b.\n\nScope of Evaluation: While LTC is shown to perform well across three tasks, the evaluation might still be limited in scope. The paper hints at future work to explore more diverse communication patterns and involve communication with humans. This suggests that the current evaluation may not fully capture the LTC's performance in more varied or complex interactive settings\u200b such as ScienceWorld, and Mind2Web.\n\nGeneralization to Human Interaction: The paper outlines future work to involve communication with humans during the learning process, which is not covered in the current evaluation. This omission indicates that the paper does not address the challenge of human-agent interaction, which is critical for practical applications of LLM agents\n\nMinor point: \nReferences that can be helpful for discussion:\n1) ScienceWorld: Is your Agent Smarter than a 5th Grader?\n2) Mind2Web: Towards a Generalist Agent for the Web\n3) AgentBench: Evaluating LLMs as Agents\n4) SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks\n5) ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"
                },
                "questions": {
                    "value": "1. The LTC method relies heavily on predefined communication patterns. Have the authors considered how LTC might generalize to tasks that require more flexible or less structured forms of communication?\n\n2. Given that certain combined methods like ReAct and CoT-SC have outperformed LTC, what are the authors' perspectives on the limitations of LTC in its current form? Are there specific enhancements they are considering to improve upon these existing methods?\n\n3. The paper suggests future work will involve human communication. Can the authors provide preliminary insights into how they expect human-in-the-loop interactions to affect the learning process and the adaptability of the LTC method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698888631969,
            "cdate": 1698888631969,
            "tmdate": 1699636050517,
            "mdate": 1699636050517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JlFxbP1CLg",
                "forum": "wOelVq8fwL",
                "replyto": "e5smMdHsSz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response to Reviewer 2FJ7 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for the constructive comments. We just open-sourced our code in an anonymous Github repo (https://github.com/AIAnonymous/LTC) to help the research community further explore this interesting topic. We'll de-anonymize the repo after the review process.\n\n1. **Model Comparisons**\n\n    We thank the reviewer for noting the comparison between LTC and other methods such as ReAct and CoT-SC in Table 2. We would like to point out that results on ReAct and CoT-SC are reported based on the PaLM (540B) model, while LTC is on Llama (7B).\u00a0To further investigate the scaling-up effectiveness of our method, we conduct extra experiments based on smaller (1B) and larger (13B) models on HotpotQA, the results are shown below:\u00a0\n\n    | Model \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | Method \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | EM score |\n    |----------------------|----------------------------------------------------|----------|\n    | PaLM-540B \u00a0 \u00a0 \u00a0 | CoT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 29.4 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | CoT-SC \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 33.4 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | ReAct\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 27.4 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | ReAct \u2192 CoT-SC \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 35.1 \u00a0 \u00a0 |\n    | GPT3-175B\u00a0 \u00a0 \u00a0 | ReAct\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 30.8 \u00a0 \u00a0 |\n    | PaLM-62B\u00a0 \u00a0 \u00a0 \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 32.6 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | CoT-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 25.2 \u00a0 \u00a0 |\n    | PaLM-8B\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 25.0 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | CoT-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 14.0 \u00a0 \u00a0 |\n    | Llama-1B[*](https://github.com/jzhang38/TinyLlama) \u00a0 \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 12.0 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | LTC\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 15.4 \u00a0 \u00a0 |\n    | Llama-7B\u00a0 \u00a0 \u00a0 \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 28.2 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | LTC\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 33.2 \u00a0 \u00a0 |\n    | Llama2-13B \u00a0 | ReAct-Tuning \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 33.8 \u00a0 \u00a0 |\n    |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | LTC\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | **35.8** |\n\n\n    It shows that our method with a Llama-2-13B can even surpass the huge PaLM-540B by 0.7%, which is 41x larger. The tiny Llama model with 1B parameters can also benefit from our method, outperforming its tuning baseline. The improvement of our method is consistent when scaling up.\u00a0\n\n\n2. **Scope of Evaluation**\n\n    We conducted some extra experiments on a more challenging multi-agent environment setup (i.e., Chameleon Game). Preliminary results are updated in the paper ( pls see section 4.2 for details).\n    There are two roles in the [Chameleon](https://github.com/Farama-Foundation/chatarena) game: chameleon and non-chameleon. The topic of the secret word will be first revealed to all the players. Then the secret word will be revealed to non-chameleons. Non-chameleons try to identify the chameleon without giving away the secret word, while the chameleon tries to blend in and guess the word. \n    In our training, all the players are played by the same Llama2-7B model. While in the testing, to get the winning rate of our trained agent against GPT4, only 1 player is randomly picked to use our trained agent as the backend, and other players are played by GPT4. The following results are their winning rate against GPT-4 players:\n\n    | Method \\ #players \u00a0 | n=3\u00a0 \u00a0 | n=4\u00a0 \u00a0 | n=5\u00a0 \u00a0 |\u00a0 overall |\n    |-----------|--------|--------|--------|---------|\n    | Tuning | 20.8 \u00a0 | 20.3 \u00a0 | 23.8 \u00a0 | 21.9\u00a0 \u00a0 |\n    | LTC\u00a0 \u00a0 | **22.9** | **23.4** | **27.5** | **25.0**\u00a0 |\n\n\n    As shown above, LTC outperforms the instruction tuning baselines by 3.1% overall winning rates. To further clarify, we aren\u2019t trying to demonstrate that multi-agent setup is superior to single-agent setup in experiments, but we show that \u201clearning through communication\u201d allows agents to be online-trainable in various complex single/multiple agent environments through its conversation data and feedback signals."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696111714,
                "cdate": 1700696111714,
                "tmdate": 1700705195373,
                "mdate": 1700705195373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4ijb5KqaMp",
                "forum": "wOelVq8fwL",
                "replyto": "e5smMdHsSz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response to Reviewer 2FJ7 (2/2)"
                    },
                    "comment": {
                        "value": "3. **flexible forms of communication**\n\n    Great question. We explored recent open source multi-agent libraries (i.e., AutoGen, ChatArena, Ai-Town etc), however most of them require users to predefine the communication patterns as well, while some of them define some randomness in the environment to allow the agent to communicate with another random agent (i.e., AI-town). We are actively exploring to build a large-scale multi-agents environment for simulating general collaboration and competition tasks between random-agents. We are hoping such a sandbox system could be a good testbed for comparing different LTC strategies.\n\n4. **Limitation of LTC**\n\n    As shown above, we have demonstrated that by scaling up LTC to Llama2-13B, we can achieve superior results, surpassing even the larger PaLM-540B combined methods. This underscores the scalability and potential of LTC. Actually our LTC is orthogonal to these promising methods like ReAct and CoT, which focus on prompting and we are focusing on training. So the integration of our methods with these advanced prompting methods to improve the communication effectiveness is what we are investigating to further enhance our framework.\n    Of course there are also some limitations of LTC:\n    - Cost: computation resource requirement would be huge if we allow personal assistant agents to be trainable (alternative solution could be hosting LTC agents on personal edge device)\u00a0\n    - Stability: LTC adopts an RL approach for iterative learning. Thus it doesn\u2019t guarantee to procedure a better-performance model after every epoch of training.\u00a0 \n\n5. **Generalization to Human Interaction**\n\n    The integration of human-in-the-loop interactions with the LTC agent could significantly enhance the model's adaptability and learning process, particularly in terms of customization and personalization. By directly incorporating human feedback, LTC can tailor its responses more effectively to individual user preferences and requirements. This approach not only ensures that the technology adapts to the specific needs and contexts of different users but also improves user engagement and satisfaction. Such personalized interaction is crucial for the broad applicability of LTC in diverse practical scenarios, where unique user experiences and customized responses are highly valued.\n\n\n6. **References**\n\n    Thanks for providing these helpful References, we\u2019ve add them into our discussion of related works"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696172640,
                "cdate": 1700696172640,
                "tmdate": 1700696172640,
                "mdate": 1700696172640,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3PXUQRRqcE",
            "forum": "wOelVq8fwL",
            "replyto": "wOelVq8fwL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1240/Reviewer_CuLf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1240/Reviewer_CuLf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method that uses LLM based agents to solve tasks that require reasoning and sequential decision making. Authors propose a paradigm for learning through communication where communication can be 1.) monologue - self communication, 2.) dialogue - multi-agent communication or 3.) analogue - communicating with a teacher agent. Authors provide results on three environments  1.) ALFworld - sequential decision making environment that require performing household tasks, 2.) GSM8K - grade school math problem solving and 3.) HotPotQA - reasoning and language understanding."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to read.\n- The paper provides a  comprehensive summary of related work and the contributions of the paper are well placed in the relevant literature\n- Environments considered in the paper are diverse and capture essential features of reasoning and sequential decision making"
                },
                "weaknesses": {
                    "value": "- Multi-agent interactions highlighted in the paper are not properly formulated and motivated.\n- Authors are not considering any environments that require multi-agent interactions. This reviewer is not convinced that the paper adds value in terms of communication between multiple agents that would lead to better performance.\n- Authors have not provided the code and hence the results are not reproducible."
                },
                "questions": {
                    "value": "- According to the provided discussion the only difference between dialogue and analogue is in analogue, teacher roles can directly provide reward signals and new examples. What is the significance of this distinction?\n- How does this approach extend to multi-agent environments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699496705709,
            "cdate": 1699496705709,
            "tmdate": 1699636050433,
            "mdate": 1699636050433,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jh4OgDG9DE",
                "forum": "wOelVq8fwL",
                "replyto": "3PXUQRRqcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Our Response to Reviewer CuLf"
                    },
                    "comment": {
                        "value": "Thank you very much for the constructive comments. We just open-sourced our code in an anonymous Github repo (https://github.com/AIAnonymous/LTC) to help the research community further explore this interesting topic. We'll de-anonymize the repo after the review process.\n\n 1. ____The multi-agent environments____\n\n    Thanks for your suggestion. We conducted some extra experiments on a multi-agent environment setup (i.e., Chameleon Game). Preliminary results are updated in the paper ( pls see section 4.2 for details). There are two roles in the [Chameleon](https://github.com/Farama-Foundation/chatarena) game: chameleon and non-chameleon. The topic of the secret word will be first revealed to all the players. Then the secret word will be revealed to non-chameleons. Non-chameleons try to identify the chameleon without giving away the secret word, while the chameleon tries to blend in and guess the word. \n    In our training, all the players are played by the same Llama2-7B model. While in the testing, to get the winning rate of our trained agent against GPT4, only 1 player is randomly picked to use our trained agent as the backend, and other players are played by GPT4. The following results are their winning rate against GPT-4 players:\n    \n    | Method \\ #players \u00a0 \u00a0 \u00a0 | n=3\u00a0 \u00a0 | n=4\u00a0 \u00a0 | n=5\u00a0 \u00a0 |\u00a0 overall |\n    |--------------|--------|--------|--------|---------|\n    | Tuning | 20.8 \u00a0 | 20.3 \u00a0 | 23.8 \u00a0 | 21.9\u00a0 \u00a0 |\n    | LTC\u00a0 \u00a0 | **22.9** | **23.4** | **27.5** | **25.0**\u00a0 |\n\n\n    As shown above, LTC outperforms the instruction tuning baselines by 3.1% overall winning rates. To further clarify, we aren\u2019t trying to demonstrate that multi-agent setup is superior to single-agent setup in experiments, but we show that \u201clearning through communication\u201d allows agents to be online-trainable in various complex single/multiple agent environments through its conversation data and feedback signals.\n\n\n2. _____Multi-agent interactions are not properly formulated_____\n\n    To further highlight the multi-agent interaction and make it more clear, we reformulate the communication patterns as follows:\n    - (1) **Single-agent Monologue** allows a single agent to generate trajectories in a self-talk style and get interactive feedback from environments;\u00a0\n    - (2) **Multi-agent Dialogue** enables the multiple agents to interact with each other and external tools, and utilizes the reward signals provided by the environments;\u00a0\n    - (3) **Teacher-student Dialogue** is a variant of multi-agent dialog in which the teacher agent can provide not only linguistic feedback but also non-linguistic reward signals.\n\n3. ____The difference between Dialogue and Analogue____ \n\t\n    As shown in our new multi-agent formulations, both the second and third patterns are designed for dialogue between multiple agents. The difference is that the agents in the second pattern can only receive the rewards provided by the environments, while the Teacher-student Dialogue patterns enable the teacher agent to directly give the reward to the student."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666331878,
                "cdate": 1700666331878,
                "tmdate": 1700666331878,
                "mdate": 1700666331878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]