[
    {
        "title": "CogVLM: Visual Expert for Large Language Models"
    },
    {
        "review": {
            "id": "65fTVbAa0D",
            "forum": "c72vop46KY",
            "replyto": "c72vop46KY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3349/Reviewer_U75G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3349/Reviewer_U75G"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an innovative VLM framework that integrates a pre-trained image encoder and a language model through a deeper fusion and fine-tuning process. The method has demonstrated SOTA performance on multiple Vision-Language benchmarks. Furthermore, the model incorporates an alignment stage to further enhance its capabilities."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method represents a novel approach to multimodal techniques, distinguishing itself from previous Vision-Language Models (VLMs) like Flamingo and PaLI. The method's innovative and effective feature fusion into the language model sets it apart.\n\n2. The proposed method has achieved SOTA performance on a range of Vision-Language benchmarks, spanning image captioning, Visual Question Answering, and visual grounding tasks.\n\n3. The paper meticulously provides all experimental details, and the ablation study helps to validate the design components, enhancing the overall robustness of the research."
                },
                "weaknesses": {
                    "value": "Comparing the proposed method to earlier approaches such as PaLI, CoCa, and Flamingo may not be entirely fair. These prior methods do not incorporate the SFT stage, making it unclear how the model performs before this crucial phase."
                },
                "questions": {
                    "value": "Since the pretrained image encoder and LM and went through VLM finetuning, their original behavior may have changed. I wonder what the visual eval (linear probe, zero shot) will be for this finetuned encoder, compared to original model. How LM performance got affected?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698530201906,
            "cdate": 1698530201906,
            "tmdate": 1699636284701,
            "mdate": 1699636284701,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0N3qbsEJOG",
                "forum": "c72vop46KY",
                "replyto": "65fTVbAa0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive review and important comments. Please allow us to address your points in detail:\n\n> 1. Comparing the proposed method to earlier approaches such as PaLI, CoCa, and Flamingo may not be entirely fair. These prior methods do not incorporate the SFT stage, making it unclear how the model performs before this crucial phase.\n\nThis is a misunderstanding. We did not incorporate SFT to evaluate the benchmarks, so the evaluation is fair. See 2. of \"response to common concerns\" for details.\n\n> 2. Since the pretrained image encoder and LM and went through VLM finetuning, their original behavior may have changed. I wonder what the visual eval (linear probe, zero shot) will be for this finetuned encoder, compared to original model. How LM performance got affected?\n\nThis is also a misunderstanding. During the pretraining, the parameters of both the ViT image encoder and the LLM were completely **frozen**, thus the ViT is not affected, because it is also frozen in SFT.  LM is trainable during SFT with a very small learning rate of 1e-6 for 8,000 iterations, which might affect the language style but not much on knowledge.  \n\nWe find your concerns are mainly from two important misunderstandings. Could you increase your rating if our responses clarify it?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242581188,
                "cdate": 1700242581188,
                "tmdate": 1700641850381,
                "mdate": 1700641850381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TPC8H6Y6Gh",
                "forum": "c72vop46KY",
                "replyto": "65fTVbAa0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer: Thanks again for your careful and valuable comments! Since the rebuttal discussion is due soon, we\u2019ll be appreciated to know whether our replies have addressed your questions. If there are any further clarifications required or any other concerns, please feel free to contact us. Many thanks!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640224914,
                "cdate": 1700640224914,
                "tmdate": 1700640224914,
                "mdate": 1700640224914,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bQk9tPHaig",
            "forum": "c72vop46KY",
            "replyto": "c72vop46KY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3349/Reviewer_c9aL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3349/Reviewer_c9aL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a powerful open-source visual language foundation model, CogVLM, which adds a trainable visual expert module in the attention and FFN layers, to allow for deep fusion between visual and textual features. By pretraining on a large-scale image-text aligned data and benchmark datasets and utilizing a multi-stage training strategy, it achieves state-of-the-art performance on a variety of classic cross-modal benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Strong performance on a variety of popular benchmarks, including VQA, Image Caption, Visual Grounding, Document Visual Tasks, and some GPT-4 based evaluation. \n2. One pioneering work to address the shallow alignment problem for cross-modal learning by introducing visual expert module in MLLM.\n3. Open-source MLLM for better promoting the cross-modal research."
                },
                "weaknesses": {
                    "value": "1. The idea is not that novel, compared with BEIT-3 and VLMo, which also introduces different modality expert structures, although this work makes some changes to make it work in the era of LLM. \n2. The VQA/Image Caption model, Visual Grounding model and Chat model are three different models, I am wondering how the performance can be if all these models are a unified one? Since GPT-4V may be a unified one. \n3. Although I appreciate the excellent performance it achieves, the visual backbone is ViT-e, and the input resolution is 490 * 490, also the parameter size of LLM doubles, which makes the comparason a little hard."
                },
                "questions": {
                    "value": "1. Do you have more experiment on the archtecture of visual expert module and more insight about which part of the layers should be shared and which module should have separate parameters? \n2. For the generalist performance, is it possible that a model can achieve best performance on both real-world chat and benchmark datasets? since this paper has three separate training procedures to make it best in each individual dataset. If there exist some gaps between different kinds of datasets, how can the architecture be designed to better address this problem?\n3. Have you observed some new emergent ability in this strong MLLM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683372222,
            "cdate": 1698683372222,
            "tmdate": 1699636284614,
            "mdate": 1699636284614,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A52h6DNXX9",
                "forum": "c72vop46KY",
                "replyto": "bQk9tPHaig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive review and insightful comments. Please allow us to address your points in detail:\n> 1. The idea is not that novel, compared with BEIT-3 and VLMo, which also introduces different modality expert structures, although this work makes some changes to make it work in the era of LLM.\n\nOur response to this concern is in  1. of \"reply to common concerns\" above. The MoE is definitely not a new idea, but in the current LLM+ paradigm, it is very valuable to rediscover its effectiveness as a way to better scaling up. \n\n> 2. The VQA/Image Caption model, Visual Grounding model and Chat model are three different models, I am wondering how the performance can be if all these models are a unified one? Since GPT-4V may be a unified one.\n\nPreviously, we did not unify the models for VQA, chat, and grounding tasks due to their diverse output requirements, such as single-word answers for VQA, lengthy analytical responses for chat, and coordinate-heavy answers for grounding. However, our latest experiments indicate that VQA and chat models can be effectively unified. We will soon release a unified checkpoint where simple prompt modifications like \"Short answer:\" and \"Answer:\" enable the model to perform strongly in both VQA and chat domains, as shown in our latest results in Table 1(The touchstone score increase from 663 to 740). \n\nThe grounding task is slightly different; our results show it slightly reduces the model\u2019s captioning ability. For instance, performance on the COCO dataset dropped by 2 points and on the NoCaps dataset by 1.5 points, potentially due to the model\u2019s limited understanding of coordinates. Conversely, grounding training improved the model's performance by 0.8 point on the  VQAv2 dataset, likely due to enhanced object localization capabilities. We believe a fully unified model is achievable and are working towards this, with adaptations needed for grounding tasks, such as adding specific coordinate-representing words to the vocabulary and loosening parameters in the word embedding layer and LM head during training.\n> 3. Although I appreciate the excellent performance it achieves, the visual backbone is ViT-e, and the input resolution is 490 * 490, also the parameter size of LLM doubles, which makes the comparason a little hard.\n\nOur comparison is fair enough actually. It's common for multimodal models to upscale resolution for downstream tasks to enhance performance, similar to BLIP-2 (490 * 490) and Qwen-VL (448 * 448), with Google\u2019s PaLI-X even using 896 * 896 resolution for image inputs.\nFor ViT-E, we also experimented with the OpenCLIP-L visual encoder, comprising 300M parameters, and found its performance only slightly worse than that of ViT-E. For instance, the score on the VQAv2 test set was 83.4 with OpenCLIP-L compared to 84.7 with ViT-E, and 88.6 on the Visual-7W dataset versus 90.6 with ViT-E.\n> 4. Do you have more experiment on the archtecture of visual expert module and more insight about which part of the layers should be shared and which module should have separate parameters? \n\nIn the ablation study part, we experimented this. The finding basically shows that the best way is to insert new parameters in each layer.\n> 5. For the generalist performance, is it possible that a model can achieve best performance on both real-world chat and benchmark datasets? since this paper has three separate training procedures to make it best in each individual dataset. If there exist some gaps between different kinds of datasets, how can the architecture be designed to better address this problem?\n\nYes, we merged the training of chat and generalist on benchmarks, which is reported in the response to the second question. The results are even better on chat and basically the same for benchmarks (generalist).\n\n> 6. Have you observed some new emergent ability in this strong MLLM?\n\nThe definition of emergent abilities in the multimodal domain is not entirely clear. However, in our recent evaluations, we have indeed observed that our model shows significantly greater improvements in challenging tasks compared to other models, beyond what we see in VQAv2. For example, in the OCR&Spat task defined in MM-Vet, our model scored 59 compared to 14 for InstructBLIP, and in the Rec&Know task, our score was 39 while MiniGPT-4 scored 0."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242385354,
                "cdate": 1700242385354,
                "tmdate": 1700242385354,
                "mdate": 1700242385354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XTN300hemb",
                "forum": "c72vop46KY",
                "replyto": "bQk9tPHaig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer: Thanks again for your careful and valuable comments! Since the rebuttal discussion is due soon, we\u2019ll be appreciated to know whether our replies have addressed your questions. If there are any further clarifications required or any other concerns, please feel free to contact us. Many thanks!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640207655,
                "cdate": 1700640207655,
                "tmdate": 1700640207655,
                "mdate": 1700640207655,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MLPcShzvKD",
            "forum": "c72vop46KY",
            "replyto": "c72vop46KY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3349/Reviewer_e1q8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3349/Reviewer_e1q8"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents CogVLM, a new state-of-the-art vision-language model. To address some issues of previous shallow alignment methods, the authors propose to insert visual expert modules in pretrained language model. Extensive experiments are conducted to evaluate CogVLM, and several SOTA results are achieved."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Strong performance. As tables in the submission, CogVLM shows strong performance compared to other models of equal magnitude. It also exhibits competitive results compared to PaLI-X which has much more parameters.\n- Open Source. Open source multimodal fundation model with strong performance has significant impact on the whole society.\n- Extensive experiments from different angles and extensive ablation studies. The authors evaluate the superiority of CogVLM in various different kinds of benchmarks (e.g., caption, VQA, text-oriented VQA, grounding, instruction following and etc.)."
                },
                "weaknesses": {
                    "value": "- The perhaps biggest weakness with this paper is the writing.\n  - This paper starts by raising two possible drawbacks of shallow alignment methods: (i) converge fast but perform worse. (ii) weak visual understanding ability, expecially hallucination. However, both these two disadvantages proposed by the authors are just **hypothesises**, not **compelling** nor **conclusive**. First, the performance gap between BLIP-2 and PaLI-X cames from several possible differences between two framework (e.g., the visual encoder size, the way that visual encoder is pre-trained by). And both MiniGPT-4 and LLAVA have extremely little trainable parameters in the alignment between visual features and language features.\n  - Some blanket statements are used. For instance, the author claims that NLP ability is weakened when jointly train the language model in image-text training. However, there are some evidences show that jointly training can benefit both vision task as well as language task, at least in some aspect (e.g., [1]).\n  - The motivations and starting points are inconsistent with the experiments. In other words, despite the strong performance, the ablation studies cannot demonstrate that two problems of shallow alignment raised by the writers are well resolved. The ablation studies in Table 6 can prove the effectiveness of CogVLM design. But these numbers cannot prove that deep alignment is better than and solves the issues of shallow alignment, due to the results of shallow alignment method with larger visual encoder (same parameters as vision encoder + vision adapter) are remain unknown.\n- Section 2.2 mentions that CogVLM is trained via two-stage process, with 120K and 60K steps respectively. The ablation studies in Table 6 are trained for just 6K steps. However, despite with much fewer iterations, the performance gap between ablation model and the final model is not that significant (e.g., in Table 6, CogVLM achieves 142.8 COCO CIDEr, only ~4 CIDEr score less that the results in Table 3). So does this phenomenone implies that too much iterations in the two-stage training process are unnecessary?\n- The visual expert in CogVLM includes FFNs in both attention block and FFN block. Which one is more important for better performance?\n\n[1] Tu, Haoqin, et al. \"Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics.\" arXiv preprint arXiv:2309.07120 (2023)."
                },
                "questions": {
                    "value": "- In section 2.3, the author claims that errors in LLAVA-Instruct dataset are corrected by mannual inspection and annotation. Will the corrected dataset be made publicly available?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840283635,
            "cdate": 1698840283635,
            "tmdate": 1699636284544,
            "mdate": 1699636284544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tvQZNNZlDl",
                "forum": "c72vop46KY",
                "replyto": "MLPcShzvKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive review and insightful comments. Please allow us to address your points in detail:\n\n> 1. However, both these two disadvantages proposed by the authors are just hypothesises, not compelling nor conclusive. First, the performance gap between BLIP-2 and PaLI-X cames from several possible differences between two framework (e.g., the visual encoder size, the way that visual encoder is pre-trained by). And both MiniGPT-4 and LLAVA have extremely little trainable parameters in the alignment between visual features and language features.\n\nThis point is about the effectiveness of deep fusion. The comparison between PaLI-X and BLIP-2 are  just two examples of shallow / deep alignment methods, and not a strict claim. You are right that they cannot be compared directly. We will modify it in the final version to avoid confusion.\n\n> 2. Some blanket statements are used. For instance, the author claims that NLP ability is weakened when jointly train the language model in image-text training. However, there are some evidences show that jointly training can benefit both vision task as well as language task, at least in some aspect (e.g., [1]).\n\nIt is not a blanket statement. In our paper, the next sentence of this statement is \"According to PaLM-E [2]...\", which runs experiments to prove the catastrophic forgetting in LLMs during jointly training. Specifically, Palm-E without freezing the language model parameters during multimodal training, showed a 15% drop in NLU capability and an 87% drop in NLG capability across 21 NLP tasks.  Similar findings were observed in Flamingo, where not freezing the LLM resulted in an average 8% performance decrease across five multimodal benchmarks. Regarding the 'Sight Beyond Text' paper, it only trained with a smaller learning rate on approximately 600,000 data points, which likely led to minimal changes in the LLM parameters, potentially resulting in a significant gap from large-scale pretraining. Hence, it may not be a proper reference to prove no catastrophic forgetting in LLMs during jointly training. For hallucination problems, our model has a significantly higher F1 score on the challenging POPE dataset than other models, which confirms the statement in our paper. See \"Results on hard benchmarks\" for details.\n\n[2] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multi-modal language model. arXiv preprint arXiv:2303.03378, 2023.\n> 3. Ablation studies cannot prove that deep alignment is better than and solves the issues of shallow alignment, due to the results of shallow alignment method with larger visual encoder (same parameters as vision encoder + vision adapter) are remain unknown.\nThis proposed ablation study is very reasonable but unfortunately is very hard to implement, because the ViT (CLIP) is pretrained and the performance is not solely determined by the number of parameters. We cannot get a good ViT with arbitrary number of parameters easily for comparison.\n\nHowever, we can still use a hypothetical comparison. In our paper, the  \"VE-full every 4th layer\" setting (1.7B) performs much better that the \"MLP-adapter\"(shallow) setting (140M), (COCO CIDEr 131.2 vs 138.7). If there were a well-trained EVA-CLIP ViT of (5B+1.7B) parameters, it is not possible to increase that much in \"MLP-adapter\" setting, since the current \"MLP-adapter\" has already 5B parameters.\n\nIf you make the ViT trainable to increase the trainable parameters, the performance will not be better either, which is well-documented in previous works like flamingo[3] (Table 7 (xii) 68.4 ->64.5 after making vision encoder trainable).\nI hope this hypothetical comparsion can ease your concern.\n\n[3] Alayrac, Jean-Baptiste, et al. \"Flamingo: a visual language model for few-shot learning.\" Advances in Neural Information Processing Systems 35 (2022): 23716-23736.\n\n> 4. despite with much fewer iterations, the performance gap between ablation model and the final model is not that significant (e.g., in Table 6, CogVLM achieves 142.8 COCO CIDEr, only ~4 CIDEr score less that the results in Table 3). So does this phenomenone implies that too much iterations in the two-stage training process are unnecessary?\n\nMore training iterations are important. Regarding dataset evaluations, the COCO dataset is relatively simple and ground truth is relatively fixed in style and mostly includes common everyday scenes.  Thus, continued training does not show significant improvement, indicating a need for more challenging benchmarks in the research community, like MM-Vet. We observed that our model's performance continued to grow with training, ultimately far surpassing other models. CogVLM achieved a new sota of 52.8 for MM-Vet, which will be updated in our paper. Please see our comment titled \"Result for hard datasets\" above."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242095915,
                "cdate": 1700242095915,
                "tmdate": 1700497738454,
                "mdate": 1700497738454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OqkqS6nnkm",
                "forum": "c72vop46KY",
                "replyto": "MLPcShzvKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response2"
                    },
                    "comment": {
                        "value": "> 5. The visual expert in CogVLM includes FFNs in both attention block and FFN block. Which one is more important for better performance?\n\nThe ablation experiments in Table 6 suggest that the FFN plays a slightly more significant role than the attention layer. Adding a visual expert every four layers also yielded competitive results. Therefore, choosing a model structure will depend more on the balance between desired model performance and training efficiency.\n> 6.  Will the corrected dataset be made publicly available?\n\nYes, we will release the dataset if you think this behavior can benefit the community."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242122355,
                "cdate": 1700242122355,
                "tmdate": 1700242122355,
                "mdate": 1700242122355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XeqCiZAHAS",
                "forum": "c72vop46KY",
                "replyto": "MLPcShzvKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer: Thanks again for your careful and valuable comments! Since the rebuttal discussion is due soon, we\u2019ll be appreciated to know whether our replies have addressed your questions. If there are any further clarifications required or any other concerns, please feel free to contact us. Many thanks!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640188835,
                "cdate": 1700640188835,
                "tmdate": 1700640188835,
                "mdate": 1700640188835,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DemFI9QbQn",
            "forum": "c72vop46KY",
            "replyto": "c72vop46KY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3349/Reviewer_Yif3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3349/Reviewer_Yif3"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors studied multimodal LLM and pushed the limit of multimodal LLM by developing a new module called Visual Expert for LLMs. Along with the new model design, the authors curated a large-scale pretraining and instruction-tuning data for the model training. When evaluated on a wide range of vision-language tasks, the proposed model CogVLM exhibits outstanding performance across the board, and surpass models with even much larger size."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors argued that most of the previous multimodal LLMs used shallow connections between vision and models, and thus proposed a new module called visual expert. This new module prompts a more intimate interaction between visual and language tokens in LLMs.\n\n2. The authors curated a large-scale dataset for first-stage pretraining and second-stage instruction tuning. Based on the large-scale training data and the proposed visual expert module, the proposed method achieves a number of state-of-the-art results across a wide range of vision-language tasks.\n\n3. Finally, a number of ablation studies are performed and demonstrate the effectiveness of the proposed method to some extent."
                },
                "weaknesses": {
                    "value": "The main concern to me about this paper is its limited novelty and scientific merit. First of all, the dense interaction between vision and language tokens has been heavily studied prior to the so-called multimodal LLM era. For example, a lot of BERT-style models exploit dense interactions. Second, it is really hard to capture which part is really making the main contribution to the final performance. There are many confounding factors such as the number and type of pretraining data, the instruction-tuning data, different architecture designs, and finetuning strategies. According to Table 6, I can hardly see a clear improvement brought by the introduced new VE modules. The authors start with some good motivation for building more intimate interaction between vision and language, but it finally becomes the emphasis of the benefit of scaling up.\n\nAnother missed piece of this work is what we can learn from this work. The state-of-the-art performance should be appreciated. But from the paper, I can hardly tell what the researchers should proceed to further improve the performance. Do we need better model design, or more data and computations? As mentioned in the paper, the authors also used some in-house data, which I guess cannot be released to the public. Given the barrier of reproducing the reported results and also the limited insights delivered by this work, I am sharing a huge concern regarding the current trend of building multimodal LLMs manifested by this work or other related ones."
                },
                "questions": {
                    "value": "As I mentioned above, I have some concerns regarding the scientific merit of this work. I appreciate the effort of pushing the limit of open-sourced multimodal LLMs but do see some potential issues with the current trend of scaling up multimodal LLMs. Given the current state of this paper, I think it is a very good engineering work, but may not be suitable to this research venue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3349/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699008440936,
            "cdate": 1699008440936,
            "tmdate": 1699636284474,
            "mdate": 1699636284474,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TaIIkFH1RM",
                "forum": "c72vop46KY",
                "replyto": "DemFI9QbQn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive review and insightful comments. Please allow us to address your points in detail:\n> 1. The main concern to me about this paper is its limited novelty and scientific merit. First of all, the dense interaction between vision and language tokens has been heavily studied prior to the so-called multimodal LLM era.\n\nOur response to this concern is in  1. of \"response to common concerns\" above. The dense interaction for vision and language is definitely not a new idea, but in the current LLM+ paradigm, it is very valuable to rediscover its effectiveness as a way to better scaling up. \n\nFurthermore, our research extends beyond the mere application of this idea. Through comprehensive ablation studies, we have rigorously examined the influence of various model structures and training strategies on the final outcomes. This includes an in-depth analysis of often-neglected components such as the design of RoPE for image and text sequences, the construction of attention masks, the equilibrium between trainable parameters and model efficiency, and the effect of image self-supervised loss.\n\nIt's important to emphasize that each experiment in the realm of large model training is resource-intensive. Therefore, our contribution in identifying and refining effective training methodologies holds significant value in the field.\n\n> 2. I can hardly tell what the researchers should proceed to further improve the performance. Do we need better model design, or more data and computations?\n\nThank you for your advice. We will add a discussion section in the final version if accepted. \n\nYes, scaling up is definitely one of the most important ways to increase performance in our opinion. As shown in the Ablation study part, scaling up the model with visual expert is very effective, and the fewer the parameters in the visual expert, the worse the performance.  As you say, how to ensure the **expected performance** after scaling up may be related to model design (like visual expert) or data etc., which is our aim for investigation. \n\n> 3. the authors also used some in-house data, which I guess cannot be released to the public. Given the barrier of reproducing the reported results and also the limited insights delivered by this work, I am sharing a huge concern regarding the current trend of building multimodal LLMs manifested by this work or other related ones.\n\nThis is a misunderstanding. All the results except on touchstone are only trained on public datasets, and reproducible. We answer this in  2. of \"response to common concerns\" above."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241751888,
                "cdate": 1700241751888,
                "tmdate": 1700639781647,
                "mdate": 1700639781647,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BVexzFgZHU",
                "forum": "c72vop46KY",
                "replyto": "DemFI9QbQn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3349/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer: Thanks again for your careful and valuable comments! Since the rebuttal discussion is due soon, we\u2019ll be appreciated to know whether our replies have addressed your questions. If there are any further clarifications required or any other concerns, please feel free to contact us. Many thanks!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3349/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640170110,
                "cdate": 1700640170110,
                "tmdate": 1700640170110,
                "mdate": 1700640170110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]