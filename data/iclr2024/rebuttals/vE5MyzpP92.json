[
    {
        "title": "Threshold-Consistent Margin Loss for Open-World Deep Metric Learning"
    },
    {
        "review": {
            "id": "h4K2hfjC3e",
            "forum": "vE5MyzpP92",
            "replyto": "vE5MyzpP92",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2124/Reviewer_3dmW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2124/Reviewer_3dmW"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of inconsistency in threshold determination for negative samples in threshold-based image retrieval. The authors propose a new metric called Operating-Point-Inconsistency-Score (OPIS) to measure inconsistency and introduce the Threshold-Consistent Margin (TCM) loss as a regularization technique to enhance consistency. The key contributions include identifying the problem with existing method, introducing an intuitive evaluation metric and regularization approach, and demonstrating improved threshold consistency without sacrificing accuracy in large-scale experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written, making it easy to understand while offering comprehensive comparisons with current methods.\n\n- It clearly highlights issues in existing models and presents an intuitive metric and regularization technique to tackle them.\n\n- The research goes a step further by demonstrating not just improved threshold consistency but also better performance in several instances."
                },
                "weaknesses": {
                    "value": "- The biggest weakness in the paper is the lack of experiments related to face verification, where threshold importance is evident. While image retrieval mostly uses metrics like mAP or Recall@k, face verification relies heavily on thresholds and uses metrics like TAR@FAR. The introduced method appears more suited for face verification than image retrieval.\n\n- The paper suggests that high accuracy doesn't always mean high threshold consistency. However, in face verification tasks, consistency in threshold often translates to high accuracy. This amplifies the sense that the paper might be focusing on an unrelated task.\n\n- The paper mentions related works like Liu et al. (2022) and OneFace, but experiments comparing the proposed method to these in the realm of face recognition are missing. Such comparisons are necessary to understand the proposed method's improvements in threshold consistency.\n\n- The paper needs to update state-of-the-art results on the CUB and Cars-196 datasets [1].\n[1] Kim et al., HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization, CVPR 23"
                },
                "questions": {
                    "value": "Figure 3 shows ProxyAnchor (ResNet50) with a low threshold consistency. It would be beneficial to compare the improvement in R@K and OPIS when using the proposed method. Ideally, the proposed method should show a significant OPIS improvement compared to others."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2124/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2124/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2124/Reviewer_3dmW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2124/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698561773341,
            "cdate": 1698561773341,
            "tmdate": 1699636145121,
            "mdate": 1699636145121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0tuAdYjJwv",
                "forum": "vE5MyzpP92",
                "replyto": "h4K2hfjC3e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 3dmW (Part-1 out of 2)"
                    },
                    "comment": {
                        "value": "We thank you for the insightful comments. We are glad that you found our paper to be well-written and easy to understand. We have taken your feedback into account and incorporated updates, including the latest state-of-the-art results on the CUB and Cars-196 datasets, into our manuscript. We have carefully considered and addressed each of your concerns point-by-point, as detailed below.\n\n**Q1/Q3 Lack of experiments related to face verification**\n\n**A1** We sincerely appreciate the reviewer's suggestion and input. However, we would like to point out that the focus of our work is to introduce the notion of threshold consistency for image retrieval in the general image domain and bring awareness of the threshold inconsistency phenomenon in deep metric learning. We want to emphasize the crucial role that the threshold plays in real-world image retrieval systems, where a threshold-based retrieval criterion is often preferred over a top-k approach due to its ability to identify negative queries that do not have matches in the gallery. However, the issue of threshold inconsistency in DML can often complicate the process of selecting thresholds when deploying large-scale commercial image retrieval systems in practice. To address this practical challenge, our paper presents two key contributions: the OPIS metric and the TCM regularization loss, both designed to mitigate the threshold inconsistency problem in deep metric learning.\n\nRegarding the application of our work to face verification, we acknowledge the relevance of threshold consistency in this context. However, due to the potential significant legal risks with using technologies that may generate or process biometric information, we are not able to use datasets where we cannot ensure that our use the datasets complies with applicable law, such as the [Illinois Biometric Information Privacy Act](https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004&ChapterID=57) (BIPA). For example, prior to using a dataset for such purposes, we must determine where the data subjects reside and confirm that the data subjects received the required notice and provided the required consent for us to use their data for our proposed purpose. Because we cannot ensure that the proposed use of the face datasets for face verification experiments complies with applicable laws, we are unable to use these datasets as you request. In addition, noteworthy organizations like National Institute of Standards and Technology (NIST) have discontinued the distribution of all three IJB datasets (IJB-A, IJB-B, and IJB-C) required for standard face recognition evaluation in the literature (refer to [link](https://www.nist.gov/programs-projects/face-challenges)). We kindly request suggestions from the reviewer on alternative experiments that we can undertake to address your concern without compromising privacy or violating applicable laws. Your insights and recommendations would be greatly appreciated.\n\n\n**Q2 Relationship between threshold consistency and accuracy in face recognition**\n\n**A2** To our best knowledge, high threshold consistency does not necessarily translate to high accuracy in face recognition. Through an investigation into face literature, it appears that OneFace [1] is the only work that delves into the discussion of threshold consistency across different demographic groups in face recognition. The experiment results in [1] indicate that high threshold consistency does not always translate to high accuracy. CurricularFace [2] exhibits better threshold consistency than GroupFace [3], but GroupFace achieves higher accuracy than CurricularFace on IJB-B and IJB-C in TAR@FAR=1e-4.\n\n\n**Q4 The state-of-the-art results on the CUB and Cars-196 datasets**\n\n**A4** Thank you for pointing out the state-of-the-art methods. We appreciate your suggestion to ensure the accurate representation of our paper. We have promptly updated the numbers in Table 5 of the main paper to reflect the latest progress. For a fair comparison, we also conducted experiments using the same backbone (DINO) as HIER [4] on the Cars dataset. As indicated in the table, TCM outperforms HIER in both OPIS and accuracy, all while reduces training time. We acknowledge and regret any inadvertent oversight in not being initially aware of the SOTA method and we appreciate its identification being brought to our attention.\n\nTable 1. The performance of HIER and our proposed method on Cars in Recall@1 and OPIS.\n\n|  | Backbone-embedding dimension\t |Recall@1\t| OPIS x 1e-3\t  | Time for Loss Computation (s / iteration)|\n|:-----------------:|:---------:|:---:|:-------------:|:-------------------------------------------:|\n| ProxyAnchor+HIER| DINO-384|91.7 *| 0.55|0.41|\n| ProxyAnchor+TCM| DINO-384 |**91.9**|**0.41**|**0.22**|\n\n*We use the released codebase and use the same hyper-parameter described in [4], and our reproduced result is better than the Recall@1 of 91.3 reported in [4]."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379597216,
                "cdate": 1700379597216,
                "tmdate": 1700718814968,
                "mdate": 1700718814968,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QeZnf8SA5h",
                "forum": "vE5MyzpP92",
                "replyto": "h4K2hfjC3e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 3dmW (Part-2 out of 2)"
                    },
                    "comment": {
                        "value": "**Q5 Apply TCM on top of ProxyAnchor(ResNet50)**\n\n**A5** We apply TCM on top of ProxyAnchor as required. As illustrated in the table and hypothesized by the reviewer, our proposed method demonstrates a significant performance improvement over the base loss in both Recall@1 and OPIS on the iNaturalist dataset.\n\nTable 2. The performance of applying TCM on top of ProxyAnchor on iNaturalist-2018 in Recall@1 and OPIS.\n\n|  |Backbone-embedding dimension | Recall@1 |OPIS x 1e-3|Relative improvement in OPIS after incorprating TCM as a regularizer|\n|:-------------------:|:---------------:|:-------------:|:--------------:|:----------------------------------------------------------------------:|\n| ProxyAnchor + TCM  | ResNet50-512 | 65.7 (\u21916.6) | 0.22 (\u21930.76) | 77.60%  |\n\n\nReferences for Response to reviewer 3dmW:\n\n[1] Liu, Jiaheng, et al. \"OneFace: one threshold for all.\" *European Conference on Computer Vision*. Cham: Springer Nature Switzerland, 2022.\n\n[2] Huang, Yuge, et al. \"Curricularface: adaptive curriculum learning loss for deep face recognition.\" *proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2020.\n\n[3] Kim, Yonghyun, et al. \"Groupface: Learning latent groups and constructing group-based representations for face recognition.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020.\n\n[4] Kim, Sungyeon, Boseung Jeong, and Suha Kwak. \"HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379806465,
                "cdate": 1700379806465,
                "tmdate": 1700718838068,
                "mdate": 1700718838068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PVSg2WWlSI",
                "forum": "vE5MyzpP92",
                "replyto": "h4K2hfjC3e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward for further discussions"
                    },
                    "comment": {
                        "value": "We would like to extend our sincere gratitude for your dedicated review of our paper. To address your concerns and questions, we have provided additional explanations and experiment results, including a comparison between TCM and HIER, where our proposed TCM method demonstrates competitive performance. Additionally, we have made necessary changes to the manuscript by updating the correct state-of-the-art numbers for Cars and CUB, along with the corresponding citations (HIER, CVPR23). As we are approaching the end of the discussion period, please let us know if you have any additional questions or require further clarifications. Once again, we thank you for your valuable comments and insights."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534527780,
                "cdate": 1700534527780,
                "tmdate": 1700534527780,
                "mdate": 1700534527780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TZmO6s8oRp",
                "forum": "vE5MyzpP92",
                "replyto": "h4K2hfjC3e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional response for Q3 on applicability of TCM in face domain"
                    },
                    "comment": {
                        "value": "We would like to express our gratitude once again for your valuable reviews. We are committed to fully addressing all of your concerns. To avoid potential legal risks associated with using real human face datasets, we have opted to utilize a synthetic face dataset, DigiFace [5], to demonstrate the effectiveness of our TCM method in comparison with OneFace in the realm of face verification. Same as the open-world setting described in the main paper, we randomly split 90% of person identities for training, reserving the remaining 10% of identities for testing. The training and testing identities (classes) are completely disjoint to each other. The results in the table below gives a comparison ArcFace [6], OneFace [1] and our proposed TCM method. Notably, as OneFace does not open-source their code, we followed their paper and implemented based on the description in the paper. As shown in the table, our method outperforms both ArcFace and OneFace in terms of threshold consistency (measured by OPIS, the lower the better), while achieving the highest accuracy (measured by TAR@FAR=1e-4, the higher the better) among the three. We hope this experiment adequately addresses your concerns about experiments in face verification. Please do not hesitate to reach out if you have any additional questions or concerns.\n\nTable 3. The performance of ArcFace, OneFace and our proposed method on DigiFace in TAR@FAR=1e-4 and OPIS.\n\n|        \t        |    Backbone-embedding dimension\t    | TAR@FAR=1e-4 (%)\t  | OPIS x 1e-2\t  |\n|:---------------:|:---------------:|:-----------------:|:-------------:|\n|    ArcFace\t     | ResNet100-128\t  |       98.20\t       |     1.42\t     |\n|    OneFace\t     | ResNet100-128\t  |      99.03\t       |     1.41\t     |\n| ArcFace + TCM\t  | ResNet100-128\t  |      99.54\t       |     1.28\t     |\n\n\nReferences for response to reviewer 3dmW: \n\n[5] Bae, Gwangbin, et al. \"Digiface-1m: 1 million digital face images for face recognition.\" *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*. 2023.\n\n[6] Deng, Jiankang, et al. \"Arcface: Additive angular margin loss for deep face recognition.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2019."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715707947,
                "cdate": 1700715707947,
                "tmdate": 1700718961011,
                "mdate": 1700718961011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1IYQUqRV5y",
            "forum": "vE5MyzpP92",
            "replyto": "vE5MyzpP92",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2124/Reviewer_52BZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2124/Reviewer_52BZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces and addresses the threshold inconsistency problem in Deep Metric Learning (DML). To tackle this issue, the authors present the Operating-Point-Inconsistency-Score (OPIS) metric, which is based on the variance of utility score derived from the F-score. Additionally, they propose the Threshold-Consistent Margin (TCM) loss, which selectively penalizes hard sample pairs. The experimental results on various deep metric learning benchmarks validate the efficacy of their proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper effectively identifies and defines the threshold inconsistency problem within the context of Deep Metric Learning (DML). \n\n2.To address this issue, the authors introduce a novel loss function, the Threshold-Consistent Margin (TCM) loss. \n\n3.Their proposed method is rigorously evaluated through comprehensive experiments."
                },
                "weaknesses": {
                    "value": "1. The use of the term \"large-scale\" in this paper may be misleading as the experiment datasets do not contain a sufficiently large number of samples to be accurately characterized as \"large-scale.\" Typically, datasets with more than 10 million or 1 billion samples could be considered as large-scale.\n\n2. The threshold inconsistency problem, as described in this paper, is also referred to as the generalization problem and has been previously discussed in the deep metric learning (DML) literature [r1, r2]. In reference [r1], the authors proposed the adoption of a metric variance constraint (MVC) to enhance generalization ability, which is essentially a variance-based metric. Reference [r2] provided an in-depth discussion of the generalization problem in DML. It would be beneficial for this paper to incorporate discussions and comparisons with these existing works in the context of addressing the threshold inconsistency problem. \n\n[r1] Kan, Shichao, et al. \"Contrastive Bayesian Analysis for Deep Metric Learning.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).\n\n[r2] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bj\u00f6rn Ommer, and Joseph Paul Cohen. Revisiting training strategies and generalization performance in deep metric learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 8242\u20138252, 2020."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2124/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2124/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2124/Reviewer_52BZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2124/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563400710,
            "cdate": 1698563400710,
            "tmdate": 1699636145057,
            "mdate": 1699636145057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fhb4s9Wnbh",
                "forum": "vE5MyzpP92",
                "replyto": "1IYQUqRV5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 52BZ (Part-1 out of 2)"
                    },
                    "comment": {
                        "value": "We thank you for the encouraging review and constructive comments to enhance our paper. It is gratifying to hear you find that we \u201ceffectively identify and define the threshold inconsistency problem in DML\u201d, \u201cintroduce a novel loss function\u201d and \u201c rigorously evaluate our method through comprehensive experiments\u201d. We response to your comments below.\n\n**Q1 The use of the term \"large-scale\" in this paper may be misleading**\n\n**A1** Thanks for the valuable comments. In our experiments, we evaluated our models on standard image retrieval benchmarks, aligning with practices in mainstream deep metric learning studies [1, 2, 3, 4]. We follow previous works [5, 6, 7] which used the word \"large-scale\" to describe the iNaturalist-2018 dataset, which is much bigger than traditional metric learning datasets such as CUB, Cars, and etc. However, we acknowledge that the scale of iNaturalist-2018 is limited compared to many recently-released datasets such as LAION-400M and LAION-5B, although these datasets are typically not used for metric learning. To enhance clarity and better reflect the scale of our experiments, we have incorporated the suggested revisions into the manuscript by taking out the term \"large-scale experiments\"."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378629686,
                "cdate": 1700378629686,
                "tmdate": 1700378629686,
                "mdate": 1700378629686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j7axJ2bYXX",
                "forum": "vE5MyzpP92",
                "replyto": "1IYQUqRV5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 52BZ (Part-2 out of 2)"
                    },
                    "comment": {
                        "value": "**Q2 Discussions and comparisons about [1, 2]**\n\n**A2** We genuinely appreciate the reviewer for sharing the two referenced works with us. Through our examination, we have indeed identified noteworthy parallels between the generalization problem discussed in [1, 2] and our open-world setting. Particularly, one of conclusions in [2], stating that excessive feature compression actually hurts DML generalization, aligns with our observations in the ablation study for margin, as shown in Fig 6 of the paper, where overly restrictive margins lead to decreased accuracy. We have promptly incorporated references to these two works into our revised paper to further enrich the context and related research in the field. However, it's worth noting that these two works primarily focus on enhancing generalization to improve accuracy, particularly in scenarios characterized by train-test distribution or concept shifts. In contrast, our work specifically targets the crucial aspect of threshold consistency. Our proposed evaluation criteria, OPIS and epsilon-OPIS, are tailored to measure threshold consistency within the calibration range across different test distributions and classes. They are designed as orthogonal metrics to accuracy. \n\nAdditionally, comparing TCM to the Margin Variance Control (MVC) proposed in CBML[1], we find commonalities between the two techniques. However, we would like to point out the three distinctions between TCM and MVC that makes TCM more suited for encouraging threshold consistency:\n\n1. TCM employs two cosine margins to simultaneously regulate positive and negative pairs, whereas MVC is exclusively applied to negative pairs. In Appendix A.2.3, we demonstrate that relying solely on the negative term lags considerably behind the full TCM regularization in terms of Recall@1 (72.5 for negative term only vs. 84.4 for both positive and negative terms).\n2. TCM applies penalties to hard sample pairs, while MVC minimizes variance for all negative pairs. In Appendix A.2.3, we consider a TCM variant without hard mining (referred to as TCM\u2019 in Appendix A.2.3) by removing the indicator function in Eq. (5), where the regularization is applied to all positive and negative pairs. However, as shown in Appendix A.2.3, the omission of the indicator function yields a significant degradation in Recall@1 (74.1 for RS@K + TCM without indicator function vs. 84.8 for RS@K + TCM with indicator function, the higher the better), while producing a worse OPIS result (0.31 x 1e-3 for RS@K + TCM without indicator function vs 0.17 x 1e-3 for RS@K + TCM with indicator function, the lower the better). This result underscores the critical importance of hard sample mining.\n3. TCM utilizes a global margin, whereas MVC calculates sample-specific margin, namely, target value for the similarity hyperplane as defined in [1].\n\n\nReferences for Response to reviewer 52BZ: \n\n[1] Kan, Shichao, et al. \"Contrastive Bayesian Analysis for Deep Metric Learning.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).\n\n[2] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bj\u00f6rn Ommer, and Joseph Paul Cohen. Revisiting training strategies and generalization performance in deep metric learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 8242\u20138252, 2020.\n\n[3] Brown, Andrew, et al. \"Smooth-ap: Smoothing the path towards large-scale image retrieval.\" *European Conference on Computer Vision*. Cham: Springer International Publishing, 2020.\n\n[4] Patel, Yash, Giorgos Tolias, and Ji\u0159\u00ed Matas. \"Recall@ k surrogate loss with large batches and similarity mixup.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2022.\n\n[5] Cui, Yin, et al. \"Class-balanced loss based on effective number of samples.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2019.\n\n[6] Zhou, Boyan, et al. \"Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2020.\n\n[7] Zhong, Zhisheng, et al. \"Improving calibration for long-tailed recognition.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378988568,
                "cdate": 1700378988568,
                "tmdate": 1700378988568,
                "mdate": 1700378988568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sMoSiGVHZG",
                "forum": "vE5MyzpP92",
                "replyto": "1IYQUqRV5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward for further discussions"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation for your valuable feedback. We have taken out the word \u201clarge-scale\u201d and incorporated discussions regarding the two suggested works into the updated manuscript. Please kindly provide feedback on whether these updates have addressed your inquiries. We genuinely thank you once again for dedicating your time and expertise to review our work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534448074,
                "cdate": 1700534448074,
                "tmdate": 1700534448074,
                "mdate": 1700534448074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "McF29dsCFa",
                "forum": "vE5MyzpP92",
                "replyto": "sMoSiGVHZG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Reviewer_52BZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Reviewer_52BZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful response to my review and others. I will keep my current rating and hope to see this paper accepted."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538857492,
                "cdate": 1700538857492,
                "tmdate": 1700538857492,
                "mdate": 1700538857492,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W90516ZzLZ",
            "forum": "vE5MyzpP92",
            "replyto": "vE5MyzpP92",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2124/Reviewer_4pGE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2124/Reviewer_4pGE"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of threshold inconsistency in deep metric learning (DML) for image retrieval. Existing DML methods often result in uneven representation structures within and between classes, leading to significant variations in performance across different test classes and data distributions, measured by false accept rate (FAR) and false reject rate (FRR). To tackle this issue, the authors propose a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) to quantify the inconsistency in threshold performance across classes. They observe a trade-off between accuracy and threshold consistency, where improving accuracy can negatively impact threshold consistency. To mitigate this trade-off, they introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique that penalizes difficult sample pairs to encourage uniform representation structures across classes. Extensive experiments on large-scale datasets demonstrate that TCM enhances threshold consistency while maintaining or even improving accuracy, simplifying the threshold selection process in practical DML applications. The key contributions of the paper include the introduction of the OPIS metric, the identification of the accuracy-threshold consistency trade-off, and the proposal of the TCM loss as a solution to improve threshold consistency in DML. The approach outperforms state-of-the-art methods on various large-scale image retrieval benchmarks, achieving significant improvements in threshold consistency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed Operating-Point-Inconsistency Score (OPIS) and \u03f5-OPIS provide valuable insights.\n2. The experiments comparing high accuracy with high threshold consistency are objective.\n3. The proposed Threshold-Consistent Margin (TCM) loss is relatively simple and easy to understand.\n4. The visualization of the TCM effect is interesting.\n5. The experiments are comprehensive, with detailed implementation and coverage of mainstream metric learning settings.\n6. The ablation experiments are extensive, exploring margin, DML losses, different architectures, and time complexity. They also validate the proposed method against state-of-the-art approaches such as RS."
                },
                "weaknesses": {
                    "value": "It is meaningful to pull the scores of positive pairs towards a fixed value and the scores of negative pairs towards another fixed value, even though it sounds simple.\n\nApart from that, I did not see any other weaknesses."
                },
                "questions": {
                    "value": "1. Since you conducted experiments on the large-scale iNaturalist-2018 dataset, what are the differences between open-set metric learning and face recognition or re-identification (re-ID)? Can your method be applied in the field of face recognition?\n2. If your method can use a single model to maintain the same threshold across multiple test sets, would it be meaningful, such as in this work[1].\n\n[1] https://cmp.felk.cvut.cz/univ_emb/"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2124/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763108314,
            "cdate": 1698763108314,
            "tmdate": 1699636144982,
            "mdate": 1699636144982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i4OAhRpqvi",
                "forum": "vE5MyzpP92",
                "replyto": "W90516ZzLZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 4pGE"
                    },
                    "comment": {
                        "value": "We thank you for thoroughly reviewing our paper.  We are delighted to learn that our proposed metric provides valuable insights, proposed method is easy to understand, and our experiments are objective and comprehensive. And we address your concerns point-by-point below.\n\n**Q1 What are the differences between open-world metric learning and face recognition or re-identification (re-ID)?**\n\n**A1** Face recognition and re-ID focus on more constrained scenarios and data domains, compared to the general image domain addressed by image retrieval tasks. Nevertheless, given the synergy between face recognition and deep metric learning, we think our development can be beneficial for face recognition applications by improving threshold consistency across various test distributions. For example, we can treat each identity as a class and apply the TCM loss across identities to encourage threshold consistency across individuals. Additionally, in our study, we also considered the ArcFace loss, which is widely-used in face recognition, on the iNaturalist-2018 dataset, as shown in Table 2 of the main paper. The results clearly demonstrate improvement in both accuracy and threshold consistency when incorporating TCM regularization into the ArcFace loss. \n\n**Q2 If your method can use a single model to maintain the same threshold across multiple test sets?**\n\n**A2** Yes, in our method, a single model can be used to test across multiple datasets and optimize for threshold consistency. We thank the reviewer for providing pointers on the dataset and setting in [1]. However, due to the presence of human faces in [Inshop dataset](https://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html) and [Met dataset](https://cmp.felk.cvut.cz/met/), to ensure compliance with data privacy protection laws and minimize the potential infringement risks regarding personal data and privacy, we could not directly use the UnED dataset [1]. Instead, we opt for an alternative approach by merging the four datasets in our paper, namely iNaturalist-2018, Stanford Online Product, CUB-200-2011, and Cars-196, following the dataset construction methodology described in [1]. For testing, we follow [1] and equalize the number of instances per class across different domains to prevent the evaluation being dominated by majority classes with a large number of images. The following table presents the results in terms of Recall@1 and OPIS. These results highlight the efficacy of TCM regularization in cross-domain settings.\n\nTable 1. The performance of our proposed method on aforementioned dataset in Recall@1 and OPIS. The numbers in brackets represent the absolute improvement of models trained after incorporating TCM as a regularizer.\n\n|        \t         |   Backbone-embedding dimension |  Recall@1 \u2191\t  | OPIS x 1e-3 \u2193\t  | Relative improvement in OPIS after incorporating TCM as a regularizer|\n|:----------------:|:-------------:|:-------------:|:---------------:|:----------------------------------------------------------------------:|\n| SmoothAP + TCM\t  | ResNet50-512\t | 75.2 (\u21914.1)\t  | 1.55 (\u21930.64) \t  |                                29.20%\t                                 |\n| SmoothAP + TCM\t  | ViT-B/16-512\t | 76.7 (\u21915.4)\t  | 2.06 (\u21930.73) \t  |                                26.20%\t                                 |\n\nReferences for Response to reviewer 4pGE: \n\n[1] Ypsilantis, Nikolaos-Antonios, et al. \"Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations.\" *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378013979,
                "cdate": 1700378013979,
                "tmdate": 1700718743377,
                "mdate": 1700718743377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uS2UxuEhzA",
                "forum": "vE5MyzpP92",
                "replyto": "W90516ZzLZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward for further discussions"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your diligent review of our paper. We have made the necessary improvements to our manuscript by incorporating citations of the suggested work (UnED, ICCV23) as per your recommendations. Additionally, we have conducted an experiment using a setup similar to the suggested paper and have included the corresponding results in our previous response, where our TCM loss also improves both accuracy and OPIS in this cross-domain setting. Please kindly let us know if our responses have effectively addressed your concerns. Your continued feedback is greatly appreciated, and we thank you for your valuable comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534222306,
                "cdate": 1700534222306,
                "tmdate": 1700534222306,
                "mdate": 1700534222306,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BUmZIrDno3",
                "forum": "vE5MyzpP92",
                "replyto": "W90516ZzLZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2124/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional response for Q1 on applicability of TCM in face domain"
                    },
                    "comment": {
                        "value": "In response to your question regarding the applicability of our proposed method to the face domain, we have conducted experiments on DigiFace [2], a synthetic face dataset. Our results show improved performance in terms of both accuracy and threshold consistency when compared to ArcFace [3], as demonstrated in the results below:\n\nTable 2. The performance of ArcFace and our proposed method on DigiFace in TAR@FAR=1e-4 and OPIS.\n\n|        \t        |    Backbone-embedding dimension\t    |TAR@FAR=1e-4 (%) \u2191 |OPIS x 1e-2 \u2193 |\n|:---------------:|:---------------:|:----------------:|:------------:|\n| ArcFace + TCM\t  | ResNet100-128\t  |  99.54 (\u21911.34)\t  | 1.28(\u21930.14)\t |\n\nPlease feel free to reach out if you have any further questions or require additional information. Thank you for your valuable feedback and insights.\n\nReferences for Response to reviewer 4pGE: \n\n[2] Bae, Gwangbin, et al. \"Digiface-1m: 1 million digital face images for face recognition.\" *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*. 2023.\n\n[3] Deng, Jiankang, et al. \"Arcface: Additive angular margin loss for deep face recognition.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2019."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2124/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717022828,
                "cdate": 1700717022828,
                "tmdate": 1700719415519,
                "mdate": 1700719415519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]