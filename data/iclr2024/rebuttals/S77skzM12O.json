[
    {
        "title": "PROTO: Iterative Policy Regularizied Offline-to-Online Reinforcement Learning"
    },
    {
        "review": {
            "id": "tInWw2WVor",
            "forum": "S77skzM12O",
            "replyto": "S77skzM12O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission70/Reviewer_xy7L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission70/Reviewer_xy7L"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new framework Policy Regularized Offline-To-Online (PROTO) for offline pre-trained to the online fine-tuning RL problem. The proposed PROTO framework can be implemented efficiently, and it also demonstrates improvement upon several existing offline to online methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper has conducted a throughout literature review.\n2. The limitations of current existing offline-to-online methods are well introduced and explained (Section 3.2).  \n3. The proposed regularization method is clean and elegant (Equation 3).\n4. The intuition of the proposed algorithm is well explained and somewhat theoretically justified (Section 3.3).\n5. Most experiments in Section 4 demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The experiments in Figure 4 are hard to visualize, perhaps the author can change another color to demonstrate the curves (especially the `BC+PROTO+SAC`, `EQL+SAC`, `PEX+BC`)."
                },
                "questions": {
                    "value": "Would it be ok if the author also add the Cal-QL experiments for comparison? As far as the reviewer is aware of, the Cal-QL paper also has open-sourced the [code](https://github.com/nakamotoo/Cal-QL)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission70/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698271871250,
            "cdate": 1698271871250,
            "tmdate": 1699635931425,
            "mdate": 1699635931425,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LTvE39mOlb",
                "forum": "S77skzM12O",
                "replyto": "tInWw2WVor",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xy7L"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for the constructive comments and positive feedback on our work. Regarding the concerns of the revewer xy7L, we provide the following responses.\n\n>**W1. The experiments in Figure 4 are hard to visualize, perhaps the author can change another color to demonstrate the curves (especially the BC+PROTO+SAC, EQL+SAC, PEX+BC).**\n\nWe thank the reviewer for this constructive comment! We have re-drawn almost all figures in our paper according to the suggestions of the reviewer.\n\n>**Q1. Would it be ok if the author also add the Cal-QL experiments for comparison? As far as the reviewer is aware of, the Cal-QL paper also has open-sourced the code.**\n\nWe thank the reviewer for this helpful comment. We have added the results in Figure 26 in Appendix M and the hyper-parameter details for Cal-QL in our revised paper. The comparisons show that PROTO can also outperform or obtain on-par performances as compared to Cal-QL."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699869901183,
                "cdate": 1699869901183,
                "tmdate": 1699869901183,
                "mdate": 1699869901183,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gmyubeoOcT",
                "forum": "S77skzM12O",
                "replyto": "LTvE39mOlb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_xy7L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_xy7L"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you very much for the prompt response. I have no further questions and I will maintain my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699894904114,
                "cdate": 1699894904114,
                "tmdate": 1699894904114,
                "mdate": 1699894904114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GA7MxtbQzH",
            "forum": "S77skzM12O",
            "replyto": "S77skzM12O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission70/Reviewer_LmwG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission70/Reviewer_LmwG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel algorithm, PROTO, for offline-to-online RL. After the offline pre-training phase, PROTO introduces the KL entropy regularization term in policy and value function objectives between the current policy and that of the previous iteration, which is iteratively updated, and the coefficient is also linearly decayed to loosen the constraint. PROTO achieves strong performance on adroit manipulation, antmaze navigation, mujoco locomotion from D4RL dataset and exhibits lighter computational cost than prior works."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### quality and clarity\n- This paper is clearly described and easy to follow.\n\n### significance\n- PROTO achieves the notable performance across the benchmark tasks.\n- PROTO reduces the computational cost from the existing offline-to-online RL methods such as ODT, Off2On, and PEX, etc."
                },
                "weaknesses": {
                    "value": "- I think this is the naive adaptation of TD3-BC [1] in offline-to-online settings, because KL regularization term results in BC objective in online settings.\n- Similar to above, I'm not sure the difference between PROTO and PROTO-TD3 or PROTO-SAC presented in Figure 4/5. I think PROTO and TD3(-BC) is quite similar to each other. Because  PROTO modifies the objective of policy and value function, it is not intuitive to incorporate with SAC. \n- Theorem 1 seems the same as what Vieillard et al. (2020) [2] have proven, and equation 7 is the same as what Scherrer et al. (2015) [3] have proven. I don't think there is novel and offline-to-online-specific discussion around the theorem. Please let me know if my understanding is not correct.\n- In the similar setting, [4] proposes iterative trust-region update from the offline pre-trained policy.\n\n[1] https://arxiv.org/abs/2106.06860\n\n[2] https://arxiv.org/abs/2003.14089\n\n[3] https://jmlr.org/papers/v16/scherrer15a.html\n\n[4] https://arxiv.org/abs/2006.03647\n\n---\n-- Update from the author response --\n\nFor the theoretical analysis, I completely disagree with the explanation from the authors. First, the theorem from Vieillard et al., (2020) and Scherrer et al., (2015) only holds under the exact case, where MDP has finite state space and a finite set of actions, and the q-function can be updated from the Bellman equation (this does not mean the update with gradient descent). Because this paper doesn't have any assumptions on those (i.e. PROTO is only proposed with continuous state/action space and function approximation), originally, the discussion does not make any sense. Second, because the theorem from Vieillard et al., (2020) and Scherrer et al., (2015) originally holds under any initialization (initialization of matrix) and thus offline-to-online RL has already been included in online RL, this paper does not have any novel theorem or statement. insightful interpretations for PROTO do not exist. The same statements from Vieillard et al., (2020) and Scherrer et al., (2015) are repeated in the main text. I express strong concerns that the paper has the same theorem as the previous papers without deriving a novel theorem. Moreover, if the important discussion is only discussed in Appendix, that is very weird."
                },
                "questions": {
                    "value": "- Could you clarify the computational cost between SAC and PROTO? Figure 9 may say that PROTO is better than SAC, but I guess there are no algorithmic differences between the two. \n- How does PROTO conduct offline pre-training? PROTO also employs iterative policy constraint? or other algorithms (CQL, IQL, etc.)? I guess this algorithm results in TD3-BC [1] in offline settings.\n- What is the effect of delayed policy updates (Section 3.4)? Is this necessary?\n\n[1] https://arxiv.org/abs/2106.06860"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission70/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission70/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission70/Reviewer_LmwG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission70/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607223117,
            "cdate": 1698607223117,
            "tmdate": 1700326242495,
            "mdate": 1700326242495,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sCJHez15e5",
                "forum": "S77skzM12O",
                "replyto": "GA7MxtbQzH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications on Two Major Misunderstandings"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. Regarding the concerns of the reviewer LmwG, we provide the following responses.\n\nFirst, we would like to clarify two major misunderstandings in the Weakness and the Question.\n\n>**W1. I think this is the naive adaptation of TD3-BC in offline2online settings, because KL regularization term results in BC objective in online settings.**\n\n- **No. PROTO is fundamentally different from TD3+BC**.\n  - TD3+BC is over-conservative in offline2online settings. In TD3+BC, the BC regularization only imposes constraints on the slowly evolving data in replay buffer $\\mathcal{B}$. However, relying only on $\\mathcal{B}$ is too conservative to obtain near-optimal results. To address this, APL[1] and SUNG[2] proposed some adaptive pessimistic methods to reduce the conservatism during online finetuning, but they still cast constraints on the suboptimal $\\mathcal{B}$.\n  - In contrast, PROTO considers a gradually evolving trust region induced by $\\pi_k$, which is notably superior in offline2online settings. As extensively discussed in our paper, the constraints w.r.t $\\pi_k$ enable both stable and near-optimal results. See Figure 1, 2, 3, 6, 7, 8 for comprehensive details.\n  - As thoroughly discussed in Appendix A, the introduction of $\\pi_k$ as policy constraints represents a novel and versatile solution that tackles the challenges faced by existing offline2online methods.\n\n>**Q2. How does PROTO conduct offline pre-training? PROTO also employs iterative policy constraint? or other algorithms (CQL, IQL, etc.)? I guess this algorithm results in TD3-BC [1] in offline settings.**\n\n- We must clarify that PROTO is not a specific pretrain/finetune method but a **versatile component** that bridges a wide range of pretrain/finetune methods. It allows us to use almost any policy pretraining methods **without modifications**. Then, we augment actor and critic training in existing off-policy RL methods with the regularization term $\\log(\\pi/\\pi_k)$ to create effective offline-to-online RL methods. \nFor example,\n  - EQL+PROTO+SAC involves using EQL for value and policy pretraining and incorporating the iterative policy regularization term $\\log(\\pi/\\pi_k)$ from PROTO into SAC finetuning, bridging the offline2online gap; \n  - SQL+PROTO+SAC uses SQL to pretrain the policy and value networks, then we plug PROTO into SAC for online finetuning; \n  - EQL+PROTO+TD3 refers to using EQL for offline pretraining and then plugging PROTO into TD3 that uses a deterministic policy for online finetuning. Please see Appendix H for EQL+PROTO+TD3 details.\n  - To the best of the authors' knowledge, achieving or evaluating such remarkable versatility has not been reported in prior studies.\n- Also, PROTO and TD3+BC are fundamentally different as discussed above."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699869699304,
                "cdate": 1699869699304,
                "tmdate": 1699869833937,
                "mdate": 1699869833937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TCYc4IB91N",
                "forum": "S77skzM12O",
                "replyto": "GA7MxtbQzH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to concerns"
                    },
                    "comment": {
                        "value": ">**W2. Similar to above, I'm not sure the difference between PROTO and PROTO-TD3 or PROTO-SAC presented in Figure 4/5.**\n\n- Our proposed PROTO is a versatile framework that can bridge a wide range of offline pretraining and online finetuning methods. The default PROTO refers to EQL pretrain+PROTO+SAC finetune as stated in the first paragraph in Section 4 (page 6), and it can be easily adapted to other versions by replacing different online finetuning methods, for example, PROTO+TD3 refers to EQL pretrain+PROTO+TD3 finetune. See Appendix H for PROTO+TD3 details.\n\n>**W3. Theorem 1 seems the same as what Vieillard et al. (2020) [3] have proven, and equation 7 is the same as what Scherrer et al. (2015) [4] have proven. I don't think there is novel and offline-to-online-specific discussion around the theorem. Please let me know if my understanding is not correct.**\n\n- No, there are some differences. Please see Appendix B for the differences between ours and [3][4].\n- Specifically, [3] and [4] only study the conventional online RL settings without considering the offline pretrained policies and value functions, and thus **are not directly applicable in offline2online settings**. However, in our paper, we extend the analysis from [3] and [4] from the conventional online RL to our specific offline2online settings by introducing a mild assumption on $Q_0$ initialization and making several modifications.\n- Note that we aim to provide insightful interpretations for PROTO, as stated in Appendix B, rather than aiming to develop more intricate or stringent bounds.\n\n\n>**W4. BREMEN[5] proposes iterative trust-region update from the offline pre-trained policy.**\n\nWe thank the reviewer for providing this related work and we are happy to include this paper in our revision. However, note that BREMEN and PROTO are very different and belong to different problem settings.\n- BREMEN[5] focuses on the deployment-efficient RL setting which involves multiple stages of offline training and online interactions. But PROTO focuses on the offline2online RL setting. These two settings share some similarities but are not the same.\n- BREMEN[5] constrains on a replay buffer $\\mathcal{D}$ rather than $\\pi_k$ since it periodically resets the policy using the BC policy w.r.t $\\mathcal{D}$, see Eq.(3) and Algorithm 1 in [5] for details. The reason why BREMEN also uses trust-region updates is that BREMEN utilizes on-policy RL methods to train the policy using the model rollouts. As the authors stated, BREMEN actually implicitly constrains w.r.t $\\mathcal{D}$, as discussed following Eq.(3) in [5]. In contrast, PROTO enforces constraints on $\\pi_k$, which has been shown to be superior to the over-conservative replay buffer based constraint.\n- BREMEN adopts a model-based approach and trains an ensemble of dynamics models, which can be computationally expensive. On the other hand, PROTO is model-free and extremely lightweight.\n\n\n\n>**Q1. Figure 9 may say that PROTO is better than SAC**\n\n- No, we did not say PROTO is better than SAC in terms of computational cost in our paper, but \"PROTO enjoys the same computational efficiency as SAC since the cost induced by the additional regularization term is negligible\".\n- Sorry for the confusion arising from Figure 9, where the bars are too low to visualize the comparisons between SAC and PROTO. Here we translate Figure 9 into the following table for clearer comparisons.\n\n||ODT|Off2On|PEX|IQL|AWAC|SAC|PROTO|\n|---|---|---|---|---|---|---|---|\n|Time for 1M online steps/minute|1440|480|45|35|30|30|30|\n\n>**Q3. What is the effect of delayed policy updates (Section 3.4)? Is this necessary?**\n- The delayed policy updates serve to enhance stability. Specifically, it provides a smoothly evolving trust region around $\\bar\\pi$ to regularize the finetuning process. \n- We added Figure 24 in Appendix L in our revised paper to show the role of the polyak averaging trick. \n  - Figure 24 shows that PROTO (No $\\tau$) can obtain better results on offline datasets that have good coverage such as medium-replay and random datasets since good data coverage can alleviate the instability and No $\\tau$ can enhance the optimality. \n  - However, for datasets that have narrow data coverage such as medium datasets, PROTO (No $\\tau$) may undergo some initial performance drop without polyak averaging. \n  - Nevertheless, with a stabilized $\\bar\\pi_k$ induced by polyak average, PROTO ($\\tau=5e-3$) can consistently achieve stable and near-optimal finetuning results, achieving a robust trade-off between stability and optimality across diverse tasks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699869746886,
                "cdate": 1699869746886,
                "tmdate": 1699869797116,
                "mdate": 1699869797116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZkcyivVpfw",
                "forum": "S77skzM12O",
                "replyto": "GA7MxtbQzH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_LmwG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_LmwG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response.\n\nMy questions on (1) the difference between yours and BREMEN, (2) the discussion on computational cost, and (3) the effect of delayed policy updates become clear now. Here are the remaining comments:\n\n**Comparison to TD3-BC**\n\nI don't think the explanation of the difference between PROTO and TD3-BC is clear. The authors said, `In TD3+BC, the BC regularization only imposes constraints on the slowly evolving data in replay buffer `. What does this mean? I think PROTO also slowly evolves data in the replay buffer since both PROTO and TD3+BC, are off-policy methods. In other words, what is the difference between TD3+BC and PROTO around the replay buffer?  If my understanding is correct, the difference between TD3+BC and PROTO is that TD3+BC considers the KL constraint between the previous policy histories (from replay buffer) and the current policy, while PROTO only considers the policy one-step before and the current policy. This kind of KL constraint (between k-th and k+1-th) has already been proposed in MPO [1] (online RL setting) and adopted in ABM+MPO [2] (offline-to-online RL setting). Even if considering the difference to TD3+BC/MPO,  the algorithmic novelty is still limited.\n\n\n[1] https://arxiv.org/abs/1806.06920\n\n[2] https://arxiv.org/abs/2002.08396 \n\n\n**Offline Pretraining**\n\nAfter reading the author response, I understood the agnostic nature of PROTO for the pertaining. My confusion might come from the lack of explanation about offline pertaining in the methodology section. I only found one sentence in Section 4, which said PROTO uses EQL in this paper. For clarity, it might be better to include one paragraph that mentions (1) PROTO is initialized with EQL in this paper, and (2) PROTO is applicable to any offline RL methods.\n\n**PROTO and PROTO-TD3 or PROTO-SAC**\n\nIf my understanding is correct, the difference between PROTO and PROTO-TD3/PROTO-SAC, is whether further finetuning with TD3/SAC is performed. I definitely agree that PROTO can bridge EQL and TD3/SAC in Figure 4/5, but I'm not sure why this bridge is necessary/important. For example, comparing PROTO and PROTO-TD3, PROTO shows faster convergence than PROTO-TD3 in Figure 5. There are no comparisons between PROTO and PROTO-SAC. I think PROTO is already sufficient for offline-to-online RL, and I'm not sure what merit we can enjoy from further SAC/TD3 training. This is a seemingly redundant (unnecessary?) experiment to me.\n\n**Theoretical Analysis**\n\nI completely disagree with the explanation. First, the theorem from Vieillard et al., (2020) and Scherrer et al., (2015) only holds under the exact case, where MDP has finite state space and a finite set of actions, and the q-function can be updated from the Bellman equation (this does not mean the update with gradient descent). Because this paper doesn't have any assumptions on those (i.e. PROTO is only proposed with continuous state/action space and function approximation), originally, the discussion does not make any sense. Second, because the theorem from Vieillard et al., (2020) and Scherrer et al., (2015) originally holds under **any initialization** (initialization of matrix) and thus offline-to-online RL has already been included in online RL, this paper does not have any novel theorem or statement. `insightful interpretations for PROTO` do not exist. The same statements from Vieillard et al., (2020) and Scherrer et al., (2015) are repeated in the main text. I express strong concerns that the paper has the same theorem as the previous papers without deriving a novel theorem. Moreover, if the important discussion is only discussed in Appendix, that is very weird."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325518584,
                "cdate": 1700325518584,
                "tmdate": 1700325661082,
                "mdate": 1700325661082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m6zezuC8uW",
                "forum": "S77skzM12O",
                "replyto": "GA7MxtbQzH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the additional comments of the reviewer LmwG (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the additional detailed comments. Although the current rating does not meet our expectations, **we find that the reviewer still has some major misunderstandings about our work**. It appears that most of the concerns relate to the novelty and theory contribution. Regarding these remaining concerns and misunderstandings, we provide the following detailed responses.\n\n>**1. Comparison to TD3+BC**\n\n>**1.1. ...TD3+BC considers the KL constraint between the previous policy histories (from replay buffer) and the current policy, while PROTO only considers the policy one-step before and the current policy.**\n\n- Yes, TD3+BC and PROTO are different in terms of the constraints they use, as the reviewer pointed out. \n\n>**1.2 This kind of KL constraint (between k-th and k+1-th) has already been proposed in MPO [1] (online RL setting) and adopted in ABM+MPO [2] (offline-to-online RL setting). Even if considering the difference to TD3+BC/MPO, the algorithmic novelty is still limited.**\n\n**No, we respectfully disagree that our algorithm lacks novelty.**\n- First, the reviewer may have some misunderstandings on ABM+MPO [2].\n  - **ABM+MPO [2] is exclusively an offline RL work that does not consider the offline-to-online setting** (see Algorithm 1 in [2] that no online interactions are involved).\n  - Thus, PROTO and ABM+MPO[2] are fundamentaly different in the problem settings.\n- Second, our main contribution/claim is that we identify that the **trust-region style constraint can inherently address the unique challenges of offline-to-online RL problems**, including the stability-optimality dilemma, limited versatility and computational inefficiency (details in Section 3.2). It is not about claiming to be the first to propose the trust-region update method.\n  - Actually, many methods used trust-region methods to exclusively solve online (e.g., MPO[1]) or offline (e.g., ABM+MPO[2]) RL problems[3-6], as discussed in Appendix A.\n  - This is exactly the motivation of our work that we wonder **whether we can utilize trust-region style updates to bridge the offline-online gap to solve offline-to-online problems**, considering the strong ability of trust-region update, as clearly stated at the beginning of Section 3.3.\n  - Therefore, we kindly believe it is **inappropriate** for the reviewer to assert that PROTO lacks novelty only because some online or offline RL methods have used trust-region updates, since the problem settings are different.\n- Third, the reviewer may think the transfer from constraints w.r.t the replay buffer $\\mathcal{B}$ to the last policy $\\pi_k$ is simple and trivial. However, we would argue that we demystify a common misconception in prior works that one must require strict constraints or complex design choices to solve offline-to-online RL problems.\n  - **PROTO remains very competitive while providing a much higher degree of simplicity and versatility**, while eliminating a lot of unnecessary complexity, hyperparameter tuning, and computational cost, required by more sophisticated methods.\n  - We believe such simplicity and versatility provide an easy-to-implement baseline for other researchers to construct their competitive offline-to-online RL methods.\n  - Therefore, we believe this simplicity should be considered as **a novelty and a significant contribution**, rather than being mistakenly considered as lacking novelty as the reviewer stated.\n- Last, we provided a thorough literature review of existing offline-to-online RL methods in Table 2 in Appendix A, which has also been acknowledged by other reviewers. The discussions show that **using trust-region style constraint to handle the offline-to-online RL challenges is novel for the offline-to-online RL fields**.\n\nAlso, we thank the reviewer for providing these two interesting related works. We have included the discussions on them in Appendix A in our revised paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396037826,
                "cdate": 1700396037826,
                "tmdate": 1700396570529,
                "mdate": 1700396570529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xbSLJrRE7c",
                "forum": "S77skzM12O",
                "replyto": "GA7MxtbQzH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the additional comments of the reviewer LmwG (2/3)"
                    },
                    "comment": {
                        "value": ">**2. Offline Pretraining**\n\n- Sorry for this confusion. However, note that we also discussed the pretraining and finetuning versatility in Section 3.3 (the Adaptability and Computational Efficiency paragraph). Due to space limit, we only revised some parts in Section 3.3 and Section 4 to make it more clear according to the suggestions. Thanks!\n\n>**3. PROTO and PROTO-TD3 or PROTO-SAC**\n\n>**3.1 For example, comparing PROTO and PROTO-TD3, PROTO shows faster convergence than PROTO-TD3 in Figure 5. There are no comparisons between PROTO and PROTO-SAC.**\n\n- Sorry for the confusion here. In our paper, SAC is the default online finetuning instantiation in our experiment for PROTO, as stated in the first paragraph in Section 4. PROTO shows faster convergence than PROTO+TD3 as SAC adopts an entropy bonus to enhance exploration, but TD3 only utilizes added noise to enhance exploration. We have revised our paper to make this more clear. Thanks!\n\n>**3.2  I think PROTO is already sufficient for offline-to-online RL, and I'm not sure what merit we can enjoy from further SAC/TD3 training. This is a seemingly redundant (unnecessary?) experiment to me.**\n\n- The experiments with SAC/TD3 finetuning demonstrate the versatility of PROTO on diverse off-policy finetuning methods. While PROTO is already sufficient for offline-to-online RL, note that the regularization term $\\log(\\pi|\\pi_k)$ in PROTO is easy to calculate. This good property allows PROTO to act as a pluggable term since the $\\log(\\pi|\\pi_k)$ term can be seamlessly integrated into most off-policy RL methods to mitigate the initial performance drop in offline-to-online settings.\n  -  The experiments confirm this versatility, as both PROTO+SAC and PROTO+TD3 exhibit stable and near-optimal finetuning, while finetuning solely using popular off-policy methods, such as SAC, suffers severe initial performance drop, as shown in Figure 4."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396102951,
                "cdate": 1700396102951,
                "tmdate": 1700397023993,
                "mdate": 1700397023993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wrhoMwLUlq",
                "forum": "S77skzM12O",
                "replyto": "GA7MxtbQzH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the additional comments of the reviewer LmwG (3/3)"
                    },
                    "comment": {
                        "value": ">**4. Theoretical Analysis**\n\n>**the theorem from Vieillard et al., (2020) and Scherrer et al., (2015) originally holds under any initialization and thus offline-to-online RL has already been included in online RL, this paper does not have any novel theorem or statement...I express strong concerns that the paper has the same theorem as the previous papers without deriving a novel theorem. Moreover, if the important discussion is only discussed in Appendix, that is very weird.**\n\nThe reviewer's primary concern appears to revolve around our theoretical contributions.\n- First, we must clarify that **we never argued to provide some entirely new or even tighter bounds than previous works**, as clearly stated in our paper. We have revised our paper to make it more clear.\n- Instead, we only argued that **we can extend some existing theories from online to our specific offline-to-online setting** to compare different regularizations and bring some theoretical insights. This is clearly stated in the paragraph preceding Eq. (6) in the main text and Appendix B. \n  - We have discussed these previous theories with proper context, citations, and did not claim to introduce entirely novel theoretical contributions in our paper, as we focus more on providing a strong, simple, versatile and efficient offline-to-online framework rather than presenting a theoretical study to provide lots of performance bounds. In this sense, we used existing theories to analyze the proposed methods. To clarify this, we revised our paper to call them Lemma rather than Theorem.\n- Furthermore, we respectfully disagree that our extension to the offline-to-online setting is trivial.\n  - While previous theories assume the $Q_0$ under any initialization, which is versatile and implicitly includes offline-to-online RL settings, we specifically consider such theory in offline-to-online RL settings by introducing one additional Q assumption that the offline pretrained policy should be obtained by one step policy improvement step upon the randomly-initialized $Q_0$. This aspect has not been explored in previous works.\n- Moreover, note that PROTO offers a versatile framework that is capable of bridging diverse offline/online RL methods. In this sense, it becomes pretty **challenging for anyone to provide further detailed theoretical analysis**. Since different instantiation choices will introduce different theoretical properties. To address this, we have empirically verified the versatility of PROTO by extensive experiments in Figure 4, 5.\n- Finally, the discussions in Appendix B primarily aim to explain how we can apply previous theories to our offline-to-online settings. We left this content in the Appendix to avoid overwhelming readers with these detailed explanations and due to the space limits. We provided many links to Appendix B in the main text to guide the readers to read these explanations.\n\n**In summary, we respectfully disagree that the theoretical aspects of our paper are over-claimed.**\n\n>**The theorem from Vieillard et al., (2020) and Scherrer et al., (2015) only holds under the exact case, where MDP has finite state space and a finite set of actions, and the q-function can be updated from the Bellman equation (this does not mean the update with gradient descent). Because this paper doesn't have any assumptions on those (i.e. PROTO is only proposed with continuous state/action space and function approximation), originally, the discussion does not make any sense.**\n- As discussed in Vieillard et al., (2020), they also utilized the theories to analyze the methods in continuous state/action space and function approximation, such as SAC, TRPO and MPO (see Table 1 in Vieillard et al., (2020) for details).\n- We believe that it is reasonable to analyze in the simplified discrete settings to avoid overloading the discussion with terms and assumptions, such as the realizability/completeness assumption, policy class complexity and critic network complexity induced by functional approximation[7]. Also, the simplification from continuous settings with functional approximation to discrete settings has been widely adopted in previous works to facilitate analysis, including but not limited to [8-10].\n- Also note that the focus of our study is to provide a strong practical algorithm rather than presenting a theoretical study to provide lots of performance bounds. Therefore, it may place a heavy burden on the readers to consider the settings with functional approximation that overload the discussion with terms and assumptions.\n- Nevertheless, we appreciate the reviewer's input, and we have made revisions to our paper in the main text and Appendix B to enhance clarity based on these considerations.\n\n**We hope that our additional clarifications are helpful for the reviewer in re-evaluating our paper. We will be more than happy to address any remaining concerns the reviewer may have. Sincerely thank you for your time and looking forward to your response.**"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396150197,
                "cdate": 1700396150197,
                "tmdate": 1700530634249,
                "mdate": 1700530634249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JJfGjShdTE",
            "forum": "S77skzM12O",
            "replyto": "S77skzM12O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission70/Reviewer_6tXk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission70/Reviewer_6tXk"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an iterative strategy regularization approach for offline to online reinforcement learning, called PROTO. The method is designed to address three main challenges: poor performance, limited adaptability, and low computational efficiency. By incorporating an iteratively evolved regularization term, the algorithm aims to stabilize the initial online fine-tuning and provide sufficient flexibility for policy learning. PROTO seamlessly bridges various offline reinforcement learning and standard off-policy reinforcement learning, offering a flexible and efficient solution for offline to online reinforcement learning."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a thorough analysis of the three issues present in existing offline to online finetuning methods and proposes a straightforward and versatile solution that effectively overcomes these problems. \n2. Furthermore, the article includes rigorous theoretical analysis and extensive experimental validation simultaneously, resulting in a high level of completeness in the paper."
                },
                "weaknesses": {
                    "value": "1. I have some concerns regarding Polyak averaging. Is the initial policy $\\bar{\\pi}_0$ obtained by the offline pretraining? If so, does Polyak averaging potentially lead to PROTO deviating very slowly from the offline pretrained policy since the parameter is a very small value ($\\tau=5e-5,5e-3$)? Additionally, it seems that the role of Polyak averaging might **overlap** with the addition of the KL term in Equation 3. Therefore, I believe that ablation experiments for this parameter are not sufficient. An experiment without this parameter should be conducted. Also, does the update process of EQL+SAC make use of this parameter $\\tau$? If not, could this parameter potentially help prevent training collapse in EQL+SAC?\n2. Based on the analysis and evidence presented in the paper, I believe that PROTO's capabilities extend beyond improving methods that are already capable of offline to online finetuning. Can PROTO also cooperate with other algorithms that cannot solve the offline to online problem?"
                },
                "questions": {
                    "value": "please answer the questions in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission70/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836493529,
            "cdate": 1698836493529,
            "tmdate": 1699635931287,
            "mdate": 1699635931287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p1IBDnV8Sy",
                "forum": "S77skzM12O",
                "replyto": "JJfGjShdTE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6tXk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and positive feedback on our work. Regarding the concerns of the revewer 6tXk, we provide the following responses.\n\n>**W1.1 Is the initial policy $\\pi_0$ obtained by the offline pretraining?**\n\nYes.\n\n>**W1.2 If so, does a small Polyak averaging (tau=5e\u22125,5e\u22123) potentially lead to PROTO deviating very slowly from the offline pretrained policy?**\n- No. Figure 8 shows that PROTO deviates a lot from $\\pi_0$ and even obtains a similar deviation degree compared to Off2On, which poses no regularization. \n- Also, we added quantitative investigations on the distributional shift degrees in Figure 23, Appendix K in our revised paper. The results show that PROTO with a very small $\\tau$ can still deviate fast from $\\pi_0$. \n\n>**W1.3 The role of Polyak averaging might overlap with the addition of the KL term in Equation 3**\n- No. The polyak averaging trick is only utilized to provide a stabilized trust region around $\\bar\\pi_k$, but is not used to directly slow down the policy updates. In that sense, the polyak averaging assists the KL term by smoothing the potential rapid evolution of the trust region.\n- However, we believe the reviewer makes an excellent idea and insightful observation! Using the polyak averaging trick to directly slow down the policy updates may serve as some kind of implicit policy constraint, while the KL term in Eq.(3) performs as an explicit constraint. Therefore, it would be interesting if we could combine them together to develop more advanced and versatile offline2online RL methods.\n\n>**W2.1 An experiment without polyak averaging should be conducted.**\n\nWe thank the reviewer for this helpful comment. \n\n- We added new results for $\\tau=0.1$ and no polyak averaging (No $\\tau$) in Figure 24, Appendix L in our revised paper.\n  - Figure 24 shows that PROTO (No $\\tau$) can obtain better results on offline datasets that have good coverage such as medium-replay and random datasets since a good data coverage can alleviate the instability and No $\\tau$ can enhance the optimality. \n  - However, for datasets that have narrow data coverage such as medium datasets, PROTO (No $\\tau$) may undergo some initial performance drop without polyak averaging. \n  - Nevertheless, with a stabilized $\\bar\\pi_k$ induced by polyak average, PROTO ($\\tau=5e-3$) can consistently achieve stable and near-optimal finetuning results, achieving a robust trade-off between stability and optimality across all evaluated tasks.\n\n>**W2.2 Does the update process of EQL+SAC make use of $\\tau$?**\n- No, the polyak averaging trick is only used to obtain a stable trust region $\\bar\\pi_k$ to regularize the online finetuning, but not to directly slow down the policy updates. EQL+SAC has no trust region update mechanisms as it directly finetunes without any regularization.\n\n>**W2.3 Could this parameter potentially help prevent training collapse in EQL+SAC?**\n\n- No, we added results for EQL+SAC using polyak averaging with the same $\\tau$ as PROTO in Figure 25, Appendix L in our revised paper. The results show that simply using polyak averaging without explicit KL-regularization still suffers training instability in EQL+SAC. Although this attempt fails, we think it is an interesting direction to further explore whether we could utilize the implicit policy regularization induced by polyak averaging to further enhance the results.\n\n>**W3. Can PROTO also cooperate with other algorithms that cannot solve the offline to online problem?**\n- Yes. SAC and TD3 in our paper are online RL methods that cannot directly solve offline2online problems since no regularization is introduced to combat the initial performance drop during online finetuning. Figure 3, 4, 5 show that with PROTO stabilization, all PROTO+SAC and PROTO+TD3 variants obtain stable and near-optimal performances. This demonstrates the versatility of PROTO on diverse online RL methods that cannot directly obtain stable offline2online results."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699869646418,
                "cdate": 1699869646418,
                "tmdate": 1699869646418,
                "mdate": 1699869646418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aBwYQ6PWkc",
                "forum": "S77skzM12O",
                "replyto": "p1IBDnV8Sy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_6tXk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_6tXk"
                ],
                "content": {
                    "comment": {
                        "value": "The authors provided very detailed responses and addressed each of our concerns thoroughly, adding rich experiments or charts. We maintain our score and recommend to accept this paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621644143,
                "cdate": 1700621644143,
                "tmdate": 1700621644143,
                "mdate": 1700621644143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OuKYG0oHUw",
            "forum": "S77skzM12O",
            "replyto": "S77skzM12O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission70/Reviewer_Ykto"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission70/Reviewer_Ykto"
            ],
            "content": {
                "summary": {
                    "value": "This work propose to conduct KL-regularized policy gradient iterative in the offline-to-online setting. By iteratively update the old policy with the more recent policy and constrain KL distance between current policy and old policy, it achieves a good trade off for stability and optimality. \n\nThe authors conducted experiments on D4RL tasks and shows improved performance compared to AWAC, IQL etcs. Additionally, the ablation results show that iteration of policy is essential to the improvement. Also the larger the distance between final policy and initial policy, the better the final performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed iterative approach is simple, effective, and novel, as far as I know.\n- The ablation results matched with the motivation.\n- The experimental results is promising."
                },
                "weaknesses": {
                    "value": "- Are we missing EQL comparison? The EQL should also work with offline to online setting. If we are using EQL as a pre-trained approach, how does the proposed approach perform compared to EQL in the offline to online setting?\n- In this setting, the KL-regularized MDP is a moving target, this means the optimal policy per iteration is changing. This seems conflicts with the motivation of the approach. Could the author clarify the intuition here? I assume the \\pi* is the optimal policy of original MDP (without KL-constraint)."
                },
                "questions": {
                    "value": "- It would be better to clarify the updating frequency of \\pi_k controlled by \\tau. How sensitive this parameter is when we switching to different tasks and settings? Especially when we have different level of \\pi_0. Is there a clear guidance how we should choose \\tau? \n- How many iterations of policies in total in practice? How many gradient steps we did one delayed updates? \n- I think it is good to see that in Figure 7, there is a clear shift of distribution in terms of horizon. It might be more clear to see if there is state-action distribution shift. I am trying to understand if this iterative approach can reach OOD state-action pairs gradually, compared to the \\pi_0. I think this is important because: 1) with this verification, we can then understand if the learning can be stable and gradually move to an area deviated from initial state-action coverage. 2) We are not just re-weighting the existing trajectories in the offline dataset."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission70/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699595283349,
            "cdate": 1699595283349,
            "tmdate": 1699635931197,
            "mdate": 1699635931197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DP3ECWngla",
                "forum": "S77skzM12O",
                "replyto": "OuKYG0oHUw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ykto"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and positive feedback on our work. Regarding the concerns of the reviewer, we provide the following responses.\n\n>**W1. Missed EQL comparison for offline2online setting.**\n\nWe thank the reviewer for this constructive comment. \n\n- We added the EQL comparison for the offline2online setting in Figure 22, Appendix J in our revised paper. Results show that EQL cannot reach near-optimal finetuning results as it constrains the finetuned policy w.r.t the slowly evolved relay buffer like IQL does, which is over-conservative. Moreover, EQL performs more conservatively than IQL since EQL solves a reverse KL-regularization, which behaves in a mode-seeking behavior and lacks explorations.\n- We did not compare against EQL in the initial version since both EQL and IQL are similar in-sample learning methods.\n\n>**W2. The KL-regularized MDP is a moving target. This means the optimal policy per iteration is changing.**\n\n- Yes, the optimal policy per iteration is changing in the KL-regularized MDP, and we denote it as $\\pi_k$ in our paper, which is different from the optimal policy $\\pi^*$ in the original MDP. \n- However, this does not conflict with our motivation as $\\pi_k$ will gradually converge to $\\pi^*$ as stated in Eq.(6), which enjoys stable and near-optimal convergence compared to fixed regularization (Eq. (8)) and no regularization (Eq. (7)).\n\n>**Q1. How sensitive is the $\\tau$ parameter is when we switching to different tasks and settings? Especially when we have different level of $\\pi_0$. Is there a clear guidance how we should choose $\\tau?$**\n\n- PROTO is not very sensitive with the $\\tau$ parameter. \n\n   - PROTO is robust to varying levels of $\\pi_0$ while maintaining the same $\\tau$ value. In Figure 4, we obtain different qualities of $\\pi_0$ using different offline pretraining methods, but keeping $\\tau$ constant. The results show that PROTO is robust to different qualities of $\\pi_0$ with the same $\\tau$.\n   - PROTO is also robust to a wide range of $\\tau$ values. Please see Figure 12, 13 in Appendix F for the ablation studies on $\\tau$.\n   - PROTO is also robust to different tasks with the same $\\tau$ values. We use the same $\\tau=5e-3$ for all 9 Mujoco tasks and $\\tau=5e-5$ for all 7 Antmaze and Adroit tasks. Please see Table 3 in Appendix D.2 for hyper-parameter details.\n\n\n- For the guidance, $\\tau$ can consistently achieve stable and near-optimal results over a broad range of values, such as [1e-2, 2.5e-3] for all Mujoco tasks (Figure 12, 13 and Table 7). It also behaves robustly across various tasks and different levels of $\\pi_0$. Therefore, it is easy to select an appropriate $\\tau$. Here, we suggest starting with a small $\\tau$ to prioritize stable finetuning and avoid risky outcomes, then gradually increasing its value.\n\n>**Q2. How many iterations of policies in total in practice? How many gradient steps we did one delayed updates?**\n\nDuring finetuning,\n- 1M policy iterations/gradient updates are performed for all PROTO+SAC-based methods, e.g. EQL+PROTO+SAC and BC+PROTO+SAC, the numbers of policy updates, value updates, and online samples for SAC are the same. \n- 0.5M policy iterations/gradient updates are performed for PROTO+TD3 since TD3 performs one policy update per two value updates.\n\n\n>**Q3. It might be more clear to see if there is state-action distribution shift.**\n\nGood point! We thank the reviewer for this constructive comment. \n\n- We added quantitative comparisons on the distributional shift degrees of different policy regularizations in Figure 23, Appendix K in our revised paper. The results show that the iterative policy regularization method stays close to $\\pi_0$ initially and can gradually deviate from the pretrained policy $\\pi_0$ to explore more OOD regions. In contrast, fixed policy regularization always remains close to $\\pi_0$ as it constrains w.r.t $\\pi_0$. Also, without any regularization, the finetuned policy quickly shifts away from $\\pi_0$, suffering potential instability."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699869594168,
                "cdate": 1699869594168,
                "tmdate": 1699869594168,
                "mdate": 1699869594168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K6rxrgRtK2",
                "forum": "S77skzM12O",
                "replyto": "DP3ECWngla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_Ykto"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_Ykto"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the prompt and detailed response."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission70/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632160731,
                "cdate": 1700632160731,
                "tmdate": 1700632160731,
                "mdate": 1700632160731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]