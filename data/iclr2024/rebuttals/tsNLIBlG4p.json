[
    {
        "title": "Analysis of a class of stochastic component-wise soft-clipping schemes"
    },
    {
        "review": {
            "id": "jCnpX37iwf",
            "forum": "tsNLIBlG4p",
            "replyto": "tsNLIBlG4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_GBzB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_GBzB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes and studies a class of component-wise soft clipping algorithms. For strongly convex and non-convex smooth objective functions, the authors establish convergence guarantees of the proposed methods to minimizer/ stationary points. To verify the theoretical results, the authors conduct experiments demonstrating the effectiveness of the soft clipping methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well written and is easy to read. \n\nThe theoretical analysis looks correct, though I do not check all the technical details. The soft clipping framework seems novel."
                },
                "weaknesses": {
                    "value": "1) Although the authors mention in the abstract that theoretical guarantees can provide guidelines for us to choose what algorithms to use, there is little discussion about this aspect in the paper. More specifically, it is unclear through the theoretical results why or when soft clipping is better than hard clipping or vanilla SGD. For the non-convex case, the convergence rate is only $O(1/\\log K)$ which is much slower that standard results for GD/SGD or (hard) gradient clipping, though the assumptions seem to be a little different.\n\n2) It seems to me that although soft and hard clipping are not exactly equivalent, they are basically the same since the induced step size only differs by a constant scale. In the introduction, the authors motivate the study of soft clipping by saying that \"A drawback of the rescaling in (4) is that it is not a differentiable function of the gradient\", which looks confusing, since it is unclear why being differentiable here is favorable.\n\n3) I think there are some existing works on either component-wise or non component-wise clipping, and some of them are cited in this paper, but there is little comparison between these papers and the current paper."
                },
                "questions": {
                    "value": "1) In the experiments, do you compare component-wise clipped SGD with the original clipping method (non component wise)? \n\n2) Can you briefly discuss why the theoretical results in this paper imply that component-wise clipped SGD is superior to other popular methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3643/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698452607485,
            "cdate": 1698452607485,
            "tmdate": 1699636320367,
            "mdate": 1699636320367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DPt6rBkig9",
                "forum": "tsNLIBlG4p",
                "replyto": "jCnpX37iwf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to GBzB"
                    },
                    "comment": {
                        "value": "We thank the referee for the feedback.\n\nOn the weaknesses comments:\n\n1. With \"a theoretical foundation\" and \"motivates their usage\", we only intended to convey that we have proved that they converge under reasonable conditions, not that theory necessarily recommends a specific method. In fact, like we mentioned in the responses to the other referees, we do not yet have a full understanding of which method would be the optimal choice in a given situation. The benefit of soft clipping over hard clipping is mostly empirical, see e.g. the comments in [Zhang et al., 2020a]. As to why clipping should be preferred over vanilla SGD, see e.g.\\ the references [3] and [4] provided by the referee xFTp. These should obviously have been referenced in our paper already and have been added to the revised version.\n\n2. That hard and soft clipping are \"basically the same\" is essentially the argument in the paper by Zhang et al. (2020a), which has been propagated from a similar argument in previous papers with some of the same authors. This seems plausible when stated in this vague form, but when doing the details it becomes apparent that it is in fact not true. That is, the convergence proof for hard clipping does not translate to a proof of convergence for soft clipping. We have now added a few sentences commenting on this.\n\nAs stated above, the fact that soft clipping is preferable to hard clipping is mostly empirical and we do not have a rigorous proof of why differentiability is desirable. However, experience from control theory and ODE time-stepping strongly suggests that this is the case. Essentially, it makes the errors more predictable, and the method more robust.\n\n3. In the revised version, we have improved the sections on contributions and literature overview. This includes more comparisons between our work and that in related papers.\n\n\nOn the Questions:\n\n1. As stated in the text, we are comparing to the component-wise clipped SGD. We thought this was most relevant.\n\n2. We do not claim that these methods are superior, and the analysis does not suggest this. It does, however, prove convergence for a general class of methods in a unified framework. In particular, we observe the somewhat unexpected result that the trigonometric rescaling converges. This doesn't mean that we expect it to be a great method. In fact, we included it mostly to show that the analysis is applicable to a large class of functions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241215216,
                "cdate": 1700241215216,
                "tmdate": 1700241215216,
                "mdate": 1700241215216,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9PZML30Oey",
            "forum": "tsNLIBlG4p",
            "replyto": "tsNLIBlG4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_xFTp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_xFTp"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the family of SGD-type methods with non-linear transformation of the stochastic gradient such that this transformation can be seen as a linear approximation of the stochastic gradient w.r.t. the stepsize. Under standard assumptions (Lipshitz gradients, bounded variance) and some non-standard assumptions (boundedness of the third moments of the iterates) the authors derive $\\mathcal{O}(\\frac{1}{\\ln(K)})$ and $\\mathcal{O}(\\frac{1}{K})$ convergence rates for non-convex and strongly convex cases respectively. Some special cases of the methods fitting the considered framework are tested in the numerical experiments with the training of neural networks. They show similar (slightly worse sometimes) performances to the standard methods like Clipped-SGD or Adam."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "S1. The paper is clearly written and the overall idea of considering different non-linearities/clipping operators is promising.\n\nS2. The proofs are correct."
                },
                "weaknesses": {
                    "value": "W1. The paper does not provide a justification of why and when it is beneficial to use the considered methods.\n\n- W1.1. From the theoretical perspective, the existing results under considered assumptions are either stronger or match the derived ones. Indeed, under bounded variance and smoothness assumptions, standard SGD also converges (and the known rates are even better), e.g., see [1, 2].\n\n- W1.2. From a practical perspective, the considered methods do not show better performance than the standard methods in the experiments considered in the paper. Moreover, since the stepsize decreases over time, all the considered methods will be very close to standard SGD (in view of formula (9)), which is known to have bad convergence on some problems with a lack of smoothness [3] or with a presence of the heavy-tailed noise [4, 5].\n\nW2. Although the paper focuses on the theoretical analysis of the considered methods, the derived results require improvements.\n\n- W2.1. Assumption 6 is quite restrictive: it is unclear apriori whether it holds in the considered setup. Indeed, it can be the case that $\\mathbb{E}[\\|\\|w^k - w^\\ast \\|\\|^3] = \\infty$ while Assumptions 3 and 4 are satisfied, e.g., for simple quadratic function with linear noise $f(v,\\xi) = \\|\\|v\\|\\|^2 + \\langle \\xi, v \\rangle$, where $\\xi$ is zero mean random vector with a bounded variance but an unbounded third moment (e.g., one can take shifted Pareto distribution with tails decaying as $x^{-4}$) we have Assumption 3 with $L_{\\xi} \\equiv 2$, Assumption 4 is also satisfied with some $\\sigma$, but Assumption 6 is not satisfied. The authors refer to the work of Eisenmann & Stillfjord (2022), but in that work, it is assumed that the fourth moment of the gradient is bounded (and the fourth moment of $L_\\xi$) to derive Assumption 6. Such assumptions are even stronger than for the standard analysis of SGD.\n\n- W2.2. The result of Theorem 1 seems to be not tight: when full gradients are used the theorem does not recover the rate of a deterministic counterpart -- Gradient Descent. This happens because $B_i \\neq 0$ even when full gradients are used in the method. A similar issue is present in Theorem 2. Moreover, one can improve the rate from Corollary 1 to $\\mathcal{O}(1/\\sqrt{K})$ (instead of $\\mathcal{O}(\\frac{1}{\\ln(K)})$) if one takes $\\alpha_k \\equiv 1/\\sqrt{K}$.\n\n- W2.3. Assumption 1 does not include some interesting special cases as standard gradient clipping (the second part of the assumption does not hold).\n\n\n[1] Ghadimi, S., & Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4), 2341-2368.\n\n[2] Gower, R. M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., & Richt\u00e1rik, P. (2019, May). SGD: General analysis and improved rates. In International conference on machine learning (pp. 5200-5209). PMLR.\n\n[3] Zhang, J., He, T., Sra, S., & Jadbabaie, A. (2019). Why gradient clipping accelerates training: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881.\n\n[4] Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., & Sra, S. (2020). Why are adaptive methods good for attention models?. Advances in Neural Information Processing Systems, 33, 15383-15393.\n\n[5] Sadiev, A., Danilova, M., Gorbunov, E., Horv\u00e1th, S., Gidel, G., Dvurechensky, P., Gasnikov, A., & Richt\u00e1rik, P. (2023). High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance. arXiv preprint arXiv:2302.00999."
                },
                "questions": {
                    "value": "My main question is about the motivation of this work: why and when SGD-type methods with soft clipping should be used? In the current shape, the theory shows no benefit over the existing results. Moreover, the derived results are even weaker since they rely on an additional restrictive assumption. The numerical experiments also do not provide enough evidence that the methods with soft-clipping can outperform the existing algorithms (in both experiments standard Clipped-SGD performs better than other methods with soft clipping)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3643/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813522803,
            "cdate": 1698813522803,
            "tmdate": 1699636320293,
            "mdate": 1699636320293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WYZF0mhomX",
                "forum": "tsNLIBlG4p",
                "replyto": "9PZML30Oey",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to xFTp"
                    },
                    "comment": {
                        "value": "We thank the referee for the feedback.\n\nOn the Weaknesses comments:\n\nW.1.1: We might be missing something here, but as far as we can see, our rates are the same as for SGD. The result in [2] for strongly convex problems is $\\mathcal{O}(1/k)$ which is the same as in our Corollary 2 (3 in the revised version). The main result Theorem 2.1 in [1] says that $\\|\\nabla f(w)\\| = \\mathcal{O}(1 / \\sum \\alpha_k)$, where $\\alpha_k = 1/k$ would give a $\\log k$ term in the denominator. One main difference is that the results for SGD require a step size restriction, which can be hard to evaluate a priori in practice. The clipped schemes do not have such a restriction.\n\nW1.2: Similar to our responses to the other referees, we agree that the methods do not outperform the methods we compare them to. But viewed from the other side, those methods also do not outperform the generalized clipped methods, which we think is quite significant. The main point of the paper is not to suggest a new \"optimal\" method, but to analyze this large general class of methods in a unified way. One of the main motivations for clipped schemes is precisely that they perform well in the case of heavy-tailed noise, as noted in [4] (the referee's reference, not our paper's [4]).\n\nW2.1: We agree that the assumption is somewhat non-standard and may not be fulfilled in every problem. But in practice, when there is only a finite amount of data and thus also finite possibilities for choosing the stochastic approximation, the third moment will be bounded if the second moment is. The setting in Eisenmann & Stillfjord (2022) indeed does not perfectly match our setting, in that it requires strong convexity. We mainly wanted to state an example included in our setting where such a moment bound on the increments can be observed. Note that the fourth moment of the approximate gradient is not assumed to be bounded for the basic convergence result, it is only used to improve the rate\n\n\nW2.2 (first comment): The price one pays for the generality is the larger error constant in the bound. In our case, the non-linearity of the clipping functions leads to an increased error constant. In practice, however, as the experiments show, it is not an issue.\n\nW2.2 (second comment): Yes, this is true. Our emphasis in the article is on the decreasing step size regime, but a slight extension yields $\\mathcal{O}(1/\\sqrt{K})$-convergence in the non-convex case with the mentioned step size. We have added a second corollary about this in the non-convex part, and a similar remark in the convex case.\n\n\nW2.3 We have updated our main assumption on the methods in line with referee pggZ's comments, and in the new version this method is included.\n\n\nOn the comments in Questions:\n\nAs stated in the responses to the other referees, we do not agree that the Clipped-SGD outperforms the other methods. They essentially perform equally well. We have tried to state more clearly now that the point of the paper is not to suggest new methods that always outperform the state of the art."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241348577,
                "cdate": 1700241348577,
                "tmdate": 1700241348577,
                "mdate": 1700241348577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y1LrwN8GZS",
                "forum": "tsNLIBlG4p",
                "replyto": "WYZF0mhomX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Reviewer_xFTp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Reviewer_xFTp"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their responses. However, my main concerns remain.\n\nRe W.1.1. Yes, my point was about this: standard results without clipping also provide similar guarantees. Moreover, some assumptions (like Assumption 6) are not needed in the analysis of SGD. The authors point out that the results for SGD require steps size restriction, while the clipped schemes that they consider do not require such restrictions. This happens due to the change of the assumptions that authors use: in the descent, Lemma 1, one can get rid of the extra squared gradient norm (appearing in the standard analysis of SGD) using Assumption 6. In fact, if we take $H \\equiv 0$ the method becomes standard SGD, and the analysis still holds, i.e., for SGD one can also get no restrictions on the step size in the considered settings.\n\nRe W1.2. Since the proposed method can be seen as SGD with quadratic w.r.t. the stepsize perturbation, it is not obvious beforehand that these methods will behave similarly to the method with standard clipping on the heavy-tailed task. Moreover, due to the same reason, it is expected that the methods considered in the submission will behave similarly to SGD that is significantly outperformed by ClippedSGD and Adam in such tasks as considered in [4]. It is also worth mentioning that the settings considered in the submission do not allow heavy-tailed noise since they are stronger than the bounded variance assumption.\n\nRe W2.1. Even if the third moment exists (like in the finite-sum case), it is quite restrictive to assume beforehand that there exists some $M$ such that Assumption 6 holds for all $k$, because it implicitly assumes some kind of stability of the method. Such results require formal proof.\n\nRe W2.2. My comment regarding non-tightness is still valid. I acknowledge that the framework is general, but maybe the analysis can be not tight in some other non-trivial cases, if it cannot recover best-known results in the standard cases.\n\nRe W2.3. If I understand correctly, standard (component-wise) clipping still does not fit the updated assumption."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724590820,
                "cdate": 1700724590820,
                "tmdate": 1700724590820,
                "mdate": 1700724590820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tGDcS2aE1d",
            "forum": "tsNLIBlG4p",
            "replyto": "tsNLIBlG4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_GtoB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_GtoB"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyze a general soft-clipping scheme which extends vanilla clipping schemes such as the one from Eisenman & Stillfjord (2022), and the one from Zhang et al. (2020a). They provide its convergence analysis in the convex and non-convex setting under various assumptions. Furthermore they also compare such clipping schemes, instantiated with several clipping functions, with usual methods for training DNNs, and show that such method can achieve empirical results similar to state of the art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Originality\n\nThe results given in the paper are, up to my knowledge, novel. The fact that the method and its proof work for a very general range of clipping schemes makes such proof useful to a larger extend in the literature.\n\n### Quality\n\nUnless I am mistaken, overall the proofs look good and of quality.\n\n### Clarity\n\nThe previous works, context, and assumptions for the theorems, as well as the main results (in theory and practice) are clearly described.\n\n### Significance\n\nI believe the problem considered is of interest to the community, since the authors considered the soft clipping scheme method, which has been shown empirically to obtain a good performance in practice, in particular for deep learning."
                },
                "weaknesses": {
                    "value": "- 1. I think that the comparison with state of the arts results (theorems, assumptions), could be made a bit more explicit and structured (see question 1 below)\n- 2. I think that the reason why considering new special soft-clipping schemes could be elaborated on further (see question 2 below)."
                },
                "questions": {
                    "value": "1. Although the state of the art is well stated which allows one to dig further into the appropriate references, I believe a more explicit *comparison* with the state of the art, gathered at one place in the paper (perhaps in Appendix) would improve the quality of the paper. For instance, I think the two most related papers are the ones from Eisenman & Stillfjord (2022), and the one from Zhang et al. (2020a). Even though by checking those references one may get some idea on the difference between those papers and the submitted one (i.e. Eisenman & Stillfjord (2022) only deal with a strongly convex setting, while Zhang et al. (2020a) have slightly different assumptions, and also, the submitted paper studies a general form of clipping)), I still think it would be good to compare the method exposed in the paper with those two papers more systematically (perhaps in Appendix, in a structured form such as a table) (for instance, saying which algorithms is a special case of which (some algorithms may have momentum, some may have general component-wise functions etc), what assumptions they make in the papers, and what is the resulting convergence rate. For instance, in the strongly convex setting, does the results in the paper retrieve the results from Eisenman & Stillfjord (2022) ? Also, it would be interesting if possible to compare how taking different assumptions and/or considering more general settings actually impacts the proofs compared to the previous ones in the literature. \n\n2. Although a general form of clipping is analyzed, if I am not mistaken, I didn\u2019t see any motivation or related works related to the new gradient clipping schemes from the Appendix A (i.e. Examples 2-4). And in practice it seems that those do not make a big difference (most curves are superimposed so it is hard to differentiate them). Therefore, even though having a general convergence proof is always good, I think it would be even better to further motivate the use of any of the new Examples 2-4 from Appendix A, for instance by mentioning whether they are present in the literature, discussing how their shape depart from the classic soft-clipping (and why it is advantageous), or through experimental results that would show their advantage in some cases."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3643/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3643/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3643/Reviewer_GtoB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3643/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817624557,
            "cdate": 1698817624557,
            "tmdate": 1699636320193,
            "mdate": 1699636320193,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oHvpEbp29X",
                "forum": "tsNLIBlG4p",
                "replyto": "tGDcS2aE1d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to GtoB"
                    },
                    "comment": {
                        "value": "We thank the referee for the positive remarks and score.\n\nOn the Questions:\n\n1. We have slightly extended the literature review and contributions section, and hope that the differences the referee points out are now more clear. Given how many different settings that could be considered, and how slightly different assumptions lead to significantly different rates, we do not think an exhaustive comparison is feasible. The presented analysis is the first attempt at considering this general class of methods, and we think it is also somewhat unfair to compare this to e.g. highly refined proofs for SGD in special settings. However, in line with the comments by the referee xFTp, we have added some straightforward extensions of the presented analysis on certain fixed step sizes.\n\n2. As mentioned in the responses to the other referees, we do not yet have a full understanding of which rescalings are best in each situation. Precisely like the referee comments, the error curves are essentially superimposed in both our numerical examples. This indicates that the different rescalings often behave similarly in practice, so that doing some rescaling is more important than what kind of rescaling. The idea of the paper is to simultaneously analyze most such rescalings in a common framework, and finding the optimal choice in a particular situation is still an open problem."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241147812,
                "cdate": 1700241147812,
                "tmdate": 1700241147812,
                "mdate": 1700241147812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sC3QfXKhj6",
                "forum": "tsNLIBlG4p",
                "replyto": "tGDcS2aE1d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Reviewer_GtoB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Reviewer_GtoB"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for your answer, which clarifies my questions. I have also read the other reviews and responses, as well as the new revision, and have decided to keep my score: I believe such paper might be of interest to the community, since it analyzes some recent soft-clipping methods in general ways, but I am not an expert in the field, and as such, cannot guarantee full confidence in my evaluation."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652391919,
                "cdate": 1700652391919,
                "tmdate": 1700735773787,
                "mdate": 1700735773787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uxhO2w3vQL",
            "forum": "tsNLIBlG4p",
            "replyto": "tsNLIBlG4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_DnUF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_DnUF"
            ],
            "content": {
                "summary": {
                    "value": "The article introduces a class of soft clipping schemes for various applications which have not been extensively analyzed in the literature, especially in nonlinear cases. It provides a theoretical foundation, demonstrating convergence properties, and highlights that these soft clipping algorithms perform similarly to state-of-the-art methods like Adam and SGD with momentum on large-scale machine learning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe article gives proofs of convergence in expectation with rates in both the convex and the non-convex case.\n\n2.\tThe numerical experiments in this paper are beautiful which shows that soft-clipping algorithms may offer regularization benefits in cases where other algorithms tend to overfit, encouraging the use of soft-clipping algorithms and further research in the field."
                },
                "weaknesses": {
                    "value": "1.\tThe comparative analysis with other literatures is insufficient, and it is difficult to see the innovation of the convergence results or proofs in this paper.\n\n2.\tThis paper lacks some intuitive understanding and analysis of the theorems and corollaries given. Especially for the symbols $\\w_k(w)$ without interpretation in Corollary 2, it\u2019s hard for readers to understand and what insight the corollary hopes to provide."
                },
                "questions": {
                    "value": "1.\tHave similar convergence analyses been conducted in other literature? If they exist, can you provide a comprehensive comparison to highlight the advantages of the results presented in this paper?\n\n2.\tIs it reasonable to assume that $\\sum_{k=1}^\\infty \\alpha_k^2 < \\infty$ and $\\sum_{k=1}^\\infty \\alpha_k = \\infty$ as stated in Theorem 1? Could you offer a more intuitively understandable explanation for this assumption?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3643/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699023309904,
            "cdate": 1699023309904,
            "tmdate": 1699636320101,
            "mdate": 1699636320101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8NYo7tCbpr",
                "forum": "tsNLIBlG4p",
                "replyto": "uxhO2w3vQL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to DnUF"
                    },
                    "comment": {
                        "value": "We are happy that the referee likes the numerical experiments and thank them for the fair rating.\n\nOn the Weaknesses comments:\n\n1. We have slightly extended the literature review and contributions section, in line with the comments by referees pggZ and GtoB. If there is some specific additional reference which the referee thinks should also be included, please let us know.\n\n2. We apologize for the confusing statement of Corollary 2. We did not realize that after some reordering of the material, it was no longer specified that $\\omega$ was an element of the sample space $\\Omega$. It has now been reformulated. The idea of Corollary 2 is just that the norm of the gradient of $w_k$ will almost surely (i.e. for essentially every random path) go to zero. The min over the $K$ iterations is needed for technical reasons, similar to e.g. the results for standard SGD.\n\n\nOn the Questions:\n\n1. The general class of methods is, to our knowledge new, and there is no convergence existing analysis. With some specific choices of G, we acquire previously analyzed methods like SGD or the TSGD. We have commented on this in the contributions and literature overview sections.\n\n2. Yes, this is a standard assumption that goes back all the way (at least) to the paper by Robbins and Monro. It means that the step-size sequence goes to zero, but not too quickly. The idea is that we have to take large enough step sizes to actually get closer to the minimum. But once we get close enough, the noise in the stochastic gradient would prevent us from getting closer. By decreasing the step size, the noise is reduced. The assumed step size condition adequately balances these two conflicting demands."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241070373,
                "cdate": 1700241070373,
                "tmdate": 1700241070373,
                "mdate": 1700241070373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dCLwSsKwfw",
            "forum": "tsNLIBlG4p",
            "replyto": "tsNLIBlG4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_pqgZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3643/Reviewer_pqgZ"
            ],
            "content": {
                "summary": {
                    "value": "As reminded in the paper, authors such as Mikolov (2013); Duchi et al. (2011); Kingma & Ba (2015) previously proposed element-wise gradient updates. In this paper, the authors recall the soft-clipping approach presented by Zhang et al. (2020a) and propose a class of methods that combine the idea of element-wise gradient updates with soft-clipping. To be more precise, the methods analyzed here are a direct generalization of the \u201celement-wise clipped version\u201d of the soft-clipping algorithm of Zhang et al. (2020a).\n\nUnder standard assumptions on the learning rate (Robbins\u2013Monro conditions), standard regularity of the loss function, and standard noise assumptions, the authors prove that the class of stochastic optimizers they proposed converges to a stationary point. In particular, they show that the minimum norm of the gradient converges to 0. All the theoretical results presented in the paper are pretty standard, including the proof-technique aspect: Lemma 2 and Lemma 3, Theorem 1 and Theorem 2 are reminiscent of their respective version for vanilla SGD.\n\nFrom an experimental aspect, the experiments compare the performance of the stochastic methods introduced in this paper w.r.t. well-known ones such as Adam, SGD + Momentum, and others. The authors conclude that their performances are comparable."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality**: The authors present a novel class of stochastic optimizers that combine the idea of soft-clipping and element-wise gradient updates.\n\n**Quality**: The theoretical setting is well-posed and well-presented. Indeed, all assumptions are clearly stated, grounded in the literature, and are not restrictive. Together with their proof, the theoretical results are clearly stated and easy to follow and understand.\n\n**Clarity**: The key messages of the paper are clearly reported at the end of the \"Introduction\". The readers can follow the whole discussion with little effort.\n\n**Significance**: Given the success of both soft-clipping and element-wise gradient updates, it is important to study if the interaction of these two ideas brings additional non-trivial advantages."
                },
                "weaknesses": {
                    "value": "**Research Aspect:**\n\nWhile the topic is clearly of interest, I am left wondering about the effective novelty of the contribution. To be more specific, Theorem 3.1 and Theorem 3.2 in Zhang et al. (2020a) already provide convergence results for a hard-clipping algorithm. Additionally, in _Appendix F Soft Clipping_ of the same paper, the authors give a fairly reasonable explanation of why such results should easily generalize to the _soft-clipping_ version of their algorithm.\n\nIf I look at Theorem 1 and Theorem 2 of the paper under review, I observe that these are convergence results for optimizers which are a generalization of the \u201celement-wise clipped version\u201d of the soft-clipping algorithm of Zhang et al. (2020a).\n\nWhile it is clear that these are different algorithms, it is not clear which additional theoretical benefit one inherits by allowing:\n1) More general clipping functions (w.r.t. the one in Zhang et al. (2020a));\n2) The element-wise clipping itself.\n\nQuestions presented later articulate my concerns and provide actionable suggestions on how to improve the contribution of this paper.\n\n**Experimental Validation:**\n\nThe experiments presented compare the performance of the stochastic methods introduced in this paper w.r.t. well-known ones such as Adam, SGD + Momentum, and others. Here are some actionable suggestions to improve the experimental aspect of the paper:\n\n1) Figure 1 and Figure 2 lack confidence bars around the lines to represent the uncertainty. They are difficult to read and it would be more interesting to plot those lines in log-scale to clearly see the comparison between the performance of the methods.\n\n2) None of the experiments presented is meant to verify the theoretical insights provided in Section 2. Especially, it would be relevant to verify Theorem 1, Corollary 1, and Theorem 2. There is no need for sophisticated setups: I suggest starting with at least a simple landscape such as a quadratic one: $f(x) = x^{\\top} A x$ where $x \\in \\mathbb{R}^d$ and $A \\in \\mathbb{R}^{d \\times d}$ is or is not an SDP matrix would suffice.\nGiven that these are the core results of the paper, I would expect even a simple graph where you compare the decay of the minimum of the norm of the gradients w.r.t. the bound that you derive. This would not only validate the theoretical results empirically but also show how loose these bounds are. If anything, at least verifying this for Corollary 1 should be doable as the shape of the bound is very explicit.\n\n3) I suggest adding a comparison of each clipped-component-wise method with its NON-component-wise version: This would help understand the benefit of the new methods and of clipping component-wise w.r.t clipping in a non-component-wise way.\n\n**Quality of the Exposition**:\n\nRegarding style and organization of the text, I would invite the author to reformulate the text written before Section 1 to:\n\n1) Add a section named \u201cIntroduction\u201d where you discuss the problem at hand, what you achieved in this paper, and how it is relevant;\n2) Add a section called \u201cRelated Works\u201d of \u201cLiterature Review\u201d;\n3) Make sure that the exposition is fluent and pleasant to read rather than a list of points: Separating text in paragraphs is very helpful.\n\nComing to **Section 1**, currently called Setting, I suggest moving this whole section to the Appendix and just quickly presenting the content of such a section: The assumptions are pretty standard and could be easily summarized in a sentence at the beginning of Section 2 (currently named \u201cConverged Analysis\u201d). For example, one could simply start Section 2 by stating _\u201cIn this section, we will provide a convergence analysis for all methods that fit into the setting that is described in the introduction. The assumptions are standard ones from the literature and are reported in Appendix C\u201d_. Of course, if something is NOT standard in the literature, it is worth stating it clearly in the main paper.\n\nComing to **Section 2**, Lemma 2 and Lemma 3 are not key results and can be moved to the appendix.\n\nComing to **Section 3**, there is no need to report all the technical details of the numerical experiments in the main paper. I suggest moving the best portion of this section to the appendix.\n\nComing to the **Appendix**, please consider reporting the statements of the theorems before their proof.\n\nAdditionally, there are some typos. Among others, these are the most visible ones:\n\n1) Just below Eq. (3). Please, specify where $(x_i,y_i)$, even if it is $(x_i,y_i) \\in \\mathcal{X} \\times \\mathcal{Y}$;\n2) On the fifth row from the bottom of page 1: \u201cIf the function sufferS\u201d. Add the \u201cs\u201d;\n3) At the beginning of page 4: Add the capital letter T to \u201cthere\u201d in both lines and add where $x_i$ belongs;\n4) Third line from the top of page 3: \u201cstochastic optimization algorithms that mitigateS this issue, is that of performing the gradient update\u201d. Add the \u201cs\u201d.\n5) Just above Lemma 2: \u201cbut their sharpnessES\u201d. Add the plural. OR, keep it singular and at S after \u201cdiffer\u201d.\n\n**Conclusion**:\n\nThis paper proposes a class of stochastic optimizers based on the combination of soft-clipping and element-wise gradient updates. The authors provide standard convergence bounds under standard assumptions: Their proofs are mostly based on standard techniques applied to the specific case addressed in the paper. The experimental section shows the performance comparison between some of the proposed methods and classic ones such as Adam and SGD + Momentum: The Accuracy and Perplexity are comparable across optimizers. Unfortunately, none of the theoretical results is experimentally validated.\nTo conclude, the class of optimizers is interesting, but the theoretical analysis provided here is too restricted to be a substantial contribution (questions below might provide inspiration). The experimental side is also lacking and deserves more attention (see above for some suggestions). From an organizational perspective, the manuscript needs significant rewriting (see above for some suggestions)."
                },
                "questions": {
                    "value": "Here is a list of questions that I think could guide the authors toward a better final product:\n\n1) What is the behavior of the optimizers w.r.t escaping saddles? What about their preference for sharper or flatter minima?\n\n2) Could you please elaborate on the advantages of component-wise clipping?\n\n2.a) From a **theoretical perspective**,  I am not sure about the actual benefits of component-wise updates. Indeed, you use Assumption 1 only in the proof of Lemma 2 where you show that $\\lVert H(\\nabla f, \\alpha) \\rVert < c_h \\lVert\\nabla f \\rVert^2$. Then, you (implicitly) use it also to derive a bound on the expected value of the norm of G. From my perspective, Assumption 1 could be replaced with the following:\n\ni) There exists $c_g \\in \\mathbb{R}^{+}$ such that $\\lVert G(x,\\alpha) \\rVert < c_g \\lVert x \\rVert$;\n\nii) There exists $c_h \\in \\mathbb{R}^{+}$ such that $\\lVert H(x,\\alpha) \\rVert < c_h \\lVert x \\rVert^2$.\n\nThis way, you could cover more general cases, including yours and also that of Eq. (5) from Zhang et al. (2020a).\n\n2.b) From an **experimental perspective**, in both of your experiments, it seems that Clipped SGD as implemented in Abadi et al. (2015) is the best performer. Is there any case you could find where component-wise clipped methods have better performance than the non-component-wise methods?\n\nWhat I find missing is a comparison between the optimizer of Eq. (5) (which is without the component-wise clipping) and your component-wise clipped version. In general, for each component-wise clipped method you proposed, I would like to see a clear comparison with its counterpart without component-wise clipping. This would experimentally clarify that having a component-wise clipping is advantageous.\n\n2.c) Of all the component-wise soft-clipping schemes provided, which ones are more advantageous? Is there a way to somewhat understand a recommended shape of the functions $g$ and/or $h$? Which ones clearly show an advantage w.r.t. the one in Eq. (7)? If one were to find one that works better than the one in Eq. (7), how about its NON-component-wise version?\n\n**Minor Questions**:\n\n1) Why is it necessary that $l$ is a non-negative loss function?\n2) Can you be more explicit regarding G and H and their mutual relationship?\n3) What is the relative size of the constants $B_i$ w.r.t the constant you would find with vanilla SGD? \n4) The scheduler used in the experiment is in line with those used in the theoretical results. However, I am sure you noticed that in 150 epochs, it drops from an initial value of $\\beta$ to $\\sim 0.985 \\beta$. It does not strike me as a learning rate that decreases much. How would this compare to a learning rate kept constantly equal to $\\beta$?\n5) You write that _\u201cWe also note that the clipping schemes all require a higher step size for optimal performance than that of both Adam and SGD with momentum, which exemplifies their better stability properties\u201d_. Can you elaborate further on this claim?\n6) Could you please make sure that the colors of the lines of different optimizers are consistent between Figure 1 and Figure 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3643/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3643/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3643/Reviewer_pqgZ"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3643/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699454526645,
            "cdate": 1699454526645,
            "tmdate": 1699636320017,
            "mdate": 1699636320017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0pgjKX2S6H",
                "forum": "tsNLIBlG4p",
                "replyto": "dCLwSsKwfw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to pggZ (part 1)"
                    },
                    "comment": {
                        "value": "We thank the referee for the positive remarks and the effort required to write such a comprehensive report.\n\n\nOn the Research Aspect comments:\n\nWe are aware of the argument in the paper by Zhang et al. (2020a), which has been propagated from a similar argument in previous papers with some of the same authors. As the referee notes, the argument seems \"fairly reasonable\" when stated in this vague form, but when doing the details it becomes apparent that it is in fact not true. That is, the convergence proof for hard clipping does not translate to a proof of convergence for soft clipping. We have now added a few sentences commenting on this.\n\n\n\nOn the Experimental Validation comments:\n\n1. We unfortunately do not see an easy way to add confidence bars in our computational setup. We also suspect that doing so would further decrease the readability of the figures.\n\n2. The experiments are intended to illustrate the performance of the methods on real-world applications. The verification of the theorems and corollaries are their proofs in the respective appendices. We aimed for rather complex experiments as this seemed to be what this community wanted to see, but we're happy to include a more simple setup as well. The revised version has two experiments with $\\nabla F(w) = Aw$, where we choose $A$ to be a stiff diagonal matrix (large ratio between largest and smallest eigenvalue). The first one provides some motivation for why it is beneficial to use a componentwise method in the stiff case, and the second one compares the different generalized clipping functions. Finally, Figure 1 and 2 are already on a log-scale (\"semilogy\"), which means that the spacing near 100% and 90% (1 vs. 0.9) is smaller than between e.g. 30% and 40%. We have now changed it to be not logarithmic, which separates the error curves a bit more.\n\n3. We are not exactly sure what it means to apply non-component-wise versions of the component-wise schemes. Should we replace the component with e.g. the norm of the whole approximate gradient?\n\n\nOn the Quality of the Exposition comments:\n\nWe have now rearranged the first section along the lines of the referee's suggestion. We are sorry to hear that the referee thinks the text is unpleasant to read, but several of the other referees seem to think that it is easy to follow.\n\nWe appreciate the referee's suggestion on different ways to organize the text, but we feel that such drastic changes would lead to a whole new paper. Since the other referees do not argue that the layout is fundamentally broken, we will only make major changes to the first section as mentioned above. One thing we will do is to repeat the theorem statements in the appendices. The mentioned typos will be fixed, except #4. Since the subject of that sentence is \"An approach\" rather than \"optimization algorithms\", it should indeed be \"mitigates\" in the singular rather than plural form.\n\n\n\nOn the main Questions:\n\n1. We think that a proper investigation of this would be interesting, but that it is out of the scope of this paper. However, all of the methods essentially become SGD once the step size becomes small enough, so unless a saddle point is reached very early in the iteration the methods would inherit the properties of SGD.\n\n2. The new small-scale quadratic example hopefully sheds some light on the benefits of the componentwise setup. Essentially, the idea is that the components of the exact solution will tend to the components of the stationary solution at different speeds, and the componentwise updates allow the approximate solutions to replicate this behaviour.\n\n2.a) We have now changed Assumption 1 in line with the referee's suggestion.\n\n2.b) Our opinion is rather that all the methods behave roughly equally well. The minor differences in final loss/accuracy are negligible. We have tried to emphasize this better. The new experiment shows that the componentwise tamed SGD performs better than the non-componentwise version on that particular problem.\n\n2.c) We do not yet have such an understanding, but we also do not recommend any specific rescaling. The point of the paper is to show that this general class of methods can all be analyzed in a unified way, rather than to suggest a new method that always outperforms other methods. We agree that it would be more satisfying to be able to say that a given rescaling works best for a given problem, but such statements require further research."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240954728,
                "cdate": 1700240954728,
                "tmdate": 1700240954728,
                "mdate": 1700240954728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "seBKLSS2hL",
                "forum": "tsNLIBlG4p",
                "replyto": "0pgjKX2S6H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Reviewer_pqgZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Reviewer_pqgZ"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Part 1"
                    },
                    "comment": {
                        "value": "I acknowledge and appreciate the reply of the Authors. However, I still have some open points and I need to see the revised version before considering changing my score.\n\n**Research Aspect**\n\nI am still left wondering what is the theoretical and experimental benefits of these new methods you introduced. Maybe a comparison with the proof for the NON-componentwise clipping could elucidate the theoretical aspect?\n\n**Experimental Aspect**\n\n1) While I understand that adding confidence bars might be visually unpleasant, there are two alternatives to go in this direction. The first is to provide a \"zoom-in\" towards the end of training and add the bars there. The other is to provide a table where you show the average performance measure at the end of training and the average performance measure at test time, and add their standard deviations as well.\n\n2) If you truly wanted to illustrate the performance of the methods on real-world applications, you could have used some little transformers. For example, \"https://github.com/karpathy/nanoGPT\". There is no need to outperform popular optimizers: If you could show that your algorithms do as well as (or possibly even slightly worse than) the state-of-the-art but with considerably fewer resources (especially less clock time), this would be amazing. I look forward to seeing the new experiment you mentioned in the revised version.\n\n3) Yes, the component-wise clipping is your method for a given $h$ or $g$. The NON-component-wise clipping would be the equivalent method (same $h$ or $g$) where you do not clip component-wise but rather use the same clipping for all the components.\n\n**Quality of the Exposition**\n\nI truly think that moving all the text that is not key to the comprehension of the main points of the paper should go to the appendix. Repeating standard assumptions, reporting technical lemmas, and presenting the details of the experiments are very likely to distract the reader.\n\nQ1: I agree that the intuition you provide makes sense. But, I would not jump to this conclusion so easily: Just like the comment in Zhang et al. (2020a) **sounds** fairly reasonable, the one you presented here **sounds** fairly reasonable as well.\n\nQ2: I look forward to seeing the new experiment in the revised version.\n\n2.a: Thanks.\n\n2.b: I look forward to seeing the new experiment in the revised version.\n\n2.c: Thanks."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638932662,
                "cdate": 1700638932662,
                "tmdate": 1700638932662,
                "mdate": 1700638932662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MRRzcClTdR",
                "forum": "tsNLIBlG4p",
                "replyto": "VRZ0933hLS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3643/Reviewer_pqgZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3643/Reviewer_pqgZ"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Part 2"
                    },
                    "comment": {
                        "value": "**Minor Questions**\n\n1. Ok, thanks.\n2. Ok, thanks.\n3. I see, thanks.\n4. I am sorry, my bad. Still, it would be interesting to compare the performance with a learning rate kept constantly equal to $\\beta$.\n5. Ok, thanks.\n\n**Conclusion**\n\nI am still not convinced by the reply provided to me and to the other Reviewers. However, I do appreciate the discussion and I hope the Authors will manage to submit a revised paper that tries to address (some of) the points discussed with me as well as (some of) those discussed with other Reviewers."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3643/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639579892,
                "cdate": 1700639579892,
                "tmdate": 1700639579892,
                "mdate": 1700639579892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]