[
    {
        "title": "Close the Gap: Lightweight Image Captioning via Retrieval Augmentation"
    },
    {
        "review": {
            "id": "GYtZfx4siI",
            "forum": "hgayrNSbri",
            "replyto": "hgayrNSbri",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_yd7b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_yd7b"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on image captioning with retrieval augmentation. Speicifically, the author proposed to retrieve a set of captions relevant to the input image via CLIP cross-modal retrieval. The retrieved captions are then sent into a pre-trained language model for summarization. The summarized results are regarded as the caption of the input image.\n\nTo improve the quality of CLIP retrieval, the author also proposed a light-weight projection layer that can be solved in closed-form to better align image and language modalities.\n\nTo augment the available caption set, the author also proposed an iterative self-improvement strategy by including captions generated by the model itself and improving the projection weight."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Training the projection weight $\\textbf{W}$ is extremely low cost."
                },
                "weaknesses": {
                    "value": "1. The main point of this paper is a $\\textbf{lightweight}$ image captioning system. However, the proposed method is only lightweight during training in terms of training time. For inference, the model needs to encode the image, retrieve several captions, and then feed the retrieved captions into an LLM. The overall procedure may even be slower than a vanilla VL model for image captioning. For training, the proposed method did $\\textbf{NOT}$ save trainable parameters compared to recent VL models like LLaVA, which only has a single fc layer to bridge the vision and language modalities. As the reviewer can tell from the paper, the \"lightweight\" part is only on the training speed and resources. This greatly reduced the contribution of this paper in the reviewer's opinion.\n\n1. The performance of the proposed model is far below current SoTA models. Even though the proposed method saves training time and resources. The performance decrease is disproportional.\n\n1. More details and analyses about the retrieval set should be discussed. For example, what retrieval set does the author use during training and testing/validation? What retrieval set does the author use for different datasets such as COCO and Flickr? Is the performance sensitive to the quality or distribution of the retrieval set?\n\n1. There exists other retrieval augmented image captioning works that achieve reasonable performance such as REVEAL [1], RA-CM3 [2], HAAV [3], etc. Current approaches typically train the model jointly with the retrieval.\n\n    [1] https://arxiv.org/abs/2212.05221\n\n    [2] https://arxiv.org/abs/2211.12561\n\n    [3] https://arxiv.org/abs/2305.16295"
                },
                "questions": {
                    "value": "Please see the weakness section, particularly about the details and analyses of the retrieval set."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725715473,
            "cdate": 1698725715473,
            "tmdate": 1699636507592,
            "mdate": 1699636507592,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aFIpJ16ec5",
                "forum": "hgayrNSbri",
                "replyto": "GYtZfx4siI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We address the mentioned weaknesses as follows:\n\nWe **explicitly define lightweight** in the second paragraph of the introduction as the **number of trainable parameters.** The overall method is **NOT slower than other vanilla VL approaches.** We mention in Section 3.2 that inference is on-par with SmallCap (the most lightweight approach thus far). Furthermore, **LLaVa consists of a pre-training and a fine-tuning stage**, where the entire LM is trained during the latter. Even when considering only the pre-training stage of LLaVa, which only trains the linear mapping, **our method is more lightweight**, since the **embedding space of CLIP is 4 times smaller than the embedding space of LLama, which results in 4 times fewer trainable parameters. **\n\nWe admit that we did not reach SOTA on any benchmark. However, our method is comparable with other lightweight captioning approaches, while being more efficient. Further, **we could slightly improve performance** by using a different prompting strategy, as highlighted in our revised version.\n\nGenerally, the retrieval set (denoted datastore in the paper) contains the captions from the training set. This is the same during training and evaluation. Only in case when the self-improvement loop is applied, we add synthetic (i.e., self-generated) captions for the training images. We do not use additional data sources. To evaluate out-of-distribution performance we **will add results on NoCaps** in the revised version.\n\nThank you for making us aware of additional relevant literature, we will add a discussion on these works in our revised manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740428950,
                "cdate": 1700740428950,
                "tmdate": 1700740428950,
                "mdate": 1700740428950,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RpLZgCYjn7",
            "forum": "hgayrNSbri",
            "replyto": "hgayrNSbri",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_ZWpM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_ZWpM"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel and cost-effective method to bridge the modality gap between pretrained language models (LMs) and pretrained vision-language models (VLMs) for enhanced image captioning. The modality gap refers to the misalignment between image and text representations in the shared embedding space, which can hinder the performance of image captioning models. The proposed method utilizes a linear mapping optimized via a least-squares solution to bridge this gap, enabling efficient computation on CPUs without requiring gradients. This approach allows for competitive performance on image captioning tasks compared to other lightweight captioning methods, particularly when reference-based metrics are employed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces a novel method of using a linear mapping to bridge the modality gap in image captioning, avoiding the need for costly end-to-end training. The proposed method requires only 1 million trainable parameters, which is significantly lower than other deep learning models, making it computationally efficient. The Iterative Self-improvement Loop Introduces a process of iteratively refining the mapping with synthetic captions generated by the model, which could lead to better performance metrics over time without additional human-labeled data. The method also achieves strong results on well-known benchmarks like MS-COCO and Flickr30k, indicating that the method performs well compared to existing lightweight captioning approaches. Overall, the paper provides a practical, efficient, and potentially more accessible solution to the problem of image captioning, with particular relevance to real-world applications where resources might be limited."
                },
                "weaknesses": {
                    "value": "Generalizability Issue: The method is heavily relied on the referenced dataset which poses a critical limitation of the method. While the method is efficient, it may not generalize well to all types of images or domains, especially those significantly different from the reference dataset. If the testing dataset is very different from the reference set, it may cause catastrophic outcomes by error propagation and require much longer rounds of iteration. This would diminish the efficiency advantage. Further, if the reference dataset has limited topic/semantic coverage, then the potential hallucination issue would also happen as the LLM was only asked to summarize the referenced captions. Not matter how many iterations would be conducted, the hallucination may not be removed. \n\nLimited Experiment Issue: Additionally, the performance is benchmarked on specific datasets. Since both Flickr and COCO are two very common datasets and share large similarity in terms of image distribution thus there might be a lack of evidence on how the model performs on out-of-distribution data or in more diverse real-world scenarios. \n\nLLM Issue: The method also heavily relies on LLM, however the LLM never saw the image but only can summarize. The iterative refinement with synthetic captions could potentially introduce or amplify errors if the synthetic captions are not of high quality.\n\nComputational Resources: While the model has fewer parameters, the computational cost and efficiency are not only about the number of parameters but also about the complexity of operations and the size of the input data. For every image, the method requires to pre-computation of all the reference image and text data, not to mention the following newly updated synthetic captions for every iteration.\n\nMetric Sensitivity: The paper proposes a new metric for evaluation, but it\u2019s not clear how sensitive the results are to the choice of metric and whether the proposed metric has been validated extensively.\n\nSeverely Limited Novelty/Details of the Caption: A good caption is not just a high-level summarization of the image but should also cover essential fine-grained details. The characteristics of this method determines it cannot include fine-grained details in the caption.\n\nNovelty of the method: The method may lack novelty but is a practical improvement in real world scenarios."
                },
                "questions": {
                    "value": "1. If the testing dataset is very different from the reference set, it may cause catastrophic outcomes by error propagation and require much longer rounds of iteration. This would diminish the efficiency advantage. How would you resolve this problem?\n\n2. How would you guarantee the reference set necessarily has all the sufficient scenes against the testing set? If not, then certain critical scenes in testing set images can never be addressed properly. What is the criteria of selecting the reference set and testing set to ensure this method can work properly? If you cannot find a criteria, then this method has a fundamental issue.\n\n3. Have you done experiment when the reference set and testing set are very different from each other?\n\nPlease see other questions in the  \"Weakness\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699417629289,
            "cdate": 1699417629289,
            "tmdate": 1699636507506,
            "mdate": 1699636507506,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZRiDyAaAND",
                "forum": "hgayrNSbri",
                "replyto": "RpLZgCYjn7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We agree with the reviewer that our method is limited by the contents of the reference set (denoted datastore in the paper), that is the training set. However, it is **a general property of machine learning algorithms that their generalization abilities are restricted to the training distribution.** We do not claim that our method generalizes beyond the training distribution. \n\nWe **will add results on NoCaps** for a linear mapping trained on the MS-COCO dataset to evaluate the out-of-distribution performance of our method.\n\nWe agree that adding low-quality synthetic captions can amplify errors. This is the reason why **we threshold synthetic captions according to a certain metric**, which can be any of CIDEr-D, SPICE, B@4, ROUGE-L, etc. After additional experiments we found that **thresholding according to the CIDEr-D metric works the best** and usually leads to improvements across metrics.\n\nWe believe that there is a misunderstanding regarding the self-improvement loop. The self-improvement loop is only part of the training algorithm and does not run during evaluation/inference. Further, our **reference set only contains texts which are pre-computed once.** During self-improvement, **synthetic captions are embedded once** and added to the reference set.  We will make this more clear in the paper. \n\nThere must be a misunderstanding, since **we did not propose a new metric.**\n\nA new image **does not need to** be exactly covered by captions contained in the reference set. Several captions can contain objects contained in a new image and the **FLAN-T5 can interpolate between them** and find a suitable caption for a new image. We will add some qualitative examples on that in our revised version.\n\n**Questions:**\n- If a concept of a new image does not appear in the training captions then it will also not appear in the generated caption. However, **this problem is not specific to our method, but applies to any other image captioning pipeline.**\n- We believe there is a misunderstanding. Our method does **not rely on any additional data source.** It only retrieves from the training set, whereas other image captioning pipelines are trained end-to-end on the training data. As any other method **we rely on the assumption that the test distribution follows the training distribution.**\n- We will **add results on NoCaps** for a linear mapping trained on the MS-COCO dataset to evaluate the out-of-distribution performance of our method. Since our method uses the training captions as reference set, our method would not produce appropriate captions for test images that substantially differ from training images."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740275155,
                "cdate": 1700740275155,
                "tmdate": 1700740275155,
                "mdate": 1700740275155,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iTSQ4WTFPq",
            "forum": "hgayrNSbri",
            "replyto": "hgayrNSbri",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_mspE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_mspE"
            ],
            "content": {
                "summary": {
                    "value": "This paper trains linear mapping (W) on top of the CLIP image encoder and text encoder to improve or refine the caption of a given dataset. During training, the mapping weight (W) uses the least-square error as a metric. During inference, the trained mapping will be used to rerank the top K similar captions from a pre-collected image-text pair pool, and then use a Large language model to refine the captions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The topic of image captioning is important.\n2. The proposed self-improvement loop is interesting."
                },
                "weaknesses": {
                    "value": "The method presented is not one of image captioning; rather, it involves the enhancement of captions or the retrieval of text related to images. This is predicated on the assumption that there exists a preliminary set of fairly accurate captions for each image from which to draw. To better reflect this and to prevent any potential overstatement or confusion, a slight modification of the title is recommended.\n\nThe utility of the proposed method is constrained by a significant precondition: a repository of captions must be supplied to enable the retrieval process via mapping weights. Typically, we have only the image at our disposal without a pre-existing collection of initial or possible captions to draw from.\n\nThe principal innovation of this study is the introduction of a light linear mapping. However, this comes with two main limitations: (1) such a lightweight approach may be best suited to datasets that are small or of moderate size. The presumption here is that the learned weight \n$W$ is sufficient to bridge the gap, which may not hold true for datasets larger than, for example, COCO\u2014where a lightweight parameter might prove inadequate. (2) The drawback of a light parameterization is the necessity to maintain a substantial pool of captions to ensure coverage for any given image. Hence, the total memory required includes not just $W$ but also this extensive caption repository. Absent this, the method would be unable to perform image captioning autonomously.\n\nThe comparison in Table 1 appears somewhat skewed. The proposed method relies on the availability of a predetermined pool of captions for retrieval, in contrast to the baseline models which do not require such a resource."
                },
                "questions": {
                    "value": "1 How to compare your method to the large multi-modal models, such as llava, instructBLIP, BLIP2, and MiniGpt-4? Any comparison on the image captions generated by those large multi-modal models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699475903619,
            "cdate": 1699475903619,
            "tmdate": 1699636507412,
            "mdate": 1699636507412,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1ZOBZtnD6o",
                "forum": "hgayrNSbri",
                "replyto": "iTSQ4WTFPq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We address the mentioned weaknesses as follows:\n\nWe believe there is a misunderstanding. After the **retrieval of related captions from the training set,** we use a LM (FLAN-T5) to generate a new caption for the input image. As long as the retrieved captions cover different aspects of the input image, the **LM can combine them to a new caption that fits the image.** We make this more clear in the revised version. \n\nAgain, we believe that there is a misunderstanding. **Our method does not rely on any additional data source.** It only retrieves from the training set, whereas other image captioning pipelines are trained end-to-end on the training data. Thus, **training captions are generally available,** which is what we retrieve from during testing. Therefore, we **only require an input image** and no additional data. We try to clarify this in the revised version.  \n\nIn principle, we agree that there is no proof that a linear mapping is sufficient in all situations. However, there are prior works that **optimize a single linear layer on large-scale datasets [1,2].** This indicates that linear mappings are practically sufficient in similar settings. Moreover, these methods require **huge efforts to re-train a system of [1,2]** on new data, while for our method this is feasible within minutes. We agree that the datastore requires additional memory. However, **FAISS allows for compressing the datastore to reduce the memory footprint in case of large datasets.** We adapted the paragraph on the datastore in Section 5 accordingly.\n\nWe believe the **comparison in Table 1 is adequate** since it compares our method to others that aim at **parameter efficiency, i.e. training as few parameters as possible.** All compared methods (including ours) only use the training set for training and evaluate on the test set. Further, **one of our baselines (SmallCap) uses retrieval** and stores the training data in its datastore (same as ReCap).\n\n**Caption quality compared to large-scale models:** We believe this is an unfair comparison, since **all of the mentioned methods train much more parameters on datasets that are orders of magnitude larger** than the ones we use. Further, they usually consist of **two-stage training** and use **language models of much larger size.**\n\n[1] Mini-GPT4: Enhancing vision-language understanding with advanced large language models, Zhu et al., arXiv:1909.11059, 2023\n\n[2] Visual Instruction Tuning, Liu et al., arXiv:2304.08485, 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740089751,
                "cdate": 1700740089751,
                "tmdate": 1700740089751,
                "mdate": 1700740089751,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rt4VBqMC6z",
            "forum": "hgayrNSbri",
            "replyto": "hgayrNSbri",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_RtHk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_RtHk"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets the problem of lightweight captioning by proposing to optimize a linear mapping between the visual and text embedding space. They argue that this is crucial to close the modality gap. What differentiates this work from those tuning such embedding layer via a Cross-entropy loss is that they tune the linear mapping via simple least squares. The authors build a dataset of (image, text) pairs which can then be used to optimize for the linear mapping. One the linear mapping is obtained, they caption an image by first retrieving closest captions from a dataset and prompting these to an of-the-shelf LLM. They also propose a self-improvement phrase where the model is used to generate captions for the training set and then again used to refine the linear mapping. Authors claim similar performance to prior light-weight captioning methods by only tuning the linear layer and that too on a CPU in much less time. They also shows experiments on cross-dataset transfer and also ablation studies on which features to use for optimizing linear mapping. They also highlight an issue with clip-score metric.\n\nOverall the idea seems relevant to the field. However, the method proposed in the paper lacks novelty and the experimental section is also weak."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Addresses an important problem of designing light weight image captioning systems\n- Proposes a training-free (at least no NN is trained) method to align visual and textual space. Argue that this closes the modality gap.\n- Idea of using LLM to summarize the nearest NN captions for an image is interesting (although it can lack grounding esp. when the retrieved captions might be missing the key concept)\n- Show comparable results on 2 datasets. Also, show that the self-improvement phrase results in improvement\n- Highlights an issue with clip-score metric, where it seems to be assigning higher score to hallucinated example"
                },
                "weaknesses": {
                    "value": "- Overall the idea lacks novelty and depth. The key idea of doing retrieval augmented captioning is taken from prior work. Although the paper differs in how the training is being done (just doing least squares instead of training some layer), the performance is mostly below the previous works on most metrics (e.g. on Bleu small cap vs recap is 36 vs 29, numbers are similar on spice metric)\n- Authors have shown experiments on cross-dataset transfer, why didn't they consider other datasets such wizwiz, MSR-VTT.\n- Another issue that I see with this work is that the caption might not be grounded in the image. Based on Fig1, the final caption seems to be a summary of the retrieved captions which might not always be accurate. For example, what happens if the image contains a concept that is not a part of any caption\n- I agree that the finding about clip-score is interesting but it was not adding value to the key idea in the paper"
                },
                "questions": {
                    "value": "Please see weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5139/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5139/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5139/Reviewer_RtHk"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699486667360,
            "cdate": 1699486667360,
            "tmdate": 1699991749106,
            "mdate": 1699991749106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Fz8PRW9Vm",
                "forum": "hgayrNSbri",
                "replyto": "rt4VBqMC6z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We address the mentioned weaknesses as follows:\n\n- We agree that retrieval augmentation has been done in prior works. However, these works require end-to-end training which we alleviate due to the linear mapping to bridge the modality gap. Further, **our self-improvement loop is a novel approach to augment the retrieval datastore with synthetic captions.**\n\n\n- We will add experiments on additional datasets, i.e. VizWiz and MSR-VTT in the future version.\n\n\n- If a concept of an image does not appear in the training captions then it will also not appear in the generated caption. However, **this problem is not specific to our method,** but applies to **any other image captioning pipeline.** The purpose of our linear mapping is to **ground images to  training captions.** In Table 4, it can be seen that without this grounding (ReCap\u207b_Captions) there is a substantial drop in performance. \n\n\n- We believe that our analysis on CLIP-score does add value. The main reason we considered it is that we searched for an appropriate metric to **threshold our synthetic captions in the self-improvement loop.** We observed that **when optimizing for CLIP-score, our method started to hallucinate** which resulted in a decrease in all metrics except for CLIP-score itself.  We make this more clear in our revised version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739826827,
                "cdate": 1700739826827,
                "tmdate": 1700739826827,
                "mdate": 1700739826827,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NR8D4pRnWA",
            "forum": "hgayrNSbri",
            "replyto": "hgayrNSbri",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_rPFf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5139/Reviewer_rPFf"
            ],
            "content": {
                "summary": {
                    "value": "In this research, the authors address the modality gap issue in pre trained vision-language models (VLMs) without the need for costly finetuning. They propose a cost-effective solution using a linear mapping optimized through a least-squares approach, achievable within minutes even on a CPU.  The method also includes an iterative refinement process using synthetic captions from the LM, allowing explicit optimization for image captioning metrics. The results demonstrate competitive performance on MS-COCO and Flickr30k datasets, especially in comparison to lightweight captioning approaches, and highlight the limitations of reference-free metrics such as CLIP-score."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easily comprehensible. However, there is room for improvement in conveying the analysis and intuition behind the discussed concepts.\n\nWhile the paper addresses a crucial issue in the multimodal domain, the proposed solution's persuasiveness could be strengthened.\n\nThe author has presented qualitative and quantitative results across various datasets."
                },
                "weaknesses": {
                    "value": "The author proposes bridging the modality gap through a cost-effective approach using a linear mapping optimized through a least-squares solution. However, there is a need for a more in-depth discussion on how this method differs from existing solutions in the realm of joint models.\n\nThe utilization of a \"linear mapping optimized via a least-squares solution\" is a fundamental constraint optimization technique. This may raise questions about the novelty of the method. To enhance the novelty of the method, the author should delve into a comparative analysis with existing joint model solutions found in the Visual Question Answering (VQA) literature and other multimodal joint representation studies. This discussion would shed light on the distinctive aspects and advancements introduced by the proposed approach.\n\nComputational Complexity: The text mentions that certain computations, such as least-squares linear model fitting, can be done on a CPU within minutes. It would be valuable to provide a brief discussion or estimate of the computational complexity involved in each step, giving readers an idea of the method's efficiency.\n\nIterative Self-Improvement: The iterative self-improvement process is described, but the rationale behind choosing high-scoring captions for augmentation could be clarified. Explaining why high-scoring captions are selected and how this contributes to the refinement process would add depth to the method's justification.\n\nNotation and Terminology: The use of symbols and notation is clear for the most part, but some terms could be defined or explained more explicitly. For instance, it would be helpful to explicitly state what \"W\" represents in the context of the linear model. Additionally, a brief glossary or notation table might aid readers in understanding the symbols used throughout the section. The hyperparameters \"k\" and \"l\" are introduced but not thoroughly explained. Providing more insight into the rationale behind choosing specific values for these hyperparameters or discussing their impact on the results would strengthen the method's transparency.\n\nHowever, I feel that the paper misses one of the core aspects of machine learning practice: readability and reproducibility of results. What core mechanism of the proposed explanation method is not clear here. The author should provide an algorithm or pseudocode to reproduce the results, which this paper misses"
                },
                "questions": {
                    "value": "Please refer to the weakness section for this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699511338246,
            "cdate": 1699511338246,
            "tmdate": 1699636507193,
            "mdate": 1699636507193,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xWxGnt5GHj",
                "forum": "hgayrNSbri",
                "replyto": "NR8D4pRnWA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5139/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We adress the mentioned weaknesses as follows:\n\nWe **extend the related work section** to highlight the **differences to existing approaches to mitigate or avoid the modality gap.**\n\nTo the best of our knowledge we are the **first to efficiently bridge the modality gap for image captioning** which, as opposed to existing approaches, alleviates end-to-end training of the captioning pipeline. We elaborate the **difference to other vision-language approaches** in the related work section. Further, **our self-improvement loop is novel** and provides a way to leverage synthetic captions for retrieval augmentation. Finally, our analysis on metrics provides a new insight that CLIP-score is not reliable for evaluation of image captioning.\n\nWe added asymptotic complexity of the retrieval (O(n)) and the computation of the least squares solution via pseudoinverse (O(d\u00b3)), where d denotes the dimensionality of the CLIP space and n the number of embeddings stored in the datastore. We want to highlight that **neither of these components are a bottleneck of our method.** The **embeddings for the datastore are pre-computed once before training.** As mentioned in Section 3.2, inference time is approximately equal to the one of SmallCap, which, to the best of our knowledge, is the most lightweight approach that currently exists.\n\nWe **threshold synthetic captions** in order to achieve a certain **trade-off between quality and diversity.** We ran additional experiments and found that thresholding according to the CIDEr-D metric works the best. Since CIDEr-D is based on n-grams, a high threshold would enforce captions as close as possible to reference captions, hence no additional information. Lowering the threshold results in more diverse synthetic captions that can be combinations of reference captions for different images.\n\n**W** is a **linear mapping from CLIP image space to the CLIP language space.** The hyperparameter **k** is the number of retrieved captions per image. We show in the appendix (Table 9 and Table 10) that it **slightly affects the downstream captioning performance.** The hyperparameter l is only used during the self-improvement cycle, to generate several **diverse captions.** In our revised version we avoid the need for this hyperparameter by using nucleus sampling.\n\nWe agree that reproducibility is very important, which is why **we added our code in the supplementary material.** Further, to enhance understanding of our method, we **added algorithms for ReCap as well as the self-improvement loop.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739678181,
                "cdate": 1700739678181,
                "tmdate": 1700739678181,
                "mdate": 1700739678181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]