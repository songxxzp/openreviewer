[
    {
        "title": "On the Effect of Batch Size in Byzantine-Robust Distributed Learning"
    },
    {
        "review": {
            "id": "pWJGdGNLnH",
            "forum": "wriKDQqiOQ",
            "replyto": "wriKDQqiOQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4504/Reviewer_tTch"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4504/Reviewer_tTch"
            ],
            "content": {
                "summary": {
                    "value": "This work studies Byzantine-robust distributed learning for the i.i.d. non-convex smooth case. The paper proposes two tricks to improve the existing methods, i.e., large batch size and normalized momentum. The authors provide theoretical arguments to show the benefits of large batch size (variance reduction) and prove the convergence of normalized momentum trick. Empirical experiments show that the combination of these two tricks outperforms existing start-of-the-art methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed normalized momentum shows significant empirical improvement in the small batch size case as shown in table 3 and table 4. \n2. The use of large batch size also significantly boosts the test accuracy under ALIE attack and FoE attack except for KR and CM for the FoE attack cases. \n3. The combination of large batch size and normalized momentum achieves the best empirical performance in nearly every case. \n4. The use of large batch size significantly reduce the wall-clock running time for training fixed number of epochs."
                },
                "weaknesses": {
                    "value": "1. There exist little theoretical improvements regarding to existing BRDL methods in terms of problem assumptions or convergence rates. In addition, this work only considers i.i.d. cases, which is kind of restrictive if not enough theoretical improvements are obtained.\n2. The variance reduction trick using large batch size is a direct consequence of (Karimireddy et al., 2021)), so it is hard to claim that this is one of the main contributions of the current work. Furthermore, the optimization of B is conducted on the upper bounds of the convergence rates, so it does not necessarily leads to faster convergence if we set optimal B. This probably should be made clear in the paper. \n3. The technical elements in proving the convergence of ByzSGDnm are closely related to references such as (Cutkosky & Mehta, 2020), so I am not clear whether there are substantial contributions therein."
                },
                "questions": {
                    "value": "1. SGDm is not defined before first used. \n2. In proposition 2, equation (5), why the last term has a $\\sigma^2$ term, while in the work (Cutkosky & Mehta, 2020), this term is only in $\\sigma$. Can you briefly explain why? \n3. Can you explain why KR fails all cases? Is that because KR does not satisfy the definition 1? \n4. Why does CM have degraded performance in table 4 after increasing batch size, this batch size should be right based on its performance in ByzSGDnm, can you provide some comments on that? \n5. In the comparison with Byz-VR-MARINA, why do you use $512\\times 8$ batch size for ByzSGDnm, but never uses that for Byz-VR-MARINA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Reviewer_tTch"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4504/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705038069,
            "cdate": 1698705038069,
            "tmdate": 1700769049636,
            "mdate": 1700769049636,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t9J9mrD3JD",
                "forum": "wriKDQqiOQ",
                "replyto": "pWJGdGNLnH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (part 2/2)"
                    },
                    "comment": {
                        "value": "**Question 2. In proposition 2, equation (5), why the last term has a $\\sigma^2$ term, while in the work (Cutkosky & Mehta, 2020), this term is only in $\\sigma$. Can you briefly explain why?**\n\nAfter checking our proof and the proof in (Cutkosky & Mehta, 2020), we carefully guess that there might be a typo in the proof of (Cutkosky & Mehta, 2020). It seems that the term $\\frac{8\\sigma\\sqrt{T}}{\\sqrt{RL}}=O(\\sigma)$ in the last step of the proof of Theorem 1 in (Cutkosky & Mehta, 2020) should be $\\frac{8\\sigma^2\\sqrt{T}}{\\sqrt{RL}}=O(\\sigma^2)$ since it is obtained by letting $\\alpha=\\frac{\\sqrt{RL}}{\\sigma\\sqrt{T}}$ in the term $\\frac{8\\sigma}{\\alpha}$.\n\nMeanwhile, we are willing to hear from the reviewer if they have any further comments about this question.\n\n---\n\n**Question 3. Can you explain why KR fails all cases? Is that because KR does not satisfy the definition 1?**\n\nWe would like to point out that in the experiments of previous works (Karimireddy et al., 2021; Xie et al., 2020), KR also failed under FoE attack in all cases. To the best of our knowledge, it is unknown whether KR satisfies Definition 1 or not. Although we do not focus on the performance of a specific aggregator under a specific attack in this work, we are willing to provide some comments on this phenomenon below. \n\nFirstly, the theoretical guarantee for Krum (KR) in (Blanchard et al., 2017) requires that $2f+2<n$ where $f$ is the number of Byzantine workers and $n$ is the number of all workers. The condition is not satisfied in our experiment where $3$ workers among $8$ workers are Byzantine. In addition, we find that when using KR under FoE attack, the model parameter falls into an area with a small gradient norm but a large training loss. Since theoretical results only guarantee a small gradient norm in expectation, the poor empirical performance of KR does not conflict with the theoretical results.\n\n---\n\n**Question 4. Why does CM have degraded performance in table 4 after increasing batch size?**\nAs the detailed results in Appendix E.1 show, [ByzSGDm + CM] has the best performance when $B=128\\times 8$ in this case. When $B<128\\times8$, the performance of [ByzSGDm + CM] improves as $B$ increases. When $B>128\\times8$, the performance of [ByzSGDm + CM] degrades as $B$ increases. It is consistent with our theoretical results and other empirical results.\n\nFor quick access, we present part of the results in Table 10 in the revised version below, which show the top-$1$ test accuracy of different methods when there are $3$ workers under FoE attack.\n\n|Batch size|32$\\times$8 | 64$\\times$8 | 128$\\times$8 | 256$\\times$8 | 512$\\times$8| 1024$\\times$8|\n|----|----|----|----|----|----|----|\n|ByzSGDm + GM | 78.36\\% | 81.98\\% | 82.69\\% | 82.20\\% | **84.09\\%** | 78.90\\% |\n|ByzSGDnm + GM | 88.55\\% | 88.75\\% | **90.99\\%** | 90.23\\% | 89.12\\% | 88.38\\% |\n|ByzSGDm + CM | 83.97\\% | **84.28\\%** | 84.01\\% | 83.48\\% | 79.16\\% | 78.76\\% |\n|ByzSGDnm + CM | 84.12\\% | 84.77\\% | 85.23\\% | **85.74\\%** | 84.65\\% | 83.36\\% |\n|ByzSGDm + CC | 83.60\\% | 84.26\\% | 87.45\\% | **88.48\\%** | 86.24\\% | 81.36\\% |\n|ByzSGDnm + CC | 88.99\\% | 90.07\\% | **90.69\\%** | 90.54\\% | 89.32\\% | 88.20\\% |\n\n---\n\n**Question 5. In the comparison with Byz-VR-MARINA, why do you use batch size $512\\times 8$ for ByzSGDnm, but never uses that for Byz-VR-MARINA?**\n\nThis is mainly because Byz-VR-MARINA adopts the PAGE style, which prefers a relatively small batch size (Li et al., 2021, Gorbunov et al., 2023). Meanwhile, we have added the empirical results of Byz-VR-MARINA when $B=256\\times 8$ and $B=512\\times 8$ in Appendix D.5 in the revised version. The empirical results show that the performance of Byz-VR-MARINA does not improve when the batch size increases to $256\\times 8$ or $512\\times 8$.\n\n---\n\n**Added empirical results for non-i.i.d. cases**\n\nWe have also followed the other reviewers' suggestions and added the empirical results in a heterogeneous setup in Appendix E.2, which we think can improve the contribution of our work. In the added experiments, the training instances on workers are sampled by using a Dirichlet distribution with hyper-parameter $1.0$. \n\nThe empirical results show that under this non-i.i.d. setting, using a relatively large batch size can still lead to higher test accuracy. Moreover, ByzSGDnm with batch size $512\\times 8$ has the best final top-$1$ test accuracy.\nIn addition, we have also tested the empirical performance of the three methods (ByzSGDnm, ByzSGDm, Byz-VR-MARINA) when combined with nearest neighbor mixing (NNM) technique under the non-i.i.d. setting. ByzSGDnm still has the best final top-$1$ test accuracy among the methods. Please refer to Appendix E.2. for more detailed results.\n\n---\n\nWe hope that we have addressed the reviewer's concerns, and we are always willing to answer any further questions. Meanwhile, we would greatly appreciate it if the reviewer could re-evaluate our work in light of our response."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582701894,
                "cdate": 1700582701894,
                "tmdate": 1700582701894,
                "mdate": 1700582701894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x9bBagNuUe",
                "forum": "wriKDQqiOQ",
                "replyto": "pWJGdGNLnH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (part 1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the insightful comments. We have revised our work according to the suggestions and updated the revised version. The changed or added parts are marked in blue for quick recognition. Then, we respond to the raised concerns and questions point by point below.\n\n---\n\n**Weakness 1. There exist little theoretical improvements regarding to existing BRDL methods in terms of problem assumptions or convergence rates.**\n\nWe would like to clarify that the main theoretical contribution of our work lies in the analysis of the effect of batch size in BRDL. Specifically, as far as we know, we are the first to show that a relatively large batch size is preferred in BRDL (please see our response to Weakness 2(a)).\n\nIn addition, we would like to point out that although in the same order, the convergence rate in our work is tighter than those in existing works for SGD-based methods in BRDL, as far as we know. Moreover, it has been shown in (Arjevani et al., 2019) that under Assumptions 1, 2, and 3 (please see Section 2), the convergence order $\\mathbb{E}||\\nabla F(w_T)||\\leq O(1/T^\\frac{1}{4})$ is optimal for SGD in the worst case. Therefore, we carefully think that the improvement by optimizing batch size in our work is meaningful. \n\n---\n\n**Weakness 2(a). The variance reduction trick using large batch size is a direct consequence of (Karimireddy et al., 2021), so it is hard to claim that this is one of the main contributions of the current work.**\n\nWe thank the reviewer for the comment. Actually, we would like to politely clarify that the results of (Karimireddy et al., 2021) do not suggest using large batch sizes. Although increasing batch size has the positive effect of reducing the variance of each iteration, it also has the negative effect of increasing the gradient computation number for each iteration. Therefore, for a given total gradient computation number $\\mathcal{C}$, it is uncertain whether a large or small batch size is better in BRDL based only on the results (Karimireddy et al., 2021) without further analysis. \n\nIn traditional distributed learning (TDL) without attacks, the variance of stochastic gradients will also slow down the convergence. However, relatively small batch size is still preferred in TDL since it allows more iterations and leads to better empirical performance when $\\mathcal{C}$ is fixed (or equivalently, when the total epoch number is fixed). As far as we know, almost all existing works on BRDL  directly follow the setting of small batch sizes in TDL. However, our theoretical results show that a relatively large batch size is preferred in BRDL, which is contrary to that in TDL without attacks. Moreover, our theoretical findings are supported by empirical results.\n\nIn summary, in this work, we have pointed out the improperly inherited setting of small batch size in BRDL by both theoretical analysis and empirical results. We would greatly appreciate it if the reviewer could re-evaluate the contribution of our work in light of our response above.\n\n---\n\n**Weakness 2(b). The optimization of B is conducted on the upper bounds of the convergence rates, so it does not necessarily lead to faster convergence if we set optimal B. This probably should be made clear in the paper.**\n\nWe thank the reviewer for the insightful comment. We have added the following statement at the end of Section 3.1, which is marked in blue.\n\n\"In addition, we would like to clarify that although we obtain a better worst-case guarantee by optimizing $\\mathcal{U}(B)$, it does not necessarily ensure a better empirical performance given the complexity and variety in real-world applications.\"\n\n---\n\n**Weakness 3. The technical elements in proving the convergence of ByzSGDnm are closely related to references such as (Cutkosky & Mehta, 2020), so I am not clear whether there are substantial contributions therein.**\n\nWe would like to point out that the theoretical results in our work cannot be obtained by simply combining the proof in (Cutkosky & Mehta, 2020) and (Karimireddy et al., 2021). Specifically, the term $(\\frac{1}{\\alpha T}+\\sqrt{\\alpha})$ in Theorem 2 cannot be obtained by directly following the proof in (Karimireddy et al., 2021). To overcome the problem, we provide a finer analysis for $\\mathbb{E}||u_t-\\bar{u}_t||^2$ in Lemma 2 (equation (29)). The term $[\\alpha + (1-\\alpha)^{2t}]$ is originally $[\\alpha + (1-\\alpha)^t]$ in Lemma 9 in Appendix E of (Karimireddy et al., 2021).\n\n---\n\n**Question 1. SGDm is not defined before first used.** \n\nWe thank the reviewer for pointing out the undefined abbreviation. It has been modified to 'stochastic gradient descent with momentum (SGDm)' in the revised version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582745070,
                "cdate": 1700582745070,
                "tmdate": 1700582745070,
                "mdate": 1700582745070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MtHMj8zqyW",
                "forum": "wriKDQqiOQ",
                "replyto": "x9bBagNuUe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4504/Reviewer_tTch"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4504/Reviewer_tTch"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewer for detailed answers to my questions and comments. I agree that in terms of the upper bound derived in the paper (Karimireddy et al., 2021), increasing the batch size as the fraction of Byzantine clients grows is beneficial and this is the first time it is discovered. However, I still think that optimizing an existing upper bound does not constitute substantial theoretical contribution, likewise, although the authors provide convergence analysis for the proposed method ByzSGDnm, but there is no theoretical improvements claimed in the paper compared to existing results, such as https://arxiv.org/pdf/2006.09365.pdf by setting heterogeneity to be zero. However, I agree that the large batch size trick and the proposed algorithm perform well in real experiments, and thus I would not object if the AC strongly supports this work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715924524,
                "cdate": 1700715924524,
                "tmdate": 1700715924524,
                "mdate": 1700715924524,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FcudhLo7VM",
            "forum": "wriKDQqiOQ",
            "replyto": "wriKDQqiOQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4504/Reviewer_Rysg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4504/Reviewer_Rysg"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the optimization problem of Byzantine-robust distributed learning in an i.i.d. case. They propose a new method, called Byzantine-robust stochastic gradient descent with normalized momentum. They prove the convergence guarantee for this algorithm and theoretically analyze the optimal value of batch size.  Also, the dependence of the rate on the batch size is studied experimentally."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The presentation of results is written well.\n2. Good deep learning experiments."
                },
                "weaknesses": {
                    "value": "1. In the experimental part of the work, the experiments do not support theoretical analysis. The authors do not compare the performance of the method with optimal batch size value and with another possible choice of it. It would be better if such experiments were in the work. \n2. Also, the experiments with the comparison of convergence rates of the proposed method and previous methods. It would be better to provide some experiments like in this paper https://arxiv.org/pdf/2206.00529.pdf (see Figure 1). \n3. In the work the authors consider a homogeneous setting. Some results in heterogeneous setup can improve the contribution of this work dramatically."
                },
                "questions": {
                    "value": "1. There is no theoretical comparison between ByzSGDnm and ByzSGDm. Are there any theoretical benefits of ByzSGDnm compared to ByzSGDm? \n\n Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Reviewer_Rysg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4504/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827672953,
            "cdate": 1698827672953,
            "tmdate": 1700680763647,
            "mdate": 1700680763647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O40XbcDCIz",
                "forum": "wriKDQqiOQ",
                "replyto": "FcudhLo7VM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4504/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the valuable comments. We have revised our work according to the suggestions and updated the revised version. The changed or added parts are marked in blue for quick recognition. Then, we respond to the raised concerns and questions point by point below.\n\n---\n\n**Weakness 1. The authors do not compare the performance of the method with optimal batch size value and with another possible choice of it.**\n\nSince the constants $L$ and $F_0$ for the ResNet-20 deep learning model on the CIFAR-10 dataset is hard to obtain, we try different batch sizes ranging from $2^5\\times 8$ to $2^{10}\\times 8$ in the experiment. As the empirical results in Table 1 and Table 2 show, for ByzSGDm with each of the four aggregators (KR, GM, CM, and CC), the optimal batch size $B^*$ that leads to the best top-$1$ test accuracy increases as $\\delta$ increases from $0$ to $3/8$. Moreover, the top-$1$ accuracy increases as batch size $B$ increases when $B<B^*$, and decreases as $B$ increases when $B>B^*$. The empirical results are highly consistent with our theoretical results.\n\nSimilar empirical results for ByzSGDnm can be found in Table 7, Table 8, Table 9, Table 10, and Table 11 in Appendix E.1 due to limited space in the main text.\n\n---\n\n**Weakness 2. It would be better to provide some experiments with the comparison of convergence rates of the proposed method and previous methods.**\n\nWe sincerely thank the reviewer for the valuable suggestion. Due to limited space in the main text, we have added more results in the Appendix. Please see Figure 1 to Figure 9 in Appendix D.5 and Appendix E, which illustrate the empirical convergence rate of our method and the existing methods under different settings. The added results in the figures are highly consistent with those in the initial version.\n\n---\n\n**Weakness 3. Some results in heterogeneous setup can improve the contribution of this work dramatically.**\n\nWe thank the reviewer for the constructive suggestion. We have added the empirical results in heterogeneous setup in Appendix E.2. For quick access, we briefly summarize the empirical results below.\n\nWe sample from the training set of CIFAR-10 using the Dirichlet distribution with hyper-parameter $1.0$ to create the heterogeneous data. Among the $8$ workers, $1$ worker is under ALIE attack. Geometric median is used as the aggregator. The final top-$1$ test accuracy for different methods is presented in the table below.\n\n|Batch size|32$\\times$8 | 64$\\times$8 | 128$\\times$8 | 256$\\times$8 | 512$\\times$8|\n|----|----|----|----|----|----|\n|Byz-VR-MARINA $(p=0.05)$ | 23.07\\% | 25.01\\% | 22.25\\% | 24.67\\% |14.28\\% |\n|Byz-VR-MARINA $(p=0.1)$ | 27.16\\% | 23.19\\% | 23.08\\% | 13.86\\% | 11.45\\% |\n|Byz-VR-MARINA $(p=0.2)$ | 17.83\\% | 28.10\\% | 27.34\\% | 18.67\\% | 25.21\\% |\n|ByzSGDm | 10.16\\% | 23.33\\% | 27.78\\% | 29.63\\% | 32.90\\% |\n|ByzSGDnm | 23.87\\% | 26.78\\% | 28.42\\% | 30.04\\% | **35.24\\%**|\n\nAs the empirical results show, under this non-i.i.d. setting, using a relatively large batch size can still lead to higher test accuracy. Moreover, ByzSGDnm with batch size $512\\times 8$ has the best final top-$1$ test accuracy.\n\nIn addition, we have also tested the empirical performance of the three methods (ByzSGDnm, ByzSGDm, Byz-VR-MARINA) when combined with nearest neighbor mixing (NNM) technique under the non-i.i.d. setting. ByzSGDnm still has the best final top-$1$ test accuracy among the methods. Please refer to Appendix E.2. for more detailed results.\n\n---\n**Question 1. There is no theoretical comparison between ByzSGDnm and ByzSGDm. Are there any theoretical benefits of ByzSGDnm compared to ByzSGDm?**\n\nTo the best of our knowledge, the theoretical upper bounds for methods based on SGD (without normalization) in non-convex cases are usually in the form of \"$\\mathbb{E}||\\nabla F(w_t)||^2\\leq ...$\" while the upper bounds for those based on normalized SGD in the form of \"$\\mathbb{E}||\\nabla F(w_t)||\\leq...$\". Therefore, the theoretical upper bound for ByzSGDnm cannot be directly compared to ByzSGDm. As we present in Section 4, the obtained convergence order $O(1/T^\\frac{1}{4})$ for ByzSGDnm is optimal. Moreover, ByzSGDnm can recover the convergence rate of normalized momentum SGD when there is no attack ($\\delta=0$).\n\nMethods based on SGD (without normalization) and those based on normalized SGD have their own applications. Methods based on normalized SGD have been reported to have better empirical performance in large-batch cases (Goyal et al., 2017; Hoffer et al.,2017; Keskar et al., 2017; You et al., 2020). Given the findings in previous works above, we propose ByzSGDnm since our theoretical results show that a relatively large batch size is preferred under Byzantine attacks. \n\n---\nWe hope that we have addressed the reviewer's concerns, and we are always willing to respond to any further concerns. Meanwhile, we would greatly appreciate it if the reviewer could re-evaluate our work in light of our response."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580379542,
                "cdate": 1700580379542,
                "tmdate": 1700580379542,
                "mdate": 1700580379542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "azgNepmw3C",
                "forum": "wriKDQqiOQ",
                "replyto": "O40XbcDCIz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4504/Reviewer_Rysg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4504/Reviewer_Rysg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response! \n\nThe new added experiments are good! However, the experiments, which I have expected to see, might be in the same manner as Figure 1 from paper  https://arxiv.org/pdf/2206.00529.pdf. What I mean is that to support your theoretical results it would be better to compare methods from your experiments but in another metric:  how $||\\nabla F(w_t)||_2$ or $||\\nabla F(w_t)||^2_2$ decrease during iterations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680794918,
                "cdate": 1700680794918,
                "tmdate": 1700680794918,
                "mdate": 1700680794918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R9udhdQvkq",
            "forum": "wriKDQqiOQ",
            "replyto": "wriKDQqiOQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4504/Reviewer_jRhg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4504/Reviewer_jRhg"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors show the effect of batch size on the performance of robust algorithm against Byzantine attacks. More specifically, they characterize the optimal batch size $B^\\star$ to choose when the number of gradient computations is fixed. In addition, they present ByzSGDnm, a robust algorithm that uses stochastic gradient descent with normalized momentum. The authors provide a theoretical guarantee of the algorithm on non-convex functions and provide empirical results showing the efficiency of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clear and easy to follow.\n- Increasing the batch size and normalizing the gradient significantly improve the empirical performance of the model in the i.i.d case."
                },
                "weaknesses": {
                    "value": "- The proposed algorithm is only studied in a homogeneous setting (\"i.i.d. case\" in the paper), which is generally not the case in real applications where data between different clients are heterogeneous. Does the author have any ideas or perhaps experimental results on the behavior of the proposed algorithm in the presence of heterogeneous data?"
                },
                "questions": {
                    "value": "Table 6 shows the execution time of the different algorithms for different batch sizes, for a specific and fixed number of epochs. Can the authors explain why they chose to show the results for a fixed number of epochs and not for a specific accuracy achieved? As far as I know, the speed gain obtained by choosing a larger batch size is naturally explained by the fact that we can benefit from the parallelization of computations (in the system side). However, I would find it interesting to understand whether the methods presented are faster to reach a given accuracy with a larger batch size."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4504/Reviewer_jRhg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4504/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839430733,
            "cdate": 1698839430733,
            "tmdate": 1699636426395,
            "mdate": 1699636426395,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H0RQPcoQ40",
                "forum": "wriKDQqiOQ",
                "replyto": "R9udhdQvkq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4504/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the constructive comments and the support of our work. We have revised our work according to the suggestions and updated the revised version. The changed or added parts are marked in blue for quick recognition. Then, we respond to the raised concerns and questions point by point below.\n\n**Weakness 1: Does the author have any ideas or perhaps experimental results on the behavior of the proposed algorithm in the presence of heterogeneous data?**\n\nWe thank the reviewer for the constructive suggestion. We have added the empirical results in the presence of heterogeneous data in Appendix E.2. For quick access, we briefly summarize the empirical results below.\n\nWe sample from the training set of CIFAR-10 using the Dirichlet distribution with hyper-parameter $1.0$ to create the heterogeneous data. Among the $8$ workers, $1$ worker is under ALIE attack. Geometric median is used as the aggregator. The final top-$1$ test accuracy for different methods is presented in the table below.\n\n|Batch size|32$\\times$8 | 64$\\times$8 | 128$\\times$8 | 256$\\times$8 | 512$\\times$8|\n|----|----|----|----|----|----|\n|Byz-VR-MARINA $(p=0.05)$ | 23.07\\% | 25.01\\% | 22.25\\% | 24.67\\% |14.28\\% |\n|Byz-VR-MARINA $(p=0.1)$ | 27.16\\% | 23.19\\% | 23.08\\% | 13.86\\% | 11.45\\% |\n|Byz-VR-MARINA $(p=0.2)$ | 17.83\\% | 28.10\\% | 27.34\\% | 18.67\\% | 25.21\\% |\n|ByzSGDm | 10.16\\% | 23.33\\% | 27.78\\% | 29.63\\% | 32.90\\% |\n|ByzSGDnm | 23.87\\% | 26.78\\% | 28.42\\% | 30.04\\% | **35.24\\%**|\n\nAs the empirical results show, under this non-i.i.d. setting, using a relatively large batch size can still lead to higher test accuracy. Moreover, ByzSGDnm with batch size $512\\times 8$ has the best final top-$1$ test accuracy.\n\nIn addition, we have also tested the empirical performance of the three methods (ByzSGDnm, ByzSGDm, Byz-VR-MARINA) when combined with nearest neighbor mixing (NNM) technique under the non-i.i.d. setting. ByzSGDnm still has the best final top-$1$ test accuracy among the methods. Please refer to Appendix E.2. for more detailed results.\n\n**Question 1: Can the authors explain why they chose to show the results for a fixed number of epochs and not for a specific accuracy achieved?**\n\nWe show the results for a fixed number of epochs in order to support the claim that using a relatively large batch size can more efficiently utilize the computation power of GPUs. In other words, we want to empirically verify that using a relatively large batch size can make more gradient computation finished in a unit of time.\n\nWe have followed the reviewer's suggestion and presented the wall-clock running time that different methods require to reach 50\\%, 75\\%, and 85\\% top-$1$ test accuracy in Appendix E.1. The results show that ByzSGDnm requires less time to reach the target accuracy than ByzSGDm in most cases. Meanwhile, using a relatively large batch size can make ByzSGDm and ByzSGDnm reach the given accuracy faster. For quick access, we present the wall-clock running time required to reach 75\\% top-$1$ test accuracy below. The missing values mean that the target test accuracy is not reached in $160$ epochs for the corresponding methods. \n\n|Batch size|32$\\times$8 | 64$\\times$8 | 128$\\times$8 | 256$\\times$8 | 512$\\times$8|1024$\\times$8|\n|----|----|----|----|----|----|----|\n|ByzSGDm + KR | -- | -- | -- | -- | **133.69s** | 148.70s|\n|ByzSGDnm + KR | -- | -- | 328.49s | **123.12s** | 130.45s | 134.09s|\n|ByzSGDm + GM | -- | -- | 114.38s | **59.19s** | 90.74s | 124.03s|\n|ByzSGDnm + GM | -- | 252.19s | 66.13s | 54.12s | **45.07s** | 76.49s|\n|ByzSGDm + CM | -- | -- | -- | 202.97s | **110.25s** | 204.42s|\n|ByzSGDnm + CM | -- | -- | 322.83s | 150.25s | **63.85s** | 109.23s|\n|ByzSGDm + CC | -- | 727.07s | 77.19s | 87.42s | **86.53s** | 150.59s|\n|ByzSGDnm + CC | 1465.07s | 112.49s | 109.18s | **56.20s** | 61.30s | 81.28s|\n\n---\n\nWe sincerely thank the reviewer for their valuable time and their support of our work again. Meanwhile, we would greatly appreciate it if the reviewer could re-evaluate our work in light of our response."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579676465,
                "cdate": 1700579676465,
                "tmdate": 1700579676465,
                "mdate": 1700579676465,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]