[
    {
        "title": "Training-time Neuron Alignment for Improving Linear Mode Connectivity and Model Fusion"
    },
    {
        "review": {
            "id": "TbvppnkP3L",
            "forum": "VgPmCLQke7",
            "replyto": "VgPmCLQke7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1333/Reviewer_kfmS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1333/Reviewer_kfmS"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the problem linear mode connectivity(LMC), starting from the assumption that the optimal network weights lie in a lower dimensional subspace. To measure LMC they use the commonly used Accuracy Barrier as a metric. They study the setting where a subset of fixed network weights is frozen during training, which leads to better LMC compared to vanilla training for the case of a single model as well as the case of multi-model fusion.\nThey also consider the setting of federated learning where at each time step the server freezes a subset of networks weights of all the clients."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method seems to outperform vanilla pruning  in Figure 2\n- In the setting of federated learning the proposed have similar performance to previous baselines and and some cases even has better performance"
                },
                "weaknesses": {
                    "value": "- It is a bit hard to judge from the experimental results for what networks/settings one would prefer this method over other methods\n- Lack of experiments at scale (e.g. ImageNet)"
                },
                "questions": {
                    "value": "- The method is conjectured to perform better for wider networks and worse for deeper networks. It would be interesting to have a more quantitative result  i.e. plot the performance for varying width and depth.\n- For the setting of a single network and multi-model fusion why is there only a comparison to vanilla training and not other methods for improving LMC?\n- Why is the accuracy not reported in Figure 1?\n- Could the authors elaborate a bit how pruning for mask ratio 0.7 in Figure 2 causes the model to be as bad as random guessing? Do they expect similar behavior for larger datasets and models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1333/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1333/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1333/Reviewer_kfmS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681662531,
            "cdate": 1698681662531,
            "tmdate": 1699636060685,
            "mdate": 1699636060685,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jxdkufKaxW",
                "forum": "VgPmCLQke7",
                "replyto": "TbvppnkP3L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kfmS (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the valuable reviews. We accordingly respond to the reviewer's comments below.\n\n> 1. Response to \"It is a bit hard to judge from the experimental results for what networks/settings one would prefer this method over other methods.\"\n> \n\nThank you for the comment. We would like clarify that our main contribution may lie in the new perspective and insights regarding improving training-time linear mode connectivity (instead of the post-training). The TNA-PFN is just a showcase and the verification of our hypothesis/perspective. \n\nMoreover, TNA-PFN has practical implications in the following aspects: \n- a. _Enable efficient re-basin and improve re-basin._ We note that in some cases, TNA-PFN + weight matching (WM) can achieve even better interpolated generalization compared with WM alone (Table 1 of the paper: MLP_h2_w200 for CIFAR-10 and MLP_h5_w200 for MNIST); also, it costs less computation of WM. \n- b. _Extensions to federated learning._ We extend TNA-PFN in federated learning to improve the generalization of the global model after model fusion. The proposed FedPFN and FedPNU are light-weight and effective.\n\n> 2. Response to \"Lack of experiments at scale (e.g. ImageNet).\"\n> \n\nThanks for this helpful comment. Due to the time constraint of rebuttal period, we may not be able to implement the experiments on full ImageNet, as an alternative, we have conducted experiments on Tiny ImageNet [2], a subset of ImageNet.\n\nTiny ImageNet contains 100000 64\u00d764 colored images of 200 classes (500 for each class), which can be regarded as more large-scale. The results are in the following Table A, also added to the revised paper. It is observed that TNA-PFN can also reduce the barriers on Tiny ImageNet by up to 23%.\n\n**Table A. Linear mode connectivity on Tiny ImageNet.** The $\\rho$ for CNN is 0.4 and the $\\rho$ for ResNet18 is 0.3. The learning rate is 0.08.\n\n| **Models** | **Metrics**    | **TNA-PFN**                        | **Vanilla Train** |\n|------------|----------------|------------------------------------|-------------------|\n| CNN        | Avg. Acc.      | $11.4\\pm0.6$                   | $9.85\\pm0.3$   |\n|            | Interp. Acc.   | $2.91\\pm0.9$                   | $1.4\\pm0.2$     |\n|            | Acc. Barrier   | $0.75\\pm0.07$ (12.8%\u2193)         | $0.86\\pm 0.03$  |\n|            | Loss Barrier   | $0.75\\pm0.09$ (10.4%\u2193)         | $0.84\\pm0.08$   |\n| ResNet20   | Avg. Acc.      | $31.6\\pm0.4$                   | $31.8\\pm0.3$    |\n|            | Interp. Acc.   | $12.5\\pm2.1$                   | $6.86\\pm1.8$   |\n|            | Acc. Barrier   | $0.60\\pm0.07$ (23%\u2193)           | $0.78\\pm0.06$   |\n|            | Loss Barrier   | $1.2\\pm0.09$ (22.2%\u2193)          | $1.6\\pm0.2$     |\n\n> 3. Response to \"The method is conjectured to perform better for wider networks and worse for deeper networks. It would be interesting to have a more quantitative result i.e. plot the performance for varying width and depth.\"\n> \n\nThank you for the comment. Acutally, in Figures 3 and 8 of the initial submission, we have conducted experiments regarding the performances for varying width and depth. It seems that our method works well under different depths and widths."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323912141,
                "cdate": 1700323912141,
                "tmdate": 1700323912141,
                "mdate": 1700323912141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bs21suOFcY",
                "forum": "VgPmCLQke7",
                "replyto": "TbvppnkP3L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kfmS (2/2)"
                    },
                    "comment": {
                        "value": "> 4. Response to \"For the setting of a single network and multi-model fusion why is there only a comparison to vanilla training and not other methods for improving LMC?\"\n> \n\nSince it might be the first paper to improve linear mode connectivity (LMC) from the training time and previous methods are post-training, other methods are orthogonal to ours, as a result, we only compare with the vanilla training. But we have included experiments of previous post-hoc matching methods in Table 1 of the paper, showing our method is compatible with other post-hoc methods and can have advantages in efficiency.\n\n> 5. Response to \"Why is the accuracy not reported in Figure 1?\"\n> \n\nThank you for the comment. We note that Figure 1 is just a toy illustration of how TNA-PFN might work, and it is not an experimental result, therefore, no accuracy is needed in Figure 1.\n\n> 6. Response to \"Could the authors elaborate a bit how pruning for mask ratio 0.7 in Figure 2 causes the model to be as bad as random guessing? Do they expect similar behavior for larger datasets and models?\"\n> \n\nThanks for the comment. For more datasets and models, the results can be found in Figure 11 of the appendix. Also, more results and similar observations can be found in [1].\n\nPruning sets the weights to zeros, and a high pruning ratio can lead to the deaths of a substantial proportion of neurons, missing important data features. Using the first layer as an example, setting half of the input weights to zero is akin to losing half of the initial data features. However, in the case of TNA-PFN, by fixing these weights, the features remain intact and are conveyed to the second layer. Consequently, excessive pruning can degrade the model's performance to the level of random guessing.\n\n\n---\n[1] Fladmark E, Sajjad M H, Justesen L B. Exploring the Performance of Pruning Methods in Neural Networks: An Empirical Study of the Lottery Ticket Hypothesis[J]. arXiv preprint arXiv:2303.15479, 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323961660,
                "cdate": 1700323961660,
                "tmdate": 1700323961660,
                "mdate": 1700323961660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CNTy6V891r",
                "forum": "VgPmCLQke7",
                "replyto": "TbvppnkP3L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer kfmS: hoping that our response could address your concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer kfmS,\n\nMany thanks for your valuable comments. We have faithfully given detailed responses to your concerns during the rebuttal.\n\nSpecifically, we address the following points:\n\n- We have conducted experiments at scale, on a subset of ImageNet, showing that TNA-PFN is also effective in reducing the barriers.\n- We have detailedly answered some questions raised by the reviewer.\n\nWe would appreciate it if you could let us know if our response has sufficiently addressed your questions and thus kindly reconsider your score.\n\nThank you.\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646528897,
                "cdate": 1700646528897,
                "tmdate": 1700646528897,
                "mdate": 1700646528897,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aqJ0xg8HQU",
            "forum": "VgPmCLQke7",
            "replyto": "VgPmCLQke7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1333/Reviewer_M3kr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1333/Reviewer_M3kr"
            ],
            "content": {
                "summary": {
                    "value": "When training a neural network with SGD, different solutions in the parameter space can be obtained. The linear connection between two different solutions is called Linear Mode Connectivity (LMC). One commonly used approach to achieve LMC is to align the neurons of two network parameters through parameter permutation after training so that they are in the same loss basin. However, the number of permutation matrices is very large, and it requires a lot of computation because it is post-hoc. Therefore, the authors propose Training-time Neuron Alignment by Partially Fixing Neurons (TNA-PFN), which can align neurons at training time to create LMC. TNA-PFN is a method that learns in the parameter subspace by fixing part of the network parameters as initialization, based on the hypothesis that learning a network in an effective subspace with less permutation symmetry can lower the LMC barrier between trained networks. The authors support this hypothesis through both theoretical and experimental results. Also, the authors propose two algorithms, FedPFN and FedPNU, which adapt TNA-PFN to federate learning, and show that they have the potential to improve model fusion on different datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think the strengths are the simplicity of the method, the theoretical support, and the clean writing.\n- The paper presents a new method to achieve LMC, and the idea of fixing a parameter to reduce permutation symmetry is novel.\n- The paper is well written and easy to follow.\n- The hypothesis and theoretical explanation are convincing.\n- Experiments are extensive and show that TNA-PFN works in some practical applications.\n- The paper proposes FedPFN and FedPNU, which can be used practically in Federated Learning, and shows that they work well in practice."
                },
                "weaknesses": {
                    "value": "The biggest weakness seems to be the experimental results. The results are not promising enough to accept that TNA-PFN works effectively.\n\n- As mentioned in the paper, the MNIST results in the second plot of Figure 3 and the experimental results of Entezari, et al. (2022) show that the barrier actually decreases as the network width increases. According to the theorem presented in the paper, the barrier may decrease as the learned dimension decreases, but it does not apply well to these experiments.\n- In many experiments, the absolute barrier is too high when using TNA-PFN alone to be an LMC. Of course, it lowers the barrier compared to the vanilla train with no training, but many experiments show a non-negligible barrier.\n- There is almost no difference in LMC performance between weight matching (WM) after training with TNA-PFN and directly using weight matching. Weight matching after TNA-PFN requires a few fewer iterations, but I don't know how much of a cost savings this provides. From a practical perspective, it introduces an additional hyperparameter, the mask ratio $\\rho$, so I'm not sure how much benefit there is compared to the cost of tuning it.\n- It's good to have a variety of experiments, but there are no experiments on large datasets like ImageNet. It would have been nice to see some experiments on larger datasets, as they generally have different characteristics than MNIST and CIFAR10.\n---\n**Entezari, et al.** [The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks](https://openreview.net/pdf?id=dNigytemkL). *ICLR*, 2022"
                },
                "questions": {
                    "value": "- In the experiments in the paper, the mask ratio was set to 0.4; was this an experimentally tuned value, or is there some underlying theoretical basis?\n- Which of the three algorithms presented in Ainsworth et al. (2023) was used for weight matching (WM)? What is the approximate computational cost per iteration?\n- Do FedPFN or FedPNU work in general learning situations other than federated learning, and can they be used for model ensembling?\n- I'm not sure why the LoRA section was added to Appendix C.1 and what it is trying to say.\n---\n**Ainsworth et al.** [Git Re-Basin: Merging Models modulo Permutation Symmetries](https://openreview.net/forum?id=CQsmMYmlP5T). *ICLR*, 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1333/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1333/Reviewer_M3kr",
                        "ICLR.cc/2024/Conference/Submission1333/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698693109830,
            "cdate": 1698693109830,
            "tmdate": 1700538197925,
            "mdate": 1700538197925,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5Nq7viIN6g",
                "forum": "VgPmCLQke7",
                "replyto": "aqJ0xg8HQU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M3kr (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the valuable reviews. We accordingly respond to the reviewer's comments below.\n\n> 1. Response to \"The biggest weakness seems to be the experimental results. The results are not promising enough to accept that TNA-PFN works effectively.\"\n> \n\nThanks for this comment. We would like to kindly clarify our contributions. \n- **1)** *New insights into linear mode connectivity and permutation invariance from training time.* Our novelty lies in the **training-time** \"subspace hypothesis\" perspective while previous works only focus on **post-training** connectivity via permutations symmetries (re-basin). The proposed TNA-PFN is just a showcase and verification of the hypothesis/perspective. The contributions are alike [1] in that its main contribution is the hypothesis of linear mode connectivity and permutation invariance and the simulated annealing algorithm (which may not be very effective) is just the verification. We believe our new perspective can inspire more future work in the community. \n- **2)** *The applications of training-time neuron alignment in federated learning.* While linear mode connectivity is somewhat theoretical, we propose two algorithms in federated learning to showcase how improving connectivity during training can help practices. The proposed methods are lightweight and effective.\n\n\n> 2. Response to \"As mentioned in the paper, the MNIST results in the second plot of Figure 3 and the experimental results of Entezari, et al. (2022) show that the barrier actually decreases as the network width increases. According to the theorem presented in the paper, the barrier may decrease as the learned dimension decreases, but it does not apply well to these experiments.\"\n> \n\nWe apologize that the previous theorem may cause potential misleadings and we have refined the theorem to make it more solid. \n\n- The previous theorem may cause a misleading that lowering the network dimension by changing the network architecture can improve linear mode connectivity. Actually, this is not what we try to convey and is also inconsistent with the empirical observations. \n- After our revision, the revised theorem is more aligned with our hypothesis and method. It is proved that given a network architecture (without changing the dimension, i.e., width and depth), if we fix a proportion of parameters and train the remaining (like what TNA-PFN does), the linear mode connectivity of two models after training would be improved. \n- The revised theorem is Theorem 3.2 in blue on page 3, and the detailed proof is in the appendix. We hope the improved theorem can relieve the concerns and the potential misleading.\n\n> 3. Response to \"In many experiments, the absolute barrier is too high when using TNA-PFN alone to be an LMC. Of course, it lowers the barrier compared to the vanilla train with no training, but many experiments show a non-negligible barrier.\"\n> \n\nThanks for your comment. Actually, we didn't claim that we have completely _eliminated_ the barriers, and what we said is we largely _decreased_ the barriers. To the best of our knowledge, we might be the first to attempt to discover the potential of improving linear mode connectivity (LMC) during training, which is very difficult and challenging. \n\nSGD randomness results in SGD noise, and as a consequence, causes an LMC barrier after training. We make the preliminary attempt to fix a proportion of weights to reduce the permutation effects of SGD noise. However, it is impossible to eliminate the SGD noise when the model is independently trained, and as long as SGD noise remains, the LMC barrier will not be completely eliminated during training. \n\nWe note even if the barrier may still exist after applying TNA-PFN, we have already achieved up to 70% reduction during training time, and it is promising enough for promoting the theoretical and empirical studies."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323719459,
                "cdate": 1700323719459,
                "tmdate": 1700554839645,
                "mdate": 1700554839645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZD3SvuJDc3",
                "forum": "VgPmCLQke7",
                "replyto": "aqJ0xg8HQU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M3kr (2/3)"
                    },
                    "comment": {
                        "value": "> 4. Response to \"There is almost no difference in LMC performance between weight matching (WM) after training with TNA-PFN and directly using weight matching. Weight matching after TNA-PFN requires a few fewer iterations, but I don't know how much of a cost savings this provides. From a practical perspective, it introduces an additional hyperparameter, the mask ratio $\\rho$, so I'm not sure how much benefit there is compared to the cost of tuning it.\"\n> \n\nWe note that in some cases, TNA-PFN + weight matching (WM) can achieve even better interpolated generalization compared with WM alone (Table 1 of the paper: MLP_h2_w200 for CIFAR-10 and MLP_h5_w200 for MNIST); also, it costs less computation of WM. \n\nFor the hyperparameter $\\rho$, it can be seen from Figures 1 and 11 that TNA-PFN is not sensitive in terms of $\\rho$. TNA-PFN could be effetive to improve the interpolated accuracy under a wide range of $\\rho$, mainly from 0.1 to 0.6. Therefore, tuning $\\rho$ may be not essential and the cost is marginal. \n\n> 5. Response to \"It's good to have a variety of experiments, but there are no experiments on large datasets like ImageNet. It would have been nice to see some experiments on larger datasets, as they generally have different characteristics than MNIST and CIFAR10.\"\n> \n\nThanks for this helpful comment. Due to the time constraint of rebuttal period, we may not be able to implement the experiments on full ImageNet, as an alternative, we have conducted experiments on Tiny ImageNet [2], a subset of ImageNet.\n\nTiny ImageNet contains 100000 64\u00d764 colored images of 200 classes (500 for each class), which can be regarded as more large-scale. The results are in the following Table A, also added to the revised paper. It is observed that TNA-PFN can also reduce the barriers on Tiny ImageNet by up to 23%.\n\n**Table A. Linear mode connectivity on Tiny ImageNet.** The $\\rho$ for CNN is 0.4 and the $\\rho$ for ResNet18 is 0.3. The learning rate is 0.08.\n\n| **Models** | **Metrics**    | **TNA-PFN**                        | **Vanilla Train** |\n|------------|----------------|------------------------------------|-------------------|\n| CNN        | Avg. Acc.      | $11.4\\pm0.6$                   | $9.85\\pm0.3$   |\n|            | Interp. Acc.   | $2.91\\pm0.9$                   | $1.4\\pm0.2$     |\n|            | Acc. Barrier   | $0.75\\pm0.07$ (12.8%\u2193)         | $0.86\\pm 0.03$  |\n|            | Loss Barrier   | $0.75\\pm0.09$ (10.4%\u2193)         | $0.84\\pm0.08$   |\n| ResNet20   | Avg. Acc.      | $31.6\\pm0.4$                   | $31.8\\pm0.3$    |\n|            | Interp. Acc.   | $12.5\\pm2.1$                   | $6.86\\pm1.8$   |\n|            | Acc. Barrier   | $0.60\\pm0.07$ (23%\u2193)           | $0.78\\pm0.06$   |\n|            | Loss Barrier   | $1.2\\pm0.09$ (22.2%\u2193)          | $1.6\\pm0.2$     |\n\n\n> 6. Response to \"In the experiments in the paper, the mask ratio was set to 0.4; was this an experimentally tuned value, or is there some underlying theoretical basis?\"\n> \n\nIt is an experimentally appropriate value. It can be seen from Figures 1 and 11 that TNA-PFN is not sensitive in terms of $\\rho$. TNA-PFN could be effetive to improve the interpolated accuracy under a wide range of $\\rho$, mainly from 0.1 to 0.6. As $\\rho$ is not sensitive, so we use the value of 0.4 which is moderate to balance the accuracy-connectivity tradeoff in most cases."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323766013,
                "cdate": 1700323766013,
                "tmdate": 1700323766013,
                "mdate": 1700323766013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HXe0TEkGRF",
                "forum": "VgPmCLQke7",
                "replyto": "aqJ0xg8HQU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M3kr (3/3)"
                    },
                    "comment": {
                        "value": "> 7. Response to \"Which of the three algorithms presented in Ainsworth et al. (2023) was used for weight matching (WM)? What is the approximate computational cost per iteration?\"\n> \n\nIn Ainsworth et al. (2023) [3], they proposed three algorithms, activation matching, weight matching (WM), and straight-through estimators matching. The one we used is actually weight matching, which is data-independent. The computational cost of WM relies on the model's parameters, more parameters will cost more computation.\n\n> 8. Response to \"Do FedPFN or FedPNU work in general learning situations other than federated learning, and can they be used for model ensembling?\"\n> \n\nWe appreciate the intersting comment and have conducted a preliminary experiment on model ensembling. For two indepedently trained networks, we validate the a) _Avg. Acc._: averaged accuracy of two independent models; b) _Ensemble Two_: accuracy of ensembling the two models; c) _Ensemble Four_: accuracy of ensembling the two models and two linearly interpolated models (fusion $\\alpha \\in \\{0.3, 0.7\\}$). The results are shown in the following Table B. It is found that ensembling linearly interpolated models for TNA-PFN can improve the generlization but the advantage might by marginal. We reckon it could be an interesting future research direction that studies how training-time neuron alignment improves model ensembling.\n\n**Table B. Model ensembling experiments.** The model is CNN and the dataset is CIFAR-10.\n\n|  **Metrics**    | **TNA-PFN**                        | **Vanilla Train** |\n|----------------|------------------------------------|-------------------|\n|  Avg. Acc.     | $65.10\\pm0.57$         | $62.78\\pm0.90$  |\n| Ensemble Two   | $70.45\\pm0.12$         | $68.21\\pm0.82$  |\n| Ensemble Four  | $70.53\\pm0.17$         | $68.33\\pm0.79$  |\n\n> 9. Response to \"I'm not sure why the LoRA section was added to Appendix C.1 and what it is trying to say.\"\n> \n\nThanks for the comment. It may show a potential way to verify our \"subspace hypothesis\" for improving training-time neuron alignment, but further investigations on the equivalent learning rates across LoRAs with different ranks are needed, so it is out of the paper's scope and we appreciate it as future work.\n\n---\n[1] Entezari R, Sedghi H, Saukh O, et al. The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks[C]//International Conference on Learning Representations. 2022.\n\n[2] Tiny ImageNet. A subset of the ImageNet dataset. Available at: https://tiny-imagenet.herokuapp.com/. \n\n[3] Ainsworth S, Hayase J, Srinivasa S. Git Re-Basin: Merging Models modulo Permutation Symmetries[C]//The Eleventh International Conference on Learning Representations. 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323805438,
                "cdate": 1700323805438,
                "tmdate": 1700323805438,
                "mdate": 1700323805438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SlBh40HNbB",
                "forum": "VgPmCLQke7",
                "replyto": "HXe0TEkGRF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Reviewer_M3kr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Reviewer_M3kr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for detailed answers and additional experiments.\nHowever, my concerns were not fully resolved, for the following reasons.\n\n1. As mentioned in the original review, TNA-PFN is not a practical method and does not give much advantage over other LMC-related methods.\n2. The authors claim that the \"subspace hypothesis\" itself is novel in their answer, but I think it lacks novelty alone: the hypothesis itself is somewhat inferable from previous research results (Frankle, et al.), and the experiment is too simple to say that it validates it.\n\nI think the overall approach is impressive, and I look forward to future work with it.\nIn conclusion, I decided to keep my score.\n\n---\n**Frankle, et al.** [Linear mode connectivity and the lottery ticket hypothesis](https://arxiv.org/abs/1912.05671). _ICML_, 2020"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573877265,
                "cdate": 1700573877265,
                "tmdate": 1700573877265,
                "mdate": 1700573877265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AK9BvocnDe",
            "forum": "VgPmCLQke7",
            "replyto": "VgPmCLQke7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1333/Reviewer_xuvq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1333/Reviewer_xuvq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a subspace algorithm aimed at enhancing Linear Mode Connectivity (LMC) during the training. They begin by establishing a preliminary theorem to explore the possibility of LMC improvement through the reduction of the search space. Building on the insights gained from this theorem, they proposed a mask-based training scheme. To empirically validate the efficacy of their proposed methods, the authors conduct numerical experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies an important problem that is how to improve the LCM during the training. LCM is significant in terms of training dynamics and generation and model fusion. The paper is well written and without so many typo and errors. It is easy to follow and read."
                },
                "weaknesses": {
                    "value": "I have a few concerns about the novelty and results of this paper. Firstly, the authors propose a mask-based training scheme, which has already been explored in the literature. While there are variations in the masking details, such as random weight initialization for untrained weights, the overall approach resembles dropout regularization, where different subsets of neurons are masked during each iteration.\n\nFurthermore, the results presented in the paper strike me as somewhat expected. The authors themselves acknowledge in Theorem 3.2 that improving Linear Mode Connectivity (LMC) is achieved by reducing the number of neurons, denoted by $d$. This essentially means using a simpler model, and the neural network is not necessarily overparameterized. Consequently, the number of minima is reduced, leading to a reduction in barriers between different solutions."
                },
                "questions": {
                    "value": "1. Assume weight matrix $W$ are randomly initialized as IID standard Gaussian, which is a common practice in neural network initialization. Then $W$ follows a Matrix Normal distribution, i.e., $W\\sim MN(0, I, I)$. By applying permutation matrices $P$ and $Q$ to $W$, the result distribution has the same Matrix Normal form, i.e., $PWQ \\sim MN(0, I, I)$. Hence, the permutated weight matrix maintains the same distribution as the original $W$. This suggests that permuting the weight matrix is equivalent to reinitializing it. Given this equivalence, it raises the question of how to consider all permutations, given that the search space is uncountable.\n2. The \"mask ratio\" is not precisely defined, and it seems to be related to the ratio of untrained neurons to the total number of neurons. \n3. Is it possible to express the results in Theorem 3.2 in terms of \"loss barrier\" or \"accuracy barrier\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1333/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1333/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1333/Reviewer_xuvq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698948797701,
            "cdate": 1698948797701,
            "tmdate": 1700668324837,
            "mdate": 1700668324837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TElp4jLduN",
                "forum": "VgPmCLQke7",
                "replyto": "AK9BvocnDe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xuvq (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the valuable reviews. We accordingly respond to the reviewer's comments below.\n\n> 1. Response to \"I have a few concerns about the novelty and results of this paper. Firstly, the authors propose a mask-based training scheme, which has already been explored in the literature. While there are variations in the masking details, such as random weight initialization for untrained weights, the overall approach resembles dropout regularization, where different subsets of neurons are masked during each iteration.\"\n> \n\nThanks for this comment. We would like to kindly clarify our contributions. \n- **1)** *New insights into linear mode connectivity and permutation invariance from training time.* Our novelty lies in the **training-time** \"subspace hypothesis\" perspective while previous works only focus on **post-training** connectivity via permutations symmetries (re-basin). The proposed TNA-PFN is just a showcase and verification of the hypothesis/perspective. The contributions are alike [1] in that its main contribution is the hypothesis of linear mode connectivity and permutation invariance and the simulated annealing algorithm (which may not be very effective) is just the verification. We believe our new perspective can inspire more future work in the community. \n- **2)** *The applications of training-time neuron alignment in federated learning.* While linear mode connectivity is somewhat theoretical, we propose two algorithms in federated learning to showcase how improving connectivity during training can help practices. The proposed methods are lightweight and effective.\n\nThe mask-based methods are previously studied in the literature but none of them have been discovered to improve the linear mode connectivity during training, and that's where our new insights come from. We deem that new insights, new perspectives, and new findings can also be viewed as novelty. Also, as claimed in \"Discussion on gradient/model masks\" on page 4, we have elaborated on the differences between our mask and the previous masks, and it is clear that they are algorithmically different.\n\nThe reviewer mentioned that our method resembles dropout, but we think this might not be accurate. \n- Dropout means pruning some weights/neurons during training, however, in Figure 2, we have shown that our TNA-PFN is different from pruning and may have better results. \n- But we need to acknowledge that TNA-PFN shares similar regularization effects with dropout/pruning, that it can improve generalization in some cases but may reduce the performances if the mask ratio is too high.\n- Another difference between dropout and our method lies in the applications of federated learning. Dropout makes the neurons dead, whereas our masking methods enable the neurons/weights to be deactivated in one round and activated in another round, which improves training-time connectivity without hurting the structure of networks.\n\n\n> 2. Response to \"Furthermore, the results presented in the paper strike me as somewhat expected. The authors themselves acknowledge in Theorem 3.2 that improving Linear Mode Connectivity (LMC) is achieved by reducing the number of neurons, denoted by $d$. This essentially means using a simpler model, and the neural network is not necessarily overparameterized. Consequently, the number of minima is reduced, leading to a reduction in barriers between different solutions.\"\n> \n\nThanks for this helpful comment. We realize that previous theorem may cause potential misleadings and we have refined the theorem to make it more solid. \n\n- The previous theorem may cause a misleading that lowering the network dimension by changing the network architecture can improve linear mode connectivity. Actually, this is not what we try to convey and is also inconsistent with the empirical observations. \n- After our revision, the revised theorem is more aligned with our hypothesis and method. It is proved that given a network architecture (without changing the dimension, i.e., width and depth), if we fix a proportion of parameters and train the remaining (like what TNA-PFN does), the linear mode connectivity of two models after training would be improved. \n- The revised theorem is Theorem 3.2 in blue on page 3, and the detailed proof is in the appendix. We hope the improved theorem can relieve the concerns and the potential misleading."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323363362,
                "cdate": 1700323363362,
                "tmdate": 1700554762654,
                "mdate": 1700554762654,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CsOuwSAo2S",
                "forum": "VgPmCLQke7",
                "replyto": "AK9BvocnDe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xuvq (2/3)"
                    },
                    "comment": {
                        "value": "> 3. Response to \"Assume weight matrix $W$ are randomly initialized as IID standard Gaussian, which is a common practice in neural network initialization. Then $W$ follows a Matrix Normal distribution, i.e., $W \\sim MN(0, I, I)$. By applying permutation matrices $P$ and $Q$ to $W$, the result distribution has the same Matrix Normal form, i.e., $PWQ \\sim MN(0, I, I)$. Hence, the permutated weight matrix maintains the same distribution as the original $W$. This suggests that permuting the weight matrix is equivalent to reinitializing it. Given this equivalence, it raises the question of how to consider all permutations, given that the search space is uncountable.\"\n> \n\nThanks for this valuable comment. In the response, we will provide the basic background on the role of permutation invariance in linear mode connectivity to relieve the reviewer's concerns. We kindly suggest the reviewer to refer to [1]. Further, to solve the reviewer's confusion, we will provide detailed explanations from the following four aspects:\n\n- I think you may have assumed that $P$ and $Q$ are constant matrices, or $P$ and $Q$ are independent random matrices with respect to the random matrix $W$, which is $W\\sim MN(0,I,I)$. Only in such cases would you arrive at the conclusion that the random matrices $W$ and $PWQ$ have the same distribution. In reality, $P$ and $Q$ are not independent random matrices with respect to $W$. In the process of using permutations to find Linear Mode Connectivity, assuming that $W$ and $U$ are the coefficient matrices of two separate models, the distribution of $P$ and $Q$ depends on $W$ and $U$. Specifically, $P, Q = \\arg \\min_{P,Q} |PWQ-U|$. Using such $P$ and $Q$ to permute $W$ and obtain $PWQ$ generally does not make $PWQ$ follow the same distribution as $W$. So, the understanding that \"permuting the weight matrix is equivalent to reinitializing it\" is incorrect. Additionally, whether $W$ and $PWQ$ follow the same distribution is not important; what matters is the distribution of $|PWQ-U|$. Even if $PWQ$ and $U$ follow the same distribution, the distribution of $|PWQ-U|$ can still be complex because $PWQ$ and $U$ are not independent. This complexity arises from their joint distribution (if you are still confused about this, please refer to https://en.wikipedia.org/wiki/Wasserstein_metric, which explains the fact that even if $A$ and $B$ follow the same distribution, the distribution of $|A-B|$ can still be complex).\n\n- A simple experiment can illustrate that $PWQ$ does not follow the same distribution as $W$. Suppose $W = \\begin{pmatrix} w_{11}&w_{12}\\\\ w_{21}&w_{22} \\end{pmatrix}$ is a 2x2 random matrix, where $w_{ij} \\sim N(0,1)$. We define $Q = \\begin{pmatrix}1&0\\\\ 0 &1\\end{pmatrix}$, and $P$ follows the conditional distribution with respect to $W$: when $w_{11} \\ge w_{12}$, $P = \\begin{pmatrix}1&0  \\\\ 0&1\\end{pmatrix}$, and when $w_{11} < w_{12}$, $P = \\begin{pmatrix}0&1 \\\\ 1 &0\\end{pmatrix}$. In this case, the probability density function of the element $w_{11}'$ is given by $p(x) = \\frac{1}{2\\pi}e^{-\\frac{x^2}{2}}\\int_{-\\infty}^xe^{(-\\frac{t^2}{2})}dt$, which does not follow the standard normal distribution (in fact, $w_{11}'$ is the order statistic of two independent normal distribution variables). So, even if $W \\sim MN(0,I,I)$, it does not necessarily hold that $PWQ \\sim MN(0,I,I)$. Moreover, in the context of $P,Q = \\arg \\min_{P,Q}|PWQ-U|$, the conditional distributions of $P$ and $Q$ with respect to $W$ and $U$ are more complex than the conditional distributions of $P_0$ and $Q_0$ in our example here. Therefore, it is not possible for $PWQ$ to follow the same distribution as $W$.\n\n- Finding when $W$ and $U$ are random matrices, the distribution of $\\min|PWQ-U|$ is actually a famous question called Random Euclidean Matching Problem.  Please refer to [3].  \n \n- Now, let's consider the issue of how to find $P$ and $Q$ by calculation. Indeed, for a given $W$ and $U$, finding permutations $P$ and $Q$ that satisfy $P, Q = \\arg \\min_{P,Q}|PWQ-U|$ is a difficult problem. It has been proven in [2] that this is an NP-hard problem. However, for a given $W$ and $U$, the search space for $P$ and $Q$ is finite, not uncountable, because $W$ and $U$ are of finite dimension and the search space for $P$ and $Q$, which is the space of permutation matrices  for $W$ and $U$ is also finite. Since finding $P$ and $Q$ is an NP-hard problem, there is no polynomial-time exact solution formula in the computation process. Typically, we use the random approximation algorithms proposed in [2], such as weight matching/activation matching."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323409158,
                "cdate": 1700323409158,
                "tmdate": 1700554967048,
                "mdate": 1700554967048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B8RaJobOIr",
                "forum": "VgPmCLQke7",
                "replyto": "AK9BvocnDe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xuvq (3/3)"
                    },
                    "comment": {
                        "value": "> 4. Response to \"The \"mask ratio\" is not precisely defined, and it seems to be related to the ratio of untrained neurons to the total number of neurons.\"\n> \n\nThanks for this helpful comment. The reviewer understands it correctlly that the mask ratio $\\rho$ refers to \"keeping $\\rho$ fraction of the parameters **fixed** after initialization\". We have added the specific defition of $\\rho$ in the revised paper (page 4, blue sentence).\n\n\n> 5. Response to \"Is it possible to express the results in Theorem 3.2 in terms of \"loss barrier\" or \"accuracy barrier\"?\"\n> \n\nThank you for the suggestion. Taking your advice in the newly revised Theorem 3.2, we have reformuate $z_{\\alpha}$ into the \"barrier\" form same with [1]. We note that the \"barrier\" form is depicted by the network's output function, not the loss or accuracy functions, and previous theoretical analysis also adopted this form [1].\n\nActually, for a convex loss function $\\mathcal{L}$ satisfying the $C$-Lipschitz condition, the loss barrier can be bounded by $\\left|z(\\alpha)\\right|$: $B_{loss}(\\mathbf{w_1},\\mathbf{w_2})\\le C\\sup_\\alpha|z(\\alpha)|$, which differs only by the constant factor $C$, and that's why $|z(\\alpha)|$ can be used for the upper bound on the barrier instead of $B_{loss}(\\mathbf{w}_1,\\mathbf{w}_2)$, both in our Theorem 3.2 and in the previous theoretical analysis [1]. \n\n---\n[1] Entezari R, Sedghi H, Saukh O, et al. The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks[C]//International Conference on Learning Representations. 2022.\n\n[2] Ainsworth S, Hayase J, Srinivasa S. Git Re-Basin: Merging Models modulo Permutation Symmetries[C]//The Eleventh International Conference on Learning Representations. 2023.\n\n[3] Goldman M, Trevisan D. Convergence of asymptotic costs for random Euclidean matching problems[J]. Probability and Mathematical Physics, 2021, 2(2): 341-362."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323523143,
                "cdate": 1700323523143,
                "tmdate": 1700362718024,
                "mdate": 1700362718024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c7bBTPD1v6",
                "forum": "VgPmCLQke7",
                "replyto": "AK9BvocnDe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer xuvq: hoping that our response could address your concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer xuvq,\n\nMany thanks for your valuable comments. We have faithfully given detailed responses to your concerns during the rebuttal.\n\nSpecifically, we have addressed the following points:\n\n- We have clarified our novelty and contributions and explained why the perspective and the proposed method differ from the literature.\n- We have refined our theorem to avoid misleading. The revised theorem is more solid and more aligned with the proposed TNA-PFN. Also, following your advice, the revised theorem adopts the barrier function as $z_{\\alpha}$.\n- To resolve your confusion, we have detailedly explained the mechanism of the role of linear mode connectivity and permutation invariance.\n- We have corrected some minor issues, i.e., the definition of mask ratio.\n\nWe would appreciate it if you could let us know if our response has sufficiently addressed your questions and thus kindly reconsider your score.\n\nThank you.\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644709146,
                "cdate": 1700644709146,
                "tmdate": 1700646620476,
                "mdate": 1700646620476,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vmnklvbVq0",
                "forum": "VgPmCLQke7",
                "replyto": "c7bBTPD1v6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Reviewer_xuvq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Reviewer_xuvq"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' detailed response. After reviewing all the comments and responses, I've decided to raise the score to 5 as a way to acknowledge the authors' effort in addressing some of my concerns regarding the permutation notion. However, at this point, I'm unable to suggest acceptance because I still hold the view that the masking strategy resembles dropout. \nI maintain a different perspective from the authors regarding dropout. As the authors believe dropout's purpose is to prune weights or neurons, in my understanding, dropout spreads the utilization of every neuron by applying a fixed probability mask to each neuron during each epoch of training, ensuring diverse usage, whereas all neurons are employed during prediction."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668298267,
                "cdate": 1700668298267,
                "tmdate": 1700668298267,
                "mdate": 1700668298267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V8FBXAH7bR",
            "forum": "VgPmCLQke7",
            "replyto": "VgPmCLQke7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1333/Reviewer_vDht"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1333/Reviewer_vDht"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new method for reducing symmetries in a neural network. By keeping a $1-\\rho$ fraction of the parameters fixed after initialization and running gradient descent, they find a neural network that has good interpolation behavior (i.e., convex combinations of good parameters lead to parameters that are also good).\n\nThis enables federated learning and other applications, as models can be averaged without loss in performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The main advantage of the method is that it is performed during training time, and is not a post-processing step. This allows for easier averaging of models.\n\n- Extensive experiments and ablation studies.\n\n- Applications and extensions to federated learning and interesting and possibly impactful."
                },
                "weaknesses": {
                    "value": "- Some of the claims are a little overhyped, such as ``training in a subspace'' meaning keeping some set of parameters frozen and optimizing the rest. \n\n- The theory result is weak -- it doesn't imply much for the proposed method. it states that given Gaussian initialized neural networks, the gradient and the curvature of the convex combinations of these networks are bounded by the dimension, and hence lowering the dimension is desirable.\n\n- The algorithmic novelty is also limited, and most of the strengths lie in the empirical studies on the effect of width, etc."
                },
                "questions": {
                    "value": "See weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1333/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699053663181,
            "cdate": 1699053663181,
            "tmdate": 1699636060463,
            "mdate": 1699636060463,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4W5YxVmzh9",
                "forum": "VgPmCLQke7",
                "replyto": "V8FBXAH7bR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vDht"
                    },
                    "comment": {
                        "value": "We appreciate the valuable reviews. We accordingly respond to the reviewer's comments below.\n\n> 1. Response to \"By keeping a 1 - $\\rho$ fraction of the parameters fixed after initialization ...\"\n> \n\nWe apologize for the unclear definition of $\\rho$. Actually, the mask ratio $\\rho$ refers to \"keeping $\\rho$ fraction of the parameters **fixed** after initialization\". We have added the specific definition of $\\rho$ in the revised paper (page 4, blue sentence).\n\n> 2. Response to \"Some of the claims are a little overhyped, such as ``training in a subspace'' meaning keeping some set of parameters frozen and optimizing the rest.\"\n\nThanks for pointing it out. We acknowledge that there are some other definitions/meanings of \"subspaces\" which may cause misunderstanding. To make it more rigorous and clear, we have added a footnote (page 2 in blue) to clarify the definition of \"subspaces\" within this paper's scope.\n\n> 3. Response to \"The theory result is weak -- it doesn't imply much for the proposed method. it states that given Gaussian initialized neural networks, the gradient and the curvature of the convex combinations of these networks are bounded by the dimension, and hence lowering the dimension is desirable.\"\n> \n\nThanks for this helpful comment. We realize that previous theorem may cause potential misleadings and we have refined the theorem to make it more solid. \n\n- The previous theorem may cause a misleading that lowering the network dimension by changing the network architecture can improve linear mode connectivity. Actually, this is not what we try to convey and is also inconsistent with the empirical observations. \n- After our revision, the revised theorem is more aligned with our hypothesis and method. It is proved that given a network architecture (without changing the dimension, i.e., width and depth), if we fix a proportion of parameters and train the remaining (like what TNA-PFN does), the linear mode connectivity of two models after training would be improved. \n- The revised theorem is Theorem 3.2 in blue on page 3, and the detailed proof is in the appendix. We hope the improved theorem can relieve the concerns and the potential misleading.\n\n> 4. Response to \"The algorithmic novelty is also limited, and most of the strengths lie in the empirical studies on the effect of width, etc.\"\n> \n\nThanks for this comment. We apologize that the previous theorem may mislead the reviewer to understand our novelty and contributions. We would like to kindly clarify our contributions. \n- **1)** *New insights into linear mode connectivity and permutation invariance from training time.* Our novelty lies in the **training-time** \"subspace hypothesis\" perspective while previous works only focus on **post-training** connectivity via permutations symmetries (re-basin). The proposed TNA-PFN is just a showcase and verification of the hypothesis/perspective. The contributions are alike [1] in that its main contribution is the hypothesis of linear mode connectivity and permutation invariance and the simulated annealing algorithm (which may not be very effective) is just the verification. We believe our new perspective can inspire more future work in the community. \n- **2)** *The applications of training-time neuron alignment in federated learning.* While linear mode connectivity is somewhat theoretical, we propose two algorithms in federated learning to showcase how improving connectivity during training can help practices. The proposed methods are lightweight and effective.\n\nFor the width or depth issue, it is worth noting that according to the previous empirical results [1, 2], deeper networks intrinsically have higher barriers in connectivity and are therefore, harder to improve by post-training rebasin, while wider networks have lower barriers. Therefore, for our TNA-PFN, the strength of width and the weakness of depth are rational and aligned with previous understandings. \n\nHowever, we emphasize that our TNA-PFN has better results even on the deeper models compared with previous works. In Figure 7 of [1], the proposed simulated annealing algorithm fails to improve the post-training linear mode connectivity when the depths are high, whereas TNA-PFN improves training-time connectivity even under deeper models (Figures 3 and 4 of the paper).\n\n---\n[1] Entezari R, Sedghi H, Saukh O, et al. The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks[C]//International Conference on Learning Representations. 2022.\n\n[2] Ainsworth S, Hayase J, Srinivasa S. Git Re-Basin: Merging Models modulo Permutation Symmetries[C]//The Eleventh International Conference on Learning Representations. 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322372799,
                "cdate": 1700322372799,
                "tmdate": 1700555661717,
                "mdate": 1700555661717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PfvBqGs9VU",
                "forum": "VgPmCLQke7",
                "replyto": "V8FBXAH7bR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1333/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer vDht: hoping that our response could address your concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer vDht,\n\nMany thanks for your valuable comments. We have faithfully given detailed responses to your concerns during the rebuttal.\n\nSpecifically, we have addressed the following points:\n\n- We have reformulated our claim of \"subspace\" to make it more rigorous.\n- We have refined our theorem to avoid misleading. The revised theorem is more solid and _more aligned with the proposed TNA-PFN_. The revised theorem proves that by keeping some weights of neurons fixed, the linear mode connectivity will be improved.\n- We have clarified our novelty and contributions and explained more about the empirical observations regarding the width and depth.\n\nWe would appreciate it if you could let us know if our response has sufficiently addressed your questions and thus kindly reconsider your score.\n\nThank you.\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1333/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646094894,
                "cdate": 1700646094894,
                "tmdate": 1700646677455,
                "mdate": 1700646677455,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]