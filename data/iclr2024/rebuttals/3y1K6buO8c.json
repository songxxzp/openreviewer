[
    {
        "title": "Brain decoding: toward real-time reconstruction of visual perception"
    },
    {
        "review": {
            "id": "Pn5gD8wJT6",
            "forum": "3y1K6buO8c",
            "replyto": "3y1K6buO8c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m"
            ],
            "content": {
                "summary": {
                    "value": "The authors perform MEG conditioned visual decoding.\n\nCompared to other works that leverage fMRI, MEG is a different information source that presents unique challenges.\n\nThe authors use an align then generate strategy, where they learn a function that takes as input the MEG signal, and train it to align with a CLIP latent using a weighted sum of infoNCE and MSE loss. For image generation, they use Versatile Diffusion and regress the needed conditioning variables from MEG. \n\nThey observe that it is possible to recover high level semantics in the reconstructed images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "MEG decoding of full images is an under-explored area compared to fMRI based decoding, and it is a harder task, given the low channel count relative to the tens of thousands of voxels in fMRI. To my knowledge, this is the first time that image decoding has been demonstrated using MEG.\n\nThe paper is methodologically sound, outlining different training objectives for different parts of the proposed pipeline. The paper provides systematic benchmarks, showing that their MEG decoder leads to reasonable image retrieval and image generation.\n\nI applaud the authors for showing \"representative\" retrieval and best/mean/worst decoding results, which helps gauge the effectiveness of the method.\n\nIt is also interesting that they found MEG capable of recovering high level semantics. Although it is not fully clear if this is a limitation of MEG or their method (should probably discuss more)."
                },
                "weaknesses": {
                    "value": "As with other deep decoding papers, it is not super clear what the ultimate scientific insight is. This is not a criticism specific to this paper, but more generally aimed at current decoding works which leverage powerful image priors and deep non-linear decoding/embedding functions. \n\nIn this aspect, this paper is better than most, as their Figure 3 provides some insight on the temporal dynamics of decoding. I think it would benefit the paper to add some discussion (not necessarily experiments) on extending this to EEG based decoding, or other potential practical applications or scientific insights. \n\n**General clarifications:**\n\nThe clarity of many of their methods could be improved. The author repeatedly references Defossez et al. in reference to their methods. But in the main text it is not super clear. Concretely I would like the authors to clarify the following:\n1. What the the MEG conv \"encoder\" convolving over?\n\n2. How do you combine the MEG channels?\n\n3. What temporal aggregation layer did you use? You mention global pooling, affine, and attention. Which layer did you end up using? Because you discuss this and then never talk about which method you ended up using.\n\n4. How do the different aggregation layers work? I ask this question in the context of Figure 3. Because you discuss using a 1500ms window, then shift to a 250ms window. Do you train a new model? Do you re-use the 1500ms model but change the aggregation? If you do train a new model, are you taking multiple 250ms windows and supervising with the same image target? For the sliding window, what is the step size?\n\n5. For Figure 2, in the supervised models (VGG, ResNet, etc.) are you using the last layer (1000 imagenet classes layer), or the post-pooled layer. \n\n6. For retrieval, are you always using cosine/dot-product similarity?\n\n**Minor format error:**\n1. The authors have ICLR 2023 in the header, when it should be ICLR 2024. And they have line numbers, which do not seem to be present in the default ICLR template.\n\n**Minor clarifications:**\n1. Can you clarify if $N$ (line 75) denotes the number of images? It doesn't seem like you define $N$ prior/after using it.\n2. To provide more context, can you mention in line 84 that you are using the infoNCE loss, rather than just mentioning the CLIP loss.\n3. In section 2.2, can you clarify if you are normalizing $\\hat{z}$ to norm = 1 for eq. 1, and not assuming a fixed norm for eq. 2? Otherwise it seems like the two losses would have trivially the same optima, but I guess you are trying to have one loss align the direction, and have a second loss align the direction + norm.\n\n**Additional citations:**\n\nThe author discusses one approach towards decoding, but I would appreciate if the author could also discuss the brain gradient conditioned image generation work listed below, the most recent of which also leverage GANs/Diffusion models:\n\nInception loops discover what excites neurons most using deep predictive models (**Nature 2019**); Neural population control via deep image synthesis (**Science 2019**); Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences (**Cell 2019**); Computational models of category-selective brain regions enable high-throughput tests of selectivity (**Nature Communications 2021**); NeuroGen: Activation optimized image synthesis for discovery neuroscience (**Neuroimage 2022**); Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models (**NeurIPS 2023**); Energy Guided Diffusion for Generating Neurally Exciting Images (**NeurIPS 2023**)\n\nOverall I think the paper is sound, interesting, and provides good insight on neural decoding from an often overlooked modality."
                },
                "questions": {
                    "value": "Please see the weakness's section for questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5619/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m",
                        "ICLR.cc/2024/Conference/Submission5619/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786038835,
            "cdate": 1698786038835,
            "tmdate": 1700604582098,
            "mdate": 1700604582098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pfvJI4BHJA",
                "forum": "3y1K6buO8c",
                "replyto": "Pn5gD8wJT6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough review and insightful suggestions that effectively help improve the quality of our manuscript.\n\n# 1. Weakness\n\nWe agree with the reviewer that the scientific insight provided by our study could have been clearer. We have now (1) updated the contribution section, (2) provided new analyses and (3) amended the discussion section.\n\n## 1.A. Contribution\n\nThis study primarily focuses on methodological contributions. We have now amended the manuscript as follows:\n\n> \u201cOur approach provides three main contributions: our MEG decoder (1) yields a 7X increase in performance as compared to linear baselines (Figure 2), (2) helps reveal when high-level semantic features are processed in the brain (Figure 3) and (3) allows the continuous generation of images from temporally-resolved brain signals (Figure 4). Overall, this approach thus paves the way to better understand the unfolding of the brain responses to visual inputs.\u201c"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526208016,
                "cdate": 1700526208016,
                "tmdate": 1700527919502,
                "mdate": 1700527919502,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "soLRCjOZCy",
                "forum": "3y1K6buO8c",
                "replyto": "Pn5gD8wJT6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## 1.B. New analyses\n\nTo highlight the scientific insights that our MEG decoding may bring, we implemented several new analyses and updated: \nFigure 3, which now shows retrieval performance at a much higher temporal resolution;\nFigure 4, which now shows image generations as a function of time;\nSupplementary figures (growing window retrieval, model weights analysis and time-resolved image generation metrics in Appendices 8,10 and 11, respectively), which detail when low- and high-level features may be decoded from brain activity.\n\nOverall, these new analyses reveal that (1) late brain responses specifically represent high-level features and (2) the brain responses to image offset represent both low and high-level responses. These elements subsume previous neuroscientific investigation. We now indicate:\n\nIn Appendix 5:\n\n> \u201cConsistent with the decoding peaks observed after image onset and offset (Fig. 3), the retrieval performance of all growing-window models considerably improves after the offset of the image. Together, these results suggest that the brain activity represents both low- and high-level features even after image offset. This finding clarifies mixed results previously reported in the literature. Carlson et al., (2011; 2013), reported small but significant decoding performances after image offset. However, other studies (Cichy et al., 2014, Hebart et al., 2023) did not observe such a phenomenon. In all these cases, decoders were based on pairwise classification of object categories and on linear classifiers. The improved sensitivity brought by (1) our deep learning architecture, (2) its retrieval objective and (3) its use of pretrained latent features may thus help clarify the dynamics of visual representations in particular at image offset. We speculate that such offset responses could reflect an intricate interplay between low- and high-level processes that may be difficult to detect with a pairwise linear classifier. We hope that the present methodological contribution will help shine light on this understudied phenomenon.\u201d\n\nReferences:\n```Carlson, Thomas A., et al. \"High temporal resolution decoding of object position and category.\" Journal of vision 11.10 (2011): 9-9.\nCarlson, Thomas, et al. \"Representational dynamics of object vision: the first 1000 ms.\" Journal of vision 13.10 (2013): 1-1.\nCichy, Radoslaw Martin, Dimitrios Pantazis, and Aude Oliva. \"Resolving human object recognition in space and time.\" Nature neuroscience 17.3 (2014): 455-462.\nHebart, Martin N., et al. \"THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior.\" Elife 12 (2023): e82580.\n```\n\nIn Appendix 10:\n\n> \u201cWe inspect our decoders to better understand how they use information in the time domain. To do so, we leverage the fact that our architecture preserves the temporal dimension of the input up until the output of its convolutional blocks. This output is then reduced by an affine transformation learned by the temporal aggregation layer (see Section 2.3 and Appendix A.2). Consequently, the weights $w^{agg} \\in \\mathbb{R}^T$ can reveal on which time steps the models learned to focus. To facilitate inspection, we initialize $w^{agg}$ to zeros before training and plot the mean absolute weights of each model (averaged across seeds). \n\n> The results are presented in Fig. S8. While these weights are close to zero before stimulus onset, they deviate from this baseline after stimulus onset, during the maintenance period and after stimulus offset. Interestingly, and unlike high-level features (e.g., VGG-19, CLIP-Vision), low-level features (e.g., color histogram, AutoKL and DINOv2) have close-to-zero weights in the 0.2-0.5 s interval.\n\n> This result suggests that low-level representations quickly fade away at that moment. Overall, this analysis demonstrates that the models rely on these three time periods to maximize decoding performance, including the early low-level responses ($t=0-0.1$s).\u201c\n\nFinally, we now provide a time-resolved analysis of low- and high-level reconstruction metrics (Appendix A.11). These results confirm that the early responses to both image onset and offset are primarily associated with low-level metrics. On the other hand, the decoding of brain responses during the second half of the image presentation interval (0.2-0.5 s) appears to be more related to high-level features."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526325314,
                "cdate": 1700526325314,
                "tmdate": 1700528094673,
                "mdate": 1700528094673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6T7gTM3Gxi",
                "forum": "3y1K6buO8c",
                "replyto": "Pn5gD8wJT6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## 1.C. Updated impact\n\nFinally, we modified the manuscript to further discuss what scientific insights our approach may provide. The present work is primarily a methodological contribution, and a demonstration that natural images can be decoded from MEG at a much better level than anticipated (7X as compared to linear baseline). We updated the discussion section (paragraph on Impact):\n\n> \u201cOur methodological contribution has both fundamental and practical impacts. First, decoding the unfolding of perceptual representations could clarify the unfolding of visual processing in the brain. While there is considerable work on this issue, neural representations are challenging to interpret because they represent latent, abstract, feature spaces. Generative decoding, on the contrary, can provide concrete and, thus, interpretable predictions. Put simply, generating images at each time step could help neuroscientists understand whether specific \u2013 potentially unanticipated \u2013 textures or object parts are represented. For example, Cheng et al., (2023) showed that generative decoding applied to fMRI can be used to decode the subjective perception of visual illusions. Such techniques can thus help to clarify the neural bases of subjective perception and to dissociate them from those responsible for ``copying\u2019\u2019 sensory inputs. Our work shows that this endeavor could now be applied to clarify *when* these subjective representations arise.\n\n> Second, generative brain decoding has concrete applications. For example, it has been used in conjunction with encoding, to identify stimuli that maximize brain activity (Bashivan et al., 2019). Furthermore, non-invasive brain-computer interfaces (BCI) have been long-awaited by patients with communication challenges related to brain lesions. BCI, however, requires real-time decoding, and thus limits the use of neuroimaging modalities with low temporal resolution such as fMRI. This application direction, however, will likely require extending our work to EEG, which provides similar temporal resolution to MEG, but is typically much more common in clinical settings.\u201c\n\n```\nReferences\nCheng, Fan L., et al. \"Reconstructing visual illusory experiences from human brain activity.\" Science Advances 9.46 (2023): eadj3906.\n```"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526377146,
                "cdate": 1700526377146,
                "tmdate": 1700528868024,
                "mdate": 1700528868024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qNHgVSbzzJ",
                "forum": "3y1K6buO8c",
                "replyto": "Pn5gD8wJT6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "I've increased my score to 8, I am more convinced of the scientific contribution.\n\nI still want to comment on the sub-optimal presentation of the rebuttal itself.\n\nWhile the authors have indeed provided additional scientific justification which I find convincing, the authors have not addressed my requests for clarification, and the rebuttal is not well formatted.\n\nThe revision PDF also does not include highlights as typical in ICLR rebuttals to visualize the changes. \n\nHopefully the authors can improve upon this in the future.\n\nEdit: Reading the additional clarifications below, the authors use the last layer (1000 class output) of the each network as the baseline, which does not strike me as particularly sound. In this layer, there exists an exact optima, where it is an indicator function of the correct class. It is not clear that this output is necessarily reflective of different architectures. And to my knowledge, using a linear probe on the last categorical layer is not done in other works. I'm re-calibrating my score to a 6."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526919824,
                "cdate": 1700526919824,
                "tmdate": 1700604600673,
                "mdate": 1700604600673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lNSiC1HDEr",
                "forum": "3y1K6buO8c",
                "replyto": "4RG6PxGxPm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m"
                ],
                "content": {
                    "comment": {
                        "value": "Ah that makes sense. I was looking at the rebuttal and only the first half showed up.\n\nThanks for clarifying the contributions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530974731,
                "cdate": 1700530974731,
                "tmdate": 1700530974731,
                "mdate": 1700530974731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "idTWMdd4zt",
            "forum": "3y1K6buO8c",
            "replyto": "3y1K6buO8c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5619/Reviewer_hWdQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5619/Reviewer_hWdQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors developed a model based on contrastive and regression objectives to decode MEG, resulting in 7X improvement in image retrieval over a classic linear decoder. The promising results in image retrieval and generation are significant in that the presented approach allows the monitoring of the unfolding of visual processing in the brain based on MEG signals, which have much higher temporal resolution than fMRI. The work yields two potentially interesting observations: (1) late responses are best decoded with DINOv2, and (2) MEG signals contain high-level features, whereas 7T fMRI allows the recovery of low-level features, though it would be worthwhile to articulate or speculate what these findings mean for understanding the cascade of visual processes in the brain."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The work is significant in that there is no MEG decoding study that learns end-to-end to reliably generate an open set of images. Thus, it can potentially be considered a ground-breaking in this area of research, even though the techniques used are not necessarily novel from an ML perspective."
                },
                "weaknesses": {
                    "value": "The decoding work is supposed to provide new insights to the cascade of visual processing and the unfolding of visual perception in the brain.  The authors need to articulate better what insights the current observations (mentioned in the Summary) actually provide us."
                },
                "questions": {
                    "value": "What do the two observations tell us about the unfolding of visual perceptual processes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798493329,
            "cdate": 1698798493329,
            "tmdate": 1699636580536,
            "mdate": 1699636580536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JSkOmLhZg1",
                "forum": "3y1K6buO8c",
                "replyto": "idTWMdd4zt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive assessment of our work and their helpful comment.\n\nWe agree that the insights provided by our observations were insufficiently clear. The primary contributions of this study are methodological rather than hypothesis-driven. We have now amended the manuscript to indicate:\n\n> \u201cOur approach provides three main contributions: our MEG decoder (1) yields a 7X increase in performance as compared to linear baselines (Fig. 2), (2) helps reveal when high-level semantic features are processed in the brain (Fig. 3) and (3) allows the continuous generation of images from temporally-resolved brain signals (Fig. 4). Overall, this approach thus paves the way to better understand the unfolding of the brain responses to visual inputs.\u201d\n\nIn addition, to clarify and substantiate the insights provided by this study, we have now added several new analyses.\n\nFirst, we now implement a high-temporally resolved analysis of retrieval and generative decoders, using both sliding and growing windows (Updated figures 3, 4; new figures S6A, S7, S9 ). Overall, these results clarify when low- and high-level representations can be decoded from brain activity, and thus shed lights on the dynamics of visual processing in the brain:\n\n> \u201cConsistent with the decoding peaks observed after image onset and offset (Fig. 3), the retrieval performance of all growing-window models considerably improves after the offset of the image. Together, these results suggest that the brain activity represents both low- and high-level features even after image offset. This finding clarifies mixed results previously reported in the literature. Carlson et al., (2011; 2013), reported small but significant decoding performances after image offset. However, other studies (Cichy et al., 2014, Hebart et al., 2023) did not observe such a phenomenon. In all of these cases, decoders were based on pairwise classification of object categories and on linear classifiers. The improved sensitivity brought by (1) our deep learning architecture, (2) its retrieval objective and (3) its use of pretrained latent features may thus help clarify the dynamics of visual representations in particular at image offset. We speculate that such offset responses could reflect an intricate interplay between low- and high-level processes that may be difficult to detect with a pairwise linear classifier. Overall, we hope that the present methodological contribution will help shine light on this understudied phenomenon.\u201d\n\n\nReferences:\n```\nCarlson, Thomas A., et al. \"High temporal resolution decoding of object position and category.\" Journal of vision 11.10 (2011): 9-9.\nCarlson, Thomas, et al. \"Representational dynamics of object vision: the first 1000 ms.\" Journal of vision 13.10 (2013): 1-1.\nCichy, Radoslaw Martin, Dimitrios Pantazis, and Aude Oliva. \"Resolving human object recognition in space and time.\" Nature neuroscience 17.3 (2014): 455-462.\nHebart, Martin N., et al. \"THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior.\" Elife 12 (2023): e82580.\n```\n\nSecond, we inspect the model with a new analysis of its weights (Appendix A.10) and clarify that the decoding time course is different for low- and high-level features:\n\n> \u201cWe inspect our decoders to better understand how they use information in the time domain. To do so, we leverage the fact that our architecture preserves the temporal dimension of the input up until the output of its convolutional blocks. This output is then reduced by an affine transformation learned by the temporal aggregation layer (see Section 2.3 and Appendix A.1). Consequently, the weights $w^{agg} \\in \\mathbb{R}^T$ can reveal on which time steps the models learned to focus. To facilitate inspection, we initialize $w^{agg}$ to zeros before training and plot the mean absolute weights of each model (averaged across seeds). \n\n> The results are presented in Fig. S8. While these weights are close to zero before stimulus onset, they deviate from this baseline after stimulus onset, during the maintenance period and after stimulus offset. Interestingly, and unlike high-level features (e.g., VGG-19, CLIP-Vision), low-level features (e.g., color histogram, AutoKL and DINOv2) have close-to-zero weights in the 0.2-0.5 s interval.\n\n> This result suggests that low-level representations quickly fade away at that moment. Overall, this analysis demonstrates that the models rely on these three time periods to maximize decoding performance, including the early low-level responses ($t=0-0.1$s).\u201c"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561423500,
                "cdate": 1700561423500,
                "tmdate": 1700561423500,
                "mdate": 1700561423500,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MkclGYsUfK",
            "forum": "3y1K6buO8c",
            "replyto": "3y1K6buO8c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5619/Reviewer_hpxq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5619/Reviewer_hpxq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors propose a method to decode brain activity. The main idea is to train an MEG decoder which maps MEG signals to a feature space which is then used to reconstruct images using a pretrained image generator. \n\nThe authors show that MEG decoder which is a DNN leads to 7 times improvement over linear decoders which is a common approach in neuroscience studies. Image generation results suggests that it is possible to reconstruct semantically accurate from MEG activity while low-level details are difficult to reconstruct."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. 7x improvement in decoding accuracy over linear decoders. This is an important result which will encourage neuroscience researchers to use DNNs for decoding MEG/fMRI signals.\n2. Clear presentation of methods (Figure 1, Section2)."
                },
                "weaknesses": {
                    "value": "1. The reconstruction results are not impressive. Even the best examples shown in Figure 5 often do not have the reconstructions of image of same or related category.  Therefore, the title is misleading as the main contribution of this paper in my opinion is DNN based MEG decoder and retrieval results and is not correctly reflected in the title.\n2. The decoder is trained using a combination of two loss functions : MSE loss and CLIP loss (equation 3, line 91). There seems to be no ablation study investigating what is the impact of each loss function in retrieval performance. There is one figure in supplementary material Fig S2 E but I am not sure whether it indicates two terms of CLIP loss or two terms of overall loss (CLIP + MSE).\n3. In Line 110 authors mention that they select  lambda by sweeping over {0.0, 0.25, 0.5, 0.75, 1.0} and pick the model whose top-5 accuracy is the highest on the large test. Is the hyperparameter search for lambda done on test data?\n4. The claim in the abstract \"MEG signals primarily contain high-level visual features\" does not have sufficient evidence based on the reconstruction results only. It has been shown in literature (even in Things dataset paper Figure 8) that fMRI responses of early visual cortex (which can decode low-level features) are correlated with MEG responses (Cichy et al. 2014, Hebart et al. 2023) in early time windows. Therefore, a stronger evidence is required to back this claim. A possible explanation why the reconstructions can not recover low-level details might be that temporal aggregration layers leads to suppresion of low-level features which are present in a smaller time-window around 100ms. Another possible explanation is that we are predicting a high-level feature  (DINOv2/CLIP etc.) from MEG which may not need information from low-level features and thus the image generated also lack these details. \n5. The main result of the paper is 7x improvement over linear decoders. It is not clear where exactly this result is in the paper. A reader needs to compare results in supplementary and Figure 2 in the main text. Simply adding shaded bar in Figure 2 for linear decoder next to each bars can improve clarity"
                },
                "questions": {
                    "value": "Please refer to weaknesses section for points to address in rebuttal. \n\nOverall this paper has some new contributions but authors make some claims which do not have sufficient support in the results. Therefore, my recommendation would be to either tone down the claims or present good evidence to back them up"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5619/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5619/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5619/Reviewer_hpxq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698885298588,
            "cdate": 1698885298588,
            "tmdate": 1700603429428,
            "mdate": 1700603429428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ew9MOhbD4x",
                "forum": "3y1K6buO8c",
                "replyto": "MkclGYsUfK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank our reviewer for their thorough assessment and their helpful comments.\n\n1. Unimpressive reconstruction. \n\nWe agree that the quality of generated images are low in comparison to what can be obtained with 7T fMRI (Table 1). \n\nHowever, we respectfully disagree that this is not \u201cimpressive\u201d. To the best of our knowledge, this is the first study showing MEG-based reconstruction of visual processes. Relative to fMRI, MEG has a very low spatial resolution. Consequently, reconstructing these rich contents is far from obvious \u2013 for what it\u2019s worth, we certainly did not expect these results.\n\nWe thus clarify our contribution as follows:\n\n> \u201cOur approach provides three main contributions: our MEG decoder (1) yields a 7X increase in performance as compared to linear baselines (Fig. 2), (2) helps reveal when high-level semantic features are processed in the brain (Fig. 3) and (3) allows the continuous generation of images from temporally-resolved brain signals (Fig. 4). Overall, this approach thus paves the way to better understand the unfolding of the brain responses to visual inputs.\u201d\n\nBeyond these subjective considerations, part of this underwhelming feeling may be due to our original figures. We originally opted to automatically select those with the best, average and worst aggregated score across low-level (SSIM) and high-level metrics (SwAV) as there is no standard metric to rank images decoded from the brain. However, after closer inspection, we realize that this SSIM-SwAV combination may not faithfully reflect whether images are accurately reconstructed. To address this issue, we now added two novel figures (Figures 4 and S6, in Appendix A.7), which show manually-selected cases of successful and failed reconstructions, as well as their dynamics. In particular, we now note:\n\n> \u201cFigure S6 shows examples of failed generations. Overall, they appear to encompass different types of failures. Some generations appear to miss the correct category of the true object (e.g., bamboo, batteries, bullets and extinguisher in columns 1-4), but generate images with partially similar textures. Other generations appear to recover some category-level features but generate unrealistic chimeras (bed: weird furniture, alligator: swamp beast; etc, in columns 5-6). Finally, some generations seem to be plain wrong, with little-to-no preservation of low- or high-level features (columns 7-8). We speculate that these different types of failures may be partially resolved with different methods, such as better generation modules (for chimeras) and optimization on both low- and high-level features (for category errors).\u201d\n\nIn any case, we agree that these generations are far from being perfect. We thus toned down our abstract by emphasizing the preliminary aspect of our results:\n\n> \u201cOverall, these results, *while preliminary*, provide an important step towards the decoding - in real-time - of the visual processes continuously unfolding within the human brain.\u201d\n\n2. Impact of $\\lambda$ on retrieval.\n\nThe reviewer is right that we did not include $\\lambda$ from Equation 3 in the hyperparameter search presented in Appendix A.2. As such, Figure S1 presents retrieval performance for models trained with the CLIP loss term only (i.e., with $\\lambda=1.0$). We decided not to include $\\lambda$ in this hyperparameter search because the focus of this search was on the retrieval task, which is well aligned with the CLIP loss. However, as detailed in the next answer, we did implement a hyperparameter selection procedure for $\\lambda$, but that is carried out in a second step, as it relates to a generation-based evaluation.\n\n3. $\\lambda$ search on test data.\n\nThe hyperparameter search for $\\lambda$ was performed on the \u201clarge\u201d test set. The model obtained with the best $\\lambda$ was then used for the generation of images only on the \u201csmall\u201d test set. Importantly, the large and the small test sets are disjoint, thus there is no leakage. (Of note, we fixed a typo in the original manuscript: the set of $\\lambda$ values should in fact have read \\{0.0,0.25,0.5,0.75\\}.)\n\nWe now clarify this issue in the methods:\n\nIn Section 2.3:\n\n> \"We select $\\lambda$ in $\\mathcal{L}_{Combined}$ by sweeping over \\{0.0,0.25,0.5,0.75\\} and pick the model whose top-5 accuracy is the highest on the \"large\" test set (which is disjoint from the \"small\" test set used for generation experiments; see Section 2.8).\"\n\nIn Appendix A.2:\n\n> \"We run a hyperparameter grid search to find an appropriate configuration (MEG preprocessing, optimizer, brain module architecture and CLIP loss) for the MEG-to-image retrieval task.\"\n\n> \"We focus the search on the retrieval task, \\ie by setting $\\lambda=0$ in Eq. 3, and leave the selection of an optimal $\\lambda$ to a model-specific sweep using a held-out set (see Section 2.3).\""
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527730550,
                "cdate": 1700527730550,
                "tmdate": 1700527730550,
                "mdate": 1700527730550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FF2OU9lqN4",
                "forum": "3y1K6buO8c",
                "replyto": "rPXTlnvWda",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_hpxq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_hpxq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for clarification and additional results"
                    },
                    "comment": {
                        "value": "I would like to thank authors for their comprehensive responses backed by new results. I have updated my rating to 8."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603393710,
                "cdate": 1700603393710,
                "tmdate": 1700603393710,
                "mdate": 1700603393710,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aZRciEFWrN",
            "forum": "3y1K6buO8c",
            "replyto": "3y1K6buO8c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
            ],
            "content": {
                "summary": {
                    "value": "This contribution concerns the interesting topic of decoding/retrieval and reconstructing of visual input from MEG data (THINGS-MEG data set). The approach is based on representations of images and MEG data using multiple architectures and multiple levels of generalization.\n\nThere is a rich literature on decoding and reconstructing visual and audio stimulus from brain recordings, so novelty is somewhat limited.\n\nDecoding is evaluated as retrieval in closed and open set conditions (the latter using zero-shot setting).\nRetrieval is based on linking by learning to align MEG and image representations \n\nThe reconstruction of visual input is based on generative models, using frameworks that have been developed elsewhere (Ozcelik and Van Rullen). \n\nCompared to the very rich literature on methods based on MEG and other modalities, this study has an increased focus on temporal resolution of the retrieval process and furthermore, they use diffusion models for conditional generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Compared to the very rich literature on methods based on MEG and other modalities, this study has increased focus on temporal resolution of the retrieval and furthermore using sota diffusion models for conditional generation.\nIt is concluded that retrieval interesting peaks following image onset and image offset (the latter based on the after-image presumably). Retrieval performance is good for several image representations (VGG and DINOv2)\nThe generative performance is evaluated in a number of metrics, there is good consistency among the metrics.\nVisually the generation makes sense.\nUseful to see examples  stratified over good, bad and ugly cases."
                },
                "weaknesses": {
                    "value": "There is a rich literature on decoding and reconstructing visual and audio stimulus from brain recordings, so novelty is somewhat limited.\n\nBased on MEG we have high time resolution and SNR. In the temporally resolved analysis, it is interesting that VGG outperforms the more advanced representations for the direct image (after image onset) while the more complex image representations dominate retrieval based on the after-image (following image offset). We miss a discussion of this interesting finding.\n\nThe generative performance is evaluated in a number of metrics with good consistency among the metrics. Yet, we are missing uncertainty estimates to weigh the evidence in this case\n\nVisually the generated imagery is intriguing. However, we miss a discussion of the notable lack of fine grained semantic relatedness (generation seems primarily to pick up on texture, object scale(?) and high-level semantics eg. man-made vs natural)"
                },
                "questions": {
                    "value": "Based on MEG we have high time resolution and SNR. In the temporally resolved analysis, it is interesting that VGG outperforms the more advanced for the direct image (after onset) while the more complex image representations dominate retrieval based on the after image (after image offset). Missing a discussion of this interesting finding.\n\nThe generative performance while evaluated in a number of metrics with good consistency among the metrics. Yet, we are missing uncertainty estimates to weigh the evidence in this case\n\nVisually the generated imagery is intriguing. However, we miss a discussion of the notable lack of fine grained semantic relatedness (generation seems primarily to pick up on texture, object scale(?) and high-level semantics eg. man-made vs natural)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5619/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5619/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698929155683,
            "cdate": 1698929155683,
            "tmdate": 1700734985513,
            "mdate": 1700734985513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DPEZ5NQIwB",
                "forum": "3y1K6buO8c",
                "replyto": "aZRciEFWrN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive assessment of our work and their insightful comments. We address every comment and question below and highlight what we changed in the manuscript as a result.\n\n1. We agree that the *conceptual* novelty is limited: there is a lot of work on the decoding of images from brain activity. However, our method and our results are novel: Past research has focused on either (1) classification (Grootswagers et al., 2019; King & Wyart, 2021) (2) encoding (Cichy et al., 2017; Gifford et al., 2022) tasks from M/EEG or (3) image reconstruction from fMRI (Seeliger et al., 2018; VanRullen & Reddy, 2019; Ozcelik & VanRullen, 2023). To our knowledge, there is no study on image reconstruction from MEG, and thus no model or benchmark to reconstruct natural images from the unfolding of brain activity. We now clarify this issue in the discussion:\n\n> \u201cOur approach provides three main contributions: our MEG decoder (1) yields a 7X increase in performance as compared to linear baselines (Fig. 2), (2) helps reveal when high-level semantic features are processed in the brain (Fig. 3) and (3) allows the continuous generation of images from temporally-resolved brain signals (Fig. 4). Overall, this approach thus paves the way to better understand the unfolding of the brain responses to visual inputs.\u201c\n\nTo emphasize this point, we updated Figures 3 and 4 and added Appendix A.8, which now show a highly-resolved decoding of MEG responses, based on retrieval and generation with sliding or growing window decoders. \n\n2. We agree that it is interesting to highlight the differences between VGG and other high-level embedding observed after image onset and image offset. This is carried out in Appendix A.11.\n\n> \u201cLastly, we provide a sliding window analysis of these metrics in Appendix A.11. These results suggest that early responses to both image onset and offset are primarily associated with low-level metrics, while high-level features appear more related to brain activity in the 200-500 ms interval.\"\n\n3. The reviewer is right that we forgot to add uncertainty estimates. We now added SEM (standard error of the mean) for each relevant metric. Finally, we included a new table in Appendix 9 reporting individual metrics and SEM for each participant (Table S3)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526997095,
                "cdate": 1700526997095,
                "tmdate": 1700526997095,
                "mdate": 1700526997095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5hVZ0NNz4X",
                "forum": "3y1K6buO8c",
                "replyto": "DPEZ5NQIwB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
                ],
                "content": {
                    "title": {
                        "value": "Appendix missing?"
                    },
                    "comment": {
                        "value": "I thank the authors for the many last minute comments and clarifications. However, the version I can see in Openreview does not show appendices?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545735938,
                "cdate": 1700545735938,
                "tmdate": 1700545735938,
                "mdate": 1700545735938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2jGgJXehbw",
                "forum": "3y1K6buO8c",
                "replyto": "rJ0aVQRuYk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
                ],
                "content": {
                    "title": {
                        "value": "Upgrade"
                    },
                    "comment": {
                        "value": "While I am still unsure about the contribution for ICLR I acknowledge the many improvements made and therefore change my grade to 6"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734908549,
                "cdate": 1700734908549,
                "tmdate": 1700734908549,
                "mdate": 1700734908549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]