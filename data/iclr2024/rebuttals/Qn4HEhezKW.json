[
    {
        "title": "Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning"
    },
    {
        "review": {
            "id": "jcxLFEb9j1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8915/Reviewer_xgDh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8915/Reviewer_xgDh"
            ],
            "forum": "Qn4HEhezKW",
            "replyto": "Qn4HEhezKW",
            "content": {
                "summary": {
                    "value": "The paper proposes a method to adapt pretrained masked language models to diffusion language models, using it to adapt MLMs of different sizes to diffusion LMs, and then perform extensive task finetuning and instruction finetuning on the diffusion LMs. Results show that diffusion LMs can perform similarly to autoregressive LMs on tasks and generalize to unseen tasks after instruction finetuning. Diffusion LMs are also shown to manifest in-context learning and reasoning abilities, suggesting that diffusion LMs could have similar capabilities as autoregressive LMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This is one of the first papers to investigate the scalability of diffusion LMs, highlighting the potential of diffusion LMs with the increase of model size. The results could potentially draw more attention to and stimulate further research on non-autoregressive LMs.\n* The proposed diffusion adaptation technique bridges existing masked LM with diffusion LM using lightweight continual pre-training. It potentially serves as an effective strategy for training diffusion LMs under computation constraints.\n\n* The paper provides a comprehensive evaluation of diffusion LM using multiple supervised, few-shot, and zero-shot tasks, giving an overall complete perspective on the performance of diffusion LMs."
                },
                "weaknesses": {
                    "value": "* Discussion is limited on diffusion adaptation, a core method contribution of the paper. When using an existing MLM checkpoint instead of extensive pre-training, the performance of diffusion LM hinges on the effectiveness of diffusion adaptation. However, the paper does not provide enough performance metrics to validate the effectiveness of diffusion adaptation. For example, how does it compare to training from scratch, and how does the number of adaptation steps affect downstream performance? Without these results, it is unclear if diffusion adaptation is an effective approach to building diffusion LMs.\n\n* While showing diffusion LM can be promising and shows similar capabilities as auto-regressive LM, it is hard to get a concrete understanding of the performance of diffusion LM from the current paper. \n\n  * No effective performance comparison between diffusion LM and other kinds of LMs. While it is convenient to adapt an existing MLM to diffusion LMs and evaluate it, as the authors pointed out, the current available MLM checkpoints are out-of-date, and diffusion adaptation inevitably loses performance compared to training from scratch with diffusion objective. This makes the current result potentially considerably inferior to what diffusion LM can achieve in theory (with a state-of-the-art training recipe and trained from scratch). As a result, it is hard to appreciate the exact capabilities of diffusion LM from the current results. This could also make some discussions ineffective, for instance, it cannot be ruled out that the deficiencies in reasoning are due to insufficient training. \n\n  * Discussion on in-context learning and reasoning abilities is vague/ineffective. From Figure 5, it is hard to observe performance improvement due to in-context learning. For auto-regressive LM, in-context learning can be evaluated on raw pre-trained LM prior to instruction-finetuning. Maybe a similar evaluation on diffusion LMs could better visualize their in-context learning abilities. Also, as the authors pointed out, the limited context length of the original model limits the evaluation of few-shot performance. One possible solution without training from scratch could be using interpolation of position embeddings (e.g. [1]).\n\n    Similarly, there is no concrete conclusion from the discussion on reasoning abilities due to insufficient reasoning performance. One interesting discussion is on the generation order of diffusion LM and its difference from auto-regressive LMs, but a concrete conclusion would probably require a benchmark or a probe dataset for a systematic evaluation, showing the advantage of the generation order employed by diffusion LMs.\n\n  * In my opinion (without any discouragement to the authors), the evaluation of diffusion LMs is probably better done with sufficient computing resources for a head-to-head comparison with auto-regressive LMs under similar training conditions. The results would be much more useful and enable effective discussion on the capabilities of diffusion LMs."
                },
                "questions": {
                    "value": "* How does the performance of diffusion LM compare to other non-autoregressive language models?\n* What about the inference computation cost of diffusion LMs, compared to auto-regressive LMs? How does the number of denoising steps affect the performance-computation tradeoff, as it is a crucial parameter in image diffusion models?\n* The performance comparison in Table 1 is a bit confusing: 1) are the RDM models trained by the authors? if so, why not evaluate on all three datasets? 2) \"The performance of finetuned XLM-R models is competitive or superior to the common encoder-decoder Transformer baseline.\" may not be so safe to say as the decoder-only models are larger than encoder-decoder models. 3) how is XLM-R (autoregressive) evaluated on IWSLT14, as it is an encoder-only model?\n* In Table 2, why not use zero-shot AR as a reference but use supervised AR?\n* Do the AR models used in the paper use beam search during decoding?\n* Given that diffusion LMs use a different receptive field and generation order than auto-regressive LMs, could there be specific types of tasks or domains where diffusion LMs particularly excel or fall short? That could be a valuable discussion (but probably beyond the scope of the paper)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8915/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8915/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8915/Reviewer_xgDh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8915/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697355612992,
            "cdate": 1697355612992,
            "tmdate": 1699637121943,
            "mdate": 1699637121943,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G8Rw1JHMAN",
                "forum": "Qn4HEhezKW",
                "replyto": "jcxLFEb9j1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your insightful and constructive suggestions! We would like to address your concerns below. Please have a check and we are happy to address any further feedback!\n\n$$$$\n\n> `Q1:` Discussion is limited on diffusion adaptation, a core method contribution of the paper. The paper does not provide enough performance metrics to validate the effectiveness of diffusion adaptation. For example, how does it compare to training from scratch, and how does the number of adaptation steps affect downstream performance? It is unclear if diffusion adaptation is an effective approach to building diffusion LMs.\n\nThank you for your question! \nWe would like to clarify that our study does not aim to find the best practice for building diffusion language models. Instead, we hope to conduct a pioneering investigation into the scalability of diffusion language models without consuming huge computational resources. Our positive findings support this goal and can stimulate more future efforts on building large diffusion language models, either by pretraining from scratch or diffusive adaption.\n\n$$$$\n\n> `Q2:` No effective performance comparison between diffusion LM and other kinds of LMs.\n\nConducting a strict comparison across the recipes of different pretrained models can be challenging, especially given the expenses involved and the abundance of technical details. As we respond in Q1, we are not aiming to beat other language models in this study, but to validate the scalability of diffusion language models and encourage further research efforts on this. And our results in Table 1, Figure 3, and Table 2 can support that our models have decent performance compared to models at similar scales, and demonstrate generalist ability that can follow instructions to solve various tasks.\n\n$$$$\n\n> `Q3:` Discussion on in-context learning and reasoning abilities is vague/ineffective. From Figure 5, it is hard to observe performance improvement due to in-context learning.\n\nObservation in Figure 5 is actually consistent with the findings of the FLAN series paper [1,2], which shows that model performance is not sensitive to the number of demonstrations after instruction tuning, and we have mentioned this phenomenon in Section 4.2.2. Note that the most important message here is that instruction tuning helps our Diffusion LMs obtain in-context learning ability, without which we found that a raw pretrained MLM cannot learn from demonstrations. \n\n--\n\n[1] Scaling Instruction-Finetuned Language Models\n\n[2] The Flan Collection: Designing Data and Methods for Effective Instruction Tuning\n\n$$$$\n\n> `Q4:` Similarly, there is no concrete conclusion from the discussion on reasoning abilities due to insufficient reasoning performance.  One interesting discussion is on the generation order of diffusion LM and its difference from auto-regressive LMs, but a concrete conclusion would probably require a benchmark or a probe dataset for a systematic evaluation, showing the advantage of the generation order employed by diffusion LMs.\n\nThank you for your interest and valuable suggestions for a more in-depth study on causal generation order. In our paper, we mention two aspects showing promise in eliciting reasoning ability. \n- Qualitatively, we showed the causal generation order of the models, which indicates correct dependency in reasoning. \n- Quantitatively, in Section 4.5, we found the accuracy of our models on GSM8K can attain a non-trivial improvement from 5.6% to 12.8% after tuning on chain-of-thought data [1] of GSM8K distilled from code-davinci-002. This suggests the inability to perform reasoning should be attributed to insufficient model capability instead of the diffusion language modeling paradigm itself.\n\nWe believe our findings encourage effort in further scaling and improving the capability of diffusion language models, which facilitates exploring the concrete benefits of causal generation order with the support of sufficiently strong reasoning and planning ability.\n\n[1] Fu et al. Specializing smaller language models towards multi-step reasoning. ICML 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589679570,
                "cdate": 1700589679570,
                "tmdate": 1700590003099,
                "mdate": 1700590003099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XSWXJokbzU",
                "forum": "Qn4HEhezKW",
                "replyto": "jcxLFEb9j1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hello Reviewer xgDh,\n\nAs the deadline for author-reviewer discussion is drawing near, we would greatly appreciate receiving your feedback on our responses and are happy to answer further questions.\n\nWe look forward to hearing from you, and many thanks!\n\nSincerely,\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710844168,
                "cdate": 1700710844168,
                "tmdate": 1700710844168,
                "mdate": 1700710844168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8RBtO1xV8b",
            "forum": "Qn4HEhezKW",
            "replyto": "Qn4HEhezKW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8915/Reviewer_VY65"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8915/Reviewer_VY65"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the scalability of discrete diffusion language models on leveraging pretrained knowledge, model scaling and zero to few shot generalizations. The authors first draw an elaborate introduction on diffusion language models. Starting from off-the-shelf pretrained language models, the authors further fine-tune them with diffusion objective following RDM, with either task-specific tuning (MT, Summ) or instruction fine-tuning (FLAN). The authors conduct comprehensive experiments to explore the performance on both down-steam tasks and generalizations of discrete diffusion models varying their scale."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies an important topic on the scalability of diffusion language models, and goes through extensive experimental explorations, which makes a valuable contribution to developing better diffusion language models.\n- This paper poses comprehensive experimental investigations on scaling, tuning and generalizations of discrete diffusion language models, and compares them against the T5 family.\n- The author also explores emergent capabilities of autoregressive (L)LMs like in-context learning and reasoning, on their adapted large diffusion language models.\n- The paper is well written, clear to follow."
                },
                "weaknesses": {
                    "value": "- The main method in this paper is similar to existing works:\n\n  - The idea of leveraging pretrained masked models as a knowledge base and starting point for tuning discrete diffusion language models (named as \"diffusive adaption\") is introduced in [1].\n\n  - The main methods in this paper (tuning pretrained MLMs with diffusion objective) is also mainly adhering to the work RDM-Diffusion [2].\n  - The \"prompt-response\" format (partially diffusion on target tokens) essentially adopts the setting of DiffuSeq [3].\n\n- While referring BERT-like diffusion language models as 'decoder-only' models, (GPT-like) AR decoder models (which is currently most popular) is not compared or discussed throughout this study. Besides, I believe that authors need further clarifications on their definitions of 'decoder' models, since we do not generally refer BERT-like models as decoder-only models, which might cause some confusions.\n\n- The paper does not fully explore the configurations of Diffusion LMs, such as the number of diffusion steps, the noise level, and the length predictor. It would be valuable to explore how these factors affect the quality and diversity of the generated texts, especially when the inference latency due to the demand on multiple diffusion steps would limit the scaling of diffusion models. For example, would larger models remedy the demand for more steps & generalize better to predicted lengths or  opposite?\n\n[1] He, Z., Sun, T., Wang, K., Huang, X., & Qiu, X. (2022). Diffusion Bert: Improving generative masked language models with diffusion models. *ArXiv preprint arXiv:2211.15029*.\n\n[2] Zheng, L., Yuan, J., Yu, L., & Kong, L. (2023). A reparametrized discrete diffusion model for text generation. *ArXiv preprint arXiv:2302.05737*.\n\n[3] Gong, S., Li, M., Feng, J., Wu, Z., & Kong, L. (2022, September). DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models. In *The Eleventh International Conference on Learning Representations*."
                },
                "questions": {
                    "value": "- The paper lacks some important baselines and comparisons in Table 1. For example, why are there dashes on most AR transformer baselines? Besides, only small AR models (<100M) are compared against the 9.7B diffusion model. Other important baselines (e.g., GPT-structured decoder models, vanilla AR transformers at similar scale to diffusion variants (~80M), and pretrained + AR fine-tune) should be incorporated to compare against the performance of scaled diffusion models.\n- Why are the performance gaps of using oracle length and predicted length opposite in the two MT tasks in Figure 3? Are there any further investigations or explanations?\n- On table 2, what is the vanilla performance of XLM-based diffusion models before fine-tuning on FLAN? Since XLM models already adopt a multilingual pretraining objective, it seems natural that it elicits German knowledge and scalable, since larger models undertake greater-scale multilingual pretraining and has more parameters to store this knowledge.\n- Why BBH-NLP seems not following the scaling trend in Figure 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8915/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723123023,
            "cdate": 1698723123023,
            "tmdate": 1699637121827,
            "mdate": 1699637121827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LOV4Jlqbnt",
                "forum": "Qn4HEhezKW",
                "replyto": "8RBtO1xV8b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely acknowledge your constructive feedback and suggestions. We would like to address your concerns below. Please have a check and we are happy to address any further feedback!\n\n$$$$\n\n> `Q1:` The main method in this paper is similar to existing works.\n\nThank you for your question. In this work, we are not only presenting a method; more importantly, we are providing a pioneering exploration into the scalability of diffusion language models, or more broadly, non-autoregressive language models, without the inherent risk of consuming substantial computational resources. We would like to emphasize our additional contributions that go beyond the methods outlined in previous works.\n- DiffusionBERT[1] empirically finds it feasible to tune masked language models into generative models. We further establish the formal connection between reparameterized diffusion models (RDM) and masked language models in terms of training objectives.\n- Furthermore, the application of RDM to advance their methodologies [1,2] results in a commendable boost in competitive performance for this technical routine. This improvement not only facilitates our groundbreaking exploration into scaling diffusion language models but also extends to the broader realm of non-autoregressive language models, all achieved at a significantly lower cost. Our results can inspire further exploration in scaling up non-autoregressive language models, mitigating the previously perceived risk associated with the substantial demand for computational resources.\n- For the \"prompt-response\" format, we consider it a common practice to apply decoder-only language models for various tasks and do not imply it as a key contribution of our paper.\n\n--\n\n[1] He et al. \"Diffusionbert: Improving generative masked language models with diffusion models.\" 2022\n\n[2] Zheng et al. \"Structure-informed Language Models Are Protein Designers\". In ICML 2023\n\n$$$$\n\n> `Q2:` The paper lacks some important baselines and comparisons in Table 1. For example, why are there dashes on most AR transformer baselines? Besides, only small AR models (<100M) are compared against the 9.7B diffusion model. Other important baselines (e.g., GPT-structured decoder models, vanilla AR transformers at similar scale to diffusion variants (~80M), and pretrained + AR fine-tune) should be incorporated to compare against the performance of scaled diffusion models.\n\nThank you for your feedback and suggestions!\n\n**(1) About the dashes on most AR Transformers.**\nSorry for the confusion. We trained these RDM baseline models following previous practice in machine translation and summarization using Transformer-BASE architecture on WMT and Gigaword, and using Transformer-BASE-IWSLT on IWSLT14. \n\n**(2) About other important baselines you suggested.**\nIn Table 1, we have compared a XLM-R-BASE model trained from scratch in both RDM and autoregressive ways. We also conduct vanilla AR Transformer with similar model size with XLM-R-BASE as you suggested. \nTo make it clearer, we summarize the results as follows. \n| Model | achitecture | pretrained | IWSLT14 (BLEU) |\n| --- | --- | --- | --- |\n| AR  | vanilla encoder-decoder (~86M) | no | 33.56 |\n| AR (GPT-like) | XLM-R-BASE (decoder-only, ~86M) | no | 26.07 |\n| AR (GPT-like) | XLM-R-BASE (decoder-only, ~86M) | yes | 34.16 |\n| Diffusion | XLM-R-BASE (decoder-only, ~86M) | no | 28.79 |\n| Diffusion | XLM-R-BASE (decoder-only, ~86M) | yes | 34.10 |\n\nAs shown in the table, Diffusion LM attains similar performance with encoder-decoder AR LM but largely outperforms decoder-only AR LM with exact same architecture. A notable difference between the models in these two settings lies in the receptive field. Diffusion LM always has a global receptive field on the conditioning input, whereas AR can only perceive the condition with unidirectional attention if not equipped with an encoder. This supports our motivation to build diffusion language models for their favorable global receptive field.\n\n--\n\n[1] Tay et al. \"Ul2: Unifying language learning paradigms.\" In ICLR 2022.\n\n[2] Zhang et al. \"Examining scaling and transfer of language model architectures for machine translation.\" In ICML 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588570089,
                "cdate": 1700588570089,
                "tmdate": 1700589436574,
                "mdate": 1700589436574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fvpQWXFsQo",
                "forum": "Qn4HEhezKW",
                "replyto": "8RBtO1xV8b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> `Q3:` While referring BERT-like diffusion language models as 'decoder-only' models, (GPT-like) AR decoder models (which is currently most popular) is not compared or discussed throughout this study. Besides, I believe that authors need further clarifications on their definitions of 'decoder' models, since we do not generally refer BERT-like models as decoder-only models, which might cause some confusions.\n\nThank you for your questions! \n\n**(1) Clarifications on the definition of the \"decoder-only\" model.**\nWe presented our clarification on the concept of \"decoder-only\" models in our original submission, that in this paper, the decoder-only architecture, as a counterpart of encoder-decoder architecture, refers to the language model architecture that does not comprise a separate encoder stack to encode contexts/conditions. Under this definition, masked language models (e.g., BERT and XLM-R) are treated as decoder-only models. We will make more clarifications on this in the next version as you suggested. \n \n**(2) Why we used encoder-decoder models (T5) instead of GPT-like decoder-only models?**\n- As for comparisons in Table 1, conventional supervised models on machine translation and summarization are encoder-decoder Transformers, which typically outperform decoder-only Transformers. Table 1 supports this that, on IWSLT14, encoder-decoder AR or Diffusion LMs are consistently much better than decoder-only AR or Diffusion LMs. respectively. As such, we listed them there as strong references to demonstrate how well our model's performance is in comparison to commonly-used supervised baselines.\n\n- Similar reasons go with the comparisons in subsequent experiments regarding instruction-following. We consider T5 as a strong reference model because its representativeness outweighs the mismatch resulting from the architectural differences. As previous work suggests, encoder-decoder models like T5 outperform decoder-only models at moderate scales [1][2][3], and the gap diminishes as models scale up. Besides, the T5 series has both open-sourced pretrained models and instruction-tuned models on the FLAN instruction-tuning dataset at different scales similar to XLM-R. So we believe T5 can serve as a valid and strong baseline for the performance of autoregressive models.\n\n--\n\n[1] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n\n[2] UL2: Unifying Language Learning Paradigms\n\n[3] Examining Scaling and Transfer of Language Model Architectures for Machine Translation\n\n[4] Scaling Instruction-Finetuned Language Models\n\n[5] Larger-Scale Transformers for Multilingual Masked Language Modeling\n\n\n$$$$\n\n> `Q4:` The paper does not fully explore the configurations of Diffusion LMs, such as the number of diffusion steps, the noise level, and the length predictor. For example, would larger models remedy the demand for more steps & generalize better to predicted lengths or opposite?\n\nThank you for your suggestions. We conduct further analyses on these as you suggested. \n\n**(1) Number of diffusion steps.**\nHere are the results using different denoising steps of our models finetuned on IWSLT14. Using more sampling steps consistently brings performance improvement while the relative gains diminish.  The results indicate that we can use much fewer sampling steps than 50 to maintain most of the performance. Additionally, to reach similar performance, larger models require fewer steps (e.g., 10-step LARGE model surpasses the 50-step BASE models). \n\n| # Steps | 1 | 2 |  4 | 10 | 16 | 25 | 35 | 50 |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| BASE | 19.65 | 27.95 | 32.20 | 35.10 | 35.37 | 35.71 | 35.48 | 36.25 |\n| LARGE | 21.39 | 29.76 | 34.31 | 37.18 | 38.12 | 38.45 | 38.68 | 38.84 |\n| XL | 19.55 | 28.85 | 34.09 | 37.95 | 38.99 | 39.60 | 38.88 | 40.08 |\n\n**(2) About length predictor.**\nWe have tried two different parameterizations for length predictor in our preliminary study, either predicting the absolute lengths or predicting the relative length between target responses and their input prompts. We empirically found for machine translation both strategies performed similarly, while for other tasks, absolute length prediction is generally superior. As a result, we decided to use absolute length prediction for all later experiments. Please note that in this study, our focus is not to seek the best configurations but to demonstrate the effectiveness and possibilities of the Diffusion LMs for general-purpose language learning."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589414308,
                "cdate": 1700589414308,
                "tmdate": 1700589551616,
                "mdate": 1700589551616,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ixQGWTIPid",
                "forum": "Qn4HEhezKW",
                "replyto": "8RBtO1xV8b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer VY65,\n\nAs the time for author-reviewer discussion is soon to close, we are looking forward to your valuable feedback on our responses. We are also more than willing to address any further concerns you might have. \n\nAwaiting your response with anticipation and gratitude.\n\n      \n\nBest wishes,\n\nThe Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710636198,
                "cdate": 1700710636198,
                "tmdate": 1700710636198,
                "mdate": 1700710636198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KDDLMGhpDA",
            "forum": "Qn4HEhezKW",
            "replyto": "Qn4HEhezKW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8915/Reviewer_bJbT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8915/Reviewer_bJbT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to pre-train a masked language model, and fine-tune this model as a generative discrete absorbing diffusion model for translation, summarization, and instruction-following tasks. Experiments are conducted using the XLM-RoBERTa family of MLMs (86M, 304M, 2.8B, 9.7B variants) finetuned as generative models on IWSLT14 and WMT14 (translation) Gigaword-10K (summarization) and Flan 2022 (instruction-following) datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper contains a diverse set of experiments using finetuned MLMs as diffusion models, which complements previous work using this methods (e.g., DiffusionBERT). The experiments are described well, and appear to be well-executed. The results show consistent task performance improvements using the larger-scale models, which is consistent with broader observations of the importance of scale in the ML community (although perhaps unsurprising at this point)."
                },
                "weaknesses": {
                    "value": "There does not seem to be a significant methodological contribution. As far as I can tell, the methodological approach is the same as previous work adapting pre-trained MLM's for discrete diffusions [1]; the difference here is the use of the XLM-RoBERTa family of models, vs. previous results using BERT. The paper is clear that its contributions are about scale, not methodology, but it's a little frustrating that so much of the paper (up until Section 4) is dedicated to summarizing the contributions of previous work in the field.\n\nThis is not the first work to study the scaling behavior of diffusion models for text. For example, multiple diffusion models are trained in [2] together with a scaling law for these models and comparisons to scaling laws for baseline autoregressive models. Because this paper's central contribution is an empirical study of scaling behavior of text diffusions, a more thorough discussion of previous work on scaling text diffusions would be helpful.\n\n[1] DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models\nZhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, Xipeng Qiu\n\n[2] Likelihood-Based Diffusion Language Models\nIshaan Gulrajani, Tatsunori B. Hashimoto"
                },
                "questions": {
                    "value": "I am curious how the approach described in this paper compares to direct MCMC sampling from an MLM. e.g., the approach described in [3]. To be clear, I am quite sympathetic to the idea of finetuning to directly supervise generation rather than inference-time MCMC; but I do think some discussion of this alternative line of work and connections/tradeoffs vs. the proposed approach would be enlightening and strengthen the discussion in Section 3.2.\n\n\"There exist large discrepancies in architecture and data engineering between our foundation models, XLM-R (Conneau et al., 2019), which were built years ago, and the state-of-the-art large language models like LLaMA (Touvron et al., 2023a;b).\"\n\nI do not work at a scaling lab, so my insight into these questions is limited, but one explanation I have heard for the lack of recent developments in the MLM space is that the performance of these models simply does not scale well in comparison to autoregressive LM's. I'm curious what you think of that claim, and whether (potential) limitations in our ability to scale the performance of the base MLM could create a ceiling on the performance of diffusion models trained using masked lanuage modeling as a pre-training objective.\n\n\n[3] Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings\nKartik Goyal, Chris Dyer, Taylor Berg-Kirkpatrick"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8915/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698946621997,
            "cdate": 1698946621997,
            "tmdate": 1699637121702,
            "mdate": 1699637121702,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eTcLzvsL7b",
                "forum": "Qn4HEhezKW",
                "replyto": "KDDLMGhpDA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your reviews! We would like to address your concerns below. Please have a check and we are happy to address any further feedback!\n\n$$$$\n\n> `Q1:` There does not seem to be a significant methodological contribution. As far as I can tell, the methodological approach is the same as previous work adapting pre-trained MLM's for discrete diffusions [1]; the difference here is the use of the XLM-RoBERTa family of models, vs. previous results using BERT.  The paper is clear that its contributions are about scale, not methodology, but it's a little frustrating that so much of the paper (up until Section 4) is dedicated to summarizing the contributions of previous work in the field.\n\nThanks for your constructive feedback. We would like to also make clarifications on our contributions. \n\nWhile our work is not the first exploration of generative ability of masked LMs, it is the first to successfully implement and demonstrate the practical effectiveness by scaling these models. The major contribution of our study is therefore to show that the Diffusion LMs can also possess promising generative capabilities that can be elicited from large-scale pretrained MLMs by diffusive adaptation, not only for each individual specialized purpose as in DiffusionBERT, but also generalist abilities like instruction following, in-context learning and promise of reasoning, which are nowadays what the community really cares and excited about LLMs. And this is exactly what we think of something new and surprising, and what we are thrilled to share with the community.\n\nRegarding the content up to Section 4, we believe such summaries are essential. In fact, these contents serve two purposes. Firstly, we try to make necessary concepts clear and easy to follow for audiences from different backgrounds, as there are still limited successful explorations on developing large language models under the probabilistic framework of diffusion models, which are mostly studied in vision domains. Secondly, we try to make it clear (1) how diffusion language models and masked language models are formally connected, and (2) how this connection helps our goal of studying the scaling of diffusion language models.\n\n$$$$\n\n> `Q2:` This is not the first work to study the scaling behavior of diffusion models for text.  For example, multiple diffusion models are trained in [2] together with a scaling law for these models and comparisons to scaling laws for baseline autoregressive models. A more thorough discussion of previous work on scaling text diffusions would be helpful.\n\nWhile we agree that our work is not the first exploration of scaling diffusion language models, it is the first to explore the techniques of discrete diffusion language models, and successfully demonstrate the practical effectiveness of scaling the models. \n- In detail, the mentioned \"Likelihood-Based Diffusion Language Models\" paper [2] studies pretraining **continuous** diffusion language models while we explore **discrete** diffusion language models that suit the **discrete nature** of languages better and empirically perform better in language generation benchmarks like machine translation [1]. \n- Additionally, [2] scales diffusion language models up to 1B and mainly investigates the scaling law of likelihood estimation (ppl/elbo), with a showcase of the promise of prefix completion and infixing. In comparison, we study Diffusion LMs with scaling up to 10B. More importantly, we benchmark on widely-used LLM testbeds and verify the general-purpose generative abilities of large diffusion language models, which is the crucial property that motivates scaling. \n- We believe that, together with the promising findings in literature [2], our study provide valuable insights into the scalability of diffusion language models and their potential as a viable alternative in tackling generative language tasks across the board.\n\n--\n\n[1] A Reparameterized Discrete Diffusion Model for Text Generation.\n\n[2] Likelihood-Based Diffusion Language Models\n\n$$$$\n\n> `Q3:` How the approach described in this paper compares to direct MCMC sampling from an MLM. I do think some discussion of this alternative line of work and connections/tradeoffs vs. the proposed approach would be enlightening and strengthen the discussion in Section 3.2.\n\nThank you for your insight question and suggestion. The MCMC sampling methods mentioned adopt Gibbs sampling, which generates one token at a time from left to right. This is similar to autoregressive models. On the contrary, our diffusion models are non-autoregressive and can generate several tokens in parallel without a predefined generation order. In addition, it enables flexible reasoning orders and backtracing as our discussion on reasoning illustrates. We will include the discussion with MCMC sampling as you suggested in our next version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588155106,
                "cdate": 1700588155106,
                "tmdate": 1700627345253,
                "mdate": 1700627345253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2HjNj8o8JV",
                "forum": "Qn4HEhezKW",
                "replyto": "KDDLMGhpDA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer bjbT,\n\nWith the deadline for author-reviewer discussions approaching, we sincerely appreciate your insightful feedback and are delighted to answer further questions if remain. \n\nWe look forward to your reply and thank you for your discussion.\n\nBest,\nThe Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710400040,
                "cdate": 1700710400040,
                "tmdate": 1700710400040,
                "mdate": 1700710400040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XJVZukrNgO",
            "forum": "Qn4HEhezKW",
            "replyto": "Qn4HEhezKW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8915/Reviewer_gyd2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8915/Reviewer_gyd2"
            ],
            "content": {
                "summary": {
                    "value": "This work presents an initial exploration into the effects of scale and fine-tuning on large diffusion language models. In particular, the work is interested in answering whether scale and instruction-tuning for large diffusion models can unlock similar generalization capabilities that are observed in more standard, auto-regressive LMs.\n\nTo test this, the authors use Reparameterized Discrete Diffusion models (RDM), a method from prior work, which is able to adapt pre-trained masked language models (MLMs) such as BERT into generative diffusion models. The authors use pre-trained XLM-R models, of various sizes, as the backbone of their diffusion models.\n\nThe authors use RDM to fine-tune XLM-R on 3 generative NLP tasks (2 translation tasks, and one summarization task). They find that: (a) diffusion models adapted from pre-trained models outperform randomly initialized XLM-R models, suggesting that RDM can leverage pre-training; (b) that performance on all 3 tasks improves with XLM-R backbone size, suggesting that RDM improves with model scale.\n\nThe authors next explore the zero-shot capabilities of diffusion LMs after fine-tuning on an instruction-tuning dataset (FLAN), and find that these models are able to achieve relatively strong zero-shot capabilities after instruction-tuning. Finally, the authors explore the ability of these models to perform in-context learning after instruction-tuning, and once again find that diffusion LMs can perform comparably to other in-context LMs after instruction-tuning on certain tasks.\n\nThe authors end with an interesting qualitative evaluation of a diffusion LMs reasoning order over a complex reasoning task, showing an example where their diffusion LM generates in an order that holds aligns with a _causal order_, which differs from autoregressive models which must generate a response linearly."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The work demonstrates that diffusion models can be adapted from other pre-trained models, and, when done at scale, can achieve strong performance on certain downstream tasks both via fine-tuning as well as in-context learning.\n- The observation that \n- The paper is well-written; in particular, it clearly explains prior work on how diffusion LMs operate, what their issues are, and what RDM is.\n- The final example, which serves to highlight the differences between diffusion generation and autoregressive generation for chain-of-thought reasoning is very interesting and a novel observation to the best of my knowledge."
                },
                "weaknesses": {
                    "value": "- The finding that pre-training helps an adapted diffusion model, and that pre-trained model size influences the adapted models generalization, is perhaps not surprising. In particular, (a) it is already known XLM-R performance generally improves with scale on several downstream tasks, so the benefit of scale likely comes from the pre-training procedure, rather than the diffusion adaptation and (b) given that pre-training helps over no pre-training, it is not very surprising that _better_ pre-trained models help more.\n- The paper claims to be interested in how diffusion LMs scale with respect to data, but that is not tested in this work. What is tested is how diffusion LM capabilities scale w.r.t. pre-trained model size and how diffusion LMs respond to instruction-tuning.\n- The final note about how diffusion models generate causally is very interesting, but is only explored with a single qualitative example.\n- The results are sometimes a bit confusing to parse.\n   - In particular, for Table 1, there is a number of results missing, particularly for XLM-R-BASE (AR), which is perhaps the most comparable portion of the table as it directly compares autoregressive generation to RDM on the exact same model. It's also not clear what the value of having the Encoder-Decoder models there are, since the models are pre-trained differently, have different sizes, and different architectures; the take-away from this comparison is very unclear.\n   - I find it a bit surprising that the number of examples used in in-context learning have little effect on model performance (and sometimes have a very negative effect). It would be very useful to have these plotted against a model that is known to perform well at in-context learning, to see if the same trends hold. As it stands, the results suggest to me that the diffusion LMs presented are good zero-shot learners, but poor in-context learners."
                },
                "questions": {
                    "value": "What is the reason for missing fields in Table 1? Are these rows results obtained from other papers? If so, I feel that should be made clearer.\n\nWould it be possible to have Flan-T5 compared to Flan-XLM-R for various numbers of in-context exemplars?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8915/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699132488254,
            "cdate": 1699132488254,
            "tmdate": 1699637121553,
            "mdate": 1699637121553,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EZlAQFVU0P",
                "forum": "Qn4HEhezKW",
                "replyto": "XJVZukrNgO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for the constructive suggestions! We would like to address your concerns below. Please have a check and we are happy to address any further feedback!\n\n$$$$\n\n> `Q1:` It's perhaps not surprising to find the performance of the models improves with model scaling and pretraining. (a) it is already known XLM-R performance generally improves with scale on several downstream tasks, so the benefit of scale likely comes from the pre-training procedure, rather than the diffusion adaptation and (b) given that pre-training helps over no pre-training, it is not very surprising that better pre-trained models help more.\n\nThank you for your comment! We would like to make clarifications on this as follows:\n- **MLM Pretraining does not permit generative capabilities.** Despite pretraining masked LMs, such as XLM-R, improving downstream tasks with scale, these tasks are generally predictive tasks (regression/classification for NLU) as MLMs are typically thought of as encoders. Whether these pretrained MLMs can solve more tasks in general in a generative manner like causal LMs/GPTs, however, remains under-explored and non-trivial to achieve. Without diffusive adaptation as a generative surgery, only MLM pretraining at scale does not help with generative capabilities on its own.\n- Although it is common to expect that models will perform better with model scaling and pretraining, **it is imperative not to assume this as a given without the backing of empirical results**. \nIn particular, previous work on continuous diffusion language models has found it challenging to fit large-scale datasets [1,2,3]. We suggest that our success in scaling diffusion language models is attributed to both adopting the formalization of discrete diffusion models that fit language tokens well and utilizing pretrained masked language models which are well-tested to be scalable. \n- **Scaling is not only about performance improvement but, more importantly, is about eliciting general-purpose ability.** The major contribution of our study is therefore to show that the Diffusion LMs can also possess promising generative capabilities that can be elicited from large-scale pretrained MLMs by diffusive adaptation, not only for each individual specialized purpose as in DiffusionBERT, but also generalist abilities like instruction following, in-context learning and promise of reasoning, which are nowadays what the community really cares and excited about LLMs. And this is exactly what we think of something new and surprising, and what we are thrilled to share with the community.\n\n[1] Yuan et al. \"Seqdiffuseq: Text diffusion with encoder-decoder transformers.\" 2022\n\n[2] Gao et al. \"Difformer: Empowering diffusion model on embedding space for text generation.\" 2022\n\n[3] Ye et al. \"Dinoiser: Diffused conditional sequence learning by manipulating noises.\" 2023\n\n\n> `Q2:` The scaling with respect to data is not tested.\n\nOur purpose is to explore whether pretraining on large-scale data can also empower Diffusion LMs, like their autoregressive counterparts, we mainly focused on diffusive adaptation from pretrained masked LMs (i.e., XLM-Roberta) rather than building Diffusion LMs from scratch for the purpose of highly efficient proof-of-concept. As we have demonstrated in Section 4.2 and Table 1, pretraining on large-scale data gives rise to significant gains for RDM-based Diffusion LMs over the same models without pretraining (e.g., 34.10 vs 28.79 on IWSLT14). We will leave the analysis of the effects of pretraining on different data scales from scratch in the next version.\n\n> `Q3:` The final note about how diffusion models generate causally is very interesting, but is only explored with a single qualitative example.\n\nThank you for your interest and valuable suggestions for a more in-depth study on causal generation order. In our paper, we mention two aspects showing promise in eliciting reasoning ability. \n- Qualitatively, we showed the causal generation order of the models, which indicates correct dependency in reasoning. \n- Quantitatively, in Section 4.5, we found the accuracy of our models on GSM8K can attain a non-trivial improvement from 5.6% to 12.8% after tuning on chain-of-thought data [1] of GSM8K distilled from `code-davinci-002`. This suggests the inability to perform reasoning should be attributed to insufficient model capability instead of the diffusion language modeling paradigm itself.\n\nWe believe our findings encourage effort in further scaling and improving the capability of diffusion language models, which facilitates exploring the concrete benefits of causal generation order with the support of sufficiently strong reasoning and planning ability.\n\n[1] Fu et al. Specializing smaller language models towards multi-step reasoning. ICML 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587284901,
                "cdate": 1700587284901,
                "tmdate": 1700589486120,
                "mdate": 1700589486120,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m2LdzqGegj",
                "forum": "Qn4HEhezKW",
                "replyto": "XJVZukrNgO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8915/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer gyd2, \n\nAs the author-reviewer discussion deadline is quickly approaching, we would be very grateful for your valuable feedback on our responses, and be very happy to answer any further questions you may have. \nLooking forward to hearing from you, and many thanks!\n\nBest,\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8915/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710003754,
                "cdate": 1700710003754,
                "tmdate": 1700710003754,
                "mdate": 1700710003754,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]