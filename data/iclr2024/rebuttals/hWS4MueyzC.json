[
    {
        "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World"
    },
    {
        "review": {
            "id": "O1gepjsdND",
            "forum": "hWS4MueyzC",
            "replyto": "hWS4MueyzC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5053/Reviewer_Gtsv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5053/Reviewer_Gtsv"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Bongard-OpenWorld, a benchmark designed to evaluate a system's real-world few-shot reasoning abilities. By incorporating open-world concepts and real images into the classical Bongard Problems, this benchmark serves as a litmus test for current limitations in visual intelligence, motivating further research toward enhancing few-shot reasoning in visual agents. The paper conducts a comprehensive assessment, examining the effectiveness of various Vision-Language Models (VLMs) and Large Language Models (LLMs), as well as proposing a neuro-symbolic reasoning approach tailored for this benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall the paper is well written and provides the reader with good insight on why the availability of the proposed benchmark is good for the community as it promotes research into the few-shot reasoning capabilities of current black box deep learning models. The paper presents a tough benchmark that tests current systems on their ability to reason about free-form concepts in a few-shot manner by identifying the common concept in a positive set and distinguishing it from the negative set. \n\nThe paper introduces a robust benchmark that assesses systems' ability to perform few-shot reasoning on free-form concepts. It challenges models to identify commonalities in positive sets while distinguishing them from negative sets, enhanced by the inclusion of distractors and hard negatives in the dataset curation process. The evaluation framework covers a wide spectrum, encompassing four distinct approaches: few-shot learning, combined LLM+VLM in single and multiple steps, and a novel neuro-symbolic architecture.\n\nThe evaluation setup utilized in the paper is comprehensive and includes the evaluation of four different kinds of approaches that include a few shot learning approaches, LLM+VLM in a single step, LLM+VLM in multiple iteration steps, and finally a proposed neuro-symbolic architecture."
                },
                "weaknesses": {
                    "value": "I would like to look at more variants of the neurosymbolic approach proposed in this work. One avenue worth exploring is a line of research that leverages domain knowledge, such as knowledge graphs, to identify pertinent concepts within an input. Active nodes within the graph could then be employed to pinpoint the common concept within the positive set of images.\n\nThe evaluations used in this paper though really comprehensive, miss out on some more ways of evaluation. VLM-based approaches, like GPT4(V), that directly take images as input and can be prompted to obtain the desired input, could be used to identify the relevant concept from a collage of images given together. Since current VLM/LLM approaches are very susceptible to the way they are prompted, it is very important to prompt engineer them in a number of ways and then identify the best working one."
                },
                "questions": {
                    "value": "Table 2 provides a good overview of the performance of various approaches on the proposed benchmark. I would like to see more explanation of the reasoning behind the performance of these approaches. Like for example, Flamingo/ChatGPT/Otter performs significantly worse than the few-shot learning approach SNAIL despite Flamingo/Otter using the same image encoder. \n\nIncluding a section on failure case analysis for different approaches would be instrumental for the readers in identifying specific challenges and guiding improvements for tackling them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5053/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5053/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5053/Reviewer_Gtsv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697930065115,
            "cdate": 1697930065115,
            "tmdate": 1699636495284,
            "mdate": 1699636495284,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t3SkhRAQoq",
                "forum": "hWS4MueyzC",
                "replyto": "O1gepjsdND",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Gtsv (1/N)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your time and constructive comments. Below, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n> - I would like to look at more variants of the neurosymbolic approach proposed in this work. One avenue worth exploring is a line of research that leverages domain knowledge, such as knowledge graphs, to identify pertinent concepts within an input. Active nodes within the graph could then be employed to pinpoint the common concept within the positive set of images.\n\nWe thank you for the suggestion! Indeed, leveraging KG is a brilliant idea of enhancing the neurosymbolic approach presented in this paper, which currently still replies on perception from other visual foundation model like BLIP. Please feel free to list a few papers that are relevant to the idea you mentioned above, we would love to cite and discuss them in the final version of our paper and if possible, benchmarking them on our dataset. Again, thanks for the insight.\n\n\n> - The evaluations used in this paper though really comprehensive, miss out on some more ways of evaluation. VLM-based approaches, like GPT4(V), that directly take images as input and can be prompted to obtain the desired input, could be used to identify the relevant concept from a collage of images given together. Since current VLM/LLM approaches are very susceptible to the way they are prompted, it is very important to prompt engineer them in a number of ways and then identify the best working one.\n\nYour insights are notably forward-thinking, and we have already prepared results using GPT-4V. All parameters are aligned with GPT-4, except that the called model is *\"gpt-4-vision-preview\"* and the parameter \"detail\" set to \"high\" for fine-grained image representations. We splice raw images together in a manner similar to the examples in Appendix H, using only one query image each time, which is then inputted for GPT-4V reasoning. Here are the results (and they have been merged to the updated version as well). It can be seens that Bongard-OpenWorld remains a great challenge even with GPT-4V. We commit to exploring this further as our prompt engineering might not be that comprehensive given the limited time (and the non-stable performance of their API as well!).\n\n| method | image representation |  short concept | long concept | CS\u2217 concept | non-CS\u2217 concept | avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| OpenFlamingo | OpenCLIP | 50.0 | 48.4 | 50.9 | 48.6 | 49.3 |\n| Otter | OpenCLIP | 49.3 | 49.3 | 48.9 | 49.4 | 49.3 |\n| GPT-4 | InstructBLIP | 67.3 | 59.7 | 59.3 | 65.6 | 63.8 |\n| GPT-4V | Raw Images | 54.6 | 53.3 | 50.9 | 55.2 | 54.0 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417852036,
                "cdate": 1700417852036,
                "tmdate": 1700417852036,
                "mdate": 1700417852036,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pUguSDVwV8",
                "forum": "hWS4MueyzC",
                "replyto": "qDFsc9d5QU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_Gtsv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_Gtsv"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the consideration of my comments.\n\nI appreciate the extensive experimentation conducted with the latest VLM models (GPT4V), the results clearly outline the competitiveness of the proposed benchmarks. The additional sections for the broader impact statement and failure case analysis in the Appendix are also useful for the readers. In terms of the neuro-symbolic approaches that utilize structured knowledge to extract relevant visual concepts from the scene, you may want to look at the papers mentioned below.\n\n[1] K. Marino, R. Salakhutdinov and A. Gupta, \"The More You Know: Using Knowledge Graphs for Image Classification,\" 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 2017, pp. 20-28, doi: 10.1109/CVPR.2017.10.\n\n[2] Bhagat, S., Stepputtis, S., Campbell, J., & Sycara, K.P. (2023). Sample-Efficient Learning of Novel Visual Concepts. ArXiv, abs/2306.09482."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425075417,
                "cdate": 1700425075417,
                "tmdate": 1700425075417,
                "mdate": 1700425075417,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zNzO5zAVR9",
            "forum": "hWS4MueyzC",
            "replyto": "hWS4MueyzC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new benchmark called Bongard-OpenWorld, which focuses on open-world visual concept understanding. The task is to classify a query image to belong to one of two sets of images. The positive set contains 6 images depicting a common concept C, such as \"animals are running\". The negative set contains 6 images of similar concepts but not exactly matching C, e.g., showing a standing animal, or a running robot. The difficulty of this benchmark comes from the two sets sharing common objects or semantics, such that nuances in the semantic concepts need to be understood to perform well. The authors also evaluate relevant existing methods and show that there is still a large gap between current methods and human performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The new benchmark and problem setting tackles an important shortcoming of current methods to understand fine-grained semantic concepts in contrast to hard negatives.\n- An extensive set of existing methods have been evaluated showing that the best models (64%) are still far from human performance (91%). This makes it a challenging setting for new methods to be developed in the future.\n- Evaluations include a few-shot and zero-shot setting, and several different approaches to combine vision-language models and large language models to solve the task."
                },
                "weaknesses": {
                    "value": "- The presentation and writing could be clearer, making some parts of the paper difficult to understand, especially around the creation of the dataset in Sec. 2 and the precise problem setting. For example, it was not immediately clear that the labels of the two sets (positive, negative) are given and do not need to be inferred. The query image can belong to either, but the name \"positive\" suggests that this is the GT set of the query image, which is not the case. I am adding more clarifying questions in the questions section below.\n- The problem setting is imbalanced. The positive set corresponds to a single concept while the negative set does not, but instead contains a subset of the complete concept. While this is not necessarily an issue and can be a design choice, there is no justification why this choice has been made. For instance, why not have both sets correspond to a single concept where the contrasting sets are close in semantics to make it a hard problem? Similarly to how different splits are evaluated in Table 2, it would have helped to show the performance of positive query images vs. negative query images in order to understand if this imbalance makes positive/negative queries easier/harder.\n- While a lot of models have been evaluated on the proposed benchmark, a natural baseline is missing: computing the image similarity between the query image and the two sets. For instance, one can use any pre-trained image encoder (CLIP, Dino, etc.) or image-to-image retrieval method and use the mean similarly of the image embeddings per set to make a prediction. Using captioning models and LLMs seems to introduce complexity while at the same time discarding fine-grained image information by only relying on text to make the decision.\n- With around 1K tasks it is a rather small dataset. Hence, focusing on the \"zero-shot\" setting without involving training might be the better use case.\n- While it is true that the benchmark contains a large variety of concepts, positioning it heavily as an \"open-world\" and \"open-vocabulary\" task could be a bit misleading as the core problem is to identify whether an image came from set A or set B. The optional task of naming the concept is most fitting for \"open-world\", but it serves a minor role in the paper."
                },
                "questions": {
                    "value": "- Have you thought about not providing the labels \"positive\" and \"negative\" for the two sets to the methods? Why have you chosen this setup?\n- In Sec 2.1: What is a grid in this context? What is grid sampling? How do you define \"concept tuples\"? Neither the main paper, nor the supplementary clarifies this sufficiently.\n- How do you ensure that the dataset does not contain duplicate concepts? I assume this is the case because in Table 1, it is reported that the dataset has 1.01K concepts and 1.01K tasks.\n- What are the exact instructions the annotators were given? For instance, when \"annotators are instructed to write visual concepts by following a predefined set of categories illustrated in Table 7\" and when \"they are also asked to combine these challenging concepts with those mined from CC-3M\" (Sec. 2.1).\n- Images are collected by using an online search based on the concepts. What is the license of the images collected? Do the authors have the rights to distribute the images?\n- In Sec. 2.2. you write: \"the annotators are then asked to provide two sets of candidates for positive and negative queries\". How many images are collected here for possible query images? Why choose only one positive and negative image as query in the end?\n- Does defining the concepts of category 0 (from CC3M) undergo any crowd-sourcing or is it fully automated?\n- Is there performance difference between positive and negative queries?\n- In Figure 2c, both x and y-axis should be labeled. What is the scale/size of the number of concepts (x-axis)? What is the unit of the numbers on the y-axis?\n- What is meant by \"we report the overall accuracy of all models\". Does Table 2 report test set accuracy or accuracy over the whole dataset, i.e., including training samples?\n- Why are concepts from CC3M considered non-commonsense?\n- How is ChatGPT finetuned (Table 2)? Does this use the finetuning API of OpenAI? More details would help make this more reproducible.\n\nComments/suggestions:\n- Table 2 includes methods that use training data to update NN weights and others that do not update weights (\"zero-shot\" setting). It would be much clearer if the table indicates which models use training data.\n- The following phrase appears 3 times in the manuscript. I suggest to to reduce this repetition and rephrase it according to the context. \"We even designed a neuro-symbolic reasoning approach that reconciles LLMs & VLMs with logical reasoning to emulate the human problem-solving process for Bongard problems\".\n- Sec. 3.2 (at the end) promises captioning metrics, but they do not appear in the main paper, only in the supplementary.\n- The formatting of Table 7 is confusing. It would be better to clearly separate the left half from the right half, or simply just make it 10 rows."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5053/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5053/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610481835,
            "cdate": 1698610481835,
            "tmdate": 1700737176212,
            "mdate": 1700737176212,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ePyoBKYmWe",
                "forum": "hWS4MueyzC",
                "replyto": "zNzO5zAVR9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CSkd (1/N)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your time and constructive comments. Below, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n\n> - The presentation and writing could be clearer, making some parts of the paper difficult to understand, especially around the creation of the dataset in Sec. 2 and the precise problem setting. For example, it was not immediately clear that the labels of the two sets (positive, negative) are given and do not need to be inferred. The query image can belong to either, but the name \"positive\" suggests that this is the GT set of the query image, which is not the case. I am adding more clarifying questions in the questions section below.\n\nThanks for highlighting this point, we have revised our main paper for greater clarity. For your specific question here, the labels of the support set are given and without anything else. Each Bongard Problem includes both a positive and a negative query image.\n\n> - The problem setting is imbalanced. The positive set corresponds to a single concept while the negative set does not, but instead contains a subset of the complete concept. While this is not necessarily an issue and can be a design choice, there is no justification why this choice has been made. For instance, why not have both sets correspond to a single concept where the contrasting sets are close in semantics to make it a hard problem? \n\nOur problem setting adheres strictly to the original Bongard Problem (a classic puzzle of computer vision and pattern recognition: https://en.wikipedia.org/wiki/Bongard_problem), which include several features:\n\n- The support set comprises only images of positive and negative with their labels (just indicates whether it belongs to the positive or negative set).\n- Images from the positive set should depict the same concept, while those belong to the negative set should not depict this concept and the concept depicted by the negative images can be varied -- this could lead to an \"imbalance\" on the number of concepts but yes, this is a designed choice made by the original Bongard problem. \n\nBut Bongard-OpenWorld also introduces some new features:\n\n- First of all, both the positive and negative images includes distractor concepts (as we've demonstrated in Figure 1 of the original paper) -- that is, images from the positive set are in fact could also depict many more concepts besides the ground truth, alleviating the \"imbalance\" issue. \n\n- Secondly, as you can see from the example problems we showed in the Appendix H, when we picking the negative images, we also seek to make them share some common concepts, this is relevant to the solution you mentioned above and indeed help create a hard problem.\n\n- Finally, we even make positive and negative images could overlap on the concepts they depict. For example, given the ground truth concept is \"red tie\", both positie and negative concepts can depict \"dog\", resulting \"dog wearing a red tie\" (positive) and \"dog wearing a blue shirt\" (negative).\n\nAgain, we thank you for raising this issue, and we believe our design in Bongard-OpenWorld not only respect the original Bongard problem setting, but also mitigate the possible drawbacks, including imblancing and easy problems.\n\n> - ...Similarly to how different splits are evaluated in Table 2, it would have helped to show the performance of positive query images vs. negative query images in order to understand if this imbalance makes positive/negative queries easier/harder.\n> - Is there performance difference between positive and negative queries?\n\nWe thank you for your suggestion!  Here are some additional results singling out the accuracy on positive and negative queries. As you can see, there might be slight difference for a single model, but we did not observe a pattern or preference towards positive or negative queries globally. Therefore Bongard-OpenWorld does not appear to exhibit a strong imbalance issue.\n\n| method | image representation | aux. task? | positive query | negative query | avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| Meta-Baseline | OpenCLIP | &#10004; | 59.5 | 53.0 | 56.3 |\n| MetaOptNet | OpenCLIP | &#10004; | 57.9 | 57.0 | 57.5 |\n| ProtoNet | OpenCLIP | &#10004; | 58.2 | 58.7 | 58.5 |\n| SNAIL | OpenCLIP | &#10004; | 61.2 | 66.8 | 64.0 |\n| GPT-4 | InstructBLIP | &#10004; | 67.4 | 60.2 | 63.8 |\n\n> - Have you thought about not providing the labels \"positive\" and \"negative\" for the two sets to the methods? Why have you chosen this setup?\n\nAs we mentioned above, not proving the positive or negative labels could make the setting largely deviate from the original Bongard problem setting, which we choose to respect when creating this dataset. Also, we don't see a clear advantages of doing such. That include the \"imbalancing\" issue in your comment -- our response above has shown that our dataset does not appear to have a strong imbalance issue."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417563498,
                "cdate": 1700417563498,
                "tmdate": 1700417563498,
                "mdate": 1700417563498,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FGrPKZg5d9",
                "forum": "hWS4MueyzC",
                "replyto": "fNf3u6UnKB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their comprehensive response addressing my questions. Many changes have already improved the manuscript. Below, I am sharing follow-up comments on some of the responses.\n\nSeveral of my questions have been answered by stating that some design choices of the proposed setting have been made in order to adhere to the original Bongard problem formulation. These questions include:  \n> [...] For instance, why not have both sets correspond to a single concept where the contrasting sets are close in semantics to make it a hard problem?\n\n> Our problem setting adheres strictly to the original Bongard Problem (a classic puzzle of computer vision and pattern recognition: https://en.wikipedia.org/wiki/Bongard_problem), which include several features:  \n> - [...]  \n> - Images from the positive set should depict the same concept, while those belong to the negative set should not depict this concept and the concept depicted by the negative images can be varied -- this could lead to an \"imbalance\" on the number of concepts but yes, this is a designed choice made by the original Bongard problem.\n---\n> Have you thought about not providing the labels \"positive\" and \"negative\" for the two sets to the methods? Why have you chosen this setup?\n\n> As we mentioned above, not proving the positive or negative labels could make the setting largely deviate from the original Bongard problem setting, which we choose to respect when creating this dataset. [...]\n---\n> [...] Why choose only one positive and negative image as query in the end?\n\n> [...] The choice of only using 1 query is to align with the original Bongard problem, which we respect in Bongard-OpenWorld. \n\nRegardless of how the original Bongard problem was formulated, I think important design choices of contemporary benchmarks should be justified not only because the challenges of computer vision change over time, but also because it is important to understand what we are measuring when evaluating models on new benchmarks.\n\nTo further make a point, I don't think Bongard-OpenWorld actually strictly follows the original setting of Bongard problems, which, by the way, might not be well defined by the linked Wikipedia article. For instance, [https://www.oebp.org/welcome.php](https://www.oebp.org/welcome.php) describe the problem as:\n> A Bongard Problem traditionally includes  \n> - a dividing line,  \n> - six images on the left side of the dividing line, and  \n> - six images on the right side of the dividing line.  \n> Most important, there is some simple description that fits all the images on the left side (but none on the right) and, oppositely, a simple description that fits all the images on the right side (but none on the left). \n\nAnd in M. M. Bongard's \"Pattern recognition\" from which these problems originate, they follow the above description and there is always an simple common description for all images on either side. For example, the solutions of the first 3 problems from Appendix 3 are\n> Empty picture <-> Not empty picture  \n> Large figures <-> Small figures  \n> Outline figures <-> Solid figures  \n\nSo I would argue, originally the two sets are symmetrical, not labeled (positive/negative), and there is actually no \"query\" image, but the task is to simply name the common concept on either side.\n\nMy point is: It is fine to adhere or deviate from a previously established setting, but there should still be a proper reason behind each choice such that the benchmark targets a clear goal in computer vision research. For some of my original questions above, good reasoning is currently missing.\n\n>  Here are some additional results singling out the accuracy on positive and negative queries.\n\nI think these results should be included in the paper (at least in the appendix).\n\n> Please note that, even with other backbone (ex. DINO), we believe the problem should still exist, as this natural baseline can be closely resemble to ProtoNet, which also fine-tunes the backbone. In our experiment, we already demonstrate its inability on Bongard-OpenWorld, even with non-CLIP backbone.\n\nWhile I understand the motivation behind adversarial query selection was to make the benchmark challenging, it might also introduce this rather unnatural behavior and penalty for reasonable approaches that use contrastively trained VLMs (such as CLIP) and any models that build on top of pre-trained CLIP models. I think testing a separate baseline (e.g. DINO) could help better understand this dynamic. For full transparency, I suggest including the \"natural baseline\" into the paper.\n\n> [...] both problems from human annotated commonsense concepts or the mined CC-3M concepts can be labelled as commonsense problems.\n\nI agree with this sentiment. From my understanding, there are still some references where I suggest to rename \"non-CS\": Table 1, Figure 2, Table 7, Sec. 2.3."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502959147,
                "cdate": 1700502959147,
                "tmdate": 1700502959147,
                "mdate": 1700502959147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dnUulMFrrC",
                "forum": "hWS4MueyzC",
                "replyto": "zNzO5zAVR9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the promot reply! We really appreciate it."
                    },
                    "comment": {
                        "value": "Dear Reviewer CSkd,\n\nWe would like to thank you for your prompt reply and we're happy to see some of your concerns have been addressed. Per the remaining issues, we would like to offer some further clarifications:\n\n> My point is: It is fine to adhere or deviate from a previously established setting, but there should still be a proper reason behind each choice such that the benchmark targets a clear goal in computer vision research. For some of my original questions above, good reasoning is currently missing.\n\nWe thank you for posting the references to the original Bongard problem. Indeed, our implementation slightly deviates from the original Bongard problem, in terms of the following:\n\n- The two sets are not symmetrical -- in our case, only set A depicts a shared concept, while set B does not\n\n- We add a query image and switch the main task from recognizing the concept (we still have it, as the captioning task, but it is not the \"main task\" that is presented in the \"main result\" section of the paper) to determine whether the query belongs to set A or set B.\n\n- Some additional features on image selection, as we mentioned in the response above.\n\nNow we dive into some reasoning on the various design choices (why making these modifications, why introducing several additional features, etc), including what we would like to benchmark, and how is this important to computer vision research.\n\n1. both sets with labels, also an additional query\n\n    To be more accurate, this \"non-symmetrical\" setting is effectively inherited from two seminal works: Bongard-LOGO[1] and Bongard-HOI[2]. We believe the idea behind this is: **the original Bongard problem is rather challenging**, as it requires the machine to directly produce natural language descriptions of the concept after reading images from both sets. Even with today's powerful VLMs, this is still quite difficult. Therefore, it is natural to simplify the question a bit, let's say, turning the original \"concept regression\" problem into classification, so more approaches, including many popular few-shot image classification methods (ProtoNet, MetaBaselines, etc) can be benchmarked here.\n    A good idea would be to additionally introduce a query image, and categorize which set it belongs to. This creates a classification problem, while maintaining the core idea of the original Bongard problem -- to identify the concept of a set of images. Evidently, if the query is properly chosen, only when the model is able to identify the true concept and (implicitly) develop an image classifier for this concept, the query can be correctly categorized.\n    From a computer vision research perspective, such modification democratizes the evaluation of the Bongard problem by creating a \"simpler\" version of it, and makes it possible to examine the progress of current computer vision models in terms of few-shot concept understanding, which is a very interesting yet crucial capability that is originally benchmarked by Bongard problems.\n\n2. two \"non-symmetrical\" sets\n    \n    This is also inherited from Bongard-LOGO[1] and Bongard-HOI[2]. Compared to the original Bongard problems with two symmetrical sets, creating a non-symmetric \"negative\" set actually streamlines the reasoning process a bit. In the original Bongard problem, the concept induction process is repeated. We likely start with some hypothesis of the concept being depicted by set A, then we use set B images to refine them by ruling out those being depicted by set B. Next, we move to set B and do the same. If there is no clear answer, we may have to repeat the aforementioned steps until a verdict is returned. It can be seen that the induction processes of concepts from set A and set B are symmetrical as well. Therefore, by making the shared concept only exist in set A, we remove the reasoning process that is symmetrical to this in the original Bongard problem and therefore streamline the overall reasoning process.\n    We admit doing such could introduce some \"imbalance\" issues. But as we already mentioned in the previous response, we've introduced several techniques to mitigate this.\n    Back to what this design choice means to computer vision research in general: the idea is similar to 1., we would like to simplify the problem a bit so examining the progress of existing models, which are not strong enough to solve the original Bongard problem directly."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559721732,
                "cdate": 1700559721732,
                "tmdate": 1700575756428,
                "mdate": 1700575756428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k9i2zA4tfX",
                "forum": "hWS4MueyzC",
                "replyto": "8fRaq8hLLP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing an answer equally promptly.\n\nI find your additional arguments on the design choices more convincing then in the previous message. The elaborations are much appreciated. It seems reasonable that the trade-offs for focusing on classification rather than concept identification as well as using a negative set, simplifies the dataset creation and provides an adequate challenge for present models and research.\n\nI still believe a balanced problem setting, where each set of images correspond to a clear concept (not only negations on a fine-grained level), is more desirable but I now better understand the reason behind this choice.\n\n> It can be seen that, natural baselines with both CLIP and DINO as backbone fail to produce comparable performances to the existing baselines.\n\nFrom my understanding, this is primarily happening because of the adversarial query selection (based on CLIP score) of 1 from 7 possible query images for both positive and negative sets. I am still concerned about this fact because it is unclear whether this benchmarks artificially discourages certain solutions by design which might not reflect a real world setting. Ideally, a benchmark should be neutral in such a way that researchers can find the best solution without being penalized for choosing certain models, such as common image encoders.\n\nIt is related to my initial question about the choice to only select a single query image. (I didn't mean to have multiple queries for a single task, but just extend the dataset with all possible queries.) Alternatively, all 7 image could have served as test/query images.\nThis not only increases the sample size for the test set, but could also more naturally reflect real world examples including both simpler and more difficult images.\n\nIt seems quite counterintuitive that strong vision models such has CLIP and DINO perform so poorly and way below a random baseline."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578125045,
                "cdate": 1700578125045,
                "tmdate": 1700578125045,
                "mdate": 1700578125045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "prB0Ljb2DA",
                "forum": "hWS4MueyzC",
                "replyto": "8VUdKcfQ3l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_CSkd"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their time and effort in providing additional insights regarding my questions.\n\nIn general, I follow your argumentation, although I still find it surprising how these models consistently deviate below the random baseline. There seems to be on average higher visual similarity between the opposing queries and sets while maybe a fine detail determines the actual GT concept.\n\nIn conclusion, the rebuttal has addressed my questions. Preliminary, I am raising my score to 6 and will potentially make further adjustments to my evaluation after the reviewer discussion."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737157471,
                "cdate": 1700737157471,
                "tmdate": 1700737157471,
                "mdate": 1700737157471,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qWQuoNSuNe",
            "forum": "hWS4MueyzC",
            "replyto": "hWS4MueyzC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5053/Reviewer_yoip"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5053/Reviewer_yoip"
            ],
            "content": {
                "summary": {
                    "value": "The authors claim that they proposed Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. Based on this benchmark, they further present the few-shot learning baseline approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors claim that they proposed Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. Based on this benchmark, they further present the few-shot learning baseline approach."
                },
                "weaknesses": {
                    "value": "1. In the experiments, the authors primarily focus on conducting investigations using real-world datasets, particularly the their self-constructed dataset. However, given the Bongard Problem, it raises concerns about the generalizability of the conclusions/findings obtained from real-world datasets to mathematical datasets.\n\n2. The experimental results seems to ignore the traditional models, and it remains a concern."
                },
                "questions": {
                    "value": "Please refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666635971,
            "cdate": 1698666635971,
            "tmdate": 1699636495088,
            "mdate": 1699636495088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "JWLfEit0aU",
            "forum": "hWS4MueyzC",
            "replyto": "hWS4MueyzC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5053/Reviewer_bqT5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5053/Reviewer_bqT5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new benchmark, called Bongard-OpenWorld that contains visual few-shot reasoning tasks. Specifically, a Bongard problem contains a set of \u2018positive\u2019 and \u2018negative\u2019 images in the support set, where the positives all share a concept that none of the negatives do. The goal is to use this \u2018context\u2019 to infer the positive \u2018concept\u2019 in order to correctly label a disjoint set of query images as being positive or negative. While this problem has been studied in previous work, the proposed benchmark differs in that the concepts are \u2018open world\u2019 (rather than selected from a predefined small set). Specifically, they leverage Conceptual Concepts which is a massive web-crawled dataset containing image descriptions, and extract concepts from that dataset as well as through crowd-sourcing, to obtain concepts that contain factual or commonsense knowledge. Then, an image search tool is used to find appropriate images from the web to populate the \u2018positives\u2019 and \u2018negatives\u2019 for each concept (as well as query images) in order to form Bongard problems. They conduct an empirical investigation using both canonical few-shot learning methods as well as leveraging LMs and VLMs in different ways. For example, they explore a scenario where the VLM produces a caption for each of the images in the support set, and then these captions along with the positive and negative labels are fed to the LM which makes a prediction for each query image via in-context learning. This can be done in one-go or iteratively. They also propose a symbolic approach that directly applies logical operations to infer the positive concept."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies the interesting problem of few-shot visual reasoning\n- both \u2018traditional\u2019 few-shot learning methods as well as newer ideas involving LMs and VLMs are explored\n- the finding that even the best approaches lag significantly behind human performance is an interesting one, and points to the proposed benchmark as a valuable one for pushing the boundaries of of existing methods in this important direction"
                },
                "weaknesses": {
                    "value": "- Some related work is missed. [A] (see references below) studies a setting very related to the proposed benchmark (though they didn\u2019t use the terminology Bongard problems). They also created tasks using natural (\u2018real-world\u2019) images from different datasets, from using computer vision datasets (rather than scraping the web).\n\n- It would be great to add additional few-shot learning baselines to cover families of approaches that are excluded from the current analysis like approaches that perform FiLM-conditioning e.g. [B, C] (see references below) and approaches that train the backbone with gradient descent within each task, like MAML and Proto-MAML (the latter is a version proposed in the Meta-Dataset paper which is cited by this work)\n\n- The paper has some clarity issues, perhaps owing to the fact that the authors tried to \u2018squeeze\u2019 a lot of content in the required number of pages. It\u2019s hard to fully understand the different methods by reading only the main paper. I found the neuro-symbolic method proposed especially hard to understand (even after looking at the algorithm in the appendix). Please include some higher-level motivation and the intuition for the particular updates that it entails.\n\n- In Table 2, it\u2019s hard to tell which methods / rows correspond to which of the families of approaches (e.g. a, b, c, or d in Figure 3) \u2013 and e.g. which are single-round or multi-round. Perhaps a good way of indicating this is by adding an extra column in that table.\n\n- It would be great to conduct ablation analyses for design choices made in creating the benchmark, like the adversarial query selection that picks the positive query to not be too close to the prototype of the positive class. \n\n- It would be great to conduct an analysis of the effect of the \u2018shot\u2019 on these problems. Intuitively, the more positive and negative images the network sees, the easier it is to infer what is the positive class and correctly label query images. Given the negative results in the paper with the current number of shots (6 positives and 6 negatives), in terms of the very large gap from human performance, have the authors considered increasing the number of shots? Understanding how performance of different methods differs as the number of shots increases would be insightful.\n\n- it would also strengthen the paper to tie in the findings of this work with findings in related works. E.g. in the Bongard-HOI benchmark that the authors claim is the most similar, do they have similar findings e.g. in terms of which methods perform better?\n\n\nMinor\n=====\n- \u2018given 6 positive and 6 negative images [...] (see Figure 1 for illustration)\u2019 \u2013 but Figure 1 shows only 3 positive and 3 negative images (6 in total, not each). Maybe clarify that Figure 1 doesn\u2019t correspond to that setting and is used for illustration only? Or describe the task in the intro at a higher level of abstraction, e.g. P positive and N negative images.\n- in the caption of Figure 1, highlight \u2018hard negatives\u2019 in orange, like \u2018distractors\u2019 are highlighted in green, to match the (captions of the) images shown in that figure.\n- typo: \u201cprob\u201d \u2192 \u201cprobe\u201d (on page 6)\n- typo: \u201cwas not fine-tuning\u201d \u2192 \u201cwas not fine-tuned\u201d (in Table 2\u2019s caption)\n\nReferences\n=========\n\n- [A] Probing Few-Shot Generalization with Attributes. Ren et al.\n\n- [B] Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes. Requeima et al. NeurIPS 2019.\n\n- [C] Improved Few-Shot Visual Classification. Bateni et al. 2020"
                },
                "questions": {
                    "value": "- In Table 1, how is the number of tasks computed? What constitutes a unique task? Would having the same set of classes but different images in the support set count as the same task?\n\n- In Fig 3a, different few-shot learning algorithms are shown for the classification head only which seemed surprising. Some of these are meta-learning methods that also update the backbone. Is there a meta-training phase (starting possibly from a pretrained architecture) during which the backbone is also finetuned?\n\n- the authors mention that all few-shot learners excluding ChatGPT and GPT-4 use a ConvNext-base. But they also mention that SNAIL uses a transformer architecture. Should SNAIL be listed as another exception there?\n\n- The authors claim that open vocabulary is important for this benchmark and they use this as a  justification for the fact that pretraining on larger datasets leads to better results (\u201cfew-shot learners fueled with proper open-ended pretrained models [...] can alleviate this gap\u201d). But an alternative explanation could be that such large pretrained models like CLIP have already seen the specific images and / or concepts presented in the few-shot learning task and thus they simply face a weaker generalization challenge compared to models that were trained on smaller training set which may have a smaller probability of having seen these exact images or concepts. Have the authors made an attempt to examine or rule out this alternative hypothesis?  \n\n- Is it possible that some of the created Bongard problems are not solvable? E.g. this could happen if there accidentally is more than one concept that is shared between all of the positive images and none of the negative images. Is care taken to avoid this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698850726320,
            "cdate": 1698850726320,
            "tmdate": 1699636495017,
            "mdate": 1699636495017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LM7HB2HueZ",
                "forum": "hWS4MueyzC",
                "replyto": "JWLfEit0aU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bqT5 (1/N)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your time and constructive comments. Below, we provide detailed replies to your comments and hope we can resolve your major concerns.\n\n\n> - Some related work is missed. [A] (see references below) studies a setting very related to the proposed benchmark (though they didn\u2019t use the terminology Bongard problems). They also created tasks using natural (\u2018real-world\u2019) images from different datasets, from using computer vision datasets (rather than scraping the web).\n\nWe appreciate your suggestion and have incorporated this reference with discussion into our main paper. It is pertinent to note that the primary focus visual concepts of this work comprise various compositional attributes. Our proposed benchmark is characterized by its free-form nature, allowing a wide range of concepts with diverse elements.\n\n> - It would be great to add additional few-shot learning baselines to cover families of approaches that are excluded from the current analysis like approaches that perform FiLM-conditioning e.g. [B, C] (see references below) and approaches that train the backbone with gradient descent within each task, like MAML and Proto-MAML (the latter is a version proposed in the Meta-Dataset paper which is cited by this work).\n\nWe are grateful for your suggestions. We are currently implementing the FiLM-conditioning approach, this process will take a while. Regarding the MAML and Proto-MAML approaches, we encountered issues where its training process would collapse, leading to NaN. We believed similar issues have been observed in Bongard-HOI(http://arxiv.org/abs/2205.13803), i.e. some FSL approaches cannot produce meaningful results due to collapsing. Our hypothesis is having even fewer training data (in Bongard-OpenWorld) might have worsen this issue. We hence the exclusion of these results from our report.\n\n> - The paper has some clarity issues, perhaps owing to the fact that the authors tried to 'squeeze' a lot of content in the required number of pages. It\u2019s hard to fully understand the different methods by reading only the main paper. I found the neuro-symbolic method proposed especially hard to understand (even after looking at the algorithm in the appendix). Please include some higher-level motivation and the intuition for the particular updates that it entails.\n\nWe have further clarified approaches in the updated paper. For your convinience, here we offer some higher-level motivation and the intuition for the neuro-symbolic method:\n\n- In contrast to canonical FSL and large model based approaches, this approach begins with not just converting images into captions (ex. a dog is running on the grass), but further into concepts (ex. <dog, running, on grass>). This is done by employing GPT-4, the prompt is detailed in Appendix D. Our goal here is to mitigate the substantial noise present in image captions (both in single-round and multi-round approaches).\n- With these concepts, we apply logical operations like AND, OR, and NOT, aiming at mimicking the human logical thinking process when tacking Bongard problems. Specifically (please note that this is just high-level illustration and some details for handling corner cases are omitted. Again, the [code](https://github.com/Bongard-OpenWorld/Bongard-OpenWorld) includes all the implementation details):\n    1. Compute the intersection of the concepts of all positive images, this gives us the initial hypothesis on the true concept of this Bongard problem. If there is no intersection (possibly due to flaw in perception), a majority vote is adopted -- concepts that present in more than 4 positive images will be treated as the \"intersection\".\n    2. For each negative image, we substract its concept from the \"intersection\" obtained in the previous step, this is to mimick how human rule out the concepts that is depicted by both the positives and the negatives, and finally reach to the concept that is uniquely depicted by the positives.\n    3. We compare the updated \"intersection\" with the concept of the query image. If the \"intersection\" is a subset of the concepts depicted by the query, the query will be categorized as positive, otherwise deemed negative.\n    4. The \"intersection\" is also the ultimate induced concept of this Bongard problem, exclusively depicted by the positives.\n    \n- As a result, such logical operations over concepts can deterministically produce binary predictions on the query image and induced visual concepts depicted by the positives.\n\nNegatives contribute candidates that aid positives in updating their missed atomic concepts (potentially distractors). Conversely, positives provide candidates that help negatives refine their concepts (potentially overlaps)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417223436,
                "cdate": 1700417223436,
                "tmdate": 1700417223436,
                "mdate": 1700417223436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jY0pokcfCO",
                "forum": "hWS4MueyzC",
                "replyto": "xfimfbZliN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_bqT5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5053/Reviewer_bqT5"
                ],
                "content": {
                    "title": {
                        "value": "response to authors"
                    },
                    "comment": {
                        "value": "Hi authors,\n\nThank you for the very detailed response! \n\nI appreciate the discussion of other families of approaches, clarifications, comparison of the findings to those on Bongard-HOI, and discussions on the issue that some models may have been trained on the same concepts that appear in bongard tasks which may act as a confounding factor for some conclusions. I appreciate that the benchmark aims for realism and practicality, but i think in light of this confounding factor, we can't make claims like the open-vocabulary nature being responsible for better results on this benchmark (without ruling out the alternative hypothesis that those models had actually seen the same concepts so they simply face an easier generalization problem). So adjusting claims to acknowledge this is important.\n\nOverall, the authors have answered several of my questions and i maintain my opinion of weak acceptance. The reason I don't further increase my score is that I think the paper could further improve in terms of clarity and presentation, and i also agree with concerns brought up by Reviewer CSkd on strengthening the motivation of the particular design choices made in creating this benchmark and understanding the limitations (e.g. does it bias towards one family of models)."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649602970,
                "cdate": 1700649602970,
                "tmdate": 1700649602970,
                "mdate": 1700649602970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]