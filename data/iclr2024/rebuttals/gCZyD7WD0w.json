[
    {
        "title": "Guided Decoupled Exploration for Offline Reinforcement Learning Fine-tuning"
    },
    {
        "review": {
            "id": "BcKcCL7Bp2",
            "forum": "gCZyD7WD0w",
            "replyto": "gCZyD7WD0w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2067/Reviewer_XFhn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2067/Reviewer_XFhn"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies the excessive exploration problem that arises in an offline-to-online RL setup and proposes a new method that addresses the problem. The main idea is to separate exploration and exploitation policies and introduce a teacher policy that guides the exploration policy not to deviate too far from the teacher's actions. Teacher policy is updated to be the best policy so far by evaluating the exploitation policy at the specified regular interval. The proposed method is evaluated in locomotion and locomotion-based navigation tasks in D4RL Benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper clearly motivates their paper by providing supporting analysis and experiments\n- The proposed method is still simple even though it introduces several moving components. The idea of having the teacher policy that guides the exploration policy is well executed.\n- The method is compared against a lot of baselines and includes the error bar, which is commendable given the current status of this field."
                },
                "weaknesses": {
                    "value": "I liked reading the paper but there's some weaknesses possibly due to my understanding. Please see my weaknesses and questions.\n\n- Introductory analysis and experiments are helpful for understanding and motivating the method but they are mostly not new as authors also already mentioned in the paper. They are mostly already covered in works like [Fujimoto'18; Lee'22; Luo'23]\n- The proposed method needs evaluation rollouts for updating the teacher policy. Then the number of environment interactions required for this evaluation should also be incorporated into the sample count for the proposed method. It's not clear if this is reflected in the current results, and this should be properly computed if they are not because it's not a fair evaluation. Correctly doing this would also make Figure 6(b) analysis more meaningful because it's an important trade-off.\n- The paper is a bit difficult to parse in some parts. There is a room for improving the readability. For instance, Table 4 is difficult to read because it's missing the results of GDE with all the components. Including this could help improving the readability by making not scroll the paper up and down. Figure 5 is not very helpful for understanding the main method and could be improved to intuitively help the readers to understand the main idea. Augmenting the Algorithm 1 to be more self-contained could be helpful for better readability.\n\n[Fujimoto'18] Fujimoto, Scott, Herke Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" In International conference on machine learning, pp. 1587-1596. PMLR, 2018.\n\n[Lee'22] Lee, Seunghyun, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. \"Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble.\" In Conference on Robot Learning, pp. 1702-1712. PMLR, 2022.\n\n[Luo'23] Luo, Yicheng, Jackie Kay, Edward Grefenstette, and Marc Peter Deisenroth. \"Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions.\" arXiv preprint arXiv:2303.17396 (2023)."
                },
                "questions": {
                    "value": "- Could you clarify what's the unique observation that could be further emphasized in the paper?\n- Is the number of samples required for evaluating the policy is incorporated for counting the samples? It's important for a fair comparison.\n- Is n-step used for all the methods including both the proposed method and the baselines? It's not a new component proposed in the method so it needs to be included for a fair evaluation.\n- Improving some parts of Table 4, Figure 4 as in Weaknesses could be useful for improving the readability of the paper.\n- In Section 5.5, it's not clear how would the method that ablates a component work. For instance, how would the method work without the exploitation policy? Then what is the main policy you are updating with? Such things are not clear so it's difficult to understand what's going on in the analysis.\n- Please consider only making the numbers be bold when they are statistically significant, i.e., when errors do not overlap"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2067/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795020499,
            "cdate": 1698795020499,
            "tmdate": 1699636138733,
            "mdate": 1699636138733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LOLvjJFQuV",
                "forum": "gCZyD7WD0w",
                "replyto": "BcKcCL7Bp2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer XFhn [Part 1]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and insightful comments. Responses to the questions are below.\n\n> **Q1: Introductory analysis and experiments are helpful for understanding and motivating the method but they are mostly not new as authors also already mentioned in the paper.**\n\nThank you for the suggestion. Indeed, we acknowledge that certain aspects discussed in Section 3 have been covered in prior works. In Section 3, we intended to summarize and revisit some practical challenges in offline RL fine-tuning. In our view, this introductory analysis contributes to presenting a more comprehensive background and enhances the overall coherence of the paper.\n\nContrasting our work with prior research, notable distinctions include:\n\n- [Fujimoto '18] delved into the bootstrapping error issue within offline RL training.\n- [Lee '22] focused on addressing the bootstrapping error during the online fine-tuning stage.\n- [Luo '23] explored the policy crash issue during the online fine-tuning stage.\n\nIn this work, we synthesized these distinct elements and introduced a straightforward algorithm to tackle these challenges. Unlike [Fujimoto '18, Lee '22, Luo '23], which proposed specific algorithms in their respective papers, we presented a general framework. The experiments illustrate its effectiveness in collaboration with various backbone offline RL agents.\n\n\n\n> **Q2: Readbilities can be improved**\n\nAppreciate the suggestions. We have revised Table 4, Figure 5, and Algorithm 1 to enhance readability.\n\n\n\n> **Q3: unique observations**\n\nWe acknowledge that some observations in our paper overlap with prior works. However, we believe certain unique observations can be highlighted:\n\n- The optimal way to leverage offline samples during fine-tuning remains an open question. In some cases, dropping the offline buffer proves to be a surprisingly strong baseline.\n- It might be helpful to use varying degrees of conservatism in exploration and training, as demonstrated by the decoupling method.\n\n\n\n> **Q4: Is the number of samples required for evaluating the policy is incorporated for counting the samples?**\n\nThe evaluation trials were not considered in the budget for online fine-tuning. Consequently, we made an error by utilizing additional online samples for policy selection.\n\nAs discussed in the common question section, we have rectified this issue. The overall results align closely with the previous findings. The results in Table 2, Table 3, Table 4, and Figure 6 are updated accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705781326,
                "cdate": 1700705781326,
                "tmdate": 1700705781326,
                "mdate": 1700705781326,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "201PbKaqCr",
                "forum": "gCZyD7WD0w",
                "replyto": "Pwylh6Wusq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2067/Reviewer_XFhn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2067/Reviewer_XFhn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and additional results. Currently I have no questions or concerns, but there could be some things that I missed as I quickly went through the responses because there is not many time left. I'll go through the other reviews and responses and update the score during the internal discussion period."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739731220,
                "cdate": 1700739731220,
                "tmdate": 1700739731220,
                "mdate": 1700739731220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1oLk4Nbsmq",
            "forum": "gCZyD7WD0w",
            "replyto": "gCZyD7WD0w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2067/Reviewer_uJfn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2067/Reviewer_uJfn"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents three issues afflicting the performance of offline-to-online fine-tuning methods: insufficient exploration due to conservative pre-training, distribution-shift between offline and online distribution, distorted value functions. The paper then motivates and designs an algorithm based on IQL, where a decoupled exploration policy collects the online data. The loss for the exploration is based on the TD3 update, in contrast with the AWR loss used for policy update in IQL. However, TD3 update can cause the policy performance to crash, to avoid which a KL penalty with the best exploitation policy (aka teacher policy) is introduced, which also helps take safer actions. Overall, the proposed framework allows for more efficient offline-to-online fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Good coverage of the literature\n- The discretization into the three problems for offline-to-online RL makes sense and pedagogically useful\n- Section 3.2 presents an interesting experiment where removing the offline data during online fine-tuning improves the performance. Presenting more concrete evidence for this observation would improve the paper further.\n- Lots of experiments and the performance improvements are substantial, though more comparisons are needed to ascertain if the gains are for the hypothesized reasons."
                },
                "weaknesses": {
                    "value": "The clarity of Section 4 and method description can be improved quite a bit. Please see questions for further clarifications. \n\nOverall, I am not entirely sure about the generality of the framework, while the paper claims to be general. For example, if the actor loss for the base algorithm already uses a TD3 update, it is not obvious to me that the decoupled exploration does anything different.\n\nThe main benefit of GDE likely comes from the ability to allow the behavior policy to be updated using TD3, which cannot be done with IQL naively, as Q-values aren\u2019t  trained on OOD actions in IQL. The policy crash at low levels of $\\rho$ is likely because of using TD3 loss with an IQL trained Q-value function. This make the comparison with [2] quite critical. They report fairly sample efficient results, but beyond empirical gains, I suspect most of the benefit in GDE is derived from the fact that using a TD3 update for exploration policy, which improves the policy much faster than AWR update and thus generates better exploration data. CalQL shows that calibrated Q-value functions can allow direct usage of TD3 style updates for policy improvement, without the whole decoupled framework.\n\nOverall I am willing to improve the score for the paper, if some of these conditions can be met:\n\n(1) The phenomenon in 3.2 is established in more environments, with algorithms beyond IQL\n\n(2) Comparisons with CalQL are added, and GDE outperforms CalQL.\n\n(3) Alternately, the framework is shown to be compatible with CalQL, and demonstrates an improvement in the performance over it\n\n(4) Clarifying the writing in Section 4, and confirming that the evaluation rollouts for exploitation policy are duly counted in the fine-tuning budget\n\n\n[1] Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. Hong et al. NeurIPS 2023.\n\n[2] Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. Nakamoto et al. NeurIPS 2023."
                },
                "questions": {
                    "value": "- Section 3.1 inefficient exploration; have you tried adding RND reward to the offline agent to encourage wider/broader exploration? This issue seems to be part of the desiderata\n- Section 3.2: This is an interesting point, but might be worth rephrasing the open question  \u201chow to leverage prior knowledge without hurting performance\u201d to clarify how to best use offline data during online fine-tuning. Figure 3 (a) is quite interesting. Am I understanding it correctly that for IQL and SAC, the policy and Q-values are pre-trained (using IQL and CQL respectively on the offline data), but removing the offline data during online fine-tuning and collecting data in the replay buffer from scratch improves the performance and efficiency during training? Can you reproduce this phenomenon on other environments, potentially AntMaze or Kitchen environments? Would this be less of a problem if the offline data contained higher proportion of expert trajectories? One possible way to continue using offline data during online fine-tuning is to rebalance the offline distribution, sampling more relevant transitions more frequently. See [1].\n\nFor Table 1:\n- Can you report the default IQL performance for comparison, ie, IQL that continues to use offline data naively during online fine-tuning? If using the official code for IQL, please note that it is not setup to fine-tune on locomotion environments, as the reward using during offline and online fine-tuning are different. Fixing that is important before reporting the IQL fine-tuning results.\n- Can you report the average performances as well (clustering locomotion envs, antmaze environments)?\n\nSection 4:\n- The clarity of writing in this section can be improved as it is missing quite a few details \u2014 more explicit details would greatly improve the understanding of GDE + minimizing or defining notation clearly with \\phi, \\mu, \\hat{\\phi}, \\bar{\\phi}\n- Is exploration policy initialized randomly?\n- How many trials are done for evaluation? Are those trials counted towards the budget for online fine-tuning? Are you evaluating every 2500 (Appendix B3) steps or 25000 steps (Table 7)?\n- How is the exploitation policy updated over the course of training? What does \u201cexploitation policy pi_e which is responsible for policy extraction from newly collected online samples mean\u201d? Is it updated only on the online samples or does it continue to use offline samples?\n- Do the exploration policy and exploitation update on the same value functions? I understand that this is possible to do for IQL where the Q-values are trained using the replay buffer data and does not query the policy, but if you use a different base algorithm, for example, CalQL, would this still make sense?\n- Which policy is used for reporting the performance? Have you tried evaluating the exploration policy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2067/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2067/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2067/Reviewer_uJfn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2067/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828140761,
            "cdate": 1698828140761,
            "tmdate": 1699636138654,
            "mdate": 1699636138654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lj5qNU1Tuc",
                "forum": "gCZyD7WD0w",
                "replyto": "1oLk4Nbsmq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer uJfn [Part 1]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and insightful comments. Responses to the questions are below.\n\n> **Q1: The clarity of Section 4 and method description can be improved quite a bit**\n\nThank you for the suggestion. We have rewritten Section 4 and the method description to enhance clarity.\n\n\n\n> **Q2: not entirely sure about the generality of the framework, while the paper claims to be general. For example, if the actor loss for the base algorithm already uses a TD3 update, it is not obvious to me that the decoupled exploration does anything different.**\n\nIn stating, 'It is notable that the proposed GDE is a general method that can be combined with different offline RL algorithms,' our intention was to convey that GDE can be employed to fine-tune various offline RL agents.\n\nUnlike certain other approaches, such as CAL-QL, which is a specific offline RL fine-tuning algorithm limited to a single backbone offline RL algorithm, GDE is a framework indifferent to the choice of backbone algorithms.\n\nIndeed, our method aligns with TD3+BC in using the same TD3 actor loss. The distinction lies in GDE introducing adaptive conservatism for the exploration and exploitation policy. Here, we encourage the exploration policy to take dissimilar actions while maintaining that the exploitation policy remains more conservative to avoid the policy crash issue.\n\n\n\n> **Q3: I suspect most of the benefit in GDE is derived from the fact that using a TD3 update for exploration policy,**\n\nExcellent question. We concur with the reviewer that the primary advantages of GDE stem from improved exploration data. As highlighted in the concluding section of the introduction, 'We then focus on addressing the exploration issue to achieve sample-efficient fine-tuning,' our emphasis is on mitigating the inefficient exploration problem arising from an overly conservative policy in offline RL fine-tuning.\n\nIn this paper, utilizing a TD3-like update for the exploration policy serves as a straightforward method to validate our assumptions. We acknowledge that the TD3-like update is a common factor contributing to the effectiveness of both GDE and CAL-QL.\n\nWe introduced the decoupled teacher policy to prevent policy crash issues, whereas CAL-QL addresses this matter through a modified CQL critic loss. GDE and CAL-QL aim to resolve this issue using different methods.\n\nThe key distinction lies in our decoupled framework's ease of integration with other offline RL algorithms.\n\n\n\n> **Q4: Phenomenon in 3.2 with on more environments with more algorithms. Will more expert data help?**\n\nThank you for the feedback. We have incorporated the results of fine-tuning a TD3+BC agent with and without the offline buffer into Figure 3(a), and the outcomes align with those of IQL and SAC.\n\nAs mentioned in our response to Q1 for R1, we have conducted similar experiments where we trained a SAC agent from scratch using a preload buffer. Our observations indicate that omitting the offline replay buffer proves beneficial when the quality of offline data is low, such as in the case of a random dataset.\n\nTo further investigate this phenomenon across multiple environments, we fine-tuned a TD3+BC agent and a SAC agent using the pretrained checkpoint of a TD3+BC and a CQL offline RL agent, respectively. The average evaluation scores at the end of 2.5e5 steps are reported below:\n\n|                  | TD3+BC without buffer | TD3+BC with buffer | SAC without buffer | SAC with buffer |\n| :--------------: | :-------------------: | :----------------: | ------------------ | --------------- |\n|  halfcheetah-r   |      49.1 (2.3)       |     35.5 (1.6)     | 80.7 (2.4)         | 68.7 (1.6)      |\n|     hopper-r     |      17.1 (3.4)       |     9.9 (0.3)      | 102.4 (10.8)       | 43.8 (21.8)     |\n|    walker2d-r    |       7.0 (2.6)       |     5.1 (5.7)      | 39.1 (14.6)        | 15.9 (6.8)      |\n|  halfcheetah-m   |      71.0 (1.5)       |     56.6 (0.5)     | 85.6 (1.8)         | 66.6 (0.9)      |\n|     hopper-m     |      102.3 (0.3)      |     60.1 (3.2)     | 100.4 (12.9)       | 105.7 (0.2)     |\n|    walker2d-m    |      70.7 (40.5)      |     86.0 (0.7)     | 72.1 (24.3)        | 97.6 (0.3)      |\n| halfcheetah-m-re |      64.4 (3.0)       |     51.7 (0.4)     | 78.9 (7.1)         | 71.3 (0.3)      |\n|   hopper-m-re    |      103.9 (1.2)      |    79.2 (29.7)     | 108.2 (0.9)        | 102.8 (3.8)     |\n|  walker2d-m-re   |      100.4 (3.5)      |     91.1 (0.3)     | 109.0 (7.3)        | 99.0 (1.3)      |\n|      Total       |         585.9         |       475.2        | 776                | 671.4           |\n\nWe note that fine-tuning a TD3+BC or SAC agent without the replay buffer tends to yield strong performance on locomotion tasks. However, these methods often struggle in antmaze tasks, resulting in 0 rewards, aligning with the observations discussed in the response to Q1 for R1."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705596560,
                "cdate": 1700705596560,
                "tmdate": 1700705596560,
                "mdate": 1700705596560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "72mi9SuKgZ",
            "forum": "gCZyD7WD0w",
            "replyto": "gCZyD7WD0w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2067/Reviewer_Se55"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2067/Reviewer_Se55"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the challenges of offline-to-online RL with interesting experimental exploration, namely inefficient exploration, distributional shifted samples and distorted value functions. Based on the empirical findings and analysis, it proposes a simple yet effective algorithm called Guided Decoupled Exploration (GDE), which maintains a exploration policy and a teacher policy in addition to the main exploitation policy. GDE outperforms prior approaches in multiple domains with various backbone algorithms. Ablation study and hyperparameter tests are provided to verify the effectiveness of GDE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies an interesting and important problem, which is to finetune an offline learned policy in online environments. \n- The paper starts by demonstrating the key challenges in this setup with motivating experiments. Although the studied challenges have been discussed a lot in literature, the experiments in Sec 3 provide factual evidence, which I find interesting and helpful.\n- The proposed algorithm is based on the empirical findings, which makes intuitive sense and works well in standard benchmarks."
                },
                "weaknesses": {
                    "value": "- The analysis of challenges can be made more in-depth. The current experiments are more like proof-of-concept and the results can be expected.\n- GDE maintains 3 policies, rendering extra computation and memory costs. Although the authors emphasize the minimalist algorithm design, I feel that the current design is not necessarily the most efficient. For example, can the exploration poilcy directly be a function of the exploitation policy (one can just adjust the output action distribution by exploration objectives, without training an extra policy.)"
                },
                "questions": {
                    "value": "1. There are a lot of exploration approachs, is there a reason of selecting Eq (5) as the loss for the exploration policy? Will other methods work here, such as curiosity-driven ones?\n2. Why is the performce of train from scratch with SAC so low? Given enough samples, shouldn't SAC be able to learn a good policy in many of these tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2067/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699074974179,
            "cdate": 1699074974179,
            "tmdate": 1699636138586,
            "mdate": 1699636138586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vhuq5UsqCC",
                "forum": "gCZyD7WD0w",
                "replyto": "72mi9SuKgZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and insightful comments. Responses to the questions are below.\n\n> **Q1: The analysis of challenges can be made more in-depth.**\n\nThank you for the valuable suggestion. We acknowledge that our focus has been primarily on illustrating the challenges in current offline RL fine-tuning. We provided analysis on these challenges in Appendix B.5.\n\nGiven the constraints of space, delving deeply into the three challenges might be challenging. Our approach in this work has been to analyze key observations and demonstrate the efficacy of the proposed method. We are open to the prospect of conducting a more in-depth theoretical analysis of these challenges in future work.\n\n\n> **Q2: GDE maintains 3 policies, rendering extra computation and memory costs.**\n\nYes, GDE indeed maintains three policies: a teacher policy, an exploration policy, and an exploitation policy. It's important to note that the teacher policy is essentially a checkpoint of the previous exploitation policy. Consequently, during training, we only need to train two policies. Moreover, the exploration policy is implemented as a simple 2-layer MLP, which means the additional computation overhead is relatively light.\n\n\n> **Q3:  can the exploration policy directly be a function of the exploitation policy**\n\nExcellent question. The primary focus of this paper is to outline practical challenges in current offline RL fine-tuning, specifically inefficient exploration due to the over-conservative policy, and then propose a straightforward solution to address this issue. The current design was chosen for its simplicity and intuitiveness.\n\nAcknowledging that there might be more efficient algorithm designs, one potential solution to make the exploration policy directly a function of the exploitation policy is to modify the current policy structure into a conditioned policy $\\pi(a\\vert s, z)$. Here, we would learn a new latent variable $z$ to control conservatism during action sampling, thereby unifying the exploration policy and exploitation policy. The exploration of effective training methods for such a policy is a topic we leave for future work.\n\n\n\n> **Q4: Why select Eq(5)? Will curiosity-driven method work?**\n\nOne of the main reasons for using Equation 5 lies in its simplicity. Beginning with a minimalist perspective in designing our algorithm, we opted to add a single regularization term to the canonical actor loss in the TD3 and SAC.\n\nOur primary goal was to demonstrate that by introducing this new actor loss, we could address the issue of inefficient exploration while staying relatively close to the behavior policy. We believe that other actor losses with a similar concept could yield positive results as well, and we chose one of the simplest forms to validate our assumptions. For instance, incorporating a Random Network Distillation (RND) module to augment the task reward and encourage exploration could be beneficial in certain tasks, as illustrated in Table 1.\n\nHowever, such methods typically entail learning an additional model and introduce additional computational complexity. Given the effective performance of our current simple design in experiments, we opted to retain it.\n\n\n> **Q5: Performance of Fromscratch SAC**\n\nThe seemingly low performance of 'Fromscratch SAC' can be attributed to two primary reasons:\n\n- **Limited Interaction Steps:** Our focus in this study was on the challenge of sample-efficient offline RL fine-tuning. Consequently, we constrained the agent to interact with the environment for only 2.5e5 environmental steps in the experiments. The suboptimal performance of Fromscratch SAC suggests that the randomly initialized policy is less efficient in collecting useful samples during the early stages of learning.\n- **Normalized Evaluation Scores:** The reported scores are normalized evaluation scores, and as a result, they may appear low. If we were to run Fromscratch SAC for 1e6 environmental steps, it would likely achieve scores near 100 in the locomotion tasks."
                    },
                    "title": {
                        "value": "Responses to Reviewer Se55"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705512939,
                "cdate": 1700705512939,
                "tmdate": 1700705910144,
                "mdate": 1700705910144,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JHh31K0Vhw",
            "forum": "gCZyD7WD0w",
            "replyto": "gCZyD7WD0w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2067/Reviewer_DsmU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2067/Reviewer_DsmU"
            ],
            "content": {
                "summary": {
                    "value": "The paper describe a novel technique were exploration in offline to online RL is decoupled from the offline learning. This guided exploration avoid three of the current pitfalls of the offline RL techniques. Inefficient exploration because of biased conservatism. The difference in probability distribution between offline and online samples. Finally the value function learned from the offline dataset is far away from the optimal value function. In this work a teacher policy is introduced which guide the exploration policy to avoid policy crashing. The teacher policy is updated frequently. This decoupled avoid the conservatism bias and focusing on the latest online samples and using a n-step return made this algorithm more sample efficient."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "originality: It's quite novel the approximation even though the community have proposed related solution for some of the problems. CA-CQL for efficiency when jumping to the online phase. (Some concurrent work that also use decoupling: Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias Mark et al 2023) What I like about the paper is that address all the three approach in one coherent algorithm.\n\n quality: the paper have presented clear equation to backup the claims, and have provided a strong methodology and well written experiments section, with proper problems accepted by the community.\n\nclarity: the paper is quite clear in its presentation, the structure and flow of the paper is well done.\n\n significance: the paper in an incremental change on the field of offline RL."
                },
                "weaknesses": {
                    "value": "Probably one weakness I see is how it compared with a off-policy algorithm with a preload buffer.\nIt would interesting to see how it compared with CA-CQL as well."
                },
                "questions": {
                    "value": "What's not clear to me in this paper is what the difference between this and having a off-policy algorithm  that start with a preload buffer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2067/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699144888802,
            "cdate": 1699144888802,
            "tmdate": 1699636138519,
            "mdate": 1699636138519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F17V79fAMe",
                "forum": "gCZyD7WD0w",
                "replyto": "JHh31K0Vhw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A question about the CA-CQL baseline."
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nCould you kindly tell me which paper is the CA-CQL method or if you intended to mention Cal-QL [1]?\n\n[1] Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699841929591,
                "cdate": 1699841929591,
                "tmdate": 1699841929591,
                "mdate": 1699841929591,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w1a0rZxXh6",
                "forum": "gCZyD7WD0w",
                "replyto": "F17V79fAMe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2067/Reviewer_DsmU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2067/Reviewer_DsmU"
                ],
                "content": {
                    "comment": {
                        "value": "Cal-QL"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699842033042,
                "cdate": 1699842033042,
                "tmdate": 1699842033042,
                "mdate": 1699842033042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YtilzxHENH",
                "forum": "gCZyD7WD0w",
                "replyto": "JHh31K0Vhw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2067/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and insightful comments. Responses to the questions are below.\n\n>  **Q1: how it compared with an off-policy algorithm with a preload buffer.**\n\nThanks for the question. We have incorporated experiments in Appendix C.1 to investigate the performance of training an off-policy algorithm with a preload buffer. We compare GDE to training a SAC agent from scratch with and without a preload buffer, respectively.\n\n|                  | SAC without buffer | SAC with buffer |       GDE       |\n| :--------------: | :----------------: | :-------------: | :-------------: |\n|  halfcheetah-r   |   **59.5 (5.4)**   |   52.4 (2.4)    |   57.6 (13.4)   |\n|     hopper-r     |    82.7 (19.1)     |   25.6 (4.0)    | **92.6 (7.0)**  |\n|    walker2d-r    |  **47.4 (17.8)**   |   15.4 (8.1)    |   33.3 (20.3)   |\n|  halfcheetah-m   |     59.5 (5.4)     |   61.1 (0.8)    | **75.7 (1.8)**  |\n|     hopper-m     |    82.7 (19.1)     |   66.6 (11.2)   | **101.9 (6.9)** |\n|    walker2d-m    |    47.7 (17.8)     |   73.0 (11.2)   | **101.7 (6.1)** |\n| halfcheetah-m-re |     59.5 (5.4)     |   67.5 (1.8)    | **70.9 (1.2)**  |\n|   hopper-m-re    |    82.7 (19.1)     |   85.1 (19.4)   | **101.5 (8.0)** |\n|  walker2d-m-re   |    47.7 (17.8)     |   84.0 (1.1)    | **100.9 (3.9)** |\n|      Total       |       569.7        |      530.7      |    **736.1**    |\n\n|             | SAC without buffer | SAC with buffer |      GDE       |\n| :---------: | :----------------: | :-------------: | :------------: |\n|  antmaze-u  |       0 (0)        |    2.4 (0.5)    | **97.6 (1.0)** |\n| antmaze-u-d |       0 (0)        |      0 (0)      | **90.2 (2.5)** |\n| antmaze-m-p |       0 (0)        |      0 (0)      | **94.8 (2.1)** |\n| antmaze-m-d |       0 (0)        |      0 (0)      | **94.4 (2.1)** |\n| antmaze-l-p |       0 (0)        |      0 (0)      | **73.2 (6.7)** |\n| antmaze-l-d |       0 (0)        |      0 (0)      | **80.4 (2.3)** |\n|    Total    |         0          |       2.4       |   **530.6**    |\n\nWe can observe that training a SAC agent with a preload buffer yields satisfactory results on the locomotion tasks. However, its performance significantly degrades on more challenging antmaze tasks.\n\nThe primary reason behind this discrepancy lies in the nature of the antmaze task, characterized by sparse rewards and goal-reaching challenges. The preload buffer contains sub-optimal trajectories that make direct training of an SAC agent challenging. \n\nUsually, we need to leverage the *stitching ability* from offline RL to learn a sub-optimal policy, and then fine-tune the policy with further online interactions.\n\n\n\n> **Q2: comparison with CAL-QL**\n\nAppreciate the suggestion! We have addressed this query in the Common Questions section.\n\n\n\n> **Q3: difference w.r.t. an off-policy algorithm with a preload buffer** \n\nThank you for the thoughtful question. Our method differs from an off-policy algorithm with a preload buffer in the following key aspects:\n\n- **Knowledge Transfer:**\n    - Our method involves fine-tuning a pre-trained offline RL checkpoint with additional online interactions to enhance performance. This means transferring the learned policy and value function from the offline learning stage to the online learning stage.\n    - In contrast, training an off-policy algorithm with a preload buffer primarily transfers collected samples from the offline learning stage to the online learning stage.\n- **Curriculum Learning Perspective:**\n    - While the off-policy algorithm with a preload buffer is a general approach applicable to all tasks, it may face challenges in quickly learning certain challenging tasks, such as the antmaze scenarios. The preload samples might not be effective enough to initiate efficient learning.\n    - Our proposed method, on the other hand, employs a pre-trained policy to collect online samples during the fine-tuning stage, providing the agent with a more manageable task.\n    - This difference can be interpreted through the lens of curriculum learning. The off-policy algorithm with a preload buffer might expose the agent to a challenging task right from the start, whereas our method, using the pre-trained policy, allows the agent to first collect sub-optimal trajectories, easing it into the learning process.\""
                    },
                    "title": {
                        "value": "Responses to Reviewer DsmU"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2067/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705444714,
                "cdate": 1700705444714,
                "tmdate": 1700705924988,
                "mdate": 1700705924988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]