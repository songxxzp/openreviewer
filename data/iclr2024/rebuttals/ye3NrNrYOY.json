[
    {
        "title": "Temporal Causal Mechanism Transfer for Few-shot Action Recognition"
    },
    {
        "review": {
            "id": "o1FIBnsAFx",
            "forum": "ye3NrNrYOY",
            "replyto": "ye3NrNrYOY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission342/Reviewer_K1pA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission342/Reviewer_K1pA"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces temporal causal mechanism transfer (TCMT) for few-shot action recognition. It considers the action sequences from a generative model perspective. Specifically, it assumes that base and novel action videos share some common causal relationships. By learning these causal relationships, the model can work better with less training data (few-shot recognition). The overall causal learning framework is built as a variational autoencoder. After the training, only the encoder is kept to perform action recognition with the intermediate representations. The proposed TCMT is evaluated on benchmark datasets including UCF101, HMDB51, and SSv2."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The idea is easy to follow and modeling the causal relationship for few-shot action recognition is novel and reasonable\n2) This paper proposed to model the causal relationship between hidden variables and action sequences. By learning the invariant part of the relationship, the parameters of few-shot action recognition model can be reduced since only the auxiliary variable is needed to be considered at each time step. \n3) Comparison of non-causal and causal demonstrates the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1) In the introduction part, there are no red arrows in Figure 2. But the explanation in the second last paragraph is explaining it using red arrows, which makes the time-delayed causal relations confusing. \n2) Based on the proposed causal modeling process, it seems only first-order dependency is modeled. However, the action sequences probably has high-order dependencies.\n3) The comparison is incomplete, missing many recent work such as:\n[1] Wang, Xiang, et al. \"MoLo: Motion-augmented Long-short Contrastive Learning for Few-shot Action Recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[2] Zheng, Sipeng, Shizhe Chen, and Qin Jin. \"Few-shot action recognition with hierarchical matching and contrastive learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[3] Wang, Xiang, et al. \"Hybrid relation guided set matching for few-shot action recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n4) There is no justification whether the causal relationship is learned correctly besides the performance improvement. \n5) For the comparison number of parameters, all parameters besides the parameters in the adaption process should be counted since they are needed for inference."
                },
                "questions": {
                    "value": "0) It is very slow to open and scroll the submitted document locally. Perhaps Figure 1 (b) has too many objects. I don\u2019t know if this only happen on my site.\n1) For equation (11), is the ratio of L_{ELBO} and L_{cls} 1:1?\n\n2) Just for curiosity, does the hidden variable theta have interpretable meanings? If theta control certain aspects of the action generation process, it would be easier to justify the causal relationship.  \n\n3) To training the autoencoder, joint training may not be optimal. If the CVAE is firstly trained for causal modeling and then jointly trained for maximizing ELBO and classification, maybe the causal relationship can be better learned. In addition, the results from the first step can be used to verify if the causal relationship is correctly captured. \n\n4) In Table 5, what is the \u201cN\u201d used for non-causal, non-temporal, and without theta?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission342/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission342/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission342/Reviewer_K1pA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642779848,
            "cdate": 1698642779848,
            "tmdate": 1699635961444,
            "mdate": 1699635961444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hw32xs3Gxg",
                "forum": "ye3NrNrYOY",
                "replyto": "o1FIBnsAFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K1pA P1"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the insightful comments and helpful suggestions. We have carefully considered your feedback and addressed each point in our response, aiming to clarify and enhance the understanding of our research. \n\n>**Weakness 1:** In the introduction part, there are no red arrows in Figure 2. But the explanation in the second last paragraph is explaining it using red arrows, which makes the time-delayed causal relations confusing.\n\n**Answer:** We have corrected the typo referring to red arrows and polish Figure 2. Thank you for pointing this out.\n\n>**Weakness 2:** Based on the proposed causal modeling process, it seems only first-order dependency is modeled. However, the action sequences probably has high-order dependencies.\n\n**Answer:** This is an excellent point. Higher-order dependencies are certainly possible. In response we have added new results on the Sth-Else dataset with models that account for causal relations between non-consecutive time steps in Table A. We observe that the improvements are marginal although we experience a considerable increase in computational costs with higher-order models; with four 2080Ti GPUs, it takes 20 hours to train with only first-order dependencies $(\\tau=1)$ compared to 30 hours for second-order $(\\tau=2)$ and 36 hours for third-order $(\\tau=3)$. We note that the failure of higher-order models to yield a benefit may be due to the nature of the Sth-Else dataset, which primarily consists of 1-2 second video sequences. Due to the computation cost of large-scale pre-training, we are not sure whether it can be done before the 22nd Nov for all datasets. We will update the results at our earliest once we obtain the results. These results has been added to Table 10 in the appendix.\n\n**Table A:** Ablation study allowing parents of $z$ to be in $z_{t-1}$ through $z_{t-\\tau}$ for $\\tau \\in {1,2,3}$.\n|  |  5-shot | 10-shot |  \n|--------|:--------:|:--------:|\n| $\\tau=1$ | 48.5 | 59.9 |\n| $\\tau=2$ | 48.5 | 60.0 | \n| $\\tau=3$ | 48.8 | 60.2 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153359488,
                "cdate": 1700153359488,
                "tmdate": 1700279735825,
                "mdate": 1700279735825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pwtbt7yXLS",
                "forum": "ye3NrNrYOY",
                "replyto": "o1FIBnsAFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K1pA P2"
                    },
                    "comment": {
                        "value": ">**Weakness 3:** The comparison is incomplete, missing many recent work such as: [1] Wang, Xiang, et al. \"MoLo: Motion-augmented Long-short Contrastive Learning for Few-shot Action Recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. \n[2] Zheng, Sipeng, Shizhe Chen, and Qin Jin. \"Few-shot action recognition with hierarchical matching and contrastive learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. \n[3] Wang, Xiang, et al. \"Hybrid relation guided set matching for few-shot action recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n**Answer:** In response we provide additional experiments. Table B and C below (Tables 6 and 7 in the appendix) showcase our TCMT$_H$ results against leading metric-based methods like MoLo, HySRM, and HCL in a 5-way-k-shot framework, following your suggestion. In the SSv2-Full and SSv2-Small datasets, we randomly selected 64 classes for $\\mathcal{D}$ and 24 for $\\mathcal{S}$ and $\\mathcal{Q}$. The main difference between SSv2-Full and SSv2-Small is the dataset size, with SSv2-Full containing all samples per category and SSv2-Small including only 100 samples per category. For HMDB-51, we chose 31 action categories for $\\mathcal{D}$ and 10 for $\\mathcal{S}$ and $\\mathcal{Q}$, while for UCF-101, the selection was 70 and 21 categories, respectively. For Kinect, we used 64 action categories for $\\mathcal{D}$ and 24 for $\\mathcal{S}$ and $\\mathcal{Q}$. To maintain statistical significance, we executed 200 trials, each involving random samplings across categories. After training on $\\mathcal{D}$, we used $k$ video sequences from each action category to form $\\mathcal{S}$ for model updates. The inference phase utilized the remaining data from $\\mathcal{Q}$.\n\n**Table B:** Comparing TCMT$_H$ to benchmarks for 5-way-k-shot learning on the SSv2 and SSv2-small\n\n|  |   | SSv2 |  |   | SSv2-small |  | \n|--------|:--------:|:--------:|--------:|--------:|:--------:|--------:|\n|  |  1-shot | 3-shot | 5-shot| 1-shot | 3-shot | 5-shot| \n| OTAM | 42.8 | 51.5 | 52.3 |36.4 | 45.9 | 48.0\n| TRX | 42.0 | 57.6 | 62.6 | 36.0 | 51.9 | 56.7\n| STRM | 42.0 | 59.1| 68.1 | 37.1 | 49.2 | 55.3\n| HyRSM | 54.3 | 65.1 | 69.0 | 40.6 | 52.3 | 56.1\n| HCL | 47.3 | 59.0 | 64.9 | 38.7 | 49.1 | 55.4 \n| MoLo | 56.6 | 67.0 | 70.6  | 42.7 | 52.9 | 56.4\n| TCMT$_H$ | **60.0** | **68.3** | **71.9** | **45.8** | **53.6** | **58.0**\n\n**Table C:** Comparing TCMT$_H$ to benchmarks for 5-way-k-shot learning on the UCF-101, HMDB-51, and Kinectics datasets.\n\n|  |  | UCF-101  |   |  HMDB-51 |   | Kinectic|  \n|--------|:--------:|:--------:|--------:|--------:|:--------:|--------:|\n|  |  1-shot | 5-shot | 1-shot| 5-shot | 1-shot | 5-shot| \n| OTAM | 79.9 | 88.9  | 54.5 | 68.0 | 79.9 | 88.9\n| TRX | 78.2 | 96.1 | 53.1 | 75.6 | 78.2 | 96.2\n| STRM | 80.5 | **96.9** | 52.3 | 77.3 | 80.5 | 96.9\n| HyRSM | 83.9 | 94.7 | 60.3 | 76.0 | 83.9 | 94.7\n| HCL | 82.8 | 93.3 | 59.1 | 76.3 | 73.7 | 85.8 \n| MoLo | 86.0 | 95.5 | 60.8 | 77.4 | 86.0 | 95.5\n| TCMT$_H$ | **87.3** | 96.5 | **61.9** | **80.5** | **86.1** | **98.0**\n\nWe observe that TCMT$_H$ had the highest accuracy in 11 out of 12 of these experiments, and had the second highest accuracy in the remaining experiment, trailing by only 0.4."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171849223,
                "cdate": 1700171849223,
                "tmdate": 1700279753574,
                "mdate": 1700279753574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CowkNjbd1b",
                "forum": "ye3NrNrYOY",
                "replyto": "o1FIBnsAFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K1pA P3"
                    },
                    "comment": {
                        "value": ">**Weakness 4:** There is no justification whether the causal relationship is learned correctly besides the performance improvement.\n\n**Answer:** Thank you for raising this point. We would like to address your question from the following points.\n\n1. **No ground-truth:** In our work, the causal relationships are inherently latent, and a significant challenge arises from the lack of ground-truth causal relations in the dataset. \n\n2. **Alternative measurement:** An possible alternative is that ELBO can help to assess how well the causal representations are captured under the constraint of the identifiability results, which is based on Independence Noise Conditoin (IN). To justify this point, we use ELBO as a metric to conduct an experiment on the Sth-Else dataset. We compare the full $\\text{TCMT}_C$ against non-temporal baseline. \nWe would like to understand that, if ELBO can measure our causal representation versus the one without temporal causal relationships. Given the importance of temporal causal relationships for sequential data such as videos, this comparison is helpful to understand the impact of these relationships on model performance.\n\n3. **Preliminary justifications:** We noted that the non-temporal baseline achieved ELBO values of 9.42 for 5-shot and 6.29 for 10-shot settings on the Sth-Else dataset. In comparison, our TCMT model recorded ELBO scores of 7.51 for 5-shot and 4.27 for 10-shot settings. These findings align with the results presented in Table 5 of the main paper, suggesting that lower ELBO scores indicate superior learning of causal representations, which in turn leads to improved recognition performance.\n\n4. **Future work:** It is noteworthy that since ELBO can capture the causal relationships based on some assumptions like IN, which is challenging to verify in the real-world data. Thus, the best way to measure it is to compare with ground truth.\nOne potential method is to utilize synthetic data with ground-truth of the causal relationships. However, currently, there are no suitable synthetic datasets available for few-shot action recognition. Recognizing the importance of this aspect, we plan to address this gap in our future work, potentially through the development or identification of appropriate synthetic datasets that allow for a more direct validation of learned causal relationships. If there is a particular method of verifying the quality of the causal relationship that you would suggest for our setting we would appreciate the recommendation.\n\n>**Weakness 5:** For the comparison number of parameters, all parameters besides the parameters in the adaption process should be counted since they are needed for inference.\n\n**Answer:** Including the backbone, the total number of parameters in our models are 74M for $\\text{TCMT}_H$ and 96M for $\\text{TCMT}_C$. In comparison, ViFi-CLIP has 124M parameters, VL-Prompting contains 135M, ORViT possesses 148M, and SViT has 152M parameters.\n\nIt's important to note that our primary goal is to facilitate efficient adaptation of the model from base to novel data. Therefore, in the body we focus on the number of parameters relevant for this transfer process. \n\n>**Question 0:** It is very slow to open and scroll the submitted document locally. Perhaps Figure 1 (b) has too many objects. I don\u2019t know if this only happen on my site.\n\n**Answer:** This is not unique to you. This is because the images in Figure 1 are populated with a large number of data points, maintaining high resolution.\n\n>**Question 1:** For equation (11), is the ratio of $L_{ELBO}$ and $L_{cls}$ 1:1?\n\n**Answer:** Yes, it is 1:1.\n\n>**Question 2:** Just for curiosity, does the hidden variable theta have interpretable meanings? If theta control certain aspects of the action generation process, it would be easier to justify the causal relationship.\n\n**Answer:** $\\theta$ is not easily interpretable because we do not have a ground truth to use for interpretation from the real-world data.\nWe probably can understand $\\theta$ from a toy example: \"Sliding downhill\" always involves downward motion with some constant rate of acceleration (possibly zero if speed is constant). At each time step our observation $x$ presents the position of the object while the latent variable $z$ can capture the object's velocity and displacement. The transition function models how the velocity and displacement change over time, while the mixing function outputs the position of the object as a function of its velocity and displacement. The auxiliary variable $\\theta$ captures aspects like the angle of motion and acceleration. If we learn \"sliding downhill\"  on the base data, we should be able to learn \u201cdropping\" on the novel data by updating only $\\theta$ and the classifier. However,  with the real world data, $\\theta$ will not be easily interpreted.\n\nOne possible future way to interpreter $\\theta$ would be to investigate interventions for post-hoc analysis, i.e., examine the effects of changing $\\theta$."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172352185,
                "cdate": 1700172352185,
                "tmdate": 1700584255033,
                "mdate": 1700584255033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7eLIioUW2h",
                "forum": "ye3NrNrYOY",
                "replyto": "o1FIBnsAFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K1pA P4"
                    },
                    "comment": {
                        "value": ">**Question 3:** To training the autoencoder, joint training may not be optimal. If the CVAE is firstly trained for causal modeling and then jointly trained for maximizing ELBO and classification, maybe the causal relationship can be better learned. In addition, the results from the first step can be used to verify if the causal relationship is correctly captured.\n\n**Answer:** Excellent point.\nWe have added experimental results in Table D below (Table 11 in the appendix) using the Sth-Else dataset to address this point. First we train the autoencoder and context network using ELBO, and then train the classifier separately. There does not appear to be big advantage from training separately. \n\n**Table D:** Ablation study for separate training vs. joint training.\n|  |  5-shot  | 10-shot | \n|--------|:--------:|:--------:|\n| Joint Training  | 48.5 | 59.9 |\n| Separate Training | 48.5 | 60.0 | \n\nHowever, we agree that it is possible that ELBO is helpful to understand how well the causal representations are learned. To illustrate this, we offer a comparative analysis by comparing the ELBO scores obtained by non-temporal baseline and our TCMT.  The non-temporal baseline achieves ELBO values of 9.42 for 5-shot and 6.29 for 10-shot settings on the Sth-Else dataset. In comparison, our TCMT model recorded ELBO scores of 7.51 for 5-shot and 4.27 for 10-shot settings. These outcomes demonstrate that our TCMT, with its temporal causal relationships, achieves a superior ELBO score. \n\nIn light of your suggestion, we are now trying to expand the two-stage training on other datasets. Due to the computation cost of large-scale pre-training, we are not sure whether it can be done before the 22nd Nov. We will update the results at our earliest once we obtain them.\n\n>**Question 4:** In Table 5, what is the \u201cN\u201d used for non-causal, non-temporal, and without theta?\n\n**Answer:** N = 12 for the ablation studies of non-causal, non-temporal, and without $\\theta$. We have edited our mention of this at the beginning of paragraph 2 of Section 3.3 to make it more explicit."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172658797,
                "cdate": 1700172658797,
                "tmdate": 1700279815590,
                "mdate": 1700279815590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nFawaaA4FC",
                "forum": "ye3NrNrYOY",
                "replyto": "o1FIBnsAFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update our response to Question 3"
                    },
                    "comment": {
                        "value": "We have conducted extensive experiments to assess the effectiveness of separate training compared to joint training on the SSv2, HMDB-51, and UCF-101 datasets, using all-way-k-shot settings. The results are meticulously detailed in Tables E and F for general performance, and Tables G and onward focus on the Evidence Lower Bound (ELBO) outcomes for each training method.\n\nUpon analyzing these results, it's clear that the superiority of separate training over joint training is not conclusive. For example, in our study, joint training outperforms separate training in half of the experiments (6 out of 12). Furthermore, the ability of ELBO in separate training to better capture causal relationships than joint training is still uncertain. In our findings, joint training surpasses separate training in one-third of the experiments (4 out of 12), with both methods achieving identical scores in 3 out of 12 experiments.\n\nWe recognize the critical importance of accurately measuring the correctness of learned causal relationships, especially in the absence of ground truth data. This remains a complex and challenging issue, and we are committed to exploring it more thoroughly in our future research endeavors, bearing in mind the time-sensitive nature of rebuttal.\n\n**Table E:** Comparing the accuracy of separate training to joint training for all-way-k-shot on the SSv2 dataset\n|  |  | SSv2  |   |   |   \n|--------|:--------:|:--------:|--------:|--------:|\n|  |  2-shot | 4-shot | 8-shot| 16-shot | \n| Joint training |  7.5 | 9.6 |11.8 |15.5\n| Separate training | 8.1 | 9.5 | 11.4 |16.1\n\n**Table F:** Comparing the accuracy of separate training to joint training for all-way-k-shot on the HMDB-51 and UCF-101 datasets\n|  |  |  | HMDB-51   |   |   |  UCF-101  |   |   |\n|--------|:--------:|:--------:|--------:|--------:|:--------:|:--------:|--------:|--------:|\n|  |  2-shot | 4-shot | 8-shot| 16-shot |  2-shot | 4-shot | 8-shot| 16-shot | \n| Joint training |  65.8 | 70.2 | 72.5 | 75.7 |  90.6 | 94.7 | 96.2 | 98.5\n| Separate training | 65.5 | 70.8 | 72.2 | 75.9 | 89.7 | 94.9 | 96.1 | 98.4\n\n\n**Table G:** Comparing the ELBO of separate training to the ELBO of joint training on the Sth-Else dataset\n|  |  5-shot  | 10-shot | \n|--------|:--------:|:--------:|\n| Joint Training  | 6.29 | 4.27 |\n| Separate Training | 6.15 | 4.27 | \n\n\n**Table H:** Comparing the ELBO of separate training to the ELBO of joint training for all-way-k-shot on the SSv2 dataset\n|  |  | SSv2  |   |   |   \n|--------|:--------:|:--------:|--------:|--------:|\n|  |  2-shot | 4-shot | 8-shot| 16-shot | \n| Joint training |  36.8 | 34.0 | 31.4 | 29.1\n| Separate training | 36.5 | 34.0 | 31.9 | 29.1\n\n\n**Table I:** Comparing the ELBO of separate training to the ELBO of joint training for all-way-k-shot on the HMDB-51 and UCF-101 datasets\n|  |  |  | HMDB-51   |   |   |  UCF-101  |   |   |\n|--------|:--------:|:--------:|--------:|--------:|:--------:|:--------:|--------:|--------:|\n|  |  2-shot | 4-shot | 8-shot| 16-shot |  2-shot | 4-shot | 8-shot| 16-shot | \n| Joint training |  4.66 | 4.50 | 4.18 | 4.11 | 2.56 | 2.39 | 2.37 | 2.26\n| Separate training | 4.66 | 4.47 | 4.22 | 4.09 | 2.57 | 2.41 | 2.37 | 2.26"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605236359,
                "cdate": 1700605236359,
                "tmdate": 1700618346468,
                "mdate": 1700618346468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kvUr04X0sC",
                "forum": "ye3NrNrYOY",
                "replyto": "o1FIBnsAFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Have your concerns been properly addressed?"
                    },
                    "comment": {
                        "value": "Dear Reviewer K1pA,\n\nThank you for the time and effort you have dedicated to reviewing and providing feedback on our submission. Hopefully our responses and the revisions made to our work effectively address your concerns. Should there be any additional points or matters you wish for us to consider, we are keen and ready to respond to them.\n\nAuthors of submission 342"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605734806,
                "cdate": 1700605734806,
                "tmdate": 1700605734806,
                "mdate": 1700605734806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WHAKrfv6D2",
                "forum": "ye3NrNrYOY",
                "replyto": "CowkNjbd1b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Reviewer_K1pA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Reviewer_K1pA"
                ],
                "content": {
                    "title": {
                        "value": "Second-round feedback to authors"
                    },
                    "comment": {
                        "value": "Thanks the authors for the rebuttal. I am satisfied with the answers to my concerns."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718973630,
                "cdate": 1700718973630,
                "tmdate": 1700718973630,
                "mdate": 1700718973630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3kTlXHoJ7L",
                "forum": "ye3NrNrYOY",
                "replyto": "o1FIBnsAFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Could you update the score and confidence?"
                    },
                    "comment": {
                        "value": "Dear Reviewer K1pA, \n\nThank you for your response and for acknowledging our efforts in addressing your comments. We would greatly appreciate it if you could consider updating your evaluation score and confidence level. Should you require any further clarifications, we are fully committed to addressing them to the best of our ability for the remainder of discussion period."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720921624,
                "cdate": 1700720921624,
                "tmdate": 1700723226022,
                "mdate": 1700723226022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AehPnaBEmJ",
            "forum": "ye3NrNrYOY",
            "replyto": "ye3NrNrYOY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission342/Reviewer_sFNL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission342/Reviewer_sFNL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for solving few-shot action recognition, which utilises the idea of variational inference to solve the problem, effectively reducing the number of parameters to be learned during adapation phase."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Pros:\n1. The basic motivation is feasible.\n2. The paper gives a good theoretical analysis."
                },
                "weaknesses": {
                    "value": "Cons:\n1. The paper mentions that TCMT is capable of \u201cadapt a base model effectively and efficiently when the base and novel data have significant distributional disparities.\u201d However, there is no experimental verification of such performance, and it is hoped that additional experiments in this area or further additions will be made to show that the existing dataset satisfies such conditions.\n2. The authors should add an experiment on the observed time frequency to the section on ablation experiments.\n3. This paper needs further improvement in the writing. For example, in Fig.2, $Z_{1,1}$  has an extra bracket around the variable. And all tables in the paper should be of a uniform size. There are numerous other grammatical errors that I have not mentioned but which take away from the reading experience significantly. I hope the author will review and correct these."
                },
                "questions": {
                    "value": "As mentioned above, how does TCMT perform when the base and novel data have significant distributional disparities?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765040388,
            "cdate": 1698765040388,
            "tmdate": 1699635961373,
            "mdate": 1699635961373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qvjushXkw8",
                "forum": "ye3NrNrYOY",
                "replyto": "AehPnaBEmJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sFNL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and comments on our paper. The following is our response.\n\n>**Weakness 1:** The paper mentions that TCMT is capable of \u201cadapt a base model effectively and efficiently when the base and novel data have significant distributional disparities.\u201d However, there is no experimental verification of such performance, and it is hoped that additional experiments in this area or further additions will be made to show that the existing dataset satisfies such conditions.\n\n**Answer:** \nWe have justified the capability of TCMT to handle the distributional disparities from following two aspects:\n\n**Comparing with the State-Of-The-Art:** *In our initial submission*,  Figure 1 (a)  in our paper contains a UMap visualization showing the severity of distributional disparities between base and novel data on the Sth-Else dataset. For effectiveness, Table 1 demonstrates the advantage of our TCMT against other state-of-the-art methods, and our ablation studies verify that this benefit comes from handling these distributional disparities. \n\n**The efficacy of causal representation:** We have now extended our ablation study with Table A below (it has been added as Table 8 in the appendix) to show that TCMT outperforms other methods even when they augmented with auxiliary context variables, thus validating the important of the causal approach. Figure 4 demonstrates the improvement in efficiency during adaptation by comparing the number of parameters that need to be updated using the novel data. \n\n**Table A:** Additional comparisons by augmenting existing methods on the Sth-Else dataset. $+\\theta$ means the method updates the Context Network when adapting instead of fine-tuning. Since VL Prompting uses VPT \\citep{vpt_eccv22} within the ViFi-CLIP framework, we only test ViFi-CLIP$+\\theta$.\n| Method |  5-shot | 10-shot |  \n|--------|:--------:|:--------:|\n| ORViT | 33.3 | 40.2 |\n| ORViT+$\\theta$ | 33.9 | 41.8 | \n| SViT | 34.4 | 42.6 | \n| SViT+$\\theta$ | 35.2| 44.0 | \n| TCMT$_H$ | 37.6| 44.0 | \n| ViFi-CLIP |44.5| 54.0 | \n| ViFi-CLIP+$\\theta$ |45.2| 58.0 | \n| VL Prompting |44.9| 58.2 | \n|TCMT$_C$ |**48.5**| **59.9** | \n\nTo further validate fixing the transition and mixing functions during adaptation, we conduct additional comparisons between TCMT$_C$ and a fine-tuning baseline, TCMT-FT, with the all-way-k-shot settings. The results, as presented in Table B and Table C below  (Table 5, as well as Tables 9 and 10 in our submission), confirm that TCMT$_C$ generally achieves better scores compared to TCMT-FT.\n\n**Table B:** Comparing TCMTC to TCMT-FT for all-way-k-shot on the SSv2 dataset\n|  |  | SSv2  |   |   |   \n|--------|:--------:|:--------:|--------:|--------:|\n|  |  2-shot | 4-shot | 8-shot| 16-shot | \n| TCMT-FT | 6.1 | 7.9 | 10.4 |14.1\n| TCMT$_C$ |  7.5 | 9.6 |11.8 |15.5\n\n**Table C:** Comparing TCMTC to TCMT-FT for all-way-k-shot on the HMDB-51 and UCF-101 datasets\n|  |  |  | HMDB-51   |   |   |  UCF-101  |   |   |\n|--------|:--------:|:--------:|--------:|--------:|:--------:|:--------:|--------:|--------:|\n|  |  2-shot | 4-shot | 8-shot| 16-shot |  2-shot | 4-shot | 8-shot| 16-shot | \n| TCMT-FT | 61.5 | 64.8 | 70.0 | 72.9 | 87.7 | 93.7 | 95.1 | 96.4\n| TCMT$_C$ |  65.8 | 70.2 | 72.5 | 75.7 |  90.6 | 94.7 | 96.2 | 98.5\n\n>**Weakness 2:** The authors should add an experiment on the observed time frequency to the section on ablation experiments.\n\n**Answer:** Thank you for drawing our attention to this.\nWe have rerun our ablation experiment on the Sth-Else dataset with 4, 8 and 16 of frames per video sequence uniformly spaced to see if this has any impact on the results. The results are reported in the Table D, which is Table 9 in the appendix. When we use 16 frames as the input, we observe a slight improvement. However using 16 frames doubles the computational cost, requiring eight 2080-ti GPUs to complete training within 24 hours.\n\n**Table D:** Ablation study for selecting the different length of input of TCMT$_C$ on the Sth-Else dataset..\n| Input frames |  5-shot | 10-shot |  \n|--------|:--------:|:--------:|\n| 4 | 43.3 | 51.0 |\n| 8 | 48.5 | 59.9 | \n| 16 | 50.8 | 61.2 | \n\n>**Weakness 3:** This paper needs further improvement in the writing. For example, in Fig.2, has an extra bracket around the variable. And all tables in the paper should be of a uniform size. There are numerous other grammatical errors that I have not mentioned but which take away from the reading experience significantly. I hope the author will review and correct these.\n\n**Answer:** We have fixed the typo in Figure 2 and all other typos that we have found. We are happy to edit any other typographical or grammatical errors that are brought to our attention. However, we do not find it beneficial to make all tables a uniform size, and this is not standard in ICLR papers. This would require small tables to take up large amounts of space, for large tables to be squeezed too small to be readable, or for larger tables to be broken up."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152440135,
                "cdate": 1700152440135,
                "tmdate": 1700279711308,
                "mdate": 1700279711308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b5OxFRzjOW",
                "forum": "ye3NrNrYOY",
                "replyto": "qvjushXkw8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Reviewer_sFNL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Reviewer_sFNL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your careful answer to my confusion. But I have some confusion as below, regarding Weakness 1, whether such phenomenon is commonly found in different datasets and how such distributional disparities affect the performance of the model. (It might be more intuitive to give some sample examples here, the brackets are only suggestions and will not affect the final scoring)\n\nSimilarly, I'm concerned about the validity of the assumptions made by other reviewers about the assumptions in INTRODUCTION. You can respond directly to other reviewers' related confusions, and I'll measure my score directly against your responses under their reviews."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653450620,
                "cdate": 1700653450620,
                "tmdate": 1700653450620,
                "mdate": 1700653450620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EohsDk3cAP",
                "forum": "ye3NrNrYOY",
                "replyto": "AehPnaBEmJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the additional comment"
                    },
                    "comment": {
                        "value": "Thank you for your additional comment.\n\nWe assume \"such phenomenon\" to refer to the distributional disparities observed in the datasets used for our experiments and how our Temporal Causal Mechanism Transfer (TCMT) model addresses these disparities. Figures 7, 8, and 9 in the appendix of our revised submission illustrate these distributional disparities between base and novel data under our all-way-k-shot settings. These figures also support our assumption that the transition and mixing functions remain invariant across the base and novel data.\n\nIt is a common assumption, as highlighted by our comparing approaches [1, 2], that the base and novel data distributions differ. Generally, in few-shot learning tasks, a larger disparity between these distributions makes it more challenging for models to adapt to novel data. For example, the authors of [3] explicitly mention that a method can \"fail to generalize to unseen domains due to a large discrepancy in feature distribution.\" A similar observation is made by the authors of [4]. This aligns with our experimental results. For instance, Figure 7 in our revision's appendix shows that the distributions from the base dataset (K-400) and the novel dataset (UCF-101) have a large discrepancy, leading to low accuracy scores reported in Table 3. However, the superior scores achieved by TCMT underscore the better effectiveness of TCMT comparing with other approaches.\n\nRegarding \"the assumptions made by other reviewers,\" we understand you are referencing Question 2 posed by Reviewer G3yX. We had provided a response to this query BEFORE receiving your further comment. In light of your suggestions, we will update our response to Reviewer G3yX as well.\n\nAdditionally, *we note that the concerns you raised, particularly those pertaining to specific points in brackets, appear to have significantly influenced your scoring of our representations.* We would greatly appreciate any further clarifications you could provide on these issues. \n\n[1] Phoo, Cheng Perng, and Bharath Hariharan. \"Self-training For Few-shot Transfer Across Extreme Task Differences.\" International Conference on Learning Representations. 2021.\n\n[2] Wang, Xiang, et al. \"Cross-domain few-shot action recognition with unlabeled videos.\" Computer Vision and Image Understanding (2023): 103737.\n\n[3] Tseng, Hung-Yu, et al. \"Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation.\" International Conference on Learning Representations. 2020.\n\n[4] Luo, Xu, et al. \"A Closer Look at Few-shot Classification Again.\" arXiv preprint arXiv:2301.12246 (2023)."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686400273,
                "cdate": 1700686400273,
                "tmdate": 1700710043940,
                "mdate": 1700710043940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wcuc2xrfTA",
            "forum": "ye3NrNrYOY",
            "replyto": "ye3NrNrYOY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission342/Reviewer_G3yX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission342/Reviewer_G3yX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a few-shot learning for action recognition based on temporal casual representation, called Temporal Causal Mechanism Transfer. The method is built on an assumption that the base data and novel data share certain aspects of the temporal causal mechanism, transition function and mixing function. It conducts experiments on multiple datasets and achieves great performance. Thw writing is somehow good."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of using temporal causal mechanism for few-shot video recognition is new to me.  \n2. The method is effective and achieves good results on multiple datasets."
                },
                "weaknesses": {
                    "value": "1. The third paragraph in the Intro is very highlight and intuitive. The motivation of using casual representation for few-shot action recognition is not clear to me from the paper. \n2. Fig. 2 lacks illustration in both caption and main contents. I can not understand well the methods without much casual representation background. And there is less introduction for the causal representation.\n3. All datasets miss details.\n4. Miss conclusions for all figures of results. The statements for results only list numbers but lack analysis. For example, in Fig. 5, the paper compares the proposed method and a previous method VL-Prompting. What's the difference between the two methods? What makes difference between their results? Why the proposed one is better than the previous one?"
                },
                "questions": {
                    "value": "I have two very serious question. Without clarification on the two points, I can not understand the paper well.\n\n1. What\u2019s the motivation/intuition to use causal representation learning for few-shot action recognition? I feel it is not clear to me from the paper.\n2. In the third paragraph in Intro, there is an assumption \"the base data and novel data share certain\naspects of the temporal causal mechanism \u2013 namely, transition function and mixing function \u2013 and\nthat an auxiliary variable captures the disparate aspects of the two data distributions\" which is the base of the method. However, I can not find why the assumption is acceptable?  Is there any support or reference?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission342/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission342/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission342/Reviewer_G3yX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818090807,
            "cdate": 1698818090807,
            "tmdate": 1699654860320,
            "mdate": 1699654860320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Yv53zI2tn",
                "forum": "ye3NrNrYOY",
                "replyto": "wcuc2xrfTA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer G3yX P1"
                    },
                    "comment": {
                        "value": "We are grateful for the time you spent on our paper and suggestions, the following is our response.\n\n>**Weakness 1:** The third paragraph in the Intro is very highlight and intuitive. The motivation of using casual representation for few-shot action recognition is not clear to me from the paper.\n\n**Answer:**\nTo make few-shot learning more efficient and effective, we would like to train a model such that parts of the model can remain fixed during adaptation. The fewer parameters we have to update, the more efficient adaptation will be.\n*We therefore need to understand what causes the distributional disparities, which is unanswered in the current literature on few-shot action recognition. This motivates us to model a causal mechanism to identify what components of the model can be held fixed and what factors lead to disparities in the action representations and labels.* The literature on causal representation learning and principle of ``minimum change\" give us the theoretical grounding to isolate the factors that cause disparities so that we can hold those parts of our model fixed during adaptation [1,2]. \n\nBy developing a temporal causal mechanism we are able to separate (1) the causal relationships between latent causal variables over time (transition function), (2) the dependence of the observation on the causal variables at a given time step (mixing function), and other time-dependent causal factors that are likely to vary between the base and novel data (auxiliary variable). \n\n>**Weakness 2:** Fig. 2 lacks illustration in both caption and main contents. I can not understand well the methods without much casual representation background. And there is less introduction for the causal representation.\n\n**Answer:** Consider the following toy example for intuition. \"Sliding downhill\" always involves downward motion with some constant rate of acceleration (possibly zero if speed is constant). At each time step our observation $x$ presents the position of the object while the latent variable $z$ can capture the object's velocity and displacement. The transition function models how the velocity and displacement change over time, while the mixing function outputs the position of the object as a function of its velocity and displacement. The auxiliary variable $\\theta$ captures aspects like the angle of motion and acceleration. If we learn \"sliding downhill\"  on the base data, we should be able to learn \"dropping\" on the novel data by updating only $\\theta$ and the classifier.\n\nThe contents of Figure 2 (a) (Figure 2 in original submission) are discussed in detail in the fourth paragraph of the introduction. The purpose of the figure is only to add intuition by illustrating the in-depth descriptions which directly precede it. We choose not extend the caption because the figure environment is narrow, which would lead the caption to be very long, and the caption contents would be highly redundant.\n\nWe believe that a comprehensive background on causal representation learning would be too distracting to our paper, and take up considerable space. We believe the ``Generative Model\" section of our methodology should be sufficient for understanding  the aspects of causal representation learning relevant to our work and we give the critical references for related work on causal representation (the third paragraph in the introduction and the \"causal representation learning\" part in the related work). Appendix A.1 gives further details on the role of the independent noise condition used for causal representation learning.\n\n>**Weakness 3:** All datasets miss details.\n\n**Answer:** Thank you for drawing our attention to this.\nWe realize that we have provided the references in the paper for describing the dataset, but the details should be added for clarification. We have added them in Appendix A.2:\n\n1. Something-Something v2 (SSv2) is a dataset containing 174 action categories of common human-object interactions;\n\n2. Something-Else (Sth-Else) exploits the compositional structure of SSv2, where a combination of a verb and a noun defines an action; \n\n3. HMDB-51 contains 7k videos of 51 categories; \n\n4. UCF-101 covers 13k videos spanning 101 categories; \n\n5. Kinetics covers around 230k 10-second video clips sourced from YouTube. \n\nPlease refer to the ``Datasets\" section in the main paper for the segmentation of $\\mathcal{D}$, $\\mathcal{S}$ and $\\mathcal{Q}$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151857373,
                "cdate": 1700151857373,
                "tmdate": 1700627242619,
                "mdate": 1700627242619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1LXImKm1jx",
                "forum": "ye3NrNrYOY",
                "replyto": "wcuc2xrfTA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer G3yX P2"
                    },
                    "comment": {
                        "value": ">**Weakness 4:** Miss conclusions for all figures of results. The statements for results only list numbers but lack analysis. For example, in Fig. 5, the paper compares the proposed method and a previous method VL-Prompting. What's the difference between the two methods? What makes difference between their results? Why the proposed one is better than the previous one?\n\n**Answer:** For all comparisons we make to all other models, ours is the only causal model based on causal representation. VL-Prompting does not invoke a causal model and therefore struggles to handle certain distribution disparities between the base and novel data. VL-Prompting is also shown in the plot of Figure 4, demonstrating that it requires a far greater number of parameters to be updated during transfer. \n\nWe would like to draw attention to the results in Table 1 of our paper which demonstrate the superiority of our TCMT framework over state-of-the-art methods like ViFi-CLIP and VL-Prompting. We succinctly describe both ViFi-CLIP and VL-Prompting in the second paragraph in the introduction, and third paragraph of the Related Work section, highlighting that while these methods successfully leverage the pre-trained CLIP model for few-shot action recognition, they may fall short when faced with significant distribution disparities between base and novel datasets. TCMT is distinguished by its use of causal representation learning to construct generative models of temporal causal processes, a strategy that our results show is particularly effective in addressing distribution disparities. This conclusion is not only theoretically sound but is also empirically validated through our experiments, underscoring TCMT's superiority over methods like VL Prompting.\n\nFor the statements of results, it is not entirely clear to us what you mean when you say we lack analysis, and clarification would be greatly appreciated. It would greatly benefit us if you could be more explicit with the form of analysis you have in mind. We include explanations for each of our experiments, tables, and figures.\n\n>**Question 1:** What\u2019s the motivation/intuition to use causal representation learning for few-shot action recognition? I feel it is not clear to me from the paper.\n\n**Answer:** Consider the following toy example for intuition. \"Sliding downhill\" always involves downward motion with some constant rate of acceleration (possibly zero if speed is constant). At each time step our observation $x$ presents the position of the object while the latent variable $z$ can capture the object's velocity and displacement. The transition function models how the velocity and displacement change over time, while the mixing function outputs the position of the object as a function of its velocity and displacement. The auxiliary variable $\\theta$ captures aspects like the angle of motion and acceleration. If we learn \"sliding downhill\"  on the base data, we should be able to learn \"dropping\" on the novel data by updating only $\\theta$ and the classifier.\n\nTo make few-shot learning more efficient and effective, we would like to train a model such that parts of the model can remain fixed during adaptation. The fewer parameters we have to update, the more efficient adaptation will be.\n*We therefore need to understand what causes the distributional disparities, which is unanswered in the current literature on few-shot action recognition. This motivates us to model a causal mechanism to identify what components of the model can be held fixed and what factors lead to disparities in the action representations and labels.* The literature on causal representation learning and principle of ``minimum change\" give us the theoretical grounding to isolate the factors that cause disparities so that we can hold those parts of our model fixed during adaptation [1,2].\n\n>**Question 2:** In the third paragraph in Intro, there is an assumption \"\" which is the base of the method. However, I can not find why the assumption is acceptable? Is there any support or reference?\n\n**Answer:** We have justified this assumption in the *initial submission* by:\n\n1. Figure 1b gives a snapshot of the invariance exhibited by the transition function between the base and novel data of the Sth-Else dataset. We have added a figure to show the invariance captured by the mixing function (Figure 1 (c) in the updated paper). We therefore expect limited benefit from updating these components during adaptation, and holding them fixed acts as a form of regularization to prevent over-fitting. \n\n2. Our original Figure 1 (c) (Figure 2 (b) in the updated version) shows that when holding the transition and mixing function fixed we see faster convergence to a higher accuracy on the Sth-Else dataset compared to updating the transition and mixing functions. We therefore have strong motivation to hypothesize that updating fewer parameters both improves efficiency and acts as a form of regularization. Our main experiments bear this out."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152037710,
                "cdate": 1700152037710,
                "tmdate": 1700584329773,
                "mdate": 1700584329773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aQ7ijv2luE",
                "forum": "ye3NrNrYOY",
                "replyto": "wcuc2xrfTA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Do the answers address your concerns?"
                    },
                    "comment": {
                        "value": "Dear Reviewer G3yX,\n\nThank you for your time and comments. Could you let us know your thoughts on whether our responses and revisions have adequately addressed your concerns? We are fully prepared to address any further questions you may have. Your prompt feedback is highly appreciated given we hope to have the opportunity to respond further during the period of discussions.\n\nThe author of 342"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606563074,
                "cdate": 1700606563074,
                "tmdate": 1700606563074,
                "mdate": 1700606563074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jog8hPpouU",
            "forum": "ye3NrNrYOY",
            "replyto": "ye3NrNrYOY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission342/Reviewer_oCST"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission342/Reviewer_oCST"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Temporal Causal Mechanism Transfer (TCMT), a new method for few-shot action recognition in videos. The key ideas and contributions are:\n\n- TCMT learns a generative model of a temporal causal process from the base data. This includes a transition function that models time-delayed causal relations between latent variables, and a mixing function that generates action representations from the latent variables.\n\n- For adaptation on novel data, TCMT updates an auxiliary context variable that captures distribution shifts between base and novel data, along with the classifier weights. The transition and mixing functions remain fixed. \n\n- TCMT is evaluated on standard few-shot action recognition benchmarks and achieves state-of-the-art or comparable accuracy with fewer parameter updates during adaptation. \n\n- The effectiveness of TCMT is attributed to the transferability of the learned causal mechanism. Ablations validate the benefits of modeling temporal relations and using auxiliary variables.\n\n- The approach demonstrates the promise of causal representation learning for few-shot action recognition. Limitations include assumptions on temporal delays and difficulty inferring the auxiliary variables.\n\nIn summary, the key contribution is a new few-shot learning method based on learning and transferring temporal causal mechanisms, which is shown to be accurate and efficient for adapting models to new action recognition tasks with limited labeled video data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Originality: The idea of learning and transferring a temporal causal mechanism is highly original. Causal representation learning has not been applied in this way for few-shot action recognition before. Modeling latent causal variables, time-delayed transitions, and mixing functions is creative.\n\n2. Quality: The method is technically sound, with reasonable assumptions justified from first principles of causality. Experiments across multiple datasets demonstrate state-of-the-art accuracy and efficiency. The ablation study provides insight into design choices.\n\n3. Clarity: Overall the paper is clearly written and easy to follow. The background gives sufficient context, and the methodology explains the approach in detail. More intuition could be provided for how the causal mechanism aids adaptation.\n\n4. Significance: This provides a new paradigm for few-shot video understanding based on causal representation learning. The ability to adapt models with fewer updates could enable deploying action recognition systems to new domains with limited labeled data. Limitations around temporal delays and auxiliary variables indicate interesting directions for future work."
                },
                "weaknesses": {
                    "value": "1. The motivation for why the causal mechanism transfers well could be clarified. Intuition or analysis on how the transition and mixing functions capture invariances would strengthen the core hypothesis.\n2. The inference of the auxiliary context variables \u03b8 seems coarse. More details on this convolutional LSTM approach and why it is effective would be helpful. Alternate ways to model \u03b8 could improve performance.\n3. Assumptions like time-delayed transitions between latent variables may not hold for data with low time resolution. Discussion of this limitation and ways to incorporate instantaneous effects would make the model more broadly applicable.\n4. More comparisons to understand tradeoffs versus other representation learning approaches like self-supervision may be informative."
                },
                "questions": {
                    "value": "Please see the 'weaknesses' above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832950910,
            "cdate": 1698832950910,
            "tmdate": 1699635961201,
            "mdate": 1699635961201,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mgcdGaHTg0",
                "forum": "ye3NrNrYOY",
                "replyto": "Jog8hPpouU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oCST P1"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your efforts and suggestions on our paper and provide our feedback below:\n\n>**Weakness 1:** The motivation for why the causal mechanism transfers well could be clarified. Intuition or analysis on how the transition and mixing functions capture invariances would strengthen the core hypothesis.\n\n**Answer:** Our answer is as following.\n\n**Explanation from a toy example** Consider the following toy example for intuition. \"Sliding downhill\" always involves downward motion with some constant rate of acceleration (possibly zero if speed is constant). At each time step our observation $x$ presents the position of the object while the latent variable $z$ can capture the object's velocity and displacement. The transition function models how the velocity and displacement change over time, while the mixing function outputs the position of the object as a function of its velocity and displacement. The auxiliary variable $\\theta$ captures aspects like the angle of motion and acceleration. If we learn \"sliding downhill\"  on the base data, we should be able to learn \u201cdropping\" on the novel data by updating only $\\theta$ and the classifier.\n\n**Motivation** To make few-shot learning more efficient and effective, we would like to train a model such that parts of the model can remain fixed during adaptation. The fewer parameters we have to update, the more efficient adaptation will be.\n*We therefore need to understand what causes the distributional disparities, which is unanswered in the current literature on few-shot action recognition. This motivates us to model a causal mechanism to identify what components of the model can be held fixed and what factors lead to disparities in the action representations and labels.* The literature on causal representation learning and principle of ``minimum change\" give us the theoretical grounding to isolate the factors that cause disparities so that we can hold those parts of our model fixed during adaptation [1,2]. \n\nBy developing a temporal causal mechanism we are able to separate (1) the causal relationships between latent causal variables over time (transition function), (2) the dependence of the observation on the causal variables at a given time step (mixing function), and other time-dependent causal factors that are likely to vary between the base and novel data (auxiliary variable).\n\n**Justification** In our initial submission, Figure 1b gives a snapshot of the invariance exhibited by the transition function between the base and novel data of the Sth-Else dataset. We have now added a UMap figure to show the invariance captured by the mixing function as well in Figure 1 (c) of the updated paper. We therefore expect limited benefit from updating these components of our model during adaptation, and holding them fixed acts as a form of regularization to prevent over-fitting. Our Figure 2 (b) (Figure 1 (c) from the initial submission) shows that when holding the transition and mixing function fixed we see faster convergence to a higher accuracy on the Sth-Else dataset compared to updating the transition and mixing functions. We therefore have strong motivation to hypothesize that updating fewer parameters both improves efficiency and acts as a form of regularization. Our main experiments bear this out.\n\n>**Weakness 2:** The inference of the auxiliary context variables $\\theta$ seems coarse. More details on this convolutional LSTM approach and why it is effective would be helpful. Alternate ways to model $\\theta$ could improve performance.\n\n**Answer:** *This is a limitation we identified explicitly in our conclusions and motivates future work.* Recent theoretical work [1,2] has demonstrated identifiability results built upon $\\theta$. To interpret $\\theta$ at a ``fine-grained\" level, a deeper understanding of its role in the latent causal process is necessary. This is beyond the limits of the identifiability results in the existing literature. In our work, we experimented with various methods to model $\\theta={\\theta_1,\\theta_2,...,\\theta_t}$ as a sequence, including ConvLSTM, 1D CNN, and Transformer. Ultimately, we opted for ConvLSTM based on a balanced consideration of performance and the efficiency of adaptation. As highlighted in our conclusion, this is an area we are keen to explore in our future research."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149163855,
                "cdate": 1700149163855,
                "tmdate": 1700584278271,
                "mdate": 1700584278271,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DfKNNTsb4v",
                "forum": "ye3NrNrYOY",
                "replyto": "Jog8hPpouU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oCST P2"
                    },
                    "comment": {
                        "value": ">**Weakness 3:** Assumptions like time-delayed transitions between latent variables may not hold for data with low time resolution. Discussion of this limitation and ways to incorporate instantaneous effects would make the model more broadly applicable.\n\n**Answer:** *This is another limitation we identified explicitly in our conclusions and motivates future work.* The problem of incorporating instantaneous effects is a substantial problem on extending identifiability and is beyond the scope of our present work. For example, one would need to address cyclic relationships between causal variables, and the available theoretical identifiability results on causal representation learning do not give us a principled approach for doing this.\n\n>**Weakness 4:** More comparisons to understand trade-offs versus other representation learning approaches like self-supervision may be informative.\n\n**Answer:** In the absence of a ground truth for the latent variable $z$, our TCMT approach effectively adopts a self-supervised methodology, utilizing a CVAE. This choice aligns with the requirements for the identifiability of causal representations, as elucidated in [1,2,3] Extending identifiability to other ``self-supervised\" frameworks is a valuable and insightful suggestion. We recognize the potential in this direction and will definitely consider it in our future research endeavors. Additionally, we note that we provide direct comparisons with several self-supervised approaches such as SEEN [4] and STARTUP [5] in Table 4. We believe these results clearly demonstrate the advantage of TCMT. If there are particular results from the latest literature on self-supervision for few-shot action recognition which you believe we should add, we are happy to add such comparisons to our paper.\n\n[1] Kong, Lingjing, et al. \"Partial disentanglement for domain adaptation.\" International Conference on Machine Learning. PMLR, 2022.\n\n[2] Xie, Shaoan, et al. \"Multi-domain image generation and translation with identifiability guarantees.\" The Eleventh International Conference on Learning Representations. 2023.\n\n[3] Huang, Biwei, et al. \"AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning.\" International Conference on Learning Representations. 2022.\n\n[4] Wang, Xiang, et al. \"Cross-domain few-shot action recognition with unlabeled videos.\" Computer Vision and Image Understanding (2023): 103737.\n\n[5] Phoo, Cheng Perng, and Bharath Hariharan. \"Self-training For Few-shot Transfer Across Extreme Task Differences.\" International Conference on Learning Representations. 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149841821,
                "cdate": 1700149841821,
                "tmdate": 1700279519053,
                "mdate": 1700279519053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LVjII1l57i",
                "forum": "ye3NrNrYOY",
                "replyto": "Jog8hPpouU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission342/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any further comments?"
                    },
                    "comment": {
                        "value": "Dear Reviewer oCST,\n\nWe would like to express our gratitude once again for your time. We hope that our responses have satisfactorily addressed your concerns. If you have any additional comments or questions, we kindly request you to share them with us as soon as possible, so we can provide our responses within the allocated discussion period.\n\nThe authors of 342"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission342/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606900478,
                "cdate": 1700606900478,
                "tmdate": 1700606900478,
                "mdate": 1700606900478,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]