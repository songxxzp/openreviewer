[
    {
        "title": "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks"
    },
    {
        "review": {
            "id": "Qc212gAv2q",
            "forum": "mzxKLZNbrQ",
            "replyto": "mzxKLZNbrQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4734/Reviewer_kj3u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4734/Reviewer_kj3u"
            ],
            "content": {
                "summary": {
                    "value": "- The paper introduces Youku-mPLUG, the largest public Chinese video-language dataset and benchmarks, collected from Youku, a Chinese video-sharing website, with strict criteria of safety, diversity, and quality.\n- The paper also proposes mPLUG-video, a decoder-only video-language model that leverages a frozen large language model and a visual abstractor module to reduce the computation burden and improve the performance.\n- The paper evaluates mPLUG-video and other models on three downstream tasks: video category classification, video captioning, and video-text retrieval. The results show that mPLUG-video achieves good results in video category classification and video captioning, and demonstrates impressive zero-shot video instruction understanding ability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a novel and large-scale Chinese video-language dataset and benchmarks, which can facilitate the research and development of video-language models for the Chinese language and culture. The paper also proposes a decoder-only model that leverages a frozen large language model and a visual abstractor module, which is a creative combination of existing ideas that reduces the computation burden and improves the performance.\n- The paper is well-written and organized, with clear figures and tables. The paper provides details and analysis on the proposed method and dataset. \n- The paper explains the problem statement, the motivation, the challenges, and the gap in the existing literature clearly in the abstract and introduction. The paper also describes the dataset collection, annotation, and preprocessing process, and provides some statistics and examples of the data. The paper also explains the model architecture, training, and fine-tuning process, and provides some examples.\n- The paper makes a significant contribution to the field of video-language modeling, especially for the Chinese language and culture. The paper presents a large-scale and diverse dataset that can enable various downstream tasks, such as video category classification, video captioning, video-text retrieval, and video instruction understanding. The paper also presents a state-of-the-art model that can achieve impressive results on these tasks."
                },
                "weaknesses": {
                    "value": "- After downloading the dataset, it was found that there were many duplicate clips from the same source and static clips. Does the situation exist where these 400 million video clips come from the same original video? If so, during the filtering process, how is the quality of the selected videos ensured given the lack of quantifiable performance measures, such as CLIP similarity?\n- There is a lack of exploration into the status of text annotation in the dataset. Chinese and Latin languages such as English have significant differences in vocabulary, grammar, and sentence structure. The diversity of the text part of this dataset is not sufficiently demonstrated, and the text quality is slightly lower compared to the WebVid10M dataset. The paper should also compare the dataset with other existing video-language datasets, such as translated HowTo100M, WebVid10M or CNVid-3.5M[1], and discuss the advantages and limitations of the dataset.\n- This paper only explores the zero-shot capability in instruction understanding. Why not further investigate the zero-shot performance in video classification, retrieval, and description?\n- In instruction understanding, does VideoLLaMA also receive Chinese prompts? Has it been trained on Chinese instruction data? Comparing a MLLM trained on English datasets with one training in Chinese is unfair.\n- During data collection, the online model achieved a performance of about 94% in video category classification. However, in Table 4, the model trained by Youku-mPLUG actually performs worse than the unfiltered online model.\n\n----\n\nReference:\n[1] https://github.com/CNVid/CNVid-3.5M"
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802395655,
            "cdate": 1698802395655,
            "tmdate": 1699636455466,
            "mdate": 1699636455466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PpZGBuhy5U",
                "forum": "mzxKLZNbrQ",
                "replyto": "Qc212gAv2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and suggestions regarding data processing, analysis and experiments. We will now provide a detailed explanation of duplicate video clips, text annotation analysis, additional experiment, and discuss the reasons for the higher performance of the online model.\n\n## Duplicate and Source\n\n**Duplicate**: Thank you for the suggestion. To count the number of static videos, we sample frames from each video and then compute the difference between frames from the same video. Our statistical analysis revealed that a significant majority, 99.6%, of the videos were dynamic, with only a small proportion being entirely static with dynamic audio.\n\n**Source**: The 400 million videos mentioned correspond to the entire content pool of short videos on the Youku website. This vast collection primarily consists of user-generated uploads and platform-generated content. In Section 3.1, we have elucidated the process involved in constructing the pre-training dataset. The videos are carefully classified into 45 diverse categories covering various domains, e.g., Daily life, Comedy, and Pet, with a balanced distribution. If there are any aspects of this that remain unclear, we would be more than happy to provide further clarification.\n\n## Text Annotation\n\n### Translation\n\nThere are several problems with translating English video data into Chinese.\n\n1. Due to the limitations of translation tools and the proficiency of annotators, the translated text often does not take into account the usage habits of Chinese language, and may even introduce errors. To verify this, we used GPT-3.5 to automatically translate a few of captions from webvid2m, and demonstrated the problems caused by translation. For example, the caption \"Woman showing victory sign, indoor\" was translated as \"\u5ba4\u5185\u5973\u6027\u5c55\u793a\u80dc\u5229\u624b\u52bf\". The translation result does not align with the actual Chinese language conventions in terms of word choice and language habits. A more natural Chinese expression should be \"\u623f\u95f4\u91cc\uff0c\u4e00\u4e2a\u5973\u6027\u6b63\u5728\u6bd4\u8036\". Another example is the translation of \"Cottage village surrounded by mountains on the shores of the Atlantic Ocean\" as \"\u5c0f\u5c4b\u6751\u5e84\u73af\u7ed5\u5728\u5927\u897f\u6d0b\u6d77\u5cb8\u7684\u5c71\u8109\u4e0a\", where the translated text expresses mountains surrounded by a village, which is completely different from the original meaning in the English text. These inaccuracies greatly impact the training of the model.\n\n2. We further validated the cultural and linguistic differences between the two by analyzing the word clouds of keywords (translated to Chinese) in the webvid2m dataset (Figure 1) and video keywords in mPLUG-Youku (Figure 2). The webvid2m dataset contains a large number of keywords related to places (e.g., Moscow, Bangkok, Spain) and time periods (e.g., 2019, 2015, 1950s). In Chinese videos, the high-frequency words are mainly related to variety shows (e.g., \"\u738b\u724c\u5bf9\u738b\u724c\" which translates to \"Ace VS Ace\"), games (e.g., \"\u82f1\u96c4\u8054\u76df\" which translates to \"League of Legends\"), film and television works (e.g., \"\u4e61\u6751\u7231\u60c5\" which translates to \"Country Love\"), and celebrities (e.g., \"\u5f20\u5927\u4ed9\" which translates to \"Zhang Daxian\", \"\u5cb3\u4e91\u9e4f\" which translates to \"Yue Yunpeng\"). It can be seen that Chinese videos cover a large number of named entities related to Chinese culture, which is significantly different from the distribution in English videos. If only translated videos are used for training, the model will lack the ability to understand these types of named entities.\n\n**Figure 1**\n![Figure 1](https://z1.ax1x.com/2023/11/16/pitJMPH.png)\n\n**Figure 2**\n![Figure 2](https://z1.ax1x.com/2023/11/16/pitJF2R.png)\n\n3. Although English text can be translated into Chinese, it is difficult to translate audio (e.g., narrations, dialogues) and text in the form of images in the video into Chinese. The learned textual-visual relationship from English content may lead to domain inconsistency when applied to Chinese content.\n\n### Diversity and Quality\n\nFor the pre-training dataset, we filter 10 million high-quality video-text pairs from 400 million raw videos with strict safety, diversity, and quality criteria. In chapter 3.1, we have provided a detailed explanation of the process of constructing pre-training dataset.  Especially, we employ the Chinese image-text pre-trained model CLIP to improve the data quality by deprecating those with low similarities between the mean frame features and text features. The videos are carefully classified into 45 diverse categories covering various domains, e.g., Daily life, Comedy, and Pet, with a balanced distribution in Figure 2,3. Figure3 shows the distribution of clip score.\n\n**Figure 3**\n![Figure 3](https://z1.ax1x.com/2023/11/16/pitJNdS.png)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148305264,
                "cdate": 1700148305264,
                "tmdate": 1700148305264,
                "mdate": 1700148305264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UHUnEX7KFN",
                "forum": "mzxKLZNbrQ",
                "replyto": "Qc212gAv2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## CNVid-3.5M\n\n1. **Open-sourcing**: Our work mainly focuses on open-sourcing the largest Chinese video pre-training dataset and a well-developed benchmark with human-written ground truth The data set has already been released on the open-source platform and been downloaded by more than 26k times. Due to the anonymity policy, we are not sure if we can provide the link during rebuttal, but will definitely attach the link if the work can be accepted. We have been continuously optimizing the download process, including adding streaming and batching downloads. There is no open-source model and code for CNVid-3.5M, and the pre-training dataset has not been open-sourced (https://github.com/CNVid/CNVid-3.5M/issues)\n2. **Benchmark**: we carefully build the largest human-annotated Chinese benchmarks covering three popular video-language tasks across cross-modal retrieval, video captioning, and video category classification.\n3. **Performance**: There is no open-source model and code for CNVid-3.5M, and the pre-training dataset has not been open-sourced. Therefore, we evaluate the results of a structure similar to mPLUG-2 on VATEX and compare it with the model based on CNVid-3.5M. We can find that the models based on our dataset achieve better performance on VATEX \uff08R1+2.7, R5+3.8, R10+2.3.\n\n| |R1|R5|R10\n|-|-|-|-|\nCNVid-3.5M (w/o CNVid-3.5M pt)|36.4|75.4|85.2\nmPLUG-2 (w/o Youku pt)|36.3|74.8|85.7\nCNVid-3.5M (w CNVid-3.5M pt)|41.5|78.2|87.2\nmPLUG-2 (w Youku pt)|44.2|82.0|89.5\n\n4. **Models**: We also provide comprehensive benchmark evaluations of models across different architectures including encoder-only (i.e., ALPRO), encoder-decoder (i.e., mPLUG-2), and decoder-only (i.e., mPLUG-Video) for comparison. Especially, we train the first Chinese Multimodal LLM for video understanding and conduct comprehensive experiments (both benchmark and human evaluation) to validate the effectiveness of pretraining on our datasets.\n\n## Zeroshot\n\nwe directly tested the results of zero-shot methods with pretraining. \n\n**Zero-shot Caption w pretraining**\n\n||BLEU4|METEOR|ROUGE_L|CIDEr|\n|-|-|-|-|-|\n|mPLUG-2|5.9|6.2|13.2|49.2|\n|mPLUG-video (1.3B)|7.1|8.5|17.37|58.9|\n\n**Zero-shot retrieval w pretraining**\n\n| |T2V R1|T2V R5|T2V R10|V2T R1|V2T R5|V2T R10\n|-|-|-|-|-|-|-|\nALPRO|14.17|31.48|39.69|10.43|24.68|31.98\nmPLUG-2|18.14|35.82|43.93|18.14|35.82|43.93\n\n## Evaluation of VideoLLaMA\n\nFor VideoLLaMA, we evaluate the performance of model trained with bilingual backbone (i.e., BiLLA) in our comparison. \n\nBesides, VideoLLaMA utilizes the video instruction data during instructional tuning, while our mPLUG-video only leverages fewer image instruction data than those of VideoLLaMA. Moreover, due to the lack of Chinese instruction data, we utilize the English instruction only, which further demonstrates the effectiveness of our proposed method.\n\n## Performance of Online Model\n\nThe online model we utilize is not a standalone entity, but rather a dedicated pipeline composed of several models and diverse input signals. Specifically, the online model integrates multiple classification models that use user behavior data, similarity retrieval, and user annotations as multi-dimensional information. In contrast, our mPLUG-video operates independently as a single model trained on the downstream task dataset, devoid of any additional features or complexities.\n\nIf you have any more questions or concerns, please feel free to further discuss."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148335107,
                "cdate": 1700148335107,
                "tmdate": 1700148335107,
                "mdate": 1700148335107,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "waX5GcesUo",
                "forum": "mzxKLZNbrQ",
                "replyto": "Qc212gAv2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4734/Reviewer_kj3u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4734/Reviewer_kj3u"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for addressing my concerns; I have gained some clarity on certain concerns. Collecting such a large Chinese video text dataset and open-sourcing it to the public is a very meaningful task.  However, I have some questions:\n\n1. The author claims that only 99.6% of the videos are completely static. The specific method used in calculating the differences between video frames and the detailed description of the criteria or thresholds for determining static videos need further elaboration. Could more examples be provided?\n\n2. The author mentions that the original video pool has 400 million videos, but it was not explained whether the issue of duplicates in different videos was considered when selecting videos. For example, the videos `videos/pretrain/1411131211714-B11-4BY-Eb-CCB172Y-385Aba445Ba5JJYa-7Y4a43A4A4F72FBJ.mp4` and `videos/pretrain/1411131211714-B11bCb4A834BCY3-bC1823C2a3A4Ba5bF2a-bEAa55AJJE25A2Fb.mp4` appear to be similar.\n\n3. According to the word cloud provided by the author, the videos on WebVid appear to be more diverse, while Youku-MPLUG's videos are characterized by keywords related to TV programs or dramas. This discrepancy with the statistics in Figure 2 of the text is somewhat confusing. At the same time, although these captions have undergone manual annotation, it seems that the majority of the captions are primarily static information. Would the author consider comparing them with Chinese image datasets?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632504043,
                "cdate": 1700632504043,
                "tmdate": 1700708724238,
                "mdate": 1700708724238,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g1ZNJaArIX",
            "forum": "mzxKLZNbrQ",
            "replyto": "mzxKLZNbrQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4734/Reviewer_bL3j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4734/Reviewer_bL3j"
            ],
            "content": {
                "summary": {
                    "value": "This paper argues that the development and application of Chinese VLP and multimodal LLM are lagging behind the English counterpart, due to the lack of a large-scale Chinese video-language dataset. Thus, they propose a new dataset Youku-mPLUG, which consists of 10 million Chinese video-text pairs for pertaining, and a dataset with 0.3 million videos for downstream benchmarks, including video-text retrieval, video captioning, and video category classification. Meanwhile, they investigate popular video-language models (e.g., ALPRO, mPLUG-2), and the new proposed model mPLUG-video. The model mPLUG-video consists of a trainable video encoder, a visual abstractor module, and a frozen pre-trained LLM decoder. Experiments show that models pre-trained on Youku-mPLUG gain on multiple tasks. Furthermore, by building on top of Bloomz, mPLUG-video can achieve impressive zero-shot performance with very few trainable parameter."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This paper proposes a large-scale dataset with 10 million Chinese video-text pairs for pertaining, and a dataset with 0.3 million videos for downstream benchmarks. Several off-the-shelf techniques have been used to ensure the high-quality of training videos."
                },
                "weaknesses": {
                    "value": "+ The novelty of the new model mPLUG-video is limited. The proposed three modules, and partially efficient tuning are all well studied techniques in this area.\n\n+ The improvements brought by the proposed mPLUG-video are limited.\n\n+ One of the key contributions in this paper is the proposed new dataset. It would be better to demonstrate the high quality of the newly collected data. Based on the example shown in Figure 9, the text annotations look very noisy."
                },
                "questions": {
                    "value": "In the first paragraph of the introduction section, the authors argue that existing methods of translating English to Chinese suffer intrinsic linguistic and cultural gaps. Could you give more explicit examples to show the harmfulness of these methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839190544,
            "cdate": 1698839190544,
            "tmdate": 1699636455398,
            "mdate": 1699636455398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ymTUXS3bb4",
                "forum": "mzxKLZNbrQ",
                "replyto": "g1ZNJaArIX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for recognizing the quality of our dataset and the technology we have employed. Below, we will first elaborate on the novelty and improvements of our contribution and thoroughly explain and validate the quality of our dataset. Finally, we will use the webvid dataset as an example to demonstrate that relying solely on translated data is far from sufficient.\n\n## Novelty and improvement\n\n**Open-sourcing**: With the overarching aim of advancing the development of Chinese multimodal technology, our work primarily revolves around open-sourcing the largest Chinese video pre-training dataset and establishing a robust benchmark complemented by human-written ground truth. The data set has already been released on the open-source platform and been downloaded by more than 26k times. Due to the anonymity policy, we are not sure if we can provide the link during rebuttal, but will definitely attach the link if the work can be accepted. We are committed to refining the download process continually and have introduced features like streaming and batch downloads to enhance user convenience.\n\n**Models**: we offer comprehensive benchmark evaluations of models across a variety of architectures. This includes encoder-only models (ALPRO), encoder-decoder models (mPLUG-2), and decoder-only models (mPLUG-Video) for comparison purposes.\n\n**MLLM**: we've trained the first Chinese Multimodal LLM specifically for video understanding. To validate the effectiveness of pretraining on our datasets, we have conducted an extensive range of experiments. These include both benchmark evaluations and human assessments.\n\n**Performance**: While the development of a novel model isn't the primary focus of our work, our experiments do yield valuable insights into creating more effective models, such as the potential benefits of increasing model size (as presented in Tables 4 and 6) as well as exploring modality complementarity (as shown in Table 6)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147682882,
                "cdate": 1700147682882,
                "tmdate": 1700147682882,
                "mdate": 1700147682882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rVDoAI9DV1",
                "forum": "mzxKLZNbrQ",
                "replyto": "g1ZNJaArIX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Data Quality\n\n**Contruction**: For the pre-training dataset, we filter 10 million high-quality video-text pairs from 400 million raw videos with strict safety, diversity, and quality criteria. In Section 3.1, we have provided a detailed explanation of the process of constructing pre-training dataset.  Especially, we employ the Chinese image-text pre-trained model CLIP to improve the data quality by deprecating those with low similarities between the mean frame features and text features. Figure 1 shows the distribution of clip score.\n\n**Figure1**\n![Figure1](https://z1.ax1x.com/2023/11/16/pitJNdS.png)\n\n**Common Practice**: The pre-training dataset is primarily aimed at improving the diversity and generalization of the model. Noise is inevitable when generating a huge data set and can be observed in many prevalent data sets, e.g., ImageNet, LAION, etc. While noise may influence the learning process, empirical results show that pre-training is robust to a small amout of noise and still can obtain impressive models from them, e.g., OpenCLIP by training on LAION. Our experiments also confirms the effectiveness of the proposed data set in Table 5.\n\n**Performance**: Table 5 also demonstrates models can benefit from pre-training on Youku-mPLUG. In specific, by pretraining on Youku-mPLUG, the model's performance has increased by 20.4 point of CIDEr on VATEX datasets. Meanwhile, the result in Table 7 also demonstrates the effectiveness of our proposed dataset as the accuracy has increased 8.7 points on the classification task.\n\n**Downstream Task**: The data quality of downstream tasks is  high. For each downstream task, we hire well-educated people and adopt a two-step verification to ensure the quality and diversity of the annotations.\n\n**Open-sourcing**: Our work mainly focuses on open-sourcing the largest Chinese video pre-training dataset and a well-developed benchmark with human-written ground truth. The data set has already been released on the open-source platform and been downloaded by more than 26k times. Due to the anonymity policy, we are not sure if we can provide the link during rebuttal, but will definitely attach the link if the work can be accepted . We have been continuously optimizing the download process, including adding streaming and batching downloads. \n\n## Translation\n\nThere are several problems with translating English video data into Chinese.\n\n1. Due to the limitations of translation tools and the proficiency of annotators, the translated text often does not take into account the usage habits of Chinese language, and may even introduce errors. To verify this, we used GPT-3.5 to automatically translate a few of captions from webvid2m, and demonstrated the problems caused by translation. For example, the caption \"Woman showing victory sign, indoor\" was translated as \"\u5ba4\u5185\u5973\u6027\u5c55\u793a\u80dc\u5229\u624b\u52bf\". The translation result does not align with the actual Chinese language conventions in terms of word choice and language habits. A more natural Chinese expression should be \"\u623f\u95f4\u91cc\uff0c\u4e00\u4e2a\u5973\u6027\u6b63\u5728\u6bd4\u8036\". Another example is the translation of \"Cottage village surrounded by mountains on the shores of the Atlantic Ocean\" as \"\u5c0f\u5c4b\u6751\u5e84\u73af\u7ed5\u5728\u5927\u897f\u6d0b\u6d77\u5cb8\u7684\u5c71\u8109\u4e0a\", where the translated text expresses mountains surrounded by a village, which is completely different from the original meaning in the English text. These inaccuracies greatly impact the training of the model.\n2. We further validated the cultural and linguistic differences between the two by analyzing the word clouds of keywords (translated to Chinese) in the webvid2m dataset (Figure 2) and video keywords in mPLUG-Youku (Figure 3). The webvid2m dataset contains a large number of keywords related to places (e.g., Moscow, Bangkok, Spain) and time periods (e.g., 2019, 2015, 1950s). In Chinese videos, the high-frequency words are mainly related to variety shows (e.g., \"\u738b\u724c\u5bf9\u738b\u724c\" which translates to \"Ace VS Ace\"), games (e.g., \"\u82f1\u96c4\u8054\u76df\" which translates to \"League of Legends\"), film and television works (e.g., \"\u4e61\u6751\u7231\u60c5\" which translates to \"Country Love\"), and celebrities (e.g., \"\u5f20\u5927\u4ed9\" which translates to \"Zhang Daxian\", \"\u5cb3\u4e91\u9e4f\" which translates to \"Yue Yunpeng\"). It can be seen that Chinese videos cover a large number of named entities related to Chinese culture, which is significantly different from the distribution in English videos. If only translated videos are used for training, the model will lack the ability to understand these types of named entities.\n\n**Figure 2**\n![Figure 2](https://z1.ax1x.com/2023/11/16/pitJMPH.png)\n\n**Figure 3**\n![Figure 3](https://z1.ax1x.com/2023/11/16/pitJF2R.png)\n\n3. Although English text can be translated into Chinese, it is difficult to translate audio (e.g., narrations, dialogues) and text in the form of images in the video into Chinese. The learned textual-visual relationship from English content may lead to domain inconsistency when applied to Chinese content.\n\nIf you have any more questions or concerns, please feel free to further discuss."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147708706,
                "cdate": 1700147708706,
                "tmdate": 1700147708706,
                "mdate": 1700147708706,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F2AGmEZblC",
            "forum": "mzxKLZNbrQ",
            "replyto": "mzxKLZNbrQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4734/Reviewer_umKn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4734/Reviewer_umKn"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce Youku-mPLUG, the largest high-quality video-language dataset in Chinese. And present a human-annotated benchmark encompassing three downstream tasks: Video-Text Retrieval, Video Captioning and Video Classification. The authors also propose modularized mPLUG-video, a decoder-only model that is pre-trained on Youku-mPLUG, which gain a state-of-the-art result on theses benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is going to release a 10 million Chinese video-language pretraining dataset and provide benchmarks on different model architectures, which is in great demand by the field.\n    \n- This dataset seems to be of high quality (hire well-educated people to double check the data) and well-curated (filtered 10 million Chinese video-text pairs out of 400 million raw videos).\n    \n- Propose a modularized decoder-only mPLUG-video model and achieves state-of-the-art results on these benchmarks."
                },
                "weaknesses": {
                    "value": "- The experiments are not very comprehensive. The selected baseline models in different downstream tasks is limited, two were selected only.\n    \n- No details about the selection of the original 400 million videos are provided."
                },
                "questions": {
                    "value": "This paper mentions that currently existing large-scale Chineses video-language datasets are not publicly accessible. This also demonstrates that not only the collection and curation of large datasets are challenging, but the release process is also difficult. Could the authors provide their plans to prove that you can genuinely release this dataset and make it easily accessible to researchers, thus making a real contribution to the research community?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839973945,
            "cdate": 1698839973945,
            "tmdate": 1699636455319,
            "mdate": 1699636455319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lPJUt0lNnN",
                "forum": "mzxKLZNbrQ",
                "replyto": "F2AGmEZblC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your recognition of the quality and importance of our dataset. We have included more detailed comparisons as well as clarification on data selection and dataset release. We give a comprehensive disscusion below.\n\n## Fine-tuning\n\nDue to the limited time for rebuttal and the high cost of pre-training, we directly tested the results of fine-tuning baselines. \n\n| |T2V R1|T2V R5|T2V R10|V2T R1|V2T R5|V2T R10\n|-|-|-|-|-|-|-|\nHiTeA|9.42|27.47|38.38|9.99|28.04|38.54\nALPRO|8.90    |26.76|36.29|8.87|26.45|37.80\nmPLUG-2|11.46|28.24|39.06|10.21|30.27|41.75\n\nIn addition, Table5 also demonstrates models can benefit from pre-training on Youku-mPLUG with 20.4 improvement. It would be greatly appreciated if you could provide the details about the appropriate baselines, e.g., name of papers. We will included them in the revised version. \n\n## Dataset selection\n\nThe 400 million videos mentioned correspond to the entire content pool of short videos on the Youku website. This vast collection primarily consists of user-generated uploads and platform-generated content. In Section 3.1, we have elucidated the process involved in constructing the pre-training dataset. If there are any aspects of this that remain unclear, we would be more than happy to provide further clarification.\n\n## Dataset release\n\nWe appreciate your acknowledgement of our efforts in constructing the dataset and our contributions to the open-source community. The data set has already been released on the open-source platform and been downloaded by more than 26k times. Due to the anonymity policy, we are not sure if we can provide the link during rebuttal, but will definitely attach the link if the work can be accepted.  We are committed to continuously optimizing the download process to make it more user-friendly, which includes the implementation of features such as streaming and batch downloads.\n\nIf you have any more questions or concerns, please feel free to further discuss."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147040048,
                "cdate": 1700147040048,
                "tmdate": 1700147040048,
                "mdate": 1700147040048,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EevoFiw57w",
            "forum": "mzxKLZNbrQ",
            "replyto": "mzxKLZNbrQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4734/Reviewer_HLpF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4734/Reviewer_HLpF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Youku-mPLUG, a high-quality video-language dataset in Chinese, along with a human-annotated benchmark comprising three downstream tasks. The experiments on downstream tasks (i.e. Video-Text Retrieval, Video Captioning, and Video Category Classification) evaluate the video language comprehension and modeling abilities of models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tYouku-mPLUG is currently the largest Chinese video-language dataset.\n2.\tThe exploration of different architectures (like encoder-only, encoder-decoder, decoder-only) is well done."
                },
                "weaknesses": {
                    "value": "1.\tThe zero-shot experiment is too simple. The authors should evaluate on video-text retrieval task using more models and other pre-train datasets quantitatively.\n2.\tThe results in Table 5 are not convincing enough. The authors only compare one publicly available dataset VATEX and do not show a gap with current state-of-the-art results. \n3.\tIncorrect paragraph spacing in the second and third paragraphs in \u201c2 RELATED WORD\u201d section."
                },
                "questions": {
                    "value": "Data augmentation will almost certainly bring performance improvements to the model. Therefore, how to prove that Youku-mPLUG is superior to other dataset like CNVid-3.5M?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4734/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699189113103,
            "cdate": 1699189113103,
            "tmdate": 1699636455245,
            "mdate": 1699636455245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jbZsEmnTMy",
                "forum": "mzxKLZNbrQ",
                "replyto": "EevoFiw57w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4734/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your recognization of the dataset and your suggestions on our experiments and writing. We have included additional zero-shot experiment results on our dataset as well as comparisons on the CNVid-3.5M dataset across different aspects to address your concerns. Below, we detailly discuss the results.\n\n## Fine-tuning and zero-shot\n\nDue to the limited time for rebuttal and the high cost of pre-training, we directly tested the results of zero-shot methods with pretraining and fine-tuning baselines without pretraining. \n\n**Zero-shot Caption w/ pretraining**\n\n||BLEU4|METEOR|ROUGE_L|CIDEr|\n|-|-|-|-|-|\n|mPLUG-2|5.9|6.2|13.2|49.2|\n|mPLUG-video (1.3B)|7.1|8.5|17.37|58.9|\n\n**Zero-shot retrieval w/ pretraining**\n\n| |T2V R1|T2V R5|T2V R10|V2T R1|V2T R5|V2T R10\n|-|-|-|-|-|-|-|\nALPRO|14.17|31.48|39.69|10.43|24.68|31.98\nmPLUG-2|18.14|35.82|43.93|18.14|35.82|43.93\n\n**Fintuning retrieval w/o pretraining**\n\n| |T2V R1|T2V R5|T2V R10|V2T R1|V2T R5|V2T R10|\n|-|-|-|-|-|-|-|\nHiTeA|9.42|27.47|38.38|9.99|28.04|38.54\nALPRO|8.90    |26.76|36.29|8.87|26.45|37.80\nmPLUG-2|11.46|28.24|39.06|10.21|30.27|41.75\n\nIn addition, Table5 also demonstrates models can benefit from pre-training on Youku-mPLUG with 20.4 improvement. It would be greatly appreciated if you could provide the details about the appropriate baselines, e.g., name of papers. We will included them in the revised version. \n\n## VATEX\n\nDue to the lack of open-sourcing Chinese pre-training datasets and benchmarks, there are very few public Chinese multimodal models. There is no existing method to evaluate the Vatex Chinese Caption dataset. Our mPLUG-Video model based on the Youku dataset is currently the state-of-the-art (SOTA) method for the Vatex Chinese Caption dataset. \n\nAnd we reproduced classic English models with different structures (encoder-only (i.e., ALPRO), encoder-decoder (i.e., mPLUG-2), and decoder-only (i.e., mPLUG-Video)) for comparison. Due to limited computing power and high pre-training costs, we have only reproduced these baselines.  Any suggestions about other baselines are welcome and can be included in the revised version.\n\n## CNVid-3.5M\n\n1. Open-sourcing: Our work mainly focuses on open-sourcing the largest Chinese video pre-training dataset and a well-developed benchmark with human-written ground truth. The data set has already been released on the open-source platform and been downloaded by more than 26k times. Due to the anonymity policy, we are not sure if we can provide the link during rebuttal, but will definitely attach the link if the work can be accepted. We have been continuously optimizing the download process, including adding streaming and batching downloads. There is no open-source model and code for CNVid-3.5M, and the pre-training dataset has not been open-sourced (https://github.com/CNVid/CNVid-3.5M/issues)\n2. Benchmark: we carefully build the largest human-annotated Chinese benchmarks covering three popular video-language tasks across cross-modal retrieval, video captioning, and video category classification.\n3. Performance: There is no open-source model and code for CNVid-3.5M, and the pre-training dataset has not been open-sourced (https://github.com/CNVid/CNVid-3.5M/issues). Therefore, we evaluate the results of a structure similar to mPLUG-2 on VATEX and compare it with the model based on CNVid-3.5M. We can find that the models based on our dataset achieve better performance on VATEX (R1+2.7, R5+3.8, R10+2.3).\n\n| |R1|R5|R10\n|-|-|-|-|\nCNVid-3.5M (w/o CNVid-3.5M pt)|36.4|75.4|85.2\nmPLUG-2 (w/o Youku pt)|36.3|74.8|85.7\nCNVid-3.5M (w CNVid-3.5M pt)|41.5|78.2|87.2\nmPLUG-2 (w Youku pt)|44.2|82.0|89.5\n\n4. Models: We also provide comprehensive benchmark evaluations of models across different architectures including encoder-only (i.e., ALPRO), encoder-decoder (i.e., mPLUG-2), and decoder-only (i.e., mPLUG-Video) for comparison. Especially, we train the first Chinese Multimodal LLM for video understanding and conduct comprehensive experiments (both benchmark and human evaluation) to validate the effectiveness of pretraining on our datasets.\n\nIf you have any more questions or concerns, please feel free to further discuss."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4734/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146876393,
                "cdate": 1700146876393,
                "tmdate": 1700146876393,
                "mdate": 1700146876393,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]