[
    {
        "title": "Reward Design for Justifiable Sequential Decision-Making"
    },
    {
        "review": {
            "id": "ib3BaEjhLj",
            "forum": "OUkZXbbwQr",
            "replyto": "OUkZXbbwQr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3851/Reviewer_pkzb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3851/Reviewer_pkzb"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a date framework for reward-shaping and quantifying the justifiability of a decision at a given state. An application is outlined for treating sepsis patients."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Paper is clear and straight to the point. \n\n* The authors provide adequate motivation for their model, which is quite natural.\n\n* I believe that the problem the authors attempt to address is important for any practical use of RL."
                },
                "weaknesses": {
                    "value": "* No real weakness. Some slight inaccuracies in the terminology/in some definitions (see my questions below) but this should be easily fixable."
                },
                "questions": {
                    "value": "1. Bottom of page 1, \u201cthe human as a suboptimal decision maker\u201d: do you mean that the RL agent is optimal? But if optimality refers to the actions deployed in practice, the RL agent will not be optimal either, no?\n\n2. The authors in [A,B] also consider sequential decision-making (MDPs)  influenced by a baseline policy and an agent to model partial adherence to an algorithmic recommendation. Is the setup in this submission close to the setup in [A,B]?\n\n3. Definition of $R_{J}$ : what is the expectation over? The transition probabilities (since policies are deterministic)? Is the decision to be optimized $a_{t}$ or $a^{B}_{t}$ ?\n\n4. p4: the definition of $\\mathbb{U}$ introduces the notation $\\mathcal{J}$ but the next sentence uses $J$. Please clarify.\n\n5. Section 4.4: can you please justify the exact architecture and neural nets used in the paper (two-fully connected layer, size 256, choice of parametric relu, etc.)? This section appears very ad-hoc, which it may be, and that is fine, but it requires some discussion/empirical justification.\n\n[A] Grand-Cl\u00e9ment, J., & Pauphilet, J. (2022). The best decisions are not the best advice: Making adherence-aware recommendations.\u00a0arXiv preprint arXiv:2209.01874. \n\n[B] Faros, I., Dave, A., & Malikopoulos, A. A. (2023). A Q-learning Approach for Adherence-Aware Recommendations.\u00a0arXiv preprint arXiv:2309.06519."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3851/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3851/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3851/Reviewer_pkzb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697535228652,
            "cdate": 1697535228652,
            "tmdate": 1699636343296,
            "mdate": 1699636343296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VlJ1Zd1ftl",
                "forum": "OUkZXbbwQr",
                "replyto": "ib3BaEjhLj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pkzb"
                    },
                    "comment": {
                        "value": "Thank you for your comments and questions. We are glad to see you consider the problem of justifiable decision-making to be of high practical importance, and that you consider our presentation to be clear and straight to the point. We are also thankful for the references you have provided, and are happy to discuss them further. In the following, we try to answer all of your questions and comments. The key points of our response can be summarized as follows:\n- We discuss similarities and differences to the two works you kindly suggested;\n- We answer your questions about optimality of deployed policies, definition of $R_J$ and choices of model architectures.\n\n## Discussion on General Comments\n\n> Bottom of page 1, \u201cthe human as a suboptimal decision maker\u201d: do you mean that the RL agent is optimal? But if optimality refers to the actions deployed in practice, the RL agent will not be optimal either, no?\n\nWhat we intended to say there is that a human may not act optimally or rationally in every situation, as it is inherently limited in capacity. To extend on this, our line of reasoning touched upon the fact that in certain situations, a human may not be able to comprehend the full state available to the acting agent, which may hinder its judgment. We also acknowledge your comment about optimality of the deployed agent. Indeed, the agent is learned, and will therefore also be suboptimal, although the referenced sentence does not imply any assumptions about this. Lastly, we have slightly expanded Sec. 3.1 to better clarify the assumptions made by our framework about optimality and choice of the baseline policy. \n\n> The authors in [A,B] also consider sequential decision-making (MDPs) influenced by a baseline policy and an agent to model partial adherence to an algorithmic recommendation. Is the setup in this submission close to the setup in [A,B]?\n\nThank you very much for sharing these works. We have also updated the related work section to include them. Regarding [A], we find a conceptual similarity in that both approaches aim to improve what is assumed to be a given baseline policy. However, [A] operates in a context of expert-in-loop setting, where a baseline policy is considered to represent a human decision-maker and a policy to be learned (in their terms, recommendation policy) aims to adapt its actions to it. Another prominent difference is found in a way the environment evolves: in [A] both the baseline policy and the learned policy can influence dynamics of the environment. Differently, in our setup, baseline policy never influences state transitions: the justifiable agent is a sole actor in the environment. Likewise, authors in [B] examine a setting similar to the one in [A], which also exhibits mentioned similarities and differences. As a general note, both [A] and [B] consider a different problem setup, where the primary goal is to improve and adapt the AI policy to the baseline policy representing a human, which is the main decision-maker in the environment. In contrast, we consider a problem of improving the baseline policy in a setup where a learned (i.e., justifiable) policy is the main actor in the environment, and a baseline policy is only used as a reference point to improve upon.\n\n> Definition of $R^J$: what is the expectation over? The transition probabilities (since policies are deterministic)? Is the decision to be optimized $a_t$ or $a_t^B$?\n\nIn the definition of the justifiability reward $R^J$, the expectation is over the trajectories, which are stochastic due to non-deterministic transitions. The decision to be optimized is the one coming from the justifiable policy, namely $a_t$, whereas the decision $a_t^B$ is coming from a (fixed) baseline policy. We have also slightly updated the paper to make this part more easily understandable to the reader.\n\n> p4: the definition of U introduces the notation J but the next sentence uses J. Please clarify.\n\nThank you for noticing inconsistencies in our presentation here. The definition of U introduces J as a model of a human judge. Later mentions also refer to the same entity. We have corrected the paper accordingly."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136150428,
                "cdate": 1700136150428,
                "tmdate": 1700551921001,
                "mdate": 1700551921001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ULZixpayvR",
            "forum": "OUkZXbbwQr",
            "replyto": "OUkZXbbwQr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3851/Reviewer_2TSd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3851/Reviewer_2TSd"
            ],
            "content": {
                "summary": {
                    "value": "General Approach:\nThis paper introduces a method of learning policies for sequential decisions that are justifiable to a (human) judge. The general technique is to use reward shaping, where the reward of an RL agent is determined jointly by the environment and by the judge, and parameterized by a coefficient that determines the balance between performance (determined by environment) and justifiability (determined by the judge). The novel aspect of the approach is the construction of the reward from the judge using a debate game. The reasons given for using a zero-sum debate game in extensive form is to ensure that (1) the evidence presented by the agent is sufficient yet concise, and (2) the argument (collection of evidence) should not be easy to refute by providing additional evidence to the judge.\n\nDebate Game:\nThe debate game is a zero-sum, perfect-information game between a baseline agent and an agent trying to learn more justifiable decisions, where the strategy space is composed of the evidence that each agent can introduce at each stage in the game, and the utilities/payoffs are determined by the judgement of the judge. Importantly, the number of steps in the game, and hence the size of the ultimate evidence set, is much smaller than the set of possible elements that can be added as evidence. The judge provides real-valued judgements J(action, evidence set). Two two agents each suggest an action, and then take turns introducing elements to evidence to justify their action, so they end up each introducing half of the evidence in the evidence set. The judge then judges each action with respect to the evidence set, and the agent whose action is preferred by the judge based on the evidence gets a utility of +1 (and the other -1). The reward from the judge is then proportional to the value of the game (or, rather, an approximation of it).\n\nAn empirical study is done on data for patients with Sepsis, including comparison to trainign with only environment-based and only debate-based rewards and agents who learn without an adversary."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, the paper is well-organized, the problem is well-motivated, and some of the empirical results are highly compelling (i.e. Figs 2c and 2d). This approach provides one way of capturing the tradeoff between learning an optimally performing policy and one that is justifiable to a human agent, creating the ability to interpolate in between. The use of a debate game is a novel contribution the should, in theory, help train the RL agent to produce decisions whose justifications achieve robustness in the face of new information."
                },
                "weaknesses": {
                    "value": "There are some weaknesses in the methodology and presentation of the work, but these are relatively minor. My biggest concern are the ethical considerations that are not addressed. My review would be to accept if not for the severe ethical implications of the work that have gone unstated.\n\nMethodology and Presentation:\nThe model of the game seems more complex than necessary, and bakes in assumptions that don\u2019t have clear motivation. Specifically, both agents must provide equal amounts of information, in alternating fashion. Why not simply have a game where the justifying agent provides some number of pieces of evidence L and the adversarial (baseline or confuser) agent provides any number of additional pieces of information? What is the benefit of creating a game with L stages where the agents alternate with best response? The paper motivates the use of Nash Equilibria to ensure robustness against additional information, but no motivation is given for this multi-stage debate game instead of letting each agent make 1 move each.\n\nA missing benchmark is how often the benchmark and justifying agents choose the same action. This would be highly informative.\n\n\nUnless I misunderstand the methodology, the benchmark with the isolated agent does not seem entirely appropriate. The isolated agent is trained to provide L pieces of evidence and then tested in a setting where it provides L/2 and a confuser provides L/2. It is not surprising that it is vulnerable to a confuser that provides the evidence least-supportive of its L/2 pieces of information. A better benchmark would be to train the isolated in a setting where it only provides L/2 pieces of information, and then see how robust it is to the setting where L/2 adversarially selected pieces of information are added.\n\nIn Fig 2a and 3a-d, the clinician\u2019s performance is compared to the learned policies according to WIS. But why would WIS be a reasonable way to measure the performance of the real-world clinician based on clinical decisions?\n\nThe payoff function G is introduced, but it is never specified how it relates to the utility function U.\n\nThe use of the \u201cBradley-Terry model\u201d (based on the Boltzmann distribution) is never given a justification.\n\nAlso, in the real world, not all pieces of evidence have equal complexity to evaluate, not all combinations of evidence have equal complexity to evaluate.\n\nMinor Errors:\n\nTypo: (Section 3.1) \\gamma^{i} instead of \\gamma^t in multiple places\n\nTypo (Section 3.2):  after the definition of U the text uses J instead of \\mathcal{J}\n\nThere doesn\u2019t seem to be any need to introduce the notation \\mathcal{E}(n). Just use \\mathcal{E} \\backslash n.\n\nTypo (Section 5.3, p2): \u201c\u2026even tough\u201d \u2014> \u201c\u2026even though\u201d\n\nTypo (Figure 4): The legend labels for \u201cwith confused\u201d and \u201cwithout confused\u201d appear to be inconsistent with the plots."
                },
                "questions": {
                    "value": "Isn\u2019t the value of the game always in {1, -1}?\n\nWhy does J need to be real-valued? Isn\u2019t it enough just to be a weak order over action-evidence pairs? The actual value of J seems irrelevant to the model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The method being designed is proposed for a beneficial task - improving medical treatment. However, it seems as though it would be just as easily used (and perhaps more effective) to implement it for nefarious purposes. For example, consider a chatbot being trained to get victims to reveal private personal information. The chatbot seeks to learn a strategy that maximizes value from the information victims reveal and learns what information/evidence/text to provide the agents that seems most compelling to them. It provides the arguments that are most difficult for the victims to refute, and learns from the arguments others have provided as responses in the past. In that case the judge and the confuser are the same agent. The problem is that the \u201cperformance\u201d may be based on a nefarious objective, and ultimately the debate reward simply trains the justifying agent how to most effectively deceive the judge. One can imagine many more similarly nefarious scenarios that come from this reward-shaping approach, and learning to maximize justifiability in general. Without constraints, or formal statements about what information must be provided to ensure transparency, learning justifiability necessarily implies learning deception. Static methods like SHAP that do not try to learn justifiability do not pose a similar risk."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3851/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3851/Reviewer_2TSd",
                        "ICLR.cc/2024/Conference/Submission3851/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707884649,
            "cdate": 1698707884649,
            "tmdate": 1700687618615,
            "mdate": 1700687618615,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bE1Ne5ytN6",
                "forum": "OUkZXbbwQr",
                "replyto": "ULZixpayvR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2TSd"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review, perceptive comments and questions. We are glad to see you find the problem to be of importance and that you think our method has merits. In the following, we try to answer all of your questions and comments. The key points of our response can be summarized as follows:\n- We address your comment about ethics concerns pertaining to our method;\n- We address your questions and concerns about an additional benchmark comparing actions proposed by the justifiable and baseline agents, and confuser evaluation of the isolated agent with additional experiments;\n- We address your questions about the complexity of the debate and evaluation of evidence, payoff function G, evaluation of the clinician and choice of the Bradley-Terry model.\n\n## Questions Involving Additional Experiments\n\n> A missing benchmark is how often the benchmark and justifying agents choose the same action. This would be highly informative.\n\nThank you for pointing this out. We acknowledge that there is an opportunity to provide richer information to the reader in this regard. To address this, in App. D.2 and Plot 6a, we show the percent of times an action from the justifiable policy was preferred to that of the baseline policy (JP), percent of times an action of the baseline policy was preferred to that of the justifiable policy (BP) and percent of times when the two were equally preferred (EP). In addition, we updated the Plot 2b to show the percent of times an action from the justifiable policy was preferred to that of the baseline policy, but only when the two differ. In our experiments, the judge deemed two actions equally justifiable only when they were the same.\n\n> Unless I misunderstand the methodology, the benchmark with the isolated agent does not seem entirely appropriate. The isolated agent is trained to provide L pieces of evidence and then tested in a setting where it provides L/2 and a confuser provides L/2. It is not surprising that it is vulnerable to a confuser that provides the evidence least-supportive of its L/2 pieces of information. A better benchmark would be to train the isolated in a setting where it only provides L/2 pieces of information, and then see how robust it is to the setting where L/2 adversarially selected pieces of information are added.\n\nThank you for raising this concern. The isolated argumentative agent was initially trained to propose L=6 evidence, whereas during evaluation against a confuser, it was proposing L=3 evidence in a debate-like setup. As a response to your comment, we introduce a new setup for the isolated agent (also described as part of the changes made to the paper in our general response). In the new version, the isolated agent is trained to propose L=3 evidence. When evaluating robustness, we do not perform debate, but rather let the agent first propose L=3 evidence, followed by a confuser proposing the remainder. We report the new results in the main text (Sec. 5.3, Plot 4b). We also include the previous setup and do a comparison to the updated one in App. C.2.1 and App. D.3. The conclusions we drew about robustness of the isolated agent in Sec. 5.4 remain the same in both cases."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136034841,
                "cdate": 1700136034841,
                "tmdate": 1700136034841,
                "mdate": 1700136034841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WyVBox3vWb",
                "forum": "OUkZXbbwQr",
                "replyto": "bE1Ne5ytN6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3851/Reviewer_2TSd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3851/Reviewer_2TSd"
                ],
                "content": {
                    "title": {
                        "value": "Additional Experiments / Author Response"
                    },
                    "comment": {
                        "value": "The additional work you provided addresses both of my concerns in full. I also found your responses to other reviewers to be generally thoughtful. This will be reflected in my final review.\n\nIt is good to see an explicit recognition of the theoretical considerations. Of course, the acknowledgment is important, but there remains the ethical question of whether such research directions should be pursued and published given the potential for abuse. I hope the authors take these risks into account when deciding what problems to work on, and not only acknowledge them when publishing."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687559686,
                "cdate": 1700687559686,
                "tmdate": 1700687559686,
                "mdate": 1700687559686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n6tlubzICJ",
            "forum": "OUkZXbbwQr",
            "replyto": "OUkZXbbwQr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3851/Reviewer_8w5y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3851/Reviewer_8w5y"
            ],
            "content": {
                "summary": {
                    "value": "This work describes a framework for producing RL agents which take \"justifiable\" actions. This is done by defining a \"debate game\" for a state and a pair of actions, in which two adversarial debaters (each representing one of the actions) take turns picking \"evidence\" (aka features or factors) from the state, each trying to maximize the chance that a synthetic \"judge\" which takes in all the evidence as input will rate their action higher. A justifiable action is thus an action for which the outcome of the action's debate game does indeed result in the judge rating the action highly. \n\nThe paper describes experiments performed in the RL environment of providing interventions for patients with Sepsis. The work compares a baseline (a standard RL agent trained to maximize reward in the environment) with the novel approach (an RL agent trained to maximize a combination of reward from the environment and reward from the judge's judgment of the result of the debate game for the chosen action vs. the baseline action). The optimal policies for the debate game are trained using self-play RL (the debate game is perfect-information). There is a dataset containing real clinicians' interventions in the Sepsis setting. The judge is trained to predict, for each state in the dataset, the clinician's actual action based on evidence which is sampled uniformly random from the available evidence.\n\nThe work is highly related to explainability. Its methodology is similar to research in learning pairwise preferences.\n\nMany experiments are performed, and there is ample discussion about motivations and future research.\n\n---\n\nPost-rebuttal period update: I originally rated this paper a 6. The authors addressed some comments I made, and performed additional experiments. If I could, I would raise my rating to something between 6, and 8 (but unfortunately such an integer does not exist). I still don't like that the experimental design doesn't exactly match the motivation, but the updated version more explicitly acknowledge this, and the paper is still an interesting proof-of-concept. Finally, with the addition of the new experiments, the paper contains so many experiments to investigate questions that one could want to know the answer to, that its value passes the bar for publication. Therefore, I decided to break the tie between 6 and 8 by rating the paper an 8."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is my first time hearing about debate agents, so my to eyes the very idea of using debate to train aligned agents is novel. However, this may not have been true if I had previously read one of the Debate papers in the Related Work section. Still, the research direction seems exciting, and this paper is a solid proof-of-concept exploring the specific idea of using a judge's evaluation of the result of a debate game as RL reward.\n\nThere were many experiments performed, and they answer much of the questions one would like to see answered about this work. \n\nThe writing is good. I especially enjoyed reading the Introduction and Formal Setup."
                },
                "weaknesses": {
                    "value": "- The most glaring flaw is that the paper gives its setup as being motivated by requiring agents' actions to be justifiable to humans, but the actual methodology is a few steps removed from humans: both in the training (the synthetic judge is *not* trained on a dataset of humans ranking actions by their justifiability, but instead on a dataset of ground-truth clinician actions) and in evaluation (the evaluations measure the synthetic judge's judgments, instead of human judgments). I feel that the paper would be stronger with at least human evaluations, if not human preference data for training.\n  - Also, the abstract says that \"we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and *closely aligns with human preferences*.\" Which human preferences is this referring to? Does it just mean that the actions taken are similar to the actual clinicians' actions? Does this presuppose that clinicians' actions == justifiable actions? If so, is this stated anywhere?\n\n\n\nMinor:\n\n- The definition of the Preference Dataset (Section 4.1) seems strange: the preference indicates which of the two decisions is more justified by a given set of evidence in a particular state. But what is that set of evidence? Shouldn't that evidence be included in the tuple? Indeed, in the experiments, the preference dataset used does not fulfill this definition, since $p$ simply indicates the actions that the real clinicians took, not the action that they thought was more justifiable given the (sampled at random) evidence.\n\n\nNitpicks:\n- Perhaps footnote 3 is important enough to not be a footnote. I was searching for a bit to figure out where $a^B_t$ came from in the experiments.\n\n---\n\nI think this paper isn't perfect but it's a good proof-of-concept of an interesting idea, so I recommend it for (borderline) acceptance."
                },
                "questions": {
                    "value": "- In the abstract: the reward from the debate-based reward model yields \"effective policies highly favored by the judge when compared to [the baseline]\" -- the obvious question in the reader's mind is whether the \"effective policies\" actually yield higher environment reward than the baseline or not, and it would be nice to not have that ambiguity. If I'm reading the results correctly, they yielded lower performance, so maybe something like \"while being nearly as effective as [the baseline]\" or \"while barely sacrificing any performance\" would be nice.\n- Shouldn't the description of the Baseline Agent (Section 3.1) indicate that the baseline agent is learned and therefore in practice the baseline agent is approximately optimal, but not necessarily exactly optimal?\n- In the definition of the Justifiable Agent (Section 3.1), I think it would be helpful to explain what $a^B_t$ is. Even though it's described in the next section, it would clarify the dependence of $r^d$ and thus $R_J$ on the baseline agent's policy right away.\n- Debate Game (Section 3.2): I'm just curious: Did you consider defining U to be the difference between the judge's judgments instead of the binary +1, -1? If you did, why did you choose not to use it?\n- Using uniform random evidence to train the judge is surprising at first. Do you think it works well? Are there any alternatives? (can you keep training the judge in a loop while the argumentative agents train, perhaps?) An ideal dataset would actually include pairs of action + evidence, and human rankings over those, right?\n- Creating the synthetic dataset by comparing a random action with the clinician's action seems like it would result in a lot of pairs in the dataset where one action is clearly superior to the other. Is this true? Does this cause any issues?\n- In Section 5.4: \"not only can agents significantly amplify the capabilities of a judge, but they also manage to recognize and correctly convince it of its true underlying preference\" -- what are those two clauses referring to? I can't see how they're different.\n- Section 5.4: Am I reading correctly that the isolated argumentative agent is trained with L = 6, yet in this setting it's only allowed to submit 3 evidences? How does that work (does the isolated one just pick 3 without regards to the choice of confuser, while the confuser picks 1 evidence after each isolated choice)? Wouldn't it be more correct to train it with L = 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3851/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3851/Reviewer_8w5y",
                        "ICLR.cc/2024/Conference/Submission3851/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819034490,
            "cdate": 1698819034490,
            "tmdate": 1700247203513,
            "mdate": 1700247203513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AvhLfuV5C7",
                "forum": "OUkZXbbwQr",
                "replyto": "n6tlubzICJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8w5y"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments and questions. We are happy to see that you overall enjoyed the paper and that you consider the idea of using debate in this context an interesting research direction. In the following, we try to answer all of your questions and comments. The key points of our response can be summarized as follows: \n- We address your questions pertaining to the use of judge\u2019s reward difference in the utility function, choice of an alternative action in the preference dataset and confuser evaluation of the isolated agent with additional experiments;\n- We clarify the definition of the preference dataset from Sec. 4.1, made assumption about the justifiable actions, sampling of evidence during training of the judge and approximate optimality of the baseline agent;\n\n## Questions Involving Additional Experiments\n\n> Debate Game (Section 3.2): I'm just curious: Did you consider defining U to be the difference between the judge's judgments instead of the binary +1, -1? If you did, why did you choose not to use it?\n\nThank you for the comment. We apologize for the confusion that may have occurred due to our mistake in writing of the utility function (Sec. 3.2). Throughout the paper, we assumed that when two actions are equally justifiable (e.g., as in a case when they are equal) the debate game draws, ending in both players obtaining a score of 0.\n\nInitially, we have only considered defining the debate utility function using rewards +1, 0, -1. However, the idea you put forward sounds interesting, and we examine it in more details. In particular, we have retrained maxmin and self-play agents with the utility function defined using a difference between predicted justifiability reward, namely $\\\\mathbb{U}(a_t, a_t^B, \\\\{e\\\\}) = \\\\mathcal{J}(a_t, \\\\{e\\\\}) - \\\\mathcal{J}(a_t^B, \\\\{e\\\\})$. In addition, we also train new confuser agents, so we can compare both effectiveness and robustness of the two approaches. We include the results in App. D.4, Plot 6c. The two approaches perform similarly in situations which do not involve a confuser agent. However, it seems that defining the utility using a difference in judge\u2019s rewards leads to slightly lower scores when debate agents are faced with an adversary.\n\n> Creating the synthetic dataset by comparing a random action with the clinician's action seems like it would result in a lot of pairs in the dataset where one action is clearly superior to the other. Is this true? Does this cause any issues?\n\nThank you for the question. We hypothesize that a situation where one action is clearly superior to the other would happen only in relatively rare cases (e.g., pairing an action of \u201cno medication\u201d with an action \u201chighest IV/VC dose\u201d, for a patient whose vitals are normal). In the context of our framework, the \u201cdifficulty\u201d of the synthetic dataset primarily impacts the quality of a learned judge model. To empirically test this, we now examine a total of 3 different methods for pairing an alternative action:\n- **random**: true action $a_t$ is paired with an alternative action sampled uniform-random (as described in the main text);\n- **exhaustive**: true action $a_t$ is paired with every possible alternative action;\n- **offset**: true action $a_t$ is paired with an action in the neighborhood of $a_t$. To define a neighborhood, we recall that there are 5 choices for both, vasopressors (VC) and intravenous fluids (VC). Therefore, we can write $a_t = 5 * IV + VC$, where $IV, VC \\\\in \\\\{0, 1, 2, 3, 4\\\\}$. To obtain an alternative action, we consider changing IV and VC by an offset sampled uniform-random from a set $\\\\{-1, 0, 1\\\\}$, for both IV and VC. For example, for $a_t=21$ we have $IV=4$ and $VC=1$, and we randomly select an action that has $IV \\\\in \\\\{3, 4, 5\\\\}$ and $VC \\\\in \\\\{0, 1, 2\\\\}$ (ensuring we don\u2019t end up with the same action).\n\nFor each of the 3 variants, we train a new judge model using the same procedure described in the main paper, and show the accuracy on the test set in App D.5, Plot 6d. The exhaustive variant represents the most informative, but also unrealistically large, dataset. The random variant represents somewhat of a \u201cmiddle ground\u201d in terms of dataset difficulty. Lastly, the offset variant represents the most difficult case, as differences between two actions are more nuanced. However, while the achieved accuracies reflect the difficulty of the corresponding dataset (i.e., the highest accuracy for the exhaustive variant, followed by random and offset variants), the difference seems to be small enough that we can assume our choice of the random variant did not cause issues."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700135939321,
                "cdate": 1700135939321,
                "tmdate": 1700135939321,
                "mdate": 1700135939321,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CZKPtaii6k",
                "forum": "OUkZXbbwQr",
                "replyto": "n6tlubzICJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3851/Reviewer_8w5y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3851/Reviewer_8w5y"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very thorough response and the impressive amount of additional experiments performed. I already recommended the paper for acceptance, and with these corrections and improvements, the recommendation is only stronger.\n\nIf it were possible, I would raise my rating from a 6 to a 7. Since I cannot, I will consider raising my rating to an 8 or keeping it at a 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246273513,
                "cdate": 1700246273513,
                "tmdate": 1700246548248,
                "mdate": 1700246548248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S1cY10aDWD",
                "forum": "OUkZXbbwQr",
                "replyto": "n6tlubzICJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comment"
                    },
                    "comment": {
                        "value": "Thank you for your comment. We are pleased to see that you are positive about our work. We would be happy to respond to further questions, should you have any."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700299756794,
                "cdate": 1700299756794,
                "tmdate": 1700301276034,
                "mdate": 1700301276034,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eMbgbc8kHn",
            "forum": "OUkZXbbwQr",
            "replyto": "OUkZXbbwQr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3851/Reviewer_WJVW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3851/Reviewer_WJVW"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of designing RL agents that can provide evidence supporting their chosen actions ('justification') to a human observer - an important and timely problem that has not received enough attention. The authors propose an approach where a zero-sum debate extensive form game is played out between two agents, each attempting to convince a human judge (or proxy model) that their chosen action is most justified, by selecting 'evidence' (a subset of the state-space) to support the chosen action, contingent on the current state. The result of the human judge proxy model is a reward term which is mixed with the environmental reward, leading to an RL policy that trades off environmental reward and justifiability, as encoded by a dataset of human pairwise rankings of evidence proposals.\n\nThe paper is well written, with some minor grammatical errors. The proposed method is interesting and novel to my knowledge, and seems like a promising approach toward explainable RL policies. The empirical experiments are deep, but consider only one dataset - a medical problem of sepsis treatment. I have left some extensive comments and questions - but this is not a reflection of a weak paper, instead, this is due to my interest in the proposed method. The authors appear to be breaking new theoretical and empirical ground, and to this end the paper seems like a solid first effort in a promising direction.\n\nI'm familiar with single-agent RL (specifically, my background is Inverse Reinforcement Learning), but might be unaware of some prior literature that would be relevant when performing my review. I read the main paper and appendices carefully. I have not carefully checked any theoretical derivations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* A very important problem\n * A novel approach (debate-based justifications), and a solution method that seems promising\n * Clearly written, and the paper seems to represent high quality work\n * A compelling real-world empirical experimental case study\n * The comparison to SHAP as an alternative is great to see - and the proposed method fares well"
                },
                "weaknesses": {
                    "value": "* The method appears to require explicit knowledge of the optimal deterministic policy (called the baseline policy $\\pi_B$ as part of the debate framework (unless I have misunderstood something). This seems like a big limitation in terms of practically applying this method to larger-scale problems.\n * Sec. 4.2 mentions that the judge proxy model follows a 'Bradley-Terry' model (and provides a citation). This seems like an important point in the design of your method, but I'm not familiar with this model or the background or history here. Can you elaborate on why this model is chosen?\n * The training of the justifiable agent (and the argumentative agent) appears to use offline i.i.d. sampling of (s, a, a) tuples (Sec 4.3). This seems like it could pose over/under-representation problems for some MDPs with deep trajectories that might be less or more common (i.e. problems where the i.i.d. assumption is more heavily violated by the dynamics). Can you comment about this limitation of the method?\n * Due to the additional structure of the debate game, the method introduces a number of additional hyper-parameters (e.g. evidence size $L$, mixing coefficient $\\lambda$, scaling coefficient $\\alpha$, as well as architecture of the various policy models, to say nothing of the learning algorithm hyper-parameters. Because of this, evaluation on one or more additional MDP problems would strengthen the evidence for the efficacy of this method."
                },
                "questions": {
                    "value": "# Comments and questions\n\n * The use of an evidence-proposal debate setup for explainability is interesting and seems really promising to me. It seems like some deeper connections could be drawn to prior literature on the philosophical merits of debate as a form of explainability though. For instance, I'm curious about the following questions;\n   * Why did you choose to have a multi-step debate framework, rather than a single step of evidence proposal? What is the interpretation or significance of the trajectory through the debate game tree (is there any?), or is the reached leaf node the only relevant outcome of the debate?\n   * Similarly, how is a human RL designer or domain expert to interpret the proposal of multiple pieces of 'evidence' at each step of the debate game. If I understand correctly, evidence data are individual entries from the state-space vector (I think?). Is the interpretation of multiple pieces of evidence supposed to be a Boolean AND combination of each entry? (e.g. state feature A is present, AND B is present, therefore my action is better than the baseline policy)? I think a rich area for future exploration would be more structured evidence formats - e.g. \"A is present, and C is present, but B is not present, therefore action X is chosen\". Qualitative evaluations with domain experts would be beneficial in exploring this too.\n * There seem to be similarities between the debate-judge model and actor-critic RL approaches. Do you see any connections here? Could the proposed method be connected theoretically with A-C RL methods?\n * Similarly, as you note, the debate framework seems to have connections with pairwise preference ranking (e.g. Christiano et al. 2017). Given the popularity of RLHF at the moment, it might be illustrative to dig into the connections between this method and RLHF some more.\n * Fig 1. It appears that the baseline policy $\\pi_B$ never acts in the environment - is this correct?\n * The paper refers to the debate-based reward approach as 'reward shaping' several times - please be careful with this term. Reward shaping refers technically to a specific subset of reward transformation that value functions are invariant to (see [A]). Although people often do abuse this term to refer to arbitrary designer interventions to alter environmental reward, it is best to be more careful with the terminology here to avoid confusion.\n * I note the judge reward $J \\in \\mathbb{R}$ is unbounded - this concerns me as a infinite range within which to rank items poses difficulties for a human judge (see psychology and human factors literature for example). Does your method still work with bounds on the judge reward? E.g. $J \\in [-1, 1]$?\n * Can $\\pi$ and $\\pi_B$ output the same action and/or evidence? What happens in this situation? Is the judge proxy model able to output a 'no preference for either' choice?\n * Sec 4.4 mentions self-play is used during training. However I thought the agents being trained are deterministic. How can self-play be used in this case?\n * A compelling goal for future work would be the design of sample-efficient methods for specific MDPs that can operate with actual human-in-the loop judgements, rather than a proxy model.\n * The Sepsis reward design (App. B.3 is important - please provide references for the choice of $C_0, C_1, C_2$.\n * Given the specificity of your empirical experimental case-study, it would be helpful to have some more elaboration of the nature of the problem of sepsis control in the Appendix. I also assume the authors have some prior work or expertise or domain knowledge in this area. It would be beneficial to state if this is the case in the appendix to demonstrate that the research is being undertaken by an appropriately credentialed and/or experienced team (i.e. not computer scientists pretending to be doctors).\n\n# Minor comments and grammatical points\n\n * Sec 3. - The MDP specification is missing a starting state distribution.\n * Sec 3.2 Why does $a_t$ act first in the debate game, not $a^B_t$. Did you test the reverse design?\n * You set the hyper-parameter $\\alpha = 5$ in your experiments - why this value? Were other values tried?\n * Fig 3. caption - please define WIS so the reader doesn't have to refer to the main text.\n * App. B.2 - I believe you mean to say \"setting the [log] probability of presented arguments to negative infinity\".\n\n# References\n\n * [A] Ng, Andrew Y., Daishi Harada, and Stuart Russell. \"Policy invariance under reward transformations: Theory and application to reward shaping.\" Icml. Vol. 99. 1999."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3851/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699239769879,
            "cdate": 1699239769879,
            "tmdate": 1699636343041,
            "mdate": 1699636343041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KAPCFCHwD6",
                "forum": "OUkZXbbwQr",
                "replyto": "eMbgbc8kHn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3851/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WJVW"
                    },
                    "comment": {
                        "value": "Thank you for your very perceptive comments and insightful questions. We are very pleased to see that you overall enjoyed the work and are happy to engage in further discussion. In the following, we try to answer all of your questions and comments. The key points of our response can be summarized as follows:\n- We address your concerns about requirement of the baseline policy and offline sampling during training;\n- We clarify our choices of the Bradley-Terry model, multi-step debate, and elaborate on a case when two actions are equally justifiable;\n- We discuss ways of interpreting proposed evidence, similarities to AC methods and connection to RLHF;\n\n## Discussion on Paper Weaknesses\n\n> The method appears to require explicit knowledge of the optimal deterministic policy (called the baseline policy as part of the debate framework (unless I have misunderstood something). This seems like a big limitation in terms of practically applying this method to larger-scale problems.\n\nThe method does require access to what we refer to as a baseline policy. However, this policy can be any well-performing policy, that is not necessarily optimal. The reason why we require a baseline policy to be well-performing is to avoid trivial solutions. In particular, if we consider a case involving a poor quality baseline policy, it might be easy, but useless, to be more justifiable than it. We have updated the paper to mention this.\n\n> Sec. 4.2 mentions that the judge proxy model follows a 'Bradley-Terry' model (and provides a citation). This seems like an important point in the design of your method, but I'm not familiar with this model or the background or history here. Can you elaborate on why this model is chosen?\n\nThe choice of a Bradley-Terry model was inspired by the seminal work that examined specification of the reward function for reinforcement learning agents via preference ratings over trajectories [1]. It is a standard model for estimating score functions from pairwise preferences. In our framework, it can be understood in the context of equating judge\u2019s rewards with a justifiability ranking scale. In particular, the difference in judge\u2019s reward of two actions estimates the probability that a human considers one more justified than the other, given a particular set of evidence.\n\n> The training of the justifiable agent (and the argumentative agent) appears to use offline i.i.d. sampling of (s, a, a) tuples (Sec 4.3). This seems like it could pose over/under-representation problems for some MDPs with deep trajectories that might be less or more common (i.e. problems where the i.i.d. assumption is more heavily violated by the dynamics). Can you comment about this limitation of the method?\n\nThank you for your comment. Regarding the argumentative agents (Sec. 4.3), it is important to acknowledge that in a general case of online RL there is a mismatch between a distribution of contexts used during their training (the i.i.d assumption), and a distribution of contexts that arises during the training of the justifiable agent. This distributional mismatch might affect the quality of the justifiable policy, since $\\hat{r}^d$ might not approximate well $r^d$ in states that are much more frequently visited by the justifiable policy than by the behavior policy that generated the offline data. In our case, however, because we are operating in an offline reinforcement learning setup, the dataset used to train both argumentative and justifiable agents is the same, so we do not encounter such a distributional mismatch (i.e., states are sampled i.i.d during training of both agents). Regarding the justifiable agent, we believe that the limitations one could face in this setup are the same as those encountered in standard offline reinforcement learning problems [3]. Therefore, one could tackle out-of-distribution challenges using existing offline RL approaches, such as batch-constrained Q-learning [4] or Conservative Q-Learning [5].\n\n> Due to the additional structure of the debate game, the method introduces a number of additional hyper-parameters (e.g. evidence size, mixing coefficient, scaling coefficient, as well as architecture of the various policy models, to say nothing of the learning algorithm hyper-parameters. Because of this, evaluation on one or more additional MDP problems would strengthen the evidence for the efficacy of this method.\n\nThank you for your comment. In this work, we focused on an in-depth analysis within the domain of treating sepsis, with a goal of achieving a comprehensive understanding of the intricacies of our method in a setting where learning to justify is of great importance. We agree that additional evaluation in different domains will provide further insights into the efficacy of the method, and it represents a part of the future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3851/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700135766818,
                "cdate": 1700135766818,
                "tmdate": 1700135766818,
                "mdate": 1700135766818,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]