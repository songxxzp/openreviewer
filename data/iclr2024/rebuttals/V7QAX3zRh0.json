[
    {
        "title": "Towards guarantees for parameter isolation in continual learning"
    },
    {
        "review": {
            "id": "zcvbqsdWkQ",
            "forum": "V7QAX3zRh0",
            "replyto": "V7QAX3zRh0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5981/Reviewer_oq6r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5981/Reviewer_oq6r"
            ],
            "content": {
                "summary": {
                    "value": "While a lot of empirical studies have been proposed to address catastrophic forgetting in continual learning, the theoretical investigation behind continual learning is quite limited. This paper studies parameter isolation based continual learning algorithms through the geometry of neural network loss landscape, and also theoretically characterizes the conditions for zero forgetting under certain assumptions. The theoretical guarantees of zero forgetting for several existing continual learning algorithms  are analyzed by using the framework proposed in this paper. Experimental studies are also conducted to justify the theoretical findings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Considering that the theoretical studies are still lacking for continual learning, investigating the theoretical guarantees for continual learning algorithms is a very important direction in the community.\n\n2. Analyzing existing orthogonal-projection based CL studies, such as OGD and GPM, from the perspective of parameter isolation methods is interesting.\n\n3. The authors also conduct experiments to further backup their theoretical findings."
                },
                "weaknesses": {
                    "value": "1. My major concern is about the definition of forgetting in equation (1), which is defined based on the training loss. However, in practice and in most empirical studies we are mainly concerned about the testing loss. This will make the theoretical results less interesting and also prevent the analysis of positive backward transfer.\n\n2. Another concern is about the implications of the theoretical results. Based on the theoretical results, the authors further propose an augmented version of SGD. But the effectiveness of this inspired design is only justified by comparing with vanilla SGD. Note that the theoretical investigations are usually grounded by some restrictive assumptions and particularly the results in this paper are about identifying conditions for zero forgetting. If the advantage of the inspired algorithm cannot be sufficiently verified, it is hard to tell the usefulness of the theoretical findings."
                },
                "questions": {
                    "value": "Besides the weaknesses above, I also have several questions:\n\n1. The authors claimed that minimizing the average forgetting is equivalent to minimizing the multi-task loss in section 1.1. I was wondering whether this is true. In the authors' claim, the term $L_t^*$ is treated as a constant that is independent with $\\theta$. But in continual learning, because the learnt model of previous tasks will be used as the initial model for current task $t$, $\\theta_t$ should be correlated with  $L_{i}^*$ for any previous task $i<t$. \n\n2. How do you define the task solutions in Assumption 2?\n\n3. Table 1 is difficult to understand. In what scale will the errors be considered as small?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5981/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5981/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5981/Reviewer_oq6r"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730046725,
            "cdate": 1698730046725,
            "tmdate": 1699636639953,
            "mdate": 1699636639953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fi39oMokmV",
                "forum": "V7QAX3zRh0",
                "replyto": "zcvbqsdWkQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer oq6r"
                    },
                    "comment": {
                        "value": "We are glad to hear you found our paper interesting. Hopefully, we can clarify your doubts with this answer, and improve your judgment of our work. \n\n&nbsp;\n- **\"My major concern is about the definition of forgetting in equation (1), which is defined based on the training loss. \"**\n\n    You raise a good point regarding the test loss. It is true that we establish guarantees for the training loss and, if the test loss significantly differs from the training loss, our guarantees are completely useless. However, as we know from statistical learning theory, the training loss converges to the test loss as the dataset size increases (keeping the model size fixed), therefore we can expect the train-test difference to be small for large enough datasets. And that is what we see in practice, for in all our experiments we measure forgetting on the test data (Section 5, Experimental Setup). \n\n&nbsp;\n- **\"Another concern is about the implications of the theoretical results. Based on the theoretical results, the authors further propose an augmented version of SGD. But the effectiveness of this inspired design is only justified by comparing with vanilla SGD.\"**\n\n    Regarding the augmented version of SGD, SGD$^\\dagger$, we have carefully stressed in the paper that we do not wish to contribute a new CL algorithm, and we only resort to the use of this modified SGD as a tool to test our theory. SGD$^\\dagger$ has many problems. We cite from Section 5.3:\n    > \u201cOverall, this training setup is suboptimal in many respects (performance, computation, memory) and it is designed specifically to validate the theoretical findings. Examples of efficient algorithms implementing the null-forgetting constraint are OGD, GPM and the other algorithms discussed, and competing with existing algorithms is not a goal of this study.\u201d \n\n    Nonetheless, we believe that *our contribution has a value in that it progresses the theoretical understanding of continual learning algorithms, in a field especially dominated by empirical research*. Our theoretical findings not only provide **a unifying description of a set of otherwise distinct algorithms**, showing that they are essentially equivalent (and abstraction is the goal of any theory), but **it can guide principled development of new algorithms**, perhaps more efficient and effective than the existing ones. \n\n\n&nbsp;\n- (Question 1) **\"The authors claimed that minimizing the average forgetting is equivalent to minimizing the multi-task loss in section 1.1 \"**\n\n    In the paper we say \u201c*The goal of continual learning algorithms is to minimise $\\mathcal{E}(t)$ while learning a new task $T_t$, or, equivalently, to minimise the multi-task loss*\u201d. Importantly \u2018**while learning a new task $T_t$**\u2019, which means in formula minimising $\\mathcal{E}(t) + L_t(\\theta)$. Equation (1) is a straightforward definition of forgetting and we\u2019re not the first to use it. It\u2019s not clear in this setting what you mean by correlation or independence. \n\n\n&nbsp;\n- (Question 2) **\"How do you define the task solutions in Assumption 2?\"**\n\n    The \u201ctask solutions\u201d are simply local minima of the task loss (as said in the sentences before the Assumptions).\n\n&nbsp;\n- (Question 3) **\"Table 1 is difficult to understand. In what scale will the errors be considered as small?\"**\n\n    We agree, it is confusing when the forgetting value is itself very small. In any case we would take $\\mathcal{E}_o(t)$ as a scale reference there."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700137968079,
                "cdate": 1700137968079,
                "tmdate": 1700137968079,
                "mdate": 1700137968079,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TLdtFkr2Jx",
            "forum": "V7QAX3zRh0",
            "replyto": "V7QAX3zRh0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5981/Reviewer_5dAQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5981/Reviewer_5dAQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at some parameter isolation methods for continual learning (including methods like OGD and GPM). It looks at conditions where forgetting is minimised or controllable, by making a 2nd order Taylor series expansion around the converged loss. Assuming that parameters do not change too much during training (ie that the Taylor series expansion is valid), the paper shows that parameter isolation methods satisfy some equations that imply zero forgetting. In experiments, the paper looks at whether the Taylor series expansion assumption holds in practice, and at the forgetting values of some algorithms (including their own algorithm, orthogonal SGD)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is interesting to try and theoretically explain why parameter isolation methods may work (or not work) well. Past works have focussed mostly on regularisation and memory based methods. \n\n2. As far as I can tell (aside from one small question, listed later), the derivations of the theorems are correct / intuitively make sense to me. \n\n3. I found the perturbation analysis technique in Section 5.1 interesting."
                },
                "weaknesses": {
                    "value": "1. At the end of Section 1.1, the paper claims that minimising average forgetting is equivalent to minimising the multi-task loss. I do not think this is true: there is a missing L_t(\\theta), ie the loss at the current task t. I don't think this was used anywhere, so this should be an easy fix. However, in Section 5, the authors say, \"we report forgetting in terms of accuracy\", which I did not understand. What does this mean (because forgetting is not equal to accuracy)? \n\n2. I'm not sure if OGD and GPM are 'parameter isolation' methods, a claim that seems central to the title and framing of this paper. Previous works like de Lange et al. would characterise these methods as 'constrained replay-based methods'. I do follow the authors' argument in Section 2 that these methods are not allowing parameters to be part of the parameter subspace. If anything, I would say that these methods are a combination of replay and parameter isolation methods (or, if strictly choosing one, it would be replay methods, like in de Lange it al.). \n\n3. Assumption 2 (combined with assumption 1) seems like a very strong assumption to me. The fact that a method like OGD does not have zero forgetting on CL benchmarks (see results in other papers: OGD is not perfect!) is proof to me that assumption 2 does not hold in practice. I was very unconvinced by the experiments in Section 5.1, which looks at whether assumption 2 holds. \n- In the first part (Table 1) the authors do not train until (close to) convergence, hence breaking assumption 1. It is unsurprising that training for a fixed number of epochs with a learning rate that is orders of magnitude lower leads to the parameters changing less, and hence the Taylor series expansion being more predictive (that being said, if one looks at relative values of the Taylor series expansion and E_0(t): E_0(t) is about twice the value of the Taylor value, but always within what are very large standard deviations, making this result very difficult to draw any conclusions from). \n- In the second part (Figure 2), I do not understand what reasonable values of the score or of r are. Is a score of 1 good? What does an r value of 100 mean? Is this small or large? I do not know how to interpret these results. \n\n4. Throughout the experimental section, the authors appear to break assumption 1, hence breaking the theory (at least, for small learning rates; at large learning rates, it appears that assumption 2 does not hold). This is a major problem for me. \n- Also, all the error bars in Table 2 are very large, making any conclusions here hard to see. \n\n5. If I understand correctly, the theory for OGD and GPM requires storing all input data from past tasks and constraining optimisation over this? This is a very strong requirement (and is obviously not allowed in practice in CL). This needs to be made clear, and, in my mind, reduces the significance of this result."
                },
                "questions": {
                    "value": "Please see Weaknesses section. \n\nOther minor questions / points: \n1. In the proof of Theorem 1, the authors say that \\Delta_2^T H_1* \\Delta_2 = 0 implies that \\Delta_2^T H_1 = 0. It was not clear to me why this is the case, can the authors please expand on this? \n2. Typo in the line after Equation 3: I believe it should be \\Sum_{o=1}^{t-1}, currently it says t-2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777957507,
            "cdate": 1698777957507,
            "tmdate": 1699636639845,
            "mdate": 1699636639845,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GNHfiakjRW",
                "forum": "V7QAX3zRh0",
                "replyto": "TLdtFkr2Jx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer 5dAQ"
                    },
                    "comment": {
                        "value": "We are quite surprised by your score and hope, despite the high confidence, that we can convince you to reconsider with this answer. \n\n### Weaknesses\n- \"**At the end of Section 1.1, the paper claims that minimising average forgetting is equivalent to minimising the multi-task loss.**\"\n\n   Citing from the paper, we say \u201c*The goal of continual learning algorithms is to minimise $\\mathcal{E}(t)$ **while learning a new task** $\\mathcal{T}_t$ or equivalently to minimise the multi-task loss*.\u201d If you are referring to this sentence, perhaps you might have missed the \u2018*while learning a new task*\u2019 part, which is exactly what you claim to be missing? \n\n&nbsp;\n- **\"In Section 5, the authors say, \"we report forgetting in terms of accuracy\", which I did not understand [...] What does this mean (because forgetting is not equal to accuracy)?\"**\n\n    Yes, forgetting is not equal to accuracy. As defined in Equation (1), forgetting is the difference in loss after a change in parameters. However *the loss is often not easy to interpret, so we instead quantify the results in terms of difference in accuracy after a change in parameters*. We should clarify this better in the paper. Note that a precise definition has been given in the Appendix (Section B.0.1, Experimental Setup - Metrics).\n\n&nbsp;\n- **\"I'm not sure if OGD and GPM are 'parameter isolation' methods, a claim that seems central to the title and framing of this paper.\"**\n\n   - You\u2019re right in noticing that our definition of parameter isolation algorithms is wider than the convention in the literature. This is a point we should stress more in the paper. In a way, *we are proposing a change in the vocabulary, and we do so based on the results of our analysis*. \n   - *We define parameter isolation as algorithms partitioning the network parameters between tasks*, and we show that OGD and GPM (and consequently all their variants) do it in a more sophisticated way than PackNet or Progressive Networks. \n   - Essentially, **we show that all these algorithms rely on the same mechanism** (i.e. tasks parameters orthogonality) and therefore they can be seen as special cases of a unique strategy. Similarly, all regularization-based methods rely on soft constraints on the parameter change (determined by the regulariser) and replay-based methods rely on empirical, small-sample approximations of the previous tasks' losses. \n   - Therefore *labeling OGD and GPM (and their variants) as replay methods does not really reflect the nature of these algorithms*, which is much more similar to that of PackNet than Experience Replay. However **the connection to parameter isolation has not been ascertained before in the literature**, and we accomplish this with Theorems 3-5 in this paper. \n   - Moreover, several different categorizations of continual learning algorithms exist today, each of them using different terms. *One may say that the continual learning terminology is for most parts still in progress, and should not be regarded as final yet.* \n\n&nbsp;\n- **\"Assumption 2 (combined with assumption 1) seems like a very strong assumption to me.\"** \n\n   It is. We do not claim that OGD is perfect, and we are well aware that it does not achieve zero forgetting in practice. **Our theory can explain why that happens**, which is precisely when Assumption 2 is violated. In fact, we even provide experimental data (Figure 3) to support this claim. \n\n    In Section 5.1 we ask at what point this assumption breaks and we do not conclude that it always holds (which is naturally unrealistic). Additionally, *we would like to highlight that later in the paper we do extend our theory to settings where we can drop Assumption 2*, like GPM. Therefore **our theory is not conceptually limited by the assumption**. \n\n&nbsp;\n- **\"In the first part (Table 1) the authors do not train until (close to) convergence, hence breaking assumption 1.\"**\n\n     In our first set of experiments our goal is to assess Assumption 2. Note that *we do not need the gradients to be zero, i.e. Assumption 1, for the second order Taylor expansion* (Equation 2). Therefore, **it is not necessary in this case to train to convergence**. Regarding the large standard deviations, that is the effect of using different seeds. As you might guess, the initialisation can play an important role in the geometry of the loss around it. \n\n&nbsp;\n- **\"In the second part (Figure 2), I do not understand what reasonable values of the score or of r are.\"**\n\n    We have expanded on the interpretation of $s(r)$ in the answer to Reviewer W8iZ. To give you a short answer, a score $s(r)=1$  means that moving along an eigenvector, by a distance $r$,  has the same effect as moving along a random other direction, by the same distance $r$, in the parameter space. Consequently, the values of $r$ for which the score is 1 identify regions of the parameter space where the quadratic approximation of the loss does not hold anymore.   \n\n\n(continued in the following comment)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700137813542,
                "cdate": 1700137813542,
                "tmdate": 1700137813542,
                "mdate": 1700137813542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0csH9GplxN",
                "forum": "V7QAX3zRh0",
                "replyto": "GNHfiakjRW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5981/Reviewer_5dAQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5981/Reviewer_5dAQ"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their rebuttal. I will go through the points using the same numbers as in my original review. \n\n1. Thank you for pointing this out. I think I read the 'while' as a 'when'. I see why what you have written makes sense. I request you make it clearer so others do not also get confused, but I agree this is not a negative point of the paper. \n\n2. I think it is very important to stress this as a contribution of your work, to the extent that this should probably go in your abstract and more details in the introduction. I agree with you that different works categorise the literature differently, and this is evolving. However, the works that use this terminology do not define parameter isolation methods like you do. Your definition is perfectly reasonable, and I see the logic. But the claims you make in the abstract are then misleading, as I (the reader) think I already know what 'parameter isolation' methods means, when you are in reality redefining it. \n\n3. (and 4.) Perhaps I should say, when there is a low learning rate and fixed number of epochs, then it is unsurprising that the parameters change very much, and therefore that the approximation is better. But I would also assume that the overall average accuracy of the algorithm would correspondingly be lower, as you are not training to convergence on every task (ie break Assumption 1 / have very large epsilon values), and therefore do not learn the new tasks well. Is this true for all experiments (including ones in eg Table 2)? I found some final average accuracies in Appendix C.1.2 but these seem quite low, especially for Split CIFAR100. \n\n- Thank you for explaining meanings behind s(r) and r.\n- Looking at Figs 6 and 7, the error bars seem much lower than in Table 2. I think I'm missing something, can the authors please clarify? \n- Unfortunately the large error bars make conclusions very, very difficult. This implies to me that, based on randomness, there are many times when one method is much better than another. For example, saying something like \"the effect is stronger when using 20 instead of 10 dimensions\" is not clear to me as everything is so marginal. \n\n5. It's important to note that your theory assumes access to all past data (please correct me if I'm wrong), which these methods in practice obviously do not have. \n\nQuestions: thank you for clarifying both these points, what you say makes sense. \n\nAt this stage, I am considering increasing my score to 3, however, for reasons listed above (especially the point on small learning rates), I do not think this is ready for publication."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162894190,
                "cdate": 1700162894190,
                "tmdate": 1700162894190,
                "mdate": 1700162894190,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pSK4zb2bvY",
            "forum": "V7QAX3zRh0",
            "replyto": "V7QAX3zRh0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5981/Reviewer_W8iZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5981/Reviewer_W8iZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper conducts a second-order analysis of continual learning and derives theorems that lead to small catastrophic forgetting under mild conditions. The authors show that OGD, GPM, and explicit parameter isolation strategies can be explained using the derived theorems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The theoretical framework is powerful enough to include many existing continual learning techniques."
                },
                "weaknesses": {
                    "value": "The second-order analysis is restrictive to the case when the learning rate is small."
                },
                "questions": {
                    "value": "1. On page 3, in \"Catastrophic forgetting from the loss geometry viewpoint\", you mention that parameter isolation strategies can provide stronger guarantees than regularization-based methods. Is the intention to stress that your analysis doesn't involve the optimization procedure? I want to confirm this since you did not discuss regularization-based methods in later sections in detail.\n2. Can you also give a brief comparison between your results and the guarantees of the regularization-based methods?\n3. When we talk about the strict parameter isolations, does the network know the task domain index at training and inference time?\n4. Can you explain the columns in table 1? What does \"Taylor\" mean here?\n5. In \"Perturbation analysis\" on page 7, you mention that it is to measure assumption 2, do you mean to verify eq(2)?\n6. Can you give some intuition behind the perturbation score s(r)? Why is it a good measure of the approximation error?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807636473,
            "cdate": 1698807636473,
            "tmdate": 1699636639728,
            "mdate": 1699636639728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pdekVwb7wR",
                "forum": "V7QAX3zRh0",
                "replyto": "pSK4zb2bvY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification and answers to Reviewer W8iZ"
                    },
                    "comment": {
                        "value": "Thank you for your positive review and we appreciate the good quality of your questions. With the following, we hope to clarify all your earlier concerns. \n\n- **\"The second-order analysis is restrictive to the case when the learning rate is small.\"**\n\n     A second-order Taylor expansion, like any local approximation of a function, naturally holds only within a given distance from its center.  In our work we show that **some parameter-isolation methods, like OGD, rely on a second-order approximation of the loss, and therefore are fundamentally limited by the locality of the approximation**. This is an important result in our view, because it informs us of the inherent flaws of the algorithm.  However, we can successfully apply our analysis to other algorithms such as GPM which elude the locality constraint by using different second order approximations along the optimization path (see Theorem 5 for details). **Thus the 'small learning rate' constraint is not a weakness of our analysis, rather of some of the algorithms analysed**. \n\n&nbsp;\n- (Question 1) **\"You mention that parameter isolation strategies can provide stronger guarantees than regularization-based methods.  Is the intention to stress that your analysis doesn't involve the optimization procedure?\"** \n\n    Yes, that is correct. By design *regularisation-based algorithms reach a compromise between the task loss and the regulariser*. However, the extent to which they compromise in favor or at odds with the regularisation ultimately depends on the optimization process. On the contrary, **parameter isolation methods enforce a hard constraint on the parameters, which holds independently of the optimization method employed**. If you will, the difference between regularisation-based and parameter isolation is akin to the difference between regression with L2 regulariser and sparse linear regression. \n\n&nbsp;\n- (Question 2) **\"Can you also give a brief comparison between your results and the guarantees of the regularization-based methods?\"**\n    \n    That\u2019s an important point. Yin et al. provide theoretical results akin to ours in the space of regularisation methods. They carry out an optimization-based analysis, introducing smoothness assumptions and they provide convergence results for SGD with CL regularization and a generalization result (which is akin to a guarantee of optimality) holding in probability. *In a nutshell, the regularization setup does not allow for as strong guarantees as the parameter-isolation setup* (i.e. guarantees that hold regardless of smoothness, optimization algorithm or sample set size). The reason for that is that **regularization methods enforce low forgetting as a soft constraint, while parameter isolation methods enforce a hard constraint on the parameters**. \n\n   Interestingly, our results align with those of Yin et al. for regularisation-based methods in showing that the constraint enforced by the algorithm is based on a second-order approximation of the loss. Thus **regularisation and parameter isolation methods use the same information to prevent forgetting, however the former resorts to soft while the latter to hard constraints**. \n\n&nbsp;\n- (Question 3) **\"When we talk about the strict parameter isolations, does the network know the task domain index at training and inference time?\"**\n    That is a good question. It ultimately depends on the specific method. For example, PackNet uses a task-specific parameter binary mask, and network expansion methods assign a different parameter set to each task. On the other hand, sparsity-based algorithms like SpaceNet do not need to know task identity information at test time, but use it while training. \n\n\n(continued in the following comment)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700137536623,
                "cdate": 1700137536623,
                "tmdate": 1700137536623,
                "mdate": 1700137536623,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oDyQHgTVtX",
            "forum": "V7QAX3zRh0",
            "replyto": "V7QAX3zRh0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5981/Reviewer_9KMy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5981/Reviewer_9KMy"
            ],
            "content": {
                "summary": {
                    "value": "The authors construct a unified framework to analyze the effectiveness of parameter isolation methods in continual learning. With the 2nd order Taylor expansion on the objective function, the authors derive some necessary and sufficient conditions for guaranteed non-forgetting. They further show some existing parameter isolation methods are special cases of their framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors construct a theoretical framework for parameter isolation strategies through the lens of loss landscape.\n\n- Several seminal continual learning methods can be analyzed and explained with their framework."
                },
                "weaknesses": {
                    "value": "- My major concern is the significance or novelty of the submission. The framework authors formulate, i.e., analyzing null forgetting in continual learning through loss landscape, is somehow not quite new given some previous works. Besides, though the authors analyze some continual learning methods with their framework, it would be more appreciated if a new continual learning method can be proposed guided by the theoretical framework. The current presentation also makes the experimental part weak, which doesn\u2019t provide any very significant results to the field.\n\n- The authors analyze a few continual learning methods that already have sound intuition on their effectiveness, especially the OGD and GPM. What about other more empirical methods, e.g., the prompt tuning continual learning method [1].\n\n#### Reference:\nWang, Zifeng, et al. \"Learning to prompt for continual learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "What does $\\xi_{\\tau}(t)$ in Theorem 2 mean?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698950194501,
            "cdate": 1698950194501,
            "tmdate": 1699636639624,
            "mdate": 1699636639624,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HEa7NuOBUb",
                "forum": "V7QAX3zRh0",
                "replyto": "oDyQHgTVtX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments and answers to Reviewer 9KMy"
                    },
                    "comment": {
                        "value": "- \"**My major concern is the significance or novelty of the submission.**\"\n\n     We are certainly not the first to analyze continual learning algorithms from a geometrical perspective and we do not claim to be pioneering the tools we use, such as the Taylor expansion. However, we believe our contribution to be unique because, as far as we know, we\u2019re the first to have shown that **a rather large number of methods (which we refer to as \u2018parameter isolation\u2019) follow the same general principle**. A similar statement has been made previously for all regularization-based algorithms, but that\u2019s a different set of methods. \n\n    Also, we definitely agree that a favorable outcome of this line of research would be the development of principled algorithms. However, we believe that theoretical contributions such as ours are as valuable and necessary as empirical contributions, *even more so in a field like continual learning that is largely dominated by empirical methods*.\n\n&nbsp;\n\n-  \"**The authors analyze a few continual learning methods that already have sound intuition on their effectiveness, especially the OGD and GPM.**\"\n\n     Our theory applies to any method which, implicitly or explicitly, partitions the parameter space between tasks, e.g. through a linear projection (OGD), parameter freezing (PackNet), sparsity constraints, etc\u2026 So *it could in principle also be applied to Wang et al., if the parameters used to learn the sequence of tasks were also partitioned*. We do not cover all the algorithms in the parameter isolation family in our paper, because that would be unfeasible within the space limits, and it would be pointless given that new algorithms are proposed all the time. However, *the examples we propose are meant to provide an intuition of how to apply the same strategy to other methods*. In particular, **for OGD and GPM (although they are \u2018intuitively sound\u2019) the connection to parameter isolation is not so clear**, as shown by the extensive proofs backing the result. On the other hand, for \u2018strict parameter isolation\u2019 methods (Section 4.3) we do not go into the proof detail, because the link to parameter isolation is obvious. We would like to stress that in the respective papers, the authors of OGD and GPM do not establish any theoretical guarantees for the effectiveness of their methods, and that we are the first to obtain them.\n\n&nbsp; \n\n- Regarding your question: *$\\mathcal{E}\\_\\tau(t) $ , is the forgetting of task $\\tau$ after learning task $t$* (see Section 1.1, after Equation (2)). In the specific case of Theorem 2, we identify a constraint by which $\\mathcal{E}\\_\\tau(t)=0$ for all tasks $\\tau<t$.\n\nWe hope to have addressed all your concerns and to have better communicated the contribution of our work so as to improve your judgement of the paper. If you have any new questions please let us know, we\u2019re happy to answer them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136642866,
                "cdate": 1700136642866,
                "tmdate": 1700136642866,
                "mdate": 1700136642866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]