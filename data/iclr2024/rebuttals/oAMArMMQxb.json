[
    {
        "title": "Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization"
    },
    {
        "review": {
            "id": "Hqln1YKXqA",
            "forum": "oAMArMMQxb",
            "replyto": "oAMArMMQxb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
            ],
            "content": {
                "summary": {
                    "value": "This is a theoretical work that addresses the problem, known from the literature, affecting vanilla score matching approaches to learn multimodal distributions from data.\nThe key idea is to rely on vanilla score matching, and prove that a Langevin diffusion process with early stopping, appropriately initialized at the empirical distribution, successfully generates multimodal distributions.\nThis result builds on several empirical works that show ways of overcoming the difficulty of using vanilla score functions for the estimation of multimodal distributions, and has the merit of being theoretically sound and -- to some extent -- practical."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* I really liked the pedagogical structure of the paper, which is akin to a different kind of literature such as applied mathematics, or conferences on learning theory. The main result is stated, a positioning of the work with respect to both theory and practice is clearly outlined, and open questions are discussed, in a succinct manner. Then, the bulk of the article revolves around the proof strategy used to arrive at the main theorem in the paper.\n\n* I think there are nice connections with recent work such as [1], which deal with methods to factorize the data distribution into a product of conditional probability distributions that are strongly log-concave. Ultimately, the goal is to discover ways of using the vanilla score matching to sample from simpler distributions whereby the noise injection that is typical for score-based generative models is not needed.\n\n* To the best of my understanding, the technical strategy used to prove the main result is correct, well developed and clearly exposed (in the main paper)\n\n[1] Florentin Guth and Etienne Lempereur and Joan Bruna and St\u00e9phane Mallat, Conditionally Strongly Log-Concave Generative Models, ICML 2023"
                },
                "weaknesses": {
                    "value": "* I think there is some room for improvement in the overall narrative and exposition of the paper (there are a few typos, easy to fix and not problematic for the technical understanding).\nFirst of all, the assumption that the score is easy to compute and, in particular for some data distributions it can be analytically available, should be emphasized more. Essentially, this work side-steps the problems of learning the score function alltogether: despite citing the seminal work from Hyvarinen 2005, the authors chose not to bring to the readers' attention the fact that in practical cases, even the vanilla score can be hard to compute, as it can require costly computations of the trace of the Hessian of the parametric score, which derives from a rewrite of the Fisher divergence. So, I think it is important to mention that the parametric score function does not come for free in general settings.\n\n* The main results in the paper requires very important ``ingredients'':\n1. An early stop mechanism, to set the Langevin Monte Carlo process evolution to stop at a very well defined diffusion time. There is no discussion about the practical implications of the tight bound on $T$ derived in Theorem 1, which is exponential in the data complexity (in the case of the paper, this takes the form of the number of mixture components $K$)\n\n2. The tight bound on step size for the simulation of discrete-time Langevin dynamics decreases exponentially with the data complexity. Although this is not surprising, the more complex the data, the finer-grained your simulation should be, I think a discussion on practical implications is in order.\n\n3. The quality of the approximation of the parametric score function is an assumption that is, in my opinion, very strong, and hardly achievable in practice. This is not a problem in the ``simple'' setting of this work, where data is assumed to be a mixture of $K$ log-concave, smooth components, which is needed to come up with a feasible proof strategy. However, in reality, parametric score functions can only approximate the true score.\n\n4. The number $M$ of i.i.d. samples required to define the initial condition of the discrete Langevin process is not discussed appropriately. From Theorem 1, it seems to me that we need a fairly large number of samples, but I must confess I had a hard time finding the impact of $M$ on the proof strategy outlined in Section 2 of the paper. \n\n* Experiments are weak. Of course we are not talking about using the typical datasets that the current literature on score-based generative modeling. As an example, the results displayed in Figure 2 could have been commented more in the optic of explaining the relation to the above 4 points. Some hints are available in Appendix I, such as details on the learning procedure for the vanilla score network, but I still find it hard to relate such technical details to the hypothesis required for Theorem 1 to be valid.\n\n* [minor weakness] The editorial format of the paper is somehow unconventional. There is no conclusion, and a large fraction of the real-estate available on the 9 pages is dedicated to material that often is given a prime spot in the appendix. If on the one hand I like this presentation style, as it is really helpful to go through the proof strategy detail, the downside is that it substract space to provide more insights on experiments, outline conclusion and compare to prior (albeit experimental) work upon which the authors have drawn inspiration."
                },
                "questions": {
                    "value": "Besides asking authors to provide comments, explanations and eventually additional details for the three main weaknesses discussed above, I have the following question:\n\n* A recent paper [2] studies discrete Langevin processes with approximate scores, and (very informally speaking) also finds that the approximation quality of the distribution obtained by the Langevin process can ``drift away'' in KL terms, from the true distribution if the simulation time $T$ is too large. Do you think there are connections that your work could draw to improve the discussion on the early stop mechanism you devise?\n\n[2] Kaylee Yingxi Yang and Andre Wibisono, Convergence of the Inexact Langevin Algorithm and Score-based Generative Models in KL Divergence, arXiv 2211.01512\n\n** Post rebuttal feedback **\nThe authors engaged in discussions about my concerns and made an effort to improve their paper. For these reasons I raised my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698076799384,
            "cdate": 1698076799384,
            "tmdate": 1700556423355,
            "mdate": 1700556423355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9JyN6z8iYA",
                "forum": "oAMArMMQxb",
                "replyto": "Hqln1YKXqA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to reviewer 3d8a"
                    },
                    "comment": {
                        "value": "We thank you for your feedback. Our answers to your comments are as followed.\n\n$\\textbf{First of all, the assumption that the score is easy to compute and, in particular for some data distributions it can be analytically available, should be emphasized more. \n\\\\\\\\\nEssentially, this work side-steps the problems of learning the score function altogether: despite citing the seminal work from Hyvarinen 2005, the authors chose not to bring to the readers' attention the fact that in practical cases, even the vanilla score can be hard to compute, as it can require costly computations of the trace of the Hessian of the parametric score, which derives from a rewrite of the Fisher divergence.\n\\\\\\\\\nSo, I think it is important to mention that the parametric score function does not come for free in general settings.}$\n\n\nWe agree that estimating the vanilla score is an important and nontrivial task. \nFirst we note that our analysis is following standard practice in theoretical works. In many recent influential papers on generative modeling, the score function/an L2-approximate score function is assumed to be available. See Convergence for score-based generative modeling with polynomial complexity Poster (neurips.cc), ICLR ICLR 2023 Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions Oral, PMLR Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions (mlr.press). In reality, the score function is assumed to be learned from data, but these previous papers also do not dwell on this fact. We merely follow the convention in the field when presenting our results. In addition, in section 1.1, we have already mentioned that the score function is learned from data. Whether it is easy or not to learn the score function from data is still an active field of research (see Statistical Efficiency of Score Matching: The View from Isoperimetry | OpenReview,  [2306.09332v2] Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Markov Chains (arxiv.org)), and we do not have any intention of claiming that this is an easy task.\n\nThe reviewer also mentions some practical concerns regarding computing the Hessian of the score. This happens if we try to fit the score function using Hyvarinen\u2019s algorithm, but all we assume is that it has been fit in some way and alternatives are available. In particular, iIt has been noted in previous works, e.g.  the cited work of Block et al (https://arxiv.org/abs/2002.00107) that in many cases the vanilla score can be reasonably estimated by using denoising score matching with a small amount of noise and this avoids the problem of computing the Hessian. \n\n$\\textbf{An early stop mechanism, to set the Langevin Monte Carlo process evolution to stop at a very well defined diffusion time. \n\\\\\\\\\nThere is no discussion about the practical implications of the tight bound on $T$ derived in Theorem 1, which is exponential in the data complexity (in the case of the paper, this takes the form of the number of mixture components $K$). \\\\\nThe tight bound on step size for the simulation of discrete-time Langevin dynamics decreases exponentially with the data complexity. \\\\\\\\\nAlthough this is not surprising, the more complex the data, the finer-grained your simulation should be, I think a discussion on practical implications is in order.}$\n\nIn remark 3, we briefly discuss that T and the number of steps $N=T/h$ depend polynomially in the dimension $d$ and other relevant parameters when $K= O(1).$ Such a polynomial bound is very interesting, since it is known that even for $K=2$, without access to ground truth samples, any algorithm would take exponential time to sample (see the last few lines in \u201cRelated theoretical work\u201d in page 4). As we said in the paper, it is a very interesting question for future work whether the dependence on $K$ can be improved in theory."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078244510,
                "cdate": 1700078244510,
                "tmdate": 1700078503111,
                "mdate": 1700078503111,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GPJ4KSKV9y",
                "forum": "oAMArMMQxb",
                "replyto": "9JyN6z8iYA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal part 1"
                    },
                    "comment": {
                        "value": "Thank you for your clarifications about approximating the score function by learning from data. I have no problems with the assumptions made in the main part of the paper of having the score function directly available. My suggestion was to simply point the reader to the fact that in some practical endeavors score function estimation requires care.\n\nSame line of reasoning for the discussion on early stopping. While I greatly appreciate the theoretical result, I was suggesting to point the reader to the fact that when assumptions such as $K=O(1)$ do not hold (or even when data distributions are more complex than mixtures), setting $T$ appropriately requires care."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139323594,
                "cdate": 1700139323594,
                "tmdate": 1700139323594,
                "mdate": 1700139323594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bcD0VttFri",
                "forum": "oAMArMMQxb",
                "replyto": "ux6zVFIsqb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal part 2"
                    },
                    "comment": {
                        "value": "Thanks for the clarification on $M$, it makes sense, especially when put in the perspective of $K=O(1)$."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139508321,
                "cdate": 1700139508321,
                "tmdate": 1700139508321,
                "mdate": 1700139508321,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s8IzVj5Wmj",
                "forum": "oAMArMMQxb",
                "replyto": "ivhDPSBwgd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal part 3"
                    },
                    "comment": {
                        "value": "Thanks, I see now that the stronger assumption made in the suggested reference is helpful in deriving stable bounds, and why your weaker assumption does not allow you to achieve the same stability."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139649780,
                "cdate": 1700139649780,
                "tmdate": 1700139649780,
                "mdate": 1700139649780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qVZCTa7vXg",
                "forum": "oAMArMMQxb",
                "replyto": "Hqln1YKXqA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Learning L2-approximate score function on real datasets"
                    },
                    "comment": {
                        "value": "Thank you for your prompt response. In your review, you commented that one weakness of our paper is \"The quality of the approximation of the parametric score function is an assumption that is, in my opinion, very strong, and hardly achievable in practice\". In our previous response, we have provided evidence that the assumption of having access to a good L2-approximate score is standard and has been used in several influential prior works. To further corroborate our point, the success of many empirical works is consistent with the belief that L2-approximate score function can be efficiently learned on real datasets, for e.g. MNIST, CelebA, and CIFAR-10. \n\nPrior empirical works show that one can efficiently obtain an L2 estimate of the score/gradient of the log-density function (see the highly cited work [Song-Ermon\u201919] \"https://scholar.google.com/scholar_lookup?arxiv_id=1907.05600\"). Note that the score matching algorithm used to estimate the score explicitly minimizes the L2 score error (see also this blog post \"Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song (yang-song.net)\" https://yang-song.net/blog/2021/score/).[Song-Ermon\u201919]\u2019s algorithm efficiently learned the score on real datasets (MNIST, CelebA, and CIFAR-10). For more empirical work that employs the framework introduced in [Song-Ermon'19] i.e. learning L2-estimate scores from data samples then using DDPM to produce more samples, see for example [Ho-Jain-Abbeel--NeurIPS'20] https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf, [Saharia et al.-NeurIPS'22] https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526522098,
                "cdate": 1700526522098,
                "tmdate": 1700589634610,
                "mdate": 1700589634610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S42pHJkCq2",
                "forum": "oAMArMMQxb",
                "replyto": "qVZCTa7vXg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_3d8a"
                ],
                "content": {
                    "title": {
                        "value": "Re: Learning L2-approximate score function on real datasets"
                    },
                    "comment": {
                        "value": "Thank you for the additional feedback and the willingness to help readers: I think you have plenty of references to support the claim about having access to good score approximations and the additional material you plan to include can further substantiate the idea.\n\nThank you for the effort you've put in the rebuttal. I will increase the score in my review."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556333671,
                "cdate": 1700556333671,
                "tmdate": 1700556333671,
                "mdate": 1700556333671,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zxeTimoaqn",
            "forum": "oAMArMMQxb",
            "replyto": "oAMArMMQxb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6339/Reviewer_7jZP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6339/Reviewer_7jZP"
            ],
            "content": {
                "summary": {
                    "value": "This paper utilizes the vanilla score-matching to sample from multimodal distributions. They also show that initialization using data  can help the score matching the ground truth distribution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Sampling from multimodal distributions is an extremely interesting problem."
                },
                "weaknesses": {
                    "value": "- This paper is poorly written and very hard to read and follow. It constantly jumps around. It feels like you are reading lecture notes rather than a paper. The main contribution is hidden in many topics that can easily be moved to the appendix. \n\n- Lack of comprehensive experimental results and applications to real-world data and high dimensional data. \n\n- Lack of comparison to other bounds and theoretical results."
                },
                "questions": {
                    "value": "- How does it compare to other bounds? it seems like this bound provides this on average and not the worst-case. In addition, it is mentioned in previous work: \"in high dimensions, it will not be anywhere close to the ground truth distribution unless we have an exponentially large number of samples\", how is it not the same case in their paper as well?  Furthermore, What is the computational complexity?\n - The analysis starts with several idealized assumptions, including the exact knowledge of the score function and the assumption that the ground truth distribution is supported within a specific radius. These assumptions might not hold in real-world scenarios, limiting the practical applicability of the results. Could the author please elaborate on that? \n- The method relies on various parameters such as $\\delta$ (overlap parameter), $H$ (tuning parameter), $\\epsilon$ (error threshold), and step size $h$. How can one justify the generalization of the method across different datasets or scenarios due to the fact that sensitivity to these parameters could make the approach highly sensitive to the choice of initial conditions and hyperparameters, making it challenging? \n- As mentioned earlier,  the analysis focuses on an idealized scenario and might not directly translate to real-world applications. The conditions and assumptions required for the analysis might be too strict or unrealistic for practical use cases, limiting the method's applicability in real-world data analysis or machine learning tasks. How does this work can be applied in practice? How does it apply to high-dimensional data? Experimental results with high-dimensional data are required to show the efficacy of the procedure. \n- The analysis briefly touches upon the scenario where the score function is estimated from data. In practice, obtaining a precise score function estimation can be a challenging task and might introduce significant errors in the analysis.\n- The passage does not provide a comprehensive comparison with existing methods or techniques in the field. Without a clear comparison, it's challenging to assess the novelty and superiority of the proposed approach over existing state-of-the-art methods for similar tasks. The analysis primarily focuses on theoretical aspects and lacks empirical validation on real datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701613113,
            "cdate": 1698701613113,
            "tmdate": 1699636698292,
            "mdate": 1699636698292,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s4Sp9MSjXy",
                "forum": "oAMArMMQxb",
                "replyto": "zxeTimoaqn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for reviewer 7jZP"
                    },
                    "comment": {
                        "value": "We thank you for your feedback, but we respectfully disagree with your assessment.\n\n$\\textbf{This paper is poorly written and very hard to read and follow. It constantly jumps around. It feels like you are reading lecture notes rather than a paper. The main contribution is hidden in many topics that can easily be moved to the appendix.}$\n\nWe have clearly stated our main result in theorem 1 (the main theorem of the paper). We believe you might be confused by section 2.1, where we present a high-level sketch of our proof. We believe the sketch would be appreciated by readers who want to understand the proof techniques and want to verify the proof, since the full proof in the appendix is long and contains all of the rigorous technical details.\n\n$\\textbf{Regarding comparison with previous results:}$\n\nWe believe we have provided sufficient comparison with previous theoretical works in page 4 in the paragraph named \u201crelated theoretical work\u201d. Please see the answer to your first question for further details.\n\nWe have provided experimental results for mixtures of Gaussians in low dimension and relatively high dimension ($d=32$). We also ran the same experiments in other dimensions (e.g. 16, 64,128) and did not observe interesting differences in the results. We didn\u2019t include more experimental results on real datasets since the empirical benefits of Langevin with data-based initialization has been confirmed by previous experimental works such as (Hinton (2012); Xie et al. (2016)).\n\nAnswer for your questions (we need to split it into multiple replies due to word limit):\n\n$\\textbf{How does it compare to other bounds? Furthermore, What is the computational complexity?}$\n\nWe have compared our work with the most closely related previous works, that of Lee et al. (2018) and Ge at al. (2018) on sampling from mixtures of isomorphic Gaussians. Note that because Lee at al. (2018) and Ge et al. (2018) do not assume access to samples from the ground truth, they actually prove an impossibility result when the target mixture distribution $\\mu$ is of 2 non-isomorphic Gaussian. In this case, they show that without access to ground truth samples, any algorithm would need exp(d) queries to the log-density and score function, whereas we show that with access to ground truth samples, the standard Langevin can approximately sample from $\\mu$ in poly(d) time, where $\\mu$ is supported on $\\mathbb{R}^d.$  We have emphasized this fact in Remark 3, that, as long as $K$, the number of components, is a constant, the computational complexity of our algorithm is polynomial in all relevant parameters, including the dimension $d$.\n \n$\\textbf{it seems like this bound provides this on average and not the worst-case.}$\n\nOur result works with high probability over data sampled from a distribution, which is almost always the setting considered in machine learning and statistics. An efficient algorithm that succeeds in the worst-case scenario is impossible, for example due to the aforementioned impossibility result by Lee et al. (2018) and Ge at al. (2018). Indeed, there is always some possibility that the set of ground truth samples is a fixed set of points (for e.g. the unit sphere centered at 0) that offers no extra information about the target distribution $\\mu$, thus you are back in the setting of  Lee et al. (2018) and Ge at al. (2018) where one is not given access to ground truth samples and thus need exp(d) queries to the log-density and score function to produce even one extra sample.\n\nIn addition, it is mentioned in previous work: \"in high dimensions, it will not be anywhere close to the ground truth distribution unless we have an exponentially large number of samples\", how is it not the same case in their paper as well?\n\nWe believe that you are referring to the quote in remark 4. What we meant by this is, for the empirical distribution i.e. the uniform distribution over the given set $U_{sample}$ of ground truth samples to be close to the target distribution $\\mu$, we\u2019d need $U_{samples}$ to be of size $\\exp(d)$ i.e. exponential in the dimension, which is commonly referred to as the \u201ccurse of dimensionality.\u201d This is irregardless of the property of the distribution $\\mu$, even if $\\mu$ is a log-concave distribution or a spherical Gaussian, the above fact still holds.\nAs we have stated earlier, our algorithm takes poly(d) time when the target distribution is a mixture of constantly many log-concave distributions, thus escaping this curse of dimensionality."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077564356,
                "cdate": 1700077564356,
                "tmdate": 1700077564356,
                "mdate": 1700077564356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oTOklOgP8X",
                "forum": "oAMArMMQxb",
                "replyto": "zxeTimoaqn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_7jZP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Reviewer_7jZP"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the authors for their comprehensive response. I have fully read all the rebuttal arguments and other reviews. Although some of the issues were answered I still believe some of the main issues that were raised by reviewers still stand and I absolutely do not understand some of the reviewers' scores given they think there is a structural issue. At any rate, I very much so appreciate the authors' effort to respond to my concerns."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678028592,
                "cdate": 1700678028592,
                "tmdate": 1700678028592,
                "mdate": 1700678028592,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WmF3oD1Soi",
            "forum": "oAMArMMQxb",
            "replyto": "oAMArMMQxb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6339/Reviewer_vctH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6339/Reviewer_vctH"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes Langevin sampling with approximate scores, given some samples of the target distribution are provided.  It is shown that the scheme initialized from a uniform distribution over the available data points generates a sample close to the target in TV distance when stopped at a finite time T. The considered targets are mixtures of strongly log-concave measures and the paper's focus is on showing that with data based initialization, it is possible to remove the dependence on the LSI constant of the whole mixture in Theorem 2.1 of [1] which studied the same scheme.\n\n---\n[1] Lee, Holden, Jianfeng Lu, and Yixin Tan. \"Convergence for score-based generative modeling with polynomial complexity.\" Advances in Neural Information Processing Systems 35 (2022): 22870-22882."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "*The paper formalizes an intuitive result*: Sampling from mixtures is difficult because it is hard to transition in reasonable time from one component to another. If it was known where the mixtures were as well as their relatives weights, then no transitioning would be necessary and sampling could easily be achieved by initializing inside the components' typical sets. This intuitive natural idea (dismissed as too obvious/unrealistic in [1] sec 1.2)  is what is formalized in the paper. Although the result is not very surprising, the authors go through the laborious task of linking the components needed to establish the result: data based initialization with the existing analyses on approximate scores(or inexact langevin) and discretization of langevin diffusions. \n\n *A clear and easy to follow proof outline*: There is a nice simple setting that is detailed in the main text to understand the paper's strategy, which is quite useful since the paper's result requires lengthy combinations of several results.\n\n---\n[1] Lee, Holden, Andrej Risteski, and Rong Ge. \"Beyond log-concavity: Provable guarantees for sampling multi-modal distributions using simulated tempering langevin monte carlo.\" Advances in neural information processing systems 31 (2018)."
                },
                "weaknesses": {
                    "value": "- *Structure and unconvincing arguments*: Some sections could be better restructured namely section 1.2. It jumps from motivation to related work to possible extensions. Some remarks can also feel a little unconvincing there. For example, computational hardness arguments are invoked in the motivation but the main idea of noised score learning is to have an annealing scheme where the denoising is only performed from one noise level to a slightly lower noise level. Annealing breaks down the difficulty of denoising, so the paragraph is criticizing an alternative that is never used. A further minor point: remark 5 is an excessively long paragraph to say the mean is not in the typical set in high dimensions.\n- *Log sobolev constant for well connected mixtures* : A contribution of the paper is extending the Poincare inequality through decompositions result of Madras & Randall (2002) to the log sobolev inequality. It would be very surprising if such an extension has not already been done as the result is old and functional inequalities are heavily researched. I would kindly ask the authors to check and possibly include references [1] to ensure that they are not missing prior work.\n- *On significance*: For the MCMC community this result will not be worthwhile as samples aren't available. For generative modeling where there is a dataset, the better modelling of what practitioners do uses time-varying score functions and so what is done in practice does not correspond to Langevin with a time independent score. The \"long line\" of experimental work that uses vanilla langevin is claimed to exist but never cited. The paper answers a small interesting curiosity related to data based initialization for sampling from mixtures with limited links with practice.\n---\n[1] Jerrum, M., Son, J. B., Tetali, P., & Vigoda, E. (2004). Elementary bounds on Poincar\u00e9 and log-Sobolev constants for decomposable Markov chains."
                },
                "questions": {
                    "value": "- *Number of samples needed, Proposition 23 (and 24)*: From my understanding, for the approach to work, the available samples must have the correct weights of the components. Could the authors explain why the $M$ does not seem to depend on properties of $\\mu$ besides the number of components ?\n- *Samples to train and samples to initialize*: Presumably the approximate score is learnt from the same available samples $M$ used to initialize. This dependence could break some concentration arguments. Could the authors briefly discuss whether it is necessary to hold out some samples for initialization when learning the score ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Reviewer_vctH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699124916360,
            "cdate": 1699124916360,
            "tmdate": 1699636698167,
            "mdate": 1699636698167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c82ibEStVr",
                "forum": "oAMArMMQxb",
                "replyto": "WmF3oD1Soi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to reviewer vctH"
                    },
                    "comment": {
                        "value": "We thank you for your feedback. Below are our responses to your comments and questions.\n\n$\\textbf{Some remarks can also feel a little unconvincing there. For example, computational hardness arguments are invoked in the motivation but the main idea of noised score learning is to have an annealing scheme where the denoising is only performed from one noise level to a slightly lower noise level. Annealing breaks down the difficulty of denoising, so the paragraph is criticizing an alternative that is never used.}$\n\nThe point we have made here is correct, nonetheless. We agree that in annealing schemes, the overall task of denoising is broken down into getting from one noise level to a slightly lower noise level. But in this setting, it actually doesn\u2019t matter what learning scheme is used to try to get at the annealed score functions, because the fundamental bottleneck is that the annealed score function (from the displayed equation in the top of page 4) in the sparse spiked Wigner model is hard to approximately compute. That score function is exactly what is needed to slightly denoise the data, and since we cannot compute it in polynomial time, we definitely cannot learn a computationally efficient model to do it.\n\n\"A contribution of the paper is extending the Poincare inequality through decompositions result of Madras & Randall (2002) to the log sobolev inequality.\nIt would be very surprising if such an extension has not already been done as the result is old and functional inequalities are heavily researched.\nI would kindly ask the authors to check and possibly include references [1] to ensure that they are not missing prior work.\"\n\nWe were also surprised that despite the long history of research on functional inequalities, a result like ours was not known. The most closely related prior results are [1812.06464] Poincar\u00e9 and log-Sobolev inequalities for mixtures (arxiv.org) (2019) and 2102.11476.pdf (arxiv.org) (2021) but the former only consider mixture of 2 components ($K=2$) while the latter only applies for mixture of isomorphic Gaussians. Given the recent date of these publications, especially the 2019 paper which studies log-Sobolev of mixtures in an identical setting to ours and obtains a much weaker result, it is highly unlikely that our result for log-Sobolev of mixtures was previously known.\nWe are familiar with reference [1] (we believe you mean Jerrum, M., Son, J. B., Tetali, P., & Vigoda, E. (2004)., though there are two different references labeled [1] in your response). \nThis work applies to a different setting, when the state space (read, $\\mathbb{R}^d$) are decomposed into disjoint subsets, and the log-Sobolev of a Markov chain over the entire state space is related to that of Markov chains restricted to within these disjoint subsets (the restriction chains) as well as the projection chain which moves between these subsets. Their result does not apply to our setting, since (1) we don\u2019t have a decomposition of the state space into disjoint subsets, and it\u2019s unclear if any natural decomposition can be made when the Gaussian components have high overlap; (2) the Langevin with respect to one component $\\mu_i$ of the mixture $\\mu=\\sum_i p_i \\mu_i$ (i.e. driven by the gradient $\\nabla \\log \\mu_i$ is not the restriction of the Langevin with respect to $\\mu$  (i.e. driven by the gradient $\\nabla \\log \\mu$) on any subset of the state space.\nThis is why recent works by Schliting (2019) and Chen, Chewi, Niles-Weed (2021) have needed a new analysis and also why we need a new analysis as well."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078727767,
                "cdate": 1700078727767,
                "tmdate": 1700078937425,
                "mdate": 1700078937425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FguqQVEHzV",
            "forum": "oAMArMMQxb",
            "replyto": "oAMArMMQxb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6339/Reviewer_94gx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6339/Reviewer_94gx"
            ],
            "content": {
                "summary": {
                    "value": "This work studies vanilla score matching in the context of mixtures of log-concave distributions, providing a recipe to learn multimodal distributions via vanilla score matching, a procedure that has provably failed in most multimodal settings: (1) data-based initialization for the Langevin Monte Carlo chain; and (2) early stopping of the diffusion. The work demonstrates substantive theoretical developments for the proposed method, along with a few toy examples to illustrate the effectiveness of empirical distribution initialization and early stopping in learning mixture of Gaussians."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work aims to tackle a widely accepted issue of vanilla score matching that motivated the usage of annealed langevin dynamics in learning multimodal distributions (diffusion models as a class of generative models) \u2014 it has the potential to inspire new algorithms for generative modeling.\n2. Code for the toy examples is provided via **Supplementary Material** to facilitate reproducibility."
                },
                "weaknesses": {
                    "value": "1. Experiment results do not appear to be convincing enough:\n* The ground truth distribution is not plotted in Figure 1 (b); by comparing to Figure 1(c), it\u2019s not hard to tell that the weight of the component on the right is not learned very well \u2014 one has a density around $0.15$, while the other around $0.125$. Meanwhile, there is no numerical computation on the learned mean and variance of each component, and how they compare to the ground truth.\n* Similar to the issue in Figure 1, the values of the learned projected mean, variance and weight of each component, and their comparisons with the ground truth distribution, are not reported for the experiment presented in Figure 2.\n2. It\u2019s not clear how or where early stopping is proposed as a solution throughout the theoretical development.\n3. Some minor issues in writing:\n* Page 1, first bullet point of positive aspects: \u201ca simple closed form solution <when> the class of models\u201d\n* Page 3, the line above **Theorem 1**: \u201csamplling\u201d\n* Page 3, strange expression: \u201cNote in particular that we have can draw as many samples as we like\u201d. Perhaps remove \u201chave\u201d?"
                },
                "questions": {
                    "value": "1. Could the authors comment on how the contributions in this work might help in improving diffusion models as a class of generative models?\n2. Can optimization techniques such as exponential decay learning rate schedule achieve similar results as early stopping, in the context of estimating vanilla score function of a multimodal distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6339/Reviewer_94gx"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6339/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699167338581,
            "cdate": 1699167338581,
            "tmdate": 1699636697964,
            "mdate": 1699636697964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "deHD3JUUux",
                "forum": "oAMArMMQxb",
                "replyto": "FguqQVEHzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6339/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to reviewer 94gx"
                    },
                    "comment": {
                        "value": "We thank you for your feedback. Below are our responses to your comments and questions.\n\n\"Experiment results do not appear to be convincing enough:...\"\n\nThe distributions do not _exactly_ match since we only initialized with only 40 samples. This is a practical choice for the purpose of making a useful visualization to explain the result, since if we use a very large number of samples then eventually figure 1(a) will perfectly match the ground truth as well since it is a 1-dimensional problem. Of course, the theory guarantees that if we use more data the estimator of the ground truth is consistent. Our method *does not* explicitly estimate a mean and variance of each component (i.e. it does not explicitly try to fit a mixture of 2 gaussians to the data), but instead it attempts to estimate the distribution as a density (and the distribution it outputs is not a mixture of 2 gaussians, only an approximation). This is why we plotted the densities instead of giving the parameter error. \n\n\"Similar to the issue in Figure 1, the values of the learned projected mean, variance and weight of each component, and their comparisons with the ground truth distribution, are not reported for the experiment presented in Figure 2.\nIt\u2019s not clear how or where early stopping is proposed as a solution throughout the theoretical development.\"\n\nPlease see the reply to question 2 below. \n\nAnswer to your questions:\n1. \"Could the authors comment on how the contributions in this work might help in improving diffusion models as a class of generative models?\"\n\nOur work verifies empirical findings that data-based initialization is helpful for the vanilla Langevin algorithm, showing that this is also a theoretically valid way to deal with multimodality. (As in e.g. cited paper of Xie et al \u201816). In general, it can be difficult to predict the ultimate benefits of building our theoretical understanding. One reason to care about methods which use data-based initialization like contrastive divergence is that they are approximate to Maximum Likelihood, which we know from fundamental results in statistics is the asymptotically optimal way to fit a model as the number of samples goes to infinity. So it wouldn\u2019t be too surprising if these kinds of methods are more data efficient in some applications \u2014 this is a very interesting question, but also quite out of scope of the current work which is focused on the theory. \n\n2. \"Can optimization techniques such as exponential decay learning rate schedule achieve similar results as early stopping, in the context of estimating vanilla score function of a multimodal distribution?\"\n\nIn our paper when we talk about early stopping, we are not talking about in the context of estimating the vanilla score function. Our analysis works with any learning procedure for the vanilla score function, as long as at the end of the day it is accurately learned (so decaying learning rate schedules, early stopping, etc. can all be used in the training of the score matching network). Instead, when we are talking about early stopping we are saying that the Langevin sampler is stopped, rather than run for an arbitrarily long time. This is what is illustrated in Figure 1."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6339/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079527001,
                "cdate": 1700079527001,
                "tmdate": 1700079527001,
                "mdate": 1700079527001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]