[
    {
        "title": "UniINR: Unifying Spatial-Temporal INR for RS Video Correction, Deblur, and Interpolation with an Event Camera"
    },
    {
        "review": {
            "id": "vcaUpho1H4",
            "forum": "lf7gguJgpq",
            "replyto": "lf7gguJgpq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1042/Reviewer_gAyr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1042/Reviewer_gAyr"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to recover arbitrary frame-rate sharp GS frames from an RS blur image and events. To this end, the authors introduce the spatial-temporal implicit neural representation (INR) to recover the sharp GS frames by mapping the position and time coordinates to RGB values. The input RS blur image and events are fused to construct a spatio-temporal representation and extract features from them at any time and position. This method has a lightweight and fast network structure, and also achieves high image reconstruction performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors thoroughly explain the framework and the proposed INR representation is experimentally verified for its ability to represent spatio-temporal features. \n2. This method has a lightweight and fast network structure, and also achieves high image reconstruction performance."
                },
                "weaknesses": {
                    "value": "1. The fusion of events and images is completely end-to-end, with no explicit connection between the exposure time and the timestamp in events. This looks like a whole black box and lacks interpretability. There are established methods that associate exposure time, optical flow, and events, which is more intuitive, but of course introduces more computation, which is a key claim of this method.\n2. Visual comparisons on real dataset does not show significant differences. Are there more obvious samples in the dataset? Otherwise I don't think it can support the generalizability of the model.\n3. In C.5 of the Supplementary Material, the author mentions that concat can perform better than addition, so why does the author continue to use only addition?"
                },
                "questions": {
                    "value": "The authors have done a lot of explanations and experiments and have also fully demonstrated the effectiveness of the INR representation for RS deblurring and corr. However, I don't see much new from the model design. This is neither the first application of the INR representation to event-based vision, nor is it an exciting one. My opinion is borderline reject."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1042/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1042/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1042/Reviewer_gAyr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698582354749,
            "cdate": 1698582354749,
            "tmdate": 1699636030420,
            "mdate": 1699636030420,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SodtId5MEo",
                "forum": "lf7gguJgpq",
                "replyto": "vcaUpho1H4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1042/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gAyr"
                    },
                    "comment": {
                        "value": "Dear Reviewer, gAyr,\n\nFirst and foremost, we would like to express our sincere gratitude for the time and effort you have invested in reviewing our manuscript. Your insightful comments and suggestions are immensely valuable to us. We have carefully considered each point you raised and provided detailed responses below to address your concerns.\n\n---\n## Weakness Response\n\n### W1.1: NO explicit connection between the exposure time and the timestamp in events.\nWe apologize for any confusion caused by our presentation.\nTo clarify, we do indeed model the relationship between exposure time and the timestamp of events in our paper (Section 3.1, Lines 133-169).\nSpecifically, Equations 3 and 4 in our paper introduce the connection between exposure time and timestamps. Additionally, the role of the event in this formulation is detailed from Line 158 to Line 160.\nThis approach is the linkage between the eve timestamp of events and the image exposure time.\n\n### W1.2: Associate exposure time, optical flow, and events, which is more intuitive.\nFurthermore, our INR-based method utilizes Exposure Time Embedding to incorporate time information into the INR. This allows us to directly obtain intensity values at any given time, thereby circumventing the need for more complex operations like optical flow computation. As you pointed out, while such methods are more intuitive, they also introduce significantly more computational complexity. Our approach achieves comparable results with fewer parameters, aligning with our goal of computational efficiency.\n\n### W2: More obvious samples in the dataset.\nThank you for pointing out the need for more distinct examples in our dataset. We acknowledge your concerns regarding the visual comparisons on the real dataset. To address this, we direct your attention to Figure 4 (c) in our paper, where the results from EvUnroll exhibit noticeable artifacts, as indicated by the red circles. These artifacts underscore the differences and improvements our method offers.\n\nFurthermore, we have included additional real-world data visualizations in the supplementary materials (Figure 6). These examples effectively demonstrate our method's ability to correct deformations, particularly in challenging scenarios such as outdoor windows. This robust performance in diverse real-world conditions highlights the generalizability and effectiveness of our rolling shutter correction approach.\n\n### W3: Concat and addition operate:\nThank you for your insightful query regarding our choice of operation in our model.\nIndeed, while concatenation can offer marginal improvements, it comes at the cost of increased model size and computational overhead. In our experiments, we observed that the performance differences between addition, multiplication, and concatenation were relatively minimal, with variations around 0.03 dB and 0.04 dB, which are not substantial enough to justify the additional complexity.\n\nGiven our emphasis on maintaining a lightweight and efficient design, we decided to continue using addition, which aligns with our objectives of computational efficiency and simplicity. However, recognizing the potential benefits of alternative operations, we plan to include these options in our publicly released code. This will allow users to explore and leverage different operation types according to their specific requirements and constraints."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578105662,
                "cdate": 1700578105662,
                "tmdate": 1700578105662,
                "mdate": 1700578105662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oGZQAuPXgg",
            "forum": "lf7gguJgpq",
            "replyto": "lf7gguJgpq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1042/Reviewer_QpjK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1042/Reviewer_QpjK"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, an event-based RS video correction, deblur, and interpolation method is proposed. Different from existing methods that decompose the whole process into multiple tasks, the authors take advantage of the high temporal resolution of event cameras to recover arbitrary frame-rate sharp GS frames from an RS blur image and paired event data. The key idea is unifying spatial-temporal implicit neural representation (INR) to directly map the position and time coordinates to RGB values to address the interlocking degradations in the image restoration process. Several experiments have been conducted to demonstrate the validity of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe question of using event cameras for unified RS image enhancement makes sense, although event cameras have been explored on separate tasks. This is the first attempt.\n2.\tTo this end, the authors propose a series of modules to perform the proposed task"
                },
                "weaknesses": {
                    "value": "1. Although the question raised is forward-looking, the technical details given by the authors are not very innovative, like some stacking of existing models. This makes it harder for reviewers to capture what the real contribution are. It is recommended that authors summarize the contribution visibly.\n2. A more important issue is that of experimental validation. Overall experimental validation is not sufficient to prove the validity of the proposed methodology. The authors mention the use of two different experimental settings:\n (I) the single GS sharp frame restoration, i.e., the RS correction and deblurring. However, the EvUnroll is only the event-guided RS correction method, and eSL-Net is only the event-guided deblurring method. The target algorithm that the authors should be comparing should be correction+deblurring simultaneously. Thus, it is recommended that the authors use a combination of correction + deblurring algorithms for experimental comparisons, including but not limited to EvUnroll +eSL-Net, there are many event-guided deblurring algorithms available.\n(II) a sequence of GS sharp frames restoration, i.e., the RS correction, deblurring and interpolation. However, the DeblurSR is the event-guided deblurring and interpolation method and does not target the correction. Therefore, the experimental comparison is not very fair. Meanwhile, the combination of EvUnroll and TimeLens does not target the deblurring.\nOverall, the authors are advised to conduct a fairer and more comprehensive experimental comparison\n3. Whether the algorithms being compared are using pre-trained models or re-trained? Reviewers noted that the compared methods were significantly lower than the proposed methods."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635270001,
            "cdate": 1698635270001,
            "tmdate": 1699636030349,
            "mdate": 1699636030349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uMxJMDYv1C",
                "forum": "lf7gguJgpq",
                "replyto": "oGZQAuPXgg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1042/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QpjK"
                    },
                    "comment": {
                        "value": "Dear QpjK:\nFirst and foremost, we would like to express our sincere gratitude for the time and effort you have invested in reviewing our manuscript. Your insightful comments and suggestions are immensely valuable to us. We have carefully considered each point you raised and provided detailed responses below to address your concerns.\n\n\n---\n\n### W1: Summarization of Contributions\nWe are grateful for your feedback and take this opportunity to succinctly restate the key contributions of our work, earnestly anticipating your recognition of these aspects:\n\n**Novel Research Question:** This study pioneers the recovery of high-frame-rate sharp GS frames at arbitrary frame rates from RS blur images and paired event data. Our approach diverges fundamentally from traditional methods, charting a new course in event-based imaging. This first-of-its-kind effort brings a fresh perspective to the field.\n\n**Innovative Methodology:** We have developed a one-stage framework utilizing a unified implicit neural representation (INR). Distinctively, it maps space-time coordinates to RGB values through an Exposure Time Embedding module coupled with a Pixel-by-pixel Decoding module. This framework eschews the conventional reliance on optical flow, traditionally known for its computational complexity. Our innovative integration of temporal and spatial data sets our method apart from existing approaches.\n\n**Superior Performance:** Our model demonstrates exceptional performance across datasets with varying degrees of blur, as evidenced in Tables 1 and 2 of the main paper. It consistently outperforms the existing method, EvUNRoll, while maintaining a remarkably efficient architecture with only 0.379M parameters. This efficiency, combined with its effectiveness, underscores our model's superiority over more complex models.\n\n\n### W2: Experimental Validation\nWe value your thoughtful feedback regarding our experimental validation. To clarify and address your concerns:\n\n**Single GS Sharp Frame Restoration:**\n\n- **EvUnroll:** As stated in EvUnroll's documentation, it includes an optional image deblurring module. By processing the RS image first through this deblurring module and then the RS correction module, EvUnroll effectively accomplishes both deblurring and RS correction in sequence.\n- **eSL-Net:** For our specific experimental needs, we have adapted eSL-Net by modifying its parameter initialization method and removing the up-sampling module. These adjustments ensure its compatibility and efficient functioning in our experimental framework.\n\n**Sequence of GS Sharp Frames Restoration:**\n- **Combination of EvUnroll and TimeLens:** This combination is capable of handling deblurring, RS correction, and interpolation. The process involves sequentially feeding inputs through EvUnroll\u2019s optional deblurring module, then its RS correction module, and finally, TimeLens\u2019s interpolation network. For a detailed depiction of this procedure, we refer you to **Fig. 10 (II)** in the supplementary material, which offers a comprehensive illustration of the entire process.\n\nWe trust these detailed explanations and methodological adjustments adequately address your concerns regarding the fairness and thoroughness of our experimental comparisons.\n\n\n### W3: Whether the algorithms being compared are using pre-trained models or re-trained?\nResponse: We appreciate your insightful observations regarding the training status of models in our comparative analysis. We recognize the importance of clear experimental descriptions and are committed to enhancing the clarity in the experimental section of our paper. To address your concerns, we offer the following clarifications:\n\n**Pre-trained Models or Re-trained?:**\nWe utilized the pre-trained model of TimeLens owing to the unavailability of its training code.\nFor all other models in our comparison, we re-trained them using their respective publicly released codes.\n\n**Fairness in Comparative Analysis:**\nTo ensure an equitable comparison, we evaluated our method under the same settings as those used for EvUnroll.\nThis comparison, detailed in Table 2 of our main paper, underscores the superior performance of our approach.\nIt\u2019s noteworthy that EvUnroll supports the simulated dataset used in Table 2.\n\n**Focus on larger degrees of Motion Blur and Distortion:**\nBenefiting from the one-stage method, our model has better performance for large degree motion blur.\nTo this end, we created a simulated dataset with a larger degree of blur, as presented in Table 1.\nThe parameters for this simulation are detailed in Supplementary Material C.1.\nAs indicated in Table 1, the performance of the compared methods falls below that of our proposed method.\nThis discrepancy can be attributed to the inherent challenges associated with effectively handling severe RS correction, motion deblurring, and frame interpolation simultaneously."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578569448,
                "cdate": 1700578569448,
                "tmdate": 1700578569448,
                "mdate": 1700578569448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JAJeTNIgHW",
            "forum": "lf7gguJgpq",
            "replyto": "lf7gguJgpq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1042/Reviewer_w92c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1042/Reviewer_w92c"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method named UniINR that interpolates arbitrary sharp global shutter (GS) frames from a single rolling shutter (RS) frame with the assistance of an event camera. The proposed model unifies the spatial-temporal implicit neural representation (INR) to address the interlocking degradations in the process of image restoration. Experimental results show that the proposed method outperforms other comparing methods in both quality evaluations and efficiency evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors proposed the first novel yet efficient learning framework that can recover arbitrary frame-rate sharp GS frames from an RS blur image with corresponding event signals.\n\n- The network is light-weighted with only 0.379M parameters, which is much smaller than EvUnroll model. Furthermore, the efficiency of this model is evident in its remarkably swift inference time, outperforming previous methods by a substantial margin.\n\n- The performance of the proposed method is better than the state-of-the-art methods in both quantitative evaluations such as PSNR, SSIM, LPIPS, and visual results. The videos provided in the supplementary materials show the better performance of the proposed method."
                },
                "weaknesses": {
                    "value": "- In the fusion of RS frames and event streams, there is no specific modules or design that focus on the domain gap between these two modalities. Since the format and information recorded by the intensity values and event signals are quite different.\n\n- The event signals directly record the coordinates of events triggered at that time. Unlike the intensity frames, in one event voxel tensor, not all the pixels contain event signals. As a result, directly applying INR to event voxel tensor doesn\u2019t consider the sparsity of event signals."
                },
                "questions": {
                    "value": "- The comparison of the methods in this paper is a crucial aspect of evaluating its contributions. However, it has come to my attention that the performance of the compared methods in this submission does not appear to match the results presented in their original papers. It is imperative that a fair and comprehensive evaluation of the methods is conducted to ensure the validity of the comparisons and the reliability of the conclusions drawn in this work. Specifically, the performance of EvUnroll and JCD are worse than their original papers in some metrics and qualitative evaluations. Did the authors retrained their models in a different way?\n\n- The authors claim for multiple times that they are the first to achieve arbitrary frame interpolation from an RS blur image and events. I think it is not that convincing. Because EvUnroll is also able to reconstruct multiple GS sharp frames.\n\n- In the supplementary code, I found that in the SCN module, the input images and events are multiplied to get a feature called \u2018x1\u2019. I wonder what the meaning of multiplication is.\n\n- In experimental part, this paper does not utilize optical flow for frame interpolation, unlike methods such as VideoINR [1] and MoTIF [2], which use INR to predict optical flow and then perform frame interpolation. However, in this paper, optical flow is not employed, and it seems that there is no dedicated comparison with optical flow methods in the conducted ablation experiments.\n\n[1] Chen et al., VideoINR: Learning Video Implicit Neural Representation for Continuous Space-time Super-resolution, CVPR 2022.\n\n[2] Chen et al., MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution, ICCV 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1042/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1042/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1042/Reviewer_w92c"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698644173795,
            "cdate": 1698644173795,
            "tmdate": 1699636030245,
            "mdate": 1699636030245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mdIRxBPItF",
                "forum": "lf7gguJgpq",
                "replyto": "JAJeTNIgHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1042/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer w92c"
                    },
                    "comment": {
                        "value": "Dear Reviewer w92c,\n\nFirst and foremost, we would like to express our sincere gratitude for the time and effort you have invested in reviewing our manuscript. Your insightful comments and suggestions are immensely valuable to us. We have carefully considered each point you raised and provide detailed responses below to address your concerns.\n\n---\n\n## Weaknesses\n\n### W1: Specific modulesor design that focus on the domain gap.\nIn response to your query about the fusion of RS frames and event streams, we would like to direct your attention to lines 181-198 of our paper, where we elaborate on the rationale behind our encoder's design. Our approach is influenced by the principles of sparse learning, which we've found to be particularly effective when bridging the domain gap between the two modalities. The sparse learning framework has been proven effective in previous work for handling the distinct formats and information characteristics inherent in intensity values and event signals. By integrating this framework into our encoder design, we effectively address the unique challenges posed by the fusion of these two distinct data types.\n\n### W2: Directly applying INR to event voxel tensor doesn\u2019t consider the sparsity of event signals.\nIn addressing your concern regarding the application of INR to event voxel tensors, it's important to clarify that our INR framework operates on the features outputted by the Encoder, rather than directly on the event data. The Encoder is specifically designed to capitalize on the high temporal resolution of event signals, enabling it to learn a continuous spatiotemporal representation. This design choice effectively addresses the sparsity of event signals, ensuring that our INR framework is applied to a more contextually rich and temporally informed feature set, rather than directly to the sparse event data."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578435958,
                "cdate": 1700578435958,
                "tmdate": 1700578435958,
                "mdate": 1700578435958,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XvnF4c0Z7k",
            "forum": "lf7gguJgpq",
            "replyto": "lf7gguJgpq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1042/Reviewer_xnov"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1042/Reviewer_xnov"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method that recovers HFR and sharp global shutter frames from a rolling shutter blur image with the assistance of events. The authors propose an implicit neural representation to map the position and time coordinates to RGB image values. The experimental comparison with existing methods shows its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) Compared with the existing algorithms, the efficiency of the proposed algorithm is greatly improved.\n(2) The experimental comparison shows that the proposed method exceeds the existing algorithms on both selected simulated and real dataset samples."
                },
                "weaknesses": {
                    "value": "(1) A comprehensive performance comparison with EvShutter[1] is suggested, which has been published in CVPR2023.\n(2) Both EvShutter and Evunroll demonstrate the performance of the proposed algorithms for eliminating the RS effect caused by fan rotation. To verify the robustness of the proposed method for different scenes, it is suggested to add similar experiments on real fan data.\n(3) In Figure 5, it seems unfair and unnecessary to compare the running time of the proposed algorithm with the combination of EU and TL.\n(4) The Evunroll also seems to take into account image deblurring and high frame-rate video output.\n\n[1] Julius Erbach, Stepan Tulyakov, Patricia Vitoria, Alfredo Bochicchio, Yuanyou Li; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 13904-13913."
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836158316,
            "cdate": 1698836158316,
            "tmdate": 1699636030167,
            "mdate": 1699636030167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iXwUB9nD3c",
                "forum": "lf7gguJgpq",
                "replyto": "XvnF4c0Z7k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1042/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xnov"
                    },
                    "comment": {
                        "value": "Dear Reviewer, xnov,\n\nFirst and foremost, we would like to express our sincere gratitude for the time and effort you have invested in reviewing our manuscript. Your insightful comments and suggestions are immensely valuable to us. We have carefully considered each point you raised and provided detailed responses below to address your concerns.\n\n---\n\n### W1: Comparison with EvShutter:\nWe appreciate your suggestion regarding a comprehensive performance comparison with EvShutter. We would like to provide the following clarifications to address this concern:\n\n**Accessibility of the Dataset:** The RS-ERGB dataset proposed by EvShutter is not publicly available, limiting our ability to perform a direct comparison.\n\n**Opacity of Dataset Details:** The EvShutter paper does provide comparative results on the Fastec-RS dataset. However, essential details about this dataset, specifically the degree of simulation for Rolling Shutter and Blur, are not explicitly disclosed. This ambiguity impedes our ability to perform a precise and accurate comparative analysis with EvShutter.\n\n**Lack of Code and Implementation Details:** Another challenge is the unavailability of EvShutter's code. Additionally, the paper omits certain critical implementation details. These gaps pose a considerable obstacle in ensuring an accurate and equitable comparison.\n\nDespite these hurdles, we have endeavored to present a general comparison. In the Supplementary Material C.1 (lines 539-549), we describe our approach for synthesizing Rolling Shutter and Blur on the Fastec-RS dataset. Our experimental results demonstrate robust performance even under conditions of more pronounced blurring than what is seen with EvShutter (and EvUnRoll). Specifically, our model achieves a PSNR value of 33.64 and an SSIM value of 0.9299. These metrics significantly surpass the performance of 32.41 PSNR and 0.91 SSIM reported by EvShutter, highlighting the efficacy of our method under challenging conditions.\n\n### W2: More Example:\nThank you for suggesting additional experiments with real fan data to demonstrate the robustness of our method in different scenes. We acknowledge that EvUnRoll reported fan samples in their paper. However, the four real-world data scenes provided by EvUnRoll do not include these fan examples. In our experiments, we focused on these four scenarios, two indoor and two outdoor, to evaluate our method's effectiveness in diverse environments. The results of these experiments are showcased in Figure 6 of our supplementary materials. Our model demonstrates effective correction in both indoor and outdoor settings, with noticeable corrections observed especially in outdoor window scenes. This performance highlights our model's capability to adapt and correct various real-world distortions, reinforcing its robustness and versatility."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578360078,
                "cdate": 1700578360078,
                "tmdate": 1700578360078,
                "mdate": 1700578360078,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]