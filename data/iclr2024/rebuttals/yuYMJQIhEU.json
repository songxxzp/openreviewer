[
    {
        "title": "Communication-efficient Random-Walk Optimizer for Decentralized Learning"
    },
    {
        "review": {
            "id": "fsGfAG4hio",
            "forum": "yuYMJQIhEU",
            "replyto": "yuYMJQIhEU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3292/Reviewer_na6d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3292/Reviewer_na6d"
            ],
            "content": {
                "summary": {
                    "value": "The paper \"Communication-efficient Random-Walk Optimizer for Decentralized Learning\" proposes a variant of ADAM to solve decentralized problems in which communication exchange happens via random walk agent selection. The main contribution is to propose a variant of ADAM that only requires the exchange of the iterate itself as opposed to requiring the exchange of ADAM's momentum terms. This effectively divides the communication cost by a factor of three. The authors establish convergence results and provide numerical simulations to illustrate the performance of the proposed scheme."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed work reduces communication cost of the combination of ADAM and a random walk procedure for information exchange by a factor 3 by omitting the transmission of ADAM's momentum terms. Additionally, to further save communications the authors have each agent run a mini-batch of tunable length K to further save communications. To avoid that this leads to overfitting the authors add a slight perturbation when calling the gradient oracle. The introduction of these modifications allows the authors to claim that the incurred sub-optimality, measured via the gradient magnitude, as they are dealing with non-convex problems, vanishes at the appropriate rate."
                },
                "weaknesses": {
                    "value": "The work combines existing understood tools to provide with a new scheme. While many times such a combination is non-obvious, I would suggest the authors point out in the main text  the challenges faced in the analysis to obtain the final result."
                },
                "questions": {
                    "value": "- Given that a mini-batch is introduced to reduce the communication cost by a factor of K, why is it necessarily the case that using this variant of ADAM which costs \"1\" to communicate  as opposed to \"3\" (original ADAM-based random-walk), better than using ADAM + random walk with a mini-batch size of 3K? \n- From the simulations, it seems that ADAM performs worse on the training data set (counting only computations). Can the authors explain why this seems to be the case?\n- I would suggest the authors comment on the scaling of the main result with network related parameters, and how these compare to the literature, i.e. local gradient variance, gradient heterogeneity, number of agents, etc."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672116557,
            "cdate": 1698672116557,
            "tmdate": 1699636278081,
            "mdate": 1699636278081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7yjiRK0Lmo",
                "forum": "yuYMJQIhEU",
                "replyto": "fsGfAG4hio",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses"
                    },
                    "comment": {
                        "value": "We would like to thank you for your constructive feedback. Here we answer your questions point by point:\n\n> The work combines existing understood tools to provide with a new scheme. While many times such a combination\nis non-obvious, I would suggest the authors point out in the main text the challenges faced in the analysis to obtain\nthe final result. \n\nThank you for your suggestion. While our method includes some existing understood tools, we would\nlike to clarify that a key component of our method is to use localized momentum and pre-conditioner, which has not been\nexplored in previous works (Sun et al. 2022; Triastcyn et al. 2022) on random-walk decentralized optimization. Please also\nsee our general responses regarding novelty. Using localized momentum and pre-conditioner creates additional difficulty in\ntheoretical analysis, as the momentum and pre-conditioner are not always updated and synchronized.\n\n> Given that a mini-batch is introduced to reduce the communication cost by a factor of K, why is it necessarily the\ncase that using this variant of ADAM which costs \u201d1\u201d to communicate as opposed to \u201d3\u201d (original ADAM-based\nrandom-walk), better than using ADAM + random walk with a mini-batch size of 3K? \n\nWe suppose $K$ refers to the\nnumber of local steps in the inner loop of Algortihm 3, whose influence is studied in Figure 7, Section 4.3. While using a\nlarger $K$ can reduce the communication cost, setting it too large can lead to worse models, which agrees with Theorem 3.7\nand Corollary 3.8. Therefore, solely using more local steps may not always lead to better performance, while removing the\ncommunication of auxiliary parameters (momentum or preconditioner) can be helpful in such case.\n\n> From the simulations, it seems that ADAM performs worse on the training data set (counting only computations).\nCan the authors explain why this seems to be the case? \n\nFrom Figure 2-5, it seems that Adam has almost the same\nperformance as Gossip, FedAvg or Localized SAM (comparing the orange line with purple, gray and pink lines).\n\n> I would suggest the authors comment on the scaling of the main result with network related parameters, and how these compare to the literature, i.e. local gradient variance, gradient heterogeneity, number of agents, etc.\n\nIn theorem 3.7, the convergence rate is related to the communication network with $\\lambda$, which depends on the communication network \n(namely the eigenvalues of the transition matrix). \nThe local gradient variance, denoted as $\\sigma^2$, is assumed in Assumption 3.4. The gradient heterogeneity, as measured by differences on client gradients $\\varsigma$, \nis assumed in Assumption 3.5.\nBoth $\\sigma$ and $\\varsigma$ appear in theorem 3.7 as well \nand the dependency matches previous analysis in literature \n(Sun et al. 2022; Triastcyn et al. 2022). \nThe number of agents (denoted as $n$) also appears in theorem 3.7, \nand its squared root dependency to convergence rate also matches previous analysis on random-walk decentralized algorithms (Triastcyn et al. 2022)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470361939,
                "cdate": 1700470361939,
                "tmdate": 1700470388104,
                "mdate": 1700470388104,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SauPjQJTRd",
            "forum": "yuYMJQIhEU",
            "replyto": "yuYMJQIhEU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3292/Reviewer_w8Ss"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3292/Reviewer_w8Ss"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a random-walk Adam-based optimizer for decentralized learning in the setting that only one agent is active at each communication round. The objective of the proposed algorithm is to reduce communication cost while still maintaining acceptable learning performance, which is achieved by removing the auxiliary momentum parameter, avoiding transmitting the preconditioner, and performing multiple local updates. To overcome the potential overfitting issue brought by multiple local updates, sharpness-aware minimization is adopted. Empirically and theoretical analysis are conducted."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem identified is of interest. The paper is in general well written and easy to follow. The proposed algorithm does save communication cost compared with the vanilla random walk Adam algorithm."
                },
                "weaknesses": {
                    "value": "This paper mostly combines standard algorithms (random walk, Adam without momentum, SAM), although this is not a problem, the theoretically analysis needs to be improved. Meanwhile, the experimental part lacks new insights except for some expected results.  \n\nMajor comments:\n\n1.\tTheorem 3.7 looks like a direct combination of theoretical results obtained by Adam without momentum and SAM. Furthermore, the proof in Appendix does not consider the convergence guarantee that could be achieved by random walk method. That is, the Markov chain is not considered. Note that the last equation in Page 13 is almost the same as the convergence result (Theorem 4.3, Triastcyn et al., 2022)) except it does not have the compression part. The proof also follows exactly as Triastcyn et al. (2022). The perturbed model is not used, means that sharp awareness minimization is not analyzed, which makes me question the soundness of Theorem 3.7.\n\n2.\tSince SAM is integrated to prevent potential overfitting, the experiment should present this effect compared with its counterpart that does not have the perturbed model. The lack of this experiment comparison would question the necessity of incorporating SAM in the proposed Localized framework. \n\n3.\tThe simulation only shows the loss performance of the proposed algorithms and the benchmarks, however, in practical, we would be more interested to see the classification accuracy. \n\n4.\tThe proposed algorithm is compared with FedAvg, however, for FedAvg, not all agents are communicating all the time, which does not make sense in the setting that FedAvg does not need to consider communication. That means, I suppose that if all agents in FedAvg communicate all the time, the performance of FedAvg might be much better than all the other methods, since there exists a coordinator, although the communication cost would be very high. The figures presented, however, show that Localized SAM is always better than FedAvg in the random sample setting in both performance and communication, which is not a fair comparison.\n\nMinor comments:\n\n1.\tIn Page 2, first paragraph, Localized SAM is introduced first and then \u201csharpness-aware minimization (SAM (Foret et al., 2021))\u201d is repeated again. It would be better to revise it.\n\n2.\tPage 2, second paragraph in Related Work,  the Walkman algorithm (Mao et al., 2020) is solved by ADMM, with two versions, one is to solve a local optimization problem, the other is to solve a gradient approximation. Therefore, it is not accurate to say that \u201cHowever, these works are all based on the simple SGD for decentralized optimization.\u201d\n\n3.\tSection 3, first paragraph, in \u201cIt can often have faster convergence and better generalization than the SGD-based Algorithm 1, as will be demonstrated empirically in Section 4.1.\u201d The \u201cit\u201d does not have a clear reference. \n\n4.\tIn Section 3.1, you introduced $\\boldsymbol{u}_k$, which was not defined previously and did not show up after Algorithm 3.\n\n5.\tFigure 6 seems to be reused from your previous LoL optimizer work."
                },
                "questions": {
                    "value": "Please see the weakness stated above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3292/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3292/Reviewer_w8Ss",
                        "ICLR.cc/2024/Conference/Submission3292/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887995288,
            "cdate": 1698887995288,
            "tmdate": 1699653152915,
            "mdate": 1699653152915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jR14tcTiVj",
                "forum": "yuYMJQIhEU",
                "replyto": "SauPjQJTRd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses on major comments (Part 1)"
                    },
                    "comment": {
                        "value": "We would like to thank you for your constructive feedback. Here we answer your questions point by point:\n\n> Theorem 3.7 looks like a direct combination of theoretical results obtained by Adam without momentum and SAM. \n\nTheorem 3.7 analyzes the converegence of our proposed method Localized SAM, which is not a direct combination of Adam without momentum (Triastcyn et al., 2022) and SAM. The key difference is to use localized momentum and pre-conditioner for each client, instead of sending these auxiliary parameters to next client along with the model parameter. Using localized momentum and pre-conditioner creates additional difficulty in theoretical analysis, as the momentum and pre-conditioner are not always updated and synchronized. As such, theorem 3.7 cannot be obtained by simply combining the theoretical results from Adam without momentum (Triastcyn et al., 2022) and SAM. Please also see our responses regarding novelty for more discussions.\n\n> Furthermore, the proof in Appendix does not consider the convergence guarantee that could be achieved by random walk method. That is, the Markov chain is not considered. \n\nThere might be some misunderstandings. In the proof in Appendix B, Theorem 3.7 considers Markov chain for the different cases of $t \\mod K = 0$ (we need to change to next client) and $t \\mod K \\ne 0$ (we remain in the same client). The Markov chain influences the final convergence rate by $\\lambda$, which depends on the transition matrix $P$ of Markov chain. We have include a new lemma (Lemma B.1 in Appendix B) in the revised version to clarify how the Markov chain affect the convergence rate from $\\lambda$ and the transition matrix $P$ of Markov chain. \n\n> Note that the last equation in Page 13 is almost the same as the convergence result (Theorem 4.3, Triastcyn et al., 2022)) except it does not have the compression part.\n\nThere is an error in the previous version that the last equation of our proof in Appendix B did not include terms related to sharpness-aware minimization and was inconsistent with the conclusion in the main text. We have corrected this error in our submission and the correct version is:\n\n$\\frac{1}{KT} \\sum_{t=0}^{KT-1} \\| \\nabla L(w_t) \\|^2 \\le O(\\frac{L(w_0) -L^*}{\\eta KT} + \\frac{\\eta \\sigma^2}{K} + \\eta (1-\\frac{1}{K})(\\sigma_l^2+\\varsigma^2+G^2 + \\rho^2) + \\frac{\\lambda \\eta \\sqrt{N}}{(1-\\lambda)KT}).$\n\nOur conclusion contains $\\rho^2$ term (step size of the perturbation step) that is related to sharpness-aware minimization. This is different from Theorem 4.3 in (Triastcyn et al., 2022) which does not include SAM in their optimization steps. \n\n> The proof also follows exactly as Triastcyn et al. (2022). \n\nOur proof for the proposed Localized SAM is different from Triastcyn et al. (2022) in the following important ways:\n- Our proof assumes that each client updates their own pre-conditioner $v^i$ , while the proof in Triastcyn et al. (2022) assumes that the pre-conditioner is synchornized (optionally with compression) across all clients.\n-  Our proof needs to include the perturbed model throughout the proof in Appendix B, while the proof in Triastcyn et al. (2022) does not consider such perturbations.\n\n> The perturbed model is not used, means that sharp awareness minimization is not analyzed, which makes me question the soundness of Theorem 3.7. \n\nThere might be some misunderstandings. Indeed, the perturbed model is used throughout the proof in Appendix B. The use of perturbed model is reflected by the term $E[g_t] \u2212 \\nabla L(w_t)$, which will be zero when the perturbed model in SAM is not used (as $g_t$ will be an unbiased estimator of $\\nabla L(w_t)$). When using the perturbed model, it leads to an additional $\\rho^2$ term that depends on SAM.\n\n> Since SAM is integrated to prevent potential overfitting, the experiment should present this effect compared with its counterpart that does not have the perturbed model. The lack of this experiment comparison would question the necessity of incorporating SAM in the proposed Localized framework. \n\nThere might be some misunderstandings. Indeed, we have conducted ablation study on the choice of different hyper-parameter $\\rho$\u2019s in Figure 8, Section 4.3. Setting $\\rho = 0$ reduces to the case where we do not have perturbation, and its performance is worse than more appropriate settings (e.g., $\\rho = 0.05$)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469759941,
                "cdate": 1700469759941,
                "tmdate": 1700469759941,
                "mdate": 1700469759941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DuL09LKhtX",
                "forum": "yuYMJQIhEU",
                "replyto": "SauPjQJTRd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses on major comments (Part 2)"
                    },
                    "comment": {
                        "value": "> The simulation only shows the loss performance of the proposed algorithms and the benchmarks, however, in\npractical, we would be more interested to see the classification accuracy. \n\nWe have added these results in Appendix A.1, as are also shown here. These results are generally consistent with the loss performances (Figure 2-5 in the main text): SGD and Walkman generally achieve slightly worse final performances than other methods. Among all methods that are based on adaptive optimizers, Localized SAM achieves the best overall performance.\n\n| Graph type         |      | Ring |      |       |3-regular |     |\n|--------------------|------|------|------|------|-------|------|\n| # clients          | 10   | 20   | 50   | 10   | 20        | 50   |\n| SGD                | 82.3 | 81.7 | 81.4 | 83.2 | 82.5      | 81.9 |\n| Adam               | 85.2 | 85.0 | 84.6 |  88.8    | 88.1 | 87.4   |\n| Walkman            | 81.6 | 78.2 | 78.9 | 85.7 | 85.3 | 84.8  |\n| Adam (no momentum) |   85.1 | 84.8 | 85.2 | 88.2 | 87.7 | 87.2   |\n| Gossip             |  85.6 | 85.3 | 84.9 | 88.5 | 88.2 | 87.8  |\n| FedAvg             |  85.4 | 85.5 | 85.0 | 88.3 | **88.6** | 87.6  |\n| Localized SAM      |**85.8**   |  **86.0**  |  **85.4**    |  **89.0**    |  **88.6**    |**88.0**  |\n\n> The proposed algorithm is compared with FedAvg, however, for FedAvg, not all agents are communicating all\nthe time, which does not make sense in the setting that FedAvg does not need to consider communication. \n\nThere might be some misunderstandings. In our experiments, note that only the gossip method require all agents to communicate in\neach iteration. FedAvg only requires some of the agents to communicate with the central server in each iteration, and all\nthe other methods (which are all from random-walk decentralized optimization) only requires one agent to communicate\nin each iteration. The communication cost of FedAvg is analyzed in experiments (Figure 2-5). Its communication cost is\nrelatively small compared to the gossip method or Adam optimizer in random-walk setting, but still higher than Adam (no\nmomentum) (Triastcyn et al. 2022) and our method Localized SAM.\n\n> That means, I suppose that if all agents in FedAvg communicate all the time, the performance of FedAvg might\nbe much better than all the other methods, since there exists a coordinator, although the communication cost would\nbe very high. The figures presented, however, show that Localized SAM is always better than FedAvg in the random\nsample setting in both performance and communication, which is not a fair comparison. \n\nFor FedAvg, we follow the\noriginal setting of FedAvg and only uses some of the agents in each iteration, and the communication cost is computed\nbased on the number of agents. For the proposed Localized SAM (and similarly other methods based on random-walk\ndecentralized optimization), it only activates one client in each iteration, which updates the model using its local training\ndata and sends the updated model to the next agent. Both methods do not require all agents to communicate in each iteration,\nand the comparison is fair across different methods. Moreover, in Figure 2-5, the performance of FedAvg are similar to\nLocalized SAM with respect to the number of weight updates, but has a slightly larger communication cost."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470146496,
                "cdate": 1700470146496,
                "tmdate": 1700470146496,
                "mdate": 1700470146496,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sLmmMSkfj7",
                "forum": "yuYMJQIhEU",
                "replyto": "SauPjQJTRd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses on minor comments"
                    },
                    "comment": {
                        "value": "> In Page 2, first paragraph, Localized SAM is introduced first and then \u201csharpness-aware minimization (SAM\n(Foret et al., 2021))\u201d is repeated again. It would be better to revise it. \n\nThank you for raising this question. We have\nrevised this part and removed undefined acronyms.\n\n> Page 2, second paragraph in Related Work, the Walkman algorithm (Mao et al., 2020) is solved by ADMM, with\ntwo versions, one is to solve a local optimization problem, the other is to solve a gradient approximation. Therefore,\nit is not accurate to say that \u201cHowever, these works are all based on the simple SGD for decentralized optimization.\u201d\n\nThank you for raising this question. We have changed this sentence to \u201cHowever, these works do not consider adaptive\nlearning rates or momentum in decentralized stochastic optimization.\u201d\n\n> Section 3, first paragraph, in \u201cIt can often have faster convergence and better generalization than the SGD-based\nAlgorithm 1, as will be demonstrated empirically in Section 4.1.\u201d The \u201cit\u201d does not have a clear reference. \n\nThank you\nfor mentioning this. We have replaced \u201cit\u201d with a more direct reference \u201cAlgorithm 2\u201d.\n\n> In Section 3.1, you introduced uk , which was not defined previously and did not show up after Algorithm 3.\n\nThank you for mentioning this. We have revised this error in the updated version.\n\n> Figure 6 seems to be reused from your previous LoL optimizer work. \n\nThis figure is different from any previous\npublished works to the best of our knowledge."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470254636,
                "cdate": 1700470254636,
                "tmdate": 1700470254636,
                "mdate": 1700470254636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "junQspttYk",
                "forum": "yuYMJQIhEU",
                "replyto": "sLmmMSkfj7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Reviewer_w8Ss"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Reviewer_w8Ss"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thanks the authors for providing feedback, regarding to\n- Simulation results that include accuracy comparison\n- Explanation on the effect of $\\rho$\n- Revision of Theorem 3.7\n\nThe reviewer still has some additional questions that hopes the authors could clarify/provide:\n1. \"Figure 6 seems to be reused from $\\textbf{your}$ previous LoL optimizer work.\" does not mean that your figure is the same as \"previous published works\". Figure 6(a) and (c) are the same as Figure 5 in the paper that you previously submitted to NeurIPS 2023, except that the x-axis has a different scale. This raises two questions:\n\n1.1 The Local Lookahead (LoL) Optimizer does not have SAM, but why is the loss trend the same as the Localized SAM Optimizer. This is okay if is because you set $\\rho=0$ but I did not see this description in your experiment.\n\n1.2 How is the \"Number of weight updates\" obtained? My understanding is that, in a ring network with 20 nodes, suppose the neural network has weight of $d$, since the Localized SAM allows one node communicates every K iterations, the number of weights would be $d/K$. For Fedavg, according to your setting, every K iterations, 4 nodes would be selected to communicate, therefore the number of weights would be $8d/K$. In that way, the loss vs number of weights figures and the loss vs communication cost figures actually have the similar role to show that your proposed algorithm is more communication-efficient. Again, for Figure 5 of LoL and Figure 6 in this paper, epoch 40 corresponds to weights 8000, is that true for your experiment setting? \n2. Why do you sample 4 agents for communicate for FedAvg rather than letting all agents to communicate? What will be accuracy if you let all agents to communicate? Would it be better than Localized SAM if we do not consider communication efficiency? This is actually my major comment 4.\n\nMinor comments:\n1. In your proof of Theorem 3.7, you miss use $D$ and $N$ for weight dimension and number of nodes. In your definition, they should be $d$ and $n$, respectively. \n2. The y-label of Figure 6(a) and 6(c) should be training loss instead of testing loss."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639587125,
                "cdate": 1700639587125,
                "tmdate": 1700639587125,
                "mdate": 1700639587125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LbvD2tTh3b",
                "forum": "yuYMJQIhEU",
                "replyto": "SauPjQJTRd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further responses"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for acknowledging our feedback on simulation results, explanation on the effect of $\\rho$ as well as revision of Theorem 3.7. For your additional questions, we reply to them point by point:\n\n> \"Figure 6 seems to be reused from your previous LoL optimizer work.\u201d does not mean that your figure is the\nsame as \u201cprevious published works\u201d. Figure 6(a) and (c) are the same as Figure 5 in the paper that you previously\nsubmitted to NeurIPS 2023, except that the x-axis has a different scale. \n\nSince submissions rejected by NeurIPS 2023 are not made public, we could not find the Local Lookahead (LoL) submission, and it seems impossible for us to compare one figure there with our figures.\n\n> The Local Lookahead (LoL) Optimizer does not have SAM, but why is the loss trend the same as the Localized\nSAM Optimizer. This is okay if is because you set $\\rho=0.0$ but I did not see this description in your experiment. \n\nAs mentioned in the 4th paragraph of section 4, $\\rho$ is set to 0.05 for our proposed Localized SAM optimizer unless otherwise specified (this is also highlighted in blue in our updated submission). Regarding your question on the loss trend when setting $\\rho = 0.0$, we suppose the loss trends with the number of clients should indeed be similar with different values of $\\rho$. As is shown in theorem 3.7, more clients (i.e., a larger $n$) always leads to slower convergence, which holds for any values of $\\rho$.\n\n> How is the \u201cNumber of weight updates\u201d obtained? My understanding is that, in a ring network with 20 nodes, suppose the neural network has weight of $d$, since the Localized SAM allows one node communicates $d$ every $K$ iterations, the number of weights would be $\\frac{d}{K}$. For Fedavg, according to your setting, every $K$ iterations, 4 nodes would be selected to communicate, therefore the number of weights would be $\\frac{8d}{K}$. \n\nThere might be some misunderstanding. As mentioned in the last paragraph (highlighted in blue) of section 4 of this revised version, \u201cnumber of weight updates\u201d means how many times each client has updated its model parameters, not the number of model parameters. \n\nThe reviewer might be thinking that for localized SAM, we send $d$ parameters to the next client after performing $K$ times of $d$ weight updates, and so the per-iteration communication cost is $\\frac{d}{K}$. Similarly, for FedAvg, the reviewer might be thinking that each of the 4 clients needs to communicate twice with the central server (receiving and sending model parameters) and performs $K$ times of weight updates, and so the per-iteration communication cost is $\\frac{2 \\times 4 d}{K} = \\frac{8d}{K}$. However, indeed, in each iteration of the proposed algorithm, one client is active and it performs $K$ local updates, and then sends its (updated) model parameter to the next client (at the end of this iteration). Take the proposed method Localized SAM (Algorithm 3) as an example, the number of iterations is $T$ and the total number of weight updates is $KT$, and it only performs $T$ rounds of communication. Hence, the interpretations of \"iteration\" and \"weight updates\" are different from that of the reviewer\u2019s.\n\nIn our experiments, since all baseline methods use the same model, we consider the *relative* communication cost, which\ncounts each sending of the model parameter (i.e., the whole model) as 1. Hence, as shown in Table 1 in our submission, the relative per-iteration communication cost of Localized SAM (resp. FedAvg) is $\\frac{d}{K}$(resp. $\\frac{8d}{K}$).\n\n> In that way, the loss vs number of weights figures and the loss vs communication cost figures actually have\nthe similar role to show that your proposed algorithm is more communication-efficient. \n\nAs explained in the response above, there is some misunderstanding. First, note that the figures show the loss vs the number of **weight updates**, not with the number of **weights**. Also, as explained above, each client performs $K$ weights updates before one round of communication. Hence the number of weight updates is very different from the communication cost, and that explains why the two sets of figures are so different.\n\n> Again, for Figure 5 of LoL and Figure 6 in this paper, epoch 40 corresponds to weights 8000, is that true\nfor your experiment setting? \n\nSince submissions rejected by NeurIPS 2023 are not made public, we could not comment on the figure there. Also, we do not use \"epoch\" in this submission, as its definition may be ambiguous in decentralized optimization, and it is also not used in previous works (such as (Sun et al. 2022; Triastcyn et al. 2022; Mao et al. 2020; McMahan et al. 2017))."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727691445,
                "cdate": 1700727691445,
                "tmdate": 1700728033567,
                "mdate": 1700728033567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y6VdodBwp8",
            "forum": "yuYMJQIhEU",
            "replyto": "yuYMJQIhEU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3292/Reviewer_zbhE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3292/Reviewer_zbhE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a communication-efficient random walk optimization algorithm for decentralized learning. The key ideas are: 1) Eliminate communication of auxiliary parameters like momentum and preconditioner in adaptive optimizers like Adam; 2) Perform multiple local model updates before communication to reduce communication frequency; 3) Incorporate sharpness-aware minimization to avoid overfitting during multiple local updates. Theoretically, it is shown that the proposed method achieves the same convergence rate as existing methods but with lower communication cost. Experiments on CIFAR-10 and CIFAR-100 demonstrate that the proposed method achieves comparable performance to Adam-based methods but with much lower communication cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The method effectively reduces communication cost in decentralized learning without hurting convergence, which is important for bandwidth-limited decentralized applications.\n* Convergence analysis follows standard assumptions and provides insights into how the hyperparameters affect convergence.\n* Comprehensive experiments compare with reasonable baselines, evaluate different network structures and sizes, and study sensitivity to key hyperparameters.\n* The paper is clearly motivated, easy to follow and technically sound overall. Figures are informative."
                },
                "weaknesses": {
                    "value": "* Each component (e.g. local updates, SAM) has been studied before in different contexts. The novelty lies more in the combination.\n* Missing References\n  * \"Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent\"\n  * \"Asynchronous decentralized parallel stochastic gradient descent\" where multiple local model updates is used in asynchronous decentralized training"
                },
                "questions": {
                    "value": "* Have you experimented with more complex network topologies beyond ring and expander graphs? There is a \"chord network\" topology in \"Asynchronous decentralized parallel stochastic gradient descent\" that may be beneficial.\n* Does the algorithm work with dynamic network topology where each iteration has a different communication topology?\n* Is it possible to combine your approach with compression of model parameters for further communication reduction without making the convergence rate worse?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698916783456,
            "cdate": 1698916783456,
            "tmdate": 1699636277942,
            "mdate": 1699636277942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "91JZ1GQMID",
                "forum": "yuYMJQIhEU",
                "replyto": "Y6VdodBwp8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses"
                    },
                    "comment": {
                        "value": "We would like to thank you for your constructive feedback. Here we answer your questions point by point:\n\n> Each component (e.g. local updates, SAM) has been studied before in different contexts. The novelty lies more in\nthe combination. \n\nWhile we agree that local updates or SAM has been studied before in different contexts, we would like\nto emphasize that a key component of our method is to use localized momentum and pre-conditioner, which has not been\nexplored in previous works (Sun et al. 2022; Triastcyn et al. 2022) on random-walk decentralized optimization. Please also\nsee our responses regarding novelty where we have listed our contributions in points.\n\n> Missing References\n\nThank you for mentioning these works. We have added these related works into our submission.\n\n> Have you experimented with more complex network topologies beyond ring and expander graphs? There is\na \u201dchord network\u201d topology in \u201dAsynchronous decentralized parallel stochastic gradient descent\u201d that may be\nbeneficial. \n\nThank you for your suggestion. As suggested, we have added experiments on the chord network with 20 clients.\nResults are shown in Figure 10 in Appendix A.2, and are generally similar to the ring graph. In general, methods based\non adaptive optimizers outperform SGD or Walkman, regardless of the type of communication (centralized, random-walk,\nor gossip). The gossip method has a much higher communication cost than both centralized and random-walk methods.\nWalkman achieves slightly better performance than SGD with the same communication cost, but its performance is worse\nthan that of adaptive optimizers, especially for the more difficult CIFAR-100 data set. Directly using Adam in random-walk\noptimization leads to higher communication cost than the other methods, and using Adam without momentum reduces\ncommunication cost while maintaining good performance. The proposed Localized SAM optimizer achieves similar\nconvergence rate as other methods based on adaptive optimizers, while its communication cost is significantly smaller than\nall other methods.\n\n> Does the algorithm work with dynamic network topology where each iteration has a different communication\ntopology?\n\nThank you for raising this interesting question. Our algorithm can be directly combined with dynamic network\ntopology, as it only needs local neighborhood information in each iteration. Theoretical analysis may require a little more\nwork as we may need to assume the communication topology does not drastically change. We have also added some\ndiscussions on this extension in Appendix C.\n\n> Is it possible to combine your approach with compression of model parameters for further communication\nreduction without making the convergence rate worse?\n\nThank you for this question. It should be straight-forward to\ncombine our method with model compression for further communication reduction, and the convergence rate should also\nmatch methods that also employ model/gradient compression (Lin et al. 2018; Vogels et al. 2019). We have also added\nsome discussions on this extension in Appendix C.\n\nReferences:\n\nLin et al. Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. ICLR 2018.\n\nVogels et al. PowerSGD: Practical low-rank gradient compression for distributed optimization. NeurIPS 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469135254,
                "cdate": 1700469135254,
                "tmdate": 1700469135254,
                "mdate": 1700469135254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "64E9R7uvCM",
            "forum": "yuYMJQIhEU",
            "replyto": "yuYMJQIhEU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3292/Reviewer_bmmj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3292/Reviewer_bmmj"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript aims to improve the random-walk decentralized optimization by incorporating the idea of local SGD, keeping local optimizer states, and adding SAM optimization. Empirical results justify the effectiveness of the proposal."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* In general this manuscript is well-structured and the problem is well-motivated.\n* Empirical results on various communication topologies with multiple decentralized optimizers are provided, in terms of computation cost and (relative) communication cost. Some ablation studies are also provided."
                },
                "weaknesses": {
                    "value": "1. Limited novelty. The idea of using local SGD with local optimizer states is quite standard in the field of federated learning and distributed training. The same comment could be applied to include SAM for FL. See e.g., [1, 2] and the related work section of [1].\n2. Limited theoretical contribution. The convergence analysis only considers the single worker case, and it cannot reflect the convergence of localized SAM for arbitrary decentralized communication topologies. Besides, the statement of theorem 3.7 should be more formal.\n3. Limited experimental evaluations. \n    * Some advanced decentralized communication topologies were not considered for the evaluation, e.g., the exponential graph [3, 4, 5]. Besides, the considered relative communication cost could be questionable, as it counts each sending of the model parameter as 1, instead of taking the network bandwidth / wallclock-time into account.\n    * The hyper-parameter choice should be justified.\n    * A carefully controlled evaluation should be provided. The considered baseline methods cannot guarantee a fair evaluation, as the community has a line of research on improving communication efficiency. It looks strange to directly compare an improved random walk decentralized optimizer with the other standard-form distributed optimizer. \n  \n### Reference\n1. Improving the Model Consistency of Decentralized Federated Learning, ICML 2023. http://arxiv.org/abs/2302.04083\n2. Improving generalization in federated learning by seeking flat minima, ECCV 2022. https://arxiv.org/abs/2203.11834\n3. Exponential Graph is Provably Efficient for Decentralized Deep Training, NeurIPS 2021. https://arxiv.org/abs/2110.13363\n4. Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence, NeurIPS 2023. https://arxiv.org/abs/2305.11420\n5. Stochastic Gradient Push for Distributed Deep Learning, ICML 2019. https://arxiv.org/abs/1811.10792"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698944173135,
            "cdate": 1698944173135,
            "tmdate": 1699636277860,
            "mdate": 1699636277860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cl7Y9opGYy",
                "forum": "yuYMJQIhEU",
                "replyto": "64E9R7uvCM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses on novelty and theoretical contribution"
                    },
                    "comment": {
                        "value": "We would like to thank you for your constructive feedback. Here we answer your questions point by point:\n\n### Limited novelty. \n\n> The idea of using local SGD with local optimizer states is quite standard in the field of\nfederated learning and distributed training. \n\nWe agree that using local SGD with local optimizer states have been used\nin federated learning and distributed training (McMahan et al. 2017; Reddi et al. 2021). Indeed, we also mentioned this\nin Section 3.1. However, note that they all require a central server to aggregate updates from local optimizers, and the\noptimizer states are stored in the central server, not clients. On the contrary, our focus is on extending local SGD and local\noptimizer to random-walk decentralized optimization, which does not need any central server, and this has not been explored\nin the random-walk decentralized optimization literature (such as (Sun et al. 2022; Triastcyn et al. 2022)). In this paper, we\ndemonstrate that the communication of optimizer states can be removed without performance degradation.\n\nReference: Reddi et al. Adaptive federated optimization. ICLR 2021.\n\n> The same comment could be applied to include SAM for FL. See e.g., [1, 2] and the related work section of [1].:\n\nAlthough they all use SAM in distributed learning, these works focus on different types of optimization algorithms. [1]\nis based on gossip decentralized algorithms, where all clients must always be active during the training process, and [2]\nconsiders centralized federated learning, i.e., a central server can coordinate the whole learning process. Moreover, most of\nrelated works discussed in [1] either focus on centralized federated learning (Qu et al., 2022; Sun et al., 2023) or gossip\ndecentralized algorithms (Sun et al. 2022; Dai et al. 2022), which are all different from our method. Our method focuses on\nrandom-walk decentralized optimization that does not need a central server and only require one client to be active in each\niteration. As such, it has much weaker requirements on the communication network. Please also see our general responses\non novelty part.\n\nReferences:\n\nQu et al. Generalized federated learning via sharpness aware minimization. ICML 2022.\n\nSun et al. Decentralized federated averaging. IEEE TPAMI 2022.\n\nDai et al. DisPFL: Towards communication-efficient personalized federated learning via decentralized sparse training.\nICML, 2022.\n\nSun et al. Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency\nand Smooth Landscape. ICML 2023.\n\n### Limited theoretical contribution. \n\n> The convergence analysis only considers the single worker case \n\nThere might be some misunderstandings. Note that in random-walk decentralized optimization algorithms (Mao et al. 2020; Sun et al.\n2022; Triastcyn et al. 2022), only one single worker is activated and updated in each iteration. Hence, we only consider one\nsingle worker in the convergence analysis.\n\n> and it cannot reflect the convergence of localized SAM for arbitrary decentralized communication topologies.\n\nThere might be some misunderstanding, possibly because some notations have been abused in our submission. Note that in\nSection 3.2, we used $\\lambda$ to denote the compression ratio in (Triastcyn et al. 2022). In theorem 3.7, we used $\\lambda$ again, but to\ndenote a particular eigenvalue (detailed definition can be found below) of the network\u2019s probability transition matrix $P$. Hence, the convergence rate in theorem 3.7\ndoes depend on the network via $\\lambda$. We corrected this and revised the definition of $\\lambda$ in theorem 3.7.\n\nDefinition of $\\lambda$: Denote the eigenvalues of the transition matrix $P$ as \n$\\lbrace \\lambda_i \\rbrace_{i=1}^n$, \nwhich satisfy $1 = \\lambda_1 > \\lambda_2 > \\dots > \\lambda_n > -1$. \n$\\lambda$ is defined as $\\lambda = \\max(\\lambda_2, |\\lambda_n|)$. \n\n> Besides, the statement of theorem 3.7 should be more formal.\n\nAs suggested, we have revised the statement of Theorem 3.7 \nto include more detailed explanations on notations and pre-requisite of this\ntheorem. \nThe revised version is as follows:\n\n**Theorem 1.** Under Assumption 3.1 to 3.6, $w_t$'s generated from Algorithm 3 satisfy that: \n\n$\\frac{1}{KT} \\sum_{t=0}^{KT-1} || \\nabla L(w_t) ||^2  \\le O(\\frac{L(w_0) - L^*}{\\eta KT} + \\frac{\\eta \\sigma^2}{K} + \\eta\n(1-\\frac{1}{K}) (\\sigma_l^2+\\varsigma^2+G^2+\\rho^2) +  \\frac{\\lambda \\eta \\sqrt{n}}{(1-\\lambda)KT})$\n\nwhere we denote the eigenvalues $\\lbrace \\lambda_i \\rbrace_{i=1}^n$ of transition matrix $P$ as $1 = \\lambda_1 > \\lambda_2 > \\dots > \\lambda_n > -1$ and $\\lambda = \\max(\\lambda_2, |\\lambda_n|)$."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468829057,
                "cdate": 1700468829057,
                "tmdate": 1700468829057,
                "mdate": 1700468829057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fny55JwNfI",
                "forum": "yuYMJQIhEU",
                "replyto": "64E9R7uvCM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses on experimental evaluations"
                    },
                    "comment": {
                        "value": "> Some advanced decentralized communication topologies were not considered for the evaluation, e.g., the\nexponential graph [3, 4, 5]. \n\nThank you for your comment. As suggested, we have added experiments on exponential\ngraphs with 20 clients. Results are shown in Figure 9 in the revised Appendix A.2, and are generally similar to ring or\n3-regular expander graphs. In general, methods based on adaptive optimizers outperform SGD or Walkman, regardless of\nthe type of communication (centralized, random-walk, or gossip). The gossip method has a much higher communication\ncost than both centralized and random-walk methods. Walkman achieves slightly better performance than SGD with the\nsame communication cost, but its performance is worse than that of adaptive optimizers, especially for the more difficult\nCIFAR-100 data set. Directly using Adam in random-walk optimization leads to higher communication cost than the other\nmethods, and using Adam without momentum reduces communication cost while maintaining good performance. The\nproposed Localized SAM optimizer achieves similar convergence rate as other methods based on adaptive optimizers, while\nits communication cost is significantly smaller than all other methods.\n\n> Besides, the considered relative communication cost could be questionable, as it counts each sending of the\nmodel parameter as 1, instead of taking the network bandwidth / wallclock-time into account. \n\nCounting the total\namount of transmitted data are common practice for measuring communication cost in literature (Sun et al. 2022; Triastcyn\net al. 2022; Wang et al. 2022; Wu et al. 2022; Gao et al. 2021). As for the network bandwidth or wallclock time, note that\nthe wallclock time depends on the network bandwith as well as the amount of data sent in each iteration. The network\nbandwidth should be assumed same across different algorithms for fair comparison, hence the wallclock time solely depends\non the amount of data sent in each iteration, as is measured by the relative communication cost.\n\nReferences:\n\nWang et al. ProgFed: Effective, Communication, and Computation Efficient Federated Learning by Progressive Training.\nICML 2022.\n\nWu et al. SmartIdx: Reducing Communication Cost in Federated Learning by Exploiting the CNNs Structures. AAAI 2022.\n\nGao et al. On the Convergence of Communication-Efficient Local SGD for Federated Learning. AAAI 2021\n\n> The hyper-parameter choice should be justified. \n\nThe hyper-parameters in our proposed method can be divided into\ntwo parts: (i) those that are specific to our method ($K, \\rho$), and (ii) those that also exist for other methods ($\\eta, \\delta$). For $K$ and $\\rho$, their values are chosen by a validation set and we also included studies on their sensitivity in Section 4.3 (Figure 7 for\n$K$ and Figure 8 for $\\rho$). For $\\eta$ and $\\delta$, their choices follow previous works on decentralized optimization (Sun et al. 2022;\nTriastcyn et al. 2022). We have also included these justifications in the revised version.\n\n> A carefully controlled evaluation should be provided. The considered baseline methods cannot guarantee a\nfair evaluation, as the community has a line of research on improving communication efficiency. It looks strange\nto directly compare an improved random walk decentralized optimizer with the other standard-form distributed\noptimizer. \n\nOther techniques that improves communication efficiency include model quantization (Alistarh et al. 2017;\nNadiradze et al. 2021), top-K sparsification (Wang et al. 2023) and gradient compression (Lin et al. 2018; Vogels et\nal. 2019). Note that our work focuses on reducing the communication cost by removing the communication of auxiliary\nparameters in adaptive optimizers. Our experiments verify that removing these auxiliary parameters does not harm the final\nperformance, while significantly reducing the communication cost. Thus, these existing techniques are orthogonal to the\nproposed method and can be straightforward combined. We have also added some discussions on potential future works in\nAppendix C that combine the proposed method with other techniques.\n\nReferences:\n\nAlistarh et al. QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding. NIPS 2017\n\nNadiradze et al. Asynchronous Decentralized SGD with Quantized and Local Updates. NeurIPS 2021\n\nWang et al. CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks. ICML 2023\n\nLin et al. Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. ICLR 2018\n\nVogels et al. PowerSGD: Practical low-rank gradient compression for distributed optimization. NeurIPS 2019"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469019084,
                "cdate": 1700469019084,
                "tmdate": 1700469019084,
                "mdate": 1700469019084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yh2gfyqiQ1",
                "forum": "yuYMJQIhEU",
                "replyto": "64E9R7uvCM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Reviewer_bmmj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Reviewer_bmmj"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' feedback"
                    },
                    "comment": {
                        "value": "The reviewer thanks the authors for providing feedback, regarding to \n* revise the manuscript and explain the meaning of $\\lambda$ in their Theorem 3.7.\n\nThe reviewer still has some additional questions that he/she hopes the authors could clarify/provide:\n* The technical difficulty of adding well-known SAM and local SGD to the random-walk decentralized learning algorithms, either empirically or theoretically.\n* Measure the communication cost in terms of average outward edges in the decentralized system, rather than the total number of communication edges in the whole system, as these communications could happen simultaneously. The latter one (used in the current manuscript) is a bit unfair and could naturally bias the random-walk decentralized learning algorithm. The reviewer is curious about the comparison between localized SAM and D-SGDm on the exponential graph.\n* It is a bit unclear which exponential graph is used in Figure 9, as Takezawa et al 2023 is still different from that of Ying et al 2019. More clarification should be provided, together with the accuracy results, rather than the loss results.\n* The reviewer is also curious about one crucial ablation study: what if we just straightforwardly apply SAM to all other baselines (especially the SOTA method, namely DSDGm + exponential graph), and compare the corresponding results with Localized SAM?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483737594,
                "cdate": 1700483737594,
                "tmdate": 1700483814726,
                "mdate": 1700483814726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PFchGa1vJ5",
                "forum": "yuYMJQIhEU",
                "replyto": "64E9R7uvCM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further responses"
                    },
                    "comment": {
                        "value": "Thank you for acknowledging our responses. Here we provide responses for your further questions point by point:\n\n> The technical difficulty of adding well-known SAM and local SGD to the random-walk decentralized learning\nalgorithms, either empirically or theoretically. \n\nIntroducing SAM to random-walk decentralized learning algorithms\ncreates additional difficulty in theoretical analysis, as the model update $g_t$ is no longer an unbiased estimator of gradient\n$\\nabla L(w_t)$. As such, we need to upper-bound its difference $E[g_t] - \\nabla L(w_t)$, which leads to an additional $\\rho^1$ term that depends on SAM in the final results in Theorem 3.7. We would also like to emphasize that our method is not simply adding well-known SAM and local SGD to random-walk decentralized learning algorithms. A key difference is to use localized\nmomentum and pre-conditioner for each client, instead of sending these auxiliary parameters to next client along with the\nmodel parameter. Using localized momentum and pre-conditioner creates additional difficulty in theoretical analysis as well,\nas the momentum and pre-conditioner are not always updated and synchronized. Please also see our responses regarding\nnovelty as well as our responses to reviewer w8Ss for discussions on this point.\n\n> Measure the communication cost in terms of average outward edges in the decentralized system, rather than the\ntotal number of communication edges in the whole system, as these communications could happen simultaneously.\nThe latter one (used in the current manuscript) is a bit unfair and could naturally bias the random-walk decentralized\nlearning algorithm. The reviewer is curious about the comparison between localized SAM and D-SGDm on the\nexponential graph. \n\nThank you for your suggestion. We have added such additional results for exponential graph in Appendix A.1. Under this metric, the relative per-iteration communication cost for D-SGDm on the exponential graph will be $\\lfloor \\log(20) \\rfloor +1 = 5$, which is still higher than that of most random-walk decentralized learning algorithms shown in Table 1. And from the newly-added Figure 12, D-SGDm (referred as gossip method) still has higher communication cost than random-walk decentralized learning algorithms.\n\n> It is a bit unclear which exponential graph is used in Figure 9, as Takezawa et al 2023 is still different from that\nof Ying et al 2019. More clarification should be provided \n\nWe have mentioned that we use the static exponential graph defined in (Ying et al. 2021). For easier reference, we have also plotted static exponential graph with 20 clients in the revised Figure 9(a).\n\n> together with the accuracy results, rather than the loss results. \n\nThe accuracy results are added to Table 2 in Appendix A.1 in the revised version. These results are generally consistent with the loss performances (Figure 2-5 in the main text) as well as other types of graphs (ring, 3-regular or chord). SGD and Walkman generally achieve slightly worse final performances than other methods. Among all methods that are based on adaptive optimizers, Localized SAM achieves the best overall performance.\n\n> The reviewer is also curious about one crucial ablation study: what if we just straightforwardly apply SAM to all\nother baselines (especially the SOTA method, namely DSDGm + exponential graph), and compare the corresponding\nresults with Localized SAM? \n\nAs suggested, we conducted some new experiments to apply SAM to DSGDm on the exponential graph. This new baseline (refered to as DSGDm+SAM) is added to our newly revised Figure 10 and 12 in Appendix A.2, and its accuracy on exponential graph with 20 clients is added to Table 2 in Appendix A.1 (also shown below). We can see that DSGDm+SAM has a better performance than Gossip/DSGDm, and also matches the performance of Localized SAM. However, the communication cost of DSGDm+SAM is the same as DSGDm, and is still much higher due to its gossip-type communication, as can be seen from Figures 10 and 12 in Appendix A.2.\n\n| Method         | Accuracy |\n|----------------|----------|\n| Gossip (DSGDm) | 88.5     |\n| DSGDm+SAM      | **88.7**     |\n| Localized SAM  | **88.7**     |\n\nWe are happy to address any further questions, and we will be more than grateful if you could kindly consider raising your score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618698653,
                "cdate": 1700618698653,
                "tmdate": 1700721394309,
                "mdate": 1700721394309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rQOZyTVrmP",
                "forum": "yuYMJQIhEU",
                "replyto": "PFchGa1vJ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Reviewer_bmmj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Reviewer_bmmj"
                ],
                "content": {
                    "title": {
                        "value": "additional questions"
                    },
                    "comment": {
                        "value": "The reviewer thanks the authors for providing the followup results. The reviewer has one followup question:\n* it is possible to consider the 1-peer time-varying exponential graph discussed in Ying et al 2021 or Base-2 graph (1) introduced in Takezawa et al 2023, instead of the communication-intensive static exponential graph? this type of communication topology can significantly reduce the communication cost while ensuring a similar performance as a static exponential graph."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632787715,
                "cdate": 1700632787715,
                "tmdate": 1700632787715,
                "mdate": 1700632787715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oeusKmPhUh",
                "forum": "yuYMJQIhEU",
                "replyto": "64E9R7uvCM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the additional question"
                    },
                    "comment": {
                        "value": "Thank you for your suggestion. We use the static graph to ensure a fair comparison between random-walk and gossip-type algorithms. While using these dynamic graphs might reduce the communication cost of gossip-type algorithms, in practice the communication graph is often defined by the application scenario and remains static during model training, and so we focus on the case of static graphs for different methods. Please also see our responses to reviewer zbhE who has raised a similar question on how random-walk algorithms can be applied to dynamic graphs, and some discussions on this possible future direction can be found in Appendix C.\n\nWe are happy to address any further questions, and we will be more than grateful if you could kindly consider raising your score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721342654,
                "cdate": 1700721342654,
                "tmdate": 1700721354957,
                "mdate": 1700721354957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]