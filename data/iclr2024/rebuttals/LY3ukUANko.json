[
    {
        "title": "On input-dependence and recall in convolutional language models"
    },
    {
        "review": {
            "id": "tffThhYAox",
            "forum": "LY3ukUANko",
            "replyto": "LY3ukUANko",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6370/Reviewer_5EW4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6370/Reviewer_5EW4"
            ],
            "content": {
                "summary": {
                    "value": "The authors systematically study the impact of neural architecture on language modeling performance. The authors identify a persisting quality gap between convolution and attention networks. Specifically, they identify a single failure mode, i.e., multi-query associative recall (MQAR), and demonstrate that convolution networks fall short in this. To verify the impact of this gap, the authors conduct systematic studies and provide both empirical and theoretical evidence. Moreover, the authors present strategies for migrating this gap."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The studied problem is important and may have a big impact. The pinpointed failure model (i.e., associative recall) is novel and reasonable. Both empirical and theoretical studies are conducted to support the argument. To further demonstrate the impact of the analyses, the authors examine two alternative strategies, which support the intuition of the author."
                },
                "weaknesses": {
                    "value": "The proposed attention hybrid method seems to perform well in the experiment. However, it is not clear how it would perform on a larger scale."
                },
                "questions": {
                    "value": "As to the training stability and the sensitivity to the training hyper-parameters, I'm wondering how the proposed look-up method and the hybrid method compare with the attention network."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794421502,
            "cdate": 1698794421502,
            "tmdate": 1699636704083,
            "mdate": 1699636704083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RdtO7gfb3W",
                "forum": "LY3ukUANko",
                "replyto": "tffThhYAox",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5EW4"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review! We appreciated that you found the work novel and insightful, and the problem impactful as gated convolution sequence models are receiving increasing adoption in the ML community. Here we address your questions.\u00a0\n\n\n**Presentation**\n\nWe take note the presentation score of 3 and have taken several steps to improve the overall delivery of our work. We hope these changes are helpful! The changes include:\u00a0\n\n1. Section 3 and Appendix C.1: We update the sections to describe how associative recall quality is measured in the real language modeling data. We also clarify the definitions and add simple examples of prior vs. our proposed recall problem.\u00a0\n2. Theory: We thoroughly checked for consistency with respect to notation and further clarified the assumptions underlying our theoretical results.\n3. Section 5.1: We provide clarification of the input-dependent architectures we evaluate.\n4. Appendix A: We include a new detailed related works section that discusses the history of associative recall in machine learning and an extensive list of previously proposed architectures to better contextualize our work.\n5. Appendix B: We provide a simple PyTorch implementation of the BaseConv architecture to complement the theoretical statements.\u00a0\n6. Appendix E: We provide a new section and algorithm box to document exactly how the MQAR synthetic data is constructed.\u00a0We also provide more details on our setup for the synthetic experiments. \n\nPlease let us know if you have any additional suggestions on how we can improve the presentation and thank you for your help!\u00a0\n\n**Scaling architectures.**\u00a0\n\nThank you for your question on scaling performance! In response, we have conducted three new experiments to better understand the scaling trends. We kindly direct you to the discussion of these experiments that is included in the main response to all reviewers as well as Appendix G of the revised submission.\u00a0\n\nIn these experiments, we show that simply scaling the gated convolution architectures does not solve the MQAR problem. We scale up to 7B parameter models in our new experiments. We hope these experiments build confidence that hybrid solutions continue to help at scale since attention layers can solve MQAR, complementing the gated-convolution backbone which struggles to solve MQAR efficiently.\n\n**Sensitivity to hyperparameters.**\u00a0\n\nYou raised the important question of how the training stability and sensitivity of the selective attention methods compare to the baseline attention network. We do not apply any hyperparameter tuning (e.g., LR, Optimizer, Batch Sizes, etc.) when producing these results. We simply use the backbone gated convolution models with hyperparameters provided by the prior work and replace a few layers with the hybrid layer. Overall, training is stable and the hybrid models work even without tuning!\u00a0\n\nFurther, to back up these results we are including our training code in the revised submission. We will also release the pretrained checkpoints upon publication. Please let us know if you have further questions about this!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291675339,
                "cdate": 1700291675339,
                "tmdate": 1700343526982,
                "mdate": 1700343526982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "luL4VyJwP5",
            "forum": "LY3ukUANko",
            "replyto": "LY3ukUANko",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6370/Reviewer_xdZW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6370/Reviewer_xdZW"
            ],
            "content": {
                "summary": {
                    "value": "This paper demonstrates that convolutional LMs are worse at bigram associative recall (AR) than transformers on real data by demonstrating that the test AR perplexity is sensitive to the number of occurences of the given bigrams in the training set. This possibly suggests that convolutional LMs are worse at in-context learning than previously thought in other works that test on synthetic tasks. This work also proposes a new synthetic task called a multiquery associative recall (MQAR), and uses new theoretical insights to devise a simple convolution-only alternative competitive to attention on AR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Using the bigram frequency and the test perplexity of real data was insightful."
                },
                "weaknesses": {
                    "value": "There's a lot of typos and confusing writing. It's hard to properly understand all the theoretical claims of the work. For starters, Proposition 4.3 should say `u \\in \\{ 0, 1 \\}^{N \\times \\log c}`. The description of definition 3.1 could be improved; a simple example would be quite helpful. In appendix, `N` sometimes means the entire sequence length or the number of triplets in MQAR; this confusion is exacerbated by the fact that the meaning changes in the same theorem/proof.\n\nPage 6 states \"[e]ach BaseConv layer uses O(Nd) parameters and ... O(Nd) operations\". I believe it uses `O(Nd +d^2)` parameters and `O(d N log N + Nd^2)` FLOPs? This makes me believe that the rest of the theoretical results may have to be carefully revisited by the authors.\n\nPage 26. Proof of C.19. It's unclear how `Q[i, :]` can be set to a zero vector when `i \\notin Q` (also boldface 0 suffices to express a vector; no need for a superscript d; also, d is defined as `log(C)` in the same page but `C` is a set. `d = log(c)` small c since `c := |C|`.) and similarly for `K` and `V`, because QKV is actually a linear projection of the input. Linear projection can not implement this non-linear operation of masking some of the activations out. There's a sentence that reads `... where Q, K, V ... are positional embeddings ...`. They are different projections of u.\n\nThe calculation of the percentage of gap due to AR hits could be better motivated and justified in appendix.\n\nOther minor typos:\n - `u * k = FFT^{-1} (FFT(u) FFT(k))` (should drop the convolution operation in frequency domain).\n - Page 5: Other tokens: ... `1,000` => `1250`."
                },
                "questions": {
                    "value": "(wrote under weaknesses)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6370/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6370/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6370/Reviewer_xdZW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698889440096,
            "cdate": 1698889440096,
            "tmdate": 1699636703924,
            "mdate": 1699636703924,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kRvkMADJgu",
                "forum": "LY3ukUANko",
                "replyto": "luL4VyJwP5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xdZW"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review! The feedback led to significant improvements in our work. We appreciate that you found the downstream analysis insightful and here we address your questions.\n\n**Theoretical corrections, clarifications, and updates**\n\nThank you for raising questions about the theoretical pieces. We apologize for the confusion from our original submission and have thoroughly examined the revised submission.\u00a0\n\n_Page 26. Proof of C.19. QKV matrices and dimensions. \u2013 Now page 47, Appendix H.7._\n\nWe have added clarifications on how we assumed the input u to be encoded for the proof (we have made this explicit as well as added some justification on why the encoding assumption is reasonable). The proof is updated where Q, K and V are linear projections. While fixing the argument we realized that we needed d=3c so we have made that correction as well. \n\n_Parameters and operations for BaseConv layers._\n\nWe had proven that we only need $\\tilde{O}(d)$ parameters for the projections to achieve the tight general arithmetic circuit result in Appendix C.4 of our original submission (now Appendix H.5), where the projections use Kaleidoscope matrices for all architectures \\[Dao et al., 2020]. In all experiments, the linear projections are dense, but for all the theory results in App H, the linear projections use the restricted poly-log Kaleidoscope matrices. Doing so is what gave the claimed O(Nd) parameter count and runtime on Page 6.\u00a0\n\nWe did not specify this clearly in the main paper and update this in our revision (page 6). We also included an explicit proposition in the appendix (Proposition H.6) that argues the claim on a single BaseConv layer. We also have added explicit statements in the proofs to show that using the near linear parameters Kaleidoscope matrices are sufficient for our proof.\n\n_Clarification on the meaning of N._ We had used N to represent both the sequence length and the number of (key, value, query) tuples. We have updated the number of tuples to read N/3 in the revised copy. We apologize for the confusing notation and thank you for pointing this out.\n\n_Shape of inputs in Proposition 4.3._ As noted by the review, we have corrected proposition 4.3 to read $u \\in \\\\{ 0, 1 \\\\}^{N \\times 3c}$_._\u00a0\n\nBeyond the comments in the review, we thoroughly checked for consistency with respect to notation and further clarified the assumptions underlying our theoretical results.\n\n**Clarifying Definition 3.1 and Descriptions of MQAR**\n\nWe have updated the manuscript in the following ways to address this feedback:\n\n1. We streamlined definition 3.1 and added a simple example per the reviewer suggestions. We also include a simple example of how AR is formulated in prior work for clear comparison.\u00a0\n2. We add Appendix D containing several examples of MQAR that we sourced from real language modeling data.\n3. We add Appendix E (Algorithm 1) detailing exactly how MQAR synthetic data gets constructed.\u00a0\u00a0\n\nWe hope these collectively clarify the MQAR contribution.\n\n**Explaining the procedure for measuring the AR gap**\n\nIn response to the review, we add Appendix C.1. to justify our method for computing the amount of quality gap between the convolution and attention models that is ascribed to associative recall capability (e.g., in Table 1):\n\n1. Given an input sequence, we identify recurring bigrams (i.e. bigrams that have already appeared in the sequence at a prior position). Since bigrams that appear frequently during training may be memorized by the model, rather than requiring the model to perform recall at inference-time, we only measure AR log-probabilities with respect to bigrams that are seen fewer than a threshold number of times during training. The threshold used in all the experiments in our submission is $1,250$ training occurrences in the 10B tokens of pretraining data.\n2. We measure the log-probability assigned to the true bigram completion. This bigram completion is referred to as an AR Hit in our work. This protocol assumes that the model can produce the completion by recalling the prior occurrence of the bigram in the sequence.\u00a0\n3. For the model being evaluated $m$, and the attention model $M$, we measure the \\% of the quality gap between $m$ and $M$ ascribed to associative recall capability as follows.\u00a0\n4. Let the average log-probability for all AR Hits across validation sequences be $l\\_{H}^m$ and $l\\_{H}^M$ for $m$ and $M$ respectively. Let the average log-probabilities of *all tokens* in the validation sequences be $l^m$ and $l^M$ respectively. Let $p\\_{H}$ be the proportion of AR Hit tokens in the validation data. As the final gap ascribed to AR, we report:\n\n$$\\min (\\frac{(l\\_{H}^m - l\\_{H}^M) p\\_{H}}{l^m - l^M}, 1.0)$$\n\nShown above, if $m$ is better than attention ($M$) overall and $M$ is better than $m$ at AR, we ascribe 100\\% of the gap to AR."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291584238,
                "cdate": 1700291584238,
                "tmdate": 1700347926607,
                "mdate": 1700347926607,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FfzFxLpYWE",
                "forum": "LY3ukUANko",
                "replyto": "luL4VyJwP5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xdZW (continued.)"
                    },
                    "comment": {
                        "value": "**We briefly discuss two important decisions in this protocol**.\u00a0\n\nFirst, we only measure _explicit bigrams_, i.e. bigrams are identified based on token ids in the sequence. However, intuitively, models may also perform associative recall between related _concepts_. For instance, language may contain bigrams in which one word is swapped by a synonym. We note that our work does not measure recall in higher dimensional recall in favor of a more transparent procedure.\u00a0\n\nNext, we measure the gap based on _log-probabilities_ rather than perplexity. This is simply because we want to make our metrics independent of the number of tokens in each of the slices of the validation set. Approximately 6.4\\% of validation tokens are AR Hits with the threshold set to consider bigrams seen less than $1,250 \\times$ during training.\n\n\n**Additional Comments**\n\nOverall we have made clarifications to the submission, which we hope makes the work more understandable.\n\nWe include Appendix C and E, which provide several additional details on our experimental protocols for synthetic and downstream training across models. We also include a zip file of our code for reproducing our results with all models. Upon publication, we will also release all the pretrained checkpoints analyzed in our paper. We hope these materials build confidence in the reproducibility and thoroughness of our work.\n\n**References**\n\n\\[1] Dao et al., Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps, 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291627977,
                "cdate": 1700291627977,
                "tmdate": 1700343452018,
                "mdate": 1700343452018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LL4kNOsUMV",
            "forum": "LY3ukUANko",
            "replyto": "LY3ukUANko",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6370/Reviewer_9Su6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6370/Reviewer_9Su6"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the performance gap between Gated Convolution Models (GCM) and Transformers on language modeling. They show that the lacking capability of GCM on the Associative Recall (AR) task can explain most of the perplexity gap in the real world scenario. They further demonstrate two architecture modifications to GCM, i.e., adding extra attention layers or selectively looking-up for associative tokens, can bridge the performance gap. A theoretical bound of the number of parameters required for GCM to solve the Multiple Query AR task is also derived."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides a comprehensive study of the associative recall capability of neural language models with different neural architectures, and examines its impact on the next token prediction performance of the models on real world data.\n\nThe authors empirically demonstrate that boosting the associative recall capability of GCMs can mostly bridge its performance gap with the attention-based model under the scale of 360M parameters.\n\nThe authors derive a theoretical scaling bound for data-independent GCMs to solve AR, and validate it with synthetic data."
                },
                "weaknesses": {
                    "value": "The novelty of the paper is limited. The lack of AR ability of State Space Models (which is a special kind of long convolution model with embedded recurrency) has been analyzed in the H3 paper [1] through synthetic data. The proposed architectural modification of hybridization is a simple replication of the Hybrid-H3.\n\nThe scale of the experiments is limited. The authors only empirically examine their hypothesis for models with the size up to 360M number of parameters. It is not clear whether their claims still holds empirically given the shrinking trend of the performance gap that can already be observed under the current setting. \n\nThe paper does not provide important technical details for reproducibility. The implementation details of the proposed modification of selectively look-up is missing.\n\nThe research problem that the authors are trying to solve has been alleviated with existing [1,2] or emerging solutions [3,4]. The authors do not examine the empirical AR ability of data-dependant convolutions by claiming technical difficulties, but there does exist data-dependant SSMs, such as Liquid-S4 [2], that support causal language modeling. Not to mention the latest GCM, Monarch Mixer [3], that also supports causality. On hybridization, a previous work [4] on dynamic input subset selection for attention modules has also been proposed for efficiently combining SSMs with attention. The authors should consider comparing the proposed architectural modifications with these works to avoid being outdated upon publication.\n\n---\n[1] Hungry Hungry Hippos: Towards Language Modeling with State Space Models (ICLR 2023)\n\n[2] Liquid Structural State-Space Models (ICLR 2023)\n\n[3] Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (NeurIPS 2023)\n\n[4] Sparse Modular Activation for Efficient Sequence Modeling (NeurIPS 2023)"
                },
                "questions": {
                    "value": "According to Table 1, it seems that as the number parameters increase the performance gap between GCMs and Transformers is shrinking. Can you explain this phenomenon? Is it possible that GCMs may outperform Transformers on a larger scale such as with 700M or 1B parameter counts?\n\nCan you provide mathematical formulas for calculating the perplexity scores of AR hit tokens mentioned in Table 1 and Table2?\n\nHow is the selectively look-up mechanism exactly implemented to produce the numbers in Table 2? Is the attention based look-up trainable or not trainable? How is it added to the GCMs layer? Can you provide a series of formulas to describe a GCM layer that is equipped with the proposed selectively look-up mechanism?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6370/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6370/Reviewer_9Su6",
                        "ICLR.cc/2024/Conference/Submission6370/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699387017789,
            "cdate": 1699387017789,
            "tmdate": 1700507991061,
            "mdate": 1700507991061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xhHoyLRPJc",
                "forum": "LY3ukUANko",
                "replyto": "LL4kNOsUMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Su6"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review! The experiments you suggested and questions you raised helped us greatly improve the work! We appreciated that you found our work comprehensive across theory, synthetic experiments, and downstream experiments. Below, we address your questions and highlight new experiments on an additional set of 6 baseline architectures (H3, Liquid S4, M2, Sliding window attention as in Mistral, Block attention, and RetNet) and a new analysis of associative recall as model size increases (up to the 7B parameter scale).\n\n**Novelty and differences from prior work (e.g H3)**\n\nThank you for your thoughtful comments about H3. You are correct that the H3 work studies synthetic associative recall tasks and previously proposes the use of hybrid models. However, we make fundamentally different claims. They argue that gated-convolutions (like H3) can solve AR. In contrast, our claim is that gated-convolutions **cannot** solve AR without scaling the hidden dimension with N. To underscore this point, we have added evaluations of H3 in the main paper (Table 1) and show that the trends we documented for other gated-convolutions also hold for H3 (Figure 2). Further, we find that the synthetic task studied in H3 and Hyena don\u2019t reflect recall in real language, so we develop and release a new synthetic better aligned with actual data.\u00a0\n\nWe use this synthetic to explain why adding attention helps with recall. The H3 paper does not provide this explanation. Finally, in downstream hybrid experiments, we go beyond the application of full quadratic attention layers to show that sparse-attention, localized to potential AR tokens, is sufficient to close the gap to the Transformer baseline. We hope this clarifies our differences and contributions.\u00a0\n\nPlease also see our summary of contributions in the main response to all reviewers.\u00a0\n\n**Experiments with increased model sizes.**\n\nThank you for raising this question! Please find our new experimental results in the main response to all reviewers.\u00a0\n\n**New Related Works Section**\n\nBeyond the three architectures discussed above, we include an extended related works section in Appendix A. In response to your feedback that recent or concurrent architectures could alleviate the problem, the section discusses relevant architectures.\u00a0\n\n**Details for Reproducibility.**\n\nIn response to your feedback on the reproducibility of our work, we provide several additional details on our experimental protocols for synthetic (details in Appendix E.2) and downstream training (details in Appendix C) across models. We also include a zip file of our code for reproducing our results with all models. Upon publication, we will also release all the pretrained checkpoints analyzed in our paper. We hope these materials build confidence in the reproducibility of our work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291276009,
                "cdate": 1700291276009,
                "tmdate": 1700291276009,
                "mdate": 1700291276009,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U6Ygo0lYpf",
                "forum": "LY3ukUANko",
                "replyto": "LL4kNOsUMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Su6 (continued.)"
                    },
                    "comment": {
                        "value": "_SeqBoat (Ren et al.)._ We thank the reviewer for this pointer! The simple learned selection function we evaluate in Section 5 is similar to SeqBoat. One major difference is that we explicitly limit the number of activated tokens to ensure sub-quadratic scaling, while SeqBoat allows all tokens to activate. This occurs in some of experiments in the SeqBoat (See Figure 3 of their paper). While SeqBoat use the sparse attention at every layer, we simply replace three of the BaseConv layers with the sparse attention mechanism. Also, unlike SeqBoat, we use an auxiliary loss that guides the model to sparsely select attention positions. Based on the reviewer\u2019s pointer, we evaluate SeqBoat\u2019s approach of using no auxiliary loss and find that it increases perplexity by 0.5.\u00a0\n\n|     |                     |                                  |\n| --- | ------------------- | -------------------------------- |\n|     | With Auxiliary Loss | Without Auxiliary Loss (SeqBoat) |\n| PPL | 11.134              | 11.634                           |\n\n\nWe include a discussion of our differences from this work in Appendix A.3 (Related Works). We also note that our objective in Section 5 is to analyze simple, common-sense approaches for closing the gap sub-quadratically. We do not claim that this simple method is novel or that we are the first to propose it, but rather our contribution lies in the explanation of why these mechanisms work.\u00a0\n\nFinally, please see synthetic MQAR and downstream Pile experiments on additional architectures (RetNet, Sliding window attention like Mistral, and Blocked attention) in Appendix F."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291437963,
                "cdate": 1700291437963,
                "tmdate": 1700347816941,
                "mdate": 1700347816941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4DeNnbpgf6",
                "forum": "LY3ukUANko",
                "replyto": "U6Ygo0lYpf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6370/Reviewer_9Su6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6370/Reviewer_9Su6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed clarifications and additional experiments. I have raised my score to 8, given that the authors' response has resolved most of my concerns."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508506275,
                "cdate": 1700508506275,
                "tmdate": 1700508506275,
                "mdate": 1700508506275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]