[
    {
        "title": "Sample Relationship from Learning Dynamics Matters for Generalisation"
    },
    {
        "review": {
            "id": "bGBDTUZsy4",
            "forum": "8Ju0VmvMCW",
            "replyto": "8Ju0VmvMCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2038/Reviewer_9biR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2038/Reviewer_9biR"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to study the interaction between samples in supervised learning from a learning dynamics viewpoint. To that end, the authors propose a the labeled pseudo Neural Tangent Kernel (lpNTK), a new adaptation of the NTK which explicitly incorporates label information into the kernel. First, lpNTK is shown to asymptotically converge to the empirical Neural Tangent Kernel (eNTK). Then, the authors demonstrate how lpNTK helps to understand phenomena such as identifying and interpreting easy/hard examples and forgetting events. Finally, the paper shows a case study in which lpNTK is used to improve the generalization performance of neural nets in image classification via pruning and de-biasing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper's main conceptual contribution of incorporating label information into the NTK is novel and interesting!\n- The connection to sample difficulty and forgetting events is an interesting use-case for the lpNTK and allows us to connect sample difficulty with training dynamics.\n- I found it pretty interesting that lpNTK allows for pruning and simultaneous utility improvements. This could open up another connection to selective prediction (learning a high-confidence subset of the distribution).\n- Related work appears sufficiently discussed."
                },
                "weaknesses": {
                    "value": "- Should Eq (1) contain the sign operator wrapped around the last factor? In Section 2.2, it is stated that the paper does not use the whole error term but only the sign as shown in Eq (2). I am also not sure I was able to follow the explanation as to why the magnitudes should not matter. Could the magnitude be useful for ranking?\n- Although Figure 2 and the displayed evolution of distinct FPC clusters is insightful, I was wondering whether additional visualizations of the lpNTK would have been possible? \n- The results in Table 1 showing the forgetting events don't seem particularly impressive (especially on MNIST). Am I missing something here?\n- It would have been great if the authors had provided a visualization of some of the pruned samples. Are there any patterns present in pruned examples in particular?\n- It is evident that the authors tried really hard to fit the paper into the page limit. There are formatting artifacts in terms of very little vspace throughout the paper, especially towards the end of the paper (pages 7-9). I would strongly encourage the authors to reduce dangling words instead of resorting to these very evident formatting tricks."
                },
                "questions": {
                    "value": "- Have the authors considered drawing connections between interchangeable and contradictory samples and forging / machine unlearning [1], i.e. by training on a specific point we could plausibly say that we have also optimized for another datapoint? \n\nOther questions embedded in Weaknesses above.\n\nI am willing to increase my score as part of the discussion phase if the authors can address my concerns.\n\nReferences:\n\n[1] Thudi, Anvith, et al. \"On the necessity of auditable algorithmic definitions for machine unlearning.\" 31st USENIX Security Symposium (USENIX Security 22). 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2038/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2038/Reviewer_9biR",
                        "ICLR.cc/2024/Conference/Submission2038/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2038/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826640891,
            "cdate": 1698826640891,
            "tmdate": 1700711710492,
            "mdate": 1700711710492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6SXpg2w3lI",
                "forum": "8Ju0VmvMCW",
                "replyto": "bGBDTUZsy4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer 9biR (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for identifying the novel and interesting topic of our work. We answer the reviewer\u2019s concerns one-by-one below.\n\n1. Why error magnitude is not so important\n    \n> Should Eq (1) contain the sign operator wrapped around the last factor? In Section 2.2, it is stated that the paper does not use the whole error term but only the sign as shown in Eq (2). I am also not sure I was able to follow the explanation as to why the magnitudes should not matter. Could the magnitude be useful for ranking?\n    \nWe thank the reviewer for pointing out this. Actually, Eq (1) should not contain the sign operator, as it comes from the definition of gradient descent and Taylor expansion. For the magnitude part, it is true that this value is useful for ranking at the early stage of training, which is what most of the learning dynamic-based methods use, like forgetting score, C-score, etc. However, our method considers a different stage during training, i.e., a fully-trained model (but before overfitting). At this stage, the training accuracy on many samples is low, which means for most of the samples, the magnitudes of |p-q| are quite small. Thus, at this stage, the |p-q| are in the similar value for most of the samples. Hence at this stage, using the sign operator is a good approximation of the original p-q. In summary, we can consider Eq (1) in this way: $\\Delta q = \\eta\\cdot A\\cdot K\\cdot sign(p-q)\\cdot |p-q|$. We merge $K$ and $sign(p-q)$ to create an approximation.\n\nMoreover, an extreme example could be two identical samples whose |p-q| is 0. If we take the magnitude of  |p-q| into consideration, then the similarity between the two identical samples is then 0, which is clearly a wrong conclusion.\n    \n2. Further visualisation of lpNTK\n    \n> Although Figure 2 and the displayed evolution of distinct FPC clusters is insightful, I was wondering whether additional visualisation of the lpNTK would have been possible?\n    \nThanks for pointing out the insights from the results. We were wondering if the reviewer has particular quantity to track during the FPC clustering? Since the computation complexity of FPC clustering is $\\mathcal{O}(N log M)$, doing it every epoch is quite expensive, so we only did it after the convergence of the model. \n    \nWe are happy to provide further visualisation if there\u2019s any interesting quantity in mind to track and visualise.\n    \n3. Results in Table 1\n    \n > The results in Table 1 showing the forgetting events don't seem particularly impressive (especially on MNIST). Am I missing something here?\n    \n We thank the reviewer for bringing this point up. Indeed, the prediction of forgetting event is not so impressive. We did the prediction with eNTK before, and it\u2019s quite accurate (>90%). When predicting the forgetting events with lpNTK, we found there's a dilemma. If we do this after the convergence of the model, the lpNTK is more accurate but the forgetting events rarely happened. On the other hand, like in the paper, if we do the prediction at the beginning of the training, there are enough many forgetting events but the lpNTK is relatively inaccurate. However, in whichever case, the prediction performance is much better than random guessing, which can show that lpNTK helps a lot on the prediction of forgetting events.\n    \n4. Visualisation of the pruned samples.\n    \n> It would have been great if the authors had provided a visualization of some of the pruned samples. Are there any patterns present in pruned examples in particular?\n    \nWe thank the reviewer for bringing up this interesting question. In fact, the samples in Figure 9 and Figure 10 are very likely to be pruned, as they are the most interchangeable ones to the centroid sample of the head cluster. Qualitatively speaking, one pattern we observed is that they are very typical and their feature is shared across each other. It will be very interesting to explore whether it is possible to quantify our observation.\n    \n5. Formatting issue\n    \n> It is evident that the authors tried really hard to fit the paper into the page limit. There are formatting artifacts in terms of very little vspace throughout the paper, especially towards the end of the paper (pages 7-9). I would strongly encourage the authors to reduce dangling words instead of resorting to these very evident formatting tricks.\n    \nWe thank the reviewer for the suggestion, we've removed dangling words, and made some space for the last few sections."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404275509,
                "cdate": 1700404275509,
                "tmdate": 1700477767622,
                "mdate": 1700477767622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DJuf1ntdQH",
                "forum": "8Ju0VmvMCW",
                "replyto": "bGBDTUZsy4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer 9biR (2/2)"
                    },
                    "comment": {
                        "value": "6. Connections between interchangeable/contradictory samples and forging/machine unlearning\n    \n> Have the authors considered drawing connections between interchangeable and contradictory samples and forging / machine unlearning [1], i.e. by training on a specific point we could plausibly say that we have also optimized for another datapoint?\n    \nWe thank the reviewer for giving this interesting reference. For the interchangeable $x_1$ and $x_2$, we observe that training on $x_1$ will enhance the model\u2019s confidence of $x_2$. Hence, it is possible that \u201coptimising one also optimised another\u201d. Actually, that is what a model does for a data sample in the test set (our model learns from $x_1$ in the training set, and predicts another $x_2$ in the test set well). For the unlearning and contradictory samples (say $x_1$ and $x_3$), we believe the phenomenon suggested in [1] might happen if some conditions are satisfied. For example, if the task is binary classification and $x_1$ has a different label than $x_3$ (but they are indeed quite similar). The update of $(x_1, y = +)$ will make the model become less confident about $(x3, y = -)$, which can be considered as \u201cunlearning\u201d. But when $K$ is large, things would become much more compicated. We will explore this interesting question in our future work about introducing the label modification into lpNTK."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404314378,
                "cdate": 1700404314378,
                "tmdate": 1700404314378,
                "mdate": 1700404314378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AhWWjUwPDy",
                "forum": "8Ju0VmvMCW",
                "replyto": "DJuf1ntdQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2038/Reviewer_9biR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2038/Reviewer_9biR"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "I thank the authors for their rebuttal which has addressed many of my concerns. Although I still believe that more visualizations for the pruning behavior would have been promising, I consider the contribution of the paper as well as the author's responses to my concerns and their update paper draft enough to increase my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711701248,
                "cdate": 1700711701248,
                "tmdate": 1700711701248,
                "mdate": 1700711701248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BXvkZKyEh1",
            "forum": "8Ju0VmvMCW",
            "replyto": "8Ju0VmvMCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2038/Reviewer_4TQg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2038/Reviewer_4TQg"
            ],
            "content": {
                "summary": {
                    "value": "This paper enhances the Neural Tangent Kernel by integrating label information, which offers a more nuanced understanding of the interaction between samples compared to the existing eNTK method. The authors explore the relationship between lpNTK and eNTK, and use vector angles to classify samples into interchangeable, unrelated, and contradictory categories. This novel categorization facilitates a 'data-centric' improvement in model training. The idea is interesting, but there lack baselines to validate their proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The integration of label information into the Neural Tangent Kernel (NTK) represents a novel approach that enhances the characterization of sample interactions during optimization. And the sign approximation also makes sense.  \n2. The application of the proposed lpNTK is both reasonable and beneficial. It effectively validates the utility of lpNTK's mapping functions (vectors), demonstrating their practical effectiveness."
                },
                "weaknesses": {
                    "value": "I have the following concerns:\n1. Concerning Theorem 1, the authors assert that 'the gap between lpNTK and eNTK will not be significantly different.' However, this seems contradictory to subsequent analysis and empirical studies presented. The paper's central theme appears to be the integration of label information for a more nuanced understanding of sample relationships. If the lpNTK kernel closely resembles the original eNTK, could the authors clarify how this supports the stated claim? This warrants further explanation.\n2. The categorization of samples into three groups mirrors the 'data-IQ' framework in Seedat et al. (2022), which also segments samples into easy, ambiguous, and hard categories. Data-IQ assesses prediction error and confidence throughout the training process, a concept seemingly echoed in this paper. I recommend that the authors draw comparisons with this methodology to highlight distinct aspects and contributions of their work.\n3. There lack baselines to validate the effectiveness of the proposed method for measuring sample similarity. For instance, the influence function is a known technique for understanding relationships between samples during training. A comparison with such established methods would provide a more robust validation of the proposed approach.\n4. It would be beneficial if the authors could include a computational complexity analysis of the lpNTK. This information would be crucial for understanding the practicality and scalability of the proposed method in different settings.\n\n[1] Seedat, N., Crabb\u00e9, J., Bica, I., & van der Schaar, M. (2022). Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data. Advances in Neural Information Processing Systems, 35, 23660-23674."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2038/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2038/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2038/Reviewer_4TQg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2038/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699686480244,
            "cdate": 1699686480244,
            "tmdate": 1700712024696,
            "mdate": 1700712024696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AJHJGn6jFQ",
                "forum": "8Ju0VmvMCW",
                "replyto": "BXvkZKyEh1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer 4TQg (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for identifying the novelty of this work, as well as the practical potential of our lpNTK. Regarding the weaknesses, our responses are given as follows.\n\n1. Inconsistent expression following Theorem 1\n    \n> Concerning Theorem 1, the authors assert that 'the gap between lpNTK and eNTK will not be significantly different.' However, this seems contradictory to subsequent analysis and empirical studies presented. The paper's central theme appears to be the integration of label information for a more nuanced understanding of sample relationships. If the lpNTK kernel closely resembles the original eNTK, could the authors clarify how this supports the stated claim? This warrants further explanation.\n    \nWe thank the reviewer for pointing out this inconsistency, which might harm the understanding of the whole story of the paper. We've updated the Theorem 1 to avoid the following misconception: readers might think when the width ($w$) increases, the RHS of equation 4 will converge to 0, and hence lpNTK converges to eNTK in terms of F-norm. However, the original equation 4 says |lpNTK - eNTK| / |eNTK| converges to $O(w^{-1/2})$. If we move the |eNTK| term, which is $O(w^{1/2})$, to RHS, the new Theorem 1 becomes |lpNTK - eNTK| \u2192 O(1). That is to say, even under the infinity width assumption, the gap between lpNTK and eNTK is relatively bounded (not diverging to an arbitrarily large value) and non-negligible (not converging to 0). We also added Appendix B.2 to interpret more about the Theorem 1.\n    \n2. Categorisation of easy, ambiguous, and hard samples\n    \n> The categorization of samples into three groups mirrors the 'data-IQ' framework in Seedat et al. (2022), which also segments samples into easy, ambiguous, and hard categories. Data-IQ assesses prediction error and confidence throughout the training process, a concept seemingly echoed in this paper. I recommend that the authors draw comparisons with this methodology to highlight distinct aspects and contributions of their work.\n    \nWe thank the reviewer for providing this good reference. We will add some discussion between our method and Data-IQ in the next version of the paper. In short, there are two major differences between these two works. First, they consider the difficulty of samples from different perspectives. Data-IQ focuses more on the properties of individual samples, while lpNTK focuses more on the interaction between different samples. Second, Data-IQ evaluates the difficulty by observing the learning curve, while lpNTK can explain why such curves might emerge. We believe the findings of these two works can support each other (e.g., contradictions in lpNTK might be a sign of the existence of a hard sample, which could be a high-confidence prediction with a wrong label.)\n    \n3. Comparison with influence function\n    \n> There lack baselines to validate the effectiveness of the proposed method for measuring sample similarity. For instance, the influence function is a known technique for understanding relationships between samples during training. A comparison with such established methods would provide a more robust validation of the proposed approach.\n    \nWe thank the reviewer for pointing out the influence function as a possible baseline. We will add influence-score as one of the baselines for the experiments in Section 4.3 in the next version. In the meantime, we also mention that, influence-score is usually not as good as EL2N or forgetting score on CIFAR datasets [1]. So, the validation in Section 4.3 is very likely to be unaffected. In fact, in our extension work of lpNTK, our lpNTK-inspired data pruning method outperforms [1] and other existing baselines, which can help to show the practical effectiveness of lpNTK.\n    \nMoreover, influence function focuses on how one sample might change the estimated parameters thus the predictions on other samples, whereas lpNTK focuses on the relationship between samples in the lpNTK representation space (a transformed gradient space).\n    \n[1] Yang, S., Xie, Z., Peng, H., Xu, M., Sun, M., & Li, P. (2022). Dataset pruning: Reducing training data by examining generalization influence.\u00a0ICLR-2023.\n    \n[2] Koh, P. W., & Liang, P. (2017, July). Understanding black-box predictions via influence functions. In\u00a0*International conference on machine learning*\u00a0(pp. 1885-1894). PMLR.\n\n(Continued below)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403418576,
                "cdate": 1700403418576,
                "tmdate": 1700477827975,
                "mdate": 1700477827975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mdho2hVb3F",
                "forum": "8Ju0VmvMCW",
                "replyto": "BXvkZKyEh1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer 4TQg (2/2)"
                    },
                    "comment": {
                        "value": "4. Discussion about the computational complexity of lpNTK\n    \n> It would be beneficial if the authors could include a computational complexity analysis of the lpNTK. This information would be crucial for understanding the practicality and scalability of the proposed method in different settings.\n    \nWe thank the reviewer for pointing out this important problem. We've updated the discussion to cover the computational complexity of lpNTK. In short, the computational complexity of lpNTK is $\\mathcal{O}(N^2d^2)$ where $N$ is the number of samples and $d$ is the number of parameters.\n    \nThe bottleneck of calculating lpNTK is mainly the size of the dataset: larger $N$ means a larger matrix (the influence of the number of classes K is circumvented by using the pNTK trick). We are considering to reduce the cost of this part using proxy quantities. For example, the learning-curve-based method (e.g., C-score or Data-IQ), or quantities like losses. Once the easy sample is identified, it is then not necessary to calculate $K(x_1, _x2)$ if $x_1$ and $x_2$ are both easy samples and look similar to each other.\n\nFurthermore, we can also use our method in some domains where the dataset size is not that big, e.g., alignment for LLM (roughly 10k samples needed) or few-shot learning. We would leave these to our future work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403478398,
                "cdate": 1700403478398,
                "tmdate": 1700477611575,
                "mdate": 1700477611575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zAL5oemvdl",
                "forum": "8Ju0VmvMCW",
                "replyto": "q4M5R65M8N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2038/Reviewer_4TQg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2038/Reviewer_4TQg"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for the further explanations, I hope the authors could add more discussions in their next version. Based on the rebuttal, I would like to raise my score to 6."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712005172,
                "cdate": 1700712005172,
                "tmdate": 1700712005172,
                "mdate": 1700712005172,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1hxH6llMAK",
            "forum": "8Ju0VmvMCW",
            "replyto": "8Ju0VmvMCW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2038/Reviewer_3vgc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2038/Reviewer_3vgc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to incorporate label information into the Neural Tangent Kernel (NTK) and designed a kernel called lpNTK to study the interaction between training examples. The author suggested classifying the relationships between a pair of examples into three types --- interchangeable, unrelated, and contradictory --- based on the angles between the vectors represented in lpNTK. The author then used these concepts to analyze some phenomena and techniques of learning dynamics, such as learning difficulty, forgetting, pruning, and redundancy. The observations and analyses were supported by experiments on the MNIST and CIFAR10 datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Disclaimer: I'm not very familiar with the literature and some technical details of NTK. I might be biased because other reviews are visible to me before I write mine.\n\n- The learning dynamics and learning difficulty is an important problem in many subfields of machine learning. The author provided nice theoretical tools for it based on NTK.\n- Incorporating label information is useful for analyzing many supervised learning tasks in a more fine-grained way. The use case of data pruning is reasonable and convincing. I believe that the proposed tools can be used to deepen our understanding of some methods for learning from noisy/imbalanced data.\n- This paper is well structured. The author raised intuitive hypotheses, asked clear questions, and then conducted reasonable experiments to verify them.\n- The author contextualized this paper well and discussed related work sufficiently."
                },
                "weaknesses": {
                    "value": "- Maybe it's because I'm unfamiliar with the literature, but I feel that this paper can benefit from mathematically clearer definitions of some terms such as interaction and learning difficulty.\n- The author stated that \"contradictory samples are rare in practice\" but didn't explain why. I suspect that it's because the MNIST and CIFAR10 datasets used in experiments are relatively clean, and there are few ambiguous training examples. The conjectures in C.3 were nice, but I would expect more solid explanations or explorations.\n- The author did not discuss much about the limitations of this work.\n\nMinor issues:\n- Section 2.1: the abbreviation $\\mathbf{z} \\in \\mathbb{R}^K$ is misleading because I think $\\mathbf{z}$ is a $\\mathbb{R}^K$-valued function, not just a vector.\n- Rigorously, a simplex with $K$ values is $(K-1)$-dimensional, i.e., it should be $\\Delta^{K-1}$.\n- Since many methods for noisy label learning and selective prediction (classification with rejection) heuristically make use of the learning dynamics, it would be convincing to apply lpNTK to those applications. However, those can be future work directions."
                },
                "questions": {
                    "value": "- It is not completely clear to me why it is reasonable to fix the similarity matrix $\\mathbf{K}$. Isn't it that a pair of training examples can be similar or dissimilar during different stages of training? How can we obtain the model that performs the best on the validation set in applications like data pruning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2038/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2038/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2038/Reviewer_3vgc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2038/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700035611983,
            "cdate": 1700035611983,
            "tmdate": 1700035611983,
            "mdate": 1700035611983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x0Cn5lK5Uy",
                "forum": "8Ju0VmvMCW",
                "replyto": "1hxH6llMAK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 3vgc (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for identifying the contribution of our work to the fundamental machine learning questions. We respond to the reviewer\u2019s concerns one-by-one below.\n\n1. Clearer definitions of terms\n    \n > I feel that this paper can benefit from mathematically clearer definitions of some terms such as interaction and learning difficulty.\n    \nWe thank the reviewer for the suggestion. We fixed the minor problems pointed out in the minor issues above to make the definitions of $\\mathbf{z}$ and $\\mathbf{q}$ more rigorous.  Regarding the definition of learning difficulty, in this paper, we measure it by accumulated loss over training, and interpret it as a result of pair-wise influence between samples. We agree that mathematically clearer definitions can further improve the formulation of the problem. Regarding the interaction between samples, in short, given a threshold $e$, intuitively, we can have the following relationships:\n    \n- interchangeable: $lpNTK(x_1, x_2) \\gg e$\n    \n- unrelated: $-e < lpNTK(x_1, x_2) < e$\n    \n- contradictory: $lpNTK(x_1, x_2) \\ll  e$\n    \nHowever, as this relative difficulty is a qualitative measurement, which highly depends on the dataset and network, we find it hard to define it rigorously.\n    \n2. Rare contradictory samples in practice\n    \n> The author stated that \"contradictory samples are rare in practice\" but didn't explain why. I suspect that it's because the MNIST and CIFAR10 datasets used in experiments are relatively clean, and there are few ambiguous training examples. The conjectures in C.3 were nice, but I would expect more solid explanations or explorations.\n    \nThanks a lot for this good question. We also believe that the labels in benchmarks like MNIST and CIFAR are relatively clean, and hence contradictory samples are rare in practice. Actually, we believe that if the input signal is **sampled uniformly**, then the more complex the input samples are (e.g., colour images are more complex than black-and-write ones, high-resolution images are more complex than low-resolution ones) the less likely that the contradictory samples occur. However, in practice, there are indeed specific cases that can introduce many contradictory signals. For example, manually flipping the label, systematical change of the data collector, or for the object we are interested in is quite nuance in the image (like the two images with exactly the same background, but the people there are different). We added Appendix C.4 to specifically discussion the rare case of contradictory samples in practice.\n    \n3. No enough discussion about the limitation of this work\n    \n> The author did not discuss much about the limitations of this work. \n    \nThanks for pointing out this. We have updated the discussion subsection of Section 4 to focus more on the practical limitations of this work. In short, the main limitation of this method is the computation of lpNTK, which highly depends on the number of input samples. Moreover, the computational complexity is $\\mathcal{O}(N^2d^2)$ where $N$ is the number of samples and $d$ is the number of parameters. Also, if the model\u2019s behaviour cannot be well approximated by NTK theory, the method proposed here will also be influenced.\n\n(Continued below)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401824905,
                "cdate": 1700401824905,
                "tmdate": 1700477372892,
                "mdate": 1700477372892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aLLfZYWyS2",
                "forum": "8Ju0VmvMCW",
                "replyto": "1hxH6llMAK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2038/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 3vgc (2/2)"
                    },
                    "comment": {
                        "value": "4. Fixed similarity matrix K \n    \n> It is not completely clear to me why it is reasonable to fix the similarity matrix $K$. Isn't it that a pair of training examples can be similar or dissimilar during different stages of training? How can we obtain the model that performs the best on the validation set in applications like data pruning?\n    \nWe thank a lot for the reviewer to point this out. In fact, we\u2019re working on a data-pruning method based on lpNTK. The limitation of lpNTK in the dynamic case is its computational complexity. So, to make it possible to approximate the lpNTK between samples during the training, we find a relatively good enough and practical proxy, i.e. the loss of samples. Following our analysis in Section 4.3, easy samples contribute not much to the generalisation performance. So, in our data pruning work based on lpNTK, we can prune samples that have similar lpNTK representations, thus smaller losses, without significantly decreasing the generalisation performance. Our experiments on ImageNet-1K shows that this lpNTK-based pruning method indeed outperforms the existing SOTA data pruning methods.\n    \nRegarding the stability of $K$, it is true that the similarity matrix $K$ would behave quite differently during training. Specifically, at the beginning of pre-training, $K$ changes a lot and we cannot get too much useful information in this phase. On the other hand, at the very end of the training, $K$ jitters around a stable value, because most of the samples are already learned well, which is also not what we want. We speculate that is because the NN does feature learning at the beginning of training and then does the NTK-style learning after training for several epochs. In our experiments, we chose the model in this phase to calculate lpNTK, i.e., trained but not overfitted yet (we can observe the validation curve to select it, the phase change is quite obvious and the value of $K$ is quite stable in this phase). We added an outline for computing lpNTK in practice at the end of Section 2.3.\n    \n5. Noisy label issue\n    \n> Since many methods for noisy label learning and selective prediction (classification with rejection) heuristically make use of the learning dynamics, it would be convincing to apply lpNTK to those applications. However, those can be future work directions.\n    \nWe appreciate the reviewer for pointing out this interesting direction. This is indeed very intriguing and promising. We will explore that in our future work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2038/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401855412,
                "cdate": 1700401855412,
                "tmdate": 1700477455516,
                "mdate": 1700477455516,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]