[
    {
        "title": "Domain-Inspired Sharpness Aware Minimization Under Domain Shifts"
    },
    {
        "review": {
            "id": "uGpWP0oKoS",
            "forum": "I4wB3HA3dJ",
            "replyto": "I4wB3HA3dJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1547/Reviewer_BeTH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1547/Reviewer_BeTH"
            ],
            "content": {
                "summary": {
                    "value": "Targeting at domain generalization scenario with possible shifts among domains, this paper proposes to take 'per domain optimality' into consideration for finding the perturbation of SAM. The proposed DISAM is shown to have an improved convergence rate. Numerically, DISAM outperforms other SAM alternatives."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The idea of tackling domain shift in SAM is novel. \n\nS2. A new algorithm, DISAM, is proposed with satisfying numerical results. DISAM improves over state-of-the-art by a large margin."
                },
                "weaknesses": {
                    "value": "W1. Stronger motivation needed. The authors motivates the domain difference using Fig. 1 (b). While the convergence behaviors among domains are indeed inconsistent at the early stage,  the losses are similar after e.g., 30 epoch. The authors should also explain why the difference of convergence in **early phase** impact the generalization of SAM.\n\n\nW2. More discussions on $\\lambda$ in eq. (7) are needed. This is a critical parameter that considers the variance/domain shifts in DISAM. However, this $\\lambda$ does not appear in Theorem 1. Can the authors illustrate more on this point? And how does the choice of $\\lambda$ influence convergence and generalization?"
                },
                "questions": {
                    "value": "Q1. Relation with a recent work (https://arxiv.org/abs/2309.15639).\n\nThe paper above also proposes approaches to reduce variance for finding perturbations, although not designed for the domain generalization setting. How does this work relate with the proposed DISAM?\n\n\nQ2. Theorem 1 illustrates that the *convergence* of DISAM benefits from $\\Gamma$. Can the authors explain more on the discussion of \n> as DISAM enjoys a smaller $\\Gamma$ than SAM, DISAM can permit the potential larger $\\rho$ than that in SAM, thus yielding a better generalization\n\nIn particular, how does the convergence rate link with generalization?\n\nQ3. The last sentence in Sec 3 claims that\n>  ... allowing larger $\\rho$ for better generalization.\n\nWhy does larger $\\rho$ relate to better generalization?\n\nQ4. (minor) The notation in e.g., eq (5) can be improved, because the multiple subscripts $i$ in $\\Sigma_{i} \\frac{C_i}{\\sum_i C_i}$ are confusing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1547/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1547/Reviewer_BeTH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697946496349,
            "cdate": 1697946496349,
            "tmdate": 1700875519237,
            "mdate": 1700875519237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WTnjeKn5EO",
                "forum": "I4wB3HA3dJ",
                "replyto": "uGpWP0oKoS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BeTH [1/2]"
                    },
                    "comment": {
                        "value": "## W1\n\n> Stronger motivation needed. The authors motivates the domain difference using Fig. 1 (b). While the convergence behaviors among domains are indeed inconsistent at the **early stage**, the losses are similar after e.g., 30 epoch. The authors should also explain why the difference of convergence in early phase impact the generalization of SAM.\n\nWe would like to kindly point out that the y-axis of Figure.1 (b) represents the convergence degree instead of the loss value, which is computed by using the current loss value to divide the converged maximum loss. For the details, **we follow the reviewer's advice and have added one section to strenghen the explanation about the motivation in the Appendix C.5 of the revised version**.\n\n**Relationship of early-stage convergence and generalization:** In Figure 7(b) of page 24, we present the curves of various domain losses during training, which can promote the understanding. Basically, the inconsistency of the convergence degree in the early phase in Figure 7(a) **actually impairs the overall convergence of the model**. As can be seen in Figure 7(b), SAM has converged at a higher loss value, which reflects that the model is optimized towards a poorer local minima, resulting in the worse generalization performance.\n\n\n## W2\n\n> More discussions on $\\lambda$ in Eq. (7) are needed. This is a critical parameter that considers the variance/domain shifts in DISAM. However, this $\\lambda$ does not appear in Theorem 1. Can the authors illustrate more on this point? And how does the choice of $\\lambda$ influence convergence and generalization?\n\nThank you very much for the reviewer's advice. We have elaborated more discussion about the role of $\\lambda$ in DISAM in Appendix B.2 (pages 18-21) in the revised submission. In the proof of Theorem 1, specifically Eq. (15) on page 20, **$\\lambda$ is integrated into $\\beta$, serving as a hyperparameter that regulates the weight adjustment in DISAM**. It functions by modulating the degree of correction for domain shifts:\n$$\n\\beta^i_t = \\alpha_i - \\frac{2\\lambda}{M} \\left (\\mathcal{L}^i(w_t) - \\frac{1}{M} \\sum_{j=1}^M \\mathcal{L}^j(w_t) \\right)\n$$\n\n**The influence of $\\lambda$:** The choice of $\\lambda$ influences how aggressively DISAM responds to variance or domain shifts, with **a higher $\\lambda$ leading to more pronounced adjustments in $\\beta$**. Our experimental analysis in Figures 5(c) and (d) on page 9, reveals that DISAM's performance remains relatively stable across a wide range of $\\lambda$ values. However, **choosing too large $\\lambda$ can result in overly aggressive early training adjustments, yielding the negative impact on the convergence process and leading to the increased variance in repeated experiments**. Consequently, we adopted a default $\\lambda$ value of 0.1 in all experiments.\n\n## Q1\n\n> Relation with a recent work (https://arxiv.org/abs/2309.15639). The paper above also proposes approaches to reduce variance for finding perturbations, although not designed for the domain generalization setting. How does this work relate with the proposed DISAM?\n\nThank you for recommending this excellent work on variance suppression (VaSSO)[1]. We have added this method into the revised submission with the proper discussion. Generally, while DISAM and VaSSO both enhance the perturbation direction generation in SAM, they target different aspects:\n\n- **VaSSO Approach:** VaSSO aims to **reduce noise from mini-batch sampling** by using averaged previous perturbation directions. Its primary focus is on **stabilizing perturbation generation within the same domain**.\n\n- **DISAM's Unique Focus:** In contrast, DISAM incorporates domain information to **specifically address domain-level convergence inconsistencies**, a challenge prevalent in domain shift scenarios. DISAM's approach is to **impose a variance minimization constraint on domain loss** during the perturbation generation process, thereby enabling a more representative perturbation location and enhancing generalization.\n\nWe are running the experiments to compare/combine DISAM with VaSSO. Once the experiments are finished, we will report the results here and include the results in the submission.\n\n**Reference:**\n\n[1]. Enhancing Sharpness-Aware Optimization Through Variance Suppression, arXiv2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235144271,
                "cdate": 1700235144271,
                "tmdate": 1700235144271,
                "mdate": 1700235144271,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P9R8Vrguc2",
                "forum": "I4wB3HA3dJ",
                "replyto": "uGpWP0oKoS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BeTH [2/2]"
                    },
                    "comment": {
                        "value": "## Q2 & Q3\n\n> Q2: Theorem 1 illustrates that the convergence of DISAM benefits from $\\Gamma$. Can the authors explain more on the discussion of as DISAM enjoys a smaller $\\Gamma$ than SAM, DISAM can permit the potential larger $\\rho$ than that in SAM, thus yielding a better generalization\". In particular, how does the convergence rate link with generalization?\n> Q3. The last sentence in Sec 3 claims that \".. allowing larger $\\rho$ for better generalization.\" Why does larger $\\rho$ relate to better generalization?\n\nThank you for the advice on explanation about convergence and generalization, and we have added more discussion and analysis in the Appendix B.3 on page 21. In the following, we provide some clarification on these points.\n\n- **Generalization Theorem of SAM:** In the SAM framework, the parameter $\\rho$ plays a crucial role in determining generalizability. As established in [1], there exists an upper bound on the generalization error for SAM, suggesting that a larger $\\rho$ could potentially enhance generalization, provided that convergence is not impeded. Here is the relevant generalization bound from [1]:\n> For any $\\rho > 0$ and any distribution $\\mathcal{D}$, with probability $1- \\delta$ over the choice of the training set $S\\sim \\mathcal{D}$,\n> $$\\mathcal{L} _{\\mathcal{D}} (w) \\leq \\max _{\\| \\epsilon\\| _2 \\leq \\rho} \\mathcal{L} _{S}(w+\\epsilon) + \\sqrt{\\frac{k \\log \\left(  1 + \\frac{\\| w \\| _2^2}{\\rho^2} (1+\\sqrt{\\frac{\\log (n)}{k}})^2 \\right) + 4 \\log \\frac{n}{\\delta} + \\tilde{O}(1)}{n-1}} $$\n> where $n = |S|$, $k$ is the number of parameters and we assumed $\\mathcal{L} _{\\mathcal{D}}(w) \\leq \\mathbb{E} _{\\epsilon_i \\approx \\mathcal{N}(0, \\rho)} [\\mathcal{L} _{\\mathcal{D}}(w+\\epsilon)]$. DISAM leverages a smaller $\\Gamma$ than SAM, as shown in Theorem 1 in page 5. This allows DISAM to employ a potentially larger $\\rho$, enhancing generalizability.\n\n- **Practical Implications:** Combining the above theorem with the convergence theorem (Theorem 1 on page 5), there is a trade-off with respect to $\\rho$. A larger $\\rho$ might theoretically enhance generalization but poses greater challenges for convergence. This reflects the intuitive notion that searching for flatter minima across a broader range is inherently more complex, which can potentially affect training efficiency. However, **if $\\mathcal{L}_{S} (w + \\epsilon)$ can be converged with a sufficiently small value, a larger $\\rho$ corresponds to better generalization**.  DISAM, with a smaller $\\Gamma$ compared to SAM, converges faster, which means that under the same convergence speed, a larger $\\rho$ can be used to achieve better generalization. \n\n\n- **Empirical Validation:** Our experiments, as illustrated in Figures 3(c) and (d) on page 6, demonstrate that DISAM effectively employs a larger $\\rho$ compared to traditional SAM. DISAM's ability to handle a larger $\\rho$ results in both consistent convergence and improved generalization compared to SAM, demonstrating its superiority in domain shift scenarios.\n\nWe appreciate the reviewer's comments on the theoretical parts and have adjusted the corresponding parts to make these points more clear.\n\n**Reference:**\n\n[1]. Sharpness-aware minimization for efficiently improving generalization, ICLR2021.\n\n\n## Q4\n\n> (minor) The notation in e.g., eq (5) can be improved, because the multiple subscripts $i$ in $\\sum_{i}\\frac{C_i}{\\sum_{i}C_i}$ are confusing.\n\nWe appreciate your detailed feedback on the notation in Eq. (5) and have updated it on page 4 to avoid confusion."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235201449,
                "cdate": 1700235201449,
                "tmdate": 1700235201449,
                "mdate": 1700235201449,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0bcLv8wwPA",
            "forum": "I4wB3HA3dJ",
            "replyto": "I4wB3HA3dJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1547/Reviewer_d1EH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1547/Reviewer_d1EH"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the Domain-Inspired Sharpness Aware Minimization (DISAM) algorithm, a novel approach for optimizing under domain shifts. The motivation behind DISAM is to address the issue of inconsistent convergence rates across different domains when using Sharpness Aware Minimization (SAM), which can lead to optimization biases and hinder overall convergence.\n\nThe key innovation of DISAM lies in its focus on maintaining consistency in domain-level convergence. It achieves this by integrating a constraint that minimizes the variance in domain loss. This strategy allows for adaptive gradient perturbation: if a domain is already well-optimized (i.e., its loss is below the average), DISAM will automatically reduce the gradient perturbation for that domain, and increase it for less optimized domains. This approach helps balance the optimization process across various domains.\n\nTheoretical analysis provided in the paper suggests that DISAM can lead to faster overall convergence and improved generalization, especially in scenarios with inconsistent domain convergence. The paper supports these claims with extensive experimental results, demonstrating that DISAM outperforms several state-of-the-art methods in various domain generalization benchmarks. Additionally, the paper highlights the efficiency of DISAM in fine-tuning parameters, particularly when combined with pretraining models, presenting a significant advancement in the field."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "As of now, there has not yet been a sharpness-aware minimization (SAM) methodology developed specifically for addressing distribution shifts. The issue of varying convergence rates across different domains, as observed in SAM, is undeniably a significant challenge.\n\nThis methodology presents an impressive degree of compatibility, as it can be integrated with a variety of sharpness-variants. An especially commendable aspect of this approach is its computational efficiency. Compared to standard SAM techniques, it does not incur additional computational costs, making it a practical option for scenarios where resource constraints are a consideration.\n\nIn summary, the development of a SAM methodology that is adept at handling distribution shifts, and particularly its implications for domain convergence, is both novel and highly relevant in the current landscape of optimization challenges."
                },
                "weaknesses": {
                    "value": "The idea of minimizing the variance between losses, a core aspect of the presented methodology, is not entirely novel. Similar concepts have been previously explored in methods like vREX (Out-of-Distribution Generalization via Risk Extrapolation) and further extended to gradient computations in methodologies like Fishr (Invariant Gradient Variances for Out-of-Distribution Generalization). In this context, the proposed approach appears to be an incremental adaptation of vREX principles applied specifically to the challenges faced in Sharpness Aware Minimization (SAM) scenarios.\n\nThe improvement in out-of-distribution (OOD) performance using the DISAM methodology does not appear intuitive. In fact, when comparing its performance enhancements to those achieved with CLIPOOD, as reported, the difference seems marginal. This observation raises questions about the actual effectiveness of DISAM, particularly in the context of fine-tuning methodologies."
                },
                "questions": {
                    "value": "Similar to how transitioning from ERM to vREX in optimization has been shown to enhance domain generalization performance, the application of vREX to SAM in the form of this methodology could be seen as a natural extension that brings comparable performance improvements. Furthermore, it is a valid assertion that incorporating various algorithms tailored for domain generalization (such as Fish, Fishr, gradient alignment) into the SAM optimization framework could potentially yield performance enhancements. The logic here is that these methods, when applied within the context of SAM, could enhance its ability to generalize across domains.\n\nHowever, the critique that DISAM may simply be an incremental version of applying domain generalization methodologies to SAM is not without its counterarguments. It's important to consider the specific challenges and nuances of the SAM framework and how DISAM addresses these. If DISAM introduces significant modifications or adaptations that are uniquely tailored to the idiosyncrasies of SAM, then its contribution could extend beyond a mere incremental update. The key would lie in the specifics of how DISAM modifies or enhances the existing principles of SAM and domain generalization methods, making it more than just a straightforward application of known techniques.\n\nIn summary, while the perspective that DISAM is an incremental version of existing methodologies is certainly tenable, a comprehensive evaluation would require a deeper exploration of how DISAM specifically adapts or augments the SAM framework to address its unique challenges. If such adaptations are significant, they could justify the novelty and utility of DISAM beyond a simple combination of existing techniques.\n\nCan you provide the reproducible code during the rebuttal period?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "\\"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1547/Reviewer_d1EH",
                        "ICLR.cc/2024/Conference/Submission1547/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698334156910,
            "cdate": 1698334156910,
            "tmdate": 1700717237511,
            "mdate": 1700717237511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ns7KHi3Uby",
                "forum": "I4wB3HA3dJ",
                "replyto": "0bcLv8wwPA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer d1EH [1/3]"
                    },
                    "comment": {
                        "value": "## W1 & Question\n\n> W1: The idea of minimizing the variance between losses, a core aspect of the presented methodology, is not entirely novel. Similar concepts have been previously explored in methods like vREX (Out-of-Distribution Generalization via Risk Extrapolation) and further extended to gradient computations in methodologies like Fishr (Invariant Gradient Variances for Out-of-Distribution Generalization). In this context, the proposed approach appears to be an incremental adaptation of vREX principles applied specifically to the challenges faced in Sharpness Aware Minimization (SAM) scenarios.\n> Question: In summary, while the perspective that DISAM is an incremental version of existing methodologies is certainly tenable, a comprehensive evaluation would require a deeper exploration of how DISAM specifically adapts or augments the SAM framework to address its unique challenges. If such adaptations are significant, they could justify the novelty and utility of DISAM beyond a simple combination of existing techniques.\n\nFirst, we are sorry about one typo issue in Eq.(7) that may mislead the understanding of the reviewer, which we has corrected with the proper description in the revised submission. **Concretely, the $w$ (i.e., $\\hat{w}$ in the revised version) in the variance term is actually without derivative taken, when optimzing the model parameter**. That is to say, the $w$ (i.e., $\\hat{w}$ in the revised version) only makes effect during the inner loop for the perturbation generation. This makes DISAM intrinsically different from V-REx [1] (an extension of IRM [2]). We use a table in the following the comprehensively compare DISAM and VRex to clarify our difference. Generally, V-REx focuses on **achieving consistent loss values across domains**, and Fish [3] and Fishr [4] **emphasize gradient consistency across domains** to foster out-of-domain generalization.\n\nIn comaprison, DISAM targets to address the challenge of **effective perturbation directions for sharpness estimation in domain shift scenarios** by introducing the guidance of the domain-level loss variance minimization, which actually does not affect the training objective (i.e., the first term of Eq.(7)). Unlike V-REx, which directly minimizes domain loss variance and can negatively impact convergence, or Fish and Fishr, which constrain gradient updates, **DISAM adopts a distinct strategy, minimizing sharpness of multiple domains consistently, to enhance generalization**.\n\n|  Method   |    Total Optimization Function | Optimization on $w$   | Optimization on $\\epsilon$       |\n| -------- |-------------|-------------|--------------------|\n| ERM      | $\\min _{w} \\sum _{i=1}^M  \\alpha _i \\mathcal{L} _{i}(w)$ | Same to left | $\\times$ |\n| V-REx[1] | $\\min _{w} \\sum _{i=1}^M \\alpha _i \\mathcal{L} _{i}(w) + \\beta \\text{Var} \\{ \\mathcal{L} _i(w) \\} _{i=1}^M$    | Same to left                  | $\\times$    |\n| Fish[3]  | $\\min _{w} \\sum _{i=1}^M  \\alpha _i \\mathcal{L} _{i}(w) - \\gamma \\frac{2}{M(M-1)} \\sum _{i,j \\in [1,M]}^{i \\neq j} \\nabla \\mathcal{L} _{i}(w) \\cdot \\nabla \\mathcal{L} _{j}(w)$       | Same to left   | $\\times$ |\n| Fishr[4] | $\\min _{w} \\sum _{i=1}^M  \\alpha _i \\mathcal{L} _{i}(w) - \\lambda \\frac{1}{M} \\sum _{i=1}^{M} \\| \\nabla \\mathcal{L} _{i}(w) - \\nabla \\mathcal{L}(w)\\|^2$                             | Same to left | $\\times$ |\n| SAM      | $\\min _{w}  \\max _{\\| \\epsilon\\|_2 \\leq \\rho}  \\sum _{i=1}^M \\alpha _i \\mathcal{L} _i(w + \\epsilon)$ | $\\min _{w}  \\sum _{i=1}^M \\alpha _i \\mathcal{L} _i(w + \\epsilon)$    | $\\max _{\\| \\epsilon\\|_2 \\leq \\rho}  \\sum _{i=1}^M \\alpha _i \\mathcal{L} _i(w + \\epsilon)$      |\n| DISAM    | $\\min _{w}  \\max _{\\| \\epsilon\\|_2 \\leq \\rho} \\left[ \\sum _{i=1}^M \\alpha _i \\mathcal{L} _i(w + \\epsilon) -  \\lambda \\text{Var}\\{\\mathcal{L} _i(\\hat{w} + \\epsilon)\\} _{i=1}^M \\right]$ | $\\min _{w}  \\sum _{i=1}^M \\alpha _i \\mathcal{L} _i(w + \\epsilon)$  | $\\max _{\\| \\epsilon\\|_2 \\leq \\rho} \\left[ \\sum _{i=1}^M \\alpha _i \\mathcal{L} _i(w + \\epsilon) -  \\lambda \\text{Var}\\{\\mathcal{L} _i(w + \\epsilon)\\} _{i=1}^M \\right]$ |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234842744,
                "cdate": 1700234842744,
                "tmdate": 1700234842744,
                "mdate": 1700234842744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hfGP6R0IOx",
                "forum": "I4wB3HA3dJ",
                "replyto": "0bcLv8wwPA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## W1 & Question\n> W1: The idea of minimizing the variance between losses, a core aspect of the presented methodology, is not entirely novel. Similar concepts have been previously explored in methods like vREX (Out-of-Distribution Generalization via Risk Extrapolation) and further extended to gradient computations in methodologies like Fishr (Invariant Gradient Variances for Out-of-Distribution Generalization). In this context, the proposed approach appears to be an incremental adaptation of vREX principles applied specifically to the challenges faced in Sharpness Aware Minimization (SAM) scenarios.\n> Question: In summary, while the perspective that DISAM is an incremental version of existing methodologies is certainly tenable, a comprehensive evaluation would require a deeper exploration of how DISAM specifically adapts or augments the SAM framework to address its unique challenges. If such adaptations are significant, they could justify the novelty and utility of DISAM beyond a simple combination of existing techniques.\n\n\n\nSecond, DISAM is orthogonal to existing state-of-the-art methods including V-REx and Fishr and can improve their performance regarding generalization. The following tables present some results to prove DISAM\u2019s superiority:\n\n[**Table 1.** Comparison with existing methods.]\n| **Method**      | **PACS** | **VLCS** | **OfficeHome** | **TerraInc** | **DomainNet** | **Avg.** |\n| --------------- | :--------: | :--------: | :--------------: | :------------: | :-------------: | :--------: |\n| IRM             | 83.5     | 78.5     | 64.3           | 47.6         | 33.9          | 61.6     |\n| V-REx[1]         | 84.9     | 78.3     | 66.4           | 46.4         | 33.6          | 61.9     |\n| **V-REx+DISAM**  | 85.8     | 78.4     | 70.5           | 45.9         | 42.3          | **64.6**     |\n| Fish[3]         | 85.5     | 77.8     | 68.6           | 45.1         | 42.7          | 63.9     |\n| Fishr[4]        | 86.9     | 78.2     | 68.2           | 53.6         | 41.8          | 65.7     |\n| **Fishr+DISAM** | 87.5     | 79.2     | 70.7           | 54.8         | 43.9          | **67.2** |\n\n\n\n[**Table 2.** Comparison with existing SAM-based methods.]\n| **Method**     | **PACS** | **VLCS** | **OfficeHome** | **TerraInc** | **DomainNet** | **Avg.** |\n| --------------- | :--------: | :--------: | :--------------: | :------------: | :-------------: | :--------: |\n| SAM            | 85.8     | 79.4     | 69.6           | 43.3         | 44.3          | 64.5     |\n| **SAM+DISAM**  | 87.3     | 80.1     | 70.7           | 47.9         | 45.8          | **66.4** |\n| GSAM           | 85.9     | 79.1     | 69.3           | 47.0         | 44.6          | 65.1     |\n| **GSAM+DISAM** | 87.2     | 80.0     | 70.8           | 50.6         | 45.6          | **66.8** |\n| SAGM           | 86.6     | 80.0     | 70.1           | 48.8         | 45.0          | 66.1     |\n| **SAGM+DISAM** | 87.5     | 80.7     | 71.0           | 50.0         | 46.0          | **67.0** |\n\nThese results clearly demonstrate DISAM\u2019s distinctive approach and its effectiveness in enhancing generalization, supporting its novelty and utility beyond existing methodologies.\n\nWe appreciate the reviewer's constructive suggestion, and added a comparison table and more discussion for related works (see Appendix A.2.5 on page 17-18) to clarify the novelty of DISAM in the revision.\n\n**Reference:**\n \n[1]. Out-of-distribution generalization via risk extrapolation (rex), ICML2021.\n\n[2]. Invariant risk minimization, arXiv2019.\n\n[3]. Gradient matching for domain generalization, arXiv2021.\n\n[4]. Fishr: Invariant gradient variances for out-of-distribution generalization, ICML2022."
                    },
                    "title": {
                        "value": "Response to Reviewer d1EH [2/3]"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234980104,
                "cdate": 1700234980104,
                "tmdate": 1700235003902,
                "mdate": 1700235003902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rwrGpHk36N",
                "forum": "I4wB3HA3dJ",
                "replyto": "0bcLv8wwPA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer d1EH [3/3]"
                    },
                    "comment": {
                        "value": "## W2\n\n> The improvement in out-of-distribution (OOD) performance using the DISAM methodology does not appear intuitive. In fact, when comparing its performance enhancements to those achieved with CLIPOOD, as reported, the difference seems marginal. This observation raises questions about the actual effectiveness of DISAM, particularly in the context of fine-tuning methodologies.\n\nWe would like to kindly clarify that in Table 2, the results of CLIPOOD in gray are from the orginal paper [1], but we cannot reproduce these results with the open source code of [1] (even after the considerable hyparameter searching). **The best results with their open source code are reported as CLIPOOD*** and on the basis of these results, DISAM's improvement is actually not marginal (see the following table for convenience). \n\nBesides, it's crucial to highlight that **DISAM's advantages become more evident in open-class scenarios**. As can be seen in the following table (or Table 3 on page 8), DISAM notably outperforms CLIPOOD in these scenarios, achieving an average improvement of 2.8% on new classes and 1.0% on base classes. This is significant, considering that CoOp and CLIPOOD even underperform compared to zero-shot results in new classes.\n\n| **Method** | **Results on DomainBed** | **Open-class Results on Base Classes** | **Open-class Results on New Classes** |\n| ---------- | :------------------------: | :------------------------------: | :-----------------------------: |\n| Zero-shot  | 70.2                     | 72.6                           | 67.4                          |\n| CoOp       | 73.4                     | 74.4                           | 66.3                          |\n| **+DISAM** |**74.8**                    | **75.3**                       | **69.6**                      |\n| CLIPOOD*    | 77.9                      | 76.0                           | 66.9                          |\n| **+DISAM** | **78.8**                       | **77.0**                       | **69.7**                      |\n\nFor a comprehensive understanding of how DISAM enhances fine-tuning in open-class scenarios, we kindly refer the reviewer to the in-depth analysis provided in Appendix C.6 on page 25. This section substantiates DISAM's role in improving generalization in fine-tuning, especially in contexts that existing methods are challenging to achieve improvement.\n\n**Reference:**\n\n[1]. CLIPOOD: Generalizing CLIP to Out-of-Distributions, ICML2023.\n\n\n## Reproducible code\n\n> Can you provide the reproducible code during the rebuttal period?\n\nWe provide an anonymized version of the code repository, accessible through this 2-hop link: [https://openreview.net/forum?id=I4wB3HA3dJ&noteId=e1Uu30vHqy]. To promote the understanding of the code, the reviewers can also combine with Algorithm 1 and the \"Pseudo Code of DISAM\" in Appendix D on page 25-26."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235068397,
                "cdate": 1700235068397,
                "tmdate": 1700235068397,
                "mdate": 1700235068397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ku4NzSCV00",
            "forum": "I4wB3HA3dJ",
            "replyto": "I4wB3HA3dJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1547/Reviewer_mma9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1547/Reviewer_mma9"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel optimization algorithm named Domain-Inspired Sharpness Aware Minimization (DISAM) tailored for challenges arising from domain shifts. It seeks to maintain consistency in sharpness estimation across domains by introducing a constraint to minimize the variance in domain loss. This approach facilitates adaptive gradient adjustments based on the optimization state of individual domains. Theoretical and empirical findings show the proposed method offers faster convergence and superior generalization under domain shifts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe proposed method targets at the model generalization under domain shifts, which is a common challenge in machine learning. To date, there has been a lack of thorough investigation into sharpness-based optimization in the context of domain shifts, and the idea of constraint the variance of losses among training domains is interesting.\n\n2.\tThe paper not only presents theoretical evidence showcasing the efficiency of DISAM, but it also provides empirical data to support this claim, demonstrating the improved performance across various domain generalization benchmarks.\n\n3.\tThe analytical experiments conducted in this paper are comprehensive and lucid, providing evidence of DISAM's efficacy in enhancing convergence speed and mitigating model sharpness. Additionally, the study investigates the application of DISAM for fine-tuning a clip-based model, aiming to achieve improved open-class generalization."
                },
                "weaknesses": {
                    "value": "1.\tSAM-based optimization incurs twice the computational overhead and additional storage overhead in comparison to the commonly used SGD. While DISAM, the method proposed in this paper, demonstrates faster convergence under domain shift conditions when compared to SAM, it does not include a comparison with optimizers such as SGD or Adam.\n\n2.\tThis paper employs multiple benchmarks to evaluate the performance of multi-source domain generalization. The article highlights the need for advancements in the domain shift perspective of the SAM method and suggests conducting comparisons between DISAM and the state-of-the-art (SOTA) method to further validate the effectiveness of the proposed approach.\n\n3.\tThe value of $\\rho$ in DISAM significantly influences both the convergence speed and generalizability. And it needs more discussion on how to effectively determine the value to maximize the benefits of proposed method."
                },
                "questions": {
                    "value": "1.\tThe article presents a theoretical analysis suggesting that larger values of parameter $\\rho$ should lead to improved generalization, given that convergence is guaranteed. It is important to reflect this aspect in the experiments to provide stronger evidence and validation.\n\n2.\tRegarding the open class generalization of the clip-based model, further experimental analysis should be conducted to elucidate the reasons behind the superior performance of DISAM.\n\nFor other questions, please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1547/Reviewer_mma9",
                        "ICLR.cc/2024/Conference/Submission1547/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646800924,
            "cdate": 1698646800924,
            "tmdate": 1700708260209,
            "mdate": 1700708260209,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "99OFBEAASR",
                "forum": "I4wB3HA3dJ",
                "replyto": "ku4NzSCV00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mma9 [1/2]"
                    },
                    "comment": {
                        "value": "## W1\n\n> SAM-based optimization incurs twice the computational overhead and additional storage overhead in comparison to the commonly used SGD. While DISAM, the method proposed in this paper, demonstrates faster convergence under domain shift conditions when compared to SAM, it does not include a comparison with optimizers such as SGD or Adam. \n\nThank you for highlighting the point of the computational cost. We have to admit that although SAM-based methods help  improve generalization, it is ususally at the expense of approximately double the computational effort per iteration compared to standard SGD, due to the sharpness-aware perturbation. DISAM shares the similar cost as other SAM-based methods, since we target to address the domain-level inconsistency during the sharpeness estimation, instead of the training accelaration. In the revision version, we **highlight this point with the empirical verification** in Figure 8 on page 25, pointing out that compared DISAM with ERM, revealing that although DISAM converges faster than SAM, it does not outpace ERM in terms of convergence speed. **The potential extension combined with the acceleration methods like [1,2] can be explored** in the future works. \n\n**Reference:** \n\n[1]. Efficient sharpness-aware minimization for improved training of neural networks, ICLR2022.\n\n[2]. Sharpness-aware training for free, NeurIPS2022.\n\n\n\n## W2\n\n> This paper employs multiple benchmarks to evaluate the performance of multi-source domain generalization. The article highlights the need for advancements in the domain shift perspective of the SAM method and suggests conducting comparisons between DISAM and the state-of-the-art (SOTA) method to further validate the effectiveness of the proposed approach.\n\nWe would like to kindly clarify that in Table 1, we has conducted comparison with some SOTA methods [1-2] like the latest SAM-based method SAGM [1] in combination with CORAL [2]. To further address the reviewer's concern, we here add two new SOTA methods \"Decompose, Adjust, Compose\"(DAC-SC) [3] and Fishr [4] (following reviewer d1EH's recommendation). The table below presents the performance of different methods across multiple domains:\n| **Method**            | **PACS** | **VLCS** | **OfficeHome** | **TerraInc** | **DomainNet** | **Avg.** |\n| --------------------- |:--------:|:--------:|:--------------:|:------------:|:-------------:|:--------:|\n| ERM                   |   85.5   |   77.5   |      66.5      |     46.1     |     43.8      |   63.9   |\n| SAM                   |   85.8   |   79.4   |      69.6      |     43.3     |     44.3      |   64.5   |\n| **DISAM**             |   87.3   |   80.1   |      70.7      |     47.9     |     45.8      | **66.4** |\n| Fishr (ICML2022) [4]  |   86.9   |   78.2   |      68.2      |     53.6     |     41.8      |   65.7   |\n| **Fishr+DISAM**       |   87.5   |   79.2   |      70.7      |     54.8     |     43.9      | **67.2** |\n| DAC-SC (CVPR2023) [3] |   87.5   |   78.7   |      70.3      |     46.5     |     44.9      |   65.6   |\n| **DAC-SC+DISAM**      |   88.7   |   79.1   |      70.6      |     47.4     |     45.6      | **66.3** |\n| SAGM (CVPR2023) [1]   |   86.6   |   80.0   |      70.1      |     48.8     |     45.0      |   66.1   |\n| **SAGM+DISAM**        |   87.5   |   80.7   |      71.0      |     50.0     |     46.0      | **67.0** |\n\nThese results clearly illustrate the consistent improvement of DISAM on a range of methods in the field of multi-source domain generalization.\n\n**Reference:** \n\n[1]. Sharpness-aware gradient matching for domain generalization, CVPR2023.\n\n[2]. Deep coral: Correlation alignment for deep domain adaptation, ECCV2016.\n\n[3]. Decompose, Adjust, Compose: Effective Normalization by Playing with Frequency for Domain Generalization, CVPR2023.\n\n[4]. Fishr: Invariant gradient variances for out-of-distribution generalization, ICML2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234001043,
                "cdate": 1700234001043,
                "tmdate": 1700235237252,
                "mdate": 1700235237252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C9QRllkG8q",
                "forum": "I4wB3HA3dJ",
                "replyto": "ku4NzSCV00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mma9 [2/2]"
                    },
                    "comment": {
                        "value": "## W3 & Q1\n\n> W3: The value of $\\rho$ in DISAM significantly influences both the convergence speed and generalizability. And it needs more discussion on how to effectively determine the value to maximize the benefits of proposed method.\n> Q1: The article presents a theoretical analysis suggesting that larger values of parameter $\\rho$ should lead to improved generalization, given that convergence is guaranteed. It is important to reflect this aspect in the experiments to provide stronger evidence and validation.\n\nThank you for the suggestion. Following the advice, we have enriched the discussion about the value of $\\rho$ in the Appendix B.3 (page 21) of the revised version. Regarding the experiments, we actually have conducted the corresponding experiments to verify this aspect. We kindly refer the reviewer to Appendix B.3 for more details. We summarize the parts for the reviewer's questions as follows.\n\n\n- **Generalization Theorem of SAM:** In the SAM framework, the parameter $\\rho$ plays a crucial role in determining generalizability. As established in [1], there exists an upper bound on the generalization error for SAM, suggesting that **a larger $\\rho$ could potentially enhance generalization, provided that convergence is not impeded**. Here is the relevant generalization bound from [1]:\n> For any $\\rho > 0$ and any distribution $\\mathcal{D}$, with probability $1- \\delta$ over the choice of the training set $S\\sim \\mathcal{D}$,\n> $$\\mathcal{L} _{\\mathcal{D}} (w) \\leq \\max _{\\| \\epsilon\\| _2 \\leq \\rho} \\mathcal{L} _{S}(w+\\epsilon) + \\sqrt{\\frac{k \\log \\left(  1 + \\frac{\\| w \\| _2^2}{\\rho^2} (1+\\sqrt{\\frac{\\log (n)}{k}})^2 \\right) + 4 \\log \\frac{n}{\\delta} + \\tilde{O}(1)}{n-1}} $$\n> where $n = |S|$, $k$ is the number of parameters and we assumed $\\mathcal{L} _{\\mathcal{D}}(w) \\leq \\mathbb{E} _{\\epsilon_i \\approx \\mathcal{N}(0, \\rho)} [\\mathcal{L} _{\\mathcal{D}}(w+\\epsilon)]$. This theorem's proof focuses solely on the magnitude of $\\rho$, thus affirming the applicability of this theoretical framework to DISAM.\n\n- **Practical Implications:** When considering the convergence Theorem 1 on page 5 alongside the above generalization theorem, a critical trade-off emerges with respect to $\\rho$. **A larger $\\rho$ might theoretically enhance generalization but poses greater challenges for convergence**. This reflects the intuitive notion that searching for flatter minima across a broader range is inherently more complex, which can potentially affect training efficiency.\n\n- **Empirical Validation:** DISAM, with its accelerated convergence, can utilize a larger $\\rho$ while still maintaining an acceptable convergence. This advantage is empirically showcased in Figures 3(c) and (d) on page 6, where we demonstrate that DISAM effectively employs a larger $\\rho$ compared to traditional SAM. This ensures both convergence and enhanced generalization. Such a capability to balance between convergence efficiency and generalization is a distinguishing feature of DISAM over conventional SAM methods.\n\n**Reference:** \n\n[1]. Invariant risk minimization, arXiv2019.\n\n\n## Q2\n\n> Regarding the open class generalization of the clip-based model, further experimental analysis should be conducted to elucidate the reasons behind the superior performance of DISAM.\n\nThank you for the suggestion. To clarify the reasons, we complemented more analysis in Appendix C.6 on page 25 in the revised version. Generally, according to the results in Table 3 (we select some results in the following table for reference) and Figure 4, we can see: \n\n- **ERM tends to overfit to training data classes:** As shown in the table below, although CoOp and CLIPOOD perform better on base classes than zero-shot, their performance on new classes is worse than zero-shot. This suggests that the fine-tuned parameters **overfit to the existing training data distribution from both the domain and class perspectives**. Figure 4 visualizes the change in performance trends during the training process, and we observe a trend where ERM initially performs well on base classes but then exhibits a decline on new classes, suggesting a collapse of the feature space onto the training data classes.\n\n- **DISAM improves generalization on new classes:** Although SAM offers some relief from overfitting, its performance on new classes does not match zero-shot levels. In contrast, DISAM, by minimizing sharpness more effectively, shows improved performance on new classes, especially in domain shift scenarios.\n\n\n| **Method** | **Results on Base** | **Results on New** |\n| ---------- |:-------------------:|:------------------:|\n| Zero-shot  |        72.6         |        67.4        |\n| CoOp       |        74.4         |        66.3        |\n| **CoOp+DISAM** |      **75.3**       |      **69.6**      |\n| CLIPOOD    |        76.0         |        66.9        |\n| **CLIPOOD+DISAM** |      **77.0**       |      **69.7**      |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234087934,
                "cdate": 1700234087934,
                "tmdate": 1700235253358,
                "mdate": 1700235253358,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BGpXY97QRW",
            "forum": "I4wB3HA3dJ",
            "replyto": "I4wB3HA3dJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1547/Reviewer_sVwJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1547/Reviewer_sVwJ"
            ],
            "content": {
                "summary": {
                    "value": "Due to the inconsistent convergence degree of SAM across different domains, the optimization may bias towards certain domains and thus impair the overall convergence. To address this issue, this paper considers the domain-level convergence consistency in the sharpness estimation to prevent the overwhelming perturbations for less optimized domains. Specifically, DISAM introduces the constraint of minimizing variance in the domain loss. When one domain is optimized above the averaging level w.r.t. loss, the gradient perturbation towards that domain will be weakened automatically, and vice versa."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "They identify that the use of SAM has a detrimental impact on training under domain shifts, and further analyze that the reason is the inconsistent convergence of training domains that deviates from the underlying i.i.d assumption of SAM."
                },
                "weaknesses": {
                    "value": "This paper considers the domain-level convergence consistency in SAM for multiple domains, and proposes to adopts the domain loss variance in training loss. The convergence consistency is a general issue, and the solution is normal, thus the novelty is not so clear for publication in ICLR."
                },
                "questions": {
                    "value": "1.\tIn the definition of the variance between different domain losses, the values of loss between different domains are restricted. Which one is more import? The value of losses in different domains, or the minimization speed of loss in different domains?\n2.\tIn the learning of multiple domains, there is Multi-Objective Optimization, so the domain-level convergence consistency is a general issue under domain shifts? Or the convergence consistency is a general issue in Multi-Objective Optimization?\n3.\tThis paper considers the domain-level convergence consistency in SAM for multiple domains, and proposes to adopts the domain loss variance in training loss. The convergence consistency is a general issue, and the solution is normal, thus the novelty is not so clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718731804,
            "cdate": 1698718731804,
            "tmdate": 1699636082877,
            "mdate": 1699636082877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5mg5yR6jWu",
                "forum": "I4wB3HA3dJ",
                "replyto": "BGpXY97QRW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sVwJ [1/2]"
                    },
                    "comment": {
                        "value": "## Weakness & Q3\n\n> Weakness: This paper considers the domain-level convergence consistency in SAM for multiple domains, and proposes to adopts the domain loss variance in training loss. The convergence consistency is a general issue, and the solution is normal, thus the novelty is not so clear for publication in ICLR.\n> Q3: This paper considers the domain-level convergence consistency in SAM for multiple domains, and proposes to adopts the domain loss variance in training loss. The convergence consistency is a general issue, and the solution is normal, thus the novelty is not so clear.\n\n\n\n**Differences from general convergence consistency issue:**\n- **Distinct focus:** DISAM focuses on the issue where **SAM-based methods are unable to accurately estimate sharpness in domain shift scenarios**, leading to the ineffective sharpness minimization and reduction in generalization performance.\n- **Enhancing on top of general methods:** While traditional solutions[1,2,3] aim at convergence consistency in parameter optimization, DISAM's methodology is distinct and orthogonal. It builds upon methods like V-REx[2] and Fishr[3], but **goes further in enhancing out-of-domain generalization through better sharpness minimization**. This is evident in our experiments, where combining DISAM with Fishr results in significant performance gains (as shown in the table below).\n\n| **Method**      | **PACS** | **VLCS** | **OfficeHome** | **TerraInc** | **DomainNet** | **Avg.** |\n| --------------- |:--------:|:--------:|:--------------:|:------------:|:-------------:|:--------:|\n| V-REx[2]        |   84.9   |   78.3   |      66.4      |     46.4     |     33.6      |   61.9   |\n| **V-REx+DISAM** |   85.8   |   78.4   |      70.5      |     45.9     |     42.3      |   64.6   |\n| Fishr[3]        |   86.9   |   78.2   |      68.2      |     53.6     |     41.8      |   65.7   |\n| **Fishr+DISAM** |   87.5   |   79.2   |      70.7      |     54.8     |     43.9      | **67.2** |\n\n\n**Novelty and contributions:**\n- We first identify that the straightforward application of SAM has **a detrimental impact on training under domain shifts** (as shown Figure 1 and Table 1). Specifically, we observed that the way SAM generates perturbation directions amplifies the inconsistency in convergence between domains, leading to inaccurate sharpness estimation and making sharpness minimization less effective.\n- DISAM handle the above challenge by imposing a variance minimization constraint on domain loss **during the sharpness estimation process**, thereby enabling a more representative perturbation location and enhancing generalization. \n- The **significant improvements in extensive experimental results** (as shown in Table 1-3) validate DISAM's novelty and practical relevance.\n\n**Reference:**\n\n[1]. Invariant risk minimization, arXiv2019.\n\n[2]. Out-of-distribution generalization via risk extrapolation (rex), ICML2021.\n\n[3]. Fishr: Invariant gradient variances for out-of-distribution generalization, ICML2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233543088,
                "cdate": 1700233543088,
                "tmdate": 1700235214289,
                "mdate": 1700235214289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DMlhLHgQhm",
                "forum": "I4wB3HA3dJ",
                "replyto": "BGpXY97QRW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sVwJ [2/2]"
                    },
                    "comment": {
                        "value": "## Q1\n\n> In the definition of the variance between different domain losses, the values of loss between different domains are restricted. Which one is more import? The value of losses in different domains, or the minimization speed of loss in different domains?\n\n\nFirstly, we are sorry that one typo in Eq. (7) has mislead your understanding. In the revised manuscript, we have addressed such a typo issue in the description of Eq.(7) as follows.\n\n$$\n\\min _{w} \\mathbb{E}  _{\\xi \\in \\mathcal{S}} [\\mathcal{L} _{DISAM}(w;\\xi)] \\triangleq \\min _{w}  \\max _{ \\| \n \\epsilon \\| _2 \\leq \\rho} \\left [ \\sum _{i=1} ^M \\alpha _i \\mathcal{L} _i (w + \\epsilon) -  \\lambda \\text{Var}\\{\\mathcal{L} _i(\\hat{w} + \\epsilon)\\} _{i=1} ^M \\right]\n$$\n\nHere **$\\hat{w}$ is $w$ without derivative taken during optimization**, and it only makes effect in the $\\max_{\\| \\epsilon\\|_2 \\leq \\rho}$ loop without affecting the first term.\n\n- **Variance Minimization Focus:** DISAM primarily focuses on minimizing variance during the generation of perturbation directions $\\epsilon$. The outer optimization w.r.t. $w$ does not involve a trade-off between the empirical loss term and the variance term as we enforce $\\hat{w}$ (assigned by $w$) without derivative taken.\n- **Crucial Role of Minimizing Variance:** Minimizing domain-level variance in the perturbation generation loop is critical. Our experiments, illustrated in Figures 5(c) and 5(d) on page 9, show a marked decrease in generalization performance when $\\lambda=0$, confirming its essential effectiveness in DISAM. Furthermore, DISAM exhibits a robust performance across a broad range of $\\lambda$ values.\n\n\n## Q2\n\n> In the learning of multiple domains, there is Multi-Objective Optimization, so the domain-level convergence consistency is a general issue under domain shifts? Or the convergence consistency is a general issue in Multi-Objective Optimization?\n\n**Differences from multi-objective optimization:**\n- The research topic is intrinsically different from multi-objective optimization. Specially, the goal of DISAM has a single ultimate objective, improving the generalization performance of the model trained under multiple domains. This emphasizes both in-domain generalization and out-of-domain generalization, while multi-objective optimization usually refers to improving the multiple objectives of all collaborated tasks.\n- Methodologically, we still use the SAM optimization framework, and **do not involve multi-objective optimization process during training**. Specifically, $\\{\\alpha_i=n_i/N\\}_{i=1}^M$ are constant in Eq.(7) unlike the task-specific variables to be learnt in multi-objective optimization. We observed the negative impact of domain-level convergence inconsistency on SAM-based methods during the perturbation direction generation process. DISAM achieves better perturbation directions for out-of-domain generalization by minimizing the variance of the domain losses."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233697792,
                "cdate": 1700233697792,
                "tmdate": 1700233697792,
                "mdate": 1700233697792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]