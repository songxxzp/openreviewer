[
    {
        "title": "Benchmarking Large Language Models as AI Research Agents"
    },
    {
        "review": {
            "id": "brGSVfcZ3Y",
            "forum": "N9wD4RFWY0",
            "replyto": "N9wD4RFWY0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission780/Reviewer_CHg2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission780/Reviewer_CHg2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a benchmark for evaluating large language models as AI research agents. The benchmark encompasses multiple modalities and a variety of tasks. Each task consists of reading a problem description and taking actions in an environment defined by the authors. Most of the problems are taken from Kaggle, with care taken to ensure that at least some of the problems were put online recently, so LLMs haven't had a chance to train on them. The authors then evaluate GPT-4 and Claude-1 on the benchmark, as well as agentic frameworks built around these models, such as AutoGPT and LangChain-React."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is original, novel and addresses a significant research question. The benchmark covers a range of tasks, and the largest API-only LLMs (GPT-4, Claude-1) are evaluated. The evaluation of agentic frameworks like AutoGPT and LangChain-React is also useful, and surprising \u2014\u00a0I expected them to be better."
                },
                "weaknesses": {
                    "value": "I have concerns about the relevance of the tasks chosen with respect to \"AI Research\". Some of the tasks are hardly \"research tasks\":\n- CIFAR10\n- IMDB\n- Kaggle House Prices\n- Kaggle Spaceship Titanic\n\nI'm skeptical any research is being done on the Kaggle House prices dataset or the Spaceship Titanic dataset. \n\nThe best possible action if pushing performance on CIFAR10 and IMDB at this point would be to load pretrained models from HuggingFace or TIMM and run it on CIFAR 10/IMDB, which is not research. \n\nThe LLM Tools section does not seem possible to evaluate quantitatively, and the relevance of `bibtext-generation` to AI research seems tenuous. \n\nThis means that 6/15 proposed tasks are not directly relevant to ml research. \n\nI understand that the motivation was \n> The tasks are chosen such\nthat they range in various difficulties and recency to test the generalizability of the research agent and\navoid data contamination. \n\nBut I don't see anything to constrain the actions of the agent. If the task is to improve the baseline model, can I \"improve\" it by sticking it as one branch of a model that uses a much larger pretrained model? Why / why not? \n\nRight now, the problem I see is that the work is a collection of ~15 already existing tasks, with ~6 of them having arguable relevance to contemporary ML research. There needs to be additional work on top of this to make it a generally usable benchmark, such as constraining the actions of the agent, or defining each task more precisely, or adopting a principled definition of a research workflow and only including tasks based on that."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission780/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712518833,
            "cdate": 1698712518833,
            "tmdate": 1699636005617,
            "mdate": 1699636005617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aKuN1YJ7FR",
                "forum": "N9wD4RFWY0",
                "replyto": "brGSVfcZ3Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback and identifying the novelty in the work! We comment on the weakness below:\n\n> I have concerns about the relevance of the tasks chosen with respect to \"AI Research\"...\n\nPlease see our overall response in the official comments. \n\n> But I don't see anything to constrain the actions of the agent\u2026\n\nYes we do not constrain the agent from importing pre-trained models if that gives better performance. And we agree in general that the agents mainly do well on older tasks like cifar10 or spaceship-titanic which the LM has seen during pretraining but lack of lot of creativity as seen in some newer and more challenging tasks in our benchmark such as CLRS and BabyLM which are actual open research problems. These older tasks were added to show the spread in the type of tasks agents are capable of doing and some promise in improving current agents for harder novel tasks. We view our work to give people a sense of the current state of agents on ML engineering tasks which involve a lot of open-ended decision making. \n\nThe actions were all the actions that an actual human could have taken to solve that particular problem, and hence are open-ended. The easier tasks were added to show the capabilities of current agents as opposed to having a suite a super hard tasks where all agents have a 0 performance (aren\u2019t able to making any meaningful progress in, such as BabyLM). Thus, the 15 tasks in the benchmark are not static and can evolve very easy as agent capabilities increase. We wanted to have tasks which had a decent spread of success rates so that current agents can be compared as opposed to all very easy or very hard tasks only which doesn\u2019t make a lot of sense for a benchmark.\nFurthermore, our novel framework ensures that adding a task to the benchmark is very seamless (very disentangled from a lot of underlying agent code) which would encourage open-source contributions, and hence would make the benchmark more wholesome and varied in the types of task to be tested. We hope our benchmark can provide valuable foundation for benchmarking more complex types of research in future work. One could simply add the research problem they are working on in our benchmark in <10 mins but it is very unlikely that the agent will show a lot of creativity to make meaningful progress in it (and hence the need for a benchmark to track this)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900816892,
                "cdate": 1699900816892,
                "tmdate": 1699900816892,
                "mdate": 1699900816892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e7G8ayDWnJ",
                "forum": "N9wD4RFWY0",
                "replyto": "aKuN1YJ7FR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_CHg2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_CHg2"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the response in the official comments. My issue continues to be with the design and meaningfulness of the benchmark. Let's ignore the issues around whether the benchmark constitutes research or not and focus on the classical ML engineering tasks. Is the current design enough to draw any conclusions?\n\nThere's a total of 5 tasks in the \"classical\" tier + \"classical Kaggle\" tier. First, that's a small number of evaluations. Within those three tasks, there is a huge variation between each model's performance and across the tasks. What conclusions should be drawn from this? See Fig 3. Is GPT-4 _significantly_ better at writing experiments for graph node classification (ogbn-arxiv) but mysteriously _much worse_ at tabular house price classification (house-price) compared to Claude-1? Why is IMDB so hard for all of the models? Is it that text classification is just intrinsically way harder to improve on than every other task?\n\nSome of these conclusions seem intuitively implausible given their specificity, but this is all we get from the current size / combination of tasks. If there was more than one task for, let's say, image classification in the \"classical tier\" (ideally 5+), we could draw stronger conclusions. Given that it's just image classification, why not plug in a bunch of small image classification datasets? Same for all the other tasks. Since a selling point of the proposed framework is that that adding another task to the benchmark is very easy, why such a small number of tasks in the \"classical\" tier?\n\nWe also don't know what \"simple baseline\" agent performance is. Let's say I took an agent and just told it to increase the width / depth of each starter model for all the classical tasks. Let's say a human wrote the this \"simple baseline\" solution for each task as well. How well would it do? This is what I was hinting at by \"constraining actions\". This is important to know because it gives us a sanity check for (a) the intrinsic difficulty of each task (b) the difficulty of following a prompt in the environment. Without these sanity checks, interpreting the results from the benchmark is difficult.\n\nI like the idea of the benchmark and think the initial analysis is interesting, but important pieces are missing. It's hard to see what conclusions you can draw by running the benchmark currently."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722155638,
                "cdate": 1700722155638,
                "tmdate": 1700722155638,
                "mdate": 1700722155638,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dnKbZsct2i",
            "forum": "N9wD4RFWY0",
            "replyto": "N9wD4RFWY0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission780/Reviewer_UyZC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission780/Reviewer_UyZC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new benchmark, MLAgentBench, for evaluating AI research agents that can perform open-ended ML tasks given a task description and a dataset. \nThe benchmark provides a set of diverse and tasks, an environment where agents can interact with files and execute code, and metrics to measure the competence, reasoning, and efficiency of the agents. \nThis paper tests several famous LLM and presents the experimental results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It proposes a novel and general framework for specifying and evaluating AI research agents on ML tasks. \nIt tests the feasibility and effectiveness of using common LLMs as AI research agents."
                },
                "weaknesses": {
                    "value": "The testing scenario is simplistic and greatly differs from the demands of the real world to some extent, which limits its ability to accurately reflect the true capabilities of the model."
                },
                "questions": {
                    "value": "Why is there no AutoGen test which should be much stronger than AutoGPT? \nIf the output and input exceed the LLM context length during the process, how can it be resolved?\nWill the model being tested receive feedback from the evaluation results of the submission? Will it then continue to carry out operations to improve the results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission780/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission780/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission780/Reviewer_UyZC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission780/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757783552,
            "cdate": 1698757783552,
            "tmdate": 1699636005541,
            "mdate": 1699636005541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "epk8MacTWy",
                "forum": "N9wD4RFWY0",
                "replyto": "dnKbZsct2i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback and identifying the novelty in the work! We answer the questions asked and comment on the weakness about the testing scenario being too simple below:\n\n> The testing scenario is simplistic and greatly differs from the demands of the real world to some extent, which limits its ability to accurately reflect the true capabilities of the model.\n\nPlease see our general response in the official comments.    \n\n> Why is there no AutoGen test which should be much stronger than AutoGPT? If the output and input exceed the LLM context length during the process, how can it be resolved? Will the model being tested receive feedback from the evaluation results of the submission? Will it then continue to carry out operations to improve the results?\n\nWe thank the reviewer for pointing out several important aspects for clarification. \nWe believe AutoGen was released after we finished most of the evaluation, and adding it will require more budget. We leave testing these new agent frameworks to our future work.\nWhen input exceeds the LLM context length, we simply declare it as terminated and only evaluate the progress done so far. We generally try resolve it by summarizing the observations that are too long with another LLM.\nIn this work, we treat the evaluation results of the submission as a final testing step and therefore do not propagate this feedback to the LLM. The AI research agent would have had plenty of chances to receive performance feedback from the eval split of the dataset instead, and the agent is supposed to improve results base on this feedback."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900503613,
                "cdate": 1699900503613,
                "tmdate": 1699900503613,
                "mdate": 1699900503613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oqcd2wVJ9f",
                "forum": "N9wD4RFWY0",
                "replyto": "epk8MacTWy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_UyZC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_UyZC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for the author's response which is very helpful in understanding the questions. The reviewer believes that benchmarking the single iteration result of agents is necessary, but still maintain that research should be a multi-round process. Therefore, learning from the evaluation feedback signals to the agent for further iterations carries significant evaluation value. \n\nFor research agents, the evaluation should not only consider whether the final performance improvements. Evaluating research ideas should also play a significant role in the assessment, and this is a very challenging aspect. However, currently, this part is entirely assessed manually, which diminishes the value of the benchmark. \n\nFurthermore, concerning the content of B1 in the supplementary material, it's a bit of challenging to claim that the output is a research plan but more like a parameter tuning / structure searching plan to some extend. Therefore, this benchmark seems more like a benchmark for model performance optimization agents rather than a research agents in a general meaning."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309252167,
                "cdate": 1700309252167,
                "tmdate": 1700309252167,
                "mdate": 1700309252167,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dFwXqmmUn8",
            "forum": "N9wD4RFWY0",
            "replyto": "N9wD4RFWY0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission780/Reviewer_TJMi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission780/Reviewer_TJMi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a benchmark for AI research tasks for large language model (LLM) based agents. They collect 15 tasks from various machine learning problems, ranging from \u201csolved\u201d tasks like cifar10 to \u201cunsolved\u201d tasks like BabyLM. Tasks consist of initial code and instructions to solve or improve performance for a given problem. They design baseline agents and assess their ability using the benchmark, showing that they are able to achieve >10% performance increases for many of the tasks, with varying success depending on model used, task type, and agent design. They perform a number of studies on the types of mistakes agents make, and the efficiency of agents in terms of tokens used."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The ability of LLMs to conduct research is an interesting question that seems to follow naturally from the focus on the reasoning ability of LLMs. A benchmark to measure this ability could be a valuable contribution. This paper does a lot of work analyzing the types of errors agents make, which is also valuable for the community."
                },
                "weaknesses": {
                    "value": "Overall, while this paper contains some interesting findings and raises an interesting question, I feel that it needs more polishing before it\u2019s ready for publication.\n\n1. Though the main point of the paper is proposing a benchmark, there is relatively little information on how these tasks were selected and how the benchmark was constructed. The benchmark consists of only 15 tasks, and while there is some detail on the attributes that made the authors select these tasks, there are not enough to convince me that this is a thorough enough benchmark in its current state. Issues such as potential data contamination, formalizing what aspects of research are being tested and how these are addressed by the chosen tasks should be explained further.\n2. Relatedly, the framing of this as an AI benchmark is slightly misleading. It tests LLM ability to complete research in empirical machine learning tasks, but does not test their ability to do other types of research (e.g. more qualitative or descriptive research not based purely around achieving performance increases). Though some elements of the process may remain the same, 15 tasks from machine learning are not enough to test an agent\u2019s overall research ability.\n3. The success criteria for these tasks involves models improving performance by >10%. This strongly limits the types of tasks that can be tested, as discussed in the previous point. However, it also provides limited insight into the performance of the model. One of the strengths of this paper is the analysis of the types of errors models make, but this is all done through human evaluation. For tasks involving many reasoning steps where lots can go wrong, it would be more valuable to see some breakdown of which step the model failed at or some other type of automated error analysis."
                },
                "questions": {
                    "value": "1. Did you explore how likely it is that these tasks and their solutions appeared in training data? For example, for solved tasks, it seems likely that models would have seen high performing solutions already. Did you study this at all, either by looking at the data or comparing the similarity of model output to existing solutions?\n2. Was human annotation performed by the authors? What was the criteria for annotation? If it was not performed by the authors, how were annotators recruited/paid?\n3. What aspects of the tasks chosen make them suitable for testing research in general? What kind of research do you expect an agent achieving perfect performance on your benchmark to achieve?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission780/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission780/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission780/Reviewer_TJMi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission780/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839454415,
            "cdate": 1698839454415,
            "tmdate": 1699636005443,
            "mdate": 1699636005443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qJ5bY3Hpwb",
                "forum": "N9wD4RFWY0",
                "replyto": "dFwXqmmUn8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback and recognising the usefulness of having a benchmark to keep track of the progress of LLMs to perform open-ended machine learning tasks. We have some comments about the weaknesses highlighted about the paper:\n\n> there is relatively little information on how these tasks were selected and how the benchmark was constructed. \n\nThanks for the suggestion for clarifying how tasks are selected. As noted in section 2.4, \u201cthe tasks are chosen such that they range in various difficulties and recency to test the generalizability of the research agent and avoid data contamination\u201d. To avoid the chance of data contamination, we selected Kaggle competitions that were launched only in the last 2-10 months at the time. We also narrowed the types of tasks to empirical machine learning tasks to specifically test the ability of AI research agents on these well-defined evaluable tasks. The tasks were hence chosen so that current agents can show some spread in performance on the benchmark.\n\n> It tests LLM ability to complete research in empirical machine learning tasks, but does not test their ability to do other types of research.\n\nPlease see the overall response in the official comments. Indeed, our benchmark focuses on the problem of having research agents develop/edit ML models given a task description and a dataset, since the final result can easily evaluated. In contrast, the broader types of research are generally difficult to evaluate objectively and automatically. We leave this as an important future work direction. Please also see our further response in official comments.    \n\n> It would be more valuable to see some breakdown of which step the model failed at or some other type of automated error analysis.\n\nOur success criteria of improving performance by >10% indeed only shows the broad stroke of the performance of AI research agents. However, we argue that this metric still successfully sets precedence in the evaluation of AI research agents. We have also proposed process-based evaluation, but it is one of the most challenging aspects of the agent evaluation pipeline. As mentioned in the paper, we have attempted to automate this evaluation process with LLMs such as GPT3.5, but the result is too inaccurate to be used. Thus we provide our human error analysis to shed light on the importance of such process evaluation but leave its automation to future work.\n\nTo answer your questions about the work:\n\n> Did you explore how likely it is that these tasks and their solutions appeared in training data?\n\nThe models we used are claude and gpt4, for which we have no information about the training data. So we try to avoid data contamination by selecting recently lauched Kaggle data (within 2-10 months) and current research benchmarks. \n\n> Was human annotation performed by the authors? What was the criteria for annotation?\n\nThe error analysis is performed by ourselves. The failure modes were carefully chosen by manually inspecting agent logs while also ensuring that those error modes captured some more fundamental limitations of LLM agents. For common failure modes, we outline when is an agent run classified into that mode:\nHallucination: We observe that the agent changes part of the code which degrades performance, but still claims that it performed the said task and takes the \u201cFinal Answer\u201d action incorrectly.\nDebugging: We often notice that for our engineering type of tasks, being able to debug code was very important. Very often, agents would get stuck in trying to debug a piece of code it wrote and exceed the max steps permitted.\nBad Plan: A run has a bad plan if the initial research plan of the agent itself does not help solve the goal. \nWe will include these detailed descriptions in our appendix.\n\n> What aspects of the tasks chosen make them suitable for testing research in general? What kind of research do you expect an agent achieving perfect performance on your benchmark to achieve?\n\nThe tasks we chose generally involve building an ML model on a given dataset for as high performance as possible (Note that >10% is an arbitrary threshold we choose to measure the baseline agents, but this threshold could be arbitrarily high instead as a more challenging scenario). Basic improvements could be simply achieved by applying existing knowledge on hyperparameter tuning, data augmentation, architecture changes etc. This allows even naive agents to start demonstrating performance early on. If an agent achieves perfect performance on our benchmark, the agent would have to be very good at ML engineering (which requires software eng skills + knowledge of a lot of state-of-the-art ML methods) to better performance. Furthermore, CLRS and BabyLM in our benchmark are actual open research problems and having an agent solve those would indicate some potential of actually being able to do \u201cresearch\u201d. To the extreme, the perfect AI research agent should be able to achieve groundbreaking research."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900045174,
                "cdate": 1699900045174,
                "tmdate": 1699901211228,
                "mdate": 1699901211228,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IrP0r5K0xH",
                "forum": "N9wD4RFWY0",
                "replyto": "qJ5bY3Hpwb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_TJMi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_TJMi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. While this did clear up my questions regarding data contamination and annotation, my overall concerns about the benchmark remain. My main objection is to the idea that this benchmark tests research ability. If this is the goal of the benchmark, \"research\" needs to be rigorously formalized, even within the context of ML tasks, to avoid giving the wrong impression about what this benchmark tests (especially since \"research\" is a very broad concept).\n\nIn my view, while these tasks represent a form of research, they're more accurately described as ML engineering tests (something that your main response seems to agree with). This is not to say that engineering can't be research, but it is by no means a comprehensive research benchmark, even for empirical ML. In its current form, this benchmark tests a very specific type of quantitative, performance-oriented machine learning research ability. As easy as it may be to add new tasks, I don't see how the evaluation procedure can be easily extended to other types of tasks (e.g. tasks without concrete performance thresholds to improve).\n\nYou mention components of research, for example, iterating over solutions, in your main reply. However, there is nothing in this benchmark that requires LLMs to follow this process. As reviewer CHg2 pointed out, if the best solution is loading a pre-trained model rather than iterating over solutions, the model may just do this. While it could be argued that this does demonstrate a research skill (prioritizing efficiency and knowing existing work), it isn't testing the model's ability to make novel discoveries and learn from past mistakes. Again, this may be acceptable, but it has to be clearly defined in your definition of what research is and the skills that your tasks are designed to test. What qualities specifically made you choose these tasks, beyond recency & concerns about data contamination? What skills do they require a model to demonstrate? What would it mean for a research agent to generalize? The properties of an ideal research agent are not defined clearly, and this leaves too much room for interpretation.\n\nIf an agent achieves perfect performance on your benchmark, what can we expect from it? Should we expect that it can replace a senior scientist? A PhD student? An ML engineer? How do you define research? Which facets of that definition are tested by this benchmark, and which are not? This is the kind of operationalization I would need to see---both for the definition of research and the tasks selected---to justify accepting this benchmark."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218385276,
                "cdate": 1700218385276,
                "tmdate": 1700218385276,
                "mdate": 1700218385276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4HXvTXS57S",
                "forum": "N9wD4RFWY0",
                "replyto": "vIMxC0fcqr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_TJMi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_TJMi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for another thorough response. While this does add clarity to what you hope this benchmark will achieve, I still believe that calling it a benchmark for ML research is misleading. We agree that full fledged scientific discovery is not tested by this benchmark, and also seem to agree that ML-engineering skills are. If this benchmark was described as testing ML-engineering capabilities, I would have significantly less complaints, as it fits with what is actually shown. CLRS and BabyLM are indeed open research problems, and I see how an agent that does well on this benchmark may be able to contribute to research tasks like these, but it is much more accurate as a whole to say that you are testing ML-engineering agents. \n\nRegarding iterating over solutions, I recognize that the agent tested did iterate. However, this does not mean there is something inherent to the tasks that requires them to iterate. Whether iteration is something you want to test or not goes back to the question of how research is formalized, but the same holds for any other skills going into your definition of research ability. In general, I see that providing guarantees on what tasks require may not be possible, however, describing in more detail what skills tasks are designed to test (e.g., going beyond saying you selected recent tasks and describing that X tasks test skill A, Y test skill B, etc.) would help to demonstrate that the benchmark is covering sufficiently diverse tasks.\n\nIn its current form, I don't feel this paper is ready to be published at ICLR, so I will keep my score the same. However, I encourage the authors to continue to refine the paper as I do believe the benchmark holds value."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709966757,
                "cdate": 1700709966757,
                "tmdate": 1700709966757,
                "mdate": 1700709966757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jtyrHydl9q",
            "forum": "N9wD4RFWY0",
            "replyto": "N9wD4RFWY0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission780/Reviewer_5hPW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission780/Reviewer_5hPW"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new benchmark for AI agents that conduct AI research, called MLAgentBench. The authors provide a baseline algorithm based on GPT-4 which achieves some successes on various tasks within the benchmark, but which also demonstrates the large gap between the state-of-the-art and expert performance, inspiring future research in this area. The code for the paper is open-sourced, enabling researchers to reproduce these results and build new models that push the frontier of AI research agents."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The authors provide a novel, rigorous, open-source evaluation benchmark to galvanise and focus the community on the important topic of AI research agents. \n- The benchmark includes a well-chosen range of tasks, which represent a diverse range of machine learning challenges, from standard problems to more difficult Kaggle challenges. \n- The paper is very well-motivated, and the descriptions are clear and lucid throughout. \n- The authors provide a state-of-the-art agent, alongside baselines and ablations. The \"no-retrieval\" ablation, for instance is a good choice that enables the reader to establish scientific intuition and may inspire future work. \n- Qualitative analyses of agent behavior provides an insight into the capabilities and limitations of the agents."
                },
                "weaknesses": {
                    "value": "- In the Results section figures, it is unclear what is meant by \"baseline\". Could the authors please clarify which model this is? It would be good to make this clear in the figure captions. \n- It would be useful to have a slightly longer description of each of the MLAgentBench tasks in Table 1. At the very least, a few words in the caption explaining the definitions of the terms used in the \"task\" column (e.g. node classification, improve speed, implement tool) would be very useful. \n- The future work section is too thin for a benchmark paper. Can the authors provide some more suggestions for possible future lines of agent research? For this paper to be maximally impactful they should take this opportunity to provide researchers in the field with some inspiration to drive rapid progress on this benchmark!"
                },
                "questions": {
                    "value": "See \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission780/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699129220479,
            "cdate": 1699129220479,
            "tmdate": 1699636005377,
            "mdate": 1699636005377,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nmjZ3DelGE",
                "forum": "N9wD4RFWY0",
                "replyto": "jtyrHydl9q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thanks a lot for identifying merits in our work to have a benchmark to keep track of the capabilities of Agents to perform machine learning tasks. Below, we address some of the highlighted weaknesses:\n\n> It is unclear what is meant by \"baseline\"\n\nAs mentioned in 2.1, \u201cThe starter code mostly implements a simple baseline model that we can compare with during evaluation\u201d. We will add this clarification to the experiment section and caption.\n\n> It would be useful to have a slightly longer description of each of the MLAgentBench tasks in Table 1\n\nWe will include clarification of each type of task in the caption, i.e. node classification task refers to classifying node in a networks to different classes; improve speed task refers to improving the run time of a given script; implement tool refers to implementing an LLM based helper script such as performing literature review. We will also add a more detailed per task description in the appendix.\n\n> The future work section is too thin for a benchmark paper.\n\nGood point! We will include a separate section detailing future works including: 1) improving agent architecture, such as incorporating literature review, structured memory, better hierarchical actions and even employing multiple subagents to divide up the responsibilities; 2) incorporating more tasks from different domains, such as performing experiments in biology, chemistry, and physics; 3) improving the evaluation pipeline, such as automatic evaluation of agent reasoning process and evaluating agent in more general research scenarios."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699899755320,
                "cdate": 1699899755320,
                "tmdate": 1699899755320,
                "mdate": 1699899755320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Iy28AqImGF",
                "forum": "N9wD4RFWY0",
                "replyto": "nmjZ3DelGE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_5hPW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission780/Reviewer_5hPW"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. I agree with them that this is a good starting point for a benchmark on the engineering side of AI research. While I am sympathetic to some of the concerns of the other reviewers, what stands out to me here is that this is a well-written paper with good motivation, strong open-source contribution and which provides a much-needed starting point for future work in this area. In my view, this makes it a strong paper, in spite of the tasks not yet capturing the full range of AI research possibilities. So I continue to strongly recommend acceptance here. \n\nA point of advice to the authors: it is very effective to make the changes you promise in an updated version of the PDF which you upload to OpenReview for the reviewers to comment on. As far as I can tell, you have not yet incorporated my feedback into the PDF. I trust that you will do so in time, but for future work, please consider updating the PDF during the rebuttal period."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission780/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671606469,
                "cdate": 1700671606469,
                "tmdate": 1700671606469,
                "mdate": 1700671606469,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]