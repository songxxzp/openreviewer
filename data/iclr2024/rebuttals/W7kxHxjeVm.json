[
    {
        "title": "ImAD: An End-to-End Method for Unsupervised Anomaly Detection in the Presence of Missing Values"
    },
    {
        "review": {
            "id": "r3KWZ1fCpl",
            "forum": "W7kxHxjeVm",
            "replyto": "W7kxHxjeVm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2629/Reviewer_Rb71"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2629/Reviewer_Rb71"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses the challenge of anomaly detection in data with missing values. The authors propose a novel approach called ImAD that combines data imputation and anomaly detection in a unified framework. The proposed method generates pseudo-abnormal samples from the latent space to mitigate the imputation bias that the traditional two-stage methods tend to exhibit. Experimental results show that it outperforms baseline methods in common scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The study on anomaly detection in datasets with missing values represents an underexplored area within the traditional field of anomaly detection. The recognition of imputation bias in the two-stage methods seems well-founded. The methodology for generating pseudo-anomalies in the latent space appears to be sound."
                },
                "weaknesses": {
                    "value": "- Section 2.2 introduces a few related studies on anomaly detection with incomplete data, but these methods are not included in the experimental evaluation. The results primarily rely on conventional approaches that do not explicitly account for missing data, such as DeepSVDD. \n- Experimental scenarios are too limited. Only two datasets are tested, while there exist many other benchmark datasets available for anomaly detection. The choice of two specific missing rates (0.2 and 0.5) may not realistically represent the practical scenarios. Only AUROC results are presented, omitting other essential metrics. \n- Although this paper includes some theoretical analysis in section 3.6, the derivations of the inequalities appear to be straightforward in the context of plain neural nets. It remains unclear whether these inequalities provide more valuable insights than those derived from other related methods, especially on practical utility."
                },
                "questions": {
                    "value": "- In Figure 1, OC-SVM outperforms IForest across different missing rates, but OC-SVM is excluded in other experiments. Can you provide a rationale for this selectiveness?\n- Can you provide results using additional performance metrics such as AUPRC?\n- Justification on the use of the sinkhorn divergence in the loss function will be helpful.  \n- It would be interesting to see some results on the robustness in relation to the values of r1 and r2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2629/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698584736885,
            "cdate": 1698584736885,
            "tmdate": 1699636202563,
            "mdate": 1699636202563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "URKIV4fOCD",
                "forum": "W7kxHxjeVm",
                "replyto": "r3KWZ1fCpl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2629/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2629/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**Comment 1**: Section 2.2 introduces a few related studies on anomaly detection with incomplete data, but these methods are not included in the experimental evaluation. The results primarily rely on conventional approaches that do not explicitly account for missing data, such as DeepSVDD.\n\n**Response**: Thanks for your comments. Actually, the related methods ([Zemicheal & Dietterich 2019], [Fan et al. 2022], and [Sarda et al. 2023]) discussed in Section 2.2 are all two-stage methods: perform missing data imputation first and then conduct anomaly detection. A few two-stage methods are considered in our paper. We used two strong missing data imputation methods including MissForest [1] and GAIN [2] and three strong anomaly detection methods including IForest [3], Deep SVDD [4] and NeutraL AD [5]. Our end-to-end method outperformed all two-stage methods.\n\nReferences:\n\n[1] Stekhoven, Daniel J., and Peter B\u00fchlmann. \"MissForest\u2014non-parametric missing value imputation for mixed-type data.\" Bioinformatics 28.1 (2012): 112-118.\n\n[2] Yoon, Jinsung, James Jordon, and Mihaela Schaar. \"Gain: Missing data imputation using generative adversarial nets.\" International conference on machine learning. PMLR, 2018.\n\n[3] Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. \"Isolation forest.\" 2008 eighth ieee international conference on data mining. IEEE, 2008.\n\n[4] Ruff, Lukas, et al. \"Deep one-class classification.\" International conference on machine learning. PMLR, 2018.\n\n[5] Qiu, Chen, et al. \"Neural transformation learning for deep anomaly detection beyond images.\" International Conference on Machine Learning. PMLR, 2021.\n\n**Comment 2**: Experimental scenarios are too limited. Only two datasets are tested, while there exist many other benchmark datasets available for anomaly detection. The choice of two specific missing rates (0.2 and 0.5) may not realistically represent the practical scenarios. Only AUROC results are presented, omitting other essential metrics.\n\n**Response**: We conducted experiment on **five** more publicly-available real-world datasets and added AUPRC as a new evaluation metric. For the selection of missing rates (0.2 and 0.5), we followed the previous data imputation works [1,2,3].\n\nReferences:\n\n[1] Yoon, Jinsung, James Jordon, and Mihaela Schaar. \"Gain: Missing data imputation using generative adversarial nets.\" International conference on machine learning. PMLR, 2018.\n\n[2] Camino, Ramiro D., Christian A. Hammerschmidt, and Radu State. \"Improving missing data imputation with deep generative models.\" arXiv preprint arXiv:1902.10666 (2019).\n\n[3] Li, Yuxuan, Ayse Dogan, and Chenang Liu. \"Ensemble Generative Adversarial Imputation Network with Selective Multi-Generator (ESM-GAIN) for Missing Data Imputation.\" 2022 IEEE 18th International Conference on Automation Science and Engineering (CASE). IEEE, 2022.\n\n**Comment 3**: Although this paper includes some theoretical analysis in section 3.6, the derivations of the inequalities appear to be straightforward in the context of plain neural nets. It remains unclear whether these inequalities provide more valuable insights than those derived from other related methods, especially on practical utility.\n\n**Response**: Please take a look at Figure 4 of our latest manuscript. In Figure 4, we visualize the projection results of Botnet dataset in 2-D space. It can be observed that the majority of normal training and testing samples are mapped into the target distribution while most abnormal samples fall outside of the decision boundary. This demonstrates that our method is practical and Theorem 3.1 is effective for real-world scenarios.\n\n**Question 1**: In Figure 1, OC-SVM outperforms IForest across different missing rates, but OC-SVM is excluded in other experiments. Can you provide a rationale for this selectiveness?\n\n**Response**:  First of all, in our main experiments, our method achieves better detection performance than the strong baselines Deep SVDD [1] and NeutraL AD [2] and both of them outperformed OC-SVM in their previous works. Therefore, OC-SVM is not a necessary baseline for our work. The second reason is that, our experiments include diverse experimental settings including different missing rates, three missing mechanisms, different data splitting, and different combinations of imputation and detection methods and OC-SVM has quadratic time and space complexities (very slow on large datasets such as KDD). Therefore, we do not include OC-SVM in our experiments.\n\nReferences: \n\n[1] Ruff, Lukas, et al. \"Deep one-class classification.\" International conference on machine learning. PMLR, 2018.\n\n[2] Qiu, Chen, et al. \"Neural transformation learning for deep anomaly detection beyond images.\" International Conference on Machine Learning. PMLR, 2021.\n\n**Question 2**: Can you provide results using additional performance metrics such as AUPRC?\n\n**Response**: Yes, we add AUPRC as new metric on all seven datasets and related results are provided in our overall response."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2629/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584584791,
                "cdate": 1700584584791,
                "tmdate": 1700584584791,
                "mdate": 1700584584791,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wCUNDwuHiO",
            "forum": "W7kxHxjeVm",
            "replyto": "W7kxHxjeVm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2629/Reviewer_8XjW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2629/Reviewer_8XjW"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors present an anomaly detection framework which can handle missing values. One of the key challenges being addressed is to ensure that the imputation model trained on normal data should generalize to abnormal data. Authors pursue a generative modeling approach to generate pseudo-abnormal samples and learn an imputation model on both samples.  A composite loss term evaluating imputation, anomaly and reconstruction losses is formulated for the proposed method. Authors provide results on sample UCI datasets and compare performance to deep learning based anomaly detection and other AD baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Paper is technically sound and the presentation is good.\n2) Proposed framework is simple and makes sense mostly. The problem although heavily studied, there is not a lot of literature on AD with missing data. \n3) Empirical results compare performance against both deep AD and other regular AD baselines."
                },
                "weaknesses": {
                    "value": "1) Figure 1 uses AUROC on y-axis whereas for anomaly detection I think AUPR is better to motivate as most anomaly detection techniques have poor precision. Also Adult and KDD are UCI datasets which do not have a lot of relevance today as most techniques perform superlatively on these datsets. Given that the framework is applicable for missing values, it makes more sense to benchmark/motivate on real world use cases for anomaly detection (IoT sensor data, etc)\n2) Section 3.5 seems way too short and not very insightful with the purpose of just being added in the paper to have such a section in the main paper.\n3) Also on a more real-world applicability note, imputation is not often used within industry and missing data if any is more often discarded. I would encourage authors to think of very strong real-world insights of when imputation should be used and when should it be avoided."
                },
                "questions": {
                    "value": "1) I would like to see stronger results on AUPR for high dimensional real-world datasets which actually have a lot of missing values. Some real-world datasets from IoT sensor domains or others will definitely help in improving contributions of this paper. Some other questions are mentioned in the weakness section above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2629/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849584267,
            "cdate": 1698849584267,
            "tmdate": 1699636202463,
            "mdate": 1699636202463,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wNaRF08hpL",
                "forum": "W7kxHxjeVm",
                "replyto": "wCUNDwuHiO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2629/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2629/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**Comment 1**: Figure 1 uses AUROC on y-axis whereas for anomaly detection I think AUPR is better to motivate as most anomaly detection techniques have poor precision. Also Adult and KDD are UCI datasets which do not have a lot of relevance today as most techniques perform superlatively on these datsets. Given that the framework is applicable for missing values, it makes more sense to benchmark/motivate on real world use cases for anomaly detection (IoT sensor data, etc)\n\n**Response**: Following your concern, we conduct experiments on **five** new real-world datasets and add AUPRC as a new evaluation metric. Detailed data information and experimental results can be found in our overall response.  The KDD and Adult datasets are important benchmarks frequently used in previous anomaly detection works [1,2,3,4,5] and we hence included them to improve the persuasiveness.\n\nReferences:\n\n[1] Zong, Bo, et al. \"Deep autoencoding gaussian mixture model for unsupervised anomaly detection.\" International conference on learning representations. 2018.\n\n[2] Lahoti, Preethi, et al. \"Fairness without demographics through adversarially reweighted learning.\" Advances in neural information processing systems 33 (2020): 728-740.\n\n[3] Qiu, Chen, et al. \"Neural transformation learning for deep anomaly detection beyond images.\" International Conference on Machine Learning. PMLR, 2021.\n\n[4] Buyl, Maarten, and Tijl De Bie. \"Optimal transport of classifiers to fairness.\" Advances in Neural Information Processing Systems 35 (2022): 33728-33740.\n\n[5] Han, Xiao, et al. \"Achieving Counterfactual Fairness for Anomaly Detection.\" Pacific-Asia Conference on Knowledge Discovery and Data Mining. Cham: Springer Nature Switzerland, 2023.\n\n**Comment 2**: Section 3.5 seems way too short and not very insightful with the purpose of just being added in the paper to have such a section in the main paper.\n\n**Response**: For target distribution \n$\n\\mathcal{D}_{\\mathbf{z}} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I}_d) \n$,\nwe need to a constrained sampling radius $r$ to get corresponding truncated Gaussian distribution. Section 3.5 gives the lower bound of the constrained sampling radius when given a sampling probability $p$. \n\n**Comment 3**: Also on a more real-world applicability note, imputation is not often used within industry and missing data if any is more often discarded. I would encourage authors to think of very strong real-world insights of when imputation should be used and when should it be avoided.\n\n**Response**:  In fact, in many real-world scenarios, data imputation is necessary and crucial. For instance, in rare disease diagnosis and fault detection of sophisticated equipments, we cannot discard the incomplete samples and we have to make decisions based on the incomplete samples in time. In recommendation systems especially collaborative filtering problems, samples (e.g. users) with missing values commonly cannot be thrown away. In chemical processes (e.g. [1]), we need to conduct fault detection for each sample to ensure product quality and system safety. We appreciate your suggestion and enhanced the motivation in the section of Introduction.\n\nReference:\n\n[1] Fan et al. Kernel-based statistical process monitoring and\nfault detection in the presence of missing data. IEEE Transactions on Industrial Informatics, 18\n(7):4477\u20134487, 2022.\n\n**Question 1**: I would like to see stronger results on AUPR for high dimensional real-world datasets which actually have a lot of missing values. Some real-world datasets from IoT sensor domains or others will definitely help in improving contributions of this paper. Some other questions are mentioned in the weakness section above\n\n**Response**: Yes, we have added more datasets to the experiments and included AUPRC. Our method outperformed all baselines in almost all cases in terms of both AUPRC and AUROC."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2629/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583591225,
                "cdate": 1700583591225,
                "tmdate": 1700583591225,
                "mdate": 1700583591225,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D4B1mOcJ72",
            "forum": "W7kxHxjeVm",
            "replyto": "W7kxHxjeVm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2629/Reviewer_sFKw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2629/Reviewer_sFKw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new end-to-end approach called ImAD, which integrates data imputation with anomaly detection in a unified optimization problem. ImAD addresses the imputation bias issue and demonstrates improved detection performance on balanced and skewed data compared to existing methods in various missing data scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The concept of initially generating pseudo-abnormal samples and subsequently constructing an imputation model from both the normal and pseudo-abnormal datasets is a novel approach.\n- The authors provide a theoretical analysis of the generation and detection of pseudo-abnormal samples.\n- The observed performance enhancement in comparison to the baseline methods is substantial and noteworthy."
                },
                "weaknesses": {
                    "value": "- The current scope of experiments is somewhat limited, and it is strongly recommended to expand the experimental evaluation to include a broader range of datasets. This will facilitate a more comprehensive validation of the efficacy of the proposed method.\n- The assumption of a Gaussian distribution in the latent space could potentially impose limitations on the practical applicability of the proposed method. Real-world datasets with inherent complexity may not fit into a single Gaussian distribution, raising concerns about the generalizability of the approach to such intricate data scenarios."
                },
                "questions": {
                    "value": "- How can we ensure that the generated pseudo abnormal samples are similar to real abnormal samples? Because the distribution of pseudo abnormal samples in latent space is assumed, it can differ significantly from that of real abnormal samples. This difference can lead to strange imputation.\n- When dealing with a dataset of very high dimensionality, is the proposed method scalable? Will the assumption in generating pseudo-abnormal samples still hold?\n- There is an excessive use of similar symbols placed above characters, which can impede readability.\n- Typo. The last paragraph of 2 Related Work. impuate -> impute."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2629/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854450604,
            "cdate": 1698854450604,
            "tmdate": 1699636202302,
            "mdate": 1699636202302,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZA5fonUSXw",
                "forum": "W7kxHxjeVm",
                "replyto": "D4B1mOcJ72",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2629/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2629/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**Comment 1**: The current scope of experiments is somewhat limited, and it is strongly recommended to expand the experimental evaluation to include a broader range of datasets. This will facilitate a more comprehensive validation of the efficacy of the proposed method.\n\n**Response**: Thanks for your well-intentioned suggestion. We added five new real-world datasets to the experiments and one more evaluation metric AUPRC. All experimental results are provided in our overall response and the new experimental results are complemented to section of Experiments in our latest manuscript.\n\n**Comment 2**: The assumption of a Gaussian distribution in the latent space could potentially impose limitations on the practical applicability of the proposed method. Real-world datasets with inherent complexity may not fit into a single Gaussian distribution, raising concerns about the generalizability of the approach to such intricate data scenarios.\n\n**Response**: This is really an insightful comment and we absolutely agree with you. Indeed, the real-world data scenarios could be more complex and we may consider multiple Gaussians. \n\nWe also thought about projecting the normal data into multiple truncated Gaussian distributions but there are two difficulties. First, the number of classes in normal data is usually unknown, which makes it difficult to determine the number of Gaussians. Second, the spatial locations of multiple Gaussians or hyperballs are not easy to determine.\n\nOn the other hand, if the structure of real data is not very complex, a sufficiently large neural network is able to transform the unknown data distribution to any target distribution such as a single Gaussian, which is similar to the feasibility of GAN (Goodfellow et al. 2014) converting a single Gaussian distribution to any data distribution.\nIn unsupervised anomaly detection, the most important thing is to find a reliable decision boundary using the normal samples. Therefore, the target distribution is supposed to be compact, which ensures that normal data would lie in high-density regions in the latent space and we can obtain a reliable decision boundary easily. \n\nWe appreciate your comments.\n\n**Question 1**: How can we ensure that the generated pseudo abnormal samples are similar to real abnormal samples? Because the distribution of pseudo abnormal samples in latent space is assumed, it can differ significantly from that of real abnormal samples. This difference can lead to strange imputation.\n\n**Response**: This is a great question. Actually, there is no need to ensure that the generated pseudo-abnormal samples are very similar to real abnormal samples. We only want to ensure that the generated pseudo-abnormal samples closely surround the normal samples in the original data space, which means the generated pseudo-abnormal samples interpolate between the normal region and the abnormal region in the original data space and hence form a meaningful decision boundary. This goal can be achieved by our design of the distributions in the latent space and the Lipschitz-continuity of the neural network. Specifically, as shown by Figure 2 of our paper, in the latent space, the pseudo-abnormal samples surround the normal samples. According to our Theorem 3.1(a), in the original data space, the pseudo-abnormal samples still surround the normal samples.\n\nPlease also take a look at Figure 4 of our latest manuscript. We visualize the projection results of Botnet dataset in 2-D space. It can be observed that the majority of normal training and testing samples are mapped into the target distribution while most abnormal samples fall outside of the decision boundary. This demonstrates that our method is practical and Theorem 3.1 is effective for real-world scenarios.\n\n**Question 2** :  When dealing with a dataset of very high dimensionality, is the proposed method scalable? Will the assumption in generating pseudo-abnormal samples still hold?\n\n**Response**: Our method projects the original high-dimensional data into the target distribution, which can reduce the dimensionality significantly, and the anomaly score is defined in the low-dimensional latent space. Therefore, our method is scalable to high-dimensional data. In this revision, the new datasets Segerstolpe and Usoskin especially have high-dimension and our method still outperformed all baselines significantly.\n\n**Question 3**: There is an excessive use of similar symbols placed above characters, which can impede readability.\n\n**Response**: Thanks for pointing it out. We enhanced the related descriptions and made them easy to read and understand."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2629/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582572048,
                "cdate": 1700582572048,
                "tmdate": 1700582572048,
                "mdate": 1700582572048,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]