[
    {
        "title": "Dynamic Sparse No Training:  Training-Free Fine-tuning for Sparse LLMs"
    },
    {
        "review": {
            "id": "4qA6uBfiUI",
            "forum": "1ndDmZdT4g",
            "replyto": "1ndDmZdT4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1636/Reviewer_nrCN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1636/Reviewer_nrCN"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by the weight pruning-and-growing method in dynamic sparse training, the authors propose a training-free fine-tuning method to sparsify LLMs. In practice, the proposed method iteratively performs weight pruning and growing with new importance metrics that take into account the expectation and variance of the reconstruction error reduction. The proposed metrics allow to eliminate the expensive backpropagation or any weight update in the original dynamic sparse training to enable training-free fine-tuning for LLMs. In the experiments, the authors conduct experiments on multiple benchmarks and show better performance when migrating into prior training-free methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The writing is clear and easy to follow.\n2. Although the idea of pruning-and-growing is not new, the proposed method is novel to eliminate backpropagation and weight update with new importance metrics to enable training-free LLM sparsification\n3. The proposed method consistently shows a clear performance gap on multiple benchmarks with varied sparsity rates compared to prior works."
                },
                "weaknesses": {
                    "value": "1. It's unclear how to use the proposed dynamic sparse no training in the whole fine-tuning process. In the paper, the authors mainly illustrate the training-free pruning-and-growing method for one specific layer. It's unknown how the algorithm is used for sparsifying an entire LLM. \n\n2. It seems the proposed method is an improved technique for existing training-free methods. From the Related work, it can not tell the difference between SparseGPT and Wanda when integrating the proposed method. \n\n3. The proposed method involves extra computing and running time compared to Wanda."
                },
                "questions": {
                    "value": "Detailed questions regarding Weakness:\n\n1. It's unknown how the algorithm is used for sparsifying an entire LLM. \n\n    1.1  Regarding the entire LLM, since the proposed method aims to reduce the reconstruction error in layer-wise, will the proposed method progressive prune each layer or jointly prune all the layers for an entire LLM?\n\n    1.2 How to assign the sparsity rate for each layer? \n\n2. It seems the proposed method is an improved technique for existing training-free methods. In the related work, it seems that SparseGPT and Wanda have different importance metrics to sparsify LLM, and this work proposes an orthogonal method. \n\n    2.1 What is the difference between SparseGPT and Wanda when integrating the proposed method? \n\n     2.2 Why the proposed method can not be considered as an independent method for training-free LLM sparsification?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1636/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814367761,
            "cdate": 1698814367761,
            "tmdate": 1699636092178,
            "mdate": 1699636092178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HFI022Un2j",
                "forum": "1ndDmZdT4g",
                "replyto": "4qA6uBfiUI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer nrCN"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your positive and motivating comments. We are delighted to see that you recognize the novelty and significant performance enhancement of our method. Please kindly see our response to your comment below.\n\n**Q1**: Regarding the entire LLM, since the proposed method aims to reduce the reconstruction error in layer-wise, will the proposed method progressive prune each layer or jointly prune all the layers for an entire LLM?\n\n**A1**: Thanks for this insightful question. DS$\\oslash$T is proposed as a fine-tuning method to further advance existing sparse LLMs. Instead of fine-tuning weights, we choose a much more efficient alternative, i.e., fine-tuning/editing sparse masks. In our implementation, we progressively apply DS$\\oslash$T to fine-tune each layer after pruning. This clarification is now updated to Section 4.1.\n\n**Q2**: How to assign the sparsity rate for each layer?\n\n**A2**: As previously elucidated, DS$\\oslash$T serves as a fine-tuning methodology utilized to enhance the performance of pruned LLMs. Consequently, our approach does not engage in LLM pruning operations during the initial stage. Instead, we commence by employing established LLM pruning techniques such as magnitude pruning, Wanda, or SparseGPT to initially prune the LLMs. Subsequently, we utilize DS$\\oslash$T to further refine the sparse masks obtained from the pruning process.\n\nTherefore, all the configurations of the pruning process strictly adhere to the methodologies outlined in Wanda and SparseGPT. For instance, this involves the allocation of a uniform sparsity rate across all layers as used by SparseGPT and Wanda.\n\n**Q3**: The diffference when applying DS$\\oslash$T to sparsegpt or wanda, can dsnot be an independent method?\n\n**A3**: We appreciate this question. Indeed, DS$\\oslash$T is a versatile and scalable fine-tuning approach applicable to any LLM pruning method as we dicussed before. All we need to do is applying DS$\\oslash$T to an established sparse LLM to dynamically prune-and-revive weights by looking at the pruning errors, regardless of the specific pruning method (Wanda, SparseGPT, and Magnitude) employed. DS$\\oslash$T is not an independent pruning method for LLMs, rather, it is a highly efficient fine-tuning method for enhancing the performance of sparse LLMs.\n\n**Q4**: Extra computing and running time compared to Wanda.\n\n**A4**: You are correct that DS$\\oslash$T + Wanda incurs additional overhead costs compared to Wanda. However, we would like to re-iterate that that DS$\\oslash$T is designed as a fine-tuning technique for sparse LLMs. It requires merely 4.3 seconds to fine-tune a sparse LLaMA-7B, which is dramatically more efficient than both LoRA fine-tuning (which takes 4 hours as demonstrated in Table 4) and full fine-tuning (requiring several days).\n\nEven though there are some overhead cost when integrating DS$\\oslash$T with Wanda, the whole process (including the fine-tuning process) can be finished within merely 4.3s, being much faster even than the pruning only approach, e.g., SparseGPT (using 209s). We humbly suggest that the efficiency of our approach is more a merit than a downside.\n\nWe sincerely appreciate the time and diligence you\u2019ve taken to participate in the review of our paper. If you have further questions, we are more than glad to discuss with you."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048099985,
                "cdate": 1700048099985,
                "tmdate": 1700048099985,
                "mdate": 1700048099985,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "keD80Qh3q2",
                "forum": "1ndDmZdT4g",
                "replyto": "4qA6uBfiUI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last three day reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer nrCN,\n\nThanks again for your valuable time and insightful comments. As the deadline for the Author/Reviewer discussion is approaching, it would be nice of you to let us know whether our answers have solved your concerns so that we can better improve our work. We are happy to provide any additional clarifications that you may need.\n\nBest regards!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467400186,
                "cdate": 1700467400186,
                "tmdate": 1700467400186,
                "mdate": 1700467400186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BJtLWNLT7I",
            "forum": "1ndDmZdT4g",
            "replyto": "1ndDmZdT4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1636/Reviewer_s71w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1636/Reviewer_s71w"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents DS$\\oslash$T, a novel training-free fine-tuning approach for sparse LLMs which edits sparse mask configuration inspired from DST. DS$\\oslash$T revives weights which negatively contribute to reconstruction error between dense and sparse LLMs, and prunes weights based on Wanda metric and the sign of reconstruction error. By conducting experiments across a wide range of tasks, authors show that DS$\\oslash$T can be seamlessly integrated with existing LLM-pruning techniques, achieving state-of-the-art results at  >50% sparsity regime with minimal computational overhead."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper tackles a timely and practically-relevant problem supported by a fair amount of experiments conducted spanning different tasks and domains. Notably, this is the first work tackling the LLM pruning problem at >50% sparsity regime.\n- The proposed method has two distinctive advantages: (i) it doesn't require gradient computation, resulting in minimal overhead costs, and (ii) it reconfigures existing sparse masks, making it compatible with existing LLM pruning methods.\n- In general, the paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- Throughout the paper, the authors report performance at >50% sparsity regime. However, in most cases, DS$\\oslash$T does not bring the performance of sparse neural networks even remotely close to that of the original dense network. To better understand DS$\\oslash$T, I recommend reporting experimental results at a lower sparsity regime. For instance, as mentioned in the introduction, the authors stated that baselines start to lose performance at 20% sparsity with LLaMA-30B. Does the use of DS$\\oslash$T preserve performance at 20% sparsity regime?\n- The paper notes that the overhead cost of using Wanda+DS$\\oslash$T is approximately 15 times greater than using Wanda alone in Table 3. However, it appears that the performance gain achieved in Tables 5 and 6 is relatively marginal. For instance, while a previous work [1] argues that the zero-shot classification performance of Wanda and SparseGPT is similar (as seen in Table 2 in [1]), the application of DS$\\oslash$T to Wanda or SparseGPT does not consistently result in substantial improvements over the respective baselines. This observation raises questions about the trade-off between computational cost and performance improvement when employing DS$\\oslash$T in combination with existing methods. \n- In Table 4, the authors make a comparison between DS$\\oslash$T and LoRA. Since DS$\\oslash$T alters network structure (pruning mask), the paper would benefit from analyzing whether the resulting sparse network structure from DS$\\oslash$T can be further optimized with full-finetuning or LoRA. I wonder whether Wanda+DS$\\oslash$T can achieve better fully fine-tuned accuracy compared to that of Wanda."
                },
                "questions": {
                    "value": "- How many random seeds are used throughout the experiments? \n- Why is LLM-pruner [2] not included in the baselines while N:M structured pruning is included? \n\n[1] Sun et al., \u201cA simple and effective pruning approach for large language models.\u201d 2023.\\\n[2] Ma et al., \u201cLlm-pruner: On the structural pruning of large language models.\u201d 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1636/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1636/Reviewer_s71w",
                        "ICLR.cc/2024/Conference/Submission1636/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1636/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837988077,
            "cdate": 1698837988077,
            "tmdate": 1700615975109,
            "mdate": 1700615975109,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RhhWXsq8Zy",
                "forum": "1ndDmZdT4g",
                "replyto": "BJtLWNLT7I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer s71w: Part I"
                    },
                    "comment": {
                        "value": "We thank the Reviewer s71w for the time and effort to review our paper. We are grateful for the constructive comments. We are glad that the reviewer found our paper to be timely and to be the first work tackling the LLM pruning problem at >50% sparsity regime.\n\n**Q1**: DS$\\oslash$T does not bring the performance of sparse LLMs even remotely close to that of the original dense network. Need reporting experimental results at a lower sparsity regime.\n\n**A1**:  Thank you sincerely for providing this invaluable comment. We would like to express our gratitude for the insights you've shared, which indeed help us to better understand the benefits of our approach.\n\nFirstly, we wish to clarify the primary objective of our paper. Our intention is not to propose an approach that can rival the performance of the original dense LLMs at high sparsity levels. Instead, our focus is on exploring the possibility of further refining pruned LLMs without resorting to the resource-intensive backpropagation, extensive corpus of text, and long training time\u2014conditions that are often impractical in resource-limited scenarios. In contrast to fine-tuning the remaining weights, our paper illustrates an alternative approach: fine-tuning/editing sparse masks to enhance the performance of sparse LLMs. Remarkably, this entire process can be executed with utmost efficiency, utilizing only 25GB GPU memory, 128 sequence of data, 4.3 seconds for pruning LLaMA-7B.\n\nIt is worth noting that, as mentioned in Wanda's paper, even after an full fine-tuning process employing 4x A100 GPUs for 3 days, the fine-tuned sparse model still falls short of matching the original performance. Given this context, we acknowledge the reasonable expectation that our approach may encounter similar limitations in matching with the original dense LLM performance, especially at high sparsity.\n\nMoreover, following your suggestion, we have now updated Appendix A.1 to include the results at lower sparsity levels to further showcase the effectiveness of DS$\\oslash$T. These results are also listed below for your convenience. Our results indeed show that our approach is able to match the dense performance at mild sparsity levels such as 10\\%-20\\%. \n\n- WikiText-2 perplexity performance for fine-tuning LLMs at varying sparsity rates.\n| Model        | Method          | 0%    | 10%   | 20%   | 30%   | 40%   | 50%   |\n| ------------ | --------------- | ----- | ----- | ----- | ----- | ----- | ----- |\n| LLaMA-V1-7B  | Wanda           | 5.68  | 5.70  | 5.82  | 6.00  | 6.39  | 7.26  |\n| LLaMA-V1-7B  | w. DS$\\oslash$T | 5.68  | **5.68**  | **5.73**  | **5.89**  | **6.28**  | **7.12**  |\n| LLaMA-V1-13B | Wanda           | 5.09  | 5.10  | 5.13  | 5.25  | 5.51  | 6.15  |\n| LLaMA-V1-13B | w. DS$\\oslash$T | 5.09  | **5.09**  | **5.11**  | **5.05**  | **5.29**  | **6.08**  |\n| LLaMA-V2-7B  | Wanda           | 5.47  | 5.49  | 5.59  | 5.74  | 6.06  | 6.92  |\n| LLaMA-V2-7B  | w. DS$\\oslash$T | 5.47  | **5.48**  | **5.49**  | **5.65**  | **5.85**  | **6.81**  |\n| LLaMA-V2-13B | Wanda           | 4.88  | 4.91  | 4.99  | 5.13  | 5.37  | 7.88  |\n| LLaMA-V2-13B | w. DS$\\oslash$T | 4.88  | **4.89**  | **4.91**  | **5.01**  | **5.25**  | **7.57**  |\n| OPT-13B      | Wanda           | 10.12 | 10.13 | 10.09 | 10.12 | 10.63 | 11.92 |\n| OPT-13B      | w. DS$\\oslash$T | 10.12 | **10.12** | **10.08** | **10.11** | **10.41** | **11.28** |\n\n\n\n**Q2**: Trade-off between computational cost and performance improvement\n\n**A2**: Thank you for this insightful comment. You are correct that DS$\\oslash$T + Wanda incurs additional overhead costs compared to Wanda. However, we would like to re-iterate that that DS$\\oslash$T is designed as a fine-tuning technique for sparse LLMs. It requires merely 4.3 seconds to fine-tune a sparse LLaMA-7B, which is dramatically more efficient than both LoRA fine-tuning (which takes 4 hours as demonstrated in Table 4) and full fine-tuning (requiring several days). \n\nEven though there are some overhead costs when integrating DS$\\oslash$T with Wanda, the whole process (including the fine-tuning process) can be finished within merely 4.3s, being much faster even than the pruning only approach, e.g., SparseGPT (using 209s). We humbly suggest that the efficiency of our approach is more a merit than a downside."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047811513,
                "cdate": 1700047811513,
                "tmdate": 1700050853561,
                "mdate": 1700050853561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4oAfH47Ibl",
                "forum": "1ndDmZdT4g",
                "replyto": "BJtLWNLT7I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer s71w: Part II"
                    },
                    "comment": {
                        "value": "**Q3**: Whether Wanda+DS$\\oslash$T can achieve better fully fine-tuned accuracy compared to that of Wanda?\n\n**A3**:  Thank you for this professional inquiry. The answer is yes and DS$\\oslash$T is orthogonal to LoRA fine-tuning. As evidenced in the following table, after using DS$\\oslash$T to fine-tune sparse LLMs pruned by Wanda, further performance improvements can be achieved by consequently using LoRA fine-tuning. This yields better results than simply fine-tuning the sparse network pruned by Wanda.\n\n- WikiText-2 perplexity performance for using LoRA to fine-tune 50% sparse LLaMA-7B\n| Sparsity                | 0.5  |\n| ----------------------- | ---- |\n| Wanda                   | 7.26 |\n| Wanda+LoRA              | 6.87 |\n| Wanda+DS$\\oslash$T      | 7.12 |\n| Wanda+DS$\\oslash$T+LoRA | **6.76** |\n\n**Q4**: How many random seeds are used throughout the experiments?\n\n**A4**: We use one random seed in our submission. To demonstrate the robustness of our approach, we have added the results with different calibration sets under five random seeds in Appendix A.2. The variance across random seeds is very low, suggesting the robustness of DS$\\oslash$T, corroborating its efficacy as a tool in fine-tuning sparse LLMs. For your convenience, we list the results as below.\n\n- WikiText validation perplexity for pruning LLaMA-V1 and LLaMA-V2 models at 60% sparsity. We report the mean and standard deviation under 5 random seeds.\n| Method          | LLaMA-V1-7B       | LLaMA-V1-13B     | LLaMA-V2-7B       | LLaMA-V2-13B     |\n| --------------- | ----------------- | ---------------- | ----------------- | ---------------- |\n| Dense           | 5.68 ($\\pm$0.00)  | 5.09 ($\\pm$0.00) | 5.47 ($\\pm$0.00)  | 4.88 ($\\pm$0.00) |\n| SparseGPT       | 10.42($\\pm$0.04)  | 8.43 ($\\pm$0.02) | 10.14 ($\\pm$0.03) | 7.88 ($\\pm$0.01) |\n| w. DS$\\oslash$T | **9.64** ($\\pm$0.03)  | **7.73** ($\\pm$0.02) | **9.68** ($\\pm$0.03)  | **7.57** ($\\pm$0.01) |\n| Wanda           | 10.69 ($\\pm$0.01) | 8.75 ($\\pm$0.01) | 10.79 ($\\pm$0.01) | 8.40 ($\\pm$0.01) |\n| w. DS$\\oslash$T | **10.22** ($\\pm$0.01) | **8.46** ($\\pm$0.01) | **10.59** ($\\pm$0.01) | **8.18** ($\\pm$0.01) |\n\n**Q5**: Why is LLM-pruner not included in the baselines while N:M structured pruning is included?\n\n**A5**: Thanks for your question. The primary goal of this paper is weight pruning, i.e., removing individual weights, including unstructured pruning and n:m sparsity. LLM-pruner itself is a structured pruning, i.e., completely removing entire channels and attention heads, which is not directly comparable to the unstructured pruning approaches. Also due to this reason, both SparseGPT and Wanda do not include LLM-pruner as their baselines.\n\nYour time and effort in reviewing our paper are genuinely appreciated. If there are any additional questions or points that require clarification, we would be more than delighted to engage in further discussions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047886499,
                "cdate": 1700047886499,
                "tmdate": 1700050873367,
                "mdate": 1700050873367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wzOyVo03AC",
                "forum": "1ndDmZdT4g",
                "replyto": "BJtLWNLT7I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last three day reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer s71w,\n\nThanks again for your valuable time and insightful comments. As the deadline for the Author/Reviewer discussion is approaching, it would be nice of you to let us know whether our answers have solved your concerns so that we can better improve our work. We are happy to provide any additional clarifications that you may need.\n\nBest regards!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467350125,
                "cdate": 1700467350125,
                "tmdate": 1700467350125,
                "mdate": 1700467350125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qMieVadC7M",
                "forum": "1ndDmZdT4g",
                "replyto": "wzOyVo03AC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Reviewer_s71w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Reviewer_s71w"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional experiments! As the authors addressed most of my concerns, including results on lower sparsity regime and compatibility with other fine-tuning methods, I will raise my score from 5 to 6. \n\n- I have one more lingering question regarding the structured pruning context. Given that unstructured pruning lacks practicality in real-world scenarios due to hardware limitations, I wonder whether hardware-friendly methods (e.g., structured pruning) can also benefit from DS$\\oslash$T. This will significantly elevate the impact of the study because practicality stands as one of the main contributions of DS$\\oslash$T."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616368505,
                "cdate": 1700616368505,
                "tmdate": 1700616368505,
                "mdate": 1700616368505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6tbLmVQe7h",
                "forum": "1ndDmZdT4g",
                "replyto": "BJtLWNLT7I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer s71w,\n\nWe sincerely thank you for your support. We would like to offer additional clarification regarding the benefits of DS$\\oslash$T for hardware-friendly pruning. The current implementation of DS$\\oslash$T focuses on pruning and reviving individual weights, making it equally applicable to semi-structured hardware-friendly pruning. As shown in Table 5 of our paper, DS$\\oslash$T effectively improves the performance of N:M sparse networks, which achieves notable acceleration supported by the NVIDIA Ampere Sparse Tensor Core[1,2].\n\nAs for structured sparsity, completely removing entire channels and attention heads does not directly conform to the current DS$\\oslash$T method. However, your point has indeed inspired us to consider adapting DS$\\oslash$T to structured sparsity scenarios. One idea is to use the output of the attention block as a guide for the pruning-and-reviving process for entire channels or attention heads, thereby fine-tuning structured sparse LLMs. This will require a more sophisticated design, which is challenging to execute within the limited timeframe of this rebuttal period and beyond the scope of our paper\u2019s current emphasis on weight pruning, i.e., the removal of individual weights. We are committed to leave it as a promising future work building upon this paper. Once again, we are grateful for your supportive feedback and constructive comments.\n\n[1]  Nvidia a100 tensor core gpu architecture, 2020. https:// www.nvidia.com/content/dam/enzz/Solutions/Data-Center/ nvidia-ampere-architecture-whitepaper.pdf.\n\n[2] A simple and effective pruning approach for large language models. In Arxiv, 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641393596,
                "cdate": 1700641393596,
                "tmdate": 1700641441508,
                "mdate": 1700641441508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xXdb68YZlZ",
                "forum": "1ndDmZdT4g",
                "replyto": "6tbLmVQe7h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Reviewer_s71w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Reviewer_s71w"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the further clarification, and I believe applying DS$\\oslash$T to structured pruning would make an interesting future research. I maintain my score, and vote for the acceptance."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646036715,
                "cdate": 1700646036715,
                "tmdate": 1700646036715,
                "mdate": 1700646036715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UNWADn63cj",
            "forum": "1ndDmZdT4g",
            "replyto": "1ndDmZdT4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1636/Reviewer_CfsK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1636/Reviewer_CfsK"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Dynamic Sparse No Training, a training-free fine-tuning approach for sparse Language Model (LLM) deployment. It minimizes the reconstruction error between dense and sparse LLMs through iterative weight pruning-and-growing. This approach allows for updating sparse LLMs without the expensive backpropagation and weight updates, making it more efficient for on-device deployment. The paper demonstrates the effectiveness of the proposed method on several benchmark datasets, achieving comparable or better performance than traditional fine-tuning approaches while requiring significantly less computation. The contributions of this paper include the introduction of a training-free fine-tuning approach for sparse LLMs, and the demonstration of its effectiveness on several benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In terms of originality, the paper introduces a novel approach to fine-tuning sparse LMs, called Dynamic Sparse No Training. This approach does not require backpropagation or weight updates, making it more efficient for on-device deployment.\n\nIn terms of quality, the authors provide detailed descriptions of the datasets and experimental setup, as well as a thorough analysis of the results. The paper also includes a comprehensive review of related work, highlighting the strengths and weaknesses of existing approaches.\n\nIn terms of clarity, this paper is well-written, with clear explanations of the proposed approach and experimental results.\n\nIn terms of significance, the paper addresses an important problem in the field of LMs, namely the challenge of deploying large models on resource-constrained devices."
                },
                "weaknesses": {
                    "value": "1. For the inner loop, how does the threshold affect the final performance of the model, for both perplexity and efficiency?\n\n2. The paper assesses the methods using a consistent sparsity rate of 60%. However, a 50% sparsity rate is more commonly employed in previous baselines. It would be beneficial to see the outcomes at this 50% sparsity level for comparison."
                },
                "questions": {
                    "value": "please follow weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1636/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699023424975,
            "cdate": 1699023424975,
            "tmdate": 1699636092024,
            "mdate": 1699636092024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FjgRwOFJv7",
                "forum": "1ndDmZdT4g",
                "replyto": "UNWADn63cj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer CfsK"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your careful review, positive feedback, and constructive comments.  We are delighted to see that you recognize that we have done a thorough analysis and comprehensive literature reviewing.  Please kindly see our responses to your comments below.\n\n**Q1**: How does the threshold affect the final performance of the model, for both perplexity and efficiency?\n\n**A1**: Thank you for posing this insightful question. We do agree with your insights and have analyzed the effect of the threshold $\\epsilon$ in Section 4.4 (Figure 3) of our  submission. \n\nFirst of all, we would like to highlight that the value of threshold $\\epsilon$ does not necessarily lead to too much difference in terms of the computing efficiency, typically resulting in a negligible computing time difference (often less than 1 second), owing to the efficient nature of our approach, as demonstrated in the table below. But indeed, the values of threshold $\\epsilon$  has a big impact on the fine-tuning performance.  Intuitively, Larger threshold values tend to correlate with increased reconstruction error, potentially leading to suboptimal performance. Conversely, smaller threshold values result in reduced reconstruction error, often associated with enhanced performance. Nonetheless, exceedingly small values may cause overfitting to the limited calibration data, resulting in inferior performance. \n\nThe results below verify the above intuition. First, the computing time difference of the whole fine-tuning process is only 0.6 seconds when $\\epsilon$ increases from 0 to 1. In terms of performance, perplexity continuously decreases as $\\epsilon$ decreases from 1 to 0.1, while their is a slight increase of perplexity when $\\epsilon$ becomes too low, e.g.,  $\\epsilon=0$.\n\n- WikiText-2 perplexity performance and running times of DS$\\oslash$T for fine-tuning 50% sparse\n  LLaMA-V1-7B pruned by the Wanda metric with different threshold \u03b5\n| $\\epsilon$     | 0    | 0.1  | 0.5  | 1    |\n| -------------- | ---- | ---- | ---- | ---- |\n| Computing Time | 4.6s | 4.3s | 4.2s | 4.0s |\n| PPL            | 7.14 | 7.12 | 7.16 | 7.19 |\n\n**Q2**: The outcomes at 50\\% sparsity level for comparison.\n\n**A2**: Thank you for this valuable suggestion. We have also presented the results for pruning LLaMA-V1 with 7B and 65B parameters under a 50% sparsity rate, as can be seen in Table 2. To provide a better overview of our method, we have also included more results at 50% sparsity ratio in Appendix A.1. For your convenience, we list the results below.\n\n- WikiText-2 Perplexity comparison for pruning LLMs at 50\\% sparsity rate\n| Method          | LLaMA-V1-7B | LLaMA-V1-13B | LLaMA-V2-7B | LLaMA-V2-13B | OPT-13B |\n| --------------- | ----------- | ------------ | ----------- | ------------ | ------- |\n| Dense           | 5.68        | 5.09         | 5.47        | 4.88         | 10.12   |\n| Magnitude       | 17.29       | 20.21        | 16.03       | 6.83         | 2.96e3  |\n| w. DS$\\oslash$T | **14.04**       | **15.52**        | **13.09**       | **6.31**         | **1.10e3**  |\n| SparseGPT       | 7.22        | 6.21         | 7.00        | 6.02         | 15.61   |\n| w. DS$\\oslash$T | **7.08**       | **6.13**         | **6.88**        | **5.64**         | **14.88**  |\n| Wanda           | 7.26        | 6.15         | 6.92        | 5.97         | 11.98   |\n| w. DS$\\oslash$T | **7.12**        | **6.08**         | **6.85**        | **5.87**         | **11.68**   |\n\nWe sincerely appreciate the time and efforts you have dedicated to reviewing our paper. Should you have any further inquiries, please let us know and we would be more than delighted to engage in further discussion with you."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047326862,
                "cdate": 1700047326862,
                "tmdate": 1700050831142,
                "mdate": 1700050831142,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vrvbkWwC96",
                "forum": "1ndDmZdT4g",
                "replyto": "UNWADn63cj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1636/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last three day reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer CfsK,\n\nThanks again for your valuable time and insightful comments. As the deadline for the Author/Reviewer discussion is approaching, it would be nice of you to let us know whether our answers have solved your concerns so that we can better improve our work. We are happy to provide any additional clarifications that you may need.\n\nBest regards!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1636/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467305016,
                "cdate": 1700467305016,
                "tmdate": 1700467305016,
                "mdate": 1700467305016,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]