[
    {
        "title": "Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos"
    },
    {
        "review": {
            "id": "Np1HZYdSOk",
            "forum": "A2mRcRyGdl",
            "replyto": "A2mRcRyGdl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2116/Reviewer_QtRh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2116/Reviewer_QtRh"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the problem of novel view synthesis of semantic labels is studied. Rather than rendering colour, the segmentation is rendered with a NeRF model. The authors propose a model and training procedure that can learn scene flow fields and semantic renderings of a given video sequence. Furthermore, the proposed method allows for quick adaptation on novel video sequences. Since no earlier work studies this problem, the authors label the nvidia dynamic scenes dataset with pixel-wise semantic labels. The experiments show that the semantic labels can be rendered accurately, and furthermore that we do not need labels for all frames in a sequence, and that we can use the semantic labels to mask out specific parts of the video and render it without specific objects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- There is no existing dataset for the problem setup, so authors annotated the nvidia dynamic scenes dataset with pixel-wise semantic segmentation labels.\n- A strength of the method is that it allows for quick adaptation to new scenes, e.g. with just 500 iterations it can perform well given pre-training on other scenes. The reason is that the scene flow field is not learned from scratch, but rather from frame-wise video features, which does not need to be learned from scratch for each scene.\n- Augmenting NeRF models with semantic segmentation has been done for static scenes (e.g. Zhi et al 2021) but to the best of my knowledge not for general dynamic scenes, so the paper tackles a new problem setup.\n- The method is clearly described and ablations are provided for the main components."
                },
                "weaknesses": {
                    "value": "- There are some baselines that would be reasonable to try that are missing from the paper. For instance, if we just render the rgb images with any NeRF method (e.g. MonoNeRF) and apply some video object segmentation algorithm (e.g. any top-performing method on the DAVIS dataset) or semantic segmentation method (trained on some dataset with overlapping labels), how well would that perform?\n- Since one of the applications mentioned in the paper is scene editing, i.e. removing some specific object, it is necessary to not just render semantics correctly but also rgb. There are no values provided for the standard novel view synthesis metrics (PSNR, SSIM, LPIPS) for rgb on the tested video sequences.\n- It would have been interesting to somehow visualise or discuss the flow fields. Since the objective is semantic rendering rather than colour rendering it is not clear if we need the scene flow to map to the same specific part of an object, or if it is sufficient or even beneficial to just map to anywhere within the same object. For instance, the consistency loss L_consis only enforces that points along flow trajectories should have the same label.\n\nMinor issues:\n- Missing related work: \u201cPanoptic neural fields: A semantic object-aware neural scene representation\u201d (CVPR 2022) also considers novel view synthesis for semantic segmentation from a video, although their method is limited to non-deformable dynamic objects.\n- Page 5: Does ground truth flow mean optical flow estimated from RAFT? If that is the case it should not be called ground truth.\n- Page 5: The closing parenthesis in eq. (7) is probably incorrect.\n- Page 8: \u201cBoundray\u201d typo\n- Page 8: Table 2 caption: \u201cqualitative\u201d should be \u201cquantitative\u201d"
                },
                "questions": {
                    "value": "See everything under weaknesses.\n- When training for semantic completion or tracking, only a subset of the frames are used for semantic supervision. Is the same true for RGB supervision or are all frames used for that? \n- In Fig. 3, what are the indices of the frames that are shown? How far are they from the frames with semantic labels?\n- For the DynNeRF and MonoNeRF baselines, what exactly is the input to the semantic heads that are learned?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Reviewer_QtRh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743629260,
            "cdate": 1698743629260,
            "tmdate": 1699636144337,
            "mdate": 1699636144337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TkqxsIPsHj",
                "forum": "A2mRcRyGdl",
                "replyto": "Np1HZYdSOk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QtRh"
                    },
                    "comment": {
                        "value": "# Missing baselines\nThank you for your question. During the rebuttal phrase, we test the semantic segmentation performance by applying the state-of-the-art tracking method DeAOT [1] and image segmentation method Mask R-CNN [2] to the rendered images from MonoNeRF [3]. The results are shown **in Table 2 and Figure 8**. We also discuss the performance of these methods in the **General Response**. DeAOT lacks semantic understanding of 3D scenes and hence predicts inconsistent semantic labels in different views, leading to a severe performance drop as shown in Table 2. As Mask R-CNN only learns semantic information from single images, Figure 8 shows that it predicts inconsistent semantic labels of the same instances in different frames.\n\nIn addition, we also compare more Dynamic NeRF baselines by manually adding semantic segmentation heads to these methods in the **General Response and Table 1 in the paper**.\n\n# RGB rendering preformance\nThank you for your question. We present the PSNR, SSIM and LPIPS scores of rendered RGB images **in the following table**. Our model could achieve slightly better RGB color rendering quality compared to MonoNeRF [3] and DynNeRF [4], which demonstrates that our model is able to conduct the instance-level scene editing application. We also add the results to **Section G.1 and Table 11 in the Appendix**. \n\n|PSNR / SSIM / LPIPS|  Jumping   | Skating  | Average |\n|----|----|----|----|\n|DynNeRF [4] + head|  21.91 / 0.6856 / 0.174    | 24.68 / 0.7866 / 0.175  |23.30 / 0.7361 / 0.176|\n|MonoNeRF [3] + head| 22.41 / 0.7484 / 0.145  | **26.18** / 0.8739 / 0.115 |24.30 / 0.8112 / 0.130|\n|Semantic Flow | **22.86** / **0.7658** / **0.138**  | 25.75 / **0.8774** / **0.113** | **24.31** / **0.8216** / **0.126**|\n\n# Visualization and discussion of the flow fields\n**In Section G.2 and Figure 6 in the Appendix**, we show the visualization of the rendered images, estimated flow fields, predicted semantics and the point correspondence of dynamical foreground in 3 consecutive frames. We also discuss these results **in the General Response**. We agree that we do not force the flows to map the same parts of an object moving in time. However, Figure 6 shows that our model is able to predict flows that map similar parts of the dynamic foreground.\n\n# Minor issues\n* *Missing related work*. We have inserted the Panoptic Neural Field [5] into **the Related Work section in the paper, page 3**. \n* *Ground truth flow*. Thank you for your suggestion. we have corrected \"the ground truth flows\" to \"the flows generated from the pretrained model\".\n* *Incorrect parenthesis*. Thanks. We have revised the closing parenthesis of $\\boldsymbol{F}_{dy}(I;{\\boldsymbol{\\Gamma}})$ near the equation (7) in the paper.\n* *\"Boundray\" typo.* Thank you. We have revised \"boundray\" to \"boundary\" in page 8.\n* *Table 2 caption*. Thanks. We have revised \"qualitative\" to \"quantitative\" in Table 2 caption.\n\n# Questions\n* *RGB frames in semantic completion and tracking settings.* We follow MonoNeRF [3] and use the same subset of frames for RGB supervision.\n* *Indices of the frames in Figure 3*. In the completion setting, Figure 3 shows the semantic prediction results from the frame #8, which are 2-frame far from the frame #10 with the semantic labels. For the dynamic scene tracking setting, Figure 3 shows the results from the frame #10, which are 7-frame far from the frame #3 with the semantic labels.\n* *The input to the semantic heads.* For the DynNeRF [4] and MonoNeRF [3] methods, the input is the feature vector with 256 channels obtained from a 8-layer MLP with ReLU activations.\n\nReferences:\n\n[1] Yang, Zongxin, and Yi Yang. \"Decoupling features in hierarchical propagation for video object segmentation.\" In NIPS, pp. 36324-36336, 2022.\n\n[2] He, Kaiming, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. \"Mask R-CNN.\" In ICCV, pp. 2961-2969. 2017.\n\n[3] Tian, Fengrui, Shaoyi Du, and Yueqi Duan. \"Mononerf: Learning a generalizable dynamic radiance field from monocular videos.\" In ICCV, 2023.\n\n[4] Gao, Chen, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. \"Dynamic view synthesis from dynamic monocular video.\" In ICCV, pp. 5712-5721. 2021.\n\n[5] Kundu, Abhijit, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas J. Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser. \"Panoptic neural fields: A semantic object-aware neural scene representation.\" In CVPR, pp. 12871-12881. 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408180189,
                "cdate": 1700408180189,
                "tmdate": 1700467936817,
                "mdate": 1700467936817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "emi2SAEWJb",
                "forum": "A2mRcRyGdl",
                "replyto": "Np1HZYdSOk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_QtRh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_QtRh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the answers. The additional baseline experiments strengthen the paper, although for Mask R-CNN it would be easy to use e.g. the Hungarian algorithm to match instances in consecutive frames as is common practice for video instance segmentation. The RGB rendering performance seems in line with the baselines, although it is only reported for two sequences in the Nvidia dynamic scenes datasets so it can not be easily compared to other methods than the reported ones. The flow fields look as expected, since optical flow was used for supervision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665645450,
                "cdate": 1700665645450,
                "tmdate": 1700665752035,
                "mdate": 1700665752035,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DsIUOVgDIZ",
                "forum": "A2mRcRyGdl",
                "replyto": "Np1HZYdSOk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response!\n\nThe superiority of our model compared to image-based or video-based segmentation methods is that our model predicts view-agnostic semantic labels of each point in the 3D scenes, while image-based or video-based segmentation methods lack the semantic understanding of the 3D scenes and hence predict inconsistent semantic labels across different views, which may lead to a severe performance drop. We still agree that we should evaluate the performance of Mask R-CNN + Hungarian for a much fairer comparison. **Currently we are testing the performance of Mask R-CNN + Hungarian algorithm and we will report the results as soon as possible.**\n\nThe reason why we report the RGB rendering performance on the joint learning of Jumping and Skating scenes is that the examples of instance-level editing in the paper are conducted in this setting. We totally agree that we should report the performance of the entire dataset. **In the following table**, we report the RGB rendering performance on the entire Nvidia Dynamic Scene dataset. We follow the experimental setting of learning from multiple scenes in the MonoNeRF paper. It can be seen that our model reaches slightly better performance compared to MonoNeRF.\n|method|  PSNR   | SSIM  | LPIPS |\n|----|----|----|----|\n|DynNeRF [4] + head|  21.19    | 0.6226  | 0.304|\n|MonoNeRF [3] + head| 23.10  | 0.7441 | 0.170|\n|Semantic Flow | **23.47**  | **0.7524** | **0.165** |\n\nWe also report all the details of the PSNR, SSIM and LPIPS score **in the following tables**.\n\n| PSNR          | Balloon1 | Balloon2 | Jumping | Skating | Umbrella | Playground | Truck | Average     |\n|---------------|----------|----------|---------|---------|----------|------------|-------|-------------|\n| DynNeRF [4] + head  | 19.17    | 18.98    | 21.91   | 24.68   | 20.67    | 20.57      | 22.38 | 21.19 |\n|MonoNeRF [3] + head  | 21.19    | 24.24    | 22.41   | 26.18   | 21.69    | 20.32      | 25.66 | 23.10 |\n| semantic flow | 21.15    | 24.12    | 22.86   | 25.75   | 22.26    | 21.86      | 26.32 | 23.47 |\n\n\n| SSIM          | Balloon1 | Balloon2 | Jumping | Skating | Umbrella | Playground | Truck  | Average     |\n|---------------|----------|----------|---------|---------|----------|------------|--------|-------------|\n| DynNeRF [4] + head| 0.5493   | 0.4690    | 0.6856  | 0.7866  | 0.5997   | 0.6435     | 0.6246 | 0.6226|\n|MonoNeRF [3] + head| 0.6733   | 0.7210    | 0.7484  | 0.8739  | 0.6672   | 0.7450      | 0.7800   | 0.7441 |\n| Semantic Flow | 0.6613   | 0.7053   | 0.7658  | 0.8774  | 0.6805   | 0.7614     | 0.8151 | 0.7524      |\n\n| LPIPS         | Balloon1 | Balloon2 | Jumping | Skating | Umbrella | Playground | Truck | Average     |\n|---------------|----------|----------|---------|---------|----------|------------|-------|-------------|\n|DynNeRF [4] + head| 0.381    | 0.486    | 0.174   | 0.175   | 0.332    | 0.291      | 0.290  | 0.304 |\n| MonoNeRF [3] + head | 0.204    | 0.176    | 0.145   | 0.115   | 0.201    | 0.183      | 0.167 | 0.170 |\n| Semantic Flow | 0.218    | 0.191    | 0.138   | 0.113   | 0.195    | 0.147      | 0.152 | 0.165 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670279930,
                "cdate": 1700670279930,
                "tmdate": 1700733889987,
                "mdate": 1700733889987,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r0akl1cfem",
                "forum": "A2mRcRyGdl",
                "replyto": "Np1HZYdSOk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for waiting!\n\nTo test the performance of the Mask R-CNN + Hungarian algorithm, we choose **person, truck and umbrella** classes in our dataset, which overlap with the COCO dataset. We use the Mask R-CNN pretrained on the COCO dataset to predict semantic labels in each video frame and use the Hungarian algorithm to match the same instances in different frames. We present the results **in the following table**. Our model outperforms Mask R-CNN + Hungarian algorithm by a large margin, which demonstrates the superiority of our model that leans view-agnostic semantic labels in 3D space.\n\n|method|  Total Acc   | Avg Acc  | mIOU |\n|----|----|----|----|\n|Mask R-CNN + Hungarian|  0.689   | 0.482  | 0.308|\n|Semantic Flow | **0.964**  | **0.725** | **0.623** |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719346533,
                "cdate": 1700719346533,
                "tmdate": 1700719931495,
                "mdate": 1700719931495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w7qNUjTLoQ",
            "forum": "A2mRcRyGdl",
            "replyto": "A2mRcRyGdl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2116/Reviewer_5VQj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2116/Reviewer_5VQj"
            ],
            "content": {
                "summary": {
                    "value": "The paper looks to solve the problem related to generating a novel view 2D semantic map, for dynamic scenes using continuous flow. Paper leverages optical flow for the foreground part of the images and uses volume density as a prior to determining flow feature contribution towards semantics. Authors evaluate this on Dynamic scene dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n-The paper is well written with the objective clearly identified. It is structured well and has logically moving sub-section-wise explanations. \n-Tackles a well-known problem in terms of generating novel view synthesis for dynamic scenes, but for semantics.\n- Proposes a novel idea, that leverages optical flow to predict semantic labels for dynamic foreground pixels/regions.\n- Evaluate and compare the model on the Dynamic Scene Dataset."
                },
                "weaknesses": {
                    "value": "Weakness:\n- Paper leverages optical flow output as one of the intermediate steps, but fails to discuss its shortcomings and how exactly do they handle occlusion and disocclusion related to both dynamic and static regions of the frame.\n- For the most part of the paper, the authors only compare with two dynamic scene-based works, Considering other related works in dynamic scene reconstruction, Would be great to see comparative baseline results, with a few more of these models with semantic head."
                },
                "questions": {
                    "value": "- Could Authors share some of the shortcomings (a few qualitative results) which may be due to imperfect flow prediction, which results in bad performance during inference?\n- In Section 3.4: while calculating Semantic Consistency Constraint; Do we generate some sort of valid mask here to enforce the semantic consistency or is it done for all pixels, irrespective of occlusion or uncertainty?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Reviewer_5VQj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793723798,
            "cdate": 1698793723798,
            "tmdate": 1699636144256,
            "mdate": 1699636144256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nyfYi67QFE",
                "forum": "A2mRcRyGdl",
                "replyto": "w7qNUjTLoQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5VQj"
                    },
                    "comment": {
                        "value": "# Shortcomings\nThank you for your suggestion. We present the shortcoming of our model **in Figure 10 in the Appendix**. Due to the imperfect predictions of flows in the tracking setting, our model fails to estimate precise semantic labels of dynamic parts.\n\n# Occlusion and disocclusion problems\nTo evaluate the performance of our model in the occlusion and disocclusion situations, as shown **in Section G.4 and Figure 7 in Appendix**, we add an occlusion region in both the static background and dynamic foreground in the frame #6, and remove the occlusion in the frames #5 and #7. Concretely, we manually occludes the both RGB colors and semantic labels of the region in the frame #6. As Figure 7 shows, the optical flow estimation method fails to estimate the object movement due to the occlusion. We train the model with the occluded image, wrong optical flow maps and semantic labels. The occlusion may provide wrong information to train the semantics, which leads to the incorrect semantic prediction in the occlusion part. However, since our model learns the semantics from flows, it successfully predicts the semantics in disocclusion parts by incorporating correct information among the flows in non-occluded regions.\n\n# More comparisons with other related works\nThank you for your suggestion. We have tested other methods related to dynamic scene reconstruction **in Table 1 in the paper** and have a discussion about the performance of these methods in the **General Response**. Besides, we also conduct comparisons with the state-of-the-art video tracking method and the image segmentation method **in Table 2 and Figure 8 in the paper and Appendix.**\n\n# Semantic consistency constraint\nWe conduct the constraint for all pixels. We visualize the pixel correspondence in consecutive frames **in Section G.2 and Figure 6 in the paper** and discuss the correspondence **in the General Response**.  Figure 6 shows that our model could generate the flows that map the similar parts of the dynamic foreground moving across time. We also discuss the occlusion problem **in Section G.4** and the uncertainty of noisy flows **in Table 13 and Section G.3 in the Appendix**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407502787,
                "cdate": 1700407502787,
                "tmdate": 1700457263807,
                "mdate": 1700457263807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FHiEumEw3D",
            "forum": "A2mRcRyGdl",
            "replyto": "A2mRcRyGdl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces learning a semantic field of the dynamic scene using NeRF given a monocular video. Given a sequence of input frames from a monocular video, precomputed optical flow, their Dynamic NeRF learns a semantic field so that it can render a semantic segmentation map at novel views. The method can be used for a couple of applications that output semantic field for unseen frames given partial frames."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Better accuracy over baseline methods\n\n  Compared to the two baselines, the method shows better accuracy on multiple tasks (scene completion, scene tracking, and semantic representation) in Table 1 and Table 2. Also it demonstrates better qualitative results in Fig. 3\n\n- New applications\n\n  The paper proposes interesting new applications, both dynamic scene tracking and completion that estimates semantic maps on unseen frames. (Fig. 1)"
                },
                "weaknesses": {
                    "value": "- Outdated baselines\n\n  The paper compares their method with a couple of baselines (DynNeRF and MonoNeRF) but those are a bit limited. There are many other baselines for the dynamic NeRF task such as D-NeRF, RoDynRF, NSFF (Neural Scene Flow Field), etc. It would have been great if the paper provided accuracy on more baseline methods to make the comparison much fairer. \n\n- A bit difficult to follow the equations (from Eq. (4) to Eq. (8))\n\n  I am wondering if it's possible to put the mathematic notation from Eq. (4) to Eq. (8) into Fig. 2 for better understanding.\n\n- Clarity \n\n  Some parts of the paper have lack of clarity and make it hard to understand clearly. What is the meaning of '25%/50% semantic labels' in Fig. 3? I wonder if the paper can provide more details in the figure captions. How are the 25%/50% determined? \n\n- Marginal accuracy improvement in Fig. 4\n\n  The choice of low displacement seems not so critical for the accuracy gain. Maybe it would be good to have a justification or discussion on the result."
                },
                "questions": {
                    "value": "- How much does the accuracy of the method depend on the off-the-shelf optical flow methods? Can it be critical?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814111059,
            "cdate": 1698814111059,
            "tmdate": 1700721718844,
            "mdate": 1700721718844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b8oBGSOfyR",
                "forum": "A2mRcRyGdl",
                "replyto": "FHiEumEw3D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xtsS"
                    },
                    "comment": {
                        "value": "# Outdated baselines\nThank you for your suggestion. We test the performance of NeRF [1] with time, D-NeRF [2], RoDynRF [3], NSFF [4] and discuss these methods **in Table 1 in the paper and in the General Response**. Besides, we also conduct comparisons with the state-of-the-art video tracking method and the image segmentation method **in the General Response** and **in Table 2 in the paper and Figure 8 in the Appendix.**\n\n# Difficult to follow the equations 4-8\nThank you for your suggestion. We have revised **Figure 2 in the paper** to put the mathematic notations from equations 4-8 into the figure.\n\n# Clarity\nThank you for your suggestion. In Figure 3, \"25% semantic labels\" in the tracking setting denotes that we use the frames #1, #2 and #3 to train the model and conduct semantic view synthesis on the frames #4-12. \"25%\" is determined by choosing 3 out of 12 frames to train the models in a scene. \"50% semantic labels\" in the completion setting denotes that we select the frames #1-3 and #10-12 to train the model and render semantic views in frames frames #4-9. \"50%\" is determined by choosing 6 out of 12 frames to train the models. We have added these details to the caption **in Figure 3 in the paper**.\n\n# Marginal accuracy improvement\nThank you for your question. We agree that the results in Table 4 cannot distinguish $\\boldsymbol{\\Gamma}$ \\& $\\Delta \\boldsymbol{\\Gamma}$, $\\Delta \\boldsymbol{\\Gamma}$ and $\\boldsymbol{\\Gamma}$. However, we demonstrate that flow displacements contribute to the semantic prediction performance according to the ablation study in Table 6 in the paper and our Semantic Flow has robustness with different flow displacements according to the Table 4. **In Figure 9 in the Appendix**, we visualize the semantic predictions by using $\\boldsymbol{\\Gamma}$ \\& $\\Delta \\boldsymbol{\\Gamma}$, $\\Delta \\boldsymbol{\\Gamma}$ and $\\boldsymbol{\\Gamma}$. Although Table 4 shows quantitatively comparable results by using different displacements, Figure 9 presents that the semantic prediction of using $\\boldsymbol{\\Gamma}$ \\& $\\Delta \\boldsymbol{\\Gamma}$ is qualitatively better than using $\\Delta \\boldsymbol{\\Gamma}$ or $\\boldsymbol{\\Gamma}$.\n\n# Relationship between the performance of semantic predictions and optical flows\nThank you for your question. We test how much the performance of our model depends on the predicted flows from two perspectives: choosing different flow estimation methods and manually adding noise to the flow maps. \n\nFirstly, we choose RAFT [1] and FlowNet [2] to generate the optical maps and test the performance in the setting of semantic learning from multiple scenes. The results are presented **in the following table** and **in Table 12 in the Appendix**. According to the RAFT paper, RAFT outperforms FlowNet by a large margin in various optical flow estimation tasks. Therefore, the results show that there is about 10% performance drop in mIOU matrix by using FlowNet when jointly optimizing Jumping and Skating scenes, where the dynamic foregrounds are drastically changing and FlowNet fails to predict the correct flows. On the other hand, our model reaches comparable results when jointly optimizing Balloon1 and Balloon2 scenes where the foreground variations are relatively small and FlowNet successfuly predicts the correct flows of the foregrounds.\n\n| flow estimation method| year  | Balloon1      | Balloon2      | Jumping       | Skating       |\n|------------|------------|---------------|---------------|---------------|---------------|\n| RAFT [1]   | 2020| 0.919 / 0.844 | 0.967 / 0.839 | 0.936 / 0.733 | 0.970 / 0.716 |\n| FlowNet [2] |2015  | 0.926 / 0.855 | 0.974 / 0.766 | 0.921 / 0.636 | 0.965 / 0.591 |\n\nIn addition, we manually add different percentages of noise to the predicted optical flows and test the performance of our model with noisy optical flows. The details are stated **in the Section G.3 in the Appendix** and the results are presented **in the following table** and listed **in Table 13 in the Appendix**. Our model could achieve comparable performance by using the optical flows with small percentages of noise (< 5%). Adding more than 10% noise to the predicted flows leads to a performance decrease in mIOU matrix, which is mainly because our model could not build accurate flow fields from noisy optical flows and hence learns inaccurate semantics from imprecise flows.\n\n| noise scale  | Balloon1      | Balloon2      | Jumping       | Skating       |\n|------------|---------------|---------------|---------------|---------------|\n| w/o. noise   | 0.919 / 0.844 | 0.967 / 0.839 | 0.936 / 0.733 | 0.970 / 0.716 |\n| 1% noise   | 0.923 / 0.844 | 0.963 / 0.820 | 0.937 / 0.715 | 0.970 / 0.707 |\n| 5% noise   | 0.924 / 0.808 | 0.964 / 0.813 | 0.937 / 0.656 | 0.969 / 0.714 |\n| 10 % noise | 0.922 / 0.777 | 0.964 / 0.666 | 0.936 / 0.690 | 0.969 / 0.670 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406601007,
                "cdate": 1700406601007,
                "tmdate": 1700486062167,
                "mdate": 1700486062167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Pr0PYsrWn",
                "forum": "A2mRcRyGdl",
                "replyto": "b8oBGSOfyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you so much for the responses! They are really helpful in understanding the paper!\n\nBy the way, one minor question on the experiment using FlowNet. Why does FlowNet yield better results on Balloon1 and Balloon2 than RAFT? Does it mean that the optical flow signal might not be so critical for better segmentation accuracy? I would assume that flow quality near object boundaries would matter though."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628974789,
                "cdate": 1700628974789,
                "tmdate": 1700628974789,
                "mdate": 1700628974789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jj1YvEseoZ",
                "forum": "A2mRcRyGdl",
                "replyto": "FHiEumEw3D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing the flow map visualization.\n\nThen can I ask how adding noise (or using noisy GT) can improve the performance? As it's based on NeRF, using a better GT would contribute to better accuracy, unlike training-based approaches where noise could make models more robust by perturbing GT."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711797425,
                "cdate": 1700711797425,
                "tmdate": 1700718283555,
                "mdate": 1700718283555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lB2vIF1VEy",
                "forum": "A2mRcRyGdl",
                "replyto": "FHiEumEw3D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response!\n\nTraining Semantic Flow with the noisy optical flow maps leads to the noisy flow field predicted by $\\Psi_{flow}$ in equation (3) in the paper. However, since Semantic Flow takes the flow information as input to predict semantic labels as shown in equations (6)-(9) in the paper, the noisy flows can be considered as an augmentation of flow information. In this way, although the predicted flow field is inaccurate compared to the ground truth flow map, it contributes to the semantic prediction, which is similar to adding Gaussian noise to the inputs for performance improvement in traditional training-based approaches.\n\nAgain, we appreciate the reviewer\u2019s continuous efforts to provide helpful advice and valuable input. If there is any further information or clarification you need from us, please feel free to let us know!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716722081,
                "cdate": 1700716722081,
                "tmdate": 1700721323168,
                "mdate": 1700721323168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uFOXMQ8tWw",
                "forum": "A2mRcRyGdl",
                "replyto": "lB2vIF1VEy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2116/Reviewer_xtsS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I updated my rating to marginally above the acceptance threshold. Clarity has been improved, compared with the initial draft. The authors' responses resolved most of my main concerns."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721698160,
                "cdate": 1700721698160,
                "tmdate": 1700721698160,
                "mdate": 1700721698160,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]