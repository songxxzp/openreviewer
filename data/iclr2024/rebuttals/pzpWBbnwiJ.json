[
    {
        "title": "Universal Guidance for Diffusion Models"
    },
    {
        "review": {
            "id": "sO7tXs18mC",
            "forum": "pzpWBbnwiJ",
            "replyto": "pzpWBbnwiJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6432/Reviewer_SEzD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6432/Reviewer_SEzD"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel method to control generation process by diffusion models using arbitrary guidance functions without any retraining. The proposed method comprises two types of guidance: forward guidance and backward guidance. In the forward guidance, an estimated clean data is used to compute the guidance function, and its gradient is directly employed to adjust the estimated noise at each timestep. On the other hand, in the backward guidance, the adjustment is obtained by multiple step gradient descent, which leads to more faithful results to the constraint incurred by the guidance. The exprimental results show that the proposed method works well across various types of guidance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method can be applied to a wide variety of conditional generation tasks without retraining the base diffusion model.\n\n- In the experimetns, the proposed method performs well in terms of the quality of the generated images. It is also impressive that the proposed method allows for a variety of conditional generation.\n\n- Overall, the manuscript is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "<Major ones>\n\n- The novelty in methodology is marginal.\n  - In the forward guidance, the loss function is computed based on the clean data predicted from the noisy data at each timestep, and its gradient with respect to the original noisy data is used for the guidance. This approach is quite similar to that employed in DPS and FreeDoM [R1]. \n  - In addition, the stepwise refinement is also similar to time-travel strategy in FreeDoM.\n    - [R1] \"FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model,\" ICCV 2023.\n\n- The advantage of the proposed method over existing methods is not clear. The experiments lack comparison with baseline methods, though several related studies are refered in Section 2. Specifically, Diffusion Posterior Sampling (and FreeDoM as well) is closely related to this work and should be compared qualitatively and quantitatively.\n\n\n<Minor ones>\n\n- As any kind of guidance function can be used in the proposed method, it would be interesting to see how its performance varies according to the design of the guidance function. For example, in the case of segmentation map guidance, we may have a lot of publicly-available segmentation models that can be utilized for the guidance. In this case, can we simply choose the best performing model? This point is not clear in the manuscript, because only single model is examined for each conditional generation task."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6432/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6432/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6432/Reviewer_SEzD"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This concern might be too much, but I would like to raise this just in case. The face recognition guidance introduced in the experiments could potentially be used to generate deepfakes of any target person if his/her facial image is available."
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698495077194,
            "cdate": 1698495077194,
            "tmdate": 1699636717982,
            "mdate": 1699636717982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t5Y9znzFP6",
                "forum": "pzpWBbnwiJ",
                "replyto": "sO7tXs18mC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to Reviewer SEzD"
                    },
                    "comment": {
                        "value": "We thank the reviewers for the effort put into reviewing this paper, and we appreciate the valuable insight on related works.\n\n> Comparison with other works\n\nCompared to our algorithm, DPS [R2] attempted to address guided generation using general guidance functions, but they proposed to apply the gradient of the $\\ell_{p}$ loss between the ground-truth condition and the output of the guidance function given a predicted clean image. It is unclear how DPS can handle conditions that cannot be compared using  $\\ell_{p}$ loss, such as bounding box locations, from both method perspectives. On the other hand, our forward guidance using no step-wise refinement can be viewed as a generalization of DPS that accepts any type of condition and guidance function. Please refer to Figure 10 and Section E in the appendix for comparison between whether the guided generation uses step-wise refinement or not. We also remark that, from the method perspective, we make novel technical contributions by proposing backward guidance and a step-wise refinement algorithm to further improve the results. We also empirically demonstrate the success of our algorithm on more complex guidance functions. Please also check our additional comparison with LGD, another recent work on guided image generation, in the general response. The results validate the contribution and novelty of our algorithm, especially on widely-encountered guidance functions in real world.\n\nFor comparison with FreeDoM, due to a highly unusual issue, we believe the discussion should not be visible to the public. We communicate the discussion with AC via a private post and please directly contact the AC for the discussion.\n\n> Different guidance networks for a specific task\n\nIn principle, our algorithm is not restrictive to the guidance function used. So we believe it is totally possible to experiment with different segmentation networks and simply pick the best one for empirical use.\n\n**Reference**\n\n[R1] \"FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model,\" ICCV 2023.\n\n[R2] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. \"Diffusion posterior sampling for general noisy inverse problems.\" arXiv preprint arXiv:2209.14687 (2022)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698289435,
                "cdate": 1700698289435,
                "tmdate": 1700698289435,
                "mdate": 1700698289435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ygv8g21R22",
            "forum": "pzpWBbnwiJ",
            "replyto": "pzpWBbnwiJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6432/Reviewer_xJFT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6432/Reviewer_xJFT"
            ],
            "content": {
                "summary": {
                    "value": "This submission deals with designing a universal guidance for diffusion models that can adapt to various guidance modalities such as segmentation masks, bounding boxes, in a plug-and-play fashion without any retraining from the scratch or any finetuning. To solve the problem, it proposes to add a loss gradient term to the denoising score function. Let c be the guidance that is the output of some function c=f(z0) for the unknown sample z0. Then the loss is defined to be defined between c and f(\\hat{z}), where \\hat{z} is an estimate for the z0 based on noisy observation zt based on MMSE estimation. This approximation has been commonly used in the diffusion literature e.g., in DDIM, DPS [Chung et al\u201922 ], PGDM [Song et al\u201922]. Experiments show that this loss-guidance works to properly guide stable diffusion sampling. \n\n[Song et al\u201922] Song, Jiaming, Arash Vahdat, Morteza Mardani, and Jan Kautz. \"Pseudoinverse-guided diffusion models for inverse problems.\" In International Conference on Learning Representations. 2022.\n\n[Chung et al\u201922 ] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. \"Diffusion posterior sampling for general noisy inverse problems.\" arXiv preprint arXiv:2209.14687 (2022).\n\n[Song et al\u201923] Song, Jiaming, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. \"Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation.\" (2023)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Controllable generation from diffusion models without re-training or finetuning is an important problem"
                },
                "weaknesses": {
                    "value": "The idea in this work doesn't seem to be novel. Loss guidance for diffusion models for the same purpose has been studied in previous works that have not been cited; see [Song et al\u201923]. Also, the idea of using \\hat{z} to approximate z0 based on the score has been used several times in the samping diffusion models for example for inverse problems as in PGDM [Song et al\u201922], and DPS [Chung et al\u201922 ]. \n\n\n[Song et al\u201923] Song, Jiaming, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. \"Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation.\" (2023)."
                },
                "questions": {
                    "value": "The authors need to clarify the contributions of this work compared with previous works, especially the ones in PGDM [Song et al\u201922], DPS , and [Song et al\u201923]. I am willing to change my score if the author could clarify the contributions and differences from the work in [Song et al\u201923]."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630232900,
            "cdate": 1698630232900,
            "tmdate": 1699636717850,
            "mdate": 1699636717850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aL6SxbeJCC",
                "forum": "pzpWBbnwiJ",
                "replyto": "ygv8g21R22",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to Reviewer xJFT"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the effort put into reviewing this paper, and we appreciate the valuable reference the reviewer brings up. While the papers mentioned by the reviewers have explored various ways to perform loss-guided generation on diffusion models, our work exhibits critical differences and novel contributions compared to these prior arts. We discuss the difference individually for each paper mentioned by the reviewer.\n\n> Comparison with PGDM\n\nPGDM [Song et al\u201922] assumes the existence of a pseudo-inverse function given the guidance function, and it is unclear in the paper how their algorithm can be extended to general guidance functions. In contrast, our proposed universal guidance combines forward and backward guidance to achieve guided image generation using any guidance function, where backward guidance is a more general algorithm that reduces to PGDM if the pseudo inverse function is known.\n\n> Comparison with DPS\n\nDPS [Chung et al\u201922] attempted to address guided generation using general guidance functions, but they proposed to apply the gradient of the $\\ell_{p}$ loss between the ground-truth condition and the output of the guidance function given a predicted clean image. It is unclear how DPS can handle conditions that cannot be compared using  $\\ell_{p}$ loss, such as bounding box locations, from both method and empirical perspectives. On the other hand, our forward guidance can be viewed as a generalization that accepts any type of condition and guidance function. We also propose backward guidance and a step-wise refinement algorithm to further improve the results, and demonstrate the generation quality on complex tasks such as segmentation map guidance and object location guidance.\n\n> Comparison with [Song et al\u201923]\n\nThe method proposed in [Song et al\u201923], which we abbreviate as LGD, also addresses the issue of DPS and can handle general guidance function. They further proposed to compute the guidance gradient based on the loss average over different noisy samples of the predicted clean image. Different from their work, we propose a universal refinement algorithm that refines the sampling in each diffusion step by reinjecting suitable Gaussian noise. On top of that, we also propose backward guidance that further enhances the match between the generated images and the condition. Please also check our additional comparison with LGD in the general response, which validates the contribution and novelty of our algorithm, especially on widely-encountered guidance functions empirically.\n\n> Summary\n\nIn summary, our algorithm offers novel technical contributions that are significantly different from prior works by combining forward guidance, backward guidance and step-wise refinement. Our paper also offers novel empirical contributions by showing the success on more sophisticated and empirically applicable guidance functions such as segmentation and object detection networks. We hope the discussion and the experiment address the reviewer's concern about novelty and contribution, and hope that the reviewer will raise the final rating of the paper.\n\n\n\n\n\n**Reference**\n\n[Song et al\u201922] Song, Jiaming, Arash Vahdat, Morteza Mardani, and Jan Kautz. \"Pseudoinverse-guided diffusion models for inverse problems.\" In International Conference on Learning Representations. 2022.\n\n\n[Chung et al\u201922] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. \"Diffusion posterior sampling for general noisy inverse problems.\" arXiv preprint arXiv:2209.14687 (2022).\n\n[Song et al\u201923] Song, Jiaming, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. \"Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation.\" (2023)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698169308,
                "cdate": 1700698169308,
                "tmdate": 1700698250305,
                "mdate": 1700698250305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HJNdxE4z44",
            "forum": "pzpWBbnwiJ",
            "replyto": "pzpWBbnwiJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6432/Reviewer_Dj3j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6432/Reviewer_Dj3j"
            ],
            "content": {
                "summary": {
                    "value": "Sampling images from pretrained diffusion models using conditions is now a very hot topic and plays very important roles in many application scenarios. There are two categories of methods for conditional generation: 1) retraining with new conditions, such as Control-Net; 2) using a guidance function to optimize a latent in the pretrained image space. Considering the cost of re-training, this paper targets a universal guidance strategy for conditional sampling. The experiments demonstrate the results of many different conditions: classifier label, human identity, segmentation maps and object locations etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The motivation is very clear and the writing is very easy-to-follow. \n- Evenly the idea seems simple, it is novel and reasonable. \n- The experiments contain many different condition scenarios and the results are visually good."
                },
                "weaknesses": {
                    "value": "My major concern is on the experiments:\n\n* It lacks ablation: as claimed in Sec 3.1, \"directly replacing fcl and lce with any off-the-shelf guidance and loss functions does not work in practice\". I think it is needed to include an experiment to support this claim. In addition, only \"object location guidance\" in Sec 4.2 provided the ablation of forward and backward guidance, the results for at least more than one tasks are helpful. \n\n* I appreciate the experiments on conditional stable diffusion in Sec4.1 and unconditional imagenet diffusion in Sec 4.2. However, I am curious why not also conducting segmentation map guidance in 4.2. I think including same task in both 4.1 and 4.2 can be helpful for readers to understand the differences between the two diffusion models. \n\n* My primary concern is: all presented visual results are not diverse - dogs are everywhere. I also checked all results in the supplemental, it is similar. How about other cases?"
                },
                "questions": {
                    "value": "See weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838598023,
            "cdate": 1698838598023,
            "tmdate": 1699636717730,
            "mdate": 1699636717730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PRDKgITHHj",
                "forum": "pzpWBbnwiJ",
                "replyto": "HJNdxE4z44",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to Reviewer Dj3j"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the effort put into reviewing this paper, and we appreciate the insightful feedback on further ablation studies. Please check the experiments below for each individual question raised.\n\n\n> Ablation between our algorithm and directly replacing fcl and lce with off-the-shelf guidance (Eq. 5).\n\nAs suggested by reviewers, we also present 8 more combinations of text and bounding boxes. We summarize the quantitative results in the table below, and present the qualitative results in Figure 1 of the [anonymized rebutall figures collection](https://drive.google.com/file/d/1DVmdqZ8QBcopO0npQLXGWdDDWQyZp3Qa/view?usp=sharing). The value is obtained by averaging over 10 random generations. We note that the \"text\" in the following table refers to a combination of object locations and text phrases, and please see the figure for details of each combination. Both quantitative and qualitative results demonstrate that using our proposed forward guidance with Eq. 6 leads to a much better match with the external guidance compared to Eq. 5 for off-the-shelf guidance functions.\n\n|         | text 1 | text 2 | text 3 | text 4 | text 5 | text 6 | text 7 | text 8|\n|---      | ---    | ---    | ---    | ---    | ---    | ---    | ---    | ---   |\n|Eq. 6 (Ours)|0.51 | 0.69   | 0.69   | 0.92   | 0.44   | 0.61   | 0.55| 0.80|\n|Eq. 5       | 0.13| 0.01   | 0.01   | 0.08   | 0.02   | 0.01   | 0.02| 0.06|\n\n\n> Segmentation guidance on ImageNet diffusion model\n\nAs suggested by the reviewer, we also present the generation results on ImageNet diffusion model using segmentation guidance in Figure 2 and Figure 3 of the [anonymized rebuttal figures collection](https://drive.google.com/file/d/1DVmdqZ8QBcopO0npQLXGWdDDWQyZp3Qa/view?usp=sharing). In particular, Figure 2 shows images generated with various segmentation maps, demonstrating that our algorithm is effective regardless of the underlying diffusion model. Figure 3 includes comparison between generations using ImageNet diffusion model and Stable Diffusion with the same segmentation mask.\n\n> Generations using objects other than dog as conditions\n\nIn Figure 1 of the [anonymized rebuttal figures collection](https://drive.google.com/file/d/1DVmdqZ8QBcopO0npQLXGWdDDWQyZp3Qa/view?usp=sharing), we show generations with various combinations of dogs and cats in different positions. The results indicate that our algorithm can generate images beyond the \"dog category\", and we hope this alleviates the reviewer's concern about the diversity of generated images."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698125753,
                "cdate": 1700698125753,
                "tmdate": 1700698125753,
                "mdate": 1700698125753,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3yjTwEPTlQ",
            "forum": "pzpWBbnwiJ",
            "replyto": "pzpWBbnwiJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6432/Reviewer_9puQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6432/Reviewer_9puQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a universal guidance algorithm to make the diffusion model condition on multiple modalities without retraining the model. The core idea is to use the estimated clean image in the intermediate step to provide the classifier guidance. The experiments demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The proposed method enables the pretrained diffusion model to condition multiple modality controls without any retraining.\n\n2) The paper is well-written and easy to follow.\n\n3) The experiment demonstrates the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1) The method uses the predicted clean image as the input of the conditioning model. Would the inaccurate predicted clean image affect the performance?\n\n2) Some previous work [1, 2, 3] on the conditional diffusion model should be discussed. \n\n[1] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang#, Zhongang Qi, Ying Shan, Xiaohu Qie. T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models\n\n[2] Lvmin Zhang, Anyi Rao, Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models\n\n[3] Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, Ziwei Liu. Collaborative Diffusion for Multi-Modal Face Generation and Editing."
                },
                "questions": {
                    "value": "The method uses the predicted clean image as the input of the conditioning model. Would the inaccurate predicted clean image affect the performance? The authors need to study this issue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699171511916,
            "cdate": 1699171511916,
            "tmdate": 1699636717616,
            "mdate": 1699636717616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uAfRqdKM5V",
                "forum": "pzpWBbnwiJ",
                "replyto": "3yjTwEPTlQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to Reviewer 9puQ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the effort put into reviewing the paper and providing valuable feedback.\n\n> Would the inaccurate predicted clean image affect the performance?\n\nIndeed, as the reviewer mentioned, we use predicted clean images to compute the guidance signal and perform guided image generation. We argue that inaccurate clean image prediction has limited impact due to the step-by-step nature of image sampling for diffusion model. Specifically, impacts of guidance using gradient signal is averaged out across different sampling step. In the earlier sampling steps, the guidance signal provides coarse adjustment to the image based on comparatively inaccurate clean image predictions. As the sampling step progresses, the clean image prediction becomes more accurate and the corresponding guidance signal further refines the generated image. The success of our algorithm also validates the reasoning.\n\n> Discussion of some previous works \n\nWe thank the reviewer for providing pointers to these valuable references. Collectively, these reference papers belong to a family of algorithms that freezes the unconditional diffusion backbone and train auxiliary models for controlled image generation. This family of algorithms has a different goal in mind compared to our proposed Universal Guidance, where we aim for an algorithm that does not require re-training at all for different conditional generation tasks. Nevertheless, we believe this family of algorithms is an interesting middle ground between our algorithm and full-blown conditional training,  and presents exciting directions for further research. We will include the related discussion upon further revision of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698052170,
                "cdate": 1700698052170,
                "tmdate": 1700698052170,
                "mdate": 1700698052170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bb5M3xHkCz",
                "forum": "pzpWBbnwiJ",
                "replyto": "uAfRqdKM5V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6432/Reviewer_9puQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6432/Reviewer_9puQ"
                ],
                "content": {
                    "title": {
                        "value": "After Rebuttal Comments"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for addressing my concerns. I will keep my original rating for accepting this paper.\n\nThanks,"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711733534,
                "cdate": 1700711733534,
                "tmdate": 1700711733534,
                "mdate": 1700711733534,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]