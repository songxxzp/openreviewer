[
    {
        "title": "ZegOT: Zero-shot Segmentation Through Optimal Transport of Pixels to Text Prompts"
    },
    {
        "review": {
            "id": "KzhZp0iSOl",
            "forum": "WA2mZrDTAH",
            "replyto": "WA2mZrDTAH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4612/Reviewer_41mH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4612/Reviewer_41mH"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel method called ZegOT for zero-shot semantic segmentation, which transfers text-image aligned knowledge to pixel-level classification without the need for additional image encoders or retraining the CLIP module. ZegOT utilizes a Multiple Prompt Optimal Transport Solver (MPOT) to learn an optimal mapping between multiple text prompts and pixel embeddings of the frozen image encoder layers. This allows each text prompt to focus on distinct visual semantic attributes and diversify the learned knowledge to handle previously unseen categories effectively."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The experimental results lead the existing state-of-the-art (SOTA)  under the same settings on some datasets.\n\n\n+ The author modeled the optimal transport problem into the segmentation task of open vocabulary, providing a new approach, and this module can alleviate the problem of overfitting to the seed class."
                },
                "weaknesses": {
                    "value": "-\tThe technical insight may not be enough The Deep Feature Alignment module proposed by the author is equivalent to extending the Relationship Descriptor based on Zegclip to multi level features, with the core still being the Relationship Descriptor. In addition, similar to CoOp's text prompt learning, would it be better to directly apply it to previous methods such as ZegClip?\n\n-\tThe experiment setting is not clear to me. For example, If the proposed method can effectively solve the problem of overfitting the network parameters to the seed class distribution after training, why not conduct a set of experiments under the setting of Inductive (unseen class names and images are not accessible during training.). In Table 2, the mIoU (U) and miou (S) of ZegCLIP are 87.3 and 92.3 respectively, but hiou should not be 91.1 and should be 89.7. The setting used in the experiment in table2 is Conductive. But does Tabel3 seem to be using Inductive settings? No explanation was provided, and would using Transductive settings be better than the previous method. In addition, the author claims to have obtained a sota, but it is 2.2 hiou lower on COCO-Stuff164K. For table4, I would like to know how much improvement can be achieved by using only MPOT compared to baseline.\n\n\n-\tFormula 17 is written incorrectly."
                },
                "questions": {
                    "value": "seeing Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4612/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666716831,
            "cdate": 1698666716831,
            "tmdate": 1699636440184,
            "mdate": 1699636440184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "igmn6weOlb",
                "forum": "WA2mZrDTAH",
                "replyto": "KzhZp0iSOl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4612/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4612/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 41mH"
                    },
                    "comment": {
                        "value": ">**W1-1**. The technical insight may not be enough. The Deep Feature Alignment module proposed by the author is equivalent to extending the Relationship Descriptor based on Zegclip to multi level features, with the core still being the Relationship Descriptor.\n\n-> As the reviewer correctly pointed out, DLFA is an extension of the Relationship Descriptor in ZegCLIP. However, in this work, we introduce the concept of multiple-level applications of the Relationship Descriptor for the first time. Additionally, we would like to kindly emphasize that the novelty of ZegOT extends beyond DLFA, encompassing MPOT and thorough technical analysis. We encourage you to explore the various aspects of our proposed methodology beyond DLFA.\n\n>**W1-2**. In addition, similar to CoOp's text prompt learning, would it be better to directly apply it to previous methods such as ZegClip?\n\n-> Thanks for your concrete suggestion. Per your suggestion, we conducted the experiment to incorporate MPOT into ZegCLIP. Due to the instinctual nature of the framework including network architecture, we integrate MPOT into the last layer of decoder in ZegCLIP.  To follow the original scheme of ZegCLIP, we use the multiple hand-crafted prompts and use the same number of multiple prompts, as 4. The comparison results are presented in the following table, and add the related contents in the revised manuscript.\n\n***Table.*** Performance of ZegCLIP + MPOT on PASCAL datasets.\n| Methods         | Multiple Prompts (N=4) | IoU(U) | IoU(U) | IoU(S) | hIoU |\n|-----------------|:------:|:------:|:----:|:----:|:----:|\n| ZegCLIP           |  X |  X  |89.9| 92.3 | 91.1 |\n| ZegCLIP(reproduced) |  X |  X | 89.7 | 92.2 | 90.9 |\n| ZegCLIP (a)        |  O  |  X  | 92.3 | 93.5 | 92.9 |\n| ZegCLIP (b) |  O |  O  | 93.0 | 92.8 | 92.9 |\n\n>**W2-1**. The experiment setting is not clear to me. For example, If the proposed method can effectively solve the problem of overfitting the network parameters to the seed class distribution after training, why not conduct a set of experiments under the setting of Inductive (unseen class names and images are not accessible during training.).\n\n->  In general, zero-shot semantic segmentation performance in the inductive setting is quite limited to beat the supervised methods and be directly utilized in practice. In this paper, we focus on improving zero-shot segmentation performance which can be comparable to that of the supervised learning methods, thus we conduct experiments under the transductive setting. By efficiently utilizing only class names for unseen classes, our methodology under the transductive setting boosts overall segmentation performance. Nevertheless, per the reviewer\u2019s suggestion, we have implemented the proposed framework in the inductive setting and proved the comparable results to SOTA methods as in the following table.\n\n***Table 1.*** Performance comparison on PASCAL VOC 2012 dataset for the Inductive setting.\n| Methods   | IoU(U) | IoU(S) | hIoU |\n|-----------|:------:|:------:|:----:|\n| ZegFormer |  63.6  |  86.4  | 73.3 |\n| Zsseg     |  72.5  |  83.5  | 77.6 |\n| ZegCLIP   |  77.8  |  91.9  | 84.3 |\n| Ours      |  71.6  |  86.5  | 78.4 |\n>**W2-2**.In Table 2, the mIoU (U) and mIoU (S) of ZegCLIP are 87.3 and 92.3 respectively, but hIoU should not be 91.1 and should be 89.7. \n\n-> Thanks for your careful comments. We have updated the results of ZegCLIP in the revised paper. \n\n>**W2-3**. The setting used in the experiment in table2 is Conductive. But does Table 3 seem to be using Inductive settings? No explanation was provided, and would using Transductive settings be better than the previous method.\n\n-> Thanks for your careful observation and valuable recommendation. As the reviewer pointed out, we trained our model using transductive settings on the source dataset and solely performed inference on the target test set. To ensure a fair comparison with other methods, we employ the weight of ZegCLIP, which is trained using transductive settings. \n\n>**W2-4**. In addition, the author claims to have obtained a sota, but it is 2.2 hIoU lower on COCO-Stuff164K. \n\n-> Thanks for your valuable comments. Per your suggestion, we have narrowed the claim down and revised it. \n\n>**W2-5**. For table4, I would like to know how much improvement can be achieved by using only MPOT compared to baseline.\n\n->The reviewer is kindly reminded that the results you referred to are already indicated in Table 4 (b) and marked with an 'X'. These results confirm that our proposed MPOT significantly enhances performance, demonstrating a substantial margin of improvement with 2.9 and 2.4 mIoU on the PASCAL VOC and Pascal Context datasets, respectively. \n\n>**W3**.Formula 17 is written incorrectly.\n\n-> Thanks for your detailed comments. We have corrected this formulation in the revised paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4612/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700402248255,
                "cdate": 1700402248255,
                "tmdate": 1700402248255,
                "mdate": 1700402248255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rhHegII9x4",
                "forum": "WA2mZrDTAH",
                "replyto": "KzhZp0iSOl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4612/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4612/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer 41mH"
                    },
                    "comment": {
                        "value": "As the deadline for the Reviewer-Author discussion phase is fast approaching (there is only a day left), we respectfully ask whether we have addressed your questions and concerns adequately."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4612/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556833925,
                "cdate": 1700556833925,
                "tmdate": 1700556833925,
                "mdate": 1700556833925,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RmrQFkiElU",
                "forum": "WA2mZrDTAH",
                "replyto": "igmn6weOlb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4612/Reviewer_41mH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4612/Reviewer_41mH"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I appreciate the author's efforts in responding and conducting additional experiments. However, after a thorough review of their responses and the concerns raised by other reviewers, I still believe that this paper does not meet the high threshold of significant contribution required for acceptance at ICLR, even with the new experimental data.\n\n\nIn Table 12, the results of the proposed method in the inductive setting significantly lag behind the current state-of-the-art. If the method is effective for zero-shot semantic segmentation, it should not result in a decrease in accuracy.\n\n\nIn Table 13, the use of MPOT shows a negligible improvement, equivalent to the hIoU metric without MPOT, and is comparable to ZegCLIP with multiple text prompts.\n\n\nTherefore, I hold my initial assessment of the manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4612/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571947327,
                "cdate": 1700571947327,
                "tmdate": 1700571947327,
                "mdate": 1700571947327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K6sqmPigBJ",
            "forum": "WA2mZrDTAH",
            "replyto": "WA2mZrDTAH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4612/Reviewer_Dhw2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4612/Reviewer_Dhw2"
            ],
            "content": {
                "summary": {
                    "value": "This paper utilizes the large-scale CLIP model to solve the zero-shot semantic segmentation task. In this paper, the authors have proposed a novel Multiple Prompt Optimal Transport Solver (MPOT) module, which is designed  to learn an optimal mapping between multiple\ntext prompts and pixel embeddings of the frozen image encoder layers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method solves zero-shot segmentation from a new perspective. They propose optimal transport to enable the alignment between text and pixel space."
                },
                "weaknesses": {
                    "value": "1. The authors include ZegCLIP to \" trainable image encoder-based approaches\". However, they fix the image encoder and train a new decoder. Such a statement is not accurate. \n2. In related work, the authors do not introduce open vocabulary semantic segmentation, which is highly related to this topic in this paper.\n3. The performance of COCO-Stuff does not outperform ZegCLIP.  It seems that the proposed method is more useful on simple images such as PASCAL VOC.\n4. Lack of inference speed comparison with previous methods. As the authors propose several blocks into the ZS3 framework,  it is quite essential to consider the inference speed.\n5. The optimal transport plan is just like a spatial attention map for each class and each prompt, I guess adding a self-attention layer or learnable spatial attention maps is also effective."
                },
                "questions": {
                    "value": "It seems that the proposed Optimal Transport-based method can be plugged into other methods, such as ZegCLIP. Is it possible to use Optimal Transport in ZegCLIP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4612/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820883465,
            "cdate": 1698820883465,
            "tmdate": 1699636440102,
            "mdate": 1699636440102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XXVPVbPRWW",
                "forum": "WA2mZrDTAH",
                "replyto": "K6sqmPigBJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4612/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4612/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Dhw2"
                    },
                    "comment": {
                        "value": ">**W1**. The authors include ZegCLIP to \"trainable image encoder-based approaches\". However, they fix the image encoder and train a new decoder. Such a statement is not accurate. \n\n-> Thanks for your careful comment. This was because ZegCLIP employs a vision prompt tuning approach for tuning the image encoder, which we consider as a trainable encoder.  That being said, we agree with the reviewer, so we now categorize ZegCLIP as a frozen image encoder-based approach.\n\n>**W2**. In related work, the authors do not introduce open vocabulary semantic segmentation, which is highly related to this topic in this paper. \n\n-> Thanks for your concrete and helpful comment. We have expanded the scope of zero-shot semantic segmentation to include open-vocabulary semantic segmentation topic in the related work section 2.2 in the revised paper.\n\n>**W3**.The performance of COCO-Stuff does not outperform ZegCLIP. It seems that the proposed method is more useful on simple images such as PASCAL VOC.\n\n-> As the reviewer pointed out, we admit that relying on a few learnable text prompts and lightweight decoder with limited trainable parameters limits the room for performance improvement on a large-scale dataset, compared to the retraining/fine-tuning of the image backbone. We expect this may be solved in a future study, by introducing an advanced MPOT, which balances the trainable parameters and performance for large-scale datasets. We have updated this limitation in the revised paper.\n\n>**W4**. Lack of inference speed comparison with previous methods. As the authors propose several blocks into the ZS3 framework, it is quite essential to consider the inference speed.\n\n-> Per the reviewer\u2019s valuable comment, we have measured actual runtime for inference on an NVIDIA GeForce RTX 3090 using the PASCAL VOC dataset. Despite the slight  increase in runtime latency compared to ZegCLIP, our method is superior compared to the method including Zsseg, and ZegFormer as shown in the following table.\n\n***Table 4.*** Comparison inference speed with other methods on the same device, a single GeForce RTX 3090 device. \n| Methods         | inference speed (image per seconds)|\n|-----------------|:------:|\n| Zsseg           |  4.24 |\n| ZegFormer |  6.86 |\n| ZegCLIP        |  10.8  | \n| Ours |  8.9 | \n\n>**W5**. The optimal transport plan is just like a spatial attention map for each class and each prompt, I guess adding a self-attention layer or learnable spatial attention maps is also effective.\n\n-> The reviewer is kindly reminded that a comparison with the results of the self-attention layer is already presented in Table 4 (b) of the main paper, as indicated  'Self-attention'. Additionally, we compare the results with other matching methods using a one-to-one mapping, referred to as the Bipartite matching method, within the same table. As you correctly commented, both the Self-attention and the Bipartite matching methods yield meaningful results; however, our proposed ZegOT outperforms these methods on both the Pascal VOC and Pascal Context datasets.\n\n>**Q1**. It seems that the proposed Optimal Transport-based method can be plugged into other methods, such as ZegCLIP. Is it possible to use Optimal Transport in ZegCLIP?\n\n-> Thanks for your concrete suggestion. Per your suggestion, we conducted the experiment to incorporate MPOT into ZegCLIP. Due to the instinctual nature of the framework including network architecture, we integrate MPOT as an additional pipeline for computing loss. To follow the original scheme of ZegCLIP, we use the multiple hand-crafted prompts and use the same number of multiple prompts, as 4. The comparison results are presented in the following table, and we demonstrate that our proposed MPOT acts as a plug-in module for further boosting the performance of the SOTA model. We have added the related contents in the revised manuscript.\n\n***Table 5.*** Performance of ZegCLIP + MPOT on PASCAL datasets.\n| Methods         | Multiple Prompts (N=4) | IoU(U) | IoU(U) | IoU(S) | hIoU |\n|-----------------|:------:|:------:|:----:|:----:|:----:|\n| ZegCLIP           |  X |  X  |89.9| 92.3 | 91.1 |\n| ZegCLIP(reproduced) |  X |  X | 89.7 | 92.2 | 90.9 |\n| ZegCLIP (a)        |  O  |  X  | 92.3 | 93.5 | 92.9 |\n| ZegCLIP (b) |  O |  O  | 93.0 | 92.8 | 92.9 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4612/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700402209745,
                "cdate": 1700402209745,
                "tmdate": 1700402209745,
                "mdate": 1700402209745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eKamLUqyvh",
                "forum": "WA2mZrDTAH",
                "replyto": "XXVPVbPRWW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4612/Reviewer_Dhw2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4612/Reviewer_Dhw2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' detailed replies. Although the idea is good, the performance (especially for COCO-Stuff) is still the biggest problem of this paper, attracting the attention of another reviewer.  Therefore, I keep my rating score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4612/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660472407,
                "cdate": 1700660472407,
                "tmdate": 1700660472407,
                "mdate": 1700660472407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8tDjiZFRhE",
            "forum": "WA2mZrDTAH",
            "replyto": "WA2mZrDTAH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4612/Reviewer_kYrk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4612/Reviewer_kYrk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework that utilizes a frozen vision-language model (CLIP) for zero-shot semantic segmentation. The proposed method learns a set of text prompts that align with pixel embedding at different scales. Since the learn text prompts have coherence and usually have similar score maps with image features, they propose to refine the score map with optimal transport to make them more diverse. The method shows great performance on ZS3 under Pascal VOC, and Pascal Context."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea is interesting. They show that learning multiple prompts leads to similar score map for each prompt. Then they propose a method to make different prompts to have different score maps, which improves the diversity of the score maps. Optimal transport fits this purpose. An adequate comparison with other alignment methods such as bipartite matching and self-attention has been shown in the ablation study."
                },
                "weaknesses": {
                    "value": "- What is the reason behind the similarity of the multiple learned prompts?\n- The performance of ZegCLIP is different from that in the original paper. \n- It seems the method is trained in a transductive setting; What is the performance in an inductive setting?\n- How does this work under different backbones? Previous works sometimes use Resnet for ZS3. Please consider a comparison.\n- The number of classes seems small. What are the results on datasets with larger numbers of classes, such as Pascal Context 459 and ADE-847?"
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4612/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699081859345,
            "cdate": 1699081859345,
            "tmdate": 1699636440029,
            "mdate": 1699636440029,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dCKSNNbHve",
                "forum": "WA2mZrDTAH",
                "replyto": "8tDjiZFRhE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4612/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4612/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer kYrk"
                    },
                    "comment": {
                        "value": ">**W1**. What is the reason behind the similarity of the multiple learned prompts?\n\n-> Thanks for your valuable comments. If we do not use our proposed optimal transport, the approach of using multiple learnable text prompts becomes aligning the average features of text prompts with the visual features, yielding all the text prompts cohered. Unfortunately, this cohered text prompts are not well-aligned with diverse features of semantic visual embeddings. Similar negative observations were made in the PLOT, where they conducted image classification with multiple prompts [1]. This is our main motivation of using optimal transport for multi-prompt alignment.\n\n[1] Chen et.al. PLOT: Prompt Learning with Optimal Transport for Vision-Language Models, ICLR2023\n\n>**W2**. The performance of ZegCLIP is different from that in the original paper. \n\n-> Thanks for your careful observation. We have updated the latest results of ZegCLIP in the revised paper. \n\n\n>**W3**. It seems the method is trained in a transductive setting; What is the performance in an inductive setting?\n\n-> In general, zero-shot semantic segmentation performance in the inductive setting is quite limited to beat the supervised methods, so that it cannot be directly utilized in practice. Accordingly, our methodology has focused on improving zero-shot segmentation performance, which can be comparable to that of the supervised learning methods. Consequently, we conduct experiments under the transductive setting, by efficiently utilizing only class names for unseen classes to boost overall segmentation performance. Nevertheless, per the reviewer\u2019s suggestion, we have implemented the proposed framework in the inductive setting and proved the comparable results to SOTA methods as in the following table.\n\n\n\n***Table 1.*** Performance comparison on PASCAL VOC 2012 dataset for the Inductive setting.\n| Methods   | IoU(U) | IoU(S) | hIoU |\n|-----------|:------:|:------:|:----:|\n| ZegFormer |  63.6  |  86.4  | 73.3 |\n| Zsseg     |  72.5  |  83.5  | 77.6 |\n| ZegCLIP   |  77.8  |  91.9  | 84.3 |\n| Ours      |  71.6  |  86.5  | 78.4 |\n\n>**W4**. How does this work under different backbones? Previous works sometimes use Resnet for ZS3. Please consider a comparison.\n\n-> Thanks for the constructive comment. We investigated different backbones used in various baseline models for ZS3  as shown in Table 2 below, and the ResNet backbones are mostly utilized as feature backbone. That being said, we don\u2019t need the feature backbone, since our architecture is VLM and decoder-only architecture. Accordingly, we believe that  the suggested experiments with different backbone are not necessary to verifying the performance of our algorithm.\n\n\n***Table 2.*** Different backbones for ZS3.\n| Model     | VLM(CLIP) | Feautre Backbone | Decoder |\n|-----------|:------:|:------:|:----:|\n| zsseg |  ViT |  ResNet  | - |\n| ZegFormer     |  ViT  |  ResNet  | FPN |\n| FreeSeg   |  ViT  |  ResNet  | - |\n| MVP-SEG      |  ViT  |  DeepLab  | - |\n| ZegCLIP      |  ViT  |  -  | Light-weight ViT  |\n| Ours      |  ViT  |  -  | FPN / ASPP |\n\n>**W5**. The number of classes seems small. What are the results on datasets with larger numbers of classes, such as Pascal Context 459 and ADE-847?\n\n-> Thanks for the great comment. To demonstrate our proposed model\u2019s generalization performance on large-scale datasets with larger numbers of classes, we conduct open vocabulary segmentation by training the model with full COCO-171. We are running the experiments now, however, due to the limited computational resources, the results are not prepared. We will try to update the results before the end of the rebuttal period."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4612/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700402167448,
                "cdate": 1700402167448,
                "tmdate": 1700402167448,
                "mdate": 1700402167448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]