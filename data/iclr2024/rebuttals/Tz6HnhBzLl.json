[
    {
        "title": "Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach"
    },
    {
        "review": {
            "id": "hdx0QzG92R",
            "forum": "Tz6HnhBzLl",
            "replyto": "Tz6HnhBzLl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3129/Reviewer_Uwp8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3129/Reviewer_Uwp8"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies Robust Reinforcement Learning (RRL) within the framework of positional differential game theory, presenting a novel approach to understanding agents' robust policies and deterministic payoff values. This paper introduces two algorithms: Isaacs Deep Q-Networks (IDQN) and Decomposed Isaacs Deep Q-Networks (DIDQN). Through theoretical proofs, under the Isaacs\u2019s condition, it shows that a centralized Q-learning approach can be developed for these problems. Empirical results underscore the effectiveness of these algorithms against other RRL and Multi-Agent RL baselines, also proposing a new framework for evaluating the robustness of trained policies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Firstly, it offers an exhaustive literature review, meticulously drawing contrasts between the current work and existing literature. Moreover, the clarity of writing facilitates comprehension, making the paper accessible to a broad spectrum of readers. Lastly, the experimental results provided is convincing, adding weight to the authors' arguments and hypotheses."
                },
                "weaknesses": {
                    "value": "1. While the authors adeptly show the significance of considering robustness in RL in the introduction, they do not clearly state why there's a pressing need to study robustness in the framework of positional differential game theory. This oversight makes the paper's motivation less pronounced and leaves readers questioning the specific choice of this framework.\n\n2. One point of confusion for me is in Section 3 where the authors state that \"to solve differential games by RL algorithms, it is necessary to discretize them in time\uff0cwe describe such a discretization...\". If employing RL to address differential games still necessitates discretization, then why consider robust RL problems within the framework of differential games? Wouldn't it be more straightforward to tackle the issue directly within the framework of discrete robust MDPs? What additional advantages does considering robustness in differential games provide? Furthermore, how does the efficacy of using discretization to address differential games compare with directly solving a discrete robust MDP?"
                },
                "questions": {
                    "value": "Please see the Weaknesses, I will finalize my rating after rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Non"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3129/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3129/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3129/Reviewer_Uwp8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698344723146,
            "cdate": 1698344723146,
            "tmdate": 1700517355137,
            "mdate": 1700517355137,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6g2IrULFIp",
                "forum": "Tz6HnhBzLl",
                "replyto": "hdx0QzG92R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you very much for highlighting the positive points of the paper. We are delighted that you liked the literature review, presentation and results. Below, we provide comments regarding your doubts.\n\n***While the authors adeptly show the significance of considering robustness in RL in the introduction, they do not clearly state why there's a pressing need to study robustness in the framework of positional differential game theory. This oversight makes the paper's motivation less pronounced and leaves readers questioning the specific choice of this framework.***\n\nThank you for this important comment. We tried to explain the advantage of the differential game concept compared with Markov games in mixed policies in the third paragraph of the Introduction section and compared with Markov games in pure policies before Theorem 1. Perhaps we did this not fully articulated, and to correct this, we added one paragraph to the beginning of section 2, further explaining the motivation to use differential games. Its contents are as follows:\n\n*Recent studies consider RRL problems within the framework of zero-sum Markov games in pure or mixed policies. In the case of pure policies, it is known (e.g., paper-rock-scissors) that Markov games may not have a value (Nash equilibrium), which conceptually prevents the development of centralized learning algorithms based on shared q-functions. In the case of mixed policies, such formalization may also be inappropriate if, according to the problem statement (for example, in the case of developing expensive or safe control systems), it is required to seek robust policies guaranteeing a deterministic payoff value. In this section, we describe the framework of the positional differential games, which allows us, on the one hand, to consider the pure agents' policies and deterministic values of payoffs and, on the other hand, to obtain the fact of the existence of a value in a reasonably general case.*\n\n***One point of confusion for me is in Section 3 where the authors state that \"to solve differential games by RL algorithms, it is necessary to discretize them in time\uff0cwe describe such a discretization...\". If employing RL to address differential games still necessitates discretization, then why consider robust RL problems within the framework of differential games? Wouldn't it be more straightforward to tackle the issue directly within the framework of discrete robust MDPs? What additional advantages does considering robustness in differential games provide?***\n\nWe are not sure that there is a well-established concept of robust MDP. Basically, Markov games with pure or mixed policies are taken as such. We tried to describe the advantage of differential games compared with both of these options in the answer to the previous question. If you mean Markov games in pure policies described in Section 3 by robust MDP, then, as we noted above and in the paper (before Theorem 1), their main drawback is the non-existence of value (Nash equilibrium) in general case, which conceptually prevents the use of a shared q-function. However, as we show in Theorem 1, the fact that we consider not arbitrary games but the games which are a time-discretization of differential games satisfying Isaacs's condition allows us to theoretically justify that with a sufficiently small time-discretization step, the shared q-function can be used. If you meant something else by robust MDP, please feel free to clarify, and we will also be glad to discuss this.\n\n***Furthermore, how does the efficacy of using discretization to address differential games compare with directly solving a discrete robust MDP?***\n\nIf we understand your question correctly, then the answer is as follows. In fact, in our experiments, almost all the algorithms we consider aim to solve discrete-time RRL or MARL problems. 2xDDQN represents a naive decentralized learning approach often used in MARL in discrete time. RARL represents the decentralized approach that was used by Pinto et al. (2017) to solve RRL in discrete time. MADDPG is a basic centralized algorithm for solving general MARLs in discrete time. MADQN is its development for the case of a discrete action space. NashDQN is an algorithm aimed at solving Markov games in discrete time. Only CounterDQN and our IDQN and DIDQN are algorithms for which it is theoretically essential that an environment is a discretization of some differential game (in continuous time).\n\nThank you again for your feedback. We hope that our response has helped to clear up some doubts. If this is the case in your opinion, then we respectfully ask that you consider increasing your score. If you have any more comments, questions or remarks, we would be glad to discuss them."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122981434,
                "cdate": 1700122981434,
                "tmdate": 1700122981434,
                "mdate": 1700122981434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5tomuj013G",
                "forum": "Tz6HnhBzLl",
                "replyto": "6g2IrULFIp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3129/Reviewer_Uwp8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3129/Reviewer_Uwp8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for response"
                    },
                    "comment": {
                        "value": "Thanks for the clarification, I have raised my score. Please include all the discussions in the final version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517339731,
                "cdate": 1700517339731,
                "tmdate": 1700517339731,
                "mdate": 1700517339731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6rtwoNuOWP",
            "forum": "Tz6HnhBzLl",
            "replyto": "Tz6HnhBzLl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3129/Reviewer_BWN5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3129/Reviewer_BWN5"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores robust adversarial reinforcement learning (RARL) through the lens of positional differential game theory, ensuring the worst-case deterministic payoff. Leveraging the positional differential game theory, the authors formulated multi-agent reinforcement learning (MARL) to solve the RARL problem. These techniques were then benchmarked against multiple MARL baseline methods. Finally, the authors analyze the learned policies, highlighting the superior performance of the introduced algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper provides a novel perspective of RARL in the context of positional differential games."
                },
                "weaknesses": {
                    "value": "* There is substantial room for improvement in terms of writing. There is a lack of overall organization of sections and smooth transitions between paragraphs. Some examples are:\n    * In section 2, the authors could provide more insights into what is the difference between the standard formulation and differential game and why it is important to have a deterministic payoff. How the fact of the PDG is important to this paper and when the Isaacs's condition is met or not met.\n    * In the experiment section, the metric of 'stability' appears without proper definition and explanation. For example, is 'stability' equivalent to 'deterministic payoff'? If so, why not just stick to the latter? \n    * In the experiment section again, some analysis seems to be out of place and is unclear how it is related to the topic. For example, \"it is more efficient for agents...\" What is efficiency exactly, and how is it compared across CounterDQN and MADQN?\n* The reasoning and motivation for using positional differential games instead of Markov games as the framework are unclear. Markov games can be deterministic.\n* The discretization of action space makes it unclear under what conditions the main theorem still holds.\n* The effects of time discretization are unclear. How does it affect the estimation error or the convergence?"
                },
                "questions": {
                    "value": "* In Equ. 3, should it be $t_{m+1}$ instead of $\\tau_{m+1}$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793829185,
            "cdate": 1698793829185,
            "tmdate": 1699636259635,
            "mdate": 1699636259635,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LVkxRwvfvo",
                "forum": "Tz6HnhBzLl",
                "replyto": "6rtwoNuOWP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you very much for your attention to our paper and your remarks. Indeed, they address very important issues. We thought about most of them and tried to illuminate them in the paper. However, it may not be articulated enough. To fix this, below we go into detail on each of your remarks, comment on it and describe the corrections in the paper to make it better.\n\n***In section 2, the authors could provide more insights into what is the difference between the standard formulation and differential game and why it is important to have a deterministic payoff.***\n\nYou are correct that this is a crucial issue, and we paid attention to it in the third paragraph of the Introduction section dedicated to Markov games. Following your recommendation, we have also added the following comments regarding this at the beginning of section 2: \n\n*Recent studies consider RRL problems within the framework of zero-sum Markov games in pure or mixed policies. In the case of pure policies, it is known (e.g., paper-rock-scissors) that Markov games may not have a value (Nash equilibrium), which conceptually prevents the development of centralized learning algorithms based on shared q-functions. In the case of mixed policies, such formalization may also be inappropriate if, according to the problem statement (for example, in the case of developing expensive or safe control systems), it is required to seek robust policies guaranteeing a deterministic payoff value. In this section, we describe the framework of the positional differential games, which allows us, on the one hand, to consider the pure agents' policies and deterministic values of payoffs and, on the other hand, to obtain the fact of the existence of a value in a reasonably general case.*\n\n***How the fact of the PDG is important to this paper and when the Isaacs's condition is met or not met.***\n\nThank you for this question. We discussed the importance of the fact that the Markov game from Section 3 is a time-discretization of some differential game before Theorem 1. This is what helps us to prove it. We also tried to highlight the importance of the fulfilment and non-fulfilment of Isaacs's condition in the Limitations section. Following your recommendations, we also added the following short comments at the end of section 2:\n\n*We also note that in order to obtain further results, it is essential not only the existence of a value but also the fulfilment of Isaacs's condition as such.*\n\n***In the experiment section, the metric of 'stability' appears without proper definition and explanation. For example, is 'stability' equivalent to 'deterministic payoff'?***\n\nThank you for this comment. We mean stability with respect to running. That is, we say the algorithm is stable if it shows a similar result in all 5 runnings. In Figure 3, this means a narrow pale bar. The algorithm is unstable if, in some of the runnings, the algorithm performs worse than in the others. This corresponds to a wide pale bar. For example, MADDPG on Swimmer is unstable because, on at least one of the runnings, it showed a wide bar. We have added additional clarification to the article, chenging \n\n*Thus, looking at such a visualization, we can make conclusions about the stability*\n\nto\n\n*Thus, looking at such a visualization, we can make conclusions about the stability (with respect to running)*\n\nand\n\n*which reflects, on the one hand, the potential ability of MADDPG to find policies close to optimal, but, on the other hand, its instability.*\n\nto\n\n*which reflects, on the one hand, the potential ability of MADDPG to find policies close to optimal, but, on the other hand, its instability with respect to running.*\n\n***In the experiment section again, some analysis seems to be out of place and is unclear how it is related to the topic.***\n\nLet us clarify how the experimental analysis relates to the topic of the paper. The main goal of the paper is to train policies that are robust with respect to the opponent's actions. In our experiments, we measure precisely this robustness. To do this, we consider a comprehensive evaluation scheme (the details of which can be found in the Experiments section) and offer a corresponding visualization."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700120880801,
                "cdate": 1700120880801,
                "tmdate": 1700120880801,
                "mdate": 1700120880801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5vrcKYDwQU",
            "forum": "Tz6HnhBzLl",
            "replyto": "Tz6HnhBzLl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3129/Reviewer_h5zf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3129/Reviewer_h5zf"
            ],
            "content": {
                "summary": {
                    "value": "Robust Reinforcement Learning (RRL) treats uncertainty as actions of an adversarial agent, and in this paper, the authors propose a novel approach by applying positional differential game theory to develop a centralized Q-learning method. The authors demonstrate that this method can approximate solutions to both minimax and maximin Bellman equations, and they introduce two algorithms, Isaacs Deep Q-Networks (IDQN) and Decomposed Isaacs Deep Q-Networks (DIDQN). The algorithms are tested in various environments and outperform other baseline RRL and Multi-Agent RL algorithms in the experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-organized and involving RRL in potential differential games is a neat idea. The ideas for the experiments all seem very natural to me. The paper is also nicely organized."
                },
                "weaknesses": {
                    "value": "I think the authors address the limitations of their own work as is."
                },
                "questions": {
                    "value": "Q1) In Section 5, when the environments is described, what is the difference between $R$ and $\\mathbb{R}$?\n\nQ2) What are the specifications of the system used to run the experiments?\n\nMinor comments:\n\nI think you should name your main theorem. At the moment, you referred to it as \"Theorem\" and I think it would help if you name it. Either Thereom 1, Main Theorem, or some other name. Similarly, I would enumerate the remark on page 5 and maybe call it a corollary. \n\nI was unfamiliar with some of your notation, but if it is common in the field then you should keep it, of course. In particular, $\\overline{a,b}$  to refer to the integers $a,a+1,\\ldots, b$ was new to me. Similarly, I had to look up $\\lim_{\\delta\\downarrow0}$. I merely wanted to point out that I wasn't familiar with the notation, though I guess it is not so hard to figure out from context."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698906310018,
            "cdate": 1698906310018,
            "tmdate": 1699636259514,
            "mdate": 1699636259514,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wtkVVHjana",
                "forum": "Tz6HnhBzLl",
                "replyto": "5vrcKYDwQU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nThank you very much for your positive feedback on our paper. We are glad you liked the paper in terms of organization and ideas. Also, thank you for your questions and comments. We are happy to answer them below.\n\n*Q1) In Section 5, when the environments is described, what is the difference between $R$ and $\\mathbb R$.*\n\nThank you very much for your note. It's a typo. There should be $\\mathbb{R}^n$ everywhere. We corrected this in the updated version of the paper.\n\n*Q2) What are the specifications of the system used to run the experiments?*\n\nIn our experiments, we use both well-known examples of differential games and examples based on the MuJoCo simulator. A brief description of them is given in the Environments paragraph of the Experiments section. A detailed mathematical description is given in Appendix E. If you have additional questions regarding it, we will be glad to answer them.\n\n*I think you should name your main theorem. At the moment, you referred to it as \"Theorem\" and I think it would help if you name it. Either Thereom 1, Main Theorem, or some other name. Similarly, I would enumerate the remark on page 5 and maybe call it a corollary.*\n\nThank you for this comment. Indeed, it is better to call the theorem as \"Theorem 1\", and the remark as \"Corollary 1\".\n\n*I was unfamiliar with some of your notation, but if it is common in the field then you should keep it, of course. In particular, $\\overline{a,b}$ to refer to the integers $a,a+1,\\ldots,b$ was new to me. Similarly, I had to look up $\\lim\\limits_{\\delta \\downarrow 0}$. I merely wanted to point out that I wasn't familiar with the notation, though I guess it is not so hard to figure out from context.*\n\nThank you for this note. Indeed, these notations are quite common in our specific field, but we are happy to clarify them so that they do not raise doubts among a wider circle of readers. Namely, we change the notation for consecutive integers $i \\in \\overline{a,b}$ to $i=a,a+1,\\ldots,b$ and the notation the right limit at zero $\\delta \\downarrow 0$ to $\\delta \\to 0+$. The paper is only get better from this. \n\nThank you again for your feedback. If you have any more comments, questions or remarks, we would be happy to discuss them."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119405216,
                "cdate": 1700119405216,
                "tmdate": 1700119405216,
                "mdate": 1700119405216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E5yxvFtdYp",
                "forum": "Tz6HnhBzLl",
                "replyto": "wtkVVHjana",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3129/Reviewer_h5zf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3129/Reviewer_h5zf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154857604,
                "cdate": 1700154857604,
                "tmdate": 1700154857604,
                "mdate": 1700154857604,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CWm3cgQPAH",
            "forum": "Tz6HnhBzLl",
            "replyto": "Tz6HnhBzLl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3129/Reviewer_o2s5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3129/Reviewer_o2s5"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses RRL, Robust reinforcement learning, a recent method to incorporate physics and other constraints into the RL paradigm, such as disturbances and perturbations. Finding algorithms for RRL (modeled as an extra adversary in an multi-agent RL setting) is difficult due to non-stationarity. \n\nThe paper introduces a shared-Q-function to compute policies, and compares their new algorithm (IDQN) to other algorithms, on a range of suitable games.\n\nThe contribution/text of the paper is mostly theoretical, proving theorems on why such an algorithm may work."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Robust RL is a new and under-studied problem, mostly due to the non-stationary state space. It is nice to see the problem being studied. Also nice is the part on the Isaac condition, and the derivation of the shared Q function algorithm. The experimental results comapring the performance to other algorithms is also nice"
                },
                "weaknesses": {
                    "value": "A shared-Q function algorithm is compared against non-shared Q MARL algorihtms, or to standard single agent RL algorithms such as DDQN and PPO. This is comparing apples and oranges. Of course  shared Q function algorithms outperforms the other options. As such the experimental results are not very meaningful.\nSharing the Q function is not really addressing the MARL problem of non-stationarity, it is a kind of cheating."
                },
                "questions": {
                    "value": "I find the paper sympathetic in that it addresses Robust RL. Using a shared Q function transforms the MARL problem into a single agent problem. The pages of proofs do not contribute much to deeper understanding of the problem."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699386748130,
            "cdate": 1699386748130,
            "tmdate": 1699636259458,
            "mdate": 1699636259458,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "krAjrL2HAz",
                "forum": "Tz6HnhBzLl",
                "replyto": "CWm3cgQPAH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nThank you very much for your attention to our paper, as well as for your comments. It is very nice to hear positive feedback regarding Isaacs's condition, the derivation of the shared q-function, and the experimental part. Below, we present our comments on your remarks in the Weaknesses and Questions sections.\n\n***A shared-Q function algorithm is compared against non-shared Q MARL algorihtms, or to standard single agent RL algorithms such as DDQN and PPO. This is comparing apples and oranges. Of course shared Q function algorithms outperforms the other options. As such the experimental results are not very meaningful.***\n\nThank you for this remark. We understand your doubts regarding the choice of the baseline algorithms, and therefore, we would like to present the following arguments in defence of our particular choice of them.\n\nThe decentralized approach is widely studied in general MARL problems and is sometimes quite successful. Therefore, it was essential for us to show that this approach does not work well for the zero-sum MARL under consideration. The results of the DDQN algorithm as an example of the decentralized approach show that this is indeed true. The comparison with the RARL algorithm is important because it represents the first approach proposed in the literature (Pinto et al. (2017)) for solving Robust RL problems. Comparison with the non-shared MARL algorithms (MADDPG and MADQN) is a kind of ablation. We verify experimentally that the shared q-function actually increases performance. In the general case, this fact does not seem obvious since even in our experiments, non-shared MADQN outperforms NashDQN with a shared q-function. Thus, the baseline algorithms chosen for comparison help us draw several important conclusions from the experiments, a summary of which is presented in the last paragraph of the Experiments section.\n\n***Sharing the Q function is not really addressing the MARL problem of non-stationarity, it is a kind of cheating.***\n\nIf we understand you correctly, you doubt that the shared q-function can be used to solve general MARL problems. If this is so, we are generally ready to agree since the general formulation of MARL may concern MARL with more than two agents pursuing not necessarily opposite goals. However, this paper focuses on two-agent competitive MARL problems that can be described as zero-sum differential games. This is a limitation of the paper, which nevertheless allows us to obtain theoretical results and effective algorithms (IDQN and DIDQN) superior to the baseline algorithms aimed at solving more general MARL (MADDPG and MADQN). Some considerations about when this limitation can be overcome are indicated in the Limitations section. If you meant something else, please feel free to clarify.\n\n***I find the paper sympathetic in that it addresses Robust RL. Using a shared Q function transforms the MARL problem into a single agent problem. The pages of proofs do not contribute much to deeper understanding of the problem.***\n\nIt's great to hear that you find the paper sympathetic. Let us explain why it may not be entirely correct to say that we are reducing the problem to a single agent. One agent pursues maximizing its rewards, whereas in our case, we simultaneously train two agents who pursue opposing goals. This approach (centralized approach) is reasonably typical for solving MARL problems and, in our paper, we develop it for a specific class of MARL. Do not hesitate to ask if you have additional questions regarding a deeper understanding of the problem.\n\nThank you again for your feedback. We hope that our response has helped to clear up some doubts. If this is the case in your opinion, then we respectfully ask that you consider increasing your score. If you have any more comments, questions or remarks, we would be happy to discuss them."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109668472,
                "cdate": 1700109668472,
                "tmdate": 1700109668472,
                "mdate": 1700109668472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ba8zAbLlxA",
                "forum": "Tz6HnhBzLl",
                "replyto": "krAjrL2HAz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3129/Reviewer_o2s5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3129/Reviewer_o2s5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks very much for the detailed answers. I understand your point, but remain unconvinced about the shared Q approach. I believe my original score is accurate (perhaps somewhat generous)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635207533,
                "cdate": 1700635207533,
                "tmdate": 1700635207533,
                "mdate": 1700635207533,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]