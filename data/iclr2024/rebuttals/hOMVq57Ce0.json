[
    {
        "title": "Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning"
    },
    {
        "review": {
            "id": "lYpehdPKG3",
            "forum": "hOMVq57Ce0",
            "replyto": "hOMVq57Ce0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel architecture for actor-critic algorithms with the goal of increasing the interpretability of the learned policies. The proposed architecture learns a model that maps each input to a linear function, which is used to predict the agent's action. The underlying assumption is that, if we can understand the set of linear functions used to produce actions, then the model is \"partially\" interpretable. \n\nThe new architecture, HyperCombinator (HC), is evaluated with SAC, where the actor model is replaced with an HC. The SAC-HC algorithm is evaluated on the DeepMind Control Suite benchmark. The empirical results show that the performance of SAC-HC in terms of sample efficiency is similar to that of SAC. \n\nThe paper also presents plots showing how different linear functions are used across different episodes of problems such as Cheetah."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Policy interpretability is an important topic, since policies we can understand and verify are important in real-world scenarios. \n\nIt is also interesting to see empirical evidence that it is possible to perform well in commonly used benchmarks with models with less capacity. \n\nAnother strength of the paper is its clarity and ease of understanding. While the topics discussed aren't necessarily simple, the authors have done a good job presenting their results."
                },
                "weaknesses": {
                    "value": "I have two main concerns with this paper. The first is about a whole body of work on programmatic policies that the paper overlooks. The second is about the weak evaluation related to the interpretability of the proposed model. I'll detail each concern below.\n\n**Missing Related Work**\n\nBelow are some related works that I think the paper missed. The paper should cite some of these to better place its contributions within the existing literature. Others should serve as baselines in the experiments, as I explain later.\n\nThere's a line of research called Programmatically Interpretable RL (PiRL), see Verma et al. The goal in this line of research is to create programs that encode policies for RL problems. One of the key motivations behind PiRL is interpretability. Both Verma et al. and Bastani et al. use imitation learning to train interpretable models. Later, Qiu and Zhu found a way to learn similar policies with a fully differentiable approach, so there's no need for imitation learning. The architecture of Qiu and Zhu is essentially an oblique decision tree, which could also be trained with ReLU neural networks, as discussed in the work of Lee and Jaakkola and of Orfanos and Lelis. \n\nThe properties of ReLU networks share a lot in common with what's mentioned in the paper for the HC architecture:\n\n1. \"We make the functions $a$ and $\\theta$ explicit through a new parametrization of the NN.\"\n\nThis is also possible with ReLU networks. In these networks, the function $a$ is the set of weights in the hidden layers, while $\\theta$ is the function the model learns when the activation pattern is fixed (see Lee and Jaakkola for more). So, what makes the HC different from ReLU networks?\n\n2. \"We explicitly control the number of unique sub-policies of $\\pi$ through the dimension $dG$ of the Gumbel-Softmax layer.\"\n\nThis is also the case with the number of neurons in ReLU networks. Another way to reduce the number of unique sub-policies in ReLU networks is to use a single hidden layer with strong L1 regularization (refer to Orfanos and Lelis).\n\n3. \"Policies modeled with the HyperCombinator differ from MLPs in that they usually aren't continuous at the border between linear regions.\"\n\nI'm not sure why this is crucial, but ReLU networks also switch the linear function that gives the prediction when you move from one region to another.\n\n4. \"Our approach is locally interpretable, explaining the sub-policy applied to any given example.\"\n\nThis is true for ReLU networks mapped to Oblique Decision Trees too. The function that gives the prediction is at the leaf node. The path in the tree might not be easy to understand, but the linear function at the leaves is just as clear as those in the HC architecture.\n\nGiven all these similarities, small ReLU networks should be used as baselines in the experiments. Specifically, the architecture by Qiu and Zhu showed strong results on the same benchmark problems used in this paper.\n\nOther papers I mention below might be less crucial but are related. So, it would be good to see where this paper stands within the broader literature. For example, Inala et al. discuss how to learn interpretable finite state machines for RL problems. Koul et al. learn FSM policies for RL problems from recurrent networks. Aleixo and Lelis look at programmatic policies in multi-agent RL. Trivedi et al. and Liu et al. learn a latent space of a domain-specific language which can be used to search for programmatic policies. All these papers explore potentially interpretable policies for RL problems, making them relevant to this submission.\n\n**Lack of Evaluation on Interpretability**\n\nThe paper mostly gives anecdotal evidence when it comes to the interpretability of the policies. I appreciate the plots showing how the sub-policies work, but they're just examples. For some reason, the literature on interpretable policies doesn't focus much on evaluating interpretability. The papers I've listed below are weak in this area too. But none of them emphasize interpretability as much as this one does. The big unanswered question is: are these policies really interpretable, and if so, to whom?\n\n**References**\n\n1. Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Verifiable reinforcement learning via policy extraction. In Proceedings of the International Conference on Neural Information Processing Systems, pages 2499\u20132509. Curran Associates Inc., 2018.\n\n2. Jeevana Priya Inala, Osbert Bastani, Zenna Tavares, and Armando Solar-Lezama. Synthesizing programmatic policies that inductively generalize. In International Conference on Learning Representations, 2020.\n\n3. Anurag Koul, Alan Fern, and Sam Greydanus. Learning finite state representations of recurrent policy networks. In International Conference on Learning Representations, 2019.\n\n4. Guang-He Lee and Tommi S. Jaakkola. Oblique decision trees from derivatives of relu networks. In International Conference on Learning Representations, 2020.\n\n5. David S. Aleixo and Levi H. S. Lelis. Show me the way! Bilevel search for synthesizing programmatic strategies. In Proceedings of the AAAI Conference on Artificial Intelligence. 2023.\n\n6. S. Orfanos and Levi H. S. Lelis. Synthesizing programmatic policies with actor-critic algorithms and relu networks, 2023.\n\n7. Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Programmatically interpretable reinforcement learning. 2018.\n\n8. Abhinav Verma, Hoang M. Le, Yisong Yue, and Swarat Chaudhuri. Imitation-projected programmatic reinforcement learning. In Proceedings of the International Conference on Neural Information Processing Systems. Curran Associates Inc., 2019.\n\n9. Wenjie Qiu and He Zhu. Programmatic reinforcement learning without oracles. In International Conference on Learning Representations, 2022.\n\n10. Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph J Lim. Learning to synthesize programs as interpretable and generalizable policies. Advances in neural information processing systems, 34:25146\u201325163, 2021.\n\n11. Guan-Ting Liu, En-Pei Hu, Pu-Jen Cheng, Hung-Yi Lee, and Shao-Hua Sun. Hierarchical programmatic reinforcement learning via learning to compose programs. arXiv preprint arXiv:2301.12950, 2023."
                },
                "questions": {
                    "value": "1. How does HC architecture compare with other works from the literature, especially those on the PiRL line of work? \n\n2. How can one properly evaluate the interpretability of these models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641808823,
            "cdate": 1698641808823,
            "tmdate": 1700627147305,
            "mdate": 1700627147305,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "USv3IVFgIY",
                "forum": "hOMVq57Ce0",
                "replyto": "lYpehdPKG3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed review of our submission and their precise feedback. \nWe address their concerns in detail below.\n\n\n\n### Relationship of HC with ReLU networks\nWe agree that HC is indeed extremely linked with MLP-ReLU networks (abbreviated as ReLU networks in the following), as the latter is actually the starting point from our architecture (first paragraph of Sec 3.1). However, we made some strong changes to the ReLU network architecture in order to achieve desirable interpretability properties which we identified at the end of Sec. 2 and beginning of Sec. 3.\nOverall, the differences between ReLU networks and HC are best exemplified by Fig. 1 (right):\nReLU networks induce (1) a complex partition of the input space and (2) a different linear sub-function in each linear region (one color per linear sub-function).\nHC also induces (1) a complex partition (useful to solve hard tasks from scratch), but (2) only exhibits a small number of linear sub-functions (illustrated by using only 4 colors, hence 4 linear sub-functions). This is in stark contrast with ReLU networks of all but the smallest architecture.\n\nIn addition, we have actually compared ReLU networks (including with small architectures) with HC in our submission:\nIn Appendix B, Fig. 6, we show evidence that HC reaches a better performance-interpretability trade-off than ReLU networks for a wide range of architectures. This leads HC to a high performance while using a very small number of linear sub-policies.\nIn Appendix C, Fig. 7, we show how a ReLU network uses essentially a new sub-policy at each timestep, on the contrary to HC.\n\nWe now answer your points detailing the differences between HC and ReLU networks, following your points:\n\n### 1. Explicit formulation of $a$ and $\\theta$\n\nWe agree that it is possible to recover $a$ and $\\theta$ for ReLU networks, as we indicated in the beginning of Sec 3.1. However, these functions do not appear explicitly in the computation of the output $f(x)$, and need to be recovered (in the case of the local linear coefficients, by taking the input Jacobian of $f$).\nOn the contrary, we make $a$ and $\\theta$ explicit within the computation made by the neural net, by composing a Gumbel network $a$ and the local linear coefficients $\\theta$. The explicit modeling of $a$ within the forward pass of the actor lets us set the *exact number* of sub-policies as a hyperparameter, unlike ReLU networks (we expand on this last statement in the answer to your second point).\n\nFollowing your comment, we modified the manuscript (Sec 3.1) to clarify that $a$ and $\\theta$ now appear explicitly in the computation (in purple)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070435561,
                "cdate": 1700070435561,
                "tmdate": 1700070435561,
                "mdate": 1700070435561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FxXWgkPyAF",
                "forum": "hOMVq57Ce0",
                "replyto": "lYpehdPKG3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply and all the edits to the paper. \n\nThe related work section has improved, which is nice to see. The comparison with ReLU networks in the Appendix B (Figure 6) is also in the right direction.  \n\nThe issue I still have with the paper is the lack of baselines. For example, Qiu and Zhu's work is very related and it could be used even with no skills in the leaves. Qiu and Zhu used these skills in their experiments, but they are not needed to run their method. I am sorry for suggesting yet another baseline after the first round of reviews, but it just occurred to me that network distillation could also be a good baseline. You start with a large network with many unique regions and distill it into a much smaller network with fewer regions. A comparison with these methods would be informative."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222327926,
                "cdate": 1700222327926,
                "tmdate": 1700236204465,
                "mdate": 1700236204465,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wcwh2WYgSU",
                "forum": "hOMVq57Ce0",
                "replyto": "5Iz6zoixkQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the effort to provide extra empirical results. \n\nI am not sure the comparison between $\\pi$-PRL is fair because it uses TRPO, while HC uses SAC. Qiu and Zhu also used SAC for some of their experiments (see Table 3 of Appendix C of their paper). I understand you tried to compensate for this by providing TRPO more training samples. Do you think this is still a fair experiment to run? \n\nRegarding the imitation learning results, the common trick is to use DAgger as the learning procedure. I believe it is what you called \"augmentation\". Please see [1] for more information. \n\n[1] Ross, S., Gordon, G. J., & Bagnell, D. (2011). A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS) 2011, Fort Lauderdale, FL, USA.\n\nDid you implement DAgger or something else? \n\nAnother trick that goes into the imitation learning process (used in Verma et al.'s work) is to use a loss function that mixes both the imitation part and the reward part. You want to use the imitation signal to learn faster but without losing track of the reward signal."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521989339,
                "cdate": 1700521989339,
                "tmdate": 1700521989339,
                "mdate": 1700521989339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g5oI6ppltD",
                "forum": "hOMVq57Ce0",
                "replyto": "sUhrkO8RPD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_j8Wd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for all the effort. I agree that the paper has improved substantially since the version I first read. I have updated the rating of paper accordingly."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627228250,
                "cdate": 1700627228250,
                "tmdate": 1700627228250,
                "mdate": 1700627228250,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NGDdMMwXAB",
            "forum": "hOMVq57Ce0",
            "replyto": "hOMVq57Ce0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5901/Reviewer_yR55"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5901/Reviewer_yR55"
            ],
            "content": {
                "summary": {
                    "value": "### Problem Statement\n\nThe paper addresses the challenge of crafting interpretable policies in Deep Reinforcement Learning (DRL) to foster the development of trustworthy autonomous agents. The conventional linear policies, while interpretable, lack the expressivity to tackle complex tasks. In light of this, the authors advocate for piecewise-linear policies that aim to marry the interpretability of linear policies with enhanced performance akin to complex neural models.\n\n### Methodology\n\nTo this effect, they introduce the \"HyperCombinator\" (HC), a neural architecture that embodies a piecewise-linear policy with a controlled number of linear sub-policies. Every interaction with the environment engages a specific linear sub-policy, enhancing the interpretability while maintaining competitive performance. The architecture is a preset number of sub-policies, each in the form of a set of linear coefficients, gated by a MLP with one-hot output with Gumbel-softmax reparameterization allowing end-to-end training.\n\n### Main Contributions\n\nThe key contributions encapsulated in the paper are as follows:\n\n- They delineate the attributes that a desirable interpretable piecewise-linear policy should exhibit.\n- They design the HyperCombinator, a novel architecture that encapsulates a piecewise-linear policy with a defined number of linear sub-policies, and which is amenable to a broad spectrum of RL algorithms.\n- They analyze the interpretability of HyperCombinator.\n- They conduct evaluations of the model on control and navigation tasks, showcasing that the model retains a robust performance despite its curtailed expressivity.\n- They leverage the interpretability of HC to develop two visualizations that elucidate the policy's reactions to inputs and unveil the temporal abstractions in task execution through tracking the sequence of sub-policies employed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Originality and significance\n\nThe problem of interpretable Deep Reinforcement Learning is of great practical value. Despite the simplicity of the proposed approach, it attains competitive performance while providing much more interpretability compared to conventional deep neural networks. The formulation of desired properties of an interpretable policy is also insightful.\n\n### Writing\n\nThe paper is very well written. The organization is logical, presentation accurate and efficient. First laying out the desired properties and then evaluating the method against the properties makes the paper very easy to follow."
                },
                "weaknesses": {
                    "value": "- The method has limitations that prevent it from being applied to complex problems of larger scale: The policy loses interpretable as soon as the number of linear sub-policies grows large."
                },
                "questions": {
                    "value": "- In the Cheetah control setup, is the action space continuous? If so, what does the \"action 0\" in Figure 3 mean? Is it one dimension of the action? Then it is not proper to refer to it as \"for one action of $\\tilde{\\pi}$\".\n- Why are SAC and RIS chosen as the base alogrithm for RL in the experiments?\n- Are subpolicies trainable? Besides the diversifying regularizations for the MLP, would similar regularizations encouraging a diversified sub-policy help with the learning and model capacity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Reviewer_yR55"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732190695,
            "cdate": 1698732190695,
            "tmdate": 1699636626599,
            "mdate": 1699636626599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "56cqtCK8mV",
                "forum": "hOMVq57Ce0",
                "replyto": "NGDdMMwXAB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are glad they enjoyed reading the paper.\nHC indeed does not escape the performance-interpretability trade-off, and increasing the number of sub-policies tends to improve performance but reduce interpretability (Fig. 4). We think one of the interesting aspects of our work is that the stakeholder can decide this trade-off for themselves, by fixing how many sub-policies they are willing to use.\n\n### Questions\n- In Fig. 3, you are absolutely right, we are referring to the computation of the logits for the dimension 0 of the action space. We have updated the legend accordingly (in red), thanks.\n- We chose SAC because it was a strong continuous control, actor-critic baseline, with several good open-source implementations easy to bootstrap from. SAC is however not adapted to the maze experiments, mainly due to the specificities of the environments such as sparse rewards. We therefore looked for a base algorithm that would (1) be adapted to maze experiments, (2) be open-source and (3) have no competing architectural assumptions on the policy. RIS fit all these categories, which is why we chose it.\n- The sub-policies are indeed trainable. We learned each sub-policy (jointly with how to choose them) in each of our experiments. We actually tried to diversify the initialization of each sub-policy earlier in the project, but we did not find any benefits in terms of performance at that time. We therefore find the reviewer\u2019s proposition of diversifying the sub-policies themselves (in addition to how to choose them) could be very interesting to revisit in the future, as it might help each sub-policy to specialize even more."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069976965,
                "cdate": 1700069976965,
                "tmdate": 1700069976965,
                "mdate": 1700069976965,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ju6F3hpTE6",
                "forum": "hOMVq57Ce0",
                "replyto": "56cqtCK8mV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_yR55"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_yR55"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response to the questions. I would maintain my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471591822,
                "cdate": 1700471591822,
                "tmdate": 1700471591822,
                "mdate": 1700471591822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mYAb1h98lZ",
            "forum": "hOMVq57Ce0",
            "replyto": "hOMVq57Ce0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5901/Reviewer_8HLj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5901/Reviewer_8HLj"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to make Deep RL more interpretable by training a policy which is a piecewise linear parameterization of policies. It is supposed to solve most of the set of constraints an interpretable policy should satisfy (Section 3). The method works by using a Gumbel Softmax to select the different linear polices and then interpreting predictions based on the linear models. Some experiments show the  ability of their model to converge is more or less equal to normal methods, and arguments are made that is interpretable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strength of this paper is that is is proposing a reasonable solution to a very difficult problem of interpretable deep RL. I like the general idea and it makes sense, and results are reasonable enough. It should also pave a way for counterfactual reasoning as the authors suggest, and several other possible future directions."
                },
                "weaknesses": {
                    "value": "There are two main weaknesses of the paper in my view.\n\nFirstly, there appears to be clear limitations of the method performance wise, it appears to really struggle to latch onto the similar abilities of the black-box, which can be problematic as reducing performance has catastrophic effects on user trust (and then appropriate reliance etc.) Or at least, HC64 etc. is needed to get reasonable performance as opposed to HC8.\n\nSecond, the authors don't actually show the method being particularly useful for anything. It isn't always necessary to do this, but I feel here it would have helped a lot. Usually, explainability methods are used to debug, teach, regulate, calibrate reliance, or offer recourse etc., but this method isn't shown to be able to do any of those things.\n\n### Other\n* There is a misspelling in the title of parameterization.\n* I wouldn't limit your Section 3 to just \"counterfactual reasoning\", there are many other types of contrastive explanation which your method could also generate.\n* I also don't fully get (P2) here.\n* p6: Please don't say \"remarkable\", let the readers decide themselves.\n* Would be good to briefly elaborate in Fig2 why Finger spin does differently."
                },
                "questions": {
                    "value": "* Sorry if I missed it, but does this work for pixel-based input data?\n* Do you see this being useful for debug, teach, regulate, calibrate reliance, or offer recourse etc.,? If so, why didn't you demonstrate this.\n\nI think overall, given the positives and negatives, I lean a bit towards acceptance, but will await the rebuttal etc. to mutate my score later. Thanks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Reviewer_8HLj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764821629,
            "cdate": 1698764821629,
            "tmdate": 1699636626443,
            "mdate": 1699636626443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7W8Vdkiuo0",
                "forum": "hOMVq57Ce0",
                "replyto": "mYAb1h98lZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their review of our work and their suggestions. We understand that the reviewer has concerns about (1) the performance of our method and (2) the interpretability that is gained with HC. We detail our answer to these concerns below.\n\n### Performance of HC\nWe agree with the reviewer that performance is an important factor in user trust. One of HC\u2019s main advantages is that it lets the stakeholder tune the performance-interpretability trade-off through the number of sub-policies. In certain safety-critical environments or when the stakeholder is not a domain expert, the stakeholder might be willing to sacrifice some of the performance in order to gain insights about how the policy interacts with its environment. \n\nOn the other hand, a domain expert might be comfortable with more complexity, unlocking higher performance. This is illustrated in Fig. 4: HC64 never catastrophically fails, despite the high difficulty of the maze environments and its relative simplicity compared to the MLP baselines. HC8 struggles to solve the harder tasks but leads to simpler visualizations (e.g., Fig. 5). We think this is especially encouraging given the high difficulty of the mazes. \nWe update our manuscript in Sec 3.2 to express this point more clearly, in blue.\n\nFinally, some applications demand first and foremost the highest level of performance. We think our approach is outside of this scope, since a performance cost has to be paid for the gains in interpretability. \n\n### Applications of the method\nWe thank the reviewer for their remark. In this work, our goal was to study the properties that could improve the interpretability of piecewise-linear policies, and to propose a parametrization that let us achieve as many of these properties as possible. Therefore, we studied and evaluated the HyperCombinator as a general-purpose architecture.\n\nThe two levels of interpretability of HC relate to its transparency. The linear coefficients inform of the importance of a feature in the computation made by a subpolicy, and the sequence of chosen behaviors gives insights about the structure of the inner decision process of the agent.\nIn turn, a domain expert can use this transparency to better understand the policy that they roll out (Appendix D.7 gives a low-dimensional example of such an analysis).\n\nThe sub-policy sequence visualization could also be used to detect issues at run-time, especially if no video of the agent is available. In Fig. 15 (Appendix D.6), we studied the robustness of HC to perturbations, which we could model as an adverse event. The perturbations that the agent is subjected to are translated into an unusual sub-policy sequence between timesteps 100 and 200. Hence, we could imagine a system that detects a change in the subpolicy sequence, which would in turn raise an alarm that an adverse event has been detected.\n\nOverall, HC opens the gate to new analyses thanks to its transparency, which we think is a significant improvement from the status quo.\n\n### Other remarks\n- We reformulated the \u201cremarkable\u201d mention in p.6 following your suggestion.\n- Contrastive explanations: we thank the reviewer for their suggestion. We agree that the architecture of HC opens up new possibilities, such as analyzing why a given sub-policy was chosen rather than another.\n- P2: we clarified the meaning of the property in p.3 following your suggestion.\n- Title: we believe that both \u201cparameterization\u201d and \u201cparametrization\u201d are valid ways to spell the word.\n- Finger spin consists in the very quick repetition of the same precise movement in order to maximize the reward. Policies with more degrees of freedom have an advantage here as they can reach a better precision thanks to their higher expressivity. We find this not too surprising, as complex policies end up performing better than HC given enough samples (Fig. 12, Appendix D.4). We added a remark about this phenomenon in Appendix D.4.\n\n### Questions\nIt is indeed entirely possible to use interpretable features while conditioning the Gumbel network on pixel data. For instance, in DM control, we could define the observations as multimodal: one pixel representation of the environment, and the associated (interpretable) proprioceptive features. The Gumbel network could be fed the pixel rendering of the environment, which would predict a sub-policy to apply. This sub-policy would be linear with respect to the interpretable features. We did not propose results with pixel data for now as we felt this was beyond the scope of the paper, which introduces the architecture with respect to interpretable features only. However, we do agree it would be an interesting follow-up, and intend on pursuing a pixel-based version of HC in future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069916829,
                "cdate": 1700069916829,
                "tmdate": 1700069916829,
                "mdate": 1700069916829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "urdtGUSurE",
                "forum": "hOMVq57Ce0",
                "replyto": "mYAb1h98lZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you again for the time you already dedicated to reviewing our work.\n\nAs the end of the discussion period approaches, we are reaching out again to check if our previous reply answered your concerns. If not, please let us know, as we would be happy to have the opportunity to address your remaining concerns.\n\nThe authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506008627,
                "cdate": 1700506008627,
                "tmdate": 1700506008627,
                "mdate": 1700506008627,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xJSSAghWRa",
                "forum": "hOMVq57Ce0",
                "replyto": "mYAb1h98lZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_8HLj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Reviewer_8HLj"
                ],
                "content": {
                    "comment": {
                        "value": "Hi, thanks a lot for your reply, I have read everything and thought it over for a while.\n\nI think I can agree with most of what you say, the only part I feel we part ways is about applications of the technique. I would like to offer potentially useful ideas for you. \n\nYou say *\"In turn, a domain expert can use this transparency to better understand the policy that they roll out (Appendix D.7 gives a low-dimensional example of such an analysis).\"*\n\nThis is a good start, but why exactly do you want to understand the policy? That's the key question. If it is for debugging purposes, then you need a user study showing it does this. If it is to calibrate appropriate reliance, then again you need users in the loop to show it actually does this, otherwise it's all conjecture.\n\n*The sub-policy sequence visualization could also be used to detect issues at run-time, especially if no video of the agent is available*\n\nAgain, I think you need to show how seeing these issues is helpful. I know it sounds obvious that it should be useful, but too often XAI techniques are published with the author's claiming the method is \"interpretable\", but all the while failing to demonstrate that the method is understandable and useful to intended practitioners of the system.\n\nI like the paper, which is why I accepted it, but I cannot raise my score any higher with this fundamental issue present. Thanks again and good luck!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700523194158,
                "cdate": 1700523194158,
                "tmdate": 1700523242822,
                "mdate": 1700523242822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "54eG9FQBt8",
            "forum": "hOMVq57Ce0",
            "replyto": "hOMVq57Ce0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5901/Reviewer_KsbR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5901/Reviewer_KsbR"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a framework for interpretable reinforcement learning that attempts to balance interpretability, performance, and computational complexity. They propose learning a set of piecewise linear policies, which have the benefit of interpretability of linear models. They ensure that the number of piecewise linear policies is not too large which would inhibit practical interpretability. They present the empirical performance of their framework, HyperCombinator, in control and navigation tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The communication in the paper is clear. The authors clearly describe the desiderata for the ideal interpretable RL methods, and are clear about how their proposed approach seeks to address these points.\n- The authors' commitment to maintaining linear policies at some level is good, as these will always be very interpretable on their own.\n- The authors provide nice visualizations of their approach and their empirical results."
                },
                "weaknesses": {
                    "value": "- The contribution level is low in this paper. Pasting together a set of linear policies does not seem that differentiated from prior work.\n- It's not clear practically how interpretable the resulting model is. I suppose the user is supposed to inspect the linear model coefficients to understand what the policy is doing. However, there is not much discussion of this. For example, how should I interpret the coefficient heatmap in Figure 3?\n- As it stands, it seems the approach is less of a compromise between interpretability and performance and more of a deterioration of performance while only gaining a little bit of interpretability."
                },
                "questions": {
                    "value": "How complex of environments can the HC framework handle without fully losing performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5901/Reviewer_KsbR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699269436084,
            "cdate": 1699269436084,
            "tmdate": 1699636626291,
            "mdate": 1699636626291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UHOz8cFLDF",
                "forum": "hOMVq57Ce0",
                "replyto": "54eG9FQBt8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for reviewing our submission and for their feedback. We understand that the reviewer has concerns about (1) the novelty of our work and (2) the resulting interpretability and the trade-off with performance. We address these concerns below.\n\n### Novelty of our approach\nThere are several components that distinguish our paper from previous works. Notably, several aspects of the Gumbel network, the module which decides which sub-policy to apply to a given input, are key to let HC learn in the different experiments:\n- In particular, the MLP in the Gumbel network leads to an intricate partition of the input space (unlike regular tree methods), which enables a good performance of HC in the DeepMind Control benchmark as well as in select mazes.\n- Moreover, we specifically use the straight-through estimator (STE) to guarantee the selection of a single sub-policy at each timestep (instead of a mixture of several sub-policies), during training and during evaluation. This is an important property for interpretability (enabling analyses during training) that was missing from previous works.\n- Finally, while previous works (such as Akrour 2018) focused mostly on performance, we study in detail how such a parameterization influenced the piecewise-linear functions we could express, within the scope of interpretability. The different visualizations give insights about how the agent is interacting with its environment, which are not possible with regular neural networks.\n\nTo summarize, our paper performs an in-depth study of the limits of MLP with ReLU activations as an interpretable parametrization of piecewise-linear policies. We propose an architecture that parametrizes a piecewise-linear policy along the constraints. We think this fills a gap that was present in the literature.\n\n### Characterization of the interpretability gained with HC\nThe HC architecture provides two levels of interpretability:\n1. Low-level interpretability through the value of the linear coefficients of each sub-policy,\n2. High-level interpretability by examining the sequence of choices of sub-policies over a trajectory.\n\nThe heatmap in Fig. 3 is an example of low-level interpretability. It shows the coefficients of each sub-policy (one column per sub-policy) for one dimension of the action space. For instance, subpolicy #0 assigns a positive coefficient to feature f0 as the corresponding cell is red. Therefore, we know that inputs with greater positive values for f0 will lead to greater predicted actions, all else being equal. Moreover, the coefficient quantifies the exact importance of feature f0 in the computation of the action.\n\nIn addition, we perform a more extensive analysis of the meaning of the coefficients in Appendix D.7, where we analyze the learnt policies on the cartpole environment. The lower dimensionality of the environment helps getting a better grasp of the computations and therefore of each sub-policy.\nFor instance, the coefficient values let us understand the usage of each sub-policy and how the agent specialized some to get the CartPole up, and some to maintain it straight. \nWe think this is a considerable improvement over the regular non-interpretable neural network, and makes the decision making process significantly more transparent.\n\nThe high-level interpretability brought by our agent is complementary to this low-level interpretability, as we get a better understanding of how the agent processes the information to decide which subpolicy to follow. For instance, Fig. 3 (left) illustrates how the agent learnt to chain efficiently the linear sub-policies in a repeated manner to solve Half-Cheetah with a strong performance, and the hierarchical structure in Fig. 5 reflects the structure of the maze.\n\n### Question about interpretability-performance trade-off\nWhile the performance of HC64 decreased with the increasing difficulty of the mazes, it never collapsed (Fig. 4). We think this is especially encouraging as these tasks are extremely challenging, and require dedicated algorithms to solve them (Chane-Sane 2021). This leads us to think that HC can handle complex environments without fully losing performance simply by increasing the number of sub-policies available. For instance, in Fig. 4, we had to trade-off some interpretability by going from 8 sub-policies to 64. \n\nTherefore, the answer to this question ultimately lies with the stakeholder that needs to understand the sub-policy. If that person is a domain expert, it is possible for them to increase the number of sub-policies and therefore increase the performance.\nA person less familiar with the environment might want to reduce the number of sub-policies that HC can use, sacrificing some of the performance to understand how the agent interacts with the environment.\nWe updated the manuscript (Sec 3.2) to reflect this point (in olive)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069805235,
                "cdate": 1700069805235,
                "tmdate": 1700069805235,
                "mdate": 1700069805235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1G84fcXU2h",
                "forum": "hOMVq57Ce0",
                "replyto": "54eG9FQBt8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5901/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you again for the time you already dedicated to reviewing our work.\n\nAs the end of the discussion period approaches, we are reaching out again to check if our previous reply answered your concerns. If not, please let us know, as we would be happy to have the opportunity to address your remaining concerns.\n\nThe authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5901/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505978197,
                "cdate": 1700505978197,
                "tmdate": 1700505978197,
                "mdate": 1700505978197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]