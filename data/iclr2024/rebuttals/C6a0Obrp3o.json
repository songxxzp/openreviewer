[
    {
        "title": "SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing"
    },
    {
        "review": {
            "id": "oGlycO9QP0",
            "forum": "C6a0Obrp3o",
            "replyto": "C6a0Obrp3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission711/Reviewer_n7gt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission711/Reviewer_n7gt"
            ],
            "content": {
                "summary": {
                    "value": "This research addresses challenges in image-to-text (I2T) inversion and proposes \"SingleInsert,\" a two-stage method that effectively separates foreground and background in learned embeddings. It enhances visual fidelity and flexibility in single-image concept generation, novel view synthesis, and multiple concept composition without joint training. The paper introduces the Editing Success Rate (ESR) metric for quantitative assessment of editing flexibility."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is very easy to follow."
                },
                "weaknesses": {
                    "value": "1. This paper seems to miss many related works or baselines.\n\n- Taming encoder for zero fine-tuning image customization with text-to-image diffusion models.\n\n- InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning\n\n- Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach\n\n2. Does hyper-dreambooth finetune t2i part?\n\n3. The idea is not novel. Using mask to get more accurate object embedding is not new, and BG loss is largely used for this purpose also.\n\n4. The two-stage training/finetuing plus the additional losses as restriction are more complicated than previous works, but lacking comparison with above related works.\n\n5. According to the implementation details, the model requires retraining for each new concept, and in the finetuning stage, it relies on lora for better fidelity, which makes the soundness of the method even weaker.\n\n6. The proposed ESR is worth more descriptions in the main part, since it is  a contribution."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643797220,
            "cdate": 1698643797220,
            "tmdate": 1699635998362,
            "mdate": 1699635998362,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "krgQf2sltH",
                "forum": "C6a0Obrp3o",
                "replyto": "oGlycO9QP0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer n7gt (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the careful reading and helpful feedback. We respond below to your questions and concerns:\n\n**Q1:**\nRelevant papers.\n\n**R1:**\nThanks for your reminder. We add the referred papers to related works in our revised paper. We find that all three mentioned papers[1,2,3] are released on Arxiv. In addition, we find that this paper [3] is also under the review process of ICLR 2024 like ours.\nAs for comparisons, since [1,2] didn't release their code, we mainly add comparisons with [3]. Please refer to Sec. 4.2 in our revised paper, we add qualitative and quantitative comparisons with ProFusion[3] and also highlight the quantitative comparison with [3] in the table below. Both the qualitative and quantitative comparisons demonstrate the superiority of our method.\n\n| Method | CLIP-I-f ($\\uparrow$)  | CLIP-I-b ($\\downarrow$)  | DINO-f ($\\uparrow$)  | DINO-b ($\\downarrow$)  | CLIP-T ($\\uparrow$) | DIV ($\\uparrow$)| ESR ($\\uparrow$) |\n| - | - | - | - | - | - | - | - |\n| ProFusion | 0.790 | 0.593 | 0.587 | 0.188 | 0.291 | 0.752 | 0.545 |\n| Ours (Stage I) | 0.822 | **0.555** | 0.582 | **0.152** | **0.317** | **0.776** | **0.845** |\n| Ours (Stage II) | **0.857** | 0.601 | **0.609** | 0.176 | 0.311 | 0.753 | 0.825 |\n\n[1] Jia X, Zhao Y, Chan K C K, et al. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models[J]. arxiv preprint arxiv:2304.02642, 2023.\n\n[2] Shi J, Xiong W, Lin Z, et al. Instantbooth: Personalized text-to-image generation without test-time finetuning[J]. arxiv preprint arxiv:2304.03411, 2023.\n\n[3] Zhou Y, Zhang R, Sun T, et al. Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach[J]. arxiv preprint arxiv:2305.13579, 2023.\n\n**Q2:**\nQuestion about HyperDreambooth.\n\n**R2:**\nHyperDreambooth directly predicts the LoRA weight of each instance and finetunes the LoRA weights in the finetuning stage.\n\n**Q3:**\nFG \\& BG loss novelty.\n\n**R3:**\nFirst, As described in Sec. 3.2 (Foreground Loss) of our main paper, we do NOT claim that we are the first to introduce segmentation masks and apply the foreground loss in T2I inversion tasks. We have shown in ablations and Fig. 5 in our main paper that the results are not satisfying when only applying the existing foreground loss. It is worth noting that our other two proposed regularizations (BG loss and semantic loss) are proved to be essential for ensuring editing flexibility. Besides, at the end of Sec. 2, we discussed the relevance and differences of SingleInsert with BreakAScene[4], since it is the first paper to introduce the foreground loss into I2T inversion tasks. Second, as far as we know, there are no existing methods similar to our proposed background loss. We argue that ``BG loss is largely used for this purpose'' is not convincing.\n\n[4] Avrahami O, Aberman K, Fried O, et al. Break-A-Scene: Extracting Multiple Concepts from a Single Image[J]. arxiv preprint arxiv:2305.16311, 2023.\n\n**Q4:**\nAbout missing comparisons.\n\n**R4:**\nPlease refer to the response of **R1**. Since [1,2] do not release their code, it is hard to have a comprehensive comparison with them. Moreover, Instantbooth[2] requires multiple source images as input, which differs from our single-image inversion setting.\nIn addition, we follow your suggestion to add comparisons with ProFusion[3]. Please refer to Fig. 3 for qualitative comparisons and refer to Table 1 for quantitative comparisons in our revised paper.\nBoth the qualitative and quantitative comparisons demonstrate the superiority of our SingleInsert method."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700042960053,
                "cdate": 1700042960053,
                "tmdate": 1700042960053,
                "mdate": 1700042960053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NdJ837wcor",
                "forum": "C6a0Obrp3o",
                "replyto": "oGlycO9QP0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer n7gt (part 2)"
                    },
                    "comment": {
                        "value": "**Q5:**\nMethod complexity.\n\n**R5:**\nTo address your concern, we compare the finetuning\nefficiency of methods that have a finetuning stage in the table below (we multiply the batchsize and finetuning iterations as the whole iterations). All the inference is done on a single V100 GPU. From the table we can see that our method surpasses the other two methods in finetuning efficiency and achieves the best performance in quantitative evaluations, surpassing E4T by a large margin. We do not compare ProFusion here since it needs domain-specific pretraining.\n\n| Method | Finetuning iters ($\\downarrow$) | Finetuning time ($\\downarrow$) | CLIP-I-f ($\\uparrow$) | CLIP-I-b ($\\downarrow$) | DINO-f ($\\uparrow$) | DINO-b ($\\downarrow$) | CLIP-T ($\\uparrow$) | DIV ($\\uparrow$) | ESR ($\\uparrow$)\n| - | - | - | - | - | - | - | - | - | - |\n| TI+DB | 800 | 8min | 0.769 | 0.613 | 0.519 | 0.180 | 0.296 | 0.751 | 0.480 |\n| E4T | **240** | **1min** | 0.759 | 0.634 | 0.479 | 0.200 | 0.272 | 0.684 | 0.325 |\n| BreakAScene | 400 | 3.5min | 0.835 | 0.698 | 0.603 | 0.233 | 0.273 | 0.722 | 0.375 |\n| Ours | 400 | 2min | **0.857** | **0.601** | **0.609** | **0.176** | **0.311** | **0.753** | **0.825** |\n\nIn addition, we also compare the total training time with other single-source image finetuning methods in the table below. As shown in the table, our method surpasses other finetuning-based methods in training speed.\nWe hope you will take these into consideration.\n\n| Method | Total time |\n| - | - |\n| TI | 6min |\n| DB | 8min |\n| Custom | 10min |\n| BreakAScene | 5.5min |\n| Outs | **3.5min** |\n\n**Q6:**\nAbout the two-stage finetuning pipeline and method soundness.\n\n**R6:**\nAs for the two-stage finetuning pipeline, our method focuses on the task of single-image inversion, following the practice of some other finetuning-based methods (e.g., E4T, BreakAScene, ProFusion) in this research area, we also utilize a finetuning stage for better results. Please refer to the response of **R5**, it is worth noting that our SingleInsert demonstrates high finetuning efficiency among other finetuning-based methods.\n\nAs for the method soundness, apart from the experiments in the original paper, we have tried our best to supplement the experiments in our revised paper, including qualitative and quantitative comparisons with E4T and ProFusion (Sec. 4.2 in main paper), comparisons with Subject-Diffusion using examples provided in their original paper (Fig. 20 in Appendix), comparisons of novel view synthesis and multiple-concepts inversion (Table 5 and Fig. 19 in Appendix) and more comprehensive comparisons of quantitative evaluations following Dreambooth's setting (Table 7 in Appendix). We hope these will address your concern about the soundness of our method.\n\n**Q7:**\nQuestion about the proposed metric ESR.\n\n**R7:**\nThanks for your valuable suggestions!\nWe add a brief introduction of the proposed ESR in our revised main paper (Sec. 4.1) and add more details about ESR in the Appendix (Sec. A.2)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700045831485,
                "cdate": 1700045831485,
                "tmdate": 1700045831485,
                "mdate": 1700045831485,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GsvIm17O4A",
            "forum": "C6a0Obrp3o",
            "replyto": "C6a0Obrp3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission711/Reviewer_i9TQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission711/Reviewer_i9TQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for customized text-to-image generation, which considers disentanglement in learning the concept contained in user-provided image."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method tries to disentangle the influence of foreground and background in the given image, which is reasonable and straightforward.\n\nGood results are presented in the paper, compared to related baselines.\n\nAblation studies are conducted, which help readers better understand the proposed method."
                },
                "weaknesses": {
                    "value": "The proposed method seems to require more fine-tuning time compared to some related works (E4T only requires 5~15 steps, the proposed method requires 100 steps which is mentioned in section 4.1).\n\nThe idea of disentangling the foreground and background information has also been exploit in related works [1, 2]. Some of the related work have code publicly available online [2], but are not compared in this paper's experiments.\n\nIn quantitative evaluation, the authors didn't follow the setting in Dreambooth [3] to test the proposed methods on objects comprehensively. Specifically, the prompts used in the paper, on both human face and objects domain, may not be comprehensive enough. Dreambench proposed in [3] contains recontextualization, accessorization, and property modification prompts. On the contrary, example prompts shown in the paper are less comprehensive. Thus more comparisons are suggested.\n\nSome related works also work on similar task with related ideas, which are suggested to be discussed in the paper.\n\n[1]. DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation. Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, Wenwu Zhu.\n\n\n[2]. Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning. Jian Ma, Junhao Liang, Chen Chen, Haonan Lu.\n\n[3]. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman.\n\n[4]. BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. Dongxu Li, Junnan Li, Steven C.H. Hoi.\n\n[5]. PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models.  Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, Min Zheng.\n\n[6]. Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach. Yufan Zhou, Ruiyi Zhang, Tong Sun, Jinhui Xu.\n\n[7]. InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning. Jing Shi, Wei Xiong, Zhe Lin, Hyun Joon Jung."
                },
                "questions": {
                    "value": "Can the authors provide more details about the data they collected from the web? Specifically, do those data consist of common object, human face, or both? What is the number of the collected samples?\n\nIn the fine-tuning stage, because a frozen T2I model is also used, how much extra memory do we need compared to the scenario without this model (both under LoRA setting).\n\nHave the authors considered pre-training the model on a large-scale dataset? Will it reduce the fine-tuning time on testing images?\n\nThe author mentioned number of iterations needed, what is the actual total time needed in terms of seconds/minutes for customizing a new testing image?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822562846,
            "cdate": 1698822562846,
            "tmdate": 1699635998296,
            "mdate": 1699635998296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HhTmfIXGR3",
                "forum": "C6a0Obrp3o",
                "replyto": "GsvIm17O4A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i9TQ (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the careful reading and helpful feedback. We respond below to your questions and concerns:\n\n**Q1:**\nFinetuning time.\n\n**R1:**\nThanks for your concern. E4T[1] also adopts a two-stage scheme. However, there are two main reasons why we need more finetuning iterations than E4T. First, in our first stage, we do not finetune the base T2I model or addition LoRA layers, which intends to find an existing concept in the base model that corresponds to the target concept. In comparison, in the pretraining stage of E4T, it finetunes the weight offsets of T2I model using large-scale category-specific datasets, which extends the diversity of the original T2I model on specific class domain and gives a better class-specific initialization than the base T2I model could offer. However, the pretraining pipeline of E4T also limits their method to certain classes with abundant source images such as human faces, and costs much time for a single category. Second, as described in E4T, the batch size needs to be larger than 16 during finetuning stage, while our batch size is set to 4. In fact, we could also finetune for much fewer steps when setting the batch size to 16 without sacrificing the performance, which means the finetuning efficiency gap is not that large between our method and E4T. We compare the finetuning effeciency of methods that have a finetuning stage in the table below (we multiply the batchsize and finetuning iterations as the whole iterations). All the inference is done on a single Nvidia V100 GPU. From the table we can see that, our method surpasses the other two methods in finetuning efficiency, and achieves the best performance in quantitative evaluations, surpassing E4T by a large margin. We hope you may take these into consideration.\n\n|Method | Finetuning iters ($\\downarrow$) | Finetuning time ($\\downarrow$) | CLIP-I-f ($\\uparrow$)| CLIP-I-b ($\\downarrow$)| DINO-f ($\\uparrow$)| DINO-b ($\\downarrow$)| CLIP-T ($\\uparrow$)| DIV ($\\uparrow$)| ESR ($\\uparrow$)|\n| - | - | - | - | - | - | - | - | - | - |\n| TI+DB | 800 | 8min | 0.769 | 0.613 | 0.519 | 0.180 | 0.296 | 0.751 | 0.480 |\n| E4T | **240** | **1min** | 0.759| 0.634| 0.479| 0.200| 0.272| 0.684| 0.325 |\n| BreakAScene | 400 | 3.5min | 0.835 | 0.698 | 0.603 | 0.233 | 0.273 | 0.722 | 0.375 |\n| Ours | 400 | 2min | **0.857** | **0.601** | **0.609** | **0.176** | **0.311** | **0.753** | **0.825** |\n\n[1] Gal R, Arar M, Atzmon Y, et al. Encoder-based domain tuning for fast personalization of text-to-image models[J]. ACM Transactions on Graphics (TOG), 2023, 42(4): 1-13.\n\n**Q2:**\nRelevant papers.\n\n**R2:**\nThanks for your reminder! We add DisenBooth[2] in the related works. However, since Disenbooth requires multi-images as input, and it is not open-source yet, we can only discuss it in the revised paper. As for Subject-Diffusion[3], since the model is trained on a large-scale dataset privately owned by the authors, we are unable to reproduce the performance without the dataset or provided pretrained models. It is mentioned in its code repo's issue that their dataset or pretrained model can not be released due to relevant company restrictions. To address your concern, we compare our method with some examples shown in its paper. Please refer to Fig.~20 in the Appendix of our revised paper. We find that the learned concept in Subject-Diffusion appears to have fixed positions and poses, while our method achieves better editing flexibility. We add more qualitative comparisons against these methods in the revised paper.\n\n[2] Chen H, Zhang Y, Wang X, et al. DisenBooth: Disentangled Parameter-Efficient Tuning for Subject-Driven Text-to-Image Generation[J]. arxiv preprint arxiv:2305.03374, 2023.\n\n[3] Ma J, Liang J, Chen C, et al. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning[J]. arxiv preprint arxiv:2307.11410, 2023.\n\n**Q3:**\nMore related works.\n\n**R3:**\nThanks for your valuable suggestion! We add the referred papers in our related works and compare the open source ones if accessible. Due to the popularity of the I2T inversion task, we found it hard to follow papers released on Arxiv very recently. But thank you so much! We definitely will add them to discussion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700041472679,
                "cdate": 1700041472679,
                "tmdate": 1700041472679,
                "mdate": 1700041472679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HX7eG8cFsY",
                "forum": "C6a0Obrp3o",
                "replyto": "GsvIm17O4A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i9TQ (part 2)"
                    },
                    "comment": {
                        "value": "**Q4:**\nMore quantitative evaluations.\n\n**R4:**\nThanks for your reminder! We follow the setting of Dreambooth[4] and conduct extensive experiments on the dataset in Dreambooth to compare our method against others, the results are shown in the table below. To be noted, there is only a single source image containing the intended concept for a fair comparison. Although our visual similarity score (DINO and CLIP-I) is slightly lower than BreakAScene[5], our prompt fidelity score (CLIP-T) surpasses existing methods by a large margin, indicating the editing flexibility of SingleInsert, which is consistent with the conclusion in our main paper.\n\n| Method | DINO ($\\uparrow$) | CLIP-I ($\\uparrow$) | CLIP-T ($\\uparrow$) |\n| - | - | - | - |\n| TI | 0.543 | 0.667 | *0.297* |\n| DB | 0.623 | 0.759 | 0.270 |\n| Custom | 0.637 | 0.773 | 0.276 |\n| Elite | 0.656 | 0.769 | 0.261 |\n| BreakAScene | **0.673** | **0.811** | 0.277 |\n| Outs | *0.669* | *0.803* | **0.334** |\n\n[4] Ruiz N, Li Y, Jampani V, et al. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 22500-22510.\n\n[5] Avrahami O, Aberman K, Fried O, et al. Break-A-Scene: Extracting Multiple Concepts from a Single Image[J]. arxiv preprint arxiv:2305.16311, 2023.\n\n**Q5:**\nOnline data details.\n\n**R5:**\nThanks for your reminder! All the web images in our research are collected from [Pixel], shout out to their exquisite pictures. We collect 20 photos of models (10 male and 10 female) as sources for human faces and hairstyle and clothes. We also collect 10 car photos, 10 cat/dog photos as the source images of our novel view applications. The comparisons on common object categories are conducted using the mentioned data and Dreambooth[1] dataset.\n\n**Q6:**\nExtra memory consumption.\n\n**R6:**\nThanks for your concern! During the finetuning stage, we add an extra T2I model to prevent language drift problems. Since the added T2I model is fixed during this stage, it consumes about 3.5GB vram since we loaded the model in FP16 precision. For the same purpose, many existing methods [4,5,6] double the batchsize to compute the prior preserving loss proposed in Dreambooth[4], which requires more memory.\n\n[6] Multi-Concept Customization of Text-to-Image Diffusion. Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J. (2022).\n\n**Q7:**\nLarge-scale dataset training.\n\n**R7:**\nThanks for your question! We do have experimented on larger category-specific datasets such as human faces. We use 100, 1000, 10000 human face images to train our pretraining stage as described in the main paper as well. We empirically found that the performance of the first stage is close to the single-image setting. Since the max quality of the learned concept in the pretraining stage relies on the base T2I model, it could not reduce the finetuning time in test images, unless we also finetune the T2I model or LoRA layers in the pretraining stage. We choose single-image setting as default because our method does not restrict to certain classes as some relevant methods do. The lack of large-scale unusual class dataset is also one of our concerns.\n\n**Q8:**\nTotal time.\n\n**R8:**\nThanks for your reminder! We compare the training time with other finetuning methods in the table below. As shown in the table, our method surpasses other finetuning-based method in training speed. All the tests are conducted on a single Nvidia V100 GPU.\n\n| Method | Total time |\n| - | - |\n| TI | 6min |\n| DB | 8min |\n| Custom | 10min |\n| BreakAScene | 5.5min |\n| Outs | **3.5min** |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700041534425,
                "cdate": 1700041534425,
                "tmdate": 1700041534425,
                "mdate": 1700041534425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k40svOZeiF",
            "forum": "C6a0Obrp3o",
            "replyto": "C6a0Obrp3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission711/Reviewer_QGk3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission711/Reviewer_QGk3"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a two-stage Diffusion-based Image-to-Text Inversion algorithm that can mitigate overfitting when training with a single source image. It applies constraints to suppress the inversion of undesired background and the problem of language drift. Segmentation masks for foreground and background and predictions from the original diffusion model conditioned on the class of inversed concept are utilized to form the regularizations. It also designs an editing prompt list to quantitatively evaluate the edit flexibility of the inversed concept. With the proposed algorithm, in the non-trivial single-source-image scenario, this work achieves both high visual fidelity and editing flexibility, enabling novel view synthesis and multiple inversed concepts composition without joint training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The method allows more flexible ediitng for the inversed concepts from a single image, surpassing its baselines.\n(2) The method presents a novel way to regularize the Image-to-Text Inversion process with predicted distributions by the original model.\n(3) The paper presents an ediitng prompt list and a metric for quantitative evaluation of editing flexibility of inversed concepts.\n(4) The paper clearly illustrates the motivations and the designs of the new proposed loss functions.\n(5) The ablation studies clearly presents the value of each design of the proposed method."
                },
                "weaknesses": {
                    "value": "(1) In section 4.4, the authors claim that the proposed approach enables single-image novel view synthesis. However, the experiments on this point are quite weak. Firstly, the algorithm cannot accurately control the viewpoint angle but can only control the view with text prompts \"left side\", \"frontal\", and \"back side\". Secondly, no evidence is provided to demonstrate how this constitutes an advancement compared to previous work on previous approaches. Thirdly, the generated novel view images also have drastic change on the background and even foreground appearance, which does not meet the requirement of novel view synthesis. Thus, I doubt that the claim of this contribution is not grounded.\n(2) The application scenario of multiple concept composition is only demonstrated with a few examples but without comparison to previous work.\n(3) On P6, section 4.1, a brief, if not detailed, introduction about the proposed metric ESR and the editing prompt list is expected to be given. The readers are supposed to have the basic idea about what is done in this evaluation after reading this section, instead of having to read the supplemental file to grasp it."
                },
                "questions": {
                    "value": "(1) The proposed algorithm in this paper does not have a design specified for the single-source-image scenario and achieves single-source-image scenario by finetuning a large number of parameters, i.e. the whole T2I model and a ViT-B image encoder. So it would be natural to expect that the good performances generalize to the multiple-source-image scenario. Have you tried using the proposed algorithm for the multiple source-image inversion?\n(2) Please refer to the questions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699193344378,
            "cdate": 1699193344378,
            "tmdate": 1699635998197,
            "mdate": 1699635998197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FZqR2CMBtg",
                "forum": "C6a0Obrp3o",
                "replyto": "k40svOZeiF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QGk3"
                    },
                    "comment": {
                        "value": "Thank you for the careful reading and helpful feedback. We respond below to your questions and concerns:\n\n**Q1:**\nNovel view applications claim.\n\n**R1:**\n\nWe agree that the design of our method does not improve the accuracy of viewpoint control of the base T2I model. First, the ability of novel view synthesis of our method relies on the generation capacity of the T2I model. The main reason we claim the novel view synthesis application as one of our advantages is that: most of the previous single-image inversion methods suffer from foreground-background entanglement problems. The intended concept (e.g., human face) usually appears to show the same pose as in the source image (as in Fig. 17 in the revised paper). In contrast, our method enables flexible editing, with large variations, including poses, so that our proposed method can generate reasonable novel view of the intended concept with good visual fidelity. Second, to compare the advantages of our method in novel view synthesis against existing methods, we follow PrepNeg[1] to generate 100 samples each method according to viewpoint-specific prompts and compute the average success rate of viewpoint changing among the compared methods (All the concepts are captured from the frontal view. Due to the fact that human face does not have a backside, we choose to ignore it.). The results are attached below. From the table we can see that our method surpasses existing methods in viewpoint changing editing, though slightly worse than the base SD model, which indicates the superiority of our method. Third, as we focus on foreground concept, it is inevitable that the backgrounds can change with view point changed. We aim to improve this in the future and many thanks for you to point it out.\n\n| Method | Side face | Side Car | Side Dog | Back face | Back Car | Back Dog |\n| --- | --- | --- |  --- | --- | --- | --- |\n| SD  | **0.89** | **0.92** | **0.79** | / | **0.43** | **0.94** |\n| TI  | 0.72 | 0.69 | 0.69 | / | 0.38 | 0.84 |\n| DB | 0.13 | 0.15 | 0.08 | / | 0.08 | 0.18 |\n| Custom | 0.56 | 0.43 |  0.45 | / | 0.22 | 0.54 |\n| Elite  | 0.25 | 0.38 | 0.35 | / | 0.14 | 0.27 |\n| FastComposer  | 0.10 | 0.12 | 0.09 | / | 0.06 | 0.15|\n| BreakAScene | 0.37 | 0.45 | 0.39 | / | 0.15 | 0.43 |\n| Ours | *0.79* | *0.83* |  *0.72* | / | *0.40* | *0.80* |\n\n\n\n[1] Armandpour M, Zheng H, Sadeghian A, et al. Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond[J]. arXiv preprint arXiv:2304.04968, 2023.\n\n**Q2:**\nMultiple concepts composition applications.\n\n**R2:**\nThanks for your concern! We add more comparisons in the Appendix, please refer to Fig.~19 of our revised paper, which demonstrates that our SingleInsert achieves on par or even better results against existing methods in multiple concepts composition.\n\n**Q3:**\nAdding ESR description in main paper.\n\n**R3:**\nThanks for your valuable suggestions!\nWe add a brief introduction of the proposed metric ESR in our revised main paper (Sec. 4.1) and add more details about ESR in the Appendix (Sec. A.2). Regarding the editing prompt list, due to limited space in the main paper, we attach it in the Appendix (Sec. A.2). Please refer to our revised paper.\n\n**Q4:**\nLarge finetune parameters\n\n**R4:**\nIn fact, instead of finetuning the whole T2I model, we only finetune the LoRA layers, which contain about 0.75M parameters per concept. As for the image encoder, we have actually tried other image encoders such as a small CNN-based network (about 11.7M parameters), which produces results that are also appealing. We apply a ViT-B/32 image encoder simply for faster convergence. We follow the experiment setting in the main paper and report the quantitative evaluations in the table below. We have included these hyperparameters in the revised version.\n\n| Image encoder | Params ($\\downarrow$) | Total time ($\\downarrow$) | CLIP-I-f ($\\uparrow$)| CLIP-I-b ($\\downarrow$)| DINO-f ($\\uparrow$)| DINO-b ($\\downarrow$)| CLIP-T ($\\uparrow$)| DIV ($\\uparrow$)| ESR ($\\uparrow$)|\n| - | - | - | - | - | - | - | - | - | - |\n| ViT-B/32 | **88.4M** | **3.5min** | **0.857** | **0.555** | **0.582** | **0.152** | 0.317 | 0.776 | **0.845** |\n| CNN | 11.7M | 5min | 0.839 | 0.560 | 0.568 | 0.153 | **0.320** | **0.783** | 0.820 |\n\n**Q5:**\nmultiple source-image inversion.\n\n**R5:**\nApart from the multiple-concepts composition examples shown in our original paper and supplementary materials, we add more examples in the revised paper (Fig. 16, Fig. 19 in Appendix), including comparison against other methods. We hope to bring it to your attention that our method can achieve comparable or even better results than other methods without the need for joint training in the multiple source-image inversion task."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039381403,
                "cdate": 1700039381403,
                "tmdate": 1700115005037,
                "mdate": 1700115005037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]