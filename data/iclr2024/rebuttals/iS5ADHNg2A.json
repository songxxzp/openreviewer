[
    {
        "title": "Deceptive Fairness Attacks on Graphs via Meta Learning"
    },
    {
        "review": {
            "id": "I8tEz4iMaK",
            "forum": "iS5ADHNg2A",
            "replyto": "iS5ADHNg2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8867/Reviewer_5Ww3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8867/Reviewer_5Ww3"
            ],
            "content": {
                "summary": {
                    "value": "The paper \"Deceptive Fairness Attacks on Graphs via Meta Learning\" proposes a meta-learning approach for exacerbating demographic bias in the prediction of a given, unknown, victim model operating on the input graph; It is based on the optimization of a surogate model to define edges (or features to modify). It shows that their approach is able to manipulate edges and features  of the graph so that the performance of different victim models remains constant while the bias is increased."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Very interesting approach and results. \n\nLooks theoretically sound."
                },
                "weaknesses": {
                    "value": "The paper sometimes lacks of carity, with some defintions not given (e.g., Y never defined in sec 2) or only in the appendix. In fact I had to go many times in the appendix to understand the core concepts of the approach, which is not very comfortable for the reader. I feel the formalization of the problem/method should include some separated notation for the victim and the surogate model, cause it was a bit confusing. Maybe what we want is that we are good for any victim model from a given distribution of victims ? Setting the problem like this would made the problem easier to understand from my point of view. Results are very hard to follow as some notation are not specified at all (e.g., Ptb which is the perturbation budget but we have to infer it) and every important details as the evaluation metrics are only given in the appendix (I understand the choice for a lack of space but only give some words on it (e.g. simply say delta sp is the bias) and refer to the corresponding section in the appendix would be helpful. Also please better specify the experimental questions you are inspecting in each section of the experimental part, because we can be a little lost at the first read."
                },
                "questions": {
                    "value": "1. I wish the paper could have inspected some possible ways to prevent such attacks. The approach announce that the goal is to better understand. Ok, but what can we do, that would be positive, from the outcomes of the model ? Would it be possible to give some insights for good ways to defend from such attacks ? \n\n2. The considered bias is not fully defined in sec 4, for instance for the instantiation 1: you give the way you estimate both pdf but finally what is the score considered ? an mse, a kl, or what ?  Also, would it be possible to consider a targeted attack that would favor a specific population ? \n\n3. Would it be possible to specify more deeply $\\nabla_{\\cal G} \\Theta^{(T)}$ ? is it equal to $ \\epsilon \\nabla_{\\cal G} \\nabla_\\Theta l({\\cal G}, Y,\\Theta,\\theta)|_{\\Theta^{(T-1)}}$, with $\\epsilon$ the step size of update of the surogate ? \n\n4. I have difficulties to well understand the recursive definition of (3) and its connection with (2). Could you clarify please ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There can be ethical concerns with this approach which is designed to exacerbate biases on graph models. This is done to better envision defense actions, but it could be misused online. However, authors are aware of this and will only release the code under the CC-BY-NC-ND license upon publication, which prohibits the use of our method for any commercial purposes."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8867/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698421665566,
            "cdate": 1698421665566,
            "tmdate": 1699637116061,
            "mdate": 1699637116061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bPl6CIcSxB",
                "forum": "iS5ADHNg2A",
                "replyto": "I8tEz4iMaK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 5Ww3 (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comments and valuable feedback for further improving our work. Please see our point-to-point responses as follows.\n\n**Q1. Lack of clarity, including definition of $\\mathbf{Y}$, separating victim and surrogate model, reference to Ptb. and bias measures, and experimental questions.**\n\nThank you for your suggestion in further improving the clarity. \n\n- Definition of $\\mathbf{Y}$: In the last paragraph of Section 2, we have defined $\\mathbf{Y}$ as the output of graph learning model, which is highlighted in blue.\n\n- Separating victim and surrogate model: We would like to clarify that the attacker only have access to the surrogate model, i.e., solving Eq. (1) is solely based on the surrogate model. The victim model is only used when evaluate the performance of FATE in Section 6. Thus, we believe it it not necessary to differentiate the notations for victim model and the surrogate model in Sections 3 -- 5.\n\n- Reference to Ptb. and bias measure: We have revised the table caption to reflect the meaning of Ptb. and $\\Delta_{\\text{SP}}$ in the revised version, highlighted in blue. For Table 1, the new caption is \"Attacking statistical parity on GCN under different perturbation rates (Ptb.). FATE poisons the graph via both edge flipping (FATE-flip) and edge addition (FATE-add) while all other baselines poison the graph via edge addition. Higher is better ($\\uparrow$) for micro F1 score (Micro F1) and $\\Delta_{\\text{SP}}$ (bias). Bold font indicates the most deceptive fairness attack, i.e., increasing $\\Delta_{\\text{SP}}$ and highest micro F1. Underlined cell indicates the failure of fairness attack, i.e., decreasing $\\Delta_{\\text{SP}}$ after fairness attack.\" Similar changes have also been made to Tables 2 and 5 -- 11.\n\n- Add experimental questions: Given the space limitation, we have added the philosophical experimental questions at the beginning of Appendix D, which are (1) How effective is FATE in exacerbating bias under different perturbation rates? (2) How effective is FATE in maintaining node classification accuracy for deceptiveness under different perturbation rates? And (3) can we characterize the properties of edges perturbed by FATE?\n\n\n**Q2. Possible defense strategy.**\n\nThank you for your suggestion in discussing how to defend against fairness attacks. Following our analysis on the manipulated edges in Sections 6.1 and 6.2, we believe possible defense strategies could be as follows. To defend against deceptive fairness attacks for statistical parity, one possible strategy is to preprocess the input graph by either learning a bias-free graph (e.g., [1]) or sampling over the neighborhood (e.g., [2, 3, 4]) to control which node representations to aggregate during message passing. The reason for such possible design is that Figure 1 reveals the properties of injected edges that are likely to be incident to nodes in the minority class and/or protected group. Following similar principles, it is also possible to develop a selective or probabilistic message passing strategy to achieve the same goal during model optimization. To defend against deceptive fairness attacks for individual fairness, we can apply similar neighborhood sampling strategy or selective/probabilistic message passing strategy. Instead, for individual fairness, the neighborhood sampling or selective message passing would consider the class label rather than the sensitive attribute (i.e., sample edges that connect nodes in the minority class as shown in Figure 2). Please note that the aforementioned discussion has been added in our revised version (Appendix H.G).\n\n**Q3. Bias measure in Section 4.**\n\nThank you for your suggestion in further improving the clarity. We have added the definition of bias function (i.e., $b\\left(\\mathbf{Y}, \\Theta^*, \\mathbf{S}\\right) = |\\operatorname{P}\\left[{\\tilde y} = 1\\right] = \\operatorname{P}\\left[{\\tilde y} = 1|s = 1\\right]|$) in Section 4-C, which is highlighted in blue."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476332665,
                "cdate": 1700476332665,
                "tmdate": 1700476332665,
                "mdate": 1700476332665,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WuiG2B8l4H",
                "forum": "iS5ADHNg2A",
                "replyto": "sh9z9zXCee",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Reviewer_5Ww3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Reviewer_5Ww3"
                ],
                "content": {
                    "title": {
                        "value": "thanks"
                    },
                    "comment": {
                        "value": "Thanks for the insightful answers. However, I wish authors made more to improve clarity, as in many cases they only answered my questions here without impacting the paper. I think these answers would help the reader understand every component of the approach. I am not fully convinced by the answer about their claim of no need of separation of notations for victim and surrogate. I well understood that the victim is only used for evaluation, but I feel the global objective could include it in its formulation. It would be greatly better from my point of view. \n\nAnyway, I keep my score unchanged."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555000747,
                "cdate": 1700555000747,
                "tmdate": 1700555000747,
                "mdate": 1700555000747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O7O8VgHv5q",
            "forum": "iS5ADHNg2A",
            "replyto": "iS5ADHNg2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8867/Reviewer_Jcma"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8867/Reviewer_Jcma"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates deceptive fairness attacks on graph learning, introducing a meta-learning-based framework, FATE. Designed with broad applicability, FATE targets biases in graph neural networks. Using real-world datasets for semi-supervised node classification, the authors demonstrate FATE's ability to amplify biases without compromising downstream task utility. This emphasizes the covert nature of these attacks. The study hopes to spotlight adversarial robustness in fair graph learning, guiding future endeavours towards creating robust and equitable graph models. The paper is a commendable exploration of the crossroads of fairness, adversarial attacks, and graph learning."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic of this paper is attacking the fairness of GNNs by a poisoning attack setting, which is valuable for building trustworthy GNNs. The method in this paper is easy to follow. In the experimental evaluations, the authors attempt to provide deep analysis upon validating the effectiveness of the proposed method, which is great for the audience to obtain insights and understanding of the vulnerability of GNN fairness."
                },
                "weaknesses": {
                    "value": "1.Limited novelty. The limitation of existing works is weak (section 2-c). For limitation 1, this paper still only attacks one fairness concept when generating the adversarial graph. For limitation 2, the operation space depends on the practicality of the attack. For limitation 3, the specially designed module for maintaining performance is not presented in this paper. Authors are suggested to revise this part to highlight the contribution of this paper.\n\n2.Inaccurate/Unclear statements. For the budget limitation in the \u201ccapability of the attacker\u201d, please be aware that the ||.||_{1,1} (according to the common definition of matrix norm) focuses on the maximum number of perturbations among nodes and cannot reflect the number of total perturbations. The definition of d(G,\\tilde G) should also include the changes on X. In the 4-c, the \\hat{y_(i,1)} discussing the sensitive attributes is repeated with that defining the label in 4-B.\n\n3.The design for keeping the performance of GNNs is not presented in the equation (1). Authors are suggested to discuss why the proposed method can achieve the performance goal, as presented at the beginning of this paper.\n\n4.The operation in equation 6 should be explained with more details to show how to handle different connection statuses and gradient statuses. Moreover, the flip and add operations should also be explained to show how they happen.\n\n5.For definition 1, authors are suggested to explain why the conclusion from IID assumption can be used in the graph data, where nodes are non-IID. Moreover, according to the definition of CDF, the integral starts from -infinity. Authors are suggested to revise related symbols or statements in section 4.\n\n6.In section 5, authors are suggested to explain that L_s is the Laplacian matrix of S, avoiding the confusion of audience who are not familiar with fairness.\n\n7.Limited evaluation. It is worth noting that the surrogate model is SGC while the target model is GCN, and they almost have extremely similar architecture. Authors are suggested to evaluate the effectiveness of the proposed method on other target GNN architectures (e.g., GAT, GraphSAGE) to follow the attack setting of this paper (section 3.1, attackers have only access to the graph data).\n\n8.Evaluation metric. It is worth noting that the fairness of GNNs is at the cost of performance. Although it is great to see the fairness deduction with limited performance cost. However, FA-GNN works well in a lot of cases. Considering that both fairness and model performance is the design of this paper, authors are suggested to compare the (\\delta SP)/(\\delta Acc) to evaluate the bias increment on unit acc cost.\n\n9.Confusing analysis on the effect of the perturbation rate. I understand that the authors attempt to show insights on the adversarial graph. However, it is hard to digest the operation (delete or add) on the same/different label/attribute. The discussion in Figure 1-b is also confusing. I suppose that detailed explanations of what exactly happened on each bar will make a more clear discussion. Authors are suggested to revise the paragraphs related to figures 1 and 2.\n\n10.According to the effectiveness table results, authors are suggested to explain why the proposed method can improve the performance of GNNs.\n\n11.On the \u201cEffect of the perturbation rate\u201d in section 6.2. Intuitively, more perturbations will make the learned parameters farther from that learned from the original graph. Thus, the availability scope of the learned adversarial graph will be limited by the perturbations, i.e., the higher budget will result in more unexpected behaviours of GNN trained on the poisoned graph. Authors are suggested to revise and discuss it in this paper, which is the limitation of the gradient-based attack method."
                },
                "questions": {
                    "value": "Refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8867/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8867/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8867/Reviewer_Jcma"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8867/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638472735,
            "cdate": 1698638472735,
            "tmdate": 1700555081924,
            "mdate": 1700555081924,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F3Ibm3grwi",
                "forum": "iS5ADHNg2A",
                "replyto": "O7O8VgHv5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Jcma (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comments and valuable feedback for further improving our work. Please see our point-to-point responses as follows.\n\n**Q1. Limited novelty and discussion on three limitations for existing work.**\n\nThank you for your questions. \n- For limitation 1, we would like to clarify that we attack **two** fairness definitions, i.e., statistical parity (Sections 4 and 6.1) and individual fairness (Sections 5 and 6.2), to generate the adversarial graphs.\n- For limitation 2, we agree with the reviewer that the operation space would be affected by the practicability of the attack. However, we believe both edge injection, edge deletion, and edge flipping are almost equally practical. For example, in the given malicious banker example, edge injection refers to injecting adversarial transaction record(s) in the transaction network, edge deletion refers to removing transaction record(s), and edge flipping is the combination of two.\n- For limitation 3, we implicitly assume there is a divergence in optimizing the task-specific loss function $l(\\mathcal{G},\\mathbf{Y},\\Theta,\\theta)$ and optimizing the bias function $b\\left(\\mathbf{Y},\\Theta^*,\\mathbf{F}\\right)$. Thus, maximizing $b(\\mathbf{Y},\\Theta^*,\\mathbf{F})$ may not affect $l\\left(\\mathcal{G},\\mathbf{Y},\\Theta,\\theta\\right)$ too much. Since we are minimizing the task-specific loss function in the inner loop (i.e., lower-level optimization), it helps to maintain the performance in the downstream task for deceptive fairness attacks. We think such assumption is reasonable for the following reason. In fair machine learning, a common strategy is to solve a regularized optimization problem, where the objective function to be minimized is often defined as $l(\\mathcal{G},\\mathbf{Y}, \\Theta, \\theta)+\\alpha b(\\mathbf{Y},\\Theta^*,\\mathbf{F})$ with $\\alpha$ being the regularization hyperparameter. If there is no divergence between the optimization of $l(\\mathcal{G},\\mathbf{Y},\\Theta,\\theta)$ and $b(\\mathbf{Y},\\Theta^*,\\mathbf{F})$, it would be sufficient to optimize one of them to obtain fair and high-utility learning results, or it would be impossible to achieve a good trade-off between fairness and utility if they are completely conflicting with each other. All in all, we believe that optimizing the task-specific loss function $l(\\mathcal{G},\\mathbf{Y},\\Theta,\\theta)$ in the lower-level optimization problem could help maintain deceptiveness both intuitively and empirically as shown in Section 6.1, Section 6.2, Appendix E, and Appendix F.\n\n\n**Q2. Inaccurate/unclear statements about matrix norm, definition of distance between graphs, and repeated discussion about $\\hat{y}$.**\n\nThank you for your suggestions. \n- Regarding the matrix norm: we believe the review is referring to matrix-induced norm. For the induced matrix norms, $\\|\\mathbf{A}\\|_{1,1}$ would refer to the maximum absolute column sum of the matrix difference. However, as we mentioned at the end of Section 2.C, we use the entry-wise matrix norms. It treat the elements of the matrix as one big vector (see [here](https://en.wikipedia.org/wiki/Matrix_norm#%22Entry-wise%22_matrix_norms)). Following the definition of entry-wise matrix norms, our notation means the absolute sum of each entry as shown at the end of Section 2.C.\n\n- Regarding the definition of distance between graphs: we agree with the reviewer that the distance between two graphs might also consider the distance between feature matrices in some cases. However, we believe how to define the distance is based on the attacker's capability in attacking adjacency matrix and/or feature matrix. If the attacker would only attack the adjacency matrix, it is sufficient to define the distance between two graphs as the distance between two adjacency matrices, because the distance between two feature matrices will always be 0. If the attacker would want to attack both adjacency matrix and feature matrix, the distance between two graphs should consider both the distance between adjacency matrices and the distance between feature matrices. In fact, when we describe the distance between two graphs at the end of Section 2.C, we use e.g. (for example) to provide an example. And when we describe the distance in Section 3.1 (\"The capability of attacker\"), we use i.e. (in another word) to explain the distance constraints for edges/features.\n\n- Regarding the repeated discussion about $\\hat{y}$: we would like to clarify that this is not repeated definition. We acknowledge that the previous descriptions might confuse the reviewer. In the revised version, we use ${\\widehat y}\\_{i,j}$ to denote the prediction probability of node $i$ belonging to class $j$ and ${\\tilde y}$ to denote the predicted label. Then, $P[{\\tilde y} = 1]$ refers to the complementary CDF of $p({\\widehat y}_{i,1} > \\frac{1}{2})$ because we would classify node $i$ to class 1 if the corresponding prediction probability is larger than $\\frac{1}{2}$ in a binary classification problem."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475726950,
                "cdate": 1700475726950,
                "tmdate": 1700475872988,
                "mdate": 1700475872988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gnYU2wbZwI",
                "forum": "iS5ADHNg2A",
                "replyto": "I5fkIpaFdt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Reviewer_Jcma"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Reviewer_Jcma"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nFor limitation 1, I would recommend revising it as the generalisation of the proposed method, e.g., it can be adapted to attack different fairness. The current statement \u201cattack statistical parity and individual fairness\u201d is suggested to be revised as \u201cattack statistical parity or individual fairness\u201d to avoid confusion.\n\nFor the \\Delta Acc, a potential available manner of definition could be the |Acc_{before}-Acc_{after}| to evaluate the accuracy changes.\n\nThanks for your efforts in clarifying your paper. Your answers make sense to me and I have raised the score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555214832,
                "cdate": 1700555214832,
                "tmdate": 1700555214832,
                "mdate": 1700555214832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8drJjP6EK5",
                "forum": "iS5ADHNg2A",
                "replyto": "O7O8VgHv5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further response to reviewer Jcma (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for your suggestion for better clarity and for further clarification on defining the unit accuracy cost. We have followed your suggestion and have changed \"attack statistical parity and individual fairness\" to \"attack statistical parity or individual fairness\" in the updated version (in abstract, organization of Appendix at the beginning of Appendix page, and Appendix G). \n\nRegarding the unit accuracy cost, as the reviewer suggested, we define the **absolute** unit accuracy cost as $\\text{unit-acc-cost} = \\frac{|\\Delta_{\\text{SP}}^{(\\text{before})} - \\Delta_{\\text{SP}}^{(\\text{after})}|}{|\\text{Acc}^{(\\text{before})} - \\text{Acc}^{(\\text{after})}|}$, which helps measure the absolute change in $\\Delta_{\\text{SP}}$ with one unit absolute change in accuracy. And higher is better for absolute unit accuracy, because a higher absolute unit accuracy cost means that one unit change in accuracy could cause more bias increase. The unit accuracy costs of FA-GNN, FATE-flip, and FATE-add are as follows. Please note that, in each cell of table below, the absolute unit accuracy costs are reported when the perturbation are from 0.05 to 0.25 with a step size of 0.05 in an ascending order, i.e., the leftmost value indicates the absolute unit accuracy cost when the perturbation rate is 0.05, and the rightmost value indicates the absolute unit accuracy cost when the perturbation rate is 0.25. (fail) means that the absolute unit accuracy cost refers to the case of failed fairness attack (i.e., decreasing bias after attack).\n\n||FA-GNN|FATE-flip|FATE-add|\n|-|-|-|-|\n|Pokec-n|12.666 (fail) -> 2.933 -> 5.667 -> 6.647 -> 18.000|5.500 -> 3.857 -> 8.800 -> 7.000 -> 6.250|5.500 -> 3.857 -> 8.800 -> 7.000 -> 6.250|\n|Pokec-z|14.667 (fail) -> 9.857 -> 5.722 -> 8.174 -> 5.414|0.333 -> 3.000 -> 3.167 -> 22.000 -> 20.000|0.333 -> 3.000 -> 3.167 -> 22.000 -> 20.000|\n|Bail|1.429 -> 0.885 -> 0.129 -> 0.176 (fail) -> 0.848 (fail)|1.200 -> 1.286 -> 1.222 -> 1.444 -> 1.100| 1.000 -> 0.857 -> 1.375 -> 1.625 -> 1.100|\n\nFor Pokec-n, FATE-flip and FATE-add achieves higher absolute unit accuracy cost when the perturbation rate is smaller than 0.25 (please note that FA-GNN fails the fairness attack when the perturbation rate is 0.05). For Pokec-z, FATE-flip and FATE-add achieves a much higher absolute unit accuracy cost when the perturbation rates are 0.20 and 0.25. While for Bail, FATE-flip and FATE-add consistently offers higher absolute unit accuracy cost (except for a marginal decrease for FATE-add with the perturbation rate being 0.10)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648909473,
                "cdate": 1700648909473,
                "tmdate": 1700648981098,
                "mdate": 1700648981098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M6yp0Lfd4X",
                "forum": "iS5ADHNg2A",
                "replyto": "O7O8VgHv5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further response to reviewer Jcma (Part 2)"
                    },
                    "comment": {
                        "value": "In addition, we would like to point out that an absolute unit accuracy cost may be unable to capture whether the accuracy is increased or decreased after attack. Thus, we further provide additional results on unit accuracy bias increase, which is defined as $\\text{unit-acc-bias-increase} = \\frac{\\Delta_{\\text{SP}}^{(\\text{after})} - \\Delta_{\\text{SP}}^{(\\text{before})}}{\\text{Acc}^{(\\text{before})} - \\text{Acc}^{(\\text{after})}}$. The reason for the numerator being $\\Delta_{\\text{SP}}^{(\\text{after})} - \\Delta_{\\text{SP}}^{(\\text{before})}$ is that we want the bias to increase as much as possible after fairness attack, thus measuring the bias increase after attack. While the reason for the denominator being $\\text{Acc}^{(\\text{before})} - \\text{Acc}^{(\\text{after})}$ is that we want the accuracy to be decrease as few as possible, thus measuring the accuracy decrease after attack. With this definition, it is clear that (1) a negative unit accuracy bias increase is always better than positive value, because we could increase the bias without costing accuracy; (2) higher is better if unit accuracy bias increase is positive (we increase more bias with a unit decrease in accuracy), and (3) smaller is better if unit accuracy bias increase is negative (i.e., we increase more bias with a unit increase in accuracy). The results for unit accuracy bias increase of FA-GNN, FATE-flip, and FATE-add are as follows (with the same format as the table above). It should be noted that, when the bias decreases and the accuracy increases (i.e., failed fairness attacks), it is also possible to achieve a negative unit accuracy bias increase, which will be denoted by (fail).\n\n||FA-GNN|FATE-flip|FATE-add|\n|-|-|-|-|\n|Pokec-n|12.666 (fail) -> 2.933 -> 5.667 -> 6.647 -> 18.000|-5.500 -> -3.857 -> -8.800 -> -7.000 -> -6.250|-5.500 -> -3.857 -> -8.800 -> -7.000 -> -6.250|\n|Pokec-z|-14.667 (fail) -> 9.857 -> 5.722 -> 8.174 -> 5.414|-0.333 -> -3.000 -> 3.167 -> -22.000 -> -20.000|-0.333 -> -3.000 -> 3.167 -> -22.000 -> -20.000|\n|Bail|1.429 -> 0.885 -> 0.129 -> -0.176 (fail) -> -0.848 (fail)|1.200 -> 1.286 -> 1.222 -> 1.444 -> 1.100| 1.000 -> 0.857 -> 1.375 -> 1.625 -> 1.100|\n\nFrom the table above, for Pokec-n, FATE-flip and FATE-add always achieves negative unit accuracy bias increase, which is better than positive values achieved by FA-GNN. For Pokec-z, other than the case when perturbation rate is 0.15, FATE-flip and FATE-add always achieves negative unit accuracy bias increase, which is better than positive values achieved by FA-GNN. For Bail, FATE-flip and FATE-add always achieve a better unit accuracy bias increase than FATE. \n\nWe hope this could demonstrate the effectiveness of our proposed method, as we can achieve better absolute unit accuracy cost or unit accuracy bias increase in many cases. In the remaining days, we will calculate both metrics for all experimental settings (statistical parity vs. individual fairness, GCN vs. FairGNN/InFoRM-GNN) and update the manuscript with the additional results.\n\nAgain, we sincerely appreciate the reviewer for your kind suggestion on better clarity and new evaluation metric. Should you have any remaining concerns, we are more than happy to discuss with you."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648929490,
                "cdate": 1700648929490,
                "tmdate": 1700649015016,
                "mdate": 1700649015016,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WY0HrHWbvt",
            "forum": "iS5ADHNg2A",
            "replyto": "iS5ADHNg2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8867/Reviewer_RLh1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8867/Reviewer_RLh1"
            ],
            "content": {
                "summary": {
                    "value": "In the paper Deceptive Fairness Attacks on Graphs via Meta Learning, the authors delve into the realm of deceptive fairness attacks on graph learning models, aiming to amplify bias while simultaneously maintaining or improving utility in downstream tasks. They formalize this challenge as a bi-level optimization problem, where the upper level seeks to maximize the bias function according to a user-defined fairness definition, and the lower level aims to minimize a task-specific loss function.\n\nTo address this challenge, the authors introduce FATE, a meta-learning based framework designed to poison the input graph using the meta-gradient of the bias function with respect to the input graph. FATE is rigorously evaluated and compared with three baseline methods: Random, DICE-S, and FA-GNN, across three real-world datasets in the context of binary semi-supervised node classification with binary sensitive attributes.\n\nFATE distinguishes itself through two strategies: edge flipping (FATE-flip) and edge addition (FATE-add), as opposed to other baseline methods which only perform edge addition, and consistently outperform the baseline methods in fairness attacks. Unlike some baseline methods that sometimes fail to attack individual fairness, FATE maintains its effectiveness, amplifying bias while achieving the highest micro F1 scores in node classification. This dual success highlights FATE's deceptiveness, as it can increase bias while simultaneously enhancing or maintaining model performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Clear Structure and Argumentation: The paper is well-organized, providing a clear and compelling argument for the importance of studying deceptive fairness attacks in graph learning models. \n2. Robust Experimental Design: The experimental design is of high quality, with all mathematical symbols and notations meticulously explained.\n3. Transparent Results Presentation: Results are presented clearly and concisely, with visual aids and detailed explanations that make the findings accessible and easy to understand, showcasing the effectiveness of the FATE framework.\n4. Problem Novelty: The proposed work focuses on adversarial robustness of fair graph learning, which needs more exploration."
                },
                "weaknesses": {
                    "value": "It is noted the below are minor weaknesses:\n1. Grammatical Typos: There are minor grammatical errors present, such as in Section 6.1 on page 6, where the phrase \"It randomly deleting edges...\" should be corrected to \"It randomly deletes edges...\". \n2. Incomplete Introduction of Fairness Definitions: In Section 2B, when algorithmic fairness definitions are introduced, the paper provides English definitions but omits corresponding mathematical definitions. Including these mathematical definitions would enhance clarity, especially as the individual fairness definition is important when analyzing results.\n3. Inconsistent Organization: The paper utilizes a letter-based organization in some sections and a numerical organization in others. For instance, in Section 4, ideas are laid out numerically but then referred to using letters (A, B,...)."
                },
                "questions": {
                    "value": "1. Can more be discussed on the potential other bias functions besides statistical parity? \n2. Do the authors believe this attack scenario is reasonable in real-world settings? Although an example is provided of a banker and has been used in prior work, but it feels somehow contrived and would look forward to perhaps more realistic settings where the attackers have more restricted access (e.g., visibility of the existing graph and only able to construct and connect new nodes, etc.)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8867/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810588707,
            "cdate": 1698810588707,
            "tmdate": 1699637115827,
            "mdate": 1699637115827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZjconCm2UN",
                "forum": "iS5ADHNg2A",
                "replyto": "WY0HrHWbvt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer RLh1 (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comments and valuable feedback for further improving our work. Please see our point-to-point responses as follows.\n\n**Q1. Grammatical typos.**\n\nThank you for catching the grammatical errors. We have proofread the paper and fixed them in the revised version (highlighted in blue).\n\n\n**Q2. Missing mathematical formulation of fairness definitions.**\n\nThank you for your kind suggestion. Given the space limitation, we have added a new section in Appendix (i.e., Appendix B) for mathematical formulations and formal definitions of statistical parity and individual fairness. A pointer to Appendix B is also added in Section 2-B.\n\n\n**Q3. Inconsistent organization between letter-based indexing and numerical indexing.**\n\nThank you for your suggestion about consistent indexing. We have changed the indexing in Section 4 to letter-based indexing. Meanwhile, for the second-level indexing, we are using numerical indexing to avoid confusion. For example, numerical indexing ((1), (2), ...) are used in Section 2-C. Please let us know if you would like use to change the second-level indexing to other format and we would be more than happy to accommodate it.\n\n\n**Q4. More discussion about other bias function other than statistical parity.**\n\nThank you for your suggestion. We would like to clarify that, in addition to statistical parity, we instantiate our method by attacking individual fairness in Section 5 and discuss strategies on attacking a specific demographic group in statistical parity and the best/worst accuracy group in Appendix H.D and H.E in the original manuscript. More specifically, for attacking individual fairness, we define the bias function as $b\\left(\\mathbf{Y}, \\Theta^*, \\mathbf{S}\\right) = \\operatorname{Tr}\\left(\\mathbf{Y}^T \\mathbf{L}_{\\mathbf{S}} \\mathbf{Y}\\right)$. For attacking a specific demographic group in statical parity (Appendix H.D), we can set the bias function to either $b\\left(\\mathbf{Y}, \\Theta^*, \\mathbf{F}\\right) = - \\operatorname{P}\\left[{\\tilde y} = 1 \\mid s = a\\right]$ to decrease the acceptance rate of demographic group with sensitive attribute value $s=a$ or $b\\left(\\mathbf{Y}, \\Theta^*, \\mathbf{F}\\right) = \\operatorname{P}\\left[{\\tilde y} = 1 \\mid s = 1\\right] - \\operatorname{P}\\left[{\\tilde y} = 1 \\mid s = 0\\right]$ to increase the gap of acceptance rate between group $s = 1$ and group $s = 0$. For attacking the best/worst accuracy group (Appendix H.E), we can set the bias function to be the task-specific loss (e.g., cross-entropy) of the demographic of nodes with best/worst accuracy, which is conceptually similar to adversarial attacks on the utility as shown in Metattack [1], but focusing on a subgroup of nodes determined by the sensitive attribute rather than the validation set. We would highly appreciate if the reviewer could clarify the specific bias function that needs to be discussed, and we would be more than happy to provide additional discussion if possible."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475438413,
                "cdate": 1700475438413,
                "tmdate": 1700475438413,
                "mdate": 1700475438413,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7hw2UOZYSz",
                "forum": "iS5ADHNg2A",
                "replyto": "ZjconCm2UN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Reviewer_RLh1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Reviewer_RLh1"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your replies. For Q4, this was referring to other group fairness definitions besides statistical parity. \nI am inclined to keep my score based on your feedback here and upon seeing the other reviews/responses."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699056610,
                "cdate": 1700699056610,
                "tmdate": 1700699056610,
                "mdate": 1700699056610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "372wf7csb8",
            "forum": "iS5ADHNg2A",
            "replyto": "iS5ADHNg2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8867/Reviewer_CLho"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8867/Reviewer_CLho"
            ],
            "content": {
                "summary": {
                    "value": "The paper first defines deceptive fairness attacks on graphs, which is a new type of threat model that is compatible with many of the existing settings. Then the paper proposes FATE, a framework that models this problem via a bi-level optimization treatment, and offers a solution using meta-learning. Finally, the paper empirically confirmed the effectiveness of their framework on real-world datasets in the task of semi-supervised node classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Very clear motivation and problem formulation\n2. Extensive experimental results and clear explanations.\n3. Theoretical analysis is necessary and appears to be correct."
                },
                "weaknesses": {
                    "value": "The terminology of \"deceptiveness\" may be confusing and is not clearly explained, from an intuitive level, in the manuscript."
                },
                "questions": {
                    "value": "What's the intuition behind the method being \"meta-learning\"? It appears that this is not a typical treatment of meta-learning in graph representation learning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8867/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825391734,
            "cdate": 1698825391734,
            "tmdate": 1699637115551,
            "mdate": 1699637115551,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uvPTEkBVEh",
                "forum": "iS5ADHNg2A",
                "replyto": "372wf7csb8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer CLho"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comments and valuable feedback for further improving our work. Please see our point-to-point responses as follows.\n\n**Q1. Explain the term `deceptiveness' from an intuitive level.**\n\nThank you for your question. In our revised version (Section 1 paragraph 3), we highlight that \"the lower-level problem optimizes a task-specific loss function to maintain the performance of the downstream learning task and enforces budgeted perturbations to make the fairness attacks deceptive\". Budgeted perturbations (i.e., imperceptible changes) have been widely adopted in existing works on adversarial attacks for graph neural networks. In addition, we believe a deceptive fairness attack is also important in maintaining the performance of downstream learning task, which is mentioned in Section 2-C in the original manuscript \"a performance degradation in the utility would make the fairness attacks not deceptive from the perspective of a utility-maximizing institution.\" The reason is that it is easy for a utility-maximizing institution to detect the abnormal performance of the graph learning model, which is trained on the poisoned data. Such abnormal performance may trigger the auditing toward model behaviors, which may further lead to the investigation on the data used to trained the model. With careful auditing, it is possible to identify fairness attacks as shown in our analysis of manipulated edges due to significantly more edges that have certain properties (e.g., connecting two nodes with a specific sensitive attribute value). Then the institution might take actions (e.g., data cleansing) to avoid the negative consequences caused by fairness attacks. \n\n**Q2. Intuition behind the method being `meta learning'.**\n\nThank you for your question. In our work, as we mentioned in the manuscript, we treat the input graph as a hyperparameter to be learned other than model parameters and apply meta learning to find the suitable hyperparameter (i.e., the input graph) that maximizes the bias function. Similar strategy has been adopted in hyperparameter optimization [1] and in adversarial attacks on the utility of graph neural networks [2], as well as in graph structure learning [3, 4]\n\n**References**\n\n[1] Franceschi, Luca, et al. \"Bilevel programming for hyperparameter optimization and meta-learning.\" International conference on machine learning. PMLR, 2018.\n\n[2] Z\u00fcgner, Daniel and Stephan G\u00fcnnemann. \u201cAdversarial Attacks on Graph Neural Networks via Meta Learning.\u201d ArXiv abs/1902.08412 (2019)\n\n[3] Franceschi, Luca, et al. \"Learning discrete structures for graph neural networks.\" International conference on machine learning. PMLR, 2019.\n\n[4] Xu, Zhe, Boxin Du, and Hanghang Tong. \"Graph sanitation with application to node classification.\" Proceedings of the ACM Web Conference 2022. 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475289719,
                "cdate": 1700475289719,
                "tmdate": 1700475289719,
                "mdate": 1700475289719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dYUcozJvVf",
                "forum": "iS5ADHNg2A",
                "replyto": "uvPTEkBVEh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8867/Reviewer_CLho"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8867/Reviewer_CLho"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I've maintained my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552214285,
                "cdate": 1700552214285,
                "tmdate": 1700552214285,
                "mdate": 1700552214285,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]