[
    {
        "title": "DiffSound: Differentiable Modal Sound Simulation for Inverse Reasoning"
    },
    {
        "review": {
            "id": "acT6NcwAGm",
            "forum": "6jFjYmahxu",
            "replyto": "6jFjYmahxu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5060/Reviewer_oyHm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5060/Reviewer_oyHm"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an end-to-end framework for inferring the geometry and material properties of objects based on the frequency domain representation of the sound that they make.  To overcome some of the challenges, e.g., a sparse spectrogram representation, the authors propose a hybrid loss that first uses optimal transport to compute an approximate solution, and then the L1 loss to refine the solution.  Experiments are run to test material, geometry, impact positioning, independently."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is tackling an important and challenging problem, which is especially of interest with increasing interest in AR/VR applications.\n+ The writing is good, and the problem and solution are easy to follow, even for a non-expert."
                },
                "weaknesses": {
                    "value": "- I accept that the problem being tackled here is challenging, but the experimentation seems very limited.  The paper more shows anecdotal examples rather than present summary statistics for a larger test set with examples for illustration.  \n- I am wondering how does the method fare in terms of accuracy for different materials?  Different object sizes?  and so on."
                },
                "questions": {
                    "value": "How sensitive is the approach in terms of placement of the sensor?  If the microphone is too far away, does environmental effects influence the results (e.g., reverberation or other material properties that might affect reflectance, etc.?).  \n\nHow much does the complexity of the shape influence reconstruction?  For example, an intricate and non-convex shape that impedes direct path to the sensor? \n\nFor Equation (10), why are both terms required?  One is just a compressed form of the other?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5060/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698255214730,
            "cdate": 1698255214730,
            "tmdate": 1699636495812,
            "mdate": 1699636495812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HZkKPZqmQw",
                "forum": "6jFjYmahxu",
                "replyto": "acT6NcwAGm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5060/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5060/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. Indeed, if the microphone is positioned too far, the model may be susceptible to noise interference. However, it's noteworthy that reverberation or reflectance should not impact the results since our model exclusively relies on frequency information in the sound, and these phenomena do not alter the frequency of modes.\n2. In the context of shape reconstruction, we specifically leverage the eigenmodes of objects, making the sensor placement immaterial to the results. Notably, as objects of different shapes can produce remarkably similar sounds, a challenge arises when numerous shapes yield similar sounds to the ground truth, potentially leading to optimization convergence towards an incorrect shape.\n3. The absence of logarithmic transformation results in rapid signal decay, causing the model to focus solely on the initial spectrogram frames. The use of log spectrograms proves beneficial in capturing damping effects over time. However, it is essential to note that for modes with substantial damping factors, relying solely on log spectrograms might overlook their contribution. Hence, a combination of both approaches is preferable."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5060/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477573976,
                "cdate": 1700477573976,
                "tmdate": 1700477573976,
                "mdate": 1700477573976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TCh9csEi2u",
                "forum": "6jFjYmahxu",
                "replyto": "HZkKPZqmQw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5060/Reviewer_oyHm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5060/Reviewer_oyHm"
                ],
                "content": {
                    "title": {
                        "value": "Comment from Reviewer oyHm"
                    },
                    "comment": {
                        "value": "Thanks for providing the clarifications -- these make sense."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5060/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710385064,
                "cdate": 1700710385064,
                "tmdate": 1700710385064,
                "mdate": 1700710385064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1z6glhIfsC",
            "forum": "6jFjYmahxu",
            "replyto": "6jFjYmahxu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5060/Reviewer_Kc8Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5060/Reviewer_Kc8Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents DiffSound framework that connects material parameters of a solid body and acoustic features from the body in a differentiable manner. \nUsing this model, we can construct a neural network that simulates audio signals when impacting the object or inferres the object shape from the audtory information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The differentiable simulation is carefully derived from relevant literatures such as tetrahedral mesh, generalized eigenvalue decomposition and superimposed sinusoidal signals."
                },
                "weaknesses": {
                    "value": "My major concern is that the reviewer is not convinced with the importance of shape geometry reasoning from audio signals. \nI think audio modality is not as informative to recover the shape of objects. Indeed, Figure 6 gives smoothed mesh surface. The shape may be distorted without sufficient voxel constraints. \n\nIs there any application scenario?\nPerhaps this model may be applied to non-invasive examination of solid structures like impacting the surface and observing the responding signals. \nHowever, we cannot see such usages from the current set of experimental results."
                },
                "questions": {
                    "value": "1. Eq. (5) to (6)\n\nI could follow the derivation of Eq. (6) from (5). Do you use $\\partial {\\bf u}_i=0$ or some transformation of ${\\bf u}_i ^T {\\bf M} {\\bf u}_i$?\n\n2. What do you mean by *hybrid loss*?\n\nIs it hybrid because linear and logarithm error is combined as in Eq. (10)? \nOr does this mean the use of $\\ell_1$ loss and OT-based loss?\n\n3. Using ground truth $\\nu$\n\nIn the result of Table 1, how is estimation of $\\nu$ critical to reduce the error in the spectrogram? \nDoes the error reduce if ground truth Poisson's ratio $\\nu$ is given?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Reviewer_Kc8Y"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5060/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663911204,
            "cdate": 1698663911204,
            "tmdate": 1699636495723,
            "mdate": 1699636495723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xs2pRVEybm",
                "forum": "6jFjYmahxu",
                "replyto": "1z6glhIfsC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5060/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5060/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. In the general eigendecomposition definition, $u_i^TMu_i = I$.\n2. The term \"hybrid\" denotes the utilization of both $\\ell_1$ loss and OT-based loss.\n3. Indeed, if the ground truth vector $\\nu$ is provided, the error will diminish. Conversely, if $\\nu$ is poorly estimated, the error will be substantial."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5060/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477556571,
                "cdate": 1700477556571,
                "tmdate": 1700477556571,
                "mdate": 1700477556571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9Hy0jHvbaD",
            "forum": "6jFjYmahxu",
            "replyto": "6jFjYmahxu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5060/Reviewer_aeBV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5060/Reviewer_aeBV"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a differentiable sound simulation framework called DIFFSOUND, containing three components. The first component is a differentiable tetrahedral representation, which uses implicit neural representation to encode SDF values and convert the encoded SDF into an explicit tetrahedral mesh. The second component uses a high-order finite element method to optimize material properties and shape parameters. In the end, an additive audio synthesizer synthesizes the sound."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The idea of building a differentiable sound simulation pipeline is very interesting. While the task is challenging, I am glad that the authors come up with a solution that will definitely be useful for various applications.\n2) The component introduced in this work is highly interpretable. To my best knowledge, physical properties such as Young's modulus and Poisson's ratio were not modeled in the previous audio synthesizers.\n3) Three inverse problems are conducted, and the results look reasonable.\n4) The supplementary includes the code, which will be useful for reproduction."
                },
                "weaknesses": {
                    "value": "1) One main concern is that the paper writing is very rough. For example, in 3.1 Differentiable tetrahedral representation, there is no formal mathematical definition for the input-output, INR, tetrahedron mesh, and transformation function. The description in 3.1 is high-level and not informative. In 3.3, the loss equations 7 and 10 use the same annotation, but the $i$ means totally different things. Section 4 is a weird combination of both ablation studies and experiments on three inverse problems. I believe the inverse problems should take an independent section because it is one of the main contributions of this work. In tables and figures, annotations like baselines 1, 2, and 3 could be confusing since there is no corresponding description in captions. While each of these items could be a minor issue, the overall reading experience is actually bad.\n2) I am interested in how fast the optimization could be done for each object, but there is no clue in the paper. While it is okay that the current approach could not support real-time applications, it should contain an analysis for the optimization time.\n3) One thing that confuses me is the ground truth eigenvalues. How do you obtain the ground truth eigenvalue? Without supervision on the eigenvalues, the optimization problem becomes much more challenging. Is it possible to optimize with only the audio loss?\n4) From my own experiences, the Wasserstein distance is indeed helpful in bridging the ground truth and predicted spectrograms. However, the switch timing between Wasserstein loss and L1/L2 spectrogram loss is undefined. How do you determine 'sufficient convergence'?"
                },
                "questions": {
                    "value": "See my questions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Reviewer_aeBV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5060/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716636705,
            "cdate": 1698716636705,
            "tmdate": 1699636495617,
            "mdate": 1699636495617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WMh430WBbv",
                "forum": "6jFjYmahxu",
                "replyto": "9Hy0jHvbaD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5060/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5060/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. The experiments concerning shape geometry reasoning are conducted on a synthetic dataset, where ground truth eigenvalues are computed using the standard modal analysis process. For material parameters reasoning, real-world datasets are employed, relying solely on audio loss for optimization.\n2. Currently, we arbitrarily define 1000 epochs as a point of 'sufficient convergence.'"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5060/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477505740,
                "cdate": 1700477505740,
                "tmdate": 1700477505740,
                "mdate": 1700477505740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2WHoWWAMmh",
            "forum": "6jFjYmahxu",
            "replyto": "6jFjYmahxu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5060/Reviewer_cDNR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5060/Reviewer_cDNR"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a differential simulation framework for sound synthesis of physical objects impacts. The framework is a pipeline that employs a NeRF-like MLP to reconstruct the Signed Distance Function and translate it into the shape of the object. These are then being used by Finite Elements Method to recover object shape and an Additive Synthesizer to generate sound which is optimized by minimizing loss between the expected and groundtruth spectrograms. Experiments are performed on ObjectFolder-Real dataset for sounds from 100 objects."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The work proposes an additional step of recovery of Signed Distance Function done by MLP to assist with object shape recovery and synthesis of impact sound of the object.\n\n2. Synthesized results appear to be corresponding to objects and their expected sounds.\n\n3. The paper is well written."
                },
                "weaknesses": {
                    "value": "1. The choice of baselines and whether these are strongest possible baselines is unclear.\n\n2. The experiments are done on 100 objects only.\n\n3. Train/validation/test split is not specified and thorough quantitive accuracy of these is not presented.\n\n4. Technical contribution is limited since the components of the pipeline are standard. Ablations with extensions of the components are needed to examine whether these are optimal for sound synthesis."
                },
                "questions": {
                    "value": "1. The current pipeline is split between a neural network approach and FEM simulator. Could both steps be modeled with a neural networks?\n\n2. How would the work compare with impact sound generation from videos through diffusion model?\nSu, Kun, et al. \"Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n3. What is the computational complexity of the pipeline?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5060/Reviewer_cDNR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5060/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699583662472,
            "cdate": 1699583662472,
            "tmdate": 1699636495541,
            "mdate": 1699636495541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GsTrfJ8swb",
                "forum": "6jFjYmahxu",
                "replyto": "2WHoWWAMmh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5060/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5060/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. While it is possible to model both steps using neural networks, achieving comparable accuracy to a classic solver is challenging.\n2. Our paper specifically addresses the inverse problem of inferring physical parameters from impact sound. In contrast, the referenced paper by Su, Kun, et al. focuses on predicting sound from visual features.\n3. The computational complexity of the pipeline can be approximated as O(MN^2), where N represents the number of vertices and M is the number of modes. This estimation accounts for the most time-consuming aspect, which is the eigen decomposition process."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5060/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477481314,
                "cdate": 1700477481314,
                "tmdate": 1700477481314,
                "mdate": 1700477481314,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]