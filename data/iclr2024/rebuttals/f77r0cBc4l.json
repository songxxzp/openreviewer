[
    {
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond"
    },
    {
        "review": {
            "id": "9ZvL62zRWO",
            "forum": "f77r0cBc4l",
            "replyto": "f77r0cBc4l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6788/Reviewer_ThD7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6788/Reviewer_ThD7"
            ],
            "content": {
                "summary": {
                    "value": "This work comprehensively evaluates 10+ leading LLMs, such as OpenAI's GPT series models, Claude 2, and Llama2, on 20+ curated benchmarks across 7 carefully chosen capability categories, including knowledge, reasoning, comprehension, math, code, multilingual, and safety. The comparison results offer valuable insights into the evolutionary path from GPT-3 series to GPT-3.5 series and GPT-4, and partially answer some important questions that are of curiosity to the community."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Significance**: This work provides a high-quality and comprehensive benchmark for LLMs research, which may provide a good foundation for LLMs development and comparison. \n\n**Quality**:  Each dimension of the evaluation benchmark (e.g., metric, model, used prompt, black-box evaluation vs white box evaluation, etc) is carefully chosen. The analysis about the evaluation results is well conducted and deliver some useful information. \n\n\n**Clarity**: The paper is well-written, and the structure and figure is very clear."
                },
                "weaknesses": {
                    "value": "**Originality**: l have seen that the authors clearly compare this work with previous LLMs benchmark work (in the penultimate paragraph in the introduction section), and it appears to be the first benchmark that consistently evaluates so many LLM models across multiple capability dimensions. I am curious to know if there are any novel evaluation dimensions proposed in this work."
                },
                "questions": {
                    "value": "1. Have the authors open-sourced the benchmark work, and what costs are associated with evaluating a newly trained model? \n2. Are the current capability dimensions sufficient to systematically evaluate current Low-Level Models, or is there any important metric that this work is missing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697966519800,
            "cdate": 1697966519800,
            "tmdate": 1699636784137,
            "mdate": 1699636784137,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ylkDzfVSw3",
                "forum": "f77r0cBc4l",
                "replyto": "9ZvL62zRWO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful assessment of our work and the constructive queries raised. We are delighted to hear that you found our paper to be of good quality, significance and clarity.\n\n**Response #1 - Originality**\n\nWe appreciate your feedback on the originality of evaluation dimensions. The focus of our work is to provide insights for better understanding the key techniques of leading LLMs, so as to improve their transparency for the community. Therefore, creating new LLM capability dimensions is not our first priority. Nevertheless, as LLMs are getting more and more powerful, we will continue the efforts of adding new evaluation dimensions in GPT-Fathom to assess advanced LLM capabilities, such as long-context understanding, multi-turn conversation, LLM agent, etc. With these new dimensions, we can study the strength and weakness of leading LLMs more comprehensively.\n\n**Response #2 - Open-sourcing and costs:**\n\nTo facilitate the research community for reproducible and systematic LLM evaluation, we have open-sourced GPT-Fathom and will post the link in our paper after the anonymity period. With regard to the effort required to evaluate a new model, our codebase serves as a read-to-use toolkit for evaluating new models. Users just need to implement a completion function, and we provide a step-by-step tutorial in our open-source repo. The evaluation time cost depends on the model size, inference efficiency and available computational resources / API rate limtis.\n\n**Response #3 - Sufficiency of capability dimensions:**\n\nWe believe that the chosen capability dimensions cover a broad spectrum of LLM functionalities. We acknowledge that as LLMs rapidly progress, as mentioned above, we will continue the efforts of adding new evaluation dimensions in GPT-Fathom to cover advanced capabilities. Currently, we have already added evaluations of factuality reasoning and long-context understanding, which is available in our repo (link will be posted after the anonymity period).\n\nWe hope these responses adequately address your questions. Your feedback is invaluable to us, and we look forward to further discussions on our work."
                    },
                    "title": {
                        "value": "Official Response to Reviewer ThD7"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416070617,
                "cdate": 1700416070617,
                "tmdate": 1700615269703,
                "mdate": 1700615269703,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HgKY6Eyecc",
            "forum": "f77r0cBc4l",
            "replyto": "f77r0cBc4l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6788/Reviewer_Fohj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6788/Reviewer_Fohj"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new benchmark GPT-Fathom for comparing the performance of closed-source and open-source LLMs. The benchmark is comprehensive and consistently compares different LLMs. The paper has shown the impact of evolution of closed-source LLMs, inclusion of coding datasets during training, as well as alignment methods such as SFT and RLHF."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Creating a well-designed benchmark for LLMs is an important problem statement\n- Consideration of both open source and closed source LLMs in the evaluation\n- Focus on reproducibility, consistent settings, and ablation of methods/prompts.\n- Extensive experiments and analysis of LLMs\n- Explanation of why the black box evaluation was considered for all benchmarks and LLMs.\n- Interesting analysis of evolution of OpenAI LLMs."
                },
                "weaknesses": {
                    "value": "The research questions addressed by the paper is unclear. For a research publication focused on benchmarking, it is insufficient to study a new set of LLMs and explain the results. The paper needs to explain the benchmark design and how it has resulted in a substantial improvement over existing benchmarks. Listing a few points below. \n- The paper claims results are based on \"aligned\" settings, but still includes numbers from other papers (in brackets) and optimized results (with a star). Instead, it will be useful to compare the numbers in existing papers, and show the impact of the aligned evaluation settings. Did the results change? If so, what was the reason? Such an analysis would confirm their posed hypothesis that aligned evaluation settings lead to more insights than those already published.\n- Similarly, it would be good to understand each new feature introduced by GPT-Fathom compared to prior benchmarks, and show why it led to a better evaluation outcome not just for LLMs considered but also for future LLMs that will get evaluated.\n- The paper claims that they have picked representative LLMs, benchmarks, and settings. Why are the choices made representative? No explanation have been provided. Without an explanation, the benchmark looks like a collection of benchmarks from other papers, and the benefits of the proposed benchmark is not clear. \n- The paper acknowledges that the categories in the benchmarks are chosen based on their \"own interpretation\". But no justification is provided to explain why this  interpretation should be adopted by the research community as the best benchmark to use for LLMs. Some analysis on how these benchmarks cover the range of tasks that LLMs are used for will be useful. \n- The paper repeatedly states that it is trying to answer open questions from the community. The open questions that these benchmarks provide answers for is not clearly stated. \n- Prompt sensitivity is an important issue. Two prompts show there is an issue, but it is unclear if LLMs work well for these two prompts that the sensitivity issue is resolved. A better design to evaluate sensitivity with an appropriate metric will be more useful. \n- In consistency in the number of shots across benchmarks and types of ablation does not show \"aligned\" settings claimed by the paper."
                },
                "questions": {
                    "value": "Some clarification questions:\n- How are you automating the testing of web-version of LLMs? Is that done manually or through some web toolkit?\n- I did not understand what is meant by \"pass@K\". Do you pick the best answer out of K retries? \n- Why does Table 5 and 6 not use zero-shot prompting? \n- Why do different benchmarks use different shots?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698520020108,
            "cdate": 1698520020108,
            "tmdate": 1699636784008,
            "mdate": 1699636784008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IoPAiWO33w",
                "forum": "f77r0cBc4l",
                "replyto": "HgKY6Eyecc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review and valuable feedbacks. We appreciate the opportunity to clarify and expand on certain aspects of our paper.\n\n**Response #1 - Research questions and improvement over existing benchmarks**\n\nThe key research question studied in our work is: How GPT-3 progressively improves to GPT-4? Since OpenAI has not revealed much about their evolutionary path, to answer this question, we systematically evaluate all their major LLMs to break down the impacts of techniques such as adding code data for pretraining, SFT and RLHF. Therefore, instead of just creating yet another LLM leaderboard, the focus of our work is to improve the transparency of leading LLMs (especially closed-source ones) and offer insights for better understanding how their techniques improve LLMs across various capabilities.\n\nCompared to existing works on LLM evaluation, our work improves in the following aspects:\n\n1. Reproducible evaluation under aligned settings: LLMs are known to be sensitive to the evaluation setting, i.e., different settings and prompts may lead to very different evaluation results. Yet, many existing work simply refer to scores from other papers without consistent settings and prompts, which may easily skew the observations. In contrast, our work provides systematic evaluation of leading LLMs across various capability dimensions, all under aligned settings for fair comparison. We provide an open-source LLM benchmark suite as an off-the-shelf toolkit, so that other researchers can easily reproduce our reported LLM performance, and save a lot of efforts when comparing a new model with existing LLMs.\n\n2. Novel insights on the impacts of SFT / RLHF: Our work provides in-depth analysis on how SFT / RLHF affects various LLM capabilities, such as SFT having opposite effects on weaker and stronger base models, and RLHF transforming pass@100 (intrinsic coding abilities) into pass@1 (one-take bug-free coding abilities). These novel insights offer valuable guidance for future advancement of LLMs.\n\n3. Novel challenges of advanced LLMs: Our work discovers novel challenges that leading models have to address, such as the \"seesaw phenomenon\" in LLM capabilities, where certain capabilities improve, while a few other capabilities clearly regress. We demonstrate the existence of this phenomenon in both leading closed-source and open-source LLMs, inspiring the research community to dedicate more efforts to tackle this challenge.\n\n4. Novel study on model sensitivity: Previous works on LLM evaluation rarely study the sensitivity of model, such as sampling variance and prompt sensitivity. In fact, many existing LLM leaderboards reference scores from other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In contrast, we demonstrate that even leading open-source LLMs are susceptible to prompt variations: a minor alteration of the prompt could cause the LLM to fail in solving the problem. We highlight the model sensitivity issue with our experimental results, and urge the community to improve the robustness of LLMs.\n\nTo summarize, GPT-Fathom is not just a collection of benchmarking results. We offer novel insights for community-concerned questions and discover novel challenges, aiming to improve the transparency of leading LLMs and inspire further advancements of LLMs.\n\nWe hope the clarifications above address your concerns about the research questions and novelty of our work. We appreciate your helpful feedbacks in refining our submission."
                    },
                    "title": {
                        "value": "Official Response to Reviewer Fohj[1/3]"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416742345,
                "cdate": 1700416742345,
                "tmdate": 1700615211657,
                "mdate": 1700615211657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hz6mYgt1I6",
                "forum": "f77r0cBc4l",
                "replyto": "HgKY6Eyecc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Response #2 - Numbers from other papers**\n\nWe apologize for the confusion about the evaluation settings. We clarify our choice as follows.\n\nWe refer to numbers from PaLM 2-L and Claude 2 papers since we currently have no access to their APIs (Google only provides access to smaller PaLM 2 models; Claude 2 only provides API access to organization users). We try our best to align our evaluation settings with these two models, such as the number of \"shots\", whether CoT prompting is used, and the evaluation metrics. Since PaLM 2-L and Claude 2 have different evaluation settings in their official papers, we separate them into two Tables (please refer to Table 1 and Table 2) and in each table, we evaluate other models with aligned settings on every benchmark for fair comparison.\n\nFor our own reported results, we strictly align all the evaluation settings and prompts, with the only exceptions for Llama models, where a few numbers are marked with * where we use optimized prompts. This is because we find that Llama models are quite prompt-sensitive, while OpenAI's models are pretty robust. If we randomly pick a prompt, the numbers of Llama models could be significantly lower than their officially reported scores, which may incur challenges of fairness of our evaluation suite. In fact, we study the model sensitivity in detail in later part of Sec 3.2 (please refer to Table 6), and the original scores without using optimized prompts are listed there.\n\nA direct comparison with numbers from other papers is challenging due to the lack of released prompts and few-shot samples in these papers. This lack of transparency in benchmark settings underlines the necessity of our open-source reproducible LLM benchmark suite. Nevertheless, we do carefully compare our evaluation results with officially reported numbers, and most our results match the reported performance in official papers, within a margin of slight deviation.\n\n**Response #3 - Selection criteria for benchmarks and LLMs**\n\nAt the beginning of Sec 2.2, we describe our criteria for benchmark selection: 1) cover as many aspects of LLM capabilities as possible; 2) adopt widely used benchmarks for LLM evaluation; 3) clearly distinguish strong LLMs from weaker ones; 4) align well with the actual usage experience of LLMs. Following these criteria, we carefully pick benchmarks in our evaluation suite. We admit that there is no \"perfect\" selection of benchmarks for any evaluation framework, since both benchmarks and LLMs are evolving. We are currently working on latest benchmarks such as SciBench, SummEdits and QuALITY for evaluating advanced reasoning and long-context capabilities of LLMs.\n\nWith regard to the selection criteria of LLMs, we also have a brief discussion in Sec 2.1 that our goal is to help the community better understand how OpenAI improves from GPT-3 to GPT-4 and pinpoint the position of LLMs under development. To achieve this goal, we mainly consider evaluating these types of LLMs: 1) OpenAI\u2019s leading models, such as GPT-4; 2) OpenAI\u2019s major earlier models, such as GPT-3 and InstructGPT; 3) other leading closed-source models, such as PaLM 2 and Claude 2; 4) leading open-source models, such as LLaMA and Llama 2. Since LLMs are still rapidly evolving, we continue the effort of evaluating cutting-edge LLMs, such as GPT-4-Turbo from OpenAI. In our updated submission, we have added the evaluation results of gpt-3.5-turbo-instruct-0914, a new model from OpenAI which was just released in 2023/10.\n\n**Response #4 - Benchmark categories**\n\nFor the categories of benchmarks, we investigate many existing works especially the papers of OpenAI models, Claude 2, PaLM 2, LLaMA and Llama 2, which agree on a reasonable categorization of popular benchmarks. For example, BBH is often attached to the \"reasoning\" category, while GSM8K is clearly associated with the \"math\" category. We do admit that our categorization of benchmarks is by no means the exclusive approach, but a reasonable and commonly acceptable one.\n\n**Response #5 - Addressing open questions**\n\nWe apologize for the confusion. The key question that the community is currently eager to know is how GPT-3 progressively improves to GPT-4, including technical details such as whether adding code data improves LLM's reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. We briefly introduce these open questions in the introduction, and detail them in Section 3.2.\n\n**Response #6 - Prompt sensitivity**\n\nYour point on prompt sensitivity is well-taken. We acknowledge this as a valuable area for future research. Due to resource constraints, we could not comprehensively benchmark the prompt sensitivity of all models. However, we do clearly demonstrate the existence of this issue, and inspire the research community to address this challenge in the future."
                    },
                    "title": {
                        "value": "Official Response to Reviewer Fohj[2/3]"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416768775,
                "cdate": 1700416768775,
                "tmdate": 1700615220623,
                "mdate": 1700615220623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TqWAMD6OY5",
                "forum": "f77r0cBc4l",
                "replyto": "HgKY6Eyecc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Response #7 - Aligned settings and ablation studies**\n\nOur \"aligned setting\" refers to the main evaluation results of LLMs on each benchmark are obtained by the same prompt, same number of \"shots\", same sampling hyperparameter, and same metrics, etc. Please note that these evaluation settings are aligned within each benchmark, but may differ across different benchmarks. For example, we adopt 1-shot setting for most benchmarks, but follow the most commonly used 5-shot setting for MMLU. In the ablation studies, we do alter the number of \"shots\" since we need to investigate how it impacts the performance of LLMs.\n\n**Response #8 - Clarification questions**\n\nAutomating Web-version testing: We wrapped up the Web-version ChatGPT (including GPT-3.5 and GPT-4) into a local API for the ease of bulk evaluation, following some open-source projects such as ninja (https://github.com/gngpp/ninja).\n\nClarification on \"pass@k\" metric: This is a widely used metric in evaluating the coding capability of LLMs since [1], and we follow the standard definition of pass@k in our work: for each coding problem, k code samples are generated, and a problem is considered solved if any sample passes the unit tests, and the total fraction of problems solved is reported. We have added the definition of pass@k in the \"Coding\" paragraph of Sec 2.2 in our updated submission. Thank you for your feedback helping us improve the clarity of our work.\n\nNumber of \"shots\" in Tables 5 & 6: We report the results of our ablation study on how CoT prompting and prompt variation impacts LLM performance in Table 5 & 6, respectively, and we keep the number of \"shots\" consistent with our main setting (as in Table 1) rather than zero-shot. We do have the zero-shot setting in another ablation study on the impact of number of \"shots\" (Table 4), where we report the zero-shot performance on ARC-c and HellaSwag.\n\nDifferent benchmarks use different \"shots\": Our ablation study on the number of \"shots\" (Table 4) shows that the performance of LLMs struggles to improve beyond 1-shot, particularly for stronger models. This indicates that 1-shot typically works well, which is our primary evaluation setting for most benchmarks (Table 1). However, for some benchmarks, there exists widely adopted number of \"shots\" in existing works, such as 5-shot for MMLU, 8-shot CoT for GSM8K, 0-shot for HumanEval, etc., and we follow these widely adopted \"shots\" for ease of comparing our results to existing reported scores. Note that these number of \"shots\" are also adopted in the paper of PaLM 2, so that we can compare with its performance using the same number of \"shots\" in Table 1.\n\n\nWe hope our responses above adequately address your concerns and clarify the aspects you highlighted. Your feedback has been instrumental in refining our work and we have made the suggested modifications based on your valuable feedback.\n\n\n[1] Chen, Mark, et al. \u201cEvaluating Large Language Models Trained on Code.\u201d arXiv, 7 July 2021, http://arxiv.org/abs/2107.03374. arXiv."
                    },
                    "title": {
                        "value": "Official Response to Reviewer Fohj[3/3]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416778319,
                "cdate": 1700416778319,
                "tmdate": 1700615227115,
                "mdate": 1700615227115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lONZyR3Qtj",
                "forum": "f77r0cBc4l",
                "replyto": "HgKY6Eyecc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Fohj, we revised our paper with the comparison of our scores and the official scores (refer to the *General Response to Reviewers*). Please let us know your thoughts, and we are more than happy to answer any further questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615119156,
                "cdate": 1700615119156,
                "tmdate": 1700615319805,
                "mdate": 1700615319805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SLxHlwXp1H",
                "forum": "f77r0cBc4l",
                "replyto": "tFHJoAUs5S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6788/Reviewer_Fohj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6788/Reviewer_Fohj"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the comparative analysis"
                    },
                    "comment": {
                        "value": "I was looking for something like this in the paper. So this is a good addition, it shows you can replicate results. Some suggestions below:\n1. Instead of conclusions such as \"slight deviation\" and \"pretty marginal\", make conclusions with statistical tests. \n2. Is the second table, are you following the same prompts as the official benchmark?\n3. Experiments here are not sufficient to conclude prompt sensitivity, that requires a different experiment with its own hypothesis testing. \n4. I was looking for a similar table comparing your benchmark with the other open source benchmarks. This would demonstrate that your aligned setting did improve on prior benchmarks. I still do not see conclusive evidence that you are improving on other benchmarks."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672940319,
                "cdate": 1700672940319,
                "tmdate": 1700672940319,
                "mdate": 1700672940319,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RP9R0sI48M",
                "forum": "f77r0cBc4l",
                "replyto": "HgKY6Eyecc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6788/Reviewer_Fohj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6788/Reviewer_Fohj"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Responding to each aspect discussed in your comment.\n- \"How GPT-3 progressively improves to GPT-4?\" is not a research question. \"Does adding code data to pretraining improve performance\"? is a research question. The research question needs to specific, measurable with a metric, and outcome of the result should be clear. So you are actually addressing three research questions here. What is not clear to me is: have these not been studied before? \n1. For the first comment, I was expecting a table with model name as columns and prior benchmark papers as rows, and finally your benchmark results at the bottom. I would like to see the impact of the \"aligned settings\" on the results compared to prior works. Your contribution hinges on a better benchmarking setup, but I'm not able to discern how much it is better compared to prior benchmarking setups where the prompts, shots, etc. were not aligned. \n2. Can you make conclusions with statistical tests? The hypothesis \"RLHF improves LLM capabilities\" should have a yes or no answer based on the result of this test. \n3. Are you the first to discover the \"seesaw phenomenon\"? Can you confirm this seesaw phenomenon consistently occurs with a statistical test? \n4. Just saying the results change with change with prompt is not enough. What is your metric to measure sensitivity? How much sensitivity is too much to declare that LLMs are sensitive?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673585269,
                "cdate": 1700673585269,
                "tmdate": 1700673679769,
                "mdate": 1700673679769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "olZ5vouFJr",
            "forum": "f77r0cBc4l",
            "replyto": "f77r0cBc4l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6788/Reviewer_aiZP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6788/Reviewer_aiZP"
            ],
            "content": {
                "summary": {
                    "value": "This paper curates a benchmark suite to evaluate the performance of LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed benchmark covers a range of aspects to study, including knowledge, math, coding, etc. \n\nIt also provides performance and analysis of several popular LLMs on the proposed benchmark."
                },
                "weaknesses": {
                    "value": "I appreciate the experiments and analysis, but I am mostly concerned with mismatched claims and unclear novelty.\n\n1. mismatched claimed: The paper underscores that the paper sheds light on \"the evolutionary path from GPT-3 to GPT-4,\" several times in the abstract, intro, and conclusion. However, after reading the main text, I could not find enough evidence and/or analysis on the evolutionary path. Figure 1 gives a visualization of OPENAI's announcements of different features/models over time, which the authors defined as evolutionary path. But how is it related to the proposed benchmark?\n\n2. unclear novelty: The proposed benchmark, GPT-Fathom, is effectively a selection/collection of (subsets of) existing benchmark datasets (MMLU, Bigbench, etc). Prompting and evaluation metrics are also quite standard. The analysis seems to resonate many well-known assertions, e.g., proprietary models are more performant. It is unclear to me what new message this paper brings in."
                },
                "questions": {
                    "value": "Please see my question in the weakness parts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6788/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728732701,
            "cdate": 1698728732701,
            "tmdate": 1699636783900,
            "mdate": 1699636783900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ni9vZb9fcL",
                "forum": "f77r0cBc4l",
                "replyto": "olZ5vouFJr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6788/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and feedbacks. We value the effort and time you dedicated to evaluating our submission and have carefully considered your comments. Here are our responses to the main issues raised.\n\n**Response #1 - Evolutionary path**\n\nWe apologize for the confusion of the \"evolutionary path from GPT-3 to GPT-4.\" To clarify, Figure 1 is not just a timeline of OpenAI's feature announcements. We use Figure 1 to visualize the major OpenAI models that are evaluated in our paper, and highlight the key techniques developed along the path from GPT-3 to GPT-4, e.g., adding code data for pretraining, using SFT / RLHF for fine-tuning. It is well-known that GPT-4 dramatically outperforms GPT-3, however, OpenAI has not revealed much about how they get there step-by-step. We evaluated all the major models between GPT-3 and GPT-4 to decipher their \"evolutionary path\", i.e., how each technique contributes to the advancement from GPT-3 to GPT-4. By providing a systematic evaluation suite, we aim to shed lights on the impacts of the key techniques used by advanced LLMs and improve the transparency of closed-source models.\n\n**Response #2 - Novelty**\n\n1. Beyond well-known facts: LLMs are known to be sensitive to the evaluation setting, i.e., different settings and prompts may lead to very different evaluation results. Yet, many existing work simply refer to scores from other papers without consistent settings and prompts, which may easily skew the observations. In contrast, our work provides systematic evaluation of leading LLMs across various capability dimensions, all under aligned settings for fair comparison.\n\n    Our work is not just resonating some well-known facts like proprietary models perform better. Instead, we provide an open-source LLM benchmark suite as an off-the-shelf toolkit, so that other researchers can not only easily reproduce our reported LLM performance, but also save a lot of efforts when they need to compare a new model with existing LLMs across various capabilities. To cover the common use cases, we intend to select popular benchmarks with standardized settings and widely adopted metrics.\n\n2. Novel insights on the impacts of SFT / RLHF: Our work provides in-depth analysis on how SFT / RLHF affects various LLM capabilities, such as SFT having opposite effects on weaker and stronger base models, and RLHF transforming pass@100 (intrinsic coding abilities) into pass@1 (one-take bug-free coding abilities). These novel insights offer valuable guidance for future advancement of LLMs.\n\n3. Novel challenges of advanced LLMs: Our work discovers novel challenges that leading models have to address, such as the \"seesaw phenomenon\" in LLM capabilities, where certain capabilities improve, while a few other capabilities clearly regress. We demonstrate the existence of this phenomenon in both leading closed-source and open-source LLMs, inspiring the research community to dedicate more efforts to tackle this challenge.\n\n4. Novel study on model sensitivity: Previous works on LLM evaluation rarely study the sensitivity of model, such as sampling variance and prompt sensitivity. In fact, many existing LLM leaderboards reference scores from other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In contrast, we demonstrate that even leading open-source LLMs are susceptible to prompt variations: a minor alteration of the prompt could cause the LLM to fail in solving the problem. We highlight the model sensitivity issue with our experimental results, and urge the community to improve the robustness of LLMs.\n\n    To summarize, GPT-Fathom is not just a collection of benchmarking results. We build open-source toolkit for reproducible LLM evaluation, offer novel insights for community-concerned questions, and discover novel challenges, aiming to improve the transparency of leading LLMs and inspire further advancements of LLMs.\n\n\nWe hope our responses above address your concerns and demonstrate the novelty of our work. We appreciate your helpful feedbacks in refining our submission."
                    },
                    "title": {
                        "value": "Official Response to Reviewer aiZP"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6788/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415941719,
                "cdate": 1700415941719,
                "tmdate": 1700690804763,
                "mdate": 1700690804763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]