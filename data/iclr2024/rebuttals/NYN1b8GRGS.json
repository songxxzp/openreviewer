[
    {
        "title": "GIM: Learning Generalizable Image Matcher From Internet Videos"
    },
    {
        "review": {
            "id": "04kZjNSp6L",
            "forum": "NYN1b8GRGS",
            "replyto": "NYN1b8GRGS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1206/Reviewer_XccY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1206/Reviewer_XccY"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the GIM framework, aiming to address the challenge of generalizing image matching to diverse real-world scenarios. Taking cues from established computer vision models, GIM adopts a zero-shot generalization approach, leveraging the vast and varied nature of internet videos. The methodology involves a two-step training process: initial training on domain-specific datasets followed by an integration with several image matching techniques. This combined model seeks to identify potential correspondences in close video frames, with outlier detection mechanisms ensuring data quality. Notably, while traditional methods such as SfM and MVS have exhibited limitations in handling in-the-wild videos, GIM claims to efficiently provide reliable supervision signals for these scenarios, thereby promising to push the boundaries of current state-of-the-art models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The introduction of the GIM framework is a significant advancement, marking the first attempt to leverage internet videos in training a universally applicable image matcher. This initiative promises to address generalization challenges across multiple real-world contexts.\n\n+ The formulation of the ZEB benchmark is a noteworthy achievement. As a pioneering zero-shot evaluation benchmark integrating real-world and simulated data, ZEB is poised to become an instrumental tool in gauging the generalization capacities of existing models."
                },
                "weaknesses": {
                    "value": "- The heavy reliance on internet videos for training might introduce biases or noise. The generalization capability of GIM, when trained on other diverse datasets, remains an unanswered question.\n\n- With the consistent performance improvement with increased video data, there might be concerns about potential overfitting. Addressing this, perhaps with regularization techniques or other measures, would be crucial.\n\n- Internet videos can be noisy, and their quality can vary. How resistant is GIM to such noise, and how does it handle low-quality data?"
                },
                "questions": {
                    "value": "-  While the use of internet videos is innovative, how did you ensure that the videos used for training represent a diverse range of scenarios, especially given the potential for internet content to have biases?\n\n- How does the GIM model handle noise and varying quality within internet videos? Were any preprocessing steps or filters applied to ensure data quality?\n\n- How did you address concerns of overfitting, especially with the consistent improvement seen with increasing video data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698574120448,
            "cdate": 1698574120448,
            "tmdate": 1699636047360,
            "mdate": 1699636047360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IqORBVTl80",
                "forum": "NYN1b8GRGS",
                "replyto": "04kZjNSp6L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviews"
                    },
                    "comment": {
                        "value": "**Weakness 1**: Thanks for your insightful comment. Though naively adding random internet videos can potentially introduce noise or bias, we have demonstrated in the experiments that by selecting tourism videos captured at diverse geo-locations and time, GIM can effectively generalize to various zero-shot inputs, e.g., images from different domains and BEV point clouds. As we discuss in the response to all reviewers, we have trained another model with 100h of data showing that GIM scales well with more video data.\n\nIn terms of other diverse datasets, we focus on internet videos since they are arguably the most accessible and scalable data source. This ensures that the performance improvement of GIM can be reproduced by almost anyone on any future architecture. However, the key to the generalization of GIM is the data diversity and scale, rather than the source of the videos. Hence, we expect that as long as the dataset is **diverse** and **large**, it will work with GIM. So for example, adding proprietary videos captured in various environments, will likely further boost performance.\n\nWe will clarify the importance of our video selection strategy and the key to the generalization of GIM in the camera-ready paper.\n\n**Weakness 2**: We completely agree that preventing overfitting is crucial. Specifically, overfitting can happen if the training videos are not diverse enough and biased towards certain types of scenes. An effective and implicit regularization that is applied in GIM is the selection of video data. We achieve this by selecting video data from diverse geo-locations and scene conditions; we focus on YouTube tourism videos and describe our search and filter strategies in Sec. 3 of the paper. Please refer to Table 6 of the paper for detailed statistics of the selected videos. Another potential factor in preventing overfitting is the amount of data. The continually growing amount and diversity of internet videos effectively reduces the risk of overfitting. We also added strong data augmentations during GIM training to make the model resistance to noise. Our experimental results in the zero-shot evaluation benchmark (ZEB) demonstrate the effectiveness of this implicit regularization approach. \n\nFinally, though never used in our video selection process, evaluating the trained model on the ZEB benchmark can also verify whether a set of candidate videos are potentially biased or noisy.\n\nWe will include the above answers in camera-ready paper and hope they sufficiently address your concerns.\n\n**Weakness 3**: For the selection of internet videos, our recommendation is to choose videos with high resolution, clear images, long duration and small number of scene transitions. To ensure these points, we choose tourism videos. They are generally 0.5-2 hours long, with a person holding the camera and recording his/her travel, mostly without transitions. GIM also has some robustness to transitions in the video since drastic changes from one frame to another make it very difficult for matches to survive after robust fitting. Even if a few matches survive, it is very difficult for the label propagation to continue. Finally, GIM includes strong data augmentations during training, which introduces noise to make the model more resistant to various sources of error in the input data.\n\nWe will include the above answer in camera-ready paper.\n\n\n**Question 1**: As mentioned in the answer of weakness 1, we did not randomly download videos from the internet. Instead, as mentioned in Sec. 3 of the main paper, we selectively used tourism videos for training. The selected tourism videos are usually captured by a person traveling around various places. Even only on YouTube, we can find tourism videos spanning thousands of hours and covering various regions, years, times, weather conditions, and lighting. These videos are not limited to a single type of scene; they include diverse activities like walking, cycling, driving and more. This ensures the data diversity and minimizes the bias. \n\nBased on your suggestion, we will describe our video selection process and its effectiveness in details in our camera-ready paper. The code will also be released upon acceptance. \n\n**Question 2**: Please refer to the details we have mentioned in weakness 3 for information how we handle noise and varying quality.\n\nSome pre-processing is indeed applied. First, we search YouTube for tourism videos using the keywords 'walk in' or 'walk through', and then downloaded videos that are at least 30 minutes long. Then we remove the first 5 minutes and the last 5 minutes of the video and keep only the middle part, because there may be a quick preview or summary which can contain non-smooth scene transitions. We will include these details in the appendix of camera-ready paper for clarity.\n\n**Question 3**: Please refer to the response of weakness 2."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589892932,
                "cdate": 1700589892932,
                "tmdate": 1700589892932,
                "mdate": 1700589892932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nLsNGVlTUI",
            "forum": "NYN1b8GRGS",
            "replyto": "NYN1b8GRGS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1206/Reviewer_jxyk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1206/Reviewer_jxyk"
            ],
            "content": {
                "summary": {
                    "value": "Observing that existing datasets for learning image-matching algorithm lacks diversity, the authors propose to learn image matchers from diverse tourism videos available on the internet. To obtain (pseudo-)labels for training, the authors propose to aggregate predictions from multiple matchers trained on small datasets and use a label propagation algorithm to propagate labels beyond nearby frames. Training on the pseudo-labeled data, existing image matching techniques demonstrate strong generalizability to unseen image domains, significantly outperforming image matchers trained on traditional datasets. In addition, the authors demonstrated that the proposed self-training framework showed strong scaling behavior, promising stronger image matchers for future work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Simple and scalable framework: The proposed self-training framework is simple and scalable. \n2. Strong zero-shot generalizability: Compared to image matchers trained on the traditional datasets, image matchers trained using self-training demonstrated stronger zero-shot generalizability, yielding more robust and performant image matchers. \n3. Comprehensive experiments: Experiments include large collections of datasets and downstream tasks, showing the superiority of self-trained image matchers."
                },
                "weaknesses": {
                    "value": "1. Lacking real indoor datasets in the benchmark: This is a nitpick but it would be great to have more real indoor datasets in the benchmark. Right now, most of the real datasets are driving-related and the indoor dataset only covers basements and corridors."
                },
                "questions": {
                    "value": "Questions:\n1. Will the performance continue to improve if the self-training is repeated multiple times or do the authors expect the models to start degrading due to noisy pseudo-labels?\n2. Current work focuses on tourism videos. Would the approach work for other types of videos such as egocentric videos?\n\n\nSuggestions:\n1. (Related Work) Image Matching Datasets: MegaDepth is outdoor and ScanNet is indoor?\n2. Section 3.1: \u201cMulti-method Matching\u201d was a little confusing. Would it make sense to use \u201cPseudo-matches Generation through Multi-method Matching\u201d?\n3. Table 1 KIT: GIM_LoFTR is slightly worse that LoFTR (out). \n4. Table 2 w/o video: The reviewer assumes this is DKM (IN). It would be nice to specify this in the table. \n\n\nPre-rebuttal Rating: Overall, this is a good paper that presents an alternative to learning strong image matcher. The zero-shot investigation is insightful, and the framework\u2019s strong zero-shot performance is encouraging despite its simplicity. The reviewer recommends accepting the paper prior to the rebuttal stage."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1206/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1206/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1206/Reviewer_jxyk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706634150,
            "cdate": 1698706634150,
            "tmdate": 1699636047293,
            "mdate": 1699636047293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9yQdEFaBYI",
                "forum": "NYN1b8GRGS",
                "replyto": "nLsNGVlTUI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviews"
                    },
                    "comment": {
                        "value": "Thank you for the in-depth review. We will address all 4 suggestions in the camera-ready paper and provide responses to the major comments below. \n\n**Weakness 1.**: This is a very good suggestion. To facilitate better zero-shot evaluation, we will include more diverse real-world indoor scenes (e.g., SUN3D, ScanNet V2) during the code release of GIM and ZEB.\n\n**Question 1.**: Thank you for this insightful question. Since the ``iterative self-training process'' could be interpreted in different ways, we provide answers based on two possible interpretations. We will also clarify this in the camera-ready paper.\n\n**(Case 1) Using a fixed set of videos in multiple rounds of iterative self-training**: In this case, each round of self-training uses the current best GIM model (trained in the previous round) to re-generate the labels on the same set of videos, and then updates the GIM model with the re-generated labels. The expected improvement would come from better video labels and longer training time on the same set of videos. In terms of the video labels, Table 2 of the main paper shows that a better label generation method leads to better GIM performance. Hence, we would expect that this bootstrapping could in principle provide a further performance boost. However, the improvement of a single method may not be very significant in practice, due to the use of multiple complementary methods for label generation. In terms of the longer training time, image matching models are prone to overfitting, hence early stopping is applied during training. GIM does not change the baseline architecture or training curriculum and might also exhibit the overfitting problem for long training times, if the video dataset is too small and not expanded over time. Hence, if ``iterative self-training'' refers to case 1, it may slightly improve results but will probably not be as effective as expanding the video dataset at the same time. We discuss this case below and expect it to be the best way to boost the performance of GIM with iterative self-training.\n\n**(Case 2) Expanding the number of videos in multiple rounds of iterative self-training**: In this case, each round of self-training uses the current best GIM model (trained in the previous round) to generate labels on a **new** set of videos, and then performs the next round of self-training with both the old and new videos. Table 2 of the main paper shows that the most effective way to improve GIM is to add more video data, rather than generating better labels on a fixed set of videos. Hence, given the limited time and training resources (GPUs) and the potentially infinite amount of available internet videos, using the current best GIM model to generate labels on a new set of videos can best improve the performance. In this case, albeit less likely, overfitting may still happen when training the model for a long time. Hence, we recommend keeping early stopping active and re-initializing GIM weights for multiple rounds of self-training.\n\n**Question 2.**: We expect that GIM will also work with egocentric videos since there is nothing specific to the camera perspective in the algorithm. The more important aspect is that a similar level of diversity as in the case of the tourism videos is ensured (e.g., different geo-locations, lightning conditions, etc.). Training GIM on a mixture of diverse tourism videos and ego-centric videos may also be a good choice and boost generalization performance further. It is an interesting direction for further investigation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589388863,
                "cdate": 1700589388863,
                "tmdate": 1700589388863,
                "mdate": 1700589388863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YbJWFMvG2c",
            "forum": "NYN1b8GRGS",
            "replyto": "NYN1b8GRGS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1206/Reviewer_UvwB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1206/Reviewer_UvwB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a Generalizable Image Matcher. It generalizes across training sets, view points, it works with BEV and even point cloud.\n\nThey also proposed ZEB, the first zero-shot evaluation benchmark for image matching. The claim it mixes data from diverse domains,\nby using it one can assess the cross-domain generalization performance.\n\nExtensive experiments on state-of-the-art baselines are compared."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The overview image in page.1 is impressive already. The method works on three strongest baseline (DKM, SuperGlue, and LoFTR) and improves them further more. It surprises me the method works with such huge view point differences and it also works with BEV pointcloud.\n\nThe training is using internet videos which prevents the COLMAP (SfM + MVS) bias for a single scene.\n\nThe proposed GIM is essentially a point matching ground-truth reinvention by using the label propagation through video with strong augmentation. However, it's so effective on every method by using the same training according to section 3.1.\n\nI consider the simplicity not a weakness, but as a strength. If the proposed method is reproducable, I believe it would be the new standard of image matching.\n\nThe reconstruction results in Fig.3,4,5 are very impressive especially when you know it's training on sequence video instead of SfM alike scenario."
                },
                "weaknesses": {
                    "value": "This paper is very impressive, I think the only thing left is just some implementation details becuase the self-training part is very short and only about the ground-truth instead of the training itself. \n\nThe only thing left is just open-sourcing the proposed code of the label propagation and training data to verify it's accuracy."
                },
                "questions": {
                    "value": "Please share more about the training details. By domain specific training is it just swapping the GT or there are more details that's not being covered in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789730153,
            "cdate": 1698789730153,
            "tmdate": 1699636047211,
            "mdate": 1699636047211,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "89sa2yJNXK",
                "forum": "NYN1b8GRGS",
                "replyto": "YbJWFMvG2c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviews"
                    },
                    "comment": {
                        "value": "Thank you for the encouraging review, your appreciation of our simple yet effective method, and your valuable suggestions. \n\n**Weaknesses (Implementation details and code release)**: The training part mostly follows the baseline code as mentioned in the ``implementation details'' part of the paper. \n\nTo ensure reproducibility, we will provide more details such as training time on different GPUs and for different base architectures, as discussed in the response to Reviewer-sN7h. We will also release the pre-trained GIM models, the training and ZEB evaluation data, and the code for label generation and model training.\n\n**Questions (Meaning of domain specific training)**: In the paper, we refer to training on standard image matching datasets (such as e.g., MegaDepth, ScanNet and unlike in-the-wild data from YouTube) as *domain-specific training*. In the experiments, we use the officially released models, i.e., SuperGlue (OUT), LoFTR (OUT) and DKM (OUT), as *domain-specific* models for GIM label generation. We will clarify this point in the camera-ready paper to avoid misunderstandings."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589107217,
                "cdate": 1700589107217,
                "tmdate": 1700589107217,
                "mdate": 1700589107217,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aqS2wJGwTa",
            "forum": "NYN1b8GRGS",
            "replyto": "NYN1b8GRGS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1206/Reviewer_sN7h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1206/Reviewer_sN7h"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a self-supervised method for image correspondence learning from easily accessible internet videos. The proposed method first trains an image-matching network on a standard dataset with GT supervision and then combines it with complementary image-matching methods to generate candidate correspondences between nearby frames of internet videos. Then, robust fitting is applied to remove outliers in the generated candidate correspondences. The remaining correspondences are used to re-train the image-matching network. By using this self-supervised training scheme, the performance improvement on existing image-matching models is impressive."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Many of the current deep networks suffer from poor generalization ability to unknown data distributions when the amount and diversity of training data are limited. Fine-tuning the model on the target data distribution with a small amount of data from the target domain with GT supervision is a natural way. However, obtaining GT information of the data from the target domain might not always be easy, especially in correspondence matching, pose estimation, 3D reconstruction, etc. \n\nTo address this challenge, this paper introduces a self-supervision strategy for image matchers using readily available internet videos. There is no need to run an SfM or COMAP pipeline, which is usually computationally expensive and un-robust to in-the-wild images, to obtain the GT correspondences. The performance improvement on three standard image-matching networks, diverse evaluation images, and various downstream tasks is impressive."
                },
                "weaknesses": {
                    "value": "My comments below are more like questions instead of weaknesses. \n\n(1) The proposed method combines a baseline image-matching network (e.g., SuperGlue, LoFTR, DKM) trained on a standard dataset and complementary image-matching methods to generate candidate correspondences. From the experiments section, the complementary image matching methods perform inferiorly than the baseline network. I have two questions here. \n     a. Since the performance is inferior, why are they needed? Will this increase the number of estimated correspondences between two images and thus improve the performance?\n     b. The introduction says \"multiple\" complementary image matching methods are used. While in the experiments section, it seems that only one complementary image matching method is used for each baseline method (SuperGlue, LoFTR & DKM). Will the number of complementary methods affect the performance?\n\n(2) Does batch size affect the self-supervision performance? From the implementation details, the GIM label generation on 50 hours of YouTube videos takes 4 days on 16 A100 GPUs. How long will it take to re-train a baseline network using the generated labels (and on which type and how many GPUs)? Will the proposed method still work on an RTX 3090, which is commonly used in universities?"
                },
                "questions": {
                    "value": "Please refer to the weakness section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809213756,
            "cdate": 1698809213756,
            "tmdate": 1699636047128,
            "mdate": 1699636047128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PuL3GFdAE6",
                "forum": "NYN1b8GRGS",
                "replyto": "aqS2wJGwTa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviews"
                    },
                    "comment": {
                        "value": "**Weakness (1)**: You are right, the complementary methods are introduced to increase the number of correspondences. Specifically multi-method matching is conducted only on nearby video frames ($<$80 frame interval), in which setting complementary methods are still able to provide reliable correspondences, albeit with inferior performance. The reason that complementary methods can increase the number of correspondences is that different methods often generate correspondences with different distributions. For example, RootSIFT generates correspondences on key point pixels. DKM outputs are mostly located on non-key point pixels. This strategy enhances the correspondence density between nearby video frames, which allows label propagation to produce training images with a larger pose difference.\n\nWe do not just use 1 complementary method for each baseline, but rather all available methods that are inferior to the baseline, i.e., for training GIM$\\_{SuperGlue}$, we use RootSIFT+SuperGlue to generate video labels. For training GIM$\\_{LoFTR}$, we use RootSIFT+SuperGlue+LoFTR and for training GIM$\\_{DKM}$, we use RootSIFT+SuperGlue+LoFTR+DKM to generate video labels. More complementary methods should in principle benefit the performance of GIM. For fair comparison, we use only inferior complementary methods so that we do not use labels generated by better methods to improve the baseline.\n\nWe will clarify both points in camera-ready manuscript to avoid misunderstandings, thanks for your question.\n\n**Weakness (2)**: Yes, batch size affects the self-supervision performance; generally enlarging the batch size improves the performance. We follow the official implementation of the baselines to set the batch size.\n\nIn terms of the training time of GIM, if we use 8 A100 GPUs, it takes about 4, 5 and 7.5 days to train GIM$\\_{SuperGlue}$, GIM$\\_{LoFTR}$, and GIM$\\_{DKM}$ respectively.\n\nYes, GIM works on GPUs with smaller GPU memory such as the RTX-3090, which is the GPU we used for initial experiments. For label generation, it takes 15 days to process 50 hours of videos on 4 RTX-3090 GPUs. For training, it takes 9.5, 12, and 19 days to train GIM$\\_{SuperGlue}$, GIM$\\_{LoFTR}$, and GIM$\\_{DKM}$ respectively. We apply gradient accumulation when using the RTX-3090 GPU to ensure the same training batch size as on the A100 GPU.\n\nWe will clarify these points in the appendix of the camera-ready manuscript to ensure the reproducibility of GIM across various setups."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588981122,
                "cdate": 1700588981122,
                "tmdate": 1700588981122,
                "mdate": 1700588981122,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dfu2S1SfDC",
                "forum": "NYN1b8GRGS",
                "replyto": "PuL3GFdAE6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1206/Reviewer_sN7h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1206/Reviewer_sN7h"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the convincing response. I keep my original rating for acceptance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642036881,
                "cdate": 1700642036881,
                "tmdate": 1700642036881,
                "mdate": 1700642036881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]